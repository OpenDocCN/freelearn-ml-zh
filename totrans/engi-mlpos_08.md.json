["```py\n    %matplotlib inline\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import azureml.core\n    from azureml.core import Workspace\n    from azureml.core.model import Model\n    # display the core SDK version number\n    print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n    ```", "```py\n    ws = Workspace.from_config()\n    print(ws.name, ws.resource_group, ws.location, sep = '\\n')\n    scaler = Model(ws,'scaler').download(exist_ok=True)\n    model = Model(ws,'support-vector-classifier').download(exist_ok=True)\n    ```", "```py\n    %%writefile score.py\n    import json\n    import numpy as np\n    import os\n    import pickle\n    import joblib\n    import onnxruntime\n    import time\n    from azureml.core.model import Model\n    ```", "```py\n    def onnxruntime we can deserialize the support vector classifier model. The InferenceSession() function is used for deserializing and serving the model for inference, and the input_name and label_name variables are loaded from the deserialized model.  \n    ```", "```py\n    def run() function takes raw incoming data as the argument, performs ML model inference, and returns the predicted result as the output. When called, the run() function receives the incoming data, which is sanitized and loaded into a variable for scaling. The incoming data is scaled using the scaler loaded previously in the init() function. Next, the model inference step, which is the key step, is performed by inferencing scaled data to the model, as shown previously. The prediction inferred from the model is then returned as the output. This way, the scoring file is written into score.py to be used for deployment.\n    ```", "```py\n    from azureml.core.conda_dependencies import CondaDependencies \n    myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"joblib\", \"azureml-core\", \"azureml-defaults\", \"scikit-learn==0.20.3\"])\n    with open(\"myenv.yml\",\"w\") as f:\n        f.write(myenv.serialize_to_string())\n    ```", "```py\n    from azureml.core.model import InferenceConfig\n    from azureml.core.environment import Environment\n    myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n    inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n    from azureml.core.webservice import AciWebservice\n    aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n    memory_gb=1, \n    tags={\"data\": \"weather\"}, \n    description='weather-prediction')\n    ```", "```py\n    %%time\n    from azureml.core.webservice import Webservice\n    from azureml.core.model import InferenceConfig\n    from azureml.core.environment import Environment\n    from azureml.core import Workspace\n    from azureml.core.model import Model\n    ws = Workspace.from_config()\n    model1 = Model(ws, 'support-vector-classifier')\n    model2 = Model(ws, 'scaler')\n    service = Model.deploy(workspace=ws, \n                           name='weatherprediction', \n                           models=[model1, model2], \n                           inference_config=inference_config, \n                           deployment_config=aciconfig)\n    service.wait_for_deployment(show_output=True)\n    ```", "```py\n    Running..............................................................................\n    Succeeded\n    ACI service creation operation finished, operation \"Succeeded\"\n    CPU times: user 610 ms, sys: 103 ms, total: 713 ms\n    Wall time: 7min 57s\n    ```", "```py\n    print(service.scoring_uri)\n    print(service.swagger_uri)\n    ```", "```py\n    import json\n    test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]})\n    test_sample = bytes(test_sample,encoding = 'utf8')\n    prediction = Temperature_C, Humidity, Wind_speed_kmph, Wind_bearing_degrees, Visibility_km, Pressure_millibars, and Current_weather_condition. Encode the input data in UTF-8 for smooth inference. Upon inferring the model using service.run(), the model returns a prediction of 0 or 1. 0 means a clear sky and 1 means it will rain. Using this service, we can make weather predictions at the port of Turku as tasked in the business problem.\n    ```", "```py\n    import requests\n    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n    if service.auth_enabled:\n        headers['Authorization'] = 'Bearer '+ service.get_keys()[0]\n    elif service.token_auth_enabled:\n        headers['Authorization'] = 'Bearer '+ service.get_token()[0]\n    scoring_uri = service.scoring_uri\n    print(scoring_uri)\n    response = requests.post(scoring_uri, data=test_sample, headers=headers)\n    print(response.status_code)\n    print(response.elapsed)\n    print(response.json())\n    ```", "```py\n    %matplotlib inline\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    import azureml.core\n    from azureml.core import Workspace\n    from azureml.core.model import Model\n    # display the core SDK version number\n    print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n    ```", "```py\n    ws = Workspace.from_config()\n    print(ws.name, ws.resource_group, ws.location, sep = '\\n')\n    scaler = Model(ws,'scaler').download(exist_ok=True)\n    model = Model(ws,'support-vector-classifier').download(exist_ok=True)\n    ```", "```py\n    %%writefile score.py\n    import json\n    import numpy as np\n    import os\n    import pickle\n    import joblib\n    import onnxruntime\n    import time\n    from azureml.core.model import Model\n    ```", "```py\n    def init():\n        global model, scaler, input_name, label_name\n        scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'scaler/2/scaler.pkl')\n        # deserialize the model file back into a sklearn model\n        scaler = joblib.load(scaler_path)\n\n        model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'support-vector-classifier/2/svc.onnx')\n        model = onnxruntime.InferenceSession(model_onnx, None)\n        input_name = model.get_inputs()[0].name\n        label_name = model.get_outputs()[0].name\n    ```", "```py\n    def run(raw_data):\n                    try: \n                        data = np.array(json.loads(raw_data)['data']).astype('float32')\n                        data = scaler.fit_transform(data.reshape(1, 7))\n                        # make prediction\n                        model_prediction = model.run([label_name], {input_name: data.astype(np.float32)})[0]\n                        # you can return any data type as long as it is JSON-serializable\n\n                    except Exception as e:   \n                        model_prediction = 'error'\n\n                    return model_prediction\n    ```", "```py\n    from azureml.core import Environment\n    from azureml.core.conda_dependencies import CondaDependencies \n    conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=[\"numpy\", \"onnxruntime\", \"joblib\", \"azureml-core\", \"azureml-defaults\", \"scikit-learn==0.20.3\"])\n    myenv = Environment(name='myenv')\n    myenv.python.conda_dependencies = conda_deps\n    # use an image available in public Container Registry without authentication\n    myenv.docker.base_image = \"mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda\"\n    ```", "```py\n    from azureml.core.model import InferenceConfig\n    inf_config = InferenceConfig(entry_script='score.py', environment=myenv)\n    ```", "```py\n    %%time\n    from azureml.core.compute import ComputeTarget\n    from azureml.core.compute_target import ComputeTargetException\n    from azureml.core.compute import AksCompute, ComputeTarget\n    # Choose a name for your AKS cluster\n    aks_name = 'port-aks' \n    # Verify that cluster does not exist already\n    try:\n        aks_target = aks_name = port-aks) already exists, a new cluster will not be created. Rather, the existing cluster (named port-aks here) will be attached to the workspace for further deployments.\n    ```", "```py\n    from azureml.core.webservice import Webservice, \n    AksWebservice\n    # Set the web service configuration (using default here)\n    aks_config = AksWebservice.deploy_configuration()\n    %%time\n    from azureml.core.webservice import Webservice\n    from azureml.core.model import InferenceConfig\n    from azureml.core.environment import Environment\n    from azureml.core import Workspace\n    from azureml.core.model import Model\n    ws = Workspace.from_config()\n    model1 = Model(ws, 'support-vector-classifier')\n    model2 = Model(ws, 'scaler')\n    ```", "```py\n    %%time\n    aks_service_name ='weatherpred-aks'\n    aks_service = Model.deploy (workspace=ws,\n                               name=aks_service_name,\n                               models=[model1, model2],\n                               inference_config=inf_config,\n                               deployment_config=aks_config,\n                               deployment_target=aks_target)\n    aks_service.wait_for_deployment(show_output = True)\n    print(aks_service.state)\n    ```", "```py\n    Running........................ Succeeded AKS service creation operation finished, operation \"Succeeded\"\n    ```", "```py\n    import json\n    test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]})\n    test_sample = bytes(test_sample,encoding = 'utf8')\n    prediction = service.run(input_data=test_sample)\n    ```", "```py\n    import requests\n    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n    if service.auth_enabled:\n        headers['Authorization'] = 'Bearer '+ service.get_keys()[0]\n    elif service.token_auth_enabled:\n        headers['Authorization'] = 'Bearer '+ service.get_token()[0]\n    scoring_uri = service.scoring_uri\n    print(scoring_uri)\n    response = requests.post(scoring_uri, data=test_sample, headers=headers)\n    print(response.status_code)\n    print(response.elapsed)\n    print(response.json())\n    ```", "```py\n    import numpy as np\n    import mlflow.azureml \n    import azureml.core\n    # display the core SDK version number\n    print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n    ```", "```py\n    from azureml.core import Workspace\n    from azureml.core.model import Model\n    ws = Workspace.from_config()\n    print(ws.name, ws.resource_group, ws.location, sep = '\\n')\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n    ```", "```py\n    from azureml.core.webservice import AciWebservice, Webservice\n    # Set the model path to the model folder created by your run \n    model_path = \"model path\"\n    ```", "```py\n    # Configure \n    aci_config = AciWebservice.deploy_configuration\n    (cpu_cores=1, \n    memory_gb=1, \n    tags={'method' : 'mlflow'}, \n    description='weather pred model',\n    location='eastus2')\n    # Deploy on ACI\n    (webservice,model) = mlflow.azureml.deploy(model_uri=\n    'runs:/{}/{}'.format(run.id, model_path), workspace=ws, \n    model_name='svc-mlflow', service_name='port-weather-pred', deployment_config=aci_config, tags=None, mlflow_home=None, synchronous=True)\n    webservice.wait_for_deployment(show_output=True)\n    ```"]