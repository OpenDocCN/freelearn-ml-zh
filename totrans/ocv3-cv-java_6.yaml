- en: Chapter 6. Detecting Foreground and Background Regions and Depth with a Kinect
    Device
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the field of video security applications, one often needs to notice the differences
    between frames because that's where the action happens. In other fields, it is
    also very important to isolate the objects from the background. This chapter shows
    several techniques to achieve this goal, comparing their strengths and weaknesses.
    Another completely different approach for detecting foreground or background regions
    is using a depth device like a **Kinect**. This chapter also deals with how to
    accomplish this goal with this device.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering:'
  prefs: []
  type: TYPE_NORMAL
- en: Background subtraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame differencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging background method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixture of Gaussian's method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contour finding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kinect depth maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have several approaches solving the problem
    of finding foreground/background regions, either through direct image processing
    or using a depth-compatible device such as a Kinect.
  prefs: []
  type: TYPE_NORMAL
- en: Background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with surveillance cameras, it's easy to see that most of the frame
    keeps still, while the moving objects, the ones we are interested in, are the
    areas that vary most over time. Background subtraction is defined as the approach
    used to detect moving objects from static cameras, also known as **foreground
    detection**, since we're mostly interested in the foreground objects.
  prefs: []
  type: TYPE_NORMAL
- en: In order to perform some valuable background subtraction, it is important to
    account for varying luminance conditions, taking care always to update our background
    model. Although some techniques extend the idea of background subtraction beyond
    its literal meaning, such as the mixture of Gaussian approach, they are still
    named like this.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compare all the solutions in the following sections, we will come
    up with a useful interface, which is called **VideoProcessor**. This interface
    is made of a simple method called **process**. The whole interface is given in
    the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we will implement this interface in the following background processors
    so that we can easily change them and compare their results. In this context,
    `Mat inputImage` refers to the current frame in the video sequence being processed.
  prefs: []
  type: TYPE_NORMAL
- en: All the code related to background subtraction can be found in the `background`
    project, available in the `chapter6` reference code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main application consists of two windows. One of them simply plays back
    the input video or the webcam stream, while the other one shows the output of
    applying a background subtractor that implements the `VideoProcessor` interface.
    This way, our main loop looks pretty much like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that upon successful image retrieval, we pass it to our `VideoProcessor`
    and update our windows. We also sleep for 10 ms so that the video playback will
    not look like a fast forward. This 10 ms delay is not the recorded frame delay
    and it is used because the focus here is not to play back at the same speed as
    the original file. In order to try the different subtraction approaches, we simply
    change the instantiation of our `VideoProcessor` class.
  prefs: []
  type: TYPE_NORMAL
- en: Frame differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It should be straightforward to think of a simple background subtraction in
    order to retrieve foreground objects. A simple solution could look similar to
    the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function simply subtracts each pixel of `backgroundImage` from `inputImage`
    and writes its absolute value in `foregroundImage`. As long as we have initialized
    the background to `backgroundImage` and we have that clear from objects, this
    could work as a simple solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here follows the background subtraction video processor code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The main method, `process`, is really simple. It only applies the absolute difference
    method. The only detail to remember is to initialize the background image in the
    constructor, which should correspond to the whole background being free from the
    foreground objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the output of applying ordinary background subtraction in the following
    image; it is important to check that the moving leaves in the background are not
    correctly removed since this is a weak background modeling. Also, remember to
    move the **Video Playback Example** window as it might be covering the **Background
    Removal Example** window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frame differencing](img/3972OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Averaging a background method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem with the background subtractor from the previous section is that
    the background will generally change due to illumination and other effects. Another
    fact is that the background may not be readily available, or the concept of background
    can change, for instance, when someone leaves a luggage in a video surveillance
    application. The luggage might be a foreground object for the first frames, but
    afterwards, it should be forgotten.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting algorithm to deal with these problems uses the running average
    concept. Instead of always using the first frame as a clear background, it will
    update it constantly by calculating a moving average of it. Consider the following
    equation, which will be executed, updating each pixel from the old average and
    considering each pixel from the recently acquired image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Averaging a background method](img/3972OS_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that ![Averaging a background method](img/3972OS_06_12.jpg) is the new
    pixel value; ![Averaging a background method](img/3972OS_06_13.jpg) is the value
    of the average background at time `t-1`, which would be the last frame; ![Averaging
    a background method](img/3972OS_06_14.jpg) is the new value for the background;
    and ![Averaging a background method](img/3972OS_06_15.jpg) is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, OpenCV already has the `accumulateWeighted` function, which performs
    the last equation for us. Now let''s see how the average background process is
    implemented in the `RunningAverageBackground` class as we check its `process`
    method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First, we convert the input image to gray level since we will store the average
    background like this, although we could make it with three channels. Then, if
    the accumulated background hasn't been started, we will have to set it to the
    first input image in the floating point format. Then we subtract the recently
    acquired frame from the accumulated background, which yields our foreground image,
    which we later threshold in order to remove small illumination or noisy changes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this time we use `Imgproc.THRESH_BINARY_INV`, which turns every pixel
    above the given threshold black, yielding black pixels for the foreground objects
    and white pixels for the background.
  prefs: []
  type: TYPE_NORMAL
- en: This way, we can use this image as a mask for updating only background pixels
    when using the `acccumulateWeighted` method later. On the following line, we only
    convert `inputImage` to `inputFloating` so that we can have it in the floating
    point format. We then use `accumulateWeighted` to apply our commented equation
    for the running average. Finally, we invert the image and return our foreground
    objects as white pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see a better modeling of the moving leaves on the background in the
    following image. Although thresholding makes it harder to compare these results
    with simple background subtraction, it is clear that lots of moving leaves have
    been removed. Besides, a good part of the hand has also been swept away. A careful
    tuning of the threshold parameter can be used for better results as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Averaging a background method](img/3972OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The mixture of Gaussians method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we can get very good results with the previous idea, some more advanced
    methods have been proposed in literature. A great approach, proposed by Grimson
    in 1999, is to use not just one running average, but more averages so that if
    a pixel fluctuates between the two orbit points, these two running averages are
    calculated. If it does not fit any of them, it is considered foreground.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, Grimson's approach also keeps the variance of the pixels, which is
    a measure of how far a set of numbers is spread out, taken from statistics. With
    a mean and a variance, a Gaussian model can be calculated and a probability can
    be measured to be taken into consideration, yielding a **Mixture of Gaussians
    model** (**MOG**). This can be very useful when branches and leaves are moving
    in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Grimson's method suffers from slow learning in the beginning
    and it can not distinguish between the moving shadows and moving objects. Therefore,
    an improved technique has been published by KaewTraKulPong and Bowden to tackle
    these problems. This one is implemented in OpenCV and it is quite straightforward
    to use it by means of the `BackgroundSubtractorMOG2` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to show how effective is the mixture of Gaussians approach, we have
    implemented a `BackgroundSubtractorMOG2`-based `VideoProcessor`. Its entire code
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we only need to instantiate the `BackgroundSubtractorMOG2` class
    and use the `apply` method, passing the input frame, the output image, and a learning
    rate that will tell how fast it should learn the new background. Besides the factory
    method without parameters, another one exists with the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `history` is the length of the history, `varThreshold` is the threshold
    on the squared Mahalanobis distance between the pixel and the model to decide
    whether a pixel is well described by the background model, and if `detectShadows`
    is `true`, the algorithm will detect and mark the shadows. If we do not set parameters
    by using the empty constructor, the following values are used by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '`defaultHistory = 500;`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`varThreshold = 16;`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectShadows = true;`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try playing with these values in order to look for better results when making
    background subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![The mixture of Gaussians method](img/3972OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we can clearly see a great background removal
    result with very little customization. Although some leaves still account for
    noise in the removed background result, we can see a good amount of the hand being
    correctly identified as foreground. A simple open morphological operator can be
    applied to remove some of the noise, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The mixture of Gaussians method](img/3972OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Contour finding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with the binary images removed from the background, it is important
    to transform pixels into useful information, such as by grouping them into an
    object or making it very clear for the user to see. In this context, it is important
    to know the concept of connected components, which are a set of connected pixels
    in a binary image, and OpenCV's function used to find its contours.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will examine the `findContours` function, which extracts
    contours of connected components in an image as well as a helper function that
    will draw contours in an image, which is `drawContours`. The `findContours` function
    is generally applied over an image that has gone through a threshold procedure
    as well as some canny image transformation. In our example, a threshold is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `findContours` function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is implemented using Suzuki's algorithm described in his paper *Topological
    structural analysis of digitized binary images by border following*. The first
    parameter is the input image. Make sure you work on a copy of your target image
    since this function alters the image. Also, beware that the 1 pixel border of
    the image is not considered. The contours that are found are stored in the list
    of `MatOfPoints`. This is simply a structure that stores points in a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '`Mat hierarchy` is an optional output vector that is set for each contour found.
    They represent 0-based indices of the next and previous contours at the same hierarchical
    level, the first child contour, and the parent contour, represented in the `hierarcy[i][0]`,
    `hierarcy[i][1]`, `hierarcy[i][2]`, and `hierarcy[i][3]` elements, respectively
    for a given `i` contour. If there aren''t contours corresponding to those values,
    they will be negative.'
  prefs: []
  type: TYPE_NORMAL
- en: The `mode` parameter deals with how the hierarchical relationships are established.
    If this is not interesting to you, you can set it as `Imgproc.RETR_LIST`. When
    retrieving the contours, the `method` parameter controls how they are approximated.
    If `Imgproc.CHAIN_APPROX_NONE` is set, all the contour points are stored. On the
    other hand, when using `Imgproc.CHAIN_APPROX_SIMPLE` for this value, horizontal,
    vertical, and diagonal lines are compressed by using only their endpoints. Other
    approximations are available as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to draw the obtained contours outline or fill them, Imgproc''s `drawContours`
    is used. This function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Mat image` is simply the destination image, while the list of `MatOfPoint`
    contours is the one obtained while calling `findContours`. The `contourIdx` property
    is the one intended to be drawn, while `color` is the desired color for drawing.
    Overloaded functions are also available in which the user can choose the thickness,
    line type, hierarchy max level, and an offset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When deciding on which contours are interesting, a useful function to help
    in that decision is to find the contour area. OpenCV implements this function
    through `Imgproc.contourArea`. This function can be found in the `chapter6` source
    code''s sample `connected` project. This application takes an image as input,
    runs a threshold over it and then uses it for finding the contours. Several options
    are available for testing the functions discussed in this section, such as whether
    it is filling the contour or painting the contour according to the area found.
    The following is a screenshot of this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Contour finding](img/3972OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When dealing with contours, it is also important to draw shapes around them
    in order to make measures or highlight what is found. The sample application also
    offers some code with instructions on how to draw a bounding box, circle, or convex
    hull around the contour. Let''s take a look at the main `drawContours()` function,
    which is called upon pressing the button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We firstly clone our target binary image, so we won''t change it. Then, we
    initialize the `MatOfPoint` structure and define the thickness flag. We then run
    `findContours`, ignoring the output hierarchy matrix. It is time to iterate the
    contours in the `for` loop. We use the `Imgproc.contourArea` helper function for
    an area estimate. Based on that, if it is the previous `areaThreshold` defined
    by the slider, it is drawn as green using the `drawContours` function or else
    it is drawn as red. An interesting part of the code are the shape drawing functions,
    which are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Drawing a bounding box is simple; it is just a matter of calling `Imgproc.boundingRect()`
    in order to identify the shape's surrounding rectangle. Then, the Imgproc's `rectangle`
    function method is called to draw the rectangle itself.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the enclosing circle is also easy due to the existence of the `minEnclosingCircle`
    function. The only caveat is converting `MatOfPoint` to `MatOfPoint2f`, which
    is accomplished by calling Contour's `convertTo`. The Imgproc's `circle` function
    deals with drawing it.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the convex hull is a rather important problem from a computational geometry
    perspective. It can be seen as putting an elastic band around a set of points
    and checking the final shape it takes. Fortunately, OpenCV also deals with this
    problem through the Imgproc's `convexHull` function. Note that in the first and
    the second line of `drawConvexHull` in the preceding code, `MatOfInt` is created,
    and `convexHull` is called, passing the current contour and this matrix as parameters.
    This function will return convex hull indexes in `MatOfInt`. We can draw lines
    ourselves, based on the coordinates of these indexes from the original contour.
    Another idea is to use the OpenCV's `drawContour` function. In order to do this,
    you need to build a new contour. This is done in the following lines in the code
    until `drawContour` is called.
  prefs: []
  type: TYPE_NORMAL
- en: Kinect depth maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the beginning of this chapter until now, we have focused on the background
    subtraction approaches that try to model the background of the scene using ordinary
    cameras and then on applying frame differencing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the Kinect is reported to work with Linux and OSX, this section deals
    only with Windows setup on OpenCV 2.4.7 version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will take a different approach. We will set how far we
    want our objects to be considered foreground and background, which means removing
    the background by selecting a depth parameter. Unfortunately, this can not be
    done using a single ordinary camera in a single shot, so we will need a sensor
    that tells us the depth of objects or try to determine depth from stereo, which
    is not in the scope of this chapter. Thanks to both gamers and several efforts
    from all around the world, this device has become a commodity and it is called
    a **Kinect**. Some attempts can be made to use two cameras and try to get depth
    from stereo, but the results might not be as great as the ones with the Kinect
    sensor. Here is how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kinect depth maps](img/3972OS_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'What makes the Kinect really different from an average camera is that it includes
    an infrared emitter and an infrared sensor that are able to project and sense
    a structured light pattern. It also contains an ordinary VGA camera so that the
    depth data can be merged into it. The idea behind the structured light is that
    when projecting a known pattern of pixels on to the objects, the deformation of
    this pattern allows the computer vision systems to calculate the depth and surface
    information from them. If a camera capable of registering infrared is used to
    record the emitted Kinect pattern, an image similar to the following can be seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kinect depth maps](img/3972OS_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although it might look like a random set of points, they are actually pseudo-random
    patterns that have been previously generated. These patterns can be identified
    and a disparity to depth relationship can be calculated, inferring the depth.
    More information can be acquired when studying structured light concepts if it
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: One should be aware of the implications this method has. As it relies on active
    infrared projection, some outdoor effects, such as direct sunlight will confuse
    the sensors, so outdoor use is not recommended. Users should also be aware that
    the depth range is from 0.8 meters to 4.0 meters (roughly from 2.6 feet to 13.1
    feet). Some shadows related to the IR projection can also make the results not
    look as great as they should, and cause some noise in the images. Despite all
    these issues, it is one of the best results available for the near field background
    removal.
  prefs: []
  type: TYPE_NORMAL
- en: The Kinect setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a Kinect should be straightforward, but we need to consider two important
    aspects. First we need to be sure that all the device driver softwares are correctly
    installed for using them. Then we need to check whether OpenCV has been compiled
    with Kinect support. Unfortunately, if you have downloaded precompiled binaries
    of version 2.4.7 from [http://sourceforge.net/projects/opencvlibrary/files/](http://sourceforge.net/projects/opencvlibrary/files/),
    as described in the beginning of [Chapter 1](ch01.html "Chapter 1. Setting Up
    OpenCV for Java"), *Setting Up OpenCV for Java* the out-of-the-box support is
    not included. We will briefly describe the setup instructions in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that not only the Xbox 360 Kinect device is commercialized,
    but also the Kinect for Windows. Currently, if you are creating commercial applications
    with the Kinect, you should go with the Kinect for Windows, although the Xbox
    360 Kinect works with the provided drivers.
  prefs: []
  type: TYPE_NORMAL
- en: The driver setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV Kinect support relies on OpenNI and PrimeSensor Module for OpenNI. An
    OpenNI framework is an open source SDK used for the development of 3D sensing
    middleware libraries and applications. Unfortunately, [OpenNI.org](http://OpenNI.org)
    site was available only until April 23rd, 2014, but the OpenNI source code is
    available on Github at [https://github.com/OpenNI/OpenNI](https://github.com/OpenNI/OpenNI)
    and [https://github.com/OpenNI/OpenNI2](https://github.com/OpenNI/OpenNI2). We
    will focus on using version 1.5.7.10 in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Although instructions for building the binaries are readily available, we can
    use installers provided in the code repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: After installing the OpenNI library, we will need to install the Kinect drivers.
    These are available at [https://github.com/avin2/SensorKinect/](https://github.com/avin2/SensorKinect/),
    and installers are specifically at [https://github.com/avin2/SensorKinect/tree/unstable/Bin](https://github.com/avin2/SensorKinect/tree/unstable/Bin).
  prefs: []
  type: TYPE_NORMAL
- en: 'When plugging your Xbox 360 Kinect device into Windows, you should see the
    following screenshot in your Device Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The driver setup](img/3972OS_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Make sure all of the three Kinect devices—**Audio**, **Camera**, and **Motor**—show
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One caveat that can happen is that if users forget to plug the power supply
    for the XBox 360 Kinect device, only **Kinect Motor** might show up since there
    isn't enough energy for the all three of them. Also, you won't be able to retrieve
    frames in your OpenCV application. Remember to plugin your power supply, and you
    should be fine.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenCV Kinect support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After ensuring that the OpenNI and Kinect drivers have been correctly installed,
    you need to check for the OpenCV Kinect support. Fortunately, OpenCV offers quite
    a useful function to check that. It is called `Core.getBuildInformation()`. This
    function shows important information about which options have been enabled during
    the OpenCV compilation. In order to check for Kinect support, simply output the
    result of calling this function to the console by using `System.out.println(Core.getBuildInformation());`
    and look for the video I/O section which looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The OpenCV Kinect support](img/3972OS_06_output.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It means OpenNI and Kinect support has not been enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, according to [Chapter 1](ch01.html "Chapter 1. Setting Up OpenCV for Java"),
    *Setting Up OpenCV for Java*, instead of typing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember to add the `WITH_OPENNI` flag, as given in the following line of code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instead of the preceding code, make sure you tick this option when using CMake''s
    GUI. Check for an output similar to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The OpenCV Kinect support](img/3972OS_06_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Make sure you point the OPENNI paths to your OpenNI correct installation folder.
    Rebuild the library, and now your `opencv_java247.dll` will be built with Kinect
    support.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now try checking your `Core.getBuildInformation()` again. The availability
    of OpenNI will be demonstrated in your Java console, as given in the following
    lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An alternative approach is using our configured Maven repository. We have added
    a runtime dependency to the book Maven repository, only available for Windows
    x86, which is very easy to configure. Simply follow the Java OpenCV Maven configuration
    section from [Chapter 1](ch01.html "Chapter 1. Setting Up OpenCV for Java"), *Setting
    Up OpenCV for Java*, and then, instead of adding the ordinary OpenCV dependency,
    `opencvjar-runtime`, use the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The complete POM file can be accessed in this chapter's Kinect project source
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure you check for some caveats, such as not mixing 32 bit and 64 bit drivers
    and libraries as well as Java runtime. If this is the case, you might receive
    **Can't load IA 32-bit .dll on a AMD 64-bit platform**, for instance. Another
    source of problems is forgetting to plugin the power supply for Kinect XBox 360,
    which will cause it to load only Kinect Motor.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are sure that the OpenNI and Kinect Drivers have been correctly
    installed as well as the OpenCV's OpenNI support, we are ready to move on to the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Kinect depth application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application focuses on the depth-sensing information from the Kinect as
    well as on the OpenCV API for OpenNI depth sensor, which means it won't cover
    some well-known Kinect features such as skeletal tracking (which puts nodes in
    important body parts like head, heap center, shoulder, wrists, hands, knees, feet,
    and others), gesture tracking, microphone recording, or tilting the device. Although
    we will just cover depth sensing, it is one of the most fantastic features of
    the Kinect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind this application is to segment an image from its depth
    information and combine it with a background image. We will capture an RGB frame
    from the Kinect device and retrieve its depth map. From a slider, you can choose
    how much depth you want for the segmentation. Based on that, a mask is generated
    through simple thresholding. The combined RGB frame and depth are now used to
    overlay a background image, resulting in an effect similar to chroma key compositing,
    but without the need for a green screen background, of course. This process can
    be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Kinect depth application](img/3972OS_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We should notice that in OpenCV version 2.4.7, the Java API does not support
    the Kinect depth sensing, but this is built on top of `VideoCapture`, so just
    some minor modifications related to constants will be required. For the sake of
    simplicity, these constants are in the main `App` class, but they should be refactored
    to a class that only deals with the OpenNI constants. Please look for the project
    `kinect` from this chapter in order to check for source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to work with depth-sensing images, we will need to follow these simple
    guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We will use the same `VideoCapture` class as the one used in [Chapter 2](ch02.html
    "Chapter 2. Handling Matrices, Files, Cameras, and GUIs"), *Handling Matrices,
    Files, Cameras, and GUIs*, for webcam input, with the same interface, passing
    the constant `CV_CAP_OPENNI` for telling it to retrieve frames from the Kinect.
    The difference here is that instead of using the `read` method,we will break this
    step in grabbing the frame and then retrieving either the depth image or the captured
    frame. Note that this is done by firstly calling the `grab` method and then the
    `retrieve` method, passing `CV_CAP_OPENNI_DEPTH_MAP` and `CV_CAP_OPENNI_BGR_IMAGE`
    as parameters. Make sure you send it to different matrices. Note that all these
    constants are extracted from the `highgui_c.h` file, which is located in the `opencv\modules\highgui\include\opencv2\highgui`
    path from OpenCV's source code tree. We will only work with the disparity map
    and RGB images from the Kinect, but one can also use the `CV_CAP_OPENNI_DEPTH_MAP`
    constant for receiving the depth values in mm as a `CV_16UC1` matrix, or `CV_CAP_OPENNI_POINT_CLOUD_MAP`
    for a point cloud map in a `CV_32FC3` matrix in which the values are XYZ coordinates
    in meters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main loop consists of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: First, we invoke the `grab` method to get the next frame from the Kinect. Then,
    we retrieve depth map and color images. As we have previously loaded our background
    in `resizedBackground`, we just clone it to `workingBackground`. Following this,
    we threshold our disparity image according to our slider level. This will make
    pixels farther away from our desired depth go black, while the ones we still want
    become white. It is time to clear our mask and combine it with the colored image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has really covered several areas that deal with background removal
    as well as some details that arise from this problem, such as the need to use
    connected components to find their contours. Firstly, the problem of background
    removal itself was established. Then, a simple algorithm such as frame differencing
    was analyzed. After that, more interesting algorithms, such as averaging background
    and **mixture of Gaussian** (**MOG**) were covered.
  prefs: []
  type: TYPE_NORMAL
- en: After using algorithms to deal with background removal problems, an insight
    about connected components was explored. Core OpenCV algorithms such as `findContours`
    and `drawContours` were explained. Some properties of contours were also analyzed,
    such as their area as well as convex hulls.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter finished with complete explanations of how to use the Kinect's depth
    sensor device as a background removal tool, for OpenCV 2.4.7\. After depth instructions
    on the device setup, a complete application was developed, making it clear to
    deal with the depth sensing sensors API.
  prefs: []
  type: TYPE_NORMAL
- en: Well, now it's time to jump from desktop applications to web apps in the next
    chapter. There, we'll cover details on how to set up an OpenCV-based web application,
    deal with image uploads, and create a nice augmented reality application based
    on the Tomcat web server. It is going to be fun, just watch out for Einstein's
    screenshots.
  prefs: []
  type: TYPE_NORMAL
