<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer076">
<h1 class="chapter-number" id="_idParaDest-132"><a id="_idTextAnchor134"/>8</h1>
<h1 id="_idParaDest-133"><a id="_idTextAnchor135"/>Machine Learning Pipelines and MLOps with LightGBM</h1>
<p>This chapter shifts the focus from data science and modeling problems to building production services for our ML solutions. We introduce the concept of machine learning pipelines, a systematic approach to processing data, and building models that ensure consistency <span class="No-Break">and correctness.</span></p>
<p>We also introduce the concept of MLOps, a practice that blends DevOps and ML and addresses the need to deploy and maintain production-capable <span class="No-Break">ML systems.</span></p>
<p>The chapter includes an example of building an ML pipeline using scikit-learn, encapsulating data processing, model building, and tuning. We show how to wrap the pipeline in a web API, exposing a secure endpoint for prediction. Finally, we also look at the containerization of the system and deployment to <span class="No-Break">Google Cloud.</span></p>
<p>The main topics of this chapter are <span class="No-Break">as follows:</span></p>
<ul>
<li>Machine <span class="No-Break">learning pipelines</span></li>
<li>An overview <span class="No-Break">of MLOps</span></li>
<li>Deploying an ML pipeline for <span class="No-Break">customer churn</span></li>
</ul>
<h1 id="_idParaDest-134"><a id="_idTextAnchor136"/>Technical requirements</h1>
<p>The chapter includes examples of creating scikit-learn pipelines, training LightGBM models, and building a FastAPI application. The requirements for setting up your environment can be found alongside the complete code examples <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-8"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-8</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor137"/>Introducing machine learning pipelines</h1>
<p>In <a href="B16690_06.xhtml#_idTextAnchor094"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Solving Real-World Data Science Problems with LightGBM</em>, we gave a detailed overview<a id="_idIndexMarker524"/> of the data science life cycle, which includes various steps to train an ML model. If we were to focus only on the steps required to train a model, given data that has already been collected, those would be <span class="No-Break">as follows:</span></p>
<ol>
<li>Data cleaning <span class="No-Break">and preparation</span></li>
<li><span class="No-Break">Feature engineering</span></li>
<li>Model training <span class="No-Break">and tuning</span></li>
<li><span class="No-Break">Model evaluation</span></li>
<li><span class="No-Break">Model deployment</span></li>
</ol>
<p>In previous case studies, we applied these steps manually while working through a Jupyter notebook. However, what would happen if we shifted the context to a long-term ML project? If we had to repeat the process when new data becomes available, we’d have to follow the same procedure to build a <span class="No-Break">model successfully.</span></p>
<p>Similarly, when we want to use the model to score new data, we must apply the steps correctly and with the correct parameters and configuration <span class="No-Break">every time.</span></p>
<p>In a sense, these steps form a pipeline for data: data enters the pipeline, and a deployable model results from <span class="No-Break">its completion.</span></p>
<p>Formally, an ML pipeline is a systematic and automated process that guides the workflow of an ML project. It involves several interconnected stages, encapsulating the steps <span class="No-Break">listed previously.</span></p>
<p>An ML pipeline aims to ensure that these tasks are structured, reproducible, and efficient, making it easier to manage complex ML tasks. Pipelines are particularly beneficial when working with large datasets or when the steps to transform raw data into usable inputs for ML models are complex and must be repeated frequently, such as in a <span class="No-Break">production environment.</span></p>
<p>There is some fluidity in the steps involved in a pipeline: steps may be added or removed depending on how the pipeline is utilized. Some pipelines include a data collection step, pulling data from various data sources or databases, and staging the data for <span class="No-Break">ML modeling.</span></p>
<p>Many ML services<a id="_idIndexMarker525"/> and frameworks provide functionality and utilities to implement ML pipelines. Scikit-learn provides this functionality through its <strong class="source-inline">Pipeline</strong> class, which we will look <span class="No-Break">at next.</span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor138"/>Scikit-learn pipelines</h2>
<p>Scikit-learn provides the <strong class="source-inline">Pipeline</strong> class as a tool<a id="_idIndexMarker526"/> to implement ML pipelines. The <strong class="source-inline">Pipeline</strong> class provides a unified interface to perform a sequence of data- and model-related tasks. Pipelines rely on scikit-learn’s standard <strong class="source-inline">fit</strong> and <strong class="source-inline">transform</strong> interfaces to enable the chaining of operations. Each pipeline consists of any number of intermediate steps, which must be <strong class="source-inline">transforms</strong>. A transform must implement both <strong class="source-inline">fit</strong> and <strong class="source-inline">transform</strong>, and the <strong class="source-inline">Pipeline</strong> class each transform in turn, first passing the data to <strong class="source-inline">fit</strong> and then to <strong class="source-inline">transform</strong>. The final step, which usually entails fitting the model to the data, only needs to implement the <strong class="source-inline">fit</strong> method. The transformations are usually preprocessing steps that transform or augment <span class="No-Break">the data.</span></p>
<p>The primary advantage of using scikit-learn pipelines is ensuring that the workflow is implemented clearly and in a reproducible manner. It helps avoid common mistakes, such as leaking statistics from the test data into the trained model during the preprocessing steps. By including the preprocessing steps within the pipeline, we ensure that the same steps are applied consistently during training and when the model is used to predict <span class="No-Break">new data.</span></p>
<p>Furthermore, scikit-learn pipelines can be combined with tools for model selection and hyperparameter tuning, such as grid search and cross-validation. We can use grid search to automatically select the best parameters across the entire pipeline by defining a grid of parameters for the preprocessing steps and the final estimator. This can significantly simplify the code and reduce errors in a complex ML workflow. Tools such as FLAML also feature integration with <span class="No-Break">scikit-learn pipelines.</span></p>
<p>For example, a simple pipeline can be created <span class="No-Break">as follows:</span></p>
<pre class="source-code">
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
pipeline = Pipeline(
    [
        ('scaler', StandardScaler()),
        ('linear', LinearRegression())
    ]
)</pre>
<p>Here, the pipeline consists of two steps: a scaling step that standardizes the data and a final step that fits a linear <span class="No-Break">regression model.</span></p>
<p>The power of scikit-learn’s <strong class="source-inline">Pipeline</strong> is that it can, in turn, be used as we would any other estimator <span class="No-Break">or model:</span></p>
<pre class="source-code">
pipeline.fit(X_train, y_train)
pipeline.predict(X_train, y_train)</pre>
<p>This provides us with a unified interface to all the steps encapsulated in the pipeline and makes reproducibility trivial. Furthermore, we can export the pipeline as we do for other scikit-learn models to enable deployment of the pipeline and all the steps <span class="No-Break">it encapsulates:</span></p>
<pre class="source-code">
import joblib
joblib.dump(pipeline, "linear_pipeline.pkl")</pre>
<p>We will show more examples of using scikit-learn <span class="No-Break">pipelines next.</span></p>
<p>Although we have spent much time<a id="_idIndexMarker527"/> addressing the concerns of working with data and building and tuning models, we haven’t yet taken an in-depth look at what happens after a model is trained. It’s here that the world of MLOps comes into play. The next section provides a <span class="No-Break">detailed overview.</span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor139"/>Understanding MLOps</h1>
<p><strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>) is a practice that blends the fields<a id="_idIndexMarker528"/> of ML and system operations. It is designed to standardize and streamline the life cycle of ML model development and deployment, thus increasing the efficiency and effectiveness of ML solutions within a business setting. In many ways, MLOps can be considered a response to the challenges associated with operationalizing ML, bringing DevOps principles into the <span class="No-Break">ML world.</span></p>
<p>MLOps aims to bring together data scientists, who typically focus on model creation, experimentation, and evaluation, and operations professionals, who deal with deployment, monitoring, and maintenance. The goal is to facilitate better collaboration between these groups, leading to faster, more robust <span class="No-Break">model deployment.</span></p>
<p>The importance of MLOps is underscored<a id="_idIndexMarker529"/> by the unique challenges presented by ML systems. Machine learning systems are more dynamic and less predictable than traditional software systems, leading to potential challenges in reliability and robustness, especially in a rapidly changing <span class="No-Break">production environment.</span></p>
<p>A central goal of MLOps is to accelerate the ML life cycle, facilitating faster experimentation and deployment. This is achieved <a id="_idIndexMarker530"/>through the automation of ML pipelines. <strong class="bold">Automation</strong> can cover various stages, including data preprocessing, feature engineering, model training, model validation, and deployment. Another crucial aspect of MLOps is ensuring reproducibility. Given the dynamic nature of ML models, it can be challenging to replicate results exactly, especially when models are retrained with new data. MLOps emphasizes the importance of versioning code, data, and model configurations, which ensures that every experiment can be precisely reproduced, which is crucial for debugging <span class="No-Break">and auditing.</span></p>
<p><strong class="bold">Monitoring</strong> is also a vital part of MLOps. Once<a id="_idIndexMarker531"/> a model is deployed, <em class="italic">monitoring its performance and continuously validating its predictions is critical</em>. MLOps emphasizes the need for robust monitoring tools that can track model performance, input data quality, and other vital metrics. Anomalies in these metrics may indicate that a model needs to be retrained <span class="No-Break">or debugged.</span></p>
<p>MLOps also encourages the use of robust testing practices for ML. ML testing includes traditional software testing practices, such as unit tests and integration tests, but also more ML-specific tests, such as validating the statistical properties of <span class="No-Break">model predictions.</span></p>
<p>MLOps also focuses on managing and scaling ML deployments. In the real world, ML models may need to serve thousands or even millions of predictions per second. DevOps practices such as containerization and serverless computing platforms come into play here to facilitate deployment and <span class="No-Break">scaling automation.</span></p>
<p>It’s important to note how MLOps fits into the broader software ecosystem. Just like DevOps has bridged the gap between development and operations in software engineering, MLOps aims to do the same for ML. By promoting shared understanding and responsibilities, MLOps can lead to more successful <span class="No-Break">ML projects.</span></p>
<p>MLOps is a rapidly evolving field becoming increasingly important as more businesses adopt ML. By applying principles from DevOps to the unique challenges of ML, MLOps provides a framework for managing the end-to-end ML life cycle, from initial experimentation to robust, scalable deployment. MLOps emphasizes standardization, automation, reproducibility, monitoring, testing, and collaboration to enable high-throughput <span class="No-Break">ML </span><span class="No-Break"><a id="_idIndexMarker532"/></span><span class="No-Break">systems.</span></p>
<p>We’ll now look at a practical example of creating an ML pipeline using scikit-learn and deploying the pipeline behind a <span class="No-Break">REST API.</span></p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor140"/>Deploying an ML pipeline for customer churn</h1>
<p>For our practical<a id="_idIndexMarker533"/> example, we’ll use<a id="_idIndexMarker534"/> the telecommunication (<strong class="bold">telco</strong>) Customer Churn dataset we worked with in <a href="B16690_05.xhtml#_idTextAnchor083"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">LightGBM Parameter Optimization with Optuna</em>. The dataset consists of descriptive information<a id="_idIndexMarker535"/> for each customer (such as gender, billing information, and charges) and whether the customer has left the telco provider (churn is <em class="italic">yes</em> or <em class="italic">no</em>). Our task is to build a classification model to <span class="No-Break">predict churn.</span></p>
<p>Further, we’d like to deploy the model behind a REST API such that it can be integrated into a more extensive software system. The REST API should have an endpoint that makes predictions for data passed to <span class="No-Break">the API.</span></p>
<p>We’ll use <strong class="bold">FastAPI</strong>, a modern, high-performance Python<a id="_idIndexMarker536"/> web framework, to build our API. Finally, we’ll deploy our model and API to Google Cloud Platform <span class="No-Break">using Docker.</span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor141"/>Building an ML pipeline using scikit-learn</h2>
<p>We will start by building an ML pipeline <a id="_idIndexMarker537"/>using scikit-learn’s <strong class="source-inline">Pipeline</strong> toolset. Our pipeline should encapsulate data cleaning and feature engineering steps, then build and tune an <span class="No-Break">appropriate model.</span></p>
<p>We’ll evaluate two algorithms for modeling: LightGBM and random forest, and as such, it’s unnecessary to perform any scaling or normalization of the data. However, the dataset has a unique identifier for each customer, <strong class="source-inline">customerID</strong>, which we need <span class="No-Break">to remove.</span></p>
<p>Further, the dataset consists of numerical and categorical features, and we must implement one-hot encoding for the <span class="No-Break">categorical features.</span></p>
<h3>Pipeline preprocessing steps</h3>
<p>To perform these steps<a id="_idIndexMarker538"/> within a scikit-learn pipeline, we’ll use <strong class="source-inline">ColumnTransformer</strong>. <strong class="source-inline">ColumnTransformer</strong> is a <strong class="source-inline">Pipeline</strong> transformer that operates only on a subset of the columns in the dataset. The transformer accepts a list of tuples in the form (<strong class="source-inline">name, transformer, columns</strong>). It applies the sub-transformers to the specified columns and concatenates the results such that all resultant features form part of the same <span class="No-Break">result set.</span></p>
<p>For example, consider the following DataFrame <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ColumnTransformer</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
df = pd.DataFrame({
    "price": [29.99, 99.99, 19.99],
    "description": ["Luxury goods", "Outdoor goods",
        "Sports equipment"],
})
ct = ColumnTransformer(
    [("scaling", MinMaxScaler(), ["price"]),
     ("vectorize", TfidfVectorizer(), "description")])
transformed = ct.fit_transform(df)</pre>
<p>Here, we have a DataFrame with two columns: <strong class="source-inline">price</strong> and <strong class="source-inline">description</strong>. A column transformer is created with two sub-transformers: a scaling transformer and a vectorizer. The scaling transformer applies min-max scaling only to the <strong class="source-inline">price</strong> column. The vectorizer applies TF-IDF vectorization only to the <strong class="source-inline">description</strong> column. When <strong class="source-inline">fit_transform</strong> is called, a <em class="italic">single</em> array is returned with a column<a id="_idIndexMarker539"/> for the scaled price and the columns representing the <span class="No-Break">word vectors.</span></p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">TF-IDF</strong>, or <strong class="bold">term frequency-inverse document frequency</strong>, is just one way of extracting a feature<a id="_idIndexMarker540"/> from text. Analyzing and extracting features from text, and natural language processing in general, is a broad field within ML that we won’t be delving<a id="_idIndexMarker541"/> into deeply here. You are encouraged to read further on the topic <span class="No-Break">at </span><a href="https://scikit-learn.org/stable/modules/feature_extraction.xhtml#text-feature-extraction"><span class="No-Break">https://scikit-learn.org/stable/modules/feature_extraction.xhtml#text-feature-extraction</span></a><span class="No-Break">.</span></p>
<p>We can set up our preprocessing<a id="_idIndexMarker542"/> for the Customer Churn dataset as a single <strong class="source-inline">ColumnTransformer</strong>. We first define the two individual transformers, <strong class="source-inline">id_transformer</strong> and <strong class="source-inline">encode_transformer</strong>, that apply to the ID columns and the <span class="No-Break">categorical features:</span></p>
<pre class="source-code">
id_transformer = (
    "customer_id",
    CustomerIdTransformer(id_columns),
    id_columns
)
encode_transformer = (
    "encoder",
    OneHotEncoder(sparse_output=False),
    categorical_features
)</pre>
<p>And then combine the separate transformers <span class="No-Break">into </span><span class="No-Break"><strong class="source-inline">ColumnTransformer</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
preprocessor = ColumnTransformer(
    transformers=[
        id_transformer,
        encode_transformer,
    ],
    remainder='passthrough'
)</pre>
<p><strong class="source-inline">ColumnTransformer</strong> is defined with the <strong class="source-inline">remainder='passthrough'</strong> parameter. The <strong class="source-inline">remainder</strong> parameter specifies what happens to the columns that <strong class="source-inline">ColumnTransformer</strong> does not transform. These columns are dropped by default, but we would like to pass them through, untouched, to include them in <span class="No-Break">the dataset.</span></p>
<p>The encoding transformer <a id="_idIndexMarker543"/>creates and applies <strong class="source-inline">OneHotEncoder</strong> to the <span class="No-Break">categorical features.</span></p>
<p>For illustrative purposes, we have created a custom transformer class to maintain the list of ID columns and drop them from the data <span class="No-Break">during transformation.</span></p>
<p>The class is <span class="No-Break">shown here:</span></p>
<pre class="source-code">
class CustomerIdTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, id_columns):
        self.id_columns = id_columns
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        return X.drop(columns=self.id_columns, axis=1)</pre>
<p>As we can see, the class extends <strong class="source-inline">BaseEstimator</strong> and <strong class="source-inline">TranformerMixin</strong> from the scikit-learn base classes and must implement <strong class="source-inline">fit</strong> and <strong class="source-inline">transform</strong>. Implementing these methods also makes it suitable as a transformer in a pipeline. Implementing <strong class="source-inline">fit</strong> is optional if required; in our case, nothing is done during <strong class="source-inline">fit</strong>. Our transformation step drops the <span class="No-Break">relevant columns.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">It’s important to encapsulate the functionality to drop irrelevant columns (in this case, the ID column) within the pipeline itself. When deploying the pipeline for production use, we expect these columns to be passed to the pipeline when making a prediction. Removing them as part of the pipeline simplifies our pipeline’s usage for our model’s consumers and reduces the chance of <span class="No-Break">user mistakes.</span></p>
<p>This completes the transformations<a id="_idIndexMarker544"/> required for preprocessing, and we are ready to move on to the following steps: fitting and tuning <span class="No-Break">the models.</span></p>
<h3>Pipeline modeling steps</h3>
<p>For the pipeline<a id="_idIndexMarker545"/> modeling part, we’ll use FLAML. We’ll also use the opportunity to show how parameters may be passed to steps within a pipeline. First, we define the settings for our <span class="No-Break">AutoML model:</span></p>
<pre class="source-code">
automl_settings = {
    "time_budget": 120,
    "metric": "accuracy",
    "task": "classification",
    "estimator_list": ["lgbm", "rf"],
    "custom_hp": {
        "n_estimators": {
            "domain": flaml.tune.uniform(20, 500)
        }
    },
    "verbose": -1
}</pre>
<p>The preceding code sets our time budget, optimization metric, and classification task for AutoML. We also limit the estimators to LightGBM and a random forest model. Finally, we customize the search space by specifying that <strong class="source-inline">n_estimators</strong> should be uniformly sampled between 20 <span class="No-Break">and 500.</span></p>
<p>The pipeline requires the parameter<a id="_idIndexMarker546"/> for constituent steps to be prefixed with the step’s name and a double underscore. We can set up a dictionary to pass these parameters to the AutoML class within <span class="No-Break">our pipeline:</span></p>
<pre class="source-code">
pipeline_settings = {
    f"automl__{key}": value for key, value in
        automl_settings.items()
}</pre>
<p>Here, <strong class="source-inline">automl</strong> is the name of the step in the pipeline. As such, for example, the parameters for time budget and metric are set as <strong class="source-inline">automl__time_budget: 120</strong> and <strong class="source-inline">automl__metric: </strong><span class="No-Break"><strong class="source-inline">accuracy</strong></span><span class="No-Break">, respectively.</span></p>
<p>Finally, we can add FLAML’s <span class="No-Break">AutoML estimator:</span></p>
<pre class="source-code">
automl = flaml.AutoML()
pipeline = Pipeline(
    steps=[("preprocessor", preprocessor),
           ("automl", automl)]
)</pre>
<p>The final pipeline is shown in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 8.1 – Final ML pipeline for Customer Churn prediction" height="783" src="image/B16690_08_01.jpg" width="992"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Final ML pipeline for Customer Churn prediction</p>
<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows a <em class="italic">ColumnTransformer</em> that consists<a id="_idIndexMarker547"/> of two sub-transformers, feeding into the <span class="No-Break">AutoML estimator.</span></p>
<h3>Model training and validation</h3>
<p>We are now ready<a id="_idIndexMarker548"/> to fit the pipeline to our data, passing<a id="_idIndexMarker549"/> the pipeline settings we set up earlier. Pipelines support the standard scikit-learn API so that we can call on the <span class="No-Break">pipeline itself:</span></p>
<pre class="source-code">
pipeline.fit(X, y, **pipeline_settings)</pre>
<p>Running <strong class="source-inline">fit</strong> executes all the preprocessing steps and then passes the data for AutoML modeling and tuning. The single <strong class="source-inline">Pipeline</strong> object illustrates the power of an ML pipeline: the <span class="No-Break">entire end-to-end</span> process, including a trained and tuned model, is encapsulated and portable, and we can utilize the pipeline as we could a single model. For example, the following code performs F1 scoring for the <span class="No-Break">training data:</span></p>
<pre class="source-code">
print(f"F1: {f1_score(pipeline.predict(X), y,
    pos_label='Yes')}")</pre>
<p>To export the pipeline, we <strong class="source-inline">joblib</strong> to serialize the model to <span class="No-Break">a file:</span></p>
<pre class="source-code">
joblib.dump(pipeline, "churn_pipeline.pkl")</pre>
<p>Exporting the pipeline allows us to re-instantiate and use it within our production code. Next, we’ll look at building an API for <span class="No-Break">our model.</span></p>
<p>At this stage, our pipeline (which encapsulates preprocessing, training, optimization, and validation) is defined, and we<a id="_idIndexMarker550"/> are ready to deploy<a id="_idIndexMarker551"/> it to a system. We’ll accomplish this by wrapping our model in an API <span class="No-Break">with FastAPI.</span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor142"/>Building an ML API using FastAPI</h2>
<p>We will now look at building a REST API around<a id="_idIndexMarker552"/> our pipeline, enabling consumers<a id="_idIndexMarker553"/> of our pipeline to get predictions via web requests. Building a web API for a model also simplifies integration with other systems and services and is the standard method for integration in a <span class="No-Break">microservices architecture.</span></p>
<p>To build the API, we use the Python <span class="No-Break">library FastAPI.</span></p>
<h3>FastAPI</h3>
<p>FastAPI is a modern, high-performance web framework<a id="_idIndexMarker554"/> for building APIs with Python 3.6+. It was designed from the ground up to be easy to use and enable high-performance API development. The key features of FastAPI are its speed and ease of use, making it an excellent choice for developing robust, production-ready APIs. FastAPI widely adopts Python’s type checking, which aids in catching errors early in the development process. It also uses these type hints to provide data validation, serialization, and documentation, reducing the boilerplate code developers need <span class="No-Break">to write.</span></p>
<p>The performance of FastAPI is one of its defining features. It is on par with Node.js and significantly faster than traditional Python frameworks. This speed is achieved due to its use of Starlette for the web parts and Pydantic for the data parts, and its non-blocking nature makes it suitable for handling many <span class="No-Break">concurrent requests.</span></p>
<p>FastAPI provides automatic interactive API documentation, a considerable advantage while developing complex APIs. Using FastAPI, developers gain access to automatically generated interactive API docs via Swagger UI. Swagger UI also provides functionality to interact with the REST resources without writing code or using external tooling. This feature makes FastAPI very developer-friendly and accelerates the <span class="No-Break">development process.</span></p>
<p>FastAPI also supports industry-standard security protocols, such as OAuth2, and provides tooling to ease implementation. Much of FastAPI’s tooling relies on its dependency injection system, allowing developers to manage dependencies and handle shared <span class="No-Break">resources efficiently.</span></p>
<p>FastAPI is well suited to building<a id="_idIndexMarker555"/> web APIs and microservices for ML models due to its ease of use and high performance, allowing ML engineers to focus on the myriad of other concerns surrounding production <span class="No-Break">ML deployments.</span></p>
<h3>Building with FastAPI</h3>
<p>To create a REST API with FastAPI, we can create<a id="_idIndexMarker556"/> a new Python script and instantiate the FastAPI instance. After the instance starts, we can load our model from <span class="No-Break">the file:</span></p>
<pre class="source-code">
app = FastAPI()
model = joblib.load("churn_pipeline.pkl")</pre>
<p>Loading the model at the start of the application increases the startup time but ensures that the API is ready to serve requests when the application <span class="No-Break">startup completes.</span></p>
<p>Next, we need to implement a REST endpoint to make predictions. Our endpoint accepts input data and returns the predictions as JSON. The input JSON is an array of JSON objects, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
[
  {
    "customerID": "1580-BMCMR",
    ...
    "MonthlyCharges": 87.3,
    "TotalCharges": "1637.3"
  },
  {
    "customerID": "4304-XUMGI",
    ...
    "MonthlyCharges": 75.15,
    "TotalCharges": "3822.45"
  }
]</pre>
<p>With FastAPI, we implement a REST endpoint by creating a function that takes the input data as parameters. FastAPI serializes<a id="_idIndexMarker557"/> the preceding JSON structure to a Python list of dictionaries. Therefore, our function signature is implemented <span class="No-Break">as follows:</span></p>
<pre class="source-code">
@app.post('/predict')
def predict_instances(
        instances: list[dict[str, str]]
):</pre>
<p>We decorate the function using a FastAPI <strong class="source-inline">post</strong> decorator, specifying the endpoint <span class="No-Break">path (</span><span class="No-Break"><strong class="source-inline">'/predict'</strong></span><span class="No-Break">).</span></p>
<p>To make the actual predictions for the model, we convert the dictionaries to a DataFrame and perform <span class="No-Break">the predictions:</span></p>
<pre class="source-code">
instance_frame = pd.DataFrame(instances)
predictions = model.predict_proba(instance_frame)</pre>
<p>We use <strong class="source-inline">predict_proba</strong> to get the probabilities for each class (Yes or No) since we want to send this additional information to the consumers of our API. Returning probabilities alongside predictions is a recommended practice, as this affords the API consumer more control over the use of the predictions. API consumers can decide what probability threshold is good enough for their application based on how the predictions <span class="No-Break">are used.</span></p>
<p>To return the results as JSON, we construct a dictionary that FastAPI then serializes to JSON. We use NumPy’s <strong class="source-inline">argmax</strong> to get the index of the highest probability to determine the predicted class and <strong class="source-inline">amax</strong> to get the highest<a id="_idIndexMarker558"/> <span class="No-Break">probability itself:</span></p>
<pre class="source-code">
results = {}
for i, row in enumerate(predictions):
    prediction = model.classes_[np.argmax(row)]
    probability = np.amax(row)
    results[i] = {"prediction": prediction,
        "probability": probability}
return results</pre>
<p>The preceding code produces a <strong class="source-inline">prediction</strong> object for each data instance in the input list, using the position in the list as an index. When the endpoint is called, the following JSON <span class="No-Break">is returned:</span></p>
<pre class="source-code">
{
  "0": {
    "prediction": "Yes",
    "probability": 0.9758797243307111
  },
  "1": {
    "prediction": "No",
    "probability": 0.8896770039274629
  },
  "2": {
    "prediction": "No",
    "probability": 0.9149225087944103
  }
}</pre>
<p>We have now built the core of the API endpoint. However, we must also pay attention to non-functional concerns such as security. Often, ML engineers neglect aspects such as security or performance<a id="_idIndexMarker559"/> and focus only on ML concerns. We mustn’t make this mistake and must ensure we give these concerns the <span class="No-Break">necessary attention.</span></p>
<h3>Securing the API</h3>
<p>To secure our endpoint, we’ll make use of HTTP Basic<a id="_idIndexMarker560"/> authentication. We use a preset username and password, which we read from the environment. This allows us to securely pass these credentials to the application during deployment and avoids pitfalls such as hardcoding the credentials. Our endpoint also needs to be enhanced to accept credentials from the user. HTTP Basic authentication credentials are sent as an <span class="No-Break">HTTP header.</span></p>
<p>We can implement this as follows. We first set up security for FastAPI and read the credentials from <span class="No-Break">the environment:</span></p>
<pre class="source-code">
security = HTTPBasic()
USER = bytes(os.getenv("CHURN_USER"), "utf-8")
PASSWORD = bytes(os.getenv("CHURN_PASSWORD"), "utf-8")</pre>
<p>We then add the following to the <span class="No-Break"><strong class="source-inline">endpoint</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
@app.post('/predict')
def predict_instances(
        credentials: Annotated[HTTPBasicCredentials,
            Depends(security)],
        instances: list[dict[str, str]]
):
    authenticate(credentials.username.encode("utf-8"),
        credentials.password.encode("utf-8"))</pre>
<p>The <strong class="source-inline">authenticate</strong> function validates the received credentials against the API credentials we got from the environment. We can use Python’s secrets library to do <span class="No-Break">the validation:</span></p>
<pre class="source-code">
def authenticate(username: bytes, password: bytes):
    valid_user = secrets.compare_digest(
        username, USER
    )
    valid_password = secrets.compare_digest(
        password, PASSWORD
    )
    if not (valid_user and valid_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Basic"},
        )
    return username</pre>
<p>If the credentials are invalid, we throw an exception with HTTP status code <strong class="source-inline">401</strong>, signaling that the consumer is <span class="No-Break">not authorized.</span></p>
<p>Our API endpoint<a id="_idIndexMarker561"/> is now fully implemented, secured, and ready for deployment. To deploy our API, we’ll containerize it <span class="No-Break">using Docker.</span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor143"/>Containerizing our API</h2>
<p>We can build a Docker<a id="_idIndexMarker562"/> container for our API with the <span class="No-Break">following Dockerfile:</span></p>
<pre class="source-code">
FROM python:3.10-slim
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends apt-utils
RUN apt-get -y install curl
RUN apt-get install libgomp1
WORKDIR /usr/src/app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD [ "uvicorn", "telco_churn_api:app", "--host", "0.0.0.0", "--port", "8080" ]</pre>
<p>The Dockerfile is straightforward: we start with a base Python 3.10 image and install some OS dependencies that LightGBM needs (<strong class="source-inline">libgomp1</strong>). We then set up the FastAPI app: we copy the Python <strong class="source-inline">requirements</strong> file, install all of them, and then copy the necessary source files (using <strong class="source-inline">COPY . .</strong> ).</p>
<p>Finally, we run a Uvicorn server, listening<a id="_idIndexMarker563"/> on all addresses on port <strong class="source-inline">8080</strong>. Uvicorn is an ASGI web server implementation for Python that supports async I/O, significantly increasing the web server’s throughput. We bind to port <strong class="source-inline">8080</strong>, our deployment platform’s <span class="No-Break">default port.</span></p>
<p>We can build and run the Docker image using the following commands, passing the username and password <span class="No-Break">environment variables:</span></p>
<pre class="source-code">
docker build . -t churn_api:latest
docker run --rm -it -e CHURN_USER=***** -e CHURN_PASSWORD=*********** -p 8080:8080 churn_api:latest</pre>
<p>The API should now be available on your localhost, on port <strong class="source-inline">8080</strong>, secured behind the credentials you provide in the <span class="No-Break">environment variables.</span></p>
<p>With our application containerized, we are ready to deploy our application to any platform that supports<a id="_idIndexMarker564"/> containers. For the churn application, we’ll deploy it to Google <span class="No-Break">Cloud Platform.</span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor144"/>Deploying LightGBM to Google Cloud</h2>
<p>We’ll leverage the <strong class="bold">Google Cloud Run</strong> platform to deploy our application<a id="_idIndexMarker565"/> to Google <span class="No-Break">Cloud</span><span class="No-Break"><a id="_idIndexMarker566"/></span><span class="No-Break"> Platform.</span></p>
<p>Google Cloud Run<a id="_idIndexMarker567"/> is a serverless platform that allows you to develop and run applications without worrying about infrastructure management. Cloud Run allows developers to run their applications in a secure, scalable, and zero-ops environment. Cloud Run is fully managed, meaning all infrastructure (such as servers and load balancers) is abstracted away, allowing users to focus on running their applications. Cloud Run also supports full autoscaling, and the number of running containers automatically increases to respond to increasing load. Cloud Run is also very cost-effective, as you are only charged when the container runs and <span class="No-Break">serves requests.</span></p>
<p>To use Cloud Run, you need a Google Cloud account<a id="_idIndexMarker568"/> and need to create a Google Cloud project, enable billing, and set up and initialize the <strong class="bold">Google Cloud CLI</strong>. The following resources guide<a id="_idIndexMarker569"/> you through <span class="No-Break">these steps:</span></p>
<ul>
<li><a href="https://console.cloud.google.com/getting-started"><span class="No-Break">https://console.cloud.google.com/getting-started</span></a></li>
<li><a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects"><span class="No-Break">https://cloud.google.com/resource-manager/docs/creating-managing-projects</span></a></li>
<li><a href="https://cloud.google.com/sdk/docs/install"><span class="No-Break">https://cloud.google.com/sdk/docs/install</span></a></li>
<li><a href="https://cloud.google.com/sdk/docs/initializing"><span class="No-Break">https://cloud.google.com/sdk/docs/initializing</span></a></li>
</ul>
<p>Once the Google Cloud setup is completed, we can deploy our API using the CLI. This can be accomplished using a <span class="No-Break">single command:</span></p>
<pre class="source-code">
gcloud run deploy --set-env-vars CHURN_USER=*****,CHURN_PASSWORD=***********</pre>
<p>Running the command prompts you for a service name and a region to deploy your service. We also set the environment variables needed for the security credentials. For deployment, Cloud Run creates Cloud Build for you, which automatically builds and stores the Docker container and then deploys it to <span class="No-Break">Cloud Run.</span></p>
<p>Once the Cloud Run<a id="_idIndexMarker570"/> command completes, we have deployed a secure, scalable, RESTful web API<a id="_idIndexMarker571"/> serving our customer churn <span class="No-Break">ML pipeline.</span></p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor145"/>Summary</h1>
<p>This chapter introduced ML pipelines, illustrating their advantages in enabling consistency, correctness, and portability when implementing <span class="No-Break">ML solutions.</span></p>
<p>An overview was given on the nascent MLOps field, a practice combining DevOps and ML to realize tested, scalable, secure, and observable production <span class="No-Break">ML systems.</span></p>
<p>Further, we discussed the scikit-learn <strong class="source-inline">Pipeline</strong> class, a toolset to implement ML pipelines using the familiar <span class="No-Break">scikit-learn API.</span></p>
<p>A practical, end-to-end example of implementing an ML pipeline for customer churn was also given. We showed how to create a scikit-learn pipeline that performs preprocessing, modeling, and tuning and is exportable for a software system. We then built a secure RESTful web API using FastAPI that provides an endpoint for getting predictions from our customer churn pipeline. Finally, we deployed our API to Google Cloud Platform using the Cloud <span class="No-Break">Run service.</span></p>
<p>Although our deployment is secure and fully scalable, with observability, metrics, and logs provided by Cloud Run, there are some ML-specific aspects our deployment does not address: model drift, model performance monitoring, <span class="No-Break">and retraining.</span></p>
<p>In the next chapter, we look at a specialized ML cloud service with AWS SageMaker, which provides a platform-specific solution for building and hosting cloud-based <span class="No-Break">ML pipelines.</span></p>
</div>
</div></body></html>