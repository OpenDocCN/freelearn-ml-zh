- en: Scala for Learning Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala学习分类
- en: In the previous chapter, we saw how to develop a predictive model for analyzing
    insurance severity claims as a regression analysis problem. We applied very simple
    linear regression, as well as **generalized linear regression** (**GLR**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何将分析保险严重索赔的预测模型作为回归分析问题来开发。我们应用了非常简单的线性回归，以及**广义线性回归**（**GLR**）。
- en: In this chapter, we'll learn about another supervised learning task, called
    classification. We'll use widely used algorithms such as logistic regression, **Naive
    Bayes** (**NB**), and **Support Vector Machines** (**SVMs**) to analyze and predict
    whether a customer is likely to cancel the subscription of their telecommunication
    contract or not.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将学习另一个监督学习任务，称为分类。我们将使用广泛使用的算法，如逻辑回归、**朴素贝叶斯**（**NB**）和**支持向量机**（**SVMs**），来分析和预测客户是否可能取消其电信合同的订阅。 '
- en: 'In particular, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将涵盖以下主题：
- en: Introduction to classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类简介
- en: Learning classification with a real-life example
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实际案例学习分类
- en: Logistic regression for churn prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于流失预测的逻辑回归
- en: SVM for churn prediction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于流失预测的支持向量机（SVM）
- en: NB for prediction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预测的NB
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Scala 2.11.x和Java 1.8.x已安装并配置在您的机器上。
- en: 'The code files of this chapters can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03)'
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：
- en: '[http://bit.ly/2ZKVrxH](http://bit.ly/2ZKVrxH)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2ZKVrxH](http://bit.ly/2ZKVrxH)'
- en: Overview of classification
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类概述
- en: As a supervised learning task, classification is the problem of identifying
    which set of observations (sample) belongs to what based on one or more independent
    variables. This learning process is based on a training set containing observations
    (or instances) about the class or label of membership. Typically, classification
    problems are when we are training a model to predict quantitative (but discrete)
    targets, such as *s*pam detection, churn prediction, sentiment analysis, cancer
    type prediction, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为监督学习任务，分类是根据一个或多个独立变量识别哪些观测集（样本）属于哪个集合的问题。这个过程基于包含关于类别或标签成员资格的观测（或实例）的训练集。通常，分类问题是我们训练模型来预测定量（但离散）目标的情况，例如垃圾邮件检测、流失预测、情感分析、癌症类型预测等。
- en: 'Suppose we want to develop a predictive model, which will predict whether a
    student is competent enough to get admission into computer science based on his/her
    competency in TOEFL and GRE. Also, suppose we have some historical data in the
    following range/format:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要开发一个预测模型，该模型将根据学生的托福和GRE能力预测其是否有足够的竞争力被录取到计算机科学专业。此外，假设我们有一些以下范围/格式的历史数据：
- en: '**TOEFL**: Between 0 and 100'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托福成绩**：介于0到100之间'
- en: '**GRE**: Between 0 and 100'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE**：介于0到100之间'
- en: '**Admission**: 1 for admitted, 0 if not admitted'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**录取**：如果录取则为1，如果不录取则为0'
- en: 'Now, to understand whether we can use such simple data to make predictions,
    let''s create a scatter plot by putting all the records with **Admitted** and
    **Rejected** as the dependent variables and **TOEFL** and **GRE** as the independent
    variables, like so:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了了解我们是否可以使用如此简单的数据来做出预测，让我们创建一个散点图，将所有记录的**录取**和**拒绝**作为因变量，**托福**和**GRE**作为自变量，如下所示：
- en: '![](img/004d677c-7313-4a40-bf44-90ee1afbcda1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/004d677c-7313-4a40-bf44-90ee1afbcda1.png)'
- en: By looking at the data points (imagine that the diagonal line in the graph is
    not there), we can reasonably develop a linear model to separate most of the data
    points. Now, if we draw a straight line between two classes of data, those almost
    get separated. Such a line (green, in our case) is called the decision boundary.
    So, if the decision boundary has reasonably separate maximal data points, it can
    be used for making predictions on unseen data, and we can also say that the data
    point above the line we predicted is competent for admission, and below the line
    we predict that the students are not competent enough.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Although this example is for a basic head-start into regression analysis, separating
    missions of data points is not very easy. Thus, to calculate where to draw the
    line for separating such a huge number of data points, we can use logistic regression
    or other classification algorithms that we will discuss in upcoming sections.
    We'll also see that drawing an ordinary straight line might not be the right one,
    and therefore we often have to draw curved lines.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the admission-related data plot carefully, maybe a straight line
    is not the best way of separating each data point—a curved line would be better,
    as shown in the following graph:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8513e773-de23-4536-b7f1-7daa4b32d5e0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: However, to get a curved decision boundary, we have to change not only the function
    (called the decision boundary function) that's responsible from being linear to
    some high-order polynomial, but also the data to be a second-degree polynomial.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that we have to model our problem as a logistic regression model.
    That is, we need to change the data from *{GRE, TOEFL**}* format to a quadratic
    function format, *{GRE, GRE^2, TOEFL, TOEFL^2, GRE∗TOEFL**}*. However, doing so
    in a hand-crafted way is cumbersome and will not be possible for large datasets. Fortunately,
    Spark MLlib has numerous algorithms implemented for modeling such problems and
    for solving other classification problems, including the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression** (**LR**)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NB
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilayer perceptron** (**MLP**)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree** (**DT**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest** (**RF**)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosted trees** (**GBT**)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a classification problem, actual (that is, true) labels (that is, class)
    and the predicted label (that is, class) exist for the samples that are used to
    train or test a classifier; this can be assigned to one of the following categories:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive (TP)**: The true label is positive and the prediction made
    by the classifier is also positive'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative (TN)**: The true label is negative and the prediction made
    by the classifier is also negative'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive (FP)**: The true label is negative but the prediction made
    by the classifier is positive'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative (FN)**: The true label is positive but the prediction made
    by the classifier is negative'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These metrics (TP, FP, TN, and FN) are the building blocks of the evaluation
    metrics for most of the classifiers we listed previously. However, the pure accuracy
    that is often used for identifying how many predictions were correct is not generally
    a good metric, so other metrics such as precision, recall, F1 score, AUC, and
    **Matthew''s correlation coefficient** (**MCC**) are used:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标（TP、FP、TN 和 FN）是我们之前列出的大多数分类器评估指标的基础。然而，通常用于识别正确预测数量的纯准确率并不是一个好的指标，因此使用其他指标，如精度、召回率、F1
    分数、AUC 和 **Matthew 的相关系数**（**MCC**）：
- en: '*Accuracy* is the fraction of samples that the classifier correctly predicted
    (both positive and negative), divided by the total number of samples:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确率*是指分类器正确预测的样本数（包括正负样本）与总样本数的比例：'
- en: '![](img/f75a674d-2f5c-4d02-b65d-6d52bfd3b3fb.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f75a674d-2f5c-4d02-b65d-6d52bfd3b3fb.png)'
- en: '*Precision* is the number of samples correctly predicted that belong to a positive
    class (true positives), divided by the total number of samples actually belonging
    to the positive class:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精度*是指属于正类（真阳性）的正确预测样本数除以实际属于正类的总样本数：'
- en: '![](img/acf55427-ec4f-415e-974f-eb47fd364dc4.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acf55427-ec4f-415e-974f-eb47fd364dc4.png)'
- en: '*Recall* is the number of samples that were correctly predicted to belong to
    a negative class, divided by the total number of elements actually belonging to
    the negative class:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率*是指正确预测属于负类的样本数除以实际属于负类的总元素数：'
- en: '![](img/2c2e3301-f478-4212-8034-c27a95f8855e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c2e3301-f478-4212-8034-c27a95f8855e.png)'
- en: 'F1 score is the harmonic mean of precision and recall. Since the F1 score is
    a balance between recall and precision, it can be considered as an alternative
    to accuracy:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1 分数是精度和召回率的调和平均数。由于 F1 分数是召回率和精度的平衡，它可以被认为是准确率的替代品：
- en: '![](img/21df9705-5b82-4f30-8758-c7c0bfa23a7f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21df9705-5b82-4f30-8758-c7c0bfa23a7f.png)'
- en: '**Receiver Operating Characteristics** (**ROC**) is a curve that''s drawn by
    plotting **FPR** (to *x*-axis) and **TPR** (to *y*-axis) for different threshold
    values. So, for different thresholds for your classifier, we calculate the **TPR**
    and **FPR**, draw the **ROC** curve, and calculate the area under the **ROC**
    curve (also known as **AUC**). This can be visualized as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）是通过绘制不同阈值值的 **FPR**（到 *x* 轴）和 **TPR**（到 *y* 轴）而得到的曲线。因此，对于分类器的不同阈值，我们计算
    **TPR** 和 **FPR**，绘制 **ROC** 曲线，并计算 **ROC** 曲线下方的面积（也称为 **AUC**）。这可以如下可视化：'
- en: '![](img/ed60cca4-db3c-4239-a13f-054b24e18629.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed60cca4-db3c-4239-a13f-054b24e18629.png)'
- en: 'MCC is regarded as a balanced measure of a binary classifier, even for a dataset
    that has very imbalanced classes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 被视为二元分类器的平衡度量，即使对于具有非常不平衡类别的数据集也是如此：
- en: '![](img/e1e62338-9246-4eb7-8cb6-9f51f6b31057.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1e62338-9246-4eb7-8cb6-9f51f6b31057.png)'
- en: Let's discuss a more real-life example of a classification problem, which is
    churn analysis. Customer churn is the loss of clients or customers in any business,
    which is becoming a prime concern in different area of business, such as banks,
    internet service providers, insurance companies, and so on*.* Customer dissatisfaction
    and better offers from the competitor are the primary reasons behind this*.* In
    the telecommunications industry, when many subscribers switch to another service
    provider, the company not only loses those customers and revenue—this also creates
    a bad impression for other, regular customers, or people who were planning to
    start using their service.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一个更贴近现实生活的分类问题示例，即客户流失分析。客户流失是指任何业务中客户或客户的流失，这正在成为商业不同领域的首要关注点，如银行、互联网服务提供商、保险公司等。客户不满和竞争对手的更好报价是这一现象背后的主要原因。在电信行业，当许多用户转而使用其他服务提供商时，公司不仅会失去那些客户和收入——这也会给其他，常规客户或计划开始使用他们服务的人留下坏印象。
- en: Eventually, the full cost of customer churn includes both the lost revenue and
    the telemarketing costs involved with replacing those customers with new ones.
    However, these types of loss can cause a huge loss for a business. Remember the
    time when Nokia was the dominator in the cell phone market? All of a sudden, Apple
    announced iPhone 3G, which was a revolution in the smartphone era. Then, around
    10% to 12% customers discontinued using Nokia and switched to iPhone. Although
    Nokia also tried to release a smartphone later on, they could not compete with
    Apple.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，客户流失的全部成本包括失去的收入以及用新客户替换这些客户所涉及的电话营销成本。然而，这类损失可能对一家企业造成巨大损失。还记得诺基亚曾是手机市场的霸主吗？突然之间，苹果发布了iPhone
    3G，这在智能手机时代引发了一场革命。随后，大约有10%到12%的客户停止使用诺基亚，转而使用iPhone。尽管诺基亚后来也尝试发布智能手机，但他们无法与苹果竞争。
- en: In short, churn prediction is essential for businesses as it helps you detect
    different kinds of customers who are likely to cancel a subscription, product,
    or service. In short, the idea is to predict whether an existing customer will
    unsubscribe from an existing service or not, that is, a binary classification
    problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，客户流失预测对商业至关重要，因为它可以帮助你检测那些可能取消订阅、产品或服务的不同类型的客户。简而言之，这个想法是预测现有客户是否会取消现有服务的订阅，即一个二元分类问题。
- en: Developing predictive models for churn
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发客户流失预测模型
- en: 'Accurate identification of churn possibility can minimize customer defection
    if you first identify which customers are likely to cancel a subscription to an
    existing service, and offering a special offer or plan to those customers. When
    it comes to employee churn prediction and developing a predictive model, where
    the process is heavily data-driven, machine learning can be used to understand
    a customer''s behavior. This is done by analyzing the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您首先确定哪些客户可能取消现有服务的订阅，并提供特别优惠或计划给这些客户，那么准确识别流失的可能性可以最小化客户流失。当涉及到员工流失预测和开发预测模型时，这个过程高度依赖数据驱动，可以使用机器学习来理解客户的行为。这是通过分析以下内容来完成的：
- en: Demographic data, such as age, marital status, and job status
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口统计数据，如年龄、婚姻状况和就业状况
- en: Sentiment analysis based on their social media data
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于社交媒体数据的情感分析
- en: Behavior analysis using their browsing clickstream logs
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用他们的浏览点击流日志进行行为分析
- en: Calling-circle data and support call center statistics
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 呼叫圈数据和客服中心统计数据
- en: 'An automated churn analytics pipeline can be developed by following three steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下三个步骤可以开发一个自动化的客户流失分析流程：
- en: First, identify typical tasks to analyze the churn, which will depend on company
    policy
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确定分析客户流失的典型任务，这将取决于公司政策
- en: Then, collect and analyze data and develop a predictive model
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，收集和分析数据，并开发预测模型
- en: Finally, deploy the model in a production-ready environment
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将模型部署到生产就绪的环境中
- en: Eventually, telecom companies will be able to predict and enhance customer experience,
    prevent churn, and tailor marketing campaigns. In practice, such an analysis will
    be helpful to retain the customers who are most likely to leave. This means that
    we don't need to worry about the customers who are likely to stay.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，电信公司能够预测并提升客户体验，防止客户流失，并定制营销活动。在实践中，这种分析将有助于保留那些最有可能离开的客户。这意味着我们不需要担心那些可能留下的客户。
- en: Description of the dataset
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: 'We can use the Orange Telecom''s Churn Dataset to develop a predictive model,
    which will predict which customers would like to cancel their subscription to
    existing services. The dataset is well studied, comprehensive, and used for developing
    small prototypes. It contains both churn-80 and churn-20 datasets, which can be
    downloaded from the following links:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Orange电信的客户流失数据集来开发预测模型，该模型将预测哪些客户可能希望取消现有服务的订阅。该数据集经过充分研究，内容全面，用于开发小型原型。它包含churn-80和churn-20数据集，可以从以下链接下载：
- en: 'churn-80: [https://bml-data.s3.amazonaws.com/churn-bigml-80.csv](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'churn-80: [https://bml-data.s3.amazonaws.com/churn-bigml-80.csv](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv)'
- en: 'churn-20: [https://bml-data.s3.amazonaws.com/churn-bigml-20.csv](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'churn-20: [https://bml-data.s3.amazonaws.com/churn-bigml-20.csv](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)'
- en: 'Since both datasets came from the same distribution, which has the same structure,
    we will use the churn-80 dataset for the training and 10-fold cross-validation.
    Then, churn-20 will be used to evaluate the trained model. Both datasets have
    a similar structure, and therefore have the following schema:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**State**: `String`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Account length**: `Integer`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area code**: `Integer`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**International plan**: `String`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voicemail plan**: `String`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number email messages**: `Integer`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day minutes**: `Double`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day calls**: `Integer`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day charge**: `Double`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening minutes**: `Double`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening calls**: `Integer`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening charge**: `Double`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night minutes**: `Double`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night calls**: `Integer`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night charge**: `Double`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international minutes**: `Double`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international calls**: `Integer`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international charge**: `Double`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer service calls**: `Integer`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory analysis and feature engineering
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we specify exactly the same schema (that is, a custom schema) before
    loading the data as a Spark DataFrame, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we have to create a Scala case class with all the fields specified and
    align the preceding schema (variable names are self-explanatory):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s create a Spark session and import the `implicit._` package, which allows
    us to specify a DataFrame operation, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s create the training set. We read the CSV file with Spark''s recommended
    format, `com.databricks.spark.csv`. We do not need any explicit schema inference;
    hence, we are making the `inferSchema` `false`, but we are using our own schema,
    which we created previously. Then, we load the data file from our desired location,
    and finally specify our data source so that our DataFrame looks exactly the same
    as what we specified:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see in the following screenshot, the schema of the Spark DataFrame
    has been correctly identified. However, some of the features are non-numeric but
    categorical. However, as expected by ML algorithms, all the features have to be
    numeric (that is, `integer` or `double` format):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a954d3d-1355-4db4-b697-46555926af92.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Excellent! It looks exactly the same as the data structure. Now, let''s look
    at some sample data by using the `show()` method, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding line of code shows the first 20 samples of the
    DataFrame:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d567b228-865e-4c27-9d1f-99b719fb9c49.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, column names are made shorter for visibility.
    We can also see related statistics of the training set by using the `describe()`
    method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following summary statistics not only give us some idea on the distribution
    with mean and standard deviation of the data, but also some descriptive statistics,
    such as number samples (that is, count), minimum value, and maximum value for
    each feature in the DataFrame:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d3a2a16-f183-424e-85d9-6faca2aeb658.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'If this dataset can fit into RAM, we can cache it for quick and repeated access
    by using the `cache()` method from Spark:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数据集可以适应RAM，我们可以使用Spark的`cache()`方法将其缓存，以便快速和重复访问：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s look at some useful properties such as variable correlation with `churn`.
    For example, let''s see how the `churn` is related with total number of international
    calls:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些有用的属性，例如变量与`churn`的相关性。例如，让我们看看`churn`与总国际通话次数之间的关系：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see from the following output, customers who make more international
    calls are less likely (that is, `False`) to change operator:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下输出所示，通话更多国际电话的客户不太可能（即，`False`）更换运营商：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s see how the `churn` is related to total international call charges:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`churn`与总国际通话费用之间的关系：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we can see from the following output, customers who make more international
    calls (as shown earlier) are charged more, but are still less likely (that is,
    `False`) to change operator:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下输出所示，通话更多国际电话的客户（如前所述）被收取更多费用，但仍然不太可能（即，`False`）更换运营商：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we also need to have the test set prepared to evaluate the model,
    let''s prepare the same set, similar to the train set, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们还需要准备测试集来评估模型，让我们准备与训练集相似的相同集，如下所示：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s cache them for faster and quick access for further manipulation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将它们缓存起来，以便更快地进行进一步的操作：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s look at some of the related properties of the training set to understand
    how suitable it is for our purposes. First, let''s create a temp view for persistence
    for this session. Nevertheless, we can create a catalog as an interface that can
    be used to create, drop, alter, or query underlying databases, tables, functions,
    and so on:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些与训练集相关的属性，以了解它是否适合我们的目的。首先，让我们为这个会话创建一个临时视图以进行持久化。尽管如此，我们可以创建一个目录作为接口，用于创建、删除、修改或查询底层数据库、表、函数等：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now group the data by the `churn` label and count the number of instances
    in each group, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按`churn`标签对数据进行分组，并计算每个组中的实例数量，如下所示：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding line should show that only `388` customers are likely to switch
    to another operator. However, `2278` customers still have their current operator
    as their preferred one:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行应该显示只有`388`名客户可能更换到另一个运营商。然而，`2278`名客户仍然将当前运营商作为他们首选的运营商：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So, we have roughly seven times more `False` churn samples than `True` churn
    samples. Since the target is to retain the customers who are most likely to leave,
    we will prepare our training set so that it ensures that the predictive ML model
    is sensitive to the `True` churn samples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有大约七倍的`False` churn样本比`True` churn样本。由于目标是保留最有可能离开的客户，我们将准备我们的训练集，以确保预测机器学习模型对`True`
    churn样本敏感。
- en: 'Also, since the training set is highly unbalanced, we should downsample the
    `False` `churn` class to a fraction of 388/2278, which gives us `0.1675`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于训练集高度不平衡，我们应该将`False` churn类别下采样到388/2278的分数，这给我们`0.1675`：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This way, we are also mapping only `True` churn samples. Now, let''s create
    a new DataFrame for the training set containing the samples from the downsample
    one only using the `sampleBy()` method:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们也在映射仅包含`True` churn样本。现在，让我们创建一个新的DataFrame，用于训练集，只包含使用`sampleBy()`方法从下采样中提取的样本：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The third parameter is the seed that''s used for reproducibility purposes.
    Let''s take a look at this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个参数是用于可重复性的种子。让我们看看这个：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can see that the classes are almost balanced:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到类别几乎平衡：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s see how the variables are related to each other. Let''s see how
    the day, night, evening, and international voice calls contribute to the `churn`
    class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这些变量是如何相互关联的。让我们看看白天、晚上、傍晚和国际语音通话是如何对`churn`类别做出贡献的：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This, however, doesn''t give any clear correlation because customers who are
    likely to stay make more day, night, evening, and international voice calls than
    the other customers who want to leave:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并没有给出任何明显的相关性，因为可能留下的客户比想要离开的其他客户在白天、晚上、傍晚和国际语音通话上通话更多：
- en: '![](img/270ed293-4589-43ff-9b37-c742898c2e8d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/270ed293-4589-43ff-9b37-c742898c2e8d.png)'
- en: 'Now, let''s see how many minutes of voice calls on day, night, evening, and
    international voice calls have contributed to the preceding `Total_charge` for
    the `churn` class:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看白天、晚上、傍晚和国际语音通话的通话分钟数是如何对`churn`类别的`Total_charge`做出贡献的：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From the preceding two tables, it is clear that the total day minutes and total
    day charge are a highly correlated feature in this training set, which is not
    beneficial for our ML model training. Therefore, it would be better to remove
    them altogether. Let''s drop one column of each pair of correlated fields, along
    with the `state_code` and `area_code` columns too since those won''t be used:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Excellent! Finally, we have our training DataFrame, which can be used for better,
    predictive modeling. Let''s take a look at some of the columns of the resulting
    DataFrame:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'However, we are not done yet—the current DataFrame cannot be fed to the model.
    This is known as an estimator. As we described earlier, our data needs to be converted
    into a Spark DataFrame format consisting of labels (in `Double`) and features
    (in `Vector`):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/596a64fd-2b61-4725-95a9-d7d111d75fe7.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Now, we need to create a pipeline to pass the data through by chaining several
    transformers and estimators. The pipeline then works as a feature extractor. More
    specifically, we have prepared two `StringIndexer` transformers and a `VectorAssembler`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `StringIndexer` converts the `String` categorical feature `international_plan`
    and labels into number indices. The second `StringIndexer` converts the categorical
    label (that is, `churn`) into numeric format. This way, indexing categorical features
    allows decision trees and random forest-like classifiers to treat categorical
    features appropriately, thus improving performance:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we need to extract the most important features that contribute to the
    classification. Since we have dropped some columns already, the resulting column
    set consists of the following fields:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Label → churn: `True` or `False`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features → {`account_length`, `iplanIndex`, `num_voice_mail`, `total_day_mins`,
    `total_day_calls`, `total_evening_mins`, `total_evening_calls`, `total_night_mins`,
    `total_night_calls`, `total_international_mins`, `total_international_calls`, `total_international_num_calls`}
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we have already converted categorical labels into numeric ones using
    `StringIndexer`, the next task is to extract the features:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let''s transform the features into feature vectors using `VectorAssembler()`,
    which takes all the `featureCols` and combines/transforms them into a single column
    called features:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have the real training set, which consists of label and feature
    vectors ready, the next task is to create an estimator—the third element of a
    pipeline. We will start with a very simple but powerful LR classifier.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: LR for churn prediction
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LR is an algorithm for classification, which predicts a binary response. It
    is similar to linear regression, which we described in [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala
    for Regression Analysis*, except that it does not predict continuous values—it
    predicts discrete classes. The loss function is the sigmoid function (or logistic
    function):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b314c7-58a8-45bc-bb3f-1d5aca687aed.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Similar to linear regression, the intuition behind the cost function is to
    penalize models that have large errors between the real response and the predicted
    response:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归类似，成本函数背后的直觉是惩罚那些实际响应与预测响应之间误差大的模型：
- en: '![](img/9008c370-be7a-40c3-b97d-e0a9ec94501e.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9008c370-be7a-40c3-b97d-e0a9ec94501e.png)'
- en: 'For a given new data point, **x***,* the LR model makes predictions using the
    following equation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的新的数据点**x**，LR模型使用以下方程进行预测：
- en: '![](img/8193a7b2-cee1-4ccc-86f3-365b682b6739.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8193a7b2-cee1-4ccc-86f3-365b682b6739.png)'
- en: In the preceding equation, the logistic function is applied to the regression
    to get the probabilities of it belonging in either class, where *z = w^T x* and
    if *f(w^T x) > 0.5*, the outcome is positive; otherwise, it is negative. This
    means that the threshold for the classification line is assumed to be at *0.5*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，对回归应用了逻辑函数以得到它属于任一类的概率，其中*z = w^T x*，如果*f(w^T x) > 0.5*，结果为正；否则为负。这意味着分类线的阈值被假定为*0.5*。
- en: 'Now that we know how the LR algorithm works, let''s start using Spark ML-based
    LR estimator development, which will predict whether a customer is likely to get
    churn or not. First, we need to define some hyperparameters to train a LR-based
    pipeline:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了LR算法是如何工作的，让我们开始使用基于Spark ML的LR估计器开发，这将预测客户是否可能发生流失。首先，我们需要定义一些超参数来训练基于LR的流水线：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`RegParam` is a scalar that helps us adjust the strength of the constraints:
    a small value implies a soft margin, while a large value implies a hard margin.
    The `Tol` parameter is used for the convergence tolerance for iterative algorithms,
    such as LR or linear SVM. Once we have the hyperparameters defined and initialized,
    our next task is to instantiate a LR estimator, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegParam`是一个标量，帮助我们调整约束的强度：小值表示软边界，而大值表示硬边界。`Tol`参数用于迭代算法（如LR或线性SVM）的收敛容差。一旦我们定义并初始化了超参数，我们的下一个任务就是实例化一个LR估计器，如下所示：'
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s build a pipeline estimator using the `Pipeline()` method to chain
    three transformers (the `ipindexer`, `labelindexer`, and `assembler` vectors)
    and the LR estimator (that is, `lr`) in a single pipeline—that is, each of them
    acts as a stage:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`Pipeline()`方法构建一个流水线估计器，将三个转换器（`ipindexer`、`labelindexer`和`assembler`向量）和LR估计器（即`lr`）链接到一个单一的流水线中——即它们各自作为一个阶段：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A Spark ML pipeline can have the following components:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML流水线可以包含以下组件：
- en: '**DataFrame**: To hold original data and intermediate transformed ones.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrame**：用于存储原始数据和中间转换后的数据。'
- en: '**Transformer**: Used to transform one DataFrame into another by adding additional
    feature columns.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：用于通过添加额外的特征列将一个DataFrame转换为另一个DataFrame。'
- en: '**Estimator**: An estimator is an ML model, such as linear regression.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Estimator**：估计器是一个ML模型，例如线性回归。'
- en: '**Pipeline**: Used to chain the preceding components, DataFrame, transformer,
    and estimator together.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pipeline**：用于将前面的组件、DataFrame、转换器和估计器链接在一起。'
- en: '**Parameter**: An ML algorithm has many knobs to tweak. These are called hyperparameters,
    and the values learned by an ML algorithm to fit the data are called parameters.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parameter**：ML算法有许多可调整的旋钮。这些被称为超参数，而ML算法学习以适应数据的值被称为参数。'
- en: 'In order to perform such a grid search over the hyperparameter space, we need
    to define it first. Here, the functional programming properties of Scala are quite
    handy because we just add function pointers and the respective parameters to be
    evaluated to the parameter grid. Here, a cross-validation evaluator will search
    through LR''s max iteration, regularization param, tolerance, and elastic net
    for the best model:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在超参数空间上执行此类网格搜索，我们首先需要定义它。在这里，Scala的函数式编程特性非常有用，因为我们只需将函数指针和相应的参数添加到参数网格中。在这里，交叉验证评估器将搜索LR的最大迭代次数、正则化参数、容差和弹性网络以找到最佳模型：
- en: '[PRE30]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that the hyperparameters form an *n*-dimensional space, where *n* is the
    number of hyperparameters. Every point in this space is one particular hyperparameter
    configuration, which is a hyperparameter vector. Of course, we can''t explore
    every point in this space, so what we basically do is a grid search over an (ideally
    evenly distributed) subset in that space. We then need to define a `BinaryClassificationEvaluator`
    evaluator, since this is a binary classification problem:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We use a `CrossValidator` by using `ParamGridBuilder` to iterate through the
    max iteration, regression param, tolerance, and elastic net parameters of LR with
    10-fold cross-validation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The preceding code is meant to perform cross-validation. The validator itself
    uses the `BinaryClassificationEvaluator` estimator to evaluate the training in
    the progressive grid space on each fold and make sure that no overfitting occurs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there is so much stuff going on behind the scenes, the interface of
    our `CrossValidator` object stays slim and well known as `CrossValidator` also
    extends from the estimator and supports the `fit` method. This means that, after
    calling the `fit` method, the complete predefined pipeline, including all feature
    preprocessing and the LR classifier, is executed multiple times—each time with
    a different hyperparameter vector:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, it''s time to evaluate the LR model using the test dataset. First, we
    need to transform the test set, similar to the training set we described previously:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code block shows the `Predicted_label` and the raw `probability` that
    were generated by the model. Additionally, it shows the actual labels. As we can
    see, for some instances, the model predicted correctly, but for other instances,
    it got confused:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4046ceff-8ab9-4903-8201-afe6a5b67485.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'The prediction probabilities can also be very useful in ranking customers according
    to their likeliness of imperfection. This way, a limited number of resources can
    be utilized in telecommunication business that can be focused on the most valuable
    customers. However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This should show around 77% classification accuracy from our binary classification
    model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We compute another performance metric called area under the precision-recall
    curve and the area under the ROC curve. For this, we can construct an RDD containing
    the raw scores on the test set:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In this case, the evaluation returns 77% accuracy, but only 58% precision:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In the following code, we are calculating some more metrics. False and true
    positive and negative predictions are also useful to evaluate the model''s performance.
    Then, we print the results to see the metrics, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we also compute the MCC score, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The preceding line gives a Matthews correlation coefficient of `0.41676531680973805`.
    This is a positive value, which gives us some sign of a robust model. However,
    we have not received good accuracy yet, so let's move on and try other classifiers,
    such as NB. This time, we will use the liner NB implementation from the Apache
    Spark ML package.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: NB for churn prediction
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NB classifier is based on Bayes'' theorem, with the following assumptions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Independence between every pair of features
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature values are non-negative, such as counts
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if cancer is related to age, this can be used to assess the probability
    that a patient might have cancer*.* Bayes'' theorem is stated mathematically as
    follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff5bdc71-63cf-4e09-b271-1b58ec82a648.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *A* and *B* are events with *P (B) ≠ 0*. The other
    terms can be described as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '*P* (*A* | *B*) is called the posterior or the conditional probability of observing
    event *A*, given that *B* is true'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* (*B*| *A*) is the likelihood of event *B* given that *A* is true'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(A)* is the prior and *P(B)* is the prior probability, also called marginal
    likelihood or marginal probability'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian NB is a generalized version of NB that''s used for classification,
    which is based on the binomial distribution of data. For example, our churn prediction
    problem can be formulated as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27623093-baba-475e-9920-c07b85deede3.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'The preceding list can be adopted to solve our problem as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '*P(class|data)* is the posterior probability of the *class* to be predicted
    by modelling with an independent variable (*data*)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(data|class)* is the likelihood or the probability of the predictor, given
    *class*'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(class)* is the prior probability of *class* and *P(data)* of the predictor
    or marginal likelihood'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The well-known Harvard study on happiness shows that only 10% of happy people
    are rich. Although you might think that this statistic is very compelling, you
    might be somewhat interested in knowing the percentage of rich people who are
    also really happy. Bayes'' theorem helps you out with calculating this reserving
    statistic using two additional clues:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of people overall who are happy—that is, *P(A)*
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The percentage of people overall who are rich—that is, *P(B)*
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key idea behind Bayes'' theorem is reversing the statistic by considering
    the overall rates. Suppose the following pieces of information are available prior:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 40% of people are happy => *P(A)*
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5% of people are rich => *P(B)*
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s consider that the Harvard study is correct—that is, *P(B|A) = 10%*.
    Now that we know the fraction of rich people who are happy, *P(A|B)* can be calculated
    as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A|B) = {P(A)* P(B|A)} / P(B) = (40%*10%)/5% = 80%*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, a majority of people are also happy! Nice. To make this clearer,
    let''s assume that the population of the whole world is 5,000, for simplicity.
    According to our calculation, two facts exist:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**Fact 1**: This tells us there are 500 people are happy, and the Harvard study
    tells us that 50 of these happy people are also rich'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fact 2**: There are 60 rich people altogether, so the fraction of them who
    are happy is 50/60 ~ 83%'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This proves Bayes theorem and its effectiveness. To use NB, we need to instantiate
    an NB estimator, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that we have transformers and an estimator ready, the next task is to chain
    in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s define the `paramGrid` to perform such a grid search over the hyperparameter
    space. Then the cross-validator will search for the best model through the NB''s
    `smoothing` parameter. Unlike LR or SVM, there is no hyperparameter in the NB
    algorithm:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Additive smoothing, or Laplace smoothing, is a technique that's used to smooth
    categorical data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We use a `CrossValidator` to perform 10-fold cross-validation for best model
    selection:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s call the `fit()` method so that the complete predefined `pipeline`,
    including all feature preprocessing and the LR classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, it''s time to evaluate the predictive power of the SVM model on the test
    dataset. First, we need to transform the test set with the model pipeline, which
    will map the features according to the same mechanism we described in the preceding
    feature engineering step:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding line of code should show 75% classification accuracy for our
    binary classification model:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Like we did previously, we construct an RDD containing the raw scores on the
    test set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this case, the evaluation returns 75% accuracy but only 55% precision:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In the following code, again, we calculate some more metrics. False and true
    positive and negative predictions are also useful to evaluate the model''s performance:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, we also compute the MCC score, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The preceding line gives a Matthews correlation coefficient of `0.14114315409796457`
    and this time, we experienced even worse performance in terms of accuracy and
    MCC scores. So, it's worth trying this with another classifier, such as SVM. We
    will use the linear SVM implementation from the Spark ML package.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: SVM for churn prediction
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVM is also a population algorithm for classification. SVM is based on the
    concept of decision planes, which defines the decision boundaries we discussed
    at the beginning of this chapter. The following diagram shows how the SVM algorithm
    works:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b56682ec-0673-457c-abc0-bb7d2e504bbf.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: 'SVM uses kernel function, which finds the linear hyperplane that separates
    classes with the maximum margin. The following diagram shows how the data points
    (that is, support vectors) belonging to two different classes (red versus blue)
    are separated using the decision boundary based on the maximum margin:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb7dd951-c399-42d4-bddb-f43ae3fd151c.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'The preceding support vector classifier can be represented as a dot product
    mathematically, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6931a67-3fe2-4fb8-90b5-61ff9a5c4db2.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'If the data to be separated is very high-dimensional, the kernel trick uses
    the kernel function to transform the data into a higher-dimensional feature space
    so that they can be linearly separable for classification. Mathematically, the
    kernel trick is to replace the dot product with the kernel, which will allow for
    non-linear decision boundaries and computational efficiency:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/295208ef-c737-4e7c-9c4a-31742453c623.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Now that we already know about SVMs, let''s start using the Spark-based implementation
    of SVM. First, we need to define some hyperparameters to train an LR-based pipeline:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Once we have the hyperparameters defined and initialized, the next task is
    to instantiate an SVM estimator, as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now that we have transformers and an estimator ready, the next task is to chain
    in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s define the `paramGrid` to perform such a grid search over the hyperparameter
    space. This searches through SVM''s max iteration, regularization param, tolerance,
    and elastic net for the best model:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We use a `CrossValidator` to perform 10-fold cross-validation for best model
    selection:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, let''s call the fit method so that the complete predefined `pipeline`,
    including all feature preprocessing and the LR classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, it''s time to evaluate the predictive power of the SVM model on the test
    dataset:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The preceding code block shows the predicted label and the raw probability generated
    by the model. Additionally, it shows the actual labels.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, for some instances, the model predicted correctly, but for some
    other instances, it got confused:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/525773ae-dfca-4d02-ac7d-5e9ccfd6a72e.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: 'However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Therefore, we get about 75% classification accuracy from our binary classification
    model:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, we construct an RDD containing the raw scores on the test set, which will
    be used to compute performance metrics such as area under the precision-recall
    curve (AUC) and are under the received operating characteristic curve (ROC):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In this case, the evaluation returns 75% accuracy, but only 55% precision:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We can also calculate some more metrics; for example, false and true positive
    and negative predictions are also useful to evaluate the model''s performance:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Finally, we also compute the MCC score, as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This gave me a Matthews correlation coefficient of `0.3888239300421191`. Although
    we have tried to use as many as three classification algorithms, we still haven't
    received good accuracy. Considering that SVM managed to give us an accuracy of
    76%, this is still considered to be low. Moreover, there is no option for most
    suitable feature selection, which helps us train our model with the most appropriate
    features. To improve classification accuracy, we will need to use tree-based approaches,
    such as DT, RF, and GBT, which are expected to provide more powerful responses.
    We will do this in the next chapter.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about different classical classification algorithms,
    such as LR, SVM, and NB. Using these algorithms, we predicted whether a customer
    is likely to cancel their telecommunications subscription or not. We've also discussed
    what types of data are required to build a successful churn predictive model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based and tree ensemble classifiers are really useful and robust, and are
    widely used for solving both classification and regression tasks. In the next
    chapter, we will look into developing such classifiers and regressors using tree-based
    and ensemble techniques such as DT, RF, and GBT, for both classification and regression.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
