- en: Scala for Learning Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how to develop a predictive model for analyzing
    insurance severity claims as a regression analysis problem. We applied very simple
    linear regression, as well as **generalized linear regression** (**GLR**).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll learn about another supervised learning task, called
    classification. We'll use widely used algorithms such as logistic regression, **Naive
    Bayes** (**NB**), and **Support Vector Machines** (**SVMs**) to analyze and predict
    whether a customer is likely to cancel the subscription of their telecommunication
    contract or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning classification with a real-life example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression for churn prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM for churn prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NB for prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter03)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2ZKVrxH](http://bit.ly/2ZKVrxH)'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a supervised learning task, classification is the problem of identifying
    which set of observations (sample) belongs to what based on one or more independent
    variables. This learning process is based on a training set containing observations
    (or instances) about the class or label of membership. Typically, classification
    problems are when we are training a model to predict quantitative (but discrete)
    targets, such as *s*pam detection, churn prediction, sentiment analysis, cancer
    type prediction, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to develop a predictive model, which will predict whether a
    student is competent enough to get admission into computer science based on his/her
    competency in TOEFL and GRE. Also, suppose we have some historical data in the
    following range/format:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TOEFL**: Between 0 and 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRE**: Between 0 and 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Admission**: 1 for admitted, 0 if not admitted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, to understand whether we can use such simple data to make predictions,
    let''s create a scatter plot by putting all the records with **Admitted** and
    **Rejected** as the dependent variables and **TOEFL** and **GRE** as the independent
    variables, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/004d677c-7313-4a40-bf44-90ee1afbcda1.png)'
  prefs: []
  type: TYPE_IMG
- en: By looking at the data points (imagine that the diagonal line in the graph is
    not there), we can reasonably develop a linear model to separate most of the data
    points. Now, if we draw a straight line between two classes of data, those almost
    get separated. Such a line (green, in our case) is called the decision boundary.
    So, if the decision boundary has reasonably separate maximal data points, it can
    be used for making predictions on unseen data, and we can also say that the data
    point above the line we predicted is competent for admission, and below the line
    we predict that the students are not competent enough.
  prefs: []
  type: TYPE_NORMAL
- en: Although this example is for a basic head-start into regression analysis, separating
    missions of data points is not very easy. Thus, to calculate where to draw the
    line for separating such a huge number of data points, we can use logistic regression
    or other classification algorithms that we will discuss in upcoming sections.
    We'll also see that drawing an ordinary straight line might not be the right one,
    and therefore we often have to draw curved lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the admission-related data plot carefully, maybe a straight line
    is not the best way of separating each data point—a curved line would be better,
    as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8513e773-de23-4536-b7f1-7daa4b32d5e0.png)'
  prefs: []
  type: TYPE_IMG
- en: However, to get a curved decision boundary, we have to change not only the function
    (called the decision boundary function) that's responsible from being linear to
    some high-order polynomial, but also the data to be a second-degree polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that we have to model our problem as a logistic regression model.
    That is, we need to change the data from *{GRE, TOEFL**}* format to a quadratic
    function format, *{GRE, GRE^2, TOEFL, TOEFL^2, GRE∗TOEFL**}*. However, doing so
    in a hand-crafted way is cumbersome and will not be possible for large datasets. Fortunately,
    Spark MLlib has numerous algorithms implemented for modeling such problems and
    for solving other classification problems, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression** (**LR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilayer perceptron** (**MLP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree** (**DT**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest** (**RF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosted trees** (**GBT**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a classification problem, actual (that is, true) labels (that is, class)
    and the predicted label (that is, class) exist for the samples that are used to
    train or test a classifier; this can be assigned to one of the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive (TP)**: The true label is positive and the prediction made
    by the classifier is also positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative (TN)**: The true label is negative and the prediction made
    by the classifier is also negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive (FP)**: The true label is negative but the prediction made
    by the classifier is positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative (FN)**: The true label is positive but the prediction made
    by the classifier is negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These metrics (TP, FP, TN, and FN) are the building blocks of the evaluation
    metrics for most of the classifiers we listed previously. However, the pure accuracy
    that is often used for identifying how many predictions were correct is not generally
    a good metric, so other metrics such as precision, recall, F1 score, AUC, and
    **Matthew''s correlation coefficient** (**MCC**) are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy* is the fraction of samples that the classifier correctly predicted
    (both positive and negative), divided by the total number of samples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f75a674d-2f5c-4d02-b65d-6d52bfd3b3fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Precision* is the number of samples correctly predicted that belong to a positive
    class (true positives), divided by the total number of samples actually belonging
    to the positive class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/acf55427-ec4f-415e-974f-eb47fd364dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Recall* is the number of samples that were correctly predicted to belong to
    a negative class, divided by the total number of elements actually belonging to
    the negative class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2c2e3301-f478-4212-8034-c27a95f8855e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'F1 score is the harmonic mean of precision and recall. Since the F1 score is
    a balance between recall and precision, it can be considered as an alternative
    to accuracy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/21df9705-5b82-4f30-8758-c7c0bfa23a7f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Receiver Operating Characteristics** (**ROC**) is a curve that''s drawn by
    plotting **FPR** (to *x*-axis) and **TPR** (to *y*-axis) for different threshold
    values. So, for different thresholds for your classifier, we calculate the **TPR**
    and **FPR**, draw the **ROC** curve, and calculate the area under the **ROC**
    curve (also known as **AUC**). This can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed60cca4-db3c-4239-a13f-054b24e18629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'MCC is regarded as a balanced measure of a binary classifier, even for a dataset
    that has very imbalanced classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1e62338-9246-4eb7-8cb6-9f51f6b31057.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's discuss a more real-life example of a classification problem, which is
    churn analysis. Customer churn is the loss of clients or customers in any business,
    which is becoming a prime concern in different area of business, such as banks,
    internet service providers, insurance companies, and so on*.* Customer dissatisfaction
    and better offers from the competitor are the primary reasons behind this*.* In
    the telecommunications industry, when many subscribers switch to another service
    provider, the company not only loses those customers and revenue—this also creates
    a bad impression for other, regular customers, or people who were planning to
    start using their service.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, the full cost of customer churn includes both the lost revenue and
    the telemarketing costs involved with replacing those customers with new ones.
    However, these types of loss can cause a huge loss for a business. Remember the
    time when Nokia was the dominator in the cell phone market? All of a sudden, Apple
    announced iPhone 3G, which was a revolution in the smartphone era. Then, around
    10% to 12% customers discontinued using Nokia and switched to iPhone. Although
    Nokia also tried to release a smartphone later on, they could not compete with
    Apple.
  prefs: []
  type: TYPE_NORMAL
- en: In short, churn prediction is essential for businesses as it helps you detect
    different kinds of customers who are likely to cancel a subscription, product,
    or service. In short, the idea is to predict whether an existing customer will
    unsubscribe from an existing service or not, that is, a binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Developing predictive models for churn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accurate identification of churn possibility can minimize customer defection
    if you first identify which customers are likely to cancel a subscription to an
    existing service, and offering a special offer or plan to those customers. When
    it comes to employee churn prediction and developing a predictive model, where
    the process is heavily data-driven, machine learning can be used to understand
    a customer''s behavior. This is done by analyzing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Demographic data, such as age, marital status, and job status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis based on their social media data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavior analysis using their browsing clickstream logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling-circle data and support call center statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An automated churn analytics pipeline can be developed by following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, identify typical tasks to analyze the churn, which will depend on company
    policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, collect and analyze data and develop a predictive model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, deploy the model in a production-ready environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eventually, telecom companies will be able to predict and enhance customer experience,
    prevent churn, and tailor marketing campaigns. In practice, such an analysis will
    be helpful to retain the customers who are most likely to leave. This means that
    we don't need to worry about the customers who are likely to stay.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the Orange Telecom''s Churn Dataset to develop a predictive model,
    which will predict which customers would like to cancel their subscription to
    existing services. The dataset is well studied, comprehensive, and used for developing
    small prototypes. It contains both churn-80 and churn-20 datasets, which can be
    downloaded from the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'churn-80: [https://bml-data.s3.amazonaws.com/churn-bigml-80.csv](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'churn-20: [https://bml-data.s3.amazonaws.com/churn-bigml-20.csv](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since both datasets came from the same distribution, which has the same structure,
    we will use the churn-80 dataset for the training and 10-fold cross-validation.
    Then, churn-20 will be used to evaluate the trained model. Both datasets have
    a similar structure, and therefore have the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State**: `String`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Account length**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area code**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**International plan**: `String`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voicemail plan**: `String`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number email messages**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day minutes**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day calls**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total day charge**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening minutes**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening calls**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total evening charge**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night minutes**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night calls**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total night charge**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international minutes**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international calls**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total international charge**: `Double`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer service calls**: `Integer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory analysis and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we specify exactly the same schema (that is, a custom schema) before
    loading the data as a Spark DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to create a Scala case class with all the fields specified and
    align the preceding schema (variable names are self-explanatory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a Spark session and import the `implicit._` package, which allows
    us to specify a DataFrame operation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the training set. We read the CSV file with Spark''s recommended
    format, `com.databricks.spark.csv`. We do not need any explicit schema inference;
    hence, we are making the `inferSchema` `false`, but we are using our own schema,
    which we created previously. Then, we load the data file from our desired location,
    and finally specify our data source so that our DataFrame looks exactly the same
    as what we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the following screenshot, the schema of the Spark DataFrame
    has been correctly identified. However, some of the features are non-numeric but
    categorical. However, as expected by ML algorithms, all the features have to be
    numeric (that is, `integer` or `double` format):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a954d3d-1355-4db4-b697-46555926af92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Excellent! It looks exactly the same as the data structure. Now, let''s look
    at some sample data by using the `show()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding line of code shows the first 20 samples of the
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d567b228-865e-4c27-9d1f-99b719fb9c49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, column names are made shorter for visibility.
    We can also see related statistics of the training set by using the `describe()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following summary statistics not only give us some idea on the distribution
    with mean and standard deviation of the data, but also some descriptive statistics,
    such as number samples (that is, count), minimum value, and maximum value for
    each feature in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d3a2a16-f183-424e-85d9-6faca2aeb658.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If this dataset can fit into RAM, we can cache it for quick and repeated access
    by using the `cache()` method from Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at some useful properties such as variable correlation with `churn`.
    For example, let''s see how the `churn` is related with total number of international
    calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the following output, customers who make more international
    calls are less likely (that is, `False`) to change operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the `churn` is related to total international call charges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the following output, customers who make more international
    calls (as shown earlier) are charged more, but are still less likely (that is,
    `False`) to change operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we also need to have the test set prepared to evaluate the model,
    let''s prepare the same set, similar to the train set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s cache them for faster and quick access for further manipulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at some of the related properties of the training set to understand
    how suitable it is for our purposes. First, let''s create a temp view for persistence
    for this session. Nevertheless, we can create a catalog as an interface that can
    be used to create, drop, alter, or query underlying databases, tables, functions,
    and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now group the data by the `churn` label and count the number of instances
    in each group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line should show that only `388` customers are likely to switch
    to another operator. However, `2278` customers still have their current operator
    as their preferred one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, we have roughly seven times more `False` churn samples than `True` churn
    samples. Since the target is to retain the customers who are most likely to leave,
    we will prepare our training set so that it ensures that the predictive ML model
    is sensitive to the `True` churn samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, since the training set is highly unbalanced, we should downsample the
    `False` `churn` class to a fraction of 388/2278, which gives us `0.1675`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, we are also mapping only `True` churn samples. Now, let''s create
    a new DataFrame for the training set containing the samples from the downsample
    one only using the `sampleBy()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The third parameter is the seed that''s used for reproducibility purposes.
    Let''s take a look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see that the classes are almost balanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how the variables are related to each other. Let''s see how
    the day, night, evening, and international voice calls contribute to the `churn`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This, however, doesn''t give any clear correlation because customers who are
    likely to stay make more day, night, evening, and international voice calls than
    the other customers who want to leave:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/270ed293-4589-43ff-9b37-c742898c2e8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s see how many minutes of voice calls on day, night, evening, and
    international voice calls have contributed to the preceding `Total_charge` for
    the `churn` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding two tables, it is clear that the total day minutes and total
    day charge are a highly correlated feature in this training set, which is not
    beneficial for our ML model training. Therefore, it would be better to remove
    them altogether. Let''s drop one column of each pair of correlated fields, along
    with the `state_code` and `area_code` columns too since those won''t be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! Finally, we have our training DataFrame, which can be used for better,
    predictive modeling. Let''s take a look at some of the columns of the resulting
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we are not done yet—the current DataFrame cannot be fed to the model.
    This is known as an estimator. As we described earlier, our data needs to be converted
    into a Spark DataFrame format consisting of labels (in `Double`) and features
    (in `Vector`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/596a64fd-2b61-4725-95a9-d7d111d75fe7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we need to create a pipeline to pass the data through by chaining several
    transformers and estimators. The pipeline then works as a feature extractor. More
    specifically, we have prepared two `StringIndexer` transformers and a `VectorAssembler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `StringIndexer` converts the `String` categorical feature `international_plan`
    and labels into number indices. The second `StringIndexer` converts the categorical
    label (that is, `churn`) into numeric format. This way, indexing categorical features
    allows decision trees and random forest-like classifiers to treat categorical
    features appropriately, thus improving performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to extract the most important features that contribute to the
    classification. Since we have dropped some columns already, the resulting column
    set consists of the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Label → churn: `True` or `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features → {`account_length`, `iplanIndex`, `num_voice_mail`, `total_day_mins`,
    `total_day_calls`, `total_evening_mins`, `total_evening_calls`, `total_night_mins`,
    `total_night_calls`, `total_international_mins`, `total_international_calls`, `total_international_num_calls`}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we have already converted categorical labels into numeric ones using
    `StringIndexer`, the next task is to extract the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s transform the features into feature vectors using `VectorAssembler()`,
    which takes all the `featureCols` and combines/transforms them into a single column
    called features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the real training set, which consists of label and feature
    vectors ready, the next task is to create an estimator—the third element of a
    pipeline. We will start with a very simple but powerful LR classifier.
  prefs: []
  type: TYPE_NORMAL
- en: LR for churn prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LR is an algorithm for classification, which predicts a binary response. It
    is similar to linear regression, which we described in [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala
    for Regression Analysis*, except that it does not predict continuous values—it
    predicts discrete classes. The loss function is the sigmoid function (or logistic
    function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b314c7-58a8-45bc-bb3f-1d5aca687aed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to linear regression, the intuition behind the cost function is to
    penalize models that have large errors between the real response and the predicted
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9008c370-be7a-40c3-b97d-e0a9ec94501e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a given new data point, **x***,* the LR model makes predictions using the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8193a7b2-cee1-4ccc-86f3-365b682b6739.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the logistic function is applied to the regression
    to get the probabilities of it belonging in either class, where *z = w^T x* and
    if *f(w^T x) > 0.5*, the outcome is positive; otherwise, it is negative. This
    means that the threshold for the classification line is assumed to be at *0.5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how the LR algorithm works, let''s start using Spark ML-based
    LR estimator development, which will predict whether a customer is likely to get
    churn or not. First, we need to define some hyperparameters to train a LR-based
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`RegParam` is a scalar that helps us adjust the strength of the constraints:
    a small value implies a soft margin, while a large value implies a hard margin.
    The `Tol` parameter is used for the convergence tolerance for iterative algorithms,
    such as LR or linear SVM. Once we have the hyperparameters defined and initialized,
    our next task is to instantiate a LR estimator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s build a pipeline estimator using the `Pipeline()` method to chain
    three transformers (the `ipindexer`, `labelindexer`, and `assembler` vectors)
    and the LR estimator (that is, `lr`) in a single pipeline—that is, each of them
    acts as a stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A Spark ML pipeline can have the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**: To hold original data and intermediate transformed ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: Used to transform one DataFrame into another by adding additional
    feature columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**: An estimator is an ML model, such as linear regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline**: Used to chain the preceding components, DataFrame, transformer,
    and estimator together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter**: An ML algorithm has many knobs to tweak. These are called hyperparameters,
    and the values learned by an ML algorithm to fit the data are called parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to perform such a grid search over the hyperparameter space, we need
    to define it first. Here, the functional programming properties of Scala are quite
    handy because we just add function pointers and the respective parameters to be
    evaluated to the parameter grid. Here, a cross-validation evaluator will search
    through LR''s max iteration, regularization param, tolerance, and elastic net
    for the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the hyperparameters form an *n*-dimensional space, where *n* is the
    number of hyperparameters. Every point in this space is one particular hyperparameter
    configuration, which is a hyperparameter vector. Of course, we can''t explore
    every point in this space, so what we basically do is a grid search over an (ideally
    evenly distributed) subset in that space. We then need to define a `BinaryClassificationEvaluator`
    evaluator, since this is a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CrossValidator` by using `ParamGridBuilder` to iterate through the
    max iteration, regression param, tolerance, and elastic net parameters of LR with
    10-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is meant to perform cross-validation. The validator itself
    uses the `BinaryClassificationEvaluator` estimator to evaluate the training in
    the progressive grid space on each fold and make sure that no overfitting occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there is so much stuff going on behind the scenes, the interface of
    our `CrossValidator` object stays slim and well known as `CrossValidator` also
    extends from the estimator and supports the `fit` method. This means that, after
    calling the `fit` method, the complete predefined pipeline, including all feature
    preprocessing and the LR classifier, is executed multiple times—each time with
    a different hyperparameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to evaluate the LR model using the test dataset. First, we
    need to transform the test set, similar to the training set we described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block shows the `Predicted_label` and the raw `probability` that
    were generated by the model. Additionally, it shows the actual labels. As we can
    see, for some instances, the model predicted correctly, but for other instances,
    it got confused:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4046ceff-8ab9-4903-8201-afe6a5b67485.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The prediction probabilities can also be very useful in ranking customers according
    to their likeliness of imperfection. This way, a limited number of resources can
    be utilized in telecommunication business that can be focused on the most valuable
    customers. However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show around 77% classification accuracy from our binary classification
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute another performance metric called area under the precision-recall
    curve and the area under the ROC curve. For this, we can construct an RDD containing
    the raw scores on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 77% accuracy, but only 58% precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are calculating some more metrics. False and true
    positive and negative predictions are also useful to evaluate the model''s performance.
    Then, we print the results to see the metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we also compute the MCC score, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line gives a Matthews correlation coefficient of `0.41676531680973805`.
    This is a positive value, which gives us some sign of a robust model. However,
    we have not received good accuracy yet, so let's move on and try other classifiers,
    such as NB. This time, we will use the liner NB implementation from the Apache
    Spark ML package.
  prefs: []
  type: TYPE_NORMAL
- en: NB for churn prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NB classifier is based on Bayes'' theorem, with the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Independence between every pair of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature values are non-negative, such as counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if cancer is related to age, this can be used to assess the probability
    that a patient might have cancer*.* Bayes'' theorem is stated mathematically as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff5bdc71-63cf-4e09-b271-1b58ec82a648.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *A* and *B* are events with *P (B) ≠ 0*. The other
    terms can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* (*A* | *B*) is called the posterior or the conditional probability of observing
    event *A*, given that *B* is true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* (*B*| *A*) is the likelihood of event *B* given that *A* is true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(A)* is the prior and *P(B)* is the prior probability, also called marginal
    likelihood or marginal probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian NB is a generalized version of NB that''s used for classification,
    which is based on the binomial distribution of data. For example, our churn prediction
    problem can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27623093-baba-475e-9920-c07b85deede3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding list can be adopted to solve our problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(class|data)* is the posterior probability of the *class* to be predicted
    by modelling with an independent variable (*data*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(data|class)* is the likelihood or the probability of the predictor, given
    *class*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(class)* is the prior probability of *class* and *P(data)* of the predictor
    or marginal likelihood'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The well-known Harvard study on happiness shows that only 10% of happy people
    are rich. Although you might think that this statistic is very compelling, you
    might be somewhat interested in knowing the percentage of rich people who are
    also really happy. Bayes'' theorem helps you out with calculating this reserving
    statistic using two additional clues:'
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of people overall who are happy—that is, *P(A)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The percentage of people overall who are rich—that is, *P(B)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key idea behind Bayes'' theorem is reversing the statistic by considering
    the overall rates. Suppose the following pieces of information are available prior:'
  prefs: []
  type: TYPE_NORMAL
- en: 40% of people are happy => *P(A)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5% of people are rich => *P(B)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s consider that the Harvard study is correct—that is, *P(B|A) = 10%*.
    Now that we know the fraction of rich people who are happy, *P(A|B)* can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A|B) = {P(A)* P(B|A)} / P(B) = (40%*10%)/5% = 80%*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, a majority of people are also happy! Nice. To make this clearer,
    let''s assume that the population of the whole world is 5,000, for simplicity.
    According to our calculation, two facts exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fact 1**: This tells us there are 500 people are happy, and the Harvard study
    tells us that 50 of these happy people are also rich'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fact 2**: There are 60 rich people altogether, so the fraction of them who
    are happy is 50/60 ~ 83%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This proves Bayes theorem and its effectiveness. To use NB, we need to instantiate
    an NB estimator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have transformers and an estimator ready, the next task is to chain
    in a single pipeline—that is, each of them acts as a stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the `paramGrid` to perform such a grid search over the hyperparameter
    space. Then the cross-validator will search for the best model through the NB''s
    `smoothing` parameter. Unlike LR or SVM, there is no hyperparameter in the NB
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Additive smoothing, or Laplace smoothing, is a technique that's used to smooth
    categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CrossValidator` to perform 10-fold cross-validation for best model
    selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s call the `fit()` method so that the complete predefined `pipeline`,
    including all feature preprocessing and the LR classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to evaluate the predictive power of the SVM model on the test
    dataset. First, we need to transform the test set with the model pipeline, which
    will map the features according to the same mechanism we described in the preceding
    feature engineering step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code should show 75% classification accuracy for our
    binary classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Like we did previously, we construct an RDD containing the raw scores on the
    test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 75% accuracy but only 55% precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, again, we calculate some more metrics. False and true
    positive and negative predictions are also useful to evaluate the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we also compute the MCC score, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line gives a Matthews correlation coefficient of `0.14114315409796457`
    and this time, we experienced even worse performance in terms of accuracy and
    MCC scores. So, it's worth trying this with another classifier, such as SVM. We
    will use the linear SVM implementation from the Spark ML package.
  prefs: []
  type: TYPE_NORMAL
- en: SVM for churn prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVM is also a population algorithm for classification. SVM is based on the
    concept of decision planes, which defines the decision boundaries we discussed
    at the beginning of this chapter. The following diagram shows how the SVM algorithm
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b56682ec-0673-457c-abc0-bb7d2e504bbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'SVM uses kernel function, which finds the linear hyperplane that separates
    classes with the maximum margin. The following diagram shows how the data points
    (that is, support vectors) belonging to two different classes (red versus blue)
    are separated using the decision boundary based on the maximum margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb7dd951-c399-42d4-bddb-f43ae3fd151c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding support vector classifier can be represented as a dot product
    mathematically, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6931a67-3fe2-4fb8-90b5-61ff9a5c4db2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the data to be separated is very high-dimensional, the kernel trick uses
    the kernel function to transform the data into a higher-dimensional feature space
    so that they can be linearly separable for classification. Mathematically, the
    kernel trick is to replace the dot product with the kernel, which will allow for
    non-linear decision boundaries and computational efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/295208ef-c737-4e7c-9c4a-31742453c623.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we already know about SVMs, let''s start using the Spark-based implementation
    of SVM. First, we need to define some hyperparameters to train an LR-based pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the hyperparameters defined and initialized, the next task is
    to instantiate an SVM estimator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have transformers and an estimator ready, the next task is to chain
    in a single pipeline—that is, each of them acts as a stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the `paramGrid` to perform such a grid search over the hyperparameter
    space. This searches through SVM''s max iteration, regularization param, tolerance,
    and elastic net for the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CrossValidator` to perform 10-fold cross-validation for best model
    selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call the fit method so that the complete predefined `pipeline`,
    including all feature preprocessing and the LR classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to evaluate the predictive power of the SVM model on the test
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block shows the predicted label and the raw probability generated
    by the model. Additionally, it shows the actual labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, for some instances, the model predicted correctly, but for some
    other instances, it got confused:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/525773ae-dfca-4d02-ac7d-5e9ccfd6a72e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, by looking at the preceding prediction DataFrame, it is difficult
    to guess the classification accuracy. However, in the second step, the evaluator
    evaluates itself using `BinaryClassificationEvaluator`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we get about 75% classification accuracy from our binary classification
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we construct an RDD containing the raw scores on the test set, which will
    be used to compute performance metrics such as area under the precision-recall
    curve (AUC) and are under the received operating characteristic curve (ROC):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the preceding RDD can be used to compute the aforementioned performance
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the evaluation returns 75% accuracy, but only 55% precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate some more metrics; for example, false and true positive
    and negative predictions are also useful to evaluate the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment shows the true positive, false positive, true negative,
    and false negative rates, which we will use to compute the MCC score later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we also compute the MCC score, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This gave me a Matthews correlation coefficient of `0.3888239300421191`. Although
    we have tried to use as many as three classification algorithms, we still haven't
    received good accuracy. Considering that SVM managed to give us an accuracy of
    76%, this is still considered to be low. Moreover, there is no option for most
    suitable feature selection, which helps us train our model with the most appropriate
    features. To improve classification accuracy, we will need to use tree-based approaches,
    such as DT, RF, and GBT, which are expected to provide more powerful responses.
    We will do this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about different classical classification algorithms,
    such as LR, SVM, and NB. Using these algorithms, we predicted whether a customer
    is likely to cancel their telecommunications subscription or not. We've also discussed
    what types of data are required to build a successful churn predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based and tree ensemble classifiers are really useful and robust, and are
    widely used for solving both classification and regression tasks. In the next
    chapter, we will look into developing such classifiers and regressors using tree-based
    and ensemble techniques such as DT, RF, and GBT, for both classification and regression.
  prefs: []
  type: TYPE_NORMAL
