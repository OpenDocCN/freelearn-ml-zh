- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Advanced Data Preparation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级数据准备
- en: The truism that 80 percent of the time invested in real-world machine learning
    projects is spent on data preparation is so widely cited that it is mostly accepted
    without question. Earlier chapters of this book helped perpetuate the cliché by
    stating it as a matter of fact without qualification, and although it is certainly
    a common experience and perception, it is also an oversimplification, as tends
    to be the case when generalizing from a statistic. In reality, there is no single,
    uniform experience for data preparation. Yet, it is indeed true that data prep
    work almost always involves more effort than anticipated.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 80%的时间投资在现实世界的机器学习项目中用于数据准备这一普遍真理被广泛引用，以至于它通常被无异议地接受。本书的早期章节通过不附加任何条件地将这一说法作为事实陈述来帮助延续这一陈词滥调，尽管这确实是一种常见的经验和感知，但它也是一种过度简化，正如从统计数据中概括时通常发生的那样。实际上，数据准备并没有统一的单一经验。然而，确实是真的，数据准备工作几乎总是比预期的要花费更多的努力。
- en: Rare is the case in which you will be provided a single CSV formatted text file,
    which can be easily read into R and processed with just a few lines of R code,
    as was the case in previous chapters. Instead, necessary data elements are often
    distributed across databases, which must then be gathered, filtered, reformatted,
    and combined before the features can be used with machine learning. This can require
    significant effort even before considering the time expended gaining access to
    the data from stakeholders, as well as exploring and understanding the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有情况会提供单个CSV格式的文本文件，可以很容易地被R读取并使用几行R代码进行处理，就像前几章所描述的那样。相反，必要的数据元素通常分布在数据库中，然后必须收集、过滤、重新格式化并组合，才能使用机器学习来使用特征。这甚至在没有考虑从利益相关者那里获取数据所需的时间以及探索和理解数据之前，就可能需要大量的努力。
- en: 'This chapter is intended to prepare you (pun intended!) for the larger and
    more complex datasets that you’ll be preparing in the real world. You will learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为你（有意为之！）准备你在现实世界中将要准备的大型且更复杂的数据集。你将学习：
- en: Why data preparation is crucial to building better models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么数据准备对于构建更好的模型至关重要
- en: Tips and tricks for transforming data into more useful predictors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为更有用的预测因子的技巧和窍门
- en: Specialized R packages for efficiently preparing data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于高效准备数据的专用R包
- en: Different teams and different projects require their data scientists to invest
    different amounts of time preparing data for the machine learning process, and
    thus, the 80 percent statistic may overstate or understate the effort needed for
    any given project or from any single contributor.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的团队和不同的项目需要他们的数据科学家投入不同数量的时间来准备机器学习过程中的数据，因此，80%的统计数据可能高估或低估了任何特定项目或单个贡献者所需的努力。
- en: Still, whether it is you or someone else performing this work, you will soon
    discover the undeniable fact that advanced data preparation is a necessary step
    in the process of building strong machine learning projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论是由你还是别人执行这项工作，你很快就会发现一个不可否认的事实，那就是高级数据准备是构建强大的机器学习项目过程中的一个必要步骤。
- en: Performing feature engineering
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行特征工程
- en: Time, effort, and imagination are central to the process of **feature engineering**,
    which involves applying subject-matter expertise to create new features for prediction.
    In simple terms, it might be described as the art of making data more useful.
    In more complex terms, it involves a combination of domain expertise and data
    transformations. One needs to know not just what data will be useful to gather
    for the machine learning project, but also how to merge, code, and clean the data
    to conform to the algorithm’s expectations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 时间、努力和想象力是**特征工程**过程的核心，该过程涉及将专业知识应用于创建用于预测的新特征。简单来说，它可能被描述为使数据更有用的艺术。更复杂地说，它涉及领域专业知识和数据转换的结合。一个人不仅需要知道对于机器学习项目哪些数据是有用的，还需要知道如何合并、编码和清理数据以满足算法的期望。
- en: Feature engineering is closely interrelated with data exploration, as described
    in *Chapter 11*, *Being Successful with Machine Learning*. Both involve interrogating
    data through the generation and testing of hypotheses. Exploring and brainstorming
    are likely to lead to insights about which features will be useful for prediction,
    and the act of engineering the features may lead to new questions to explore.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程与数据探索密切相关，如*第十一章*，*在机器学习中取得成功*中所述。两者都涉及通过生成和测试假设来质询数据。探索和头脑风暴可能导致关于哪些特征对预测有用的见解，而特征工程的行为可能导致需要探索的新问题。
- en: '![Diagram  Description automatically generated](img/B17290_12_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_12_01.png)'
- en: 'Figure 12.1: Feature engineering is part of a cycle that helps the model and
    data work together'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：特征工程是帮助模型和数据协同工作的循环的一部分
- en: 'Feature engineering is part of a cycle within a cycle in which effort is invested
    to help the model and data work better together. A round of data exploration and
    feature engineering leads to improvements to the data, which leads to iterations
    of training better models, which then informs another round of potential improvements
    to the data. These potential improvements are not only the bare minimum cleaning
    and preparation tasks needed to address simple data issues and allow the algorithm
    to run in R, but also the steps that lead an algorithm to learn more effectively.
    These may include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是循环中的循环的一部分，其中投入努力以帮助模型和数据更好地协同工作。一轮数据探索和特征工程导致数据改进，进而导致训练更好模型的迭代，然后又指导另一轮潜在的数据改进。这些潜在改进不仅包括解决简单数据问题并允许算法在R中运行所需的最低限度的清理和准备任务，还包括使算法更有效地学习的步骤。这些可能包括：
- en: Performing complex data transformations that help the algorithm to learn faster
    or to learn a simpler representation of the data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行复杂的数据转换，帮助算法更快地学习或学习数据的简化表示
- en: Creating features that are easier to interpret or better represent the underlying
    theoretical concepts
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建易于解释或更好地表示潜在理论概念的特性
- en: Utilizing unstructured data or merging additional features onto the main source
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用非结构化数据或将额外特征合并到主要来源上
- en: All three of these require both intense thought and creativity, and are improvisational
    and domain-specific rather than formulaic. This being said, the computer and the
    practitioner can share this work using complementary strengths. What the computer
    lacks in creativity and ability to improvise, it may be able to address with computational
    horsepower, brute force, and unwavering persistence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这三者都需要深入的思考和创造力，并且是即兴的和领域特定的，而不是公式化的。话虽如此，计算机和从业者可以通过互补的优势来共享这项工作。计算机在创造力和即兴能力方面的不足，可能可以通过计算能力、蛮力和坚定不移的毅力来弥补。
- en: The role of human and machine
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类和机器的作用
- en: Feature engineering can be viewed as a collaboration between the human and the
    machine during the learning process stage of abstraction. Recall that in *Chapter
    1*, *Introducing Machine Learning*, the abstraction step was defined as the translation
    of stored data into broader concepts and representations. In other words, during
    abstraction, connections are made between elements of raw data, which will represent
    important concepts for the learning objective. These relationships are generally
    defined by a model, which links the learned concepts to an outcome of interest.
    During feature engineering, the human gently guides or nudges the abstraction
    process in a specific direction, with the goal of producing a better-performing
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以看作是人类和机器在学习过程阶段抽象阶段的合作。回想一下在*第一章*，*介绍机器学习*中，抽象步骤被定义为将存储的数据转换为更广泛的概念和表示。换句话说，在抽象过程中，原始数据元素之间建立了联系，这些联系将代表学习目标的重要概念。这些关系通常由一个模型定义，该模型将学习到的概念与感兴趣的输出联系起来。在特征工程过程中，人类温和地引导或推动抽象过程向特定方向前进，目标是产生性能更好的模型。
- en: 'Imagine it this way: recall an instance in your past where you attempted to
    learn a difficult concept—possibly even while reading this very textbook! Reading
    and later re-reading the text proves to be of no help to understanding the concept,
    and frustrated, you contact a friend or colleague for help. Perhaps this friend
    explains the concept in a different way, using analogies or examples that help
    connect the concept to your prior experience, and in doing so, it leads you to
    a moment of enlightenment: “Eureka!” All is suddenly clear, and you wonder how
    you couldn’t understand the concept in the first place. Such is the power of abstractions,
    which can be transferred from one learner to another to aid the learning process.
    The process of feature engineering allows the human to transfer their intuitive
    knowledge or subject-matter expertise to the machine through intentionally and
    purposefully designed input data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这种情况：回忆一下你过去试图学习一个困难概念的时刻——可能甚至就在阅读这本教科书的时候！阅读以及后来重读文本证明对理解概念没有帮助，沮丧的你联系了一个朋友或同事寻求帮助。也许这个朋友会用不同的方式解释这个概念，使用类比或例子帮助你将概念与你先前经验联系起来，在这个过程中，它引导你进入一个顿悟的时刻：“我找到了！”所有东西突然变得清晰，你
    wonder how you couldn’t understand the concept in the first place. Such is the
    power of abstractions, which can be transferred from one learner to another to
    aid the learning process. The process of feature engineering allows the human
    to transfer their intuitive knowledge or subject-matter expertise to the machine
    through intentionally and purposefully designed input data.
- en: Given the fact that abstraction is the cornerstone of the learning process,
    it can be argued that machine learning is fundamentally feature engineering. The
    renowned computer scientist and artificial intelligence pioneer Andrew Ng said,
    “*Coming up with features is difficult, time-consuming, and requires expert knowledge.
    Applied machine learning is basically feature engineering*.” Pedro Domingos, a
    professor of computer science and author of the machine learning book *The Master
    Algorithm*, said that “*some machine learning projects succeed and some fail.
    What makes the difference? Easily the most important factor is the features used*.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到抽象是学习过程的基础，可以说机器学习本质上就是特征工程。著名的计算机科学家和人工智能先驱Andrew Ng说，“*提出特征是困难的，耗时且需要专业知识。应用机器学习基本上就是特征工程*。”计算机科学教授和机器学习书籍
    *The Master Algorithm* 的作者Pedro Domingos说，“*一些机器学习项目成功了，而一些失败了。是什么造成了这种差异？最关键的因素无疑是使用的特征*。”
- en: Andrew Ng’s quote appears in a lecture titled *Machine Learning and AI via Brain
    simulations*, which is available online via web search. In addition to Pedro Domingos’
    book *The Master Algorithm* (2015), see also his excellent paper “A few useful
    things to know about machine learning” in *Communications of the ACM* (2012).
    [https://doi.org/10.1145/2347736.2347755](https://doi.org/10.1145/2347736.2347755).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Ng的名言出现在一个名为 *Machine Learning and AI via Brain simulations* 的讲座中，该讲座可通过网络搜索在线获取。除了Pedro
    Domingos的书籍 *The Master Algorithm*（2015）外，还可以参考他在 *Communications of the ACM*（2012）上发表的优秀论文“关于机器学习的几个有用的事情要知道”。[https://doi.org/10.1145/2347736.2347755](https://doi.org/10.1145/2347736.2347755)。
- en: Feature engineering performed well can turn weaker learners into much stronger
    learners. Many machine learning and artificial intelligence problems can be solved
    with simple linear regression methods, assuming that the data has been sufficiently
    cleaned. Even very complex machine learning methods can be replicated in standard
    linear regression given sufficient feature engineering. Linear regression can
    be adapted to model nonlinear patterns, using splines and quadratic terms, and
    can approach the performance of even the most complex neural networks, given a
    sufficient number of new features that have been engineered as carefully designed
    interactions or transformations of the original input data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 功能工程做得好的话，可以将较弱的学习者转变为更强的学习者。许多机器学习和人工智能问题可以通过简单的线性回归方法解决，假设数据已经被充分清理。即使是非常复杂的机器学习方法，在足够的特征工程下也可以在标准线性回归中复制。线性回归可以适应模型非线性模式，使用样条和二次项，并且可以接近甚至最复杂的神经网络的表现，前提是有足够数量的新特征已经被精心设计为原始输入数据的交互或转换。
- en: The idea that simple learning algorithms can be adapted to more complex problems
    is not limited to regression. For example, decision trees can work around their
    axis-parallel decision boundaries by rotations of the input data, while hyperplane-based
    support vector machines can model complex nonlinear patterns with a well-chosen
    kernel trick. A method as simple as k-NN could be used to mimic regression or
    perhaps even more complex methods, given enough effort and sufficient understanding
    of the input data and learning problem, but herein lies the catch. Why invest
    large amounts of time performing feature engineering to employ a simple method
    when a more complex algorithm will perform just as well or better, while also
    performing the feature engineering for us automatically?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的学习算法可以适应更复杂问题的想法并不仅限于回归。例如，决策树可以通过输入数据的旋转来绕过它们的轴平行决策边界，而基于超平面的支持向量机可以通过选择合适的核技巧来模拟复杂的非线性模式。只要付出足够的努力，并且对输入数据和学习问题有足够的理解，一个像k-NN这样简单的方法也可以用来模拟回归，甚至可能是更复杂的方法，但这里有一个问题。为什么要在使用一个更复杂的算法也能表现同样好或更好的情况下，投入大量时间进行特征工程，而该算法还能自动为我们进行特征工程？
- en: Indeed, it is probably best to match the complexity of the data’s underlying
    patterns with a learning algorithm capable of handling them readily. Performing
    feature engineering by hand when a computer can do it automatically is not only
    wasted effort but also prone to mistakes and missing important patterns. Algorithms
    like decision trees and neural networks with a sufficiently large number of hidden
    nodes—and especially, deep learning neural networks—are particularly capable of
    doing their own form of feature engineering, which is likely to be more rigorous
    and thorough than what can be done by hand. Unfortunately, this does not mean
    we can blindly apply these same methods to every task—after all, there is no free
    lunch in machine learning!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，可能最好是将数据的潜在模式复杂性与能够轻松处理这些模式的学习算法相匹配。当计算机可以自动完成时，手动进行特征工程不仅是一种浪费，而且容易出错，并可能错过重要的模式。像决策树和具有足够多隐藏节点的神经网络这样的算法，尤其是深度学习神经网络，特别擅长进行它们自己的形式特征工程，这可能会比手工操作更加严格和彻底。不幸的是，这并不意味着我们可以盲目地将这些相同的方法应用到每个任务中——毕竟，在机器学习中没有免费的午餐！
- en: Applying the same algorithm to every problem suggests that there is a one-size-fits-all
    approach to feature engineering, when we know that it is as much an art as it
    is a science. Consequently, if all practitioners apply the same method to all
    tasks, they will have no way of knowing whether better performance is possible.
    Perhaps a slightly different feature engineering approach could have resulted
    in a model that more accurately predicted churn or cancer, and would have led
    to greater profits or more lives saved. This is clearly a problem in the real
    world, where even a small performance boost can mean a substantial edge over the
    competition.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的算法应用到每个问题上，似乎表明存在一种适用于所有特征工程的通用方法，但我们知道这既是艺术也是科学。因此，如果所有从业者都将相同的方法应用到所有任务中，他们将无法知道是否可能获得更好的性能。也许稍微不同的特征工程方法可能会产生一个更准确地预测客户流失或癌症的模型，这将导致更大的利润或更多生命的挽救。这在现实世界中显然是一个问题，即使是一点点性能的提升也可能意味着在竞争中占据实质性的优势。
- en: 'In a high-stakes competition environment, such as the machine learning competitions
    on Kaggle, each team has access to the same learning algorithms and is readily
    capable of rapidly applying each of them to identify which one performs best.
    It is no surprise, then, that a theme emerges while reading interviews with Kaggle
    champions: they often invest significant effort into feature engineering. Xavier
    Conort, who was the top-rated data scientist on Kaggle in 2012–2013, said in an
    interview that:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险的竞赛环境中，例如Kaggle上的机器学习竞赛，每个团队都能访问到相同的机器学习算法，并且能够迅速地将它们中的每一个应用到识别哪个表现最佳。因此，在阅读Kaggle冠军的访谈时出现一个主题并不令人惊讶：他们通常会在特征工程上投入大量的努力。2012-2013年Kaggle上评分最高的数据科学家Xavier
    Conort在一次访谈中说：
- en: ”The algorithms we used are very standard for Kagglers… We spent most of our
    efforts on feature engineering… We were also very careful to discard features
    likely to expose us to the risk of overfitting.”
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们使用的算法对于Kagglers来说非常标准……我们的大部分努力都花在了特征工程上……我们也非常小心地丢弃了可能让我们面临过拟合风险的特性。”
- en: Because feature engineering is one of the few proprietary aspects of machine
    learning, it is one of the few points of distinction across teams. In other words,
    teams that perform feature engineering well tend to outperform the competition.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To read the full interview with Xavier Conort, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/](https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/).
    Interviews with other Kaggle champions are available at [https://medium.com/kaggle-blog/tagged/kaggle-competition](https://medium.com/kaggle-blog/tagged/kaggle-competition).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Conort’s statement, it would be easy to assume that the need to invest
    in feature engineering necessitates greater investment in human intelligence and
    the application of subject-matter expertise, but this is not always true. Jeremy
    Achin, a member of a top-performing “DataRobot” team on Kaggle, remarked on the
    surprisingly limited utility of human expertise. Commenting on his team’s time
    spent on feature engineering, he said in an interview that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: ”The most surprising thing was that almost all attempts to use subject matter
    knowledge or insights drawn from data visualization led to drastically worse results.
    We actually arranged a 2-hour whiteboard lecture from a very talented biochemist
    and came up with some ideas based on what we learned, but none of them worked
    out.”
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeremy Achin, along with Xavier Conort and several other high-profile Kaggle
    Grand Masters, bootstrapped their Kaggle competition successes into an artificial
    intelligence company called DataRobot, which is now worth billions of dollars.
    Their software performs machine learning automatically, suggesting that a key
    lesson learned from their Kaggle work was that computers can perform many steps
    in the machine learning process just as well as humans, if not better.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: To read the full interview with Jeremy Achin, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/](https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/).
    The DataRobot company is found on the web at [https://www.datarobot.com](https://www.datarobot.com).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is a balance between building models piece by piece using subject-matter
    expertise and throwing everything at the machine to see what sticks. Although
    feature engineering today is largely still a manual process, the future of the
    field seems to be headed toward the scattershot “see what sticks” approach, as
    **automated feature engineering** is a rapidly growing area of research. The foundation
    of automated feature engineering tools is the idea that a computer can make up
    for its lack of creativity and domain knowledge by testing many more combinations
    of features than a human would ever have time to attempt. Automated feature engineering
    exchanges narrow-but-guided human thought for broad-and-systematic computer thought,
    with the potential upside of finding a more optimal solution and potential downsides
    including loss of interpretability and greater likelihood of overfitting.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在利用专业知识逐步构建模型和将所有内容都投入机器以观其效之间需要保持平衡。尽管今天特征工程在很大程度上仍然是一个手动过程，但该领域的未来似乎正朝着散弹式的“观其效”方法发展，因为**自动化特征工程**是一个快速增长的研究领域。自动化特征工程工具的基础是这样一个观点：计算机可以通过测试比人类有更多时间尝试的更多特征组合来弥补其缺乏创造力和领域知识。自动化特征工程用狭窄但受指导的人类思维交换了广泛而系统的计算机思维，其潜在优势是找到更优的解决方案，潜在的劣势包括可解释性降低和过拟合的可能性增加。
- en: Before getting too excited about the potential for automation, it is worth noting
    that while such tools may allow a human to outsource certain parts of *thinking*
    about feature engineering, effort must still be invested in the *coding* part
    of the task. That is to say, time that was once spent hand-coding features one
    by one is instead spent coding functions that systematically find or construct
    useful features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在过于兴奋于自动化的潜力之前，值得注意的是，尽管这些工具可能允许人类将特征工程中某些“思考”的部分外包出去，但仍然需要在任务的“编码”部分投入努力。也就是说，曾经用于逐个手动编码特征的时间现在被用于编写系统性地寻找或构建有用特征的函数。
- en: There are promising algorithms in development, such as the Python-based `Featuretools`
    package (and corresponding R package `featuretoolsR`, which interacts with the
    Python code), that may help automate the feature-building process, but the use
    of such tools is not yet widespread. Additionally, such methods must be fed by
    data and computing time, both of which may be limiting factors in many machine
    learning projects.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正在开发中的一些有希望的算法，如基于Python的`Featuretools`包（以及相应的R包`featuretoolsR`，它与Python代码交互），可以帮助自动化特征构建过程，但这类工具的使用尚未普及。此外，这些方法需要数据和计算时间，这两者可能是许多机器学习项目的限制因素。
- en: 'For more information on Featuretools, visit: [https://www.featuretools.com](https://www.featuretools.com).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于Featuretools的信息，请访问：[https://www.featuretools.com](https://www.featuretools.com)。
- en: The impact of big data and deep learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据和深度学习的影响
- en: 'Whether feature engineering is performed by a human or by automated machine
    methods, a point is inevitably reached at which additional invested effort leads
    to little or no boost to the learning algorithm’s performance. The application
    of more sophisticated learning algorithms may also improve the model’s performance
    somewhat, but this is also subject to diminishing returns, as there exists only
    a finite number of potential methods to apply and their performance differences
    tend to be relatively minor. Consequently, if additional performance gains are
    truly necessary, we are left with one remaining option: increasing the size of
    the training dataset with additional features or examples. Moreover, because adding
    additional columns would require revising data generated by past business processes,
    in many cases, collecting more rows is the easier option of the two.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无论特征工程是由人类还是由自动化机器方法执行，都会不可避免地达到一个点，即额外的投入努力几乎不会提高学习算法的性能。应用更复杂的学习算法也可能在一定程度上提高模型性能，但这种提升也是递减的，因为可应用的方法数量有限，它们的性能差异通常相对较小。因此，如果确实需要额外的性能提升，我们只剩下最后一个选择：通过添加额外的特征或示例来增加训练数据集的大小。此外，由于添加额外的列需要修订过去业务过程中生成的数据，在许多情况下，收集更多行数据是两个选择中较容易的一个。
- en: In practice, there is a relatively low ceiling on the performance gains achievable
    via the inclusion of more rows of data. Most algorithms described in this book
    plateau quickly and will perform little better on a dataset of 1 million rows
    than on a dataset containing a few thousand. You may have already observed this
    firsthand if you’ve applied machine learning methods to real-world projects in
    your own areas of interest. Once a dataset is big enough—often just a few thousand
    rows for many real-world applications—additional examples merely cause additional
    problems, such as extended computation time and running out of memory. If more
    data causes more problems, then the natural follow-up question is, why is there
    so much hype around the so-called “big data” era?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通过包含更多行数据所能实现的性能提升相对有限。本书中描述的大多数算法很快就会达到顶峰，在包含一百万行数据的集合上与包含几千行数据的集合相比，性能提升很小。如果你已经在你感兴趣的领域将机器学习方法应用于实际项目，你可能已经亲身体验到这一点。一旦数据集足够大——对于许多实际应用来说，通常只是几千行——额外的例子只会带来更多问题，比如计算时间延长和内存不足。如果更多的数据导致更多问题，那么自然而然地，人们会问，为什么所谓的“大数据时代”会有如此多的炒作？
- en: To answer this question, we must first begin by making a philosophical distinction
    between datasets of various sizes. To be clear, “big data” does not merely imply
    a large number of rows or a large amount of storage consumed in a database or
    filesystem. In fact, it comprises both of these and more, as size is just one
    of four elements that may indicate the presence of big data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们首先必须对各种大小的数据集进行哲学上的区分。为了明确，“大数据”不仅仅意味着数据库或文件系统中行数或存储消耗量很大。实际上，它包括这两者以及更多，因为大小只是可能表明大数据存在的四个要素之一。
- en: 'These are the so-called **four V’s of big data**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是所谓的**大数据的四个V**：
- en: '**Volume**: The literal size of the data, whether it be more rows, more columns,
    or more storage'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数量**：数据的实际大小，无论是行数更多、列数更多还是存储更多'
- en: '**Velocity**: The speed at which data accumulates, which impacts not only the
    volume but also the complexity of data processing'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：数据积累的速度，这不仅影响数量，还影响数据处理复杂性'
- en: '**Variety**: The differences in types or definitions of data across different
    systems, particularly the addition of unstructured sources such as text, images,
    and audio data'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：不同系统间数据类型或定义的差异，尤其是文本、图像和音频数据等非结构化来源的增加'
- en: '**Veracity**: The trustworthiness of the input data and the ability to match
    data across sources'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：输入数据的可信度和跨来源匹配数据的能力'
- en: 'Reading this list from top to bottom, the elements become less intuitively
    obvious, yet are more challenging to handle when encountered. The first two elements,
    volume and velocity, are the basis of what might be dubbed the **medium data**
    space. While this is not to say that there aren’t challenges working with high-volume,
    high-velocity data, these challenges can often be solved by scaling up what we
    are already doing. For example, it may be possible to use faster computers with
    more memory or apply a more computationally efficient algorithm. The presence
    of a greater variety and reduced veracity of data requires a completely different
    approach for use in machine learning projects, especially at high-velocity and
    high-volume scales. The following table lists some of the distinctions between
    the small, medium, and big data spaces:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从上到下阅读这个列表，元素变得不那么直观，但在遇到时却更具挑战性。前两个元素，数量和速度，是所谓的**中等数据**空间的基础。虽然这并不是说处理高数量、高速度的数据没有挑战，但这些挑战通常可以通过扩大我们正在做的事情来解决。例如，可能可以使用具有更多内存的更快计算机或应用更计算高效的算法。数据多样性的增加和真实性的降低需要完全不同的方法来在机器学习项目中使用，尤其是在高速和高数量规模上。以下表格列出了小、中、大数据空间之间的一些区别：
- en: '![Table  Description automatically generated](img/B17290_12_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_12_02.png)'
- en: 'Figure 12.2: Most machine learning projects are on the scale of “medium data”
    while additional skills and tools are required to make use of “big data”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：大多数机器学习项目处于“中等数据”规模，而要利用“大数据”则需要额外的技能和工具
- en: Moving from small to medium and then from medium to big data requires exponential
    investment. As datasets increase in size and complexity, the required infrastructure
    becomes much more complex, adding increasingly specialized databases, computing
    hardware, and analysis tools, some of which will be covered in *Chapter 15*, *Making
    Use of Big Data*. These tools are rapidly changing, which necessitates constant
    training and re-skilling. With the increased scale of data, time becomes a more
    significant constraint; not only are the projects more complex with many more
    moving pieces, requiring more cycles of iteration and refinement, but the work
    simply takes longer to complete—literally! A machine learning algorithm that runs
    in minutes on a medium-sized dataset may take hours or days on a much larger dataset,
    even with the benefit of cloud computing power.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Given the high stakes of big data, there is often an order of magnitude difference
    in how such projects are staffed and resourced—it is simply considered part of
    “the cost of doing business.” There may be dozens of data scientists, with matching
    numbers of IT professionals supporting the required infrastructure and data processing
    pipeline. Typical big data solutions require numerous tools and technologies to
    work together. This creates an opportunity for **data architects** to plan and
    structure the various computing resources and to monitor their security, performance,
    and cloud hosting costs. Similarly, data scientists are often matched by an equal
    or greater number of **data engineers**, who are responsible for piping data between
    sources and doing the most complex programming work. Their efforts in processing
    large datasets allow data scientists to focus on analysis and machine learning
    model building.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of those working on the largest and most challenging machine
    learning projects today, many everyday projects, including nearly all the examples
    covered in this book, fall squarely into what has been called a **small data regime**.
    In this paradigm, datasets can grow to be “large” in terms of the number of rows
    or in sheer storage volume, but they will never truly be “big data.” Computer
    science and machine learning expert Andrew Ng has noted that in the realm of small
    data, the role of the human is still impactful; the human can greatly impact a
    project’s performance via hand-engineering features or by the selection of the
    most performant learning algorithm. However, as a dataset grows beyond “large”
    and into “huge” sizes and into the **big data regime**, a different class of algorithms
    breaks through the performance plateau to surpass the small gains of manual tweaks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.3*, which is adapted from Ng’s work, illustrates this phenomenon:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_12_03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: In the small data regime, traditional machine learning algorithms
    are competitive with and may even perform better than more complex methods, which
    perform much better as the size of data increases'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Within the confines of the small data regime, no single algorithm or class of
    algorithms performs predictably better than the others. Here, clever feature engineering
    including subject-matter expertise and hand-coded features may allow simpler algorithms
    to outperform much more sophisticated approaches or deep learning neural networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: As the size of data increases to the medium data regime, ensemble approaches
    (described in *Chapter 14*, *Building Better Learners*) tend to perform better
    than even a carefully handcrafted model that uses traditional machine learning
    algorithms. For the largest datasets found in the big data regime, only deep learning
    neural networks (introduced in *Chapter 7*, *Black-Box Methods – Neural Networks
    and Support Vector Machines*, and to be covered in more detail in *Chapter 15*,
    *Making Use of Big Data*) appear to be capable of the utmost performance, as their
    capability to learn from additional data practically never plateaus. Does this
    imply that the “no free lunch” theorem is incorrect and there truly is one learning
    algorithm to rule them all?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The visualization of the performance of different learning algorithms under
    the small and big data regimes can be found described by Andrew Ng in his own
    words. To find these, simply search YouTube for “Nuts and Bolts of Applying Deep
    Learning” (appears 3 minutes into the video) or “Artificial Intelligence is the
    New Electricity” (appears 20 minutes into the video).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: To understand why certain algorithms perform better than others under the big
    data regime and why the “no free lunch” principle still applies, we must first
    consider the relationship between the size and complexity of the data, the capability
    of a model to learn a complex pattern, and the risk of overfitting. Let’s begin
    by considering a case in which the size and complexity of the data is held constant,
    but we increase the complexity of the learning algorithm to more closely model
    what is observed in the training data. For example, we may grow a decision tree
    to an overly large size, increase the number of predictors in a regression model,
    or add hidden nodes in a neural network. This relationship is closely linked to
    the idea of the bias-variance trade-off; by increasing the model complexity, we
    allow the model to conform more closely to the training data and, therefore, reduce
    its inherent bias and increase its variance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.4* illustrates the typical pattern that occurs as models increase
    in complexity. Initially, when the model is underfitted to the training dataset,
    increases in model complexity lead to reductions in model error and increases
    in model performance. However, there is a point at which increases in model complexity
    contribute to overfitting the training dataset. Beyond this point, although the
    model’s error rate on the training dataset continues to be reduced, the test set
    error rate increases, as the model’s ability to generalize beyond training is
    dramatically hindered. Again, this assumes a limit to the dataset’s ability to
    support the model’s increasing complexity.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: For many training datasets, increasing the complexity of the learning
    algorithm runs the risk of overfitting and increased test set error'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: If we can increase the size and scope of the training dataset, the big data
    regime may unlock a second tier of machine learning performance, but only if the
    learning algorithm is likewise capable of increasing its complexity to make use
    of the additional data. Many traditional algorithms, such as those covered so
    far in this book, are incapable of making such an evolutionary leap—at least not
    without some extra help.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The missing link between the traditional machine learning algorithms and those
    capable of making this leap has to do with the number of parameters that the algorithms
    attempt to learn about the data. Recall that in *Chapter 11*, *Being Successful
    with Machine Learning*, parameters were described as the learner’s internal values
    that represent its abstraction of the data. Traditionally, for a variety of reasons,
    including the bias-variance trade-off depicted above, as well as the belief that
    simpler, more parsimonious models should be favored over more complex ones, models
    with fewer parameters have been favored. It was assumed that increasing the number
    of parameters too high would allow the dataset to simply memorize the training
    data, leading to severe overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this is true, but only to a point, as *Figure 12.5* depicts.
    As model complexity—that is, the number of parameters—increases, the test set
    error follows the same U-shaped pattern as before. However, a new pattern emerges
    once complexity and parameterization have reached the **interpolation threshold**,
    or the point at which there are enough parameters to memorize and accurately classify
    virtually all the training set examples. At this threshold, generalization error
    is at its maximum, as the model has been greatly overfitted to the training data.
    However, as model complexity increases even further, test set error once again
    begins to drop. With sufficient additional complexity, a heavily overfitted model
    may even surpass the performance of a well-tuned traditional model, at least according
    to our existing notion of “overfitted.”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B17290_12_05.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Some algorithms are able to make use of big data to generalize
    well even after they seemingly overfit the training data'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the apparent contradiction of the “double descent”
    curve depicted here, see this groundbreaking paper: *Reconciling modern machine-learning
    practice and the classical bias–variance trade-off, Belkin M, Hsu D, Ma S, and
    Mandal S, 2019, Proceedings of the National Academy of Sciences, Vol. 116(32),
    pp. 15,849-15,854*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism that explains this unexpected result has to do with an interesting
    and perhaps even magical transformation that occurs in models capable of additional
    parameterization beyond the interpolation threshold. Once a learner has sufficient
    parameters to interpolate (to sufficiently conform to) the training data, additional
    parameters lead to a state of **overparameterization**, in which the additional
    complexity enables higher levels of thinking and abstraction. In essence, an overparameterized
    learner is capable of learning higher-order concepts; in practice, this means
    it is capable of learning how to engineer features or learning how to learn. A
    significant jump in model complexity beyond the interpolation threshold is likely
    to lead to a significant leap in the way the algorithm approaches the problem,
    but of course, not every algorithm is capable of this leap.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks, which can add additional complexity endlessly and trivially
    via the addition of hidden nodes arranged in layers, are the ideal candidate for
    consuming big data. As you will learn in *Chapter 15*, *Making Use of Big Data*,
    a cleverly designed neural network can engineer its own features out of unstructured
    data such as images, text, or audio. Similarly, its designation as a universal
    function approximator implies that it can identify the best functional form to
    model any pattern it identifies in the data. Thus, we must once again revisit
    the early question of how exactly this doesn’t violate the principle of “no free
    lunch.” It would appear that for datasets of sufficient size, deep learning neural
    networks are the single best approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting a couple of practical issues aside—notably, the fact that most real-world
    projects reside in the small data regime and the fact that deep neural networks
    are computationally expensive and difficult to train—a key reason that deep learning
    doesn’t violate the “no free lunch” principle is based on the fact that once the
    neural network becomes large and substantially overparameterized, and assuming
    it has access to a sufficiently large and complex training dataset, it ceases
    to be a single learning *algorithm* and instead becomes a generalized learning
    *process*. If this seems like a distinction without a difference, perhaps a metaphor
    will help: rather than providing us with a free lunch, the process of deep learning
    provides an opportunity to teach the algorithm how to make its own lunch. Given
    the limited availability of truly big data and the limited applicability of deep
    learning to most business tasks, to produce the strongest models, it is still
    necessary for the machine learning practitioner to assist in the feature engineering
    process.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering in practice
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the project or circumstances, the practice of feature engineering
    may look very different. Some large, technology-focused companies employ one or
    more data engineers per data scientist, which allows machine learning practitioners
    to focus less on data preparation and more on model building and iteration. Certain
    projects may rely on very small or very massive quantities of data, which may
    preclude or necessitate the use of deep learning methods or automated feature
    engineering techniques. Even projects requiring little initial feature engineering
    effort may suffer from the so-called “last mile problem,” which describes the
    tendency for costs and complexity to be disproportionally high for the small distances
    to be traveled for the “last mile” of distribution. Relating this concept to feature
    engineering implies that even if most of the work is taken care of by other teams
    or automation, a surprising amount of effort may still be required for the final
    steps of preparing the data for the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that the bulk of real-world machine learning projects today require
    a substantial amount of feature engineering. Most companies have yet to achieve
    the level of analytics maturity at the organizational level needed to allow data
    scientists to focus solely on model building. Many companies and projects will
    never achieve this level due to their small size or limited scope. For many small-to-mid-sized
    companies and small-to-mid-sized projects, data scientists must take the lead
    on all aspects of the project from start to finish. Consequently, it is necessary
    for data scientists to understand the role of the feature engineer and prepare
    to perform this role if needed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, feature engineering is more art than science and requires
    as much imagination as it does programming skills. In a nutshell, the three main
    goals of feature engineering might be described as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Supplementing what data is already available with additional external sources
    of information
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the data to conform to the machine learning algorithm’s requirements
    and to assist the model with its learning
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating the noise while minimizing the loss of useful information— conversely,
    maximizing the use of available information
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overall mantra to keep in mind when practicing feature engineering is “be
    clever.” One should strive to be a clever, frugal data miner and try to think
    about the subtle insights that you might find in every single feature, working
    systematically, and avoiding letting any data go to waste. Applying this rule
    serves as a reminder of the requisite creativity and helps to inspire the competitive
    spirit needed to build the strongest-performing learners.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Although each project will require you to apply these skills in a unique way,
    experience will reveal certain patterns that emerge in many types of projects.
    The sections that follow, which provide seven “hints” for the art of feature engineering,
    are not intended to be exhaustive but, rather, provide a spark of inspiration
    on how to think creatively about making data more useful.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'There has been an unfortunate dearth of feature engineering books on the market,
    until recently, when a number have been published. Two of the earliest books on
    this subject are Packt Publishing’s *Feature Engineering Made Easy* (Ozdemir &
    Susara, 2018) and O’Reilly’s *Feature Engineering for Machine Learning* (Zheng
    & Casari, 2018). The book *Feature Engineering and Selection* (Kuhn & Johnson,
    2019) is also a standout and even has a free version, available on the web at:
    [http://www.feat.engineering](http://www.feat.engineering).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 1: Brainstorm new features'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The choice of topic for a new machine learning project is typically motivated
    by an unfulfilled need. It may be motivated by a desire for more profit, to save
    lives, or even simple curiosity, but in any case, the topic is almost surely not
    selected at random. Instead, it relates to an issue at the core of the company,
    or a topic held dear by the curious, both of which suggest a fundamental interest
    in the work. The company or individual pursuing the project is likely to already
    know a great deal about the subject and the important factors that contribute
    to the outcome of interest. With this domain experience and subject-matter expertise,
    the company, team, or individual that commissioned the project is likely to hold
    proprietary insights about the task that they alone can bring.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: To capitalize on these insights, at the beginning of a machine learning project,
    just prior to feature engineering, it can be helpful to conduct a brainstorming
    session in which stakeholders are gathered and ideas are generated about the potential
    factors that are associated with the outcome of interest. During this process,
    it is important to avoid limiting yourself to what is readily available in existing
    datasets. Instead, consider the process of cause-and-effect at a more abstract
    level, imagining the various metaphorical “levers” that can be pulled in order
    to impact the outcome in a positive or negative direction. Be as thorough as possible
    and exhaust all ideas during this session. If you could have literally anything
    you wanted in the model, what would be most useful?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The culmination of a brainstorming session may be a **mind map**, which is a
    method of diagramming ideas around a central topic. Placing the outcome of interest
    at the center of the mind map, the various potential predictors radiate out from
    the central theme, as shown in the following example of a mind mapping session
    designing a model to predict heart disease mortality.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'A mind map diagram may use a hierarchy to link associated concepts or group
    factors that are related in a similar data source:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_12_06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Mind maps can be useful to help imagine the factors that contribute
    to an outcome'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: While constructing the mind map, you may determine that some of the desired
    features are unavailable in the existing data sources. Perhaps the brainstorming
    group can help identify alternative sources of these data elements or find someone
    willing to help gather them. Alternatively, it may be possible to develop a **proxy
    measure** that effectively measures the same concept using a different method.
    For example, it may be impossible or practically infeasible to directly measure
    someone’s diet, but it may be possible to use their social media activity as a
    proxy by counting the number of fast-food restaurants they follow. This is not
    perfect, but it is something, and is certainly better than nothing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: A mind mapping session can also help reveal potential interactions between features
    in which two or more factors have a disproportionate impact on the outcome; a
    joint effect may be greater (or lesser) than the sum of its parts. In the heart
    disease example, one might hypothesize that the combined effect of stress and
    obesity is substantially more likely to cause heart disease than the sum of their
    separate effects. Algorithms such as decision trees and neural networks can find
    these interaction effects automatically, but many others cannot, and in either
    case, it may benefit the learning process or result in a simpler, more interpretable
    model if these combinations are coded explicitly in the data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 2: Find insights hidden in text'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the richest sources of hidden data and, therefore, one of the most fruitful
    areas for feature engineering is text data. Machine learning algorithms are generally
    not very good at realizing the full value of text data because they lack the external
    knowledge of semantic meaning that a human has gained over a lifetime of language
    use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Of course, given a tremendous amount of text data, a computer may be able to
    learn the same thing, but this is not feasible for many projects, and would greatly
    add to a project’s complexity. Furthermore, text data cannot be used as-is, as
    it suffers from the curse of dimensionality; each block of text is unique and,
    therefore, serves as a form of fingerprint linking the text to an outcome. If
    used in the learning process, the algorithm will severely overfit or ignore the
    text data altogether.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality applies to unstructured “big” data more generally
    as image and audio data are likewise difficult to use directly in machine learning
    models. *Chapter 15*, *Making Use of Big Data*, covers some methods that allow
    these types of data sources to be used with traditional machine learning approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The humans in charge of constructing features for the learning algorithm can
    add insight to the text data by coding reduced-dimensionality features derived
    from the interpretation of the text. In selecting a small number of categories,
    the implicit meaning is made explicit. For example, in a customer churn analysis,
    suppose a company has access to the public Twitter timeline for its customers.
    Each customer’s tweets are unique, but a human may be able to code them into three
    categories of positive, negative, and neutral. This is a simple form of **sentiment
    analysis**, which analyzes the emotion of language. Computer software, including
    some R packages, may be able to help automate this process using models or rules
    designed to understand simple semantics. In addition to sentiment analysis, it
    may be possible to categorize text data by topic; in the churn example, perhaps
    customers tweeting about customer service are more likely to switch to another
    company than customers tweeting about price.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many R packages that can perform sentiment analysis, some of which
    require subscriptions to paid services. To get started quickly and easily, check
    out the aptly named `SentimentAnalysis` and `RSentiment` packages, as well as
    the `Syuzhet` package. All of these can classify sentences as positive or negative
    with just a couple of lines of R code. For a deeper dive into text mining and
    sentiment analysis, see the book *Text Mining with R: A Tidy Approach, 2017, Silge
    J and Robinson D*, which is available on the web at [https://www.tidytextmining.com](https://www.tidytextmining.com).
    Additionally, see *Text Mining in Practice with R, 2017, Kwartler T*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Beyond coding the overt meaning of text, one of the subtle arts of feature engineering
    involves finding the covert insights hidden in the text data. In particular, there
    may be useful information encoded in the text that is not related to the direct
    interpretation of the text, but it appears in the text coincidentally or accidentally,
    like a “tell” in the game of poker—a micro-expression that reveals the player’s
    secret intentions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden text data may help reveal aspects of a person’s identity, such as age,
    gender, career level, location, wealth, or socioeconomic status. Some examples
    include:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Names and salutations such as Mr. and Mrs., or Jr. and Sr., traditional and
    modern names, male and female names, or names associated with wealth
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job titles and categories such as CEO, president, assistant, senior, or director
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographic and spatial codes such as postal codes, building floor numbers, foreign
    and domestic regions, first-class tickets, PO boxes, and similar
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic markers such as slang or other expressions that may reveal pertinent
    aspects of identities
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To begin searching for these types of hidden insights, keep the outcome of interest
    in mind while systematically reviewing the text data. Read as many of the texts
    as possible while thinking about any way in which the text might reveal a subtle
    clue that could impact the outcome. When a pattern emerges, construct a feature
    based on the insight. For instance, if the text data commonly includes job titles,
    create rules to classify the jobs into career levels such as entry-level, mid-career,
    and executive. These career levels could then be used to predict outcomes such
    as loan default or churn likelihood.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 3: Transform numeric ranges'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain learning algorithms are more capable than others of learning from numeric
    data. Among algorithms that can utilize numeric data at all, some are better at
    learning the important cut points in the range of numeric values or are better
    at handling severely skewed data. Even a method like decision trees, which is
    certainly apt at using numeric features, has a tendency to overfit on numeric
    data and, thus, may benefit from a transformation that reduces the numeric range
    into a smaller number of potential cut points. Other methods like regression and
    neural networks may benefit from nonlinear transformations of numeric data, such
    as log scaling, normalization, and step functions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Many of these methods have been covered and applied in prior chapters. For example,
    in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*, we
    considered the technique of discretization (also known as “binning” or “bucketing”)
    as a means of transforming numeric data into categorical data so that it could
    be used by the naive Bayes algorithm. This technique is also sometimes useful
    for learners that can handle numeric data natively, as it can help clarify a decision
    boundary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates this process for a hypothetical model predicting
    heart disease, using a numeric age predictor. On the left, we see that as the
    numeric age increases, the darker the color becomes, indicating a greater prevalence
    of heart disease with increasing age. Despite this seemingly clear trend, a decision
    tree model may struggle to identify an appropriate cut point and it may do so
    arbitrarily, or it may choose numerous small cut points; both of these are likely
    to be overfitted to the training data. Instead of leaving this choice to the model,
    it may be better to use *a priori* knowledge to create predefined groups for “young”
    and “old” patients. Although this loses some of the nuances of the true underlying
    gradient, it may help the model generalize better to future data by trading the
    decision tree’s “high variance” approach for a “high bias” approach of theory-driven
    discretization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_07.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Discretization and other numeric transformations can help learners
    identify patterns more easily'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In general, for datasets containing numeric features, it may be worth exploring
    each feature systematically, while also considering the learning algorithm’s approach
    to numeric data, to determine whether a transformation is necessary. Apply any
    domain or subject-matter expertise you may have to inform the creation of bins,
    buckets, step points, or nonlinear transformations in the final version of the
    feature. Even though many algorithms are capable of handling numeric data without
    recoding or transformation, additional human intelligence may help guide the model
    to a better overall fit.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 4: Observe neighbors’ behavior'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the lesser-known methods of surfacing hidden insights during feature
    engineering is to apply the common knowledge that “birds of a feather flock together.”
    We applied this principle to prediction in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*, but it is also a useful mindset for identifying useful
    predictors. The idea hinges on the fact that there may be explicit or implicit
    groupings across the dataset’s rows, and there may be insights found by examining
    how one example relates to the others in its neighborhood or grouping.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: An example of an explicit grouping found often in real-world data is households.
    Many datasets include not only rows based on individuals but also a household
    identifier, which allows you to link rows into household groups and, thus, create
    new features based on the groups’ compositions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: For instance, knowing that someone is in a household may provide an indication
    of marital status and the number of children or dependents, even if these features
    were not included in the original individual-level dataset. Simply counting or
    aggregating some of the group’s features can result in highly useful predictors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: From here, it is also possible to share information among records in the groups.
    For instance, knowing one spouse’s income is helpful, but knowing both provides
    a better indication of the total available income. Measures of variance within
    a group can also be enlightening. There may be aspects of households that provide
    a bonus effect if the partners match or disagree on certain attributes; for example,
    if both partners report satisfaction with a particular telephone company, they
    may be especially loyal compared to households where only one member is satisfied.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: These principles also apply to less obvious but still explicit groupings, like
    postal codes or geographic regions. By collecting the rows falling into the group,
    one can count, sum, average, take the maximum or minimum value, or examine the
    diversity within the group to construct new and potentially useful predictors.
    Groups with more or less agreement or diversity may be more or less robust to
    certain outcomes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: There may be value in identifying implicit groupings as well—that is, a grouping
    not directly coded in the dataset. Clustering methods, such as those described
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, are one potential
    method of finding these types of groupings, and the resulting clusters can be
    used directly as a predictor in the model. For example, in a churn project, using
    clusters as features for the model may reveal that some clusters are more likely
    to churn than others. This may imply that churn is related to the cluster’s underlying
    demographics, or that churn is somewhat contagious among cluster members.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if birds of a feather flock together, it makes sense to borrow
    leading indicators from the experiences of similar neighbors—they may have a similar
    reaction to some external factor or may directly influence one another. Implicit
    groups that exhibit rare or unique traits may be interesting in themselves; perhaps
    some are the bellwether or “canary in the coal mine”—trendsetters that respond
    to change earlier than other groups. Observing their behavior and coding these
    groups explicitly into the model may improve the model’s predictive ability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: If you do use information from neighbors (or from related rows, as described
    in the next section), beware of the problem of data leakage, which was described
    in *Chapter 11*, *Being Successful with Machine Learning*. Be sure to only engineer
    features using information that will be available at the time of prediction when
    the model is deployed. For example, it would be unwise to use both spouses’ data
    for a credit scoring model if only one household member completes the loan application
    and the other spouse’s data is added after the loan is approved.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 5: Utilize related rows'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The practice of utilizing “follow the leader” behavior as hinted in the previous
    section can be especially powerful given the related rows of time series datasets,
    where the same attribute is measured repeatedly at different points in time. Data
    that contains repeated measures offers many such additional opportunities to construct
    useful predictors. Whereas the previous section considered grouping related data
    *across* the unit of analysis, the current section considers the value of grouping
    related observations *within* the unit of analysis. Essentially, by observing
    the same units of analysis repeatedly, we can examine their prior trends and make
    better predictions of the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting the hypothetical churn example, suppose we have access to the past
    24 months of data from subscribers to an online video streaming service. The unit
    of observation is the customer-month (one row per customer per month), while our
    unit of analysis is the customer. Our goal is to predict which customers are most
    likely to churn so that we might intervene. To construct a dataset for machine
    learning, we must collect the units of observation and aggregate them into one
    row per customer. Here is where feature engineering is especially needed. In the
    process of “rolling up” the historical data into a single row for analysis, we
    can construct features that examine trends and loyalty, asking questions such
    as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Is the customer’s average monthly activity greater than or less than their peers?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the customer’s monthly activity over time? Is it up, down, or stable?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How frequent is their activity? Are they loyal? Is their loyalty stable across
    months?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How consistent is the customer’s behavior? Does the behavior vary a lot from
    month to month?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are familiar with basic calculus, it may help to reflect on the concept
    of the first and second derivative, as both can be useful features in a time series
    model. The first derivative here refers to the velocity of the behavior—that is,
    the behavior count over a unit of time. For example, we may compute the number
    of dollars spent per month on the streaming service, or the number of television
    shows and movies streamed per month. These are useful predictors alone, but they
    can be made even more useful in the context of the second derivative, which is
    the acceleration (or deceleration) of the behavior. The acceleration is the change
    in velocity over time, such as the change in monthly spending or the change in
    the shows streamed per month. High-velocity customers with high spending and usage
    might be less likely to churn, but a rapid deceleration (that is, a large reduction
    in usage or spending) from these same customers might indicate an impending churn.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In addition to velocity and acceleration, measures of consistency, reliability,
    and variability can be constructed to further enhance predictive ability. A very
    consistent behavior that suddenly changes may be more concerning than a wildly
    varying behavior that changes similarly. Calculating the proportion of recent
    months with a purchase, or with spending or behavior meeting a given threshold,
    provides a simple loyalty metric, but more sophisticated measures using variance
    are also possible.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 6: Decompose time series'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The repeated measures time series data described in the previous section, with
    multiple related rows per unit of analysis, is said to be in the **long format**.
    This contrasts with the type of data required for most R-based machine learning
    methods. Unless a learning algorithm is designed to understand the related rows
    of repeated measures data, it will require time series data to be specified in
    the **wide format**, which transposes the repeated rows of data into repeated
    columns. For example, if a weight measurement is recorded monthly for 3 months
    for 1,000 patients, the long-format dataset will have 3 * 1,000 = 3,000 rows and
    3 columns (patient identifier, month, and weight). As depicted in *Figure 12.8*,
    the same dataset in wide format would contain only 1,000 rows but 4 columns: 1
    column for the patient identifier, and 3 columns for the monthly weight readings:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_12_08.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Most machine learning models require long-format time series data
    to be transformed into a wide format'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: To construct a wide format dataset, one must first determine how much history
    will be useful for prediction. The more history that is needed, the more columns
    that will need to be added to the wide dataset. For example, if we wanted to forecast
    a customer’s energy usage 1 month into the future, we may decide to use their
    prior 12 months of energy use as predictors so that a full year of seasonality
    would be covered. Therefore, to build a model forecasting energy usage in June
    2023, we might create 12 predictor columns measuring energy use in May 2023, April
    2023, March 2023, and so on, for each of the 12 months prior to June 2023\. A
    13th column would be the target or dependent variable, recording the actual energy
    usage in June 2023\. Note that a model trained upon this dataset would learn to
    predict energy use in June 2023 based on data in the months from June 2022 to
    May 2023, but it would not be able to predict other future months because the
    target and predictors are linked to specific months.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a better approach is to construct **lagged variables**, which are computed
    relative to the target month. The lagged variables are essentially measures that
    are delayed in time to be carried forward to a later, more recent row in the dataset.
    A model using lagged variables can be retrained on a rolling, monthly basis as
    additional months of data become available over time. Rather than having column
    names like `energy_june2023` and `energy_may2023`, the resulting dataset will
    have names that indicate the relative nature of the measurements, such as `energy_lag0`,
    `energy_lag1`, and `energy_lag2`, which indicate the energy use in the current
    month, the prior month, and 2 months ago. This model will always be applied to
    the most recent data to predict the forthcoming time period.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.9* visualizes this approach. Each month, a model is trained on the
    past 13 months of data; the most recent month is used for the target or dependent
    variable (denoted as DV) while the earlier 12 months are used as lagged predictors.
    The model can then be used to predict the future month, which has not yet been
    observed. Each successive month following the first shifts the rolling window
    1 month forward, such that data older than 13 months is unused in the model. A
    model trained using data constructed in this way does not learn the relationship
    between specific calendar months, as was the case with the non-lagged variables;
    rather, it learns how prior behavior relates to future behavior, regardless of
    the calendar month.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B17290_12_09.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Constructing lagged predictors is one method to model time series
    data'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: A problem with this approach, however, is that this method has disregarded calendar
    time, yet certain calendar months may have an important impact on the target variable.
    For example, energy use may be higher in winter and summer than in spring and
    fall, and thus, it would be beneficial for the model to know not only the relationship
    between past and future behavior but also to gain a sense of seasonal effects,
    or other patterns broader than the local patterns, within the rows related to
    the unit of analysis.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'One might imagine that the value of the target to be predicted is composed
    of three sources of variation, which we would like to decompose into features
    for the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: There is the local or internal variation, which is based on the attributes unique
    to the unit of analysis. In the example of forecasting energy demand, the local
    variation may be related to the size and construction of the household, the residents’
    energy needs, where the house is located, and so on.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There may be broader global trends, such as fuel prices or weather patterns,
    that affect the energy use of most households.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There may be seasonal effects, independent of the local and global effects,
    that explain changes in the target. This is not limited to the annual weather
    patterns mentioned before, but any cyclical or predictable pattern can be considered
    a seasonal effect.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some specific examples relevant to the energy forecasting project may include
    higher or lower demand on:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Different days of the week, particularly weekdays versus weekends
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Religious or government holidays
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional school or business vacation periods
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mass gatherings such as sporting events, concerts, and elections
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If local, global, and seasonal features can be incorporated into the training
    dataset as predictors, the model can learn their effect on the outcome. The challenge
    thereafter is twofold: subject-matter knowledge or data exploration is required
    to identify the important seasonal factors, and there must be ample training data
    for the target to be observed in each of the included seasons. The latter implies
    that the training data should be composed of more than a single month cross-section
    of time; lacking this, the learning algorithm will obviously be unable to discover
    the relationship between the seasons and the target!'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Though it would seem to follow that we should revert to the original long-format
    data, this is actually not the case. In fact, the wide data with lagged variables
    from each month can be stacked in a single unified dataset with multiple rows
    per unit of analysis. Each row indicates an individual at a particular moment
    in time, with a target variable measuring the outcome at that moment, and a wide
    set of columns that have been constructed as lagged variables for periods of time
    prior to the target. Additional columns can also be added to further widen the
    matrix and decompose the various components of time variance, such as indicators
    for seasons, days of the week, and holidays; these columns will indicate whether
    the given row falls within one of these periods of interest.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The figure that follows depicts a hypothetical dataset using this approach.
    Each household (denoted by the `household_id` column) can appear repeatedly with
    different values of the target (`energy_use`) and predictors (`season`, `holiday_month`,
    `energy_lag1`, and so on). Note that the lag variables are missing (as indicated
    by the `NA` values) for the first few rows of the dataset, which means that these
    rows cannot be used for training or prediction. The remaining rows, however, can
    be used with any machine learning method capable of numeric prediction, and the
    trained model will readily forecast next month’s energy use given the row of data
    for the current month.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_12_10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Datasets including historical data may include both seasonal
    effects and lagged predictors'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Before rushing into modeling time series data, it is crucial to understand
    an important caveat about the data preparation methods described here: because
    the rows from repeated observations from the same unit of analysis are related
    to one another, including them in the training data violates the assumption of
    independent observations for methods like regression. While models built upon
    such data may still be useful, other methods for formal time series modeling may
    be more appropriate, and it is best to consider the methods described here as
    a workaround to perform forecasting with the machine learning methods previously
    covered. Linear mixed models and recurrent neural networks are two potential approaches
    that can handle this type of data natively, although both methods are outside
    the scope of this book.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The `lme4` package is used to build mixed models in R, but it would be unwise
    to jump in without understanding the statistical underpinnings of these types
    of models; they are a significant step up in complexity over traditional regression
    modeling. The book *Linear Mixed-Effects Models Using R* (Gałecki & Burzykowski,
    2013) provides the theoretical background needed to build this type of model.
    To build recurrent neural networks, R may not be the right tool for the job, as
    specialized tools exist for this purpose. However, the `rnn` package can build
    simple RNN models for time series forecasting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 7: Append external data'
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the teaching examples in this book, when a machine learning project begins
    in the real world, a dataset cannot simply be downloaded from the internet with
    prebuilt features and examples describing the topic of interest. It is unfortunate
    how many deeply interesting projects are killed before they begin for this simple
    reason. Businesses hoping to predict customer churn realize they have no historical
    data from which a model can be built; students hoping to optimize food distribution
    in poverty-stricken areas are limited by the scarce amounts of data from these
    areas; and countless projects that might increase profits or change the world
    for the better are stunted before they start. What begins as excitement around
    a machine learning project soon fizzles out due to the lack of data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Rather than ending with discouragement, it is better to channel this energy
    into an effort to create the necessary data from scratch. This may mean dialing
    colleagues on the telephone or firing off a series of email messages to connect
    with those that can grant access to databases containing relevant pieces of data.
    It may also require rolling up your sleeves and getting your hands dirty. After
    all, we live in the so-called big data era where data is not only plentiful but
    easily recorded, with assistance from electronic sensors and automated data entry
    tools.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Little effort is often sufficient to generate datasets useful
    for machine learning'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In the worst case, an investment of time, effort, and imagination can build
    useful datasets from nothing. Typically, this is easier than one might think.
    The previous figure illustrates several cases in which I created datasets to satisfy
    my own curiosity.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Fascinated by autonomous vehicles, I drove around my neighborhood and took pictures
    of road signs to build a stop sign classification algorithm. To predict used car
    prices, I copied and pasted hundreds of listings from used car websites. And,
    to understand exactly when and why names rhyming with “Aiden” became so popular
    in the United States, I gathered dozens of years of data from the Social Security
    baby name database. None of these projects required more than a few hours of effort,
    but enrolling friends, colleagues, or internet forums as a form of crowdsourcing
    the effort or even paying for data entry assistance could have parallelized the
    task and helped my database grow larger or faster. Paid services like Amazon Mechanical
    Turk ([https://www.mturk.com](https://www.mturk.com)) provide an affordable means
    of distributing large and tedious data entry or collection tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: To further enrich existing datasets, there is often the potential to append
    additional features from external sources. This is especially true when the main
    dataset of interest includes geographic identifiers such as postal codes, as many
    publicly available databases measure attributes for these regions. Of course,
    a postal code-level dataset will not reveal a specific individual’s exact characteristics;
    however, it may provide insight into whether the average person in the area is
    wealthier, healthier, younger, or more likely to have kids, among numerous other
    factors that may help improve the quality of a predictive model. These types of
    data can be readily found on many governmental agency websites and downloaded
    at no charge; simply merge them onto the main dataset for additional possible
    predictors.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, many social media companies and data aggregator services like Facebook,
    Zillow, and LinkedIn provide free access to limited portions of their data. Zillow,
    for example, provides home value estimates for postal code regions. In some cases,
    these companies or other vendors may sell access to these datasets, which can
    be a powerful means of augmenting a predictive model. In addition to the financial
    cost of such acquisitions, they often pose a significant challenge in terms of
    **record linkage**, which involves matching entities across datasets that share
    no common unique identifier. Solving this problem involves building a **crosswalk**
    table, which maps each row in one source to the corresponding row in the other
    source. For instance, the crosswalk may link a person identified by a customer
    identification number in the main dataset to a unique website URL in an external
    social media dataset. Although there are R packages such as `RecordLinkage` that
    can help perform such matching across sources, these rely on heuristics that may
    not perform as well as human intelligence and require significant computational
    expense, particularly for large databases. In general, it is safe to assume that
    record linkage is often costly from human resource and computational expense perspectives.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: When considering whether to acquire external data, be sure to research the source’s
    terms of use, as well as your region’s laws and organizational rules around using
    such sources. Some jurisdictions are stricter than others, and many rules are
    becoming stricter over time, so it is important to keep up to date on the legality
    and liability associated with outside data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Given the work involved in advanced data preparation, R itself has evolved to
    keep up with the new demands. Historically, R was notorious for struggling with
    very large and complex datasets, but over time, new packages have been developed
    to address these shortcomings and make it easier to perform the types of operations
    described so far in this chapter. In the remainder of this chapter, you will learn
    about these packages, which modernize the R syntax for real-world data challenges.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Exploring R’s tidyverse
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new approach has rapidly taken shape as the dominant paradigm for working
    with data in R. Championed by Hadley Wickham—the mind behind many of the packages
    that drove much of R’s initial surge in popularity—this new wave is now backed
    by a much larger team at Posit (formerly known as RStudio). The company’s user-friendly
    RStudio Desktop application integrates nicely into this new ecosystem, known as
    the **tidyverse**, because it provides a universe of packages devoted to tidy
    data. The entire suite of tidyverse packages can be installed with the `install.packages("tidyverse")`
    command.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: A growing number of resources are available online to learn more about the tidyverse,
    starting with its homepage at [https://www.tidyverse.org](https://www.tidyverse.org).
    Here, you can learn about the various packages included in the set, a few of which
    will be described in this chapter. Additionally, the book *R for Data Science*
    by Hadley Wickham and Garrett Grolemund is available freely online at [https://r4ds.hadley.nz](https://r4ds.hadley.nz)
    and illustrates how the tidyverse’s self-proclaimed “opinionated” approach simplifies
    data science projects.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: I am often asked the question of how R compares to Python for data science and
    machine learning. RStudio and the tidyverse are perhaps R’s greatest asset and
    point of distinction. There is arguably no easier way to begin a data science
    journey. Once you’ve learned the “tidy” way of doing data analysis, you are likely
    to wish the tidyverse functionality existed everywhere!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Making tidy table structures with tibbles
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whereas the data frame is the center of the base R universe, the data structure
    at the heart of the tidyverse is found in the `tibble` package ([https://tibble.tidyverse.org](https://tibble.tidyverse.org)),
    the name of which is a pun on the word “table” as well as a nod to the infamous
    “tribble” in *Star Trek* lore. A **tibble** acts almost exactly like a data frame
    but includes additional modern functionality for convenience and simplicity. Tibbles
    can be used almost everywhere a data frame can be used. Detailed information about
    tibbles can be found by typing the command `vignette("tibble")` in R.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, using tibbles will be transparent and seamless, as tibbles
    can pass as a data frame in most R packages. However, in the rare case where you
    need to convert a tibble to a data frame, use the `as.data.frame()` function.
    To go in the other direction and convert a data frame in to a tibble, use the
    `as_tibble()` function. Here, we’ll create a tibble from the Titanic dataset first
    introduced in the previous chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Typing the name of this object demonstrates the tibble’s cleaner and more informative
    output than a standard data frame:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with medium confidence](img/B17290_12_12.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Displaying a tibble object results in more informative output
    than a standard data frame'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note the distinctions between tibbles and data frames, as
    the tidyverse will automatically create a tibble object for many of its operations.
    Overall, you are likely to find that tibbles are faster and easier to work with
    than data frames. They generally make smarter assumptions about the data, which
    means you will spend less time redoing R’s work—like recoding strings as factors
    or vice versa.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, one simple distinction between tibbles and data frames is that a tibble
    never assumes `stringsAsFactors = TRUE`, which was the default behavior in base
    R until relatively recently with the release of R version 4.0\. As described in
    previous chapters, R’s `stringsAsFactors` setting sometimes led to confusion or
    programming bugs when character columns were automatically converted in to factors
    by default. Another distinction between tibbles and data frames is that, as long
    as the name is surrounded by the backtick (`` ` ``) character, a tibble can use
    non-standard column names like `` `my var` `` that violate base R’s object naming
    rules. Other benefits of tibbles are unlocked by complementary tidyverse packages,
    as described in the sections that follow.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Reading rectangular files faster with readr and readxl
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nearly every chapter so far has used the `read.csv()` function to load data
    into R data frames. Although we could convert these data frames in to tibbles,
    there is a faster and more direct path to get data into the tibble format. The
    tidyverse includes the `readr` package ([https://readr.tidyverse.org](https://readr.tidyverse.org))
    for loading tabular data. This is described in the data import chapter in *R for
    Data Science* at [https://r4ds.hadley.nz/data-import.html](https://r4ds.hadley.nz/data-import.html),
    but the basic functionality is simple.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The `readr` package provides a `read_csv()` function that loads data from CSV
    files much like base R’s `read.csv()` function. A key difference, aside from the
    subtle difference in their function names, is that the tidyverse’s version is
    much speedier—and not merely because it automatically converts the data into a
    tibble. It is about 10x faster at reading data according to the package authors.
    It is also smarter about the format of the columns to be loaded. For example,
    it has the capability to handle numbers with currency characters, parse date columns,
    and is better at handling international data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a tibble from a CSV file, simply use the `read_csv()` function as
    follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will use the default parsing settings, which attempt to infer the correct
    data type (that is, character or numeric) for each column. The column specification
    will be displayed in the R output upon completion of the file read. The inferred
    data types may be overridden by providing the correct column specifications via
    a `col()` function call passed to the `read_csv()` function. For more information
    on the syntax, view the documentation using the `vignette("readr")` command.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The `readxl` package ([https://readxl.tidyverse.org](https://readxl.tidyverse.org))
    provides a method to read data directly from the Microsoft Excel spreadsheet format.
    To create a tibble from an XLSX file, simply use the `read_excel()` function as
    follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, as first introduced in *Chapter 2*, *Managing and Understanding
    Data*, the RStudio desktop application can write the data import code for you.
    In the upper-right of the interface, under the **Environment** tab, there is an
    **Import Dataset** button. This menu reveals a list of data import options, including
    plaintext formats like CSV files (using base R or the `readr` package), as well
    as Excel and the SPSS, SAS, and Stata formats created by other statistical computing
    software tools. Using the **From Text (readr)** option reveals the following graphical
    interface, allowing the import process to be easily customized:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_13.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: RStudio’s Import Dataset feature automatically writes R code
    to easily import a variety of data formats'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The interface displays a preview of the data that updates as the import parameters
    are customized. The default column data types can be customized by clicking on
    the drop-down menu in the column header, and the code preview in the lower-right
    will update accordingly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Import** button will immediately execute the code, but a better
    practice is to copy and paste the code into your R source code file so that the
    import process can be easily run again in the future.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and piping data with dplyr
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `dplyr` package ([https://dplyr.tidyverse.org](https://dplyr.tidyverse.org))
    provides the infrastructure for the tidyverse, as it includes the basic functionality
    that allows data to be transformed and manipulated. It also provides a straightforward
    way to begin working with larger datasets in R. Though there are other packages
    that have greater raw speed or are capable of handling even more massive datasets,
    dplyr is still quite capable and a good first step to take if you run into speed
    or memory limitations with base R.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'When used with tibble objects, dplyr unlocks some impressive functionality:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Because dplyr focuses on data frames rather than vectors, new operators are
    introduced that allow common data transformations to be performed with much less
    code while remaining highly readable.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The package makes reasonable assumptions about data frames, which optimizes
    your effort as well as memory use. If possible, it avoids making copies of data
    by pointing to the original value instead.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key portions of the code are written in C++, which, according to the authors,
    yields a 20x to 1,000x performance increase over base R for many operations.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R data frames are limited by available memory. With dplyr, tibbles can be linked
    transparently to disk-based databases exceeding what can be stored in memory.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dplyr grammar of working with data becomes second nature after the initial
    learning curve has been passed. There are five key verbs in the grammar, which
    perform many of the most common transformations to data tables. Beginning with
    a tibble, one may choose to:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '`filter()` rows of data by values of the columns'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select()` columns of data by name'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arrange()` rows of data by sorting the values'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mutate()` columns into new columns by transforming the values'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summarize()` rows of data by aggregating values into a summary'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These five dplyr verbs are brought together in sequences using a **pipe operator**,
    which is natively supported in R as of version 4.1 or later. Represented by the
    `|>` symbols, which vaguely resembles an arrowhead pointing to the right, the
    pipe operator “pipes” data by moving it from one function to another. The use
    of pipes allows you to create powerful chains of functions to sequentially process
    datasets.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: In versions of R prior to 4.1.0 update, the pipe operator was denoted by the
    `%>%` character sequence and required the `magrittr` package. The differences
    between the old and new pipe functionality are relatively minor, but as a native
    operator the new pipe may have a small speed advantage. For a shortcut to typing
    the pipe operator, the RStudio Desktop IDE, the key combination *ctrl* + *shift*
    + *m* will insert the character sequence. Note that for this shortcut to produce
    the updated pipe, you may need to change the setting to “**Use the native pipe
    operator**, **|>**” in the RStudio “**Global Options**” menu under the “**Code**”
    heading.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the package with the `library(dplyr)` command, data transformations
    begin with a tibble being piped into one of the package’s verbs. For example,
    one might `filter()` rows of the Titanic dataset to limit rows to women:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, one might `select()` only the name, sex, and age columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Where dplyr starts to shine is through its ability to chain together verbs
    in a sequence with pipes. For example, we can combine the prior two verbs, sort
    alphabetically using the verb `arrange()`, and save the output to a tibble as
    follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Although this may not seem like a revelation just yet, when combined with the
    `mutate()` verb, we can perform complex data transformations with simpler, more
    readable code than in the base R language. We will see several examples of `mutate()`
    later on, but for now, the important thing to remember is that it is used to create
    new columns in the tibble. For example, we might create a binary `elderly` feature
    that indicates whether a passenger is at least 65 years old.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'This uses the dplyr package’s `if_else()` function to assign a value of `1`
    if the passenger is elderly, and `0` otherwise:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By separating the statements by commas, multiple columns can be created within
    a single `mutate()` statement. This is demonstrated here to create an additional
    `child` feature that indicates whether the passenger is less than 18 years old:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The remaining `dplyr` verb, `summarize()`, allows us to create aggregated or
    summarized metrics by grouping rows in the tibble. For example, suppose we would
    like to compute the survival rate by age or sex. We’ll begin with sex, as it is
    the easier of the two cases. We simply pipe the data into the `group_by(Sex)`
    function to create the male and female groups, then follow this with a `summarize()`
    statement to create a `survival_rate` feature that computes the average survival
    by group:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As shown in the output, females were substantially more likely to survive than
    males.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute survival by age, things are slightly more complicated due to the
    missing age values. We’ll need to filter out these rows and use the `group_by()`
    function to compare children (less than 18 years old) to adults as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The results suggest that children were about 40% more likely to survive than
    adults. When combined with the comparison between males and females, this provides
    strong evidence of the hypothesized “women and children first” policy for evacuation
    of the sinking ship.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Because summary statistics by group can be computed using other methods in base
    R (including the `ave()` and `aggregate()` functions described in previous chapters),
    it is important to note that the `summarize()` command is also capable of much
    more than this. In particular, one might use it for the feature engineering hints
    described earlier in this chapter, such as observing neighbors’ behavior, utilizing
    related rows, and decomposing time series. All three of these cases involve `group_by()`
    options like households, zip codes, or units of time. Using `dplyr` to perform
    the aggregation for these data preparation operations is much easier than attempting
    to do so in base R.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'To put together what we’ve learned so far and provide one more example using
    pipes, let’s build a decision tree model of the Titanic dataset. We’ll `filter()`
    missing age values, use `mutate()` to create a new `AgeGroup` feature, and `select()`
    only the columns of interest for the decision tree model. The resulting dataset
    is piped to the `rpart()` decision tree algorithm, which illustrates the ability
    to pipe data to functions outside of the tidyverse:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that the series of steps reads almost like plain-language pseudocode. It
    is also worth noting the arguments within the `rpart()` function call. The `formula
    = Survived ~ .` argument uses R’s formula interface to model survival as a function
    of all predictors; the dot here represents the other features in the dataset not
    explicitly listed. The `data = _` argument uses the underscore (_) as a placeholder
    to represent the data being fed to `rpart()` by the pipe. The underscore can be
    used in this way to indicate the function parameter to which the data should be
    piped.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: This is usually unnecessary for dplyr’s built-in functions, because they look
    for the piped data as the first parameter by default, but functions outside the
    tidyverse may require the pipe to target a specific function parameter in this
    way.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the underscore placeholder character is new as
    of R version 4.2 and will not work in prior versions! In older code that uses
    the `magrittr` package, the dot character (.) was used as the placeholder.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'For fun, we can visualize the resulting decision tree, which shows that women
    and children are more likely to survive than adults, men, and those in third passenger
    class:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following decision tree diagram:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B17290_12_14.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: A decision tree predicting Titanic survival, which was built
    using a series of pipes'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few small examples of how sequences of dplyr commands can make
    complex data manipulation tasks simpler. This is on top of the fact that, due
    to dplyr’s more efficient code, the steps often execute more quickly than the
    equivalent commands in base R! Providing a complete dplyr tutorial is beyond the
    scope of this book, but there are many learning resources available online, including
    the *R for Data Science* chapter at `https://r4ds.hadley.nz/transform.html`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Transforming text with stringr
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `stringr` package (`https://stringr.tidyverse.org`) adds functions to analyze
    and transform character strings. Base R, of course, can do this too, but the functions
    are inconsistent in how they work on vectors and are relatively slow; `stringr`
    implements these functions in a form more attuned to the tidyverse workflow. The
    free resource *R for Data Science* has a tutorial that introduces the package’s
    complete set of capabilities, at [https://r4ds.hadley.nz/strings.html](https://r4ds.hadley.nz/strings.html),
    but here, we’ll examine some of the aspects most relevant to feature engineering.
    If you’d like to follow along, be sure to load the Titanic dataset and install
    and load the `stringr` package before proceeding.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, the second tip for feature engineering was to “find
    insights hidden in text.” The `stringr` package can assist with this effort by
    providing functions to slice strings and detect patterns within text. All `stringr`
    functions begin with the prefix `str_`, and a few relevant examples are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '`str_detect()` determines whether a search term is found in a string'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_sub()` slices a string by position and returns a substring'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_extract()` searches for a string and returns the matching pattern'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_replace()` replaces characters in a string with something else'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these functions seem quite similar, they are used for quite different
    purposes. To demonstrate these purposes, we’ll begin by examining the `Cabin`
    feature to determine whether certain rooms on the *Titanic* are linked to greater
    survival. We cannot use this feature as-is, because each cabin code is unique.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'However, because the codes are in forms like `A10`, `B101`, or `E67`, perhaps
    the alphabetical prefix indicates a position on the ship, and perhaps passengers
    in some of these locations may have been more able to escape the disaster. We’ll
    use the `str_sub()` function to take a 1-character substring beginning and ending
    at position 1, and save this to a `CabinCode` feature as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To confirm that the cabin code is meaningful, we can use the `table()` function
    to see a clear relationship between it and the passenger class. The `useNA` parameter
    is set to `"ifany"` to display the `NA` values caused by missing cabin codes for
    some passengers:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `NA` values appear to be more common in the lower ticket classes, so it
    seems plausible that cheaper fares may have not received a cabin code.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot the survival probability by cabin code by piping the file
    into a `ggplot()` function:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting figure shows that even within the first-class cabin types (codes
    A, B, and C) there are differences in survival rate; additionally, the passengers
    without a cabin code are the least likely to survive:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_12_15.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: The cabin code feature seems related to survival, even within
    first-class cabins (A, B, and C)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Without processing the `Cabin` text data first, a learning algorithm would be
    unable to use the feature as the codes are unique to each cabin. Yet by applying
    a simple text transformation, we’ve decoded the cabin codes into something that
    can be used to improve the model’s survival predictions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'With this success in mind, let’s examine another potential source of hidden
    data: the `Name` column. One might assume that this is unusable in a model, because
    the name is a unique identifier per row and training a model on this data will
    inevitably lead to overfitting. Although this is true, there is useful information
    hiding within the names. Looking at the first few rows reveals some potentially
    useful text strings:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For one, the salutation (Mr., Mrs., and Miss.) might be helpful for prediction.
    The problem is that these titles are located at different positions within the
    name strings, so we cannot simply use the `str_sub()` function to extract them.
    The correct tool for this job is `str_extract()`, which is used to match and extract
    shorter patterns from longer strings. The trick with working with this function
    is knowing how to express a text pattern rather than typing each potential salutation
    separately.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The shorthand used to express a text search pattern is called a **regular expression**,
    or **regex** for short. Knowing how to create regular expressions is an incredibly
    useful skill, as they are used for the advanced find-and-replace features in many
    text editors, in addition to being useful for feature engineering in R. We’ll
    create a simple regex to extract the salutations from the name strings.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in using regular expressions is to identify the common elements
    across all the desired target strings. In the case of the Titanic names, it looks
    like each salutation is preceded by a comma followed by a blank space, then has
    a series of letters before ending with a period. This can be coded as the following
    regex string:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This seems to be nonsense but can be understood as a sequence that attempts
    to match a pattern character by character. The matching process begins with a
    comma and a blank space, as expected. Next, the square brackets tell the search
    function to look for any of the characters inside the brackets. For instance,
    `[AB]` would search for `A` or `B`, and `[ABC]` would search for `A`, `B`, or
    `C`. In our usage, the dash is used to search for any characters within the range
    between `A` and `z`. Note that capitalization is important—that is, `[A-Z]` is
    different from `[A-z]`. The former will search 26 characters comprising the uppercase
    alphabet while the latter will search 52 characters, including uppercase and lowercase.
    Keep in mind that `[A-z]` only matches a single character.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: To have the expression match more characters, we follow the brackets with a
    `+` symbol to tell the algorithm to continue matching characters until it reaches
    something not inside the brackets. Then, it checks to see whether the remaining
    part of the regex matches.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The remaining piece is the `\\.` sequence, which is three characters that represent
    the single period character at the end of our search pattern. Because the dot
    is a special term that represents an arbitrary character, we must escape the dot
    by prefixing it with a slash. Unfortunately, the slash is also a special character
    in R, so we must escape it as well by prefixing it with yet another slash.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions can be tricky to learn but are well worth the effort. You
    can find a deep dive into understanding how they work at [https://www.regular-expressions.info](https://www.regular-expressions.info).
    Alternatively, there are many text editors and web applications that demonstrate
    matching in real time. These can be hugely helpful to understand how to develop
    the regex search patterns and diagnose errors. One of the best such tools is found
    at [https://regexr.com](https://regexr.com).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'We can put this expression to work on the Titanic name data by combining it
    in a `mutate()` function with `str_extract()`, as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Looking at the first few examples, it looks like these need to be cleaned up
    a bit:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s use the `str_replace()` function to eliminate the punctuation and blank
    spaces in these titles. We begin by constructing a regex to match the punctuation
    and empty space. One way to do this is to match the comma, blank space, and period
    using the `"[, \\.]"` search string. Used with `str_replace()` as shown here,
    any comma, blank space, and period characters in `Title` will be replaced by the
    empty (null) string:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the `str_replace_all()` variant of the replace function was used
    due to the fact that multiple characters needed replacement; the basic `str_replace()`
    would have only replaced the first instance of a matching character. Many of `stringr`''s
    functions have “all” variants for this use case. Let’s see the result of our effort:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Given the small counts for some of these titles and salutations, it may make
    sense to group them together. To this end, we can use dplyr’s `recode()` function
    to change the categories. We’ll keep several of the high-count levels the same,
    while grouping the rest into variants of `Miss` and a catch-all bucket, using
    the `.missing` and `.default` values to assign the `Other` label to `NA` values
    and anything else not already coded:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Checking our work, we see that our cleanup worked as planned:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also see that the title is meaningful by examining a plot of survival
    rates by title:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This produces the following bar chart:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_16.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: The constructed salutation captures the impact of both age and
    gender on survival likelihood'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The creation of `CabinCode` and `TitleGroup` features exemplifies the feature
    engineering technique of finding hidden information in text data. These new features
    are likely to provide additional information beyond the base features in the Titanic
    dataset, which learning algorithms can use to improve performance. A bit of creativity
    combined with `stringr` and knowledge of regular expressions may provide the edge
    needed to surpass the competition.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning dates with lubridate
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lubridate` package ([https://lubridate.tidyverse.org](https://lubridate.tidyverse.org))
    is an important tool for working with date and time data. It may not be needed
    for every analysis, but when it is needed, it can save a lot of grief. With dates
    and times, seemingly simple tasks can quickly turn into adventures, due to unforeseen
    subtleties like leap years and time zones—just ask anyone who has worked on birthday
    calculations, billing cycles, or similar date-sensitive tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the other tidyverse packages, the *R for Data Science* resource has
    an in-depth lubridate tutorial at `https://r4ds.hadley.nz/datetimes.html`, but
    we’ll briefly cover three of its most important feature engineering strengths
    here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring date and time data is loaded into R correctly while accounting for
    regional differences in how dates and times are expressed
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accurately calculating differences between dates and times while accounting
    for time zones and leap years
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for differences in how increments in time are understood in the real
    world, such as the fact that people become “1 year older” on their birthday
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reading dates into R is challenge number one, because dates are presented in
    many different formats. For example, the publication date of the first edition
    of *Machine Learning with R* can be expressed as:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: October 25, 2013 (a common longhand format in the United States)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10/25/13 (a common shorthand format in the United States)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 October 2013 (a common longhand format in Europe)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25.10.13 (a common shorthand format in Europe)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2013-10-25 (the international standard)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these diverse formats, lubridate is incapable of determining the correct
    format without help because months, days, and years can all fall in the range
    from 1 to 12\. Instead, we provide it the correct date constructor—either `mdy()`,
    `dmy()`, or `ymd()`, depending on the order of the month (`m`), day (`d`), and
    year (`y`) components of the input data. Given the order of the date components,
    the functions will automatically parse longhand and shorthand variants, and will
    handle leading zeros and two- or four-digit years. To demonstrate this, the dates
    expressed previously can be handled with the appropriate lubridate function, as
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Notice that in each case, the resulting `Date` object is exactly the same.
    Let’s create a similar object for each of the three previous editions of this
    book:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can do simple math to compute the difference between two dates:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice that by default, the difference between the two dates is returned as
    days. What if we hope to have an answer in years? Unfortunately, because these
    differences are a special lubridate `difftime` object, we cannot simply divide
    these numbers by 365 days to perform the obvious calculation. One option is to
    convert them into a **duration**, which is one of the ways lubridate computes
    date differences, and in particular, tracks the passage of physical time—imagine
    it acting much like a stopwatch. The `as.duration()` function performs the needed
    conversion:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can see here that the gap between the 2nd and 3rd editions of *Machine Learning
    with R* was almost twice as long as the difference between the 1st and 2nd editions.
    We can also see that the duration seems to default to seconds while also providing
    the approximate number of years. To obtain only years, we can divide the duration
    by the duration of 1 year, which lubridate provides as a `dyears()` function:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You may find it more convenient or easier to remember the `time_length()` function,
    which can perform the same calculation:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `unit` argument can be set to units like days, months, and years, depending
    on the desired result. Notice, however, that these durations are exact to the
    second like a stopwatch, which is not always how people think about dates.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: In particular, for birthdays and anniversaries, people tend to think in terms
    of calendar time—that is, the number of times the calendar has reached a particular
    milestone. In lubridate, this approach is called an **interval**, which implies
    a timeline-or calendar-based view of date differences, rather than the stopwatch-based
    approach of the duration methods discussed previously.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine we’d like to compute the age of the United States, which was
    born, so to speak, on July 4, 1776\. This means that on July 3, 2023, the country
    will be 246 birthdays old, and on July 5, 2023, it will be 247\. Using durations,
    we don’t get quite the right answers:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The problem has to do with the fact that durations deviate from calendar time
    due to calendar irregularities such as leap years and time changes. By explicitly
    converting the date difference into an interval with the `interval()` function,
    and then dividing by the `years()` function, we get closer to the right answer:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Before going any further, be sure to notice the fact that the `interval()` uses
    `start, end` syntax, in contrast to the date difference, which used `end - start`.
    Also note that the `years()` function returns a lubridate **period**, which is
    yet another way to understand differences between dates and times. Periods are
    always relative to their position on a calendar, which means that a 1-hour period
    can be a 2-hour duration during a time change, and a 1-year period can include
    365 or 366 1-day periods, depending on the calendar year—these are the types of
    challenging subtleties when working with dates that were mentioned in this section’s
    opening paragraph!
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our final age calculation, we’ll use the `%--%` interval construction
    operator as shorthand, and use the integer division operator `%/%` to return only
    the integer component of the age. These return the expected age values:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Generalizing this work, we can create a function to compute the calendar-based
    age for a given date of birth as of today:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To prove that it works, we’ll check the ages of a few famous tech billionaires:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: If you are following along in R, be aware that your results may vary depending
    on when you run the code—we’re all, unfortunately, still getting older by the
    day!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated the importance of data preparation. Because the tools
    and algorithms used to build machine learning models are the same across projects,
    data preparation is a key that unlocks the highest levels of model performance.
    This allows some aspects of human intelligence and creativity to have a large
    impact on the machine’s learning process, although clever practitioners use their
    strengths in concert with the machine’s by developing automated data engineering
    pipelines that take advantage of the computer’s ability to tirelessly search for
    useful insights in the data. These pipelines are especially important in the so-called
    “big data regime,” where data-hungry approaches like deep learning must be fed
    large amounts of data to avoid overfitting.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: In traditional small and medium data regimes, feature engineering by hand still
    reigns supreme. Using intuition and subject matter expertise, one can guide the
    model to the most useful signal in the training dataset. As this is more art than
    science, tips and tricks are learned on the job, or passed along second-hand from
    one data scientist to another. This chapter provided seven hints to help guide
    you on the journey, but the only way to truly become skilled at feature engineering
    is through practice.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Tools like the tidyverse suite of R packages make it much less laborious than
    in years past to gain the necessary experience to perform feature engineering
    tasks. This chapter demonstrated how the tidyverse packages can be used to turn
    data into more useful predictors, and how information hidden in text data can
    be extracted to turn what seem like useless features into important predictors.
    The tidyverse packages are much more capable of handling large and ever-growing
    datasets than the base R functions, and they make R a pleasure to use even as
    datasets grow in size and complexity.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: The skills developed in this chapter will provide a foundation for the work
    to come. In the next chapter, you will add new tidyverse packages to your toolkit
    and see even more examples of how it integrates into the machine learning workflow.
    You will continue to see the importance of data preparation skills as you explore
    data issues that begin as relatively minor challenges but quickly grow into massive
    problems if taken to an extreme.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
