- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Advanced Data Preparation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级数据准备
- en: The truism that 80 percent of the time invested in real-world machine learning
    projects is spent on data preparation is so widely cited that it is mostly accepted
    without question. Earlier chapters of this book helped perpetuate the cliché by
    stating it as a matter of fact without qualification, and although it is certainly
    a common experience and perception, it is also an oversimplification, as tends
    to be the case when generalizing from a statistic. In reality, there is no single,
    uniform experience for data preparation. Yet, it is indeed true that data prep
    work almost always involves more effort than anticipated.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 80%的时间投资在现实世界的机器学习项目中用于数据准备这一普遍真理被广泛引用，以至于它通常被无异议地接受。本书的早期章节通过不附加任何条件地将这一说法作为事实陈述来帮助延续这一陈词滥调，尽管这确实是一种常见的经验和感知，但它也是一种过度简化，正如从统计数据中概括时通常发生的那样。实际上，数据准备并没有统一的单一经验。然而，确实是真的，数据准备工作几乎总是比预期的要花费更多的努力。
- en: Rare is the case in which you will be provided a single CSV formatted text file,
    which can be easily read into R and processed with just a few lines of R code,
    as was the case in previous chapters. Instead, necessary data elements are often
    distributed across databases, which must then be gathered, filtered, reformatted,
    and combined before the features can be used with machine learning. This can require
    significant effort even before considering the time expended gaining access to
    the data from stakeholders, as well as exploring and understanding the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有情况会提供单个CSV格式的文本文件，可以很容易地被R读取并使用几行R代码进行处理，就像前几章所描述的那样。相反，必要的数据元素通常分布在数据库中，然后必须收集、过滤、重新格式化并组合，才能使用机器学习来使用特征。这甚至在没有考虑从利益相关者那里获取数据所需的时间以及探索和理解数据之前，就可能需要大量的努力。
- en: 'This chapter is intended to prepare you (pun intended!) for the larger and
    more complex datasets that you’ll be preparing in the real world. You will learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为你（有意为之！）准备你在现实世界中将要准备的大型且更复杂的数据集。你将学习：
- en: Why data preparation is crucial to building better models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么数据准备对于构建更好的模型至关重要
- en: Tips and tricks for transforming data into more useful predictors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为更有用的预测因子的技巧和窍门
- en: Specialized R packages for efficiently preparing data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于高效准备数据的专用R包
- en: Different teams and different projects require their data scientists to invest
    different amounts of time preparing data for the machine learning process, and
    thus, the 80 percent statistic may overstate or understate the effort needed for
    any given project or from any single contributor.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的团队和不同的项目需要他们的数据科学家投入不同数量的时间来准备机器学习过程中的数据，因此，80%的统计数据可能高估或低估了任何特定项目或单个贡献者所需的努力。
- en: Still, whether it is you or someone else performing this work, you will soon
    discover the undeniable fact that advanced data preparation is a necessary step
    in the process of building strong machine learning projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论是由你还是别人执行这项工作，你很快就会发现一个不可否认的事实，那就是高级数据准备是构建强大的机器学习项目过程中的一个必要步骤。
- en: Performing feature engineering
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行特征工程
- en: Time, effort, and imagination are central to the process of **feature engineering**,
    which involves applying subject-matter expertise to create new features for prediction.
    In simple terms, it might be described as the art of making data more useful.
    In more complex terms, it involves a combination of domain expertise and data
    transformations. One needs to know not just what data will be useful to gather
    for the machine learning project, but also how to merge, code, and clean the data
    to conform to the algorithm’s expectations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 时间、努力和想象力是**特征工程**过程的核心，该过程涉及将专业知识应用于创建用于预测的新特征。简单来说，它可能被描述为使数据更有用的艺术。更复杂地说，它涉及领域专业知识和数据转换的结合。一个人不仅需要知道对于机器学习项目哪些数据是有用的，还需要知道如何合并、编码和清理数据以满足算法的期望。
- en: Feature engineering is closely interrelated with data exploration, as described
    in *Chapter 11*, *Being Successful with Machine Learning*. Both involve interrogating
    data through the generation and testing of hypotheses. Exploring and brainstorming
    are likely to lead to insights about which features will be useful for prediction,
    and the act of engineering the features may lead to new questions to explore.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程与数据探索密切相关，如*第十一章*，*在机器学习中取得成功*中所述。两者都涉及通过生成和测试假设来质询数据。探索和头脑风暴可能导致关于哪些特征对预测有用的见解，而特征工程的行为可能导致需要探索的新问题。
- en: '![Diagram  Description automatically generated](img/B17290_12_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_12_01.png)'
- en: 'Figure 12.1: Feature engineering is part of a cycle that helps the model and
    data work together'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：特征工程是帮助模型和数据协同工作的循环的一部分
- en: 'Feature engineering is part of a cycle within a cycle in which effort is invested
    to help the model and data work better together. A round of data exploration and
    feature engineering leads to improvements to the data, which leads to iterations
    of training better models, which then informs another round of potential improvements
    to the data. These potential improvements are not only the bare minimum cleaning
    and preparation tasks needed to address simple data issues and allow the algorithm
    to run in R, but also the steps that lead an algorithm to learn more effectively.
    These may include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是循环中的循环的一部分，其中投入努力以帮助模型和数据更好地协同工作。一轮数据探索和特征工程导致数据改进，进而导致训练更好模型的迭代，然后又指导另一轮潜在的数据改进。这些潜在改进不仅包括解决简单数据问题并允许算法在R中运行所需的最低限度的清理和准备任务，还包括使算法更有效地学习的步骤。这些可能包括：
- en: Performing complex data transformations that help the algorithm to learn faster
    or to learn a simpler representation of the data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行复杂的数据转换，帮助算法更快地学习或学习数据的简化表示
- en: Creating features that are easier to interpret or better represent the underlying
    theoretical concepts
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建易于解释或更好地表示潜在理论概念的特性
- en: Utilizing unstructured data or merging additional features onto the main source
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用非结构化数据或将额外特征合并到主要来源上
- en: All three of these require both intense thought and creativity, and are improvisational
    and domain-specific rather than formulaic. This being said, the computer and the
    practitioner can share this work using complementary strengths. What the computer
    lacks in creativity and ability to improvise, it may be able to address with computational
    horsepower, brute force, and unwavering persistence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这三者都需要深入的思考和创造力，并且是即兴的和领域特定的，而不是公式化的。话虽如此，计算机和从业者可以通过互补的优势来共享这项工作。计算机在创造力和即兴能力方面的不足，可能可以通过计算能力、蛮力和坚定不移的毅力来弥补。
- en: The role of human and machine
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类和机器的作用
- en: Feature engineering can be viewed as a collaboration between the human and the
    machine during the learning process stage of abstraction. Recall that in *Chapter
    1*, *Introducing Machine Learning*, the abstraction step was defined as the translation
    of stored data into broader concepts and representations. In other words, during
    abstraction, connections are made between elements of raw data, which will represent
    important concepts for the learning objective. These relationships are generally
    defined by a model, which links the learned concepts to an outcome of interest.
    During feature engineering, the human gently guides or nudges the abstraction
    process in a specific direction, with the goal of producing a better-performing
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以看作是人类和机器在学习过程阶段抽象阶段的合作。回想一下在*第一章*，*介绍机器学习*中，抽象步骤被定义为将存储的数据转换为更广泛的概念和表示。换句话说，在抽象过程中，原始数据元素之间建立了联系，这些联系将代表学习目标的重要概念。这些关系通常由一个模型定义，该模型将学习到的概念与感兴趣的输出联系起来。在特征工程过程中，人类温和地引导或推动抽象过程向特定方向前进，目标是产生性能更好的模型。
- en: 'Imagine it this way: recall an instance in your past where you attempted to
    learn a difficult concept—possibly even while reading this very textbook! Reading
    and later re-reading the text proves to be of no help to understanding the concept,
    and frustrated, you contact a friend or colleague for help. Perhaps this friend
    explains the concept in a different way, using analogies or examples that help
    connect the concept to your prior experience, and in doing so, it leads you to
    a moment of enlightenment: “Eureka!” All is suddenly clear, and you wonder how
    you couldn’t understand the concept in the first place. Such is the power of abstractions,
    which can be transferred from one learner to another to aid the learning process.
    The process of feature engineering allows the human to transfer their intuitive
    knowledge or subject-matter expertise to the machine through intentionally and
    purposefully designed input data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这种情况：回忆一下你过去试图学习一个困难概念的时刻——可能甚至就在阅读这本教科书的时候！阅读以及后来重读文本证明对理解概念没有帮助，沮丧的你联系了一个朋友或同事寻求帮助。也许这个朋友会用不同的方式解释这个概念，使用类比或例子帮助你将概念与你先前经验联系起来，在这个过程中，它引导你进入一个顿悟的时刻：“我找到了！”所有东西突然变得清晰，你
    wonder how you couldn’t understand the concept in the first place. Such is the
    power of abstractions, which can be transferred from one learner to another to
    aid the learning process. The process of feature engineering allows the human
    to transfer their intuitive knowledge or subject-matter expertise to the machine
    through intentionally and purposefully designed input data.
- en: Given the fact that abstraction is the cornerstone of the learning process,
    it can be argued that machine learning is fundamentally feature engineering. The
    renowned computer scientist and artificial intelligence pioneer Andrew Ng said,
    “*Coming up with features is difficult, time-consuming, and requires expert knowledge.
    Applied machine learning is basically feature engineering*.” Pedro Domingos, a
    professor of computer science and author of the machine learning book *The Master
    Algorithm*, said that “*some machine learning projects succeed and some fail.
    What makes the difference? Easily the most important factor is the features used*.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到抽象是学习过程的基础，可以说机器学习本质上就是特征工程。著名的计算机科学家和人工智能先驱Andrew Ng说，“*提出特征是困难的，耗时且需要专业知识。应用机器学习基本上就是特征工程*。”计算机科学教授和机器学习书籍
    *The Master Algorithm* 的作者Pedro Domingos说，“*一些机器学习项目成功了，而一些失败了。是什么造成了这种差异？最关键的因素无疑是使用的特征*。”
- en: Andrew Ng’s quote appears in a lecture titled *Machine Learning and AI via Brain
    simulations*, which is available online via web search. In addition to Pedro Domingos’
    book *The Master Algorithm* (2015), see also his excellent paper “A few useful
    things to know about machine learning” in *Communications of the ACM* (2012).
    [https://doi.org/10.1145/2347736.2347755](https://doi.org/10.1145/2347736.2347755).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Ng的名言出现在一个名为 *Machine Learning and AI via Brain simulations* 的讲座中，该讲座可通过网络搜索在线获取。除了Pedro
    Domingos的书籍 *The Master Algorithm*（2015）外，还可以参考他在 *Communications of the ACM*（2012）上发表的优秀论文“关于机器学习的几个有用的事情要知道”。[https://doi.org/10.1145/2347736.2347755](https://doi.org/10.1145/2347736.2347755)。
- en: Feature engineering performed well can turn weaker learners into much stronger
    learners. Many machine learning and artificial intelligence problems can be solved
    with simple linear regression methods, assuming that the data has been sufficiently
    cleaned. Even very complex machine learning methods can be replicated in standard
    linear regression given sufficient feature engineering. Linear regression can
    be adapted to model nonlinear patterns, using splines and quadratic terms, and
    can approach the performance of even the most complex neural networks, given a
    sufficient number of new features that have been engineered as carefully designed
    interactions or transformations of the original input data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 功能工程做得好的话，可以将较弱的学习者转变为更强的学习者。许多机器学习和人工智能问题可以通过简单的线性回归方法解决，假设数据已经被充分清理。即使是非常复杂的机器学习方法，在足够的特征工程下也可以在标准线性回归中复制。线性回归可以适应模型非线性模式，使用样条和二次项，并且可以接近甚至最复杂的神经网络的表现，前提是有足够数量的新特征已经被精心设计为原始输入数据的交互或转换。
- en: The idea that simple learning algorithms can be adapted to more complex problems
    is not limited to regression. For example, decision trees can work around their
    axis-parallel decision boundaries by rotations of the input data, while hyperplane-based
    support vector machines can model complex nonlinear patterns with a well-chosen
    kernel trick. A method as simple as k-NN could be used to mimic regression or
    perhaps even more complex methods, given enough effort and sufficient understanding
    of the input data and learning problem, but herein lies the catch. Why invest
    large amounts of time performing feature engineering to employ a simple method
    when a more complex algorithm will perform just as well or better, while also
    performing the feature engineering for us automatically?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的学习算法可以适应更复杂问题的想法并不仅限于回归。例如，决策树可以通过输入数据的旋转来绕过它们的轴平行决策边界，而基于超平面的支持向量机可以通过选择合适的核技巧来模拟复杂的非线性模式。只要付出足够的努力，并且对输入数据和学习问题有足够的理解，一个像k-NN这样简单的方法也可以用来模拟回归，甚至可能是更复杂的方法，但这里有一个问题。为什么要在使用一个更复杂的算法也能表现同样好或更好的情况下，投入大量时间进行特征工程，而该算法还能自动为我们进行特征工程？
- en: Indeed, it is probably best to match the complexity of the data’s underlying
    patterns with a learning algorithm capable of handling them readily. Performing
    feature engineering by hand when a computer can do it automatically is not only
    wasted effort but also prone to mistakes and missing important patterns. Algorithms
    like decision trees and neural networks with a sufficiently large number of hidden
    nodes—and especially, deep learning neural networks—are particularly capable of
    doing their own form of feature engineering, which is likely to be more rigorous
    and thorough than what can be done by hand. Unfortunately, this does not mean
    we can blindly apply these same methods to every task—after all, there is no free
    lunch in machine learning!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，可能最好是将数据的潜在模式复杂性与能够轻松处理这些模式的学习算法相匹配。当计算机可以自动完成时，手动进行特征工程不仅是一种浪费，而且容易出错，并可能错过重要的模式。像决策树和具有足够多隐藏节点的神经网络这样的算法，尤其是深度学习神经网络，特别擅长进行它们自己的形式特征工程，这可能会比手工操作更加严格和彻底。不幸的是，这并不意味着我们可以盲目地将这些相同的方法应用到每个任务中——毕竟，在机器学习中没有免费的午餐！
- en: Applying the same algorithm to every problem suggests that there is a one-size-fits-all
    approach to feature engineering, when we know that it is as much an art as it
    is a science. Consequently, if all practitioners apply the same method to all
    tasks, they will have no way of knowing whether better performance is possible.
    Perhaps a slightly different feature engineering approach could have resulted
    in a model that more accurately predicted churn or cancer, and would have led
    to greater profits or more lives saved. This is clearly a problem in the real
    world, where even a small performance boost can mean a substantial edge over the
    competition.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的算法应用到每个问题上，似乎表明存在一种适用于所有特征工程的通用方法，但我们知道这既是艺术也是科学。因此，如果所有从业者都将相同的方法应用到所有任务中，他们将无法知道是否可能获得更好的性能。也许稍微不同的特征工程方法可能会产生一个更准确地预测客户流失或癌症的模型，这将导致更大的利润或更多生命的挽救。这在现实世界中显然是一个问题，即使是一点点性能的提升也可能意味着在竞争中占据实质性的优势。
- en: 'In a high-stakes competition environment, such as the machine learning competitions
    on Kaggle, each team has access to the same learning algorithms and is readily
    capable of rapidly applying each of them to identify which one performs best.
    It is no surprise, then, that a theme emerges while reading interviews with Kaggle
    champions: they often invest significant effort into feature engineering. Xavier
    Conort, who was the top-rated data scientist on Kaggle in 2012–2013, said in an
    interview that:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险的竞赛环境中，例如Kaggle上的机器学习竞赛，每个团队都能访问到相同的机器学习算法，并且能够迅速地将它们中的每一个应用到识别哪个表现最佳。因此，在阅读Kaggle冠军的访谈时出现一个主题并不令人惊讶：他们通常会在特征工程上投入大量的努力。2012-2013年Kaggle上评分最高的数据科学家Xavier
    Conort在一次访谈中说：
- en: ”The algorithms we used are very standard for Kagglers… We spent most of our
    efforts on feature engineering… We were also very careful to discard features
    likely to expose us to the risk of overfitting.”
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们使用的算法对于Kagglers来说非常标准……我们的大部分努力都花在了特征工程上……我们也非常小心地丢弃了可能让我们面临过拟合风险的特性。”
- en: Because feature engineering is one of the few proprietary aspects of machine
    learning, it is one of the few points of distinction across teams. In other words,
    teams that perform feature engineering well tend to outperform the competition.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征工程是机器学习少数专有方面之一，因此它是团队之间区分度的一个少数点。换句话说，那些在特征工程方面做得好的团队往往能超越竞争对手。
- en: To read the full interview with Xavier Conort, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/](https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/).
    Interviews with other Kaggle champions are available at [https://medium.com/kaggle-blog/tagged/kaggle-competition](https://medium.com/kaggle-blog/tagged/kaggle-competition).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要阅读与Xavier Conort的完整访谈，该访谈最初发布在Kaggle的“No Free Hunch”博客上，请访问[https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/](https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/)。其他Kaggle冠军的访谈可在[https://medium.com/kaggle-blog/tagged/kaggle-competition](https://medium.com/kaggle-blog/tagged/kaggle-competition)找到。
- en: 'Based on Conort’s statement, it would be easy to assume that the need to invest
    in feature engineering necessitates greater investment in human intelligence and
    the application of subject-matter expertise, but this is not always true. Jeremy
    Achin, a member of a top-performing “DataRobot” team on Kaggle, remarked on the
    surprisingly limited utility of human expertise. Commenting on his team’s time
    spent on feature engineering, he said in an interview that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Conort的声明，人们可能会轻易地认为，在特征工程上的投资需求意味着需要更大的人力和学科专业知识的应用，但事实并非总是如此。Kaggle上表现优异的“DataRobot”团队的一员Jeremy
    Achin评论了人类专业知识出人意料地有限的效用。在谈到他的团队在特征工程上花费的时间时，他在一次采访中说：
- en: ”The most surprising thing was that almost all attempts to use subject matter
    knowledge or insights drawn from data visualization led to drastically worse results.
    We actually arranged a 2-hour whiteboard lecture from a very talented biochemist
    and came up with some ideas based on what we learned, but none of them worked
    out.”
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “最令人惊讶的是，几乎所有试图使用学科知识或从数据可视化中得出的洞察来指导的尝试，都导致了结果的大幅下降。我们实际上安排了一位非常出色的生物化学家进行了2小时的板书讲座，并基于我们所学提出了一些想法，但它们都没有奏效。”
- en: Jeremy Achin, along with Xavier Conort and several other high-profile Kaggle
    Grand Masters, bootstrapped their Kaggle competition successes into an artificial
    intelligence company called DataRobot, which is now worth billions of dollars.
    Their software performs machine learning automatically, suggesting that a key
    lesson learned from their Kaggle work was that computers can perform many steps
    in the machine learning process just as well as humans, if not better.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy Achin与Xavier Conort以及其他几位知名Kaggle大师一起，将他们在Kaggle竞赛中的成功转化为了一家名为DataRobot的人工智能公司，该公司现在价值数十亿美元。他们的软件能自动执行机器学习过程，这表明从他们的Kaggle工作中学到的关键教训是，计算机在机器学习过程中的许多步骤上可以像人类一样做得很好，甚至更好。
- en: To read the full interview with Jeremy Achin, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/](https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/).
    The DataRobot company is found on the web at [https://www.datarobot.com](https://www.datarobot.com).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要阅读与Jeremy Achin的完整访谈，该访谈最初发布在Kaggle的“No Free Hunch”博客上，请访问[https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/](https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/)。DataRobot公司可以在[https://www.datarobot.com](https://www.datarobot.com)上找到。
- en: Of course, there is a balance between building models piece by piece using subject-matter
    expertise and throwing everything at the machine to see what sticks. Although
    feature engineering today is largely still a manual process, the future of the
    field seems to be headed toward the scattershot “see what sticks” approach, as
    **automated feature engineering** is a rapidly growing area of research. The foundation
    of automated feature engineering tools is the idea that a computer can make up
    for its lack of creativity and domain knowledge by testing many more combinations
    of features than a human would ever have time to attempt. Automated feature engineering
    exchanges narrow-but-guided human thought for broad-and-systematic computer thought,
    with the potential upside of finding a more optimal solution and potential downsides
    including loss of interpretability and greater likelihood of overfitting.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在利用专业知识逐步构建模型和将所有内容都投入机器以观其效之间需要保持平衡。尽管今天特征工程在很大程度上仍然是一个手动过程，但该领域的未来似乎正朝着散弹式的“观其效”方法发展，因为**自动化特征工程**是一个快速增长的研究领域。自动化特征工程工具的基础是这样一个观点：计算机可以通过测试比人类有更多时间尝试的更多特征组合来弥补其缺乏创造力和领域知识。自动化特征工程用狭窄但受指导的人类思维交换了广泛而系统的计算机思维，其潜在优势是找到更优的解决方案，潜在的劣势包括可解释性降低和过拟合的可能性增加。
- en: Before getting too excited about the potential for automation, it is worth noting
    that while such tools may allow a human to outsource certain parts of *thinking*
    about feature engineering, effort must still be invested in the *coding* part
    of the task. That is to say, time that was once spent hand-coding features one
    by one is instead spent coding functions that systematically find or construct
    useful features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在过于兴奋于自动化的潜力之前，值得注意的是，尽管这些工具可能允许人类将特征工程中某些“思考”的部分外包出去，但仍然需要在任务的“编码”部分投入努力。也就是说，曾经用于逐个手动编码特征的时间现在被用于编写系统性地寻找或构建有用特征的函数。
- en: There are promising algorithms in development, such as the Python-based `Featuretools`
    package (and corresponding R package `featuretoolsR`, which interacts with the
    Python code), that may help automate the feature-building process, but the use
    of such tools is not yet widespread. Additionally, such methods must be fed by
    data and computing time, both of which may be limiting factors in many machine
    learning projects.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正在开发中的一些有希望的算法，如基于Python的`Featuretools`包（以及相应的R包`featuretoolsR`，它与Python代码交互），可以帮助自动化特征构建过程，但这类工具的使用尚未普及。此外，这些方法需要数据和计算时间，这两者可能是许多机器学习项目的限制因素。
- en: 'For more information on Featuretools, visit: [https://www.featuretools.com](https://www.featuretools.com).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于Featuretools的信息，请访问：[https://www.featuretools.com](https://www.featuretools.com)。
- en: The impact of big data and deep learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据和深度学习的影响
- en: 'Whether feature engineering is performed by a human or by automated machine
    methods, a point is inevitably reached at which additional invested effort leads
    to little or no boost to the learning algorithm’s performance. The application
    of more sophisticated learning algorithms may also improve the model’s performance
    somewhat, but this is also subject to diminishing returns, as there exists only
    a finite number of potential methods to apply and their performance differences
    tend to be relatively minor. Consequently, if additional performance gains are
    truly necessary, we are left with one remaining option: increasing the size of
    the training dataset with additional features or examples. Moreover, because adding
    additional columns would require revising data generated by past business processes,
    in many cases, collecting more rows is the easier option of the two.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无论特征工程是由人类还是由自动化机器方法执行，都会不可避免地达到一个点，即额外的投入努力几乎不会提高学习算法的性能。应用更复杂的学习算法也可能在一定程度上提高模型性能，但这种提升也是递减的，因为可应用的方法数量有限，它们的性能差异通常相对较小。因此，如果确实需要额外的性能提升，我们只剩下最后一个选择：通过添加额外的特征或示例来增加训练数据集的大小。此外，由于添加额外的列需要修订过去业务过程中生成的数据，在许多情况下，收集更多行数据是两个选择中较容易的一个。
- en: In practice, there is a relatively low ceiling on the performance gains achievable
    via the inclusion of more rows of data. Most algorithms described in this book
    plateau quickly and will perform little better on a dataset of 1 million rows
    than on a dataset containing a few thousand. You may have already observed this
    firsthand if you’ve applied machine learning methods to real-world projects in
    your own areas of interest. Once a dataset is big enough—often just a few thousand
    rows for many real-world applications—additional examples merely cause additional
    problems, such as extended computation time and running out of memory. If more
    data causes more problems, then the natural follow-up question is, why is there
    so much hype around the so-called “big data” era?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通过包含更多行数据所能实现的性能提升相对有限。本书中描述的大多数算法很快就会达到顶峰，在包含一百万行数据的集合上与包含几千行数据的集合相比，性能提升很小。如果你已经在你感兴趣的领域将机器学习方法应用于实际项目，你可能已经亲身体验到这一点。一旦数据集足够大——对于许多实际应用来说，通常只是几千行——额外的例子只会带来更多问题，比如计算时间延长和内存不足。如果更多的数据导致更多问题，那么自然而然地，人们会问，为什么所谓的“大数据时代”会有如此多的炒作？
- en: To answer this question, we must first begin by making a philosophical distinction
    between datasets of various sizes. To be clear, “big data” does not merely imply
    a large number of rows or a large amount of storage consumed in a database or
    filesystem. In fact, it comprises both of these and more, as size is just one
    of four elements that may indicate the presence of big data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们首先必须对各种大小的数据集进行哲学上的区分。为了明确，“大数据”不仅仅意味着数据库或文件系统中行数或存储消耗量很大。实际上，它包括这两者以及更多，因为大小只是可能表明大数据存在的四个要素之一。
- en: 'These are the so-called **four V’s of big data**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是所谓的**大数据的四个V**：
- en: '**Volume**: The literal size of the data, whether it be more rows, more columns,
    or more storage'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数量**：数据的实际大小，无论是行数更多、列数更多还是存储更多'
- en: '**Velocity**: The speed at which data accumulates, which impacts not only the
    volume but also the complexity of data processing'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：数据积累的速度，这不仅影响数量，还影响数据处理复杂性'
- en: '**Variety**: The differences in types or definitions of data across different
    systems, particularly the addition of unstructured sources such as text, images,
    and audio data'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：不同系统间数据类型或定义的差异，尤其是文本、图像和音频数据等非结构化来源的增加'
- en: '**Veracity**: The trustworthiness of the input data and the ability to match
    data across sources'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：输入数据的可信度和跨来源匹配数据的能力'
- en: 'Reading this list from top to bottom, the elements become less intuitively
    obvious, yet are more challenging to handle when encountered. The first two elements,
    volume and velocity, are the basis of what might be dubbed the **medium data**
    space. While this is not to say that there aren’t challenges working with high-volume,
    high-velocity data, these challenges can often be solved by scaling up what we
    are already doing. For example, it may be possible to use faster computers with
    more memory or apply a more computationally efficient algorithm. The presence
    of a greater variety and reduced veracity of data requires a completely different
    approach for use in machine learning projects, especially at high-velocity and
    high-volume scales. The following table lists some of the distinctions between
    the small, medium, and big data spaces:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从上到下阅读这个列表，元素变得不那么直观，但在遇到时却更具挑战性。前两个元素，数量和速度，是所谓的**中等数据**空间的基础。虽然这并不是说处理高数量、高速度的数据没有挑战，但这些挑战通常可以通过扩大我们正在做的事情来解决。例如，可能可以使用具有更多内存的更快计算机或应用更计算高效的算法。数据多样性的增加和真实性的降低需要完全不同的方法来在机器学习项目中使用，尤其是在高速和高数量规模上。以下表格列出了小、中、大数据空间之间的一些区别：
- en: '![Table  Description automatically generated](img/B17290_12_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_12_02.png)'
- en: 'Figure 12.2: Most machine learning projects are on the scale of “medium data”
    while additional skills and tools are required to make use of “big data”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：大多数机器学习项目处于“中等数据”规模，而要利用“大数据”则需要额外的技能和工具
- en: Moving from small to medium and then from medium to big data requires exponential
    investment. As datasets increase in size and complexity, the required infrastructure
    becomes much more complex, adding increasingly specialized databases, computing
    hardware, and analysis tools, some of which will be covered in *Chapter 15*, *Making
    Use of Big Data*. These tools are rapidly changing, which necessitates constant
    training and re-skilling. With the increased scale of data, time becomes a more
    significant constraint; not only are the projects more complex with many more
    moving pieces, requiring more cycles of iteration and refinement, but the work
    simply takes longer to complete—literally! A machine learning algorithm that runs
    in minutes on a medium-sized dataset may take hours or days on a much larger dataset,
    even with the benefit of cloud computing power.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从小数据过渡到中等数据，再从中等数据过渡到大数据，需要指数级投资。随着数据集规模和复杂性的增加，所需的基础设施变得更加复杂，增加了越来越多的专用数据库、计算硬件和分析工具，其中一些将在第15章“利用大数据”中介绍。这些工具正在迅速变化，这需要不断的培训和技能提升。随着数据规模的增加，时间成为一个更重要的约束；项目更加复杂，涉及更多的移动部件，需要更多的迭代和改进周期，而且工作完成的时间更长——实际上是这样！在一个中等规模的数据集上运行只需几分钟的机器学习算法，在一个大得多得数据集上可能需要几个小时或几天，即使有云计算能力的帮助。
- en: Given the high stakes of big data, there is often an order of magnitude difference
    in how such projects are staffed and resourced—it is simply considered part of
    “the cost of doing business.” There may be dozens of data scientists, with matching
    numbers of IT professionals supporting the required infrastructure and data processing
    pipeline. Typical big data solutions require numerous tools and technologies to
    work together. This creates an opportunity for **data architects** to plan and
    structure the various computing resources and to monitor their security, performance,
    and cloud hosting costs. Similarly, data scientists are often matched by an equal
    or greater number of **data engineers**, who are responsible for piping data between
    sources and doing the most complex programming work. Their efforts in processing
    large datasets allow data scientists to focus on analysis and machine learning
    model building.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大数据的高风险，这类项目的人员配置和资源投入通常存在一个数量级的差异——这被认为是“业务成本”的一部分。可能会有几十个数据科学家，以及相应数量的IT专业人员来支持所需的基础设施和数据处理流程。典型的大数据解决方案需要多种工具和技术协同工作。这为**数据架构师**提供了规划和构建各种计算资源的机会，并监控其安全性、性能和云托管成本。同样，数据科学家通常与相等或更多的**数据工程师**相匹配，他们负责在数据源之间传输数据，并执行最复杂的编程工作。他们在处理大数据集方面的努力使得数据科学家能够专注于分析和机器学习模型构建。
- en: From the perspective of those working on the largest and most challenging machine
    learning projects today, many everyday projects, including nearly all the examples
    covered in this book, fall squarely into what has been called a **small data regime**.
    In this paradigm, datasets can grow to be “large” in terms of the number of rows
    or in sheer storage volume, but they will never truly be “big data.” Computer
    science and machine learning expert Andrew Ng has noted that in the realm of small
    data, the role of the human is still impactful; the human can greatly impact a
    project’s performance via hand-engineering features or by the selection of the
    most performant learning algorithm. However, as a dataset grows beyond “large”
    and into “huge” sizes and into the **big data regime**, a different class of algorithms
    breaks through the performance plateau to surpass the small gains of manual tweaks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从今天从事最大和最具挑战性的机器学习项目的人员的角度来看，包括本书中涵盖的几乎所有示例在内的大多数日常项目，都完全属于所谓的**小数据范畴**。在这个范式下，数据集在行数或存储容量方面可以增长到“大”规模，但它们永远不会真正成为“大数据”。计算机科学和机器学习专家安德鲁·吴（Andrew
    Ng）指出，在小数据领域，人类的作用仍然具有影响力；人类可以通过手动设计特征或选择最有效的学习算法来极大地影响项目的性能。然而，当数据集规模从“大”增长到“巨大”并进入**大数据范畴**时，一类不同的算法突破了性能瓶颈，超越了手动调整的小幅提升。
- en: '*Figure 12.3*, which is adapted from Ng’s work, illustrates this phenomenon:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.3*，改编自吴的工作，说明了这一现象：'
- en: '![Diagram  Description automatically generated](img/B17290_12_03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17290_12_03.png)'
- en: 'Figure 12.3: In the small data regime, traditional machine learning algorithms
    are competitive with and may even perform better than more complex methods, which
    perform much better as the size of data increases'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：在小数据规模下，传统机器学习算法与更复杂的方法具有竞争力，甚至可能表现更好，而随着数据规模的增加，这些方法的表现会更好。
- en: Within the confines of the small data regime, no single algorithm or class of
    algorithms performs predictably better than the others. Here, clever feature engineering
    including subject-matter expertise and hand-coded features may allow simpler algorithms
    to outperform much more sophisticated approaches or deep learning neural networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在小数据规模范围内，没有任何单个算法或算法类别比其他算法可预测地表现更好。在这里，包括领域专业知识的手工编码特征在内的巧妙特征工程可能允许简单的算法超越更复杂的方法或深度学习神经网络。
- en: As the size of data increases to the medium data regime, ensemble approaches
    (described in *Chapter 14*, *Building Better Learners*) tend to perform better
    than even a carefully handcrafted model that uses traditional machine learning
    algorithms. For the largest datasets found in the big data regime, only deep learning
    neural networks (introduced in *Chapter 7*, *Black-Box Methods – Neural Networks
    and Support Vector Machines*, and to be covered in more detail in *Chapter 15*,
    *Making Use of Big Data*) appear to be capable of the utmost performance, as their
    capability to learn from additional data practically never plateaus. Does this
    imply that the “no free lunch” theorem is incorrect and there truly is one learning
    algorithm to rule them all?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据规模增加到中等数据规模时，集成方法（在第14章“构建更好的学习者”中描述）往往比使用传统机器学习算法的精心手工制作的模型表现更好。对于大数据环境中发现的最大数据集，只有深度学习神经网络（在第7章“黑盒方法——神经网络和支持向量机”中介绍，将在第15章“利用大数据”中更详细地介绍）似乎能够达到极致的性能，因为它们从额外数据中学习的能力几乎从未达到平台期。这难道意味着“没有免费的午餐”定理是错误的，并且真的存在一个可以统治所有算法的学习算法吗？
- en: The visualization of the performance of different learning algorithms under
    the small and big data regimes can be found described by Andrew Ng in his own
    words. To find these, simply search YouTube for “Nuts and Bolts of Applying Deep
    Learning” (appears 3 minutes into the video) or “Artificial Intelligence is the
    New Electricity” (appears 20 minutes into the video).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在小数据和大数据环境下不同学习算法性能的可视化，可以在Andrew Ng用自己的话描述中找到。要找到这些，只需在YouTube上搜索“应用深度学习的螺丝和螺母”（视频3分钟处出现）或“人工智能是新的电力”（视频20分钟处出现）。
- en: To understand why certain algorithms perform better than others under the big
    data regime and why the “no free lunch” principle still applies, we must first
    consider the relationship between the size and complexity of the data, the capability
    of a model to learn a complex pattern, and the risk of overfitting. Let’s begin
    by considering a case in which the size and complexity of the data is held constant,
    but we increase the complexity of the learning algorithm to more closely model
    what is observed in the training data. For example, we may grow a decision tree
    to an overly large size, increase the number of predictors in a regression model,
    or add hidden nodes in a neural network. This relationship is closely linked to
    the idea of the bias-variance trade-off; by increasing the model complexity, we
    allow the model to conform more closely to the training data and, therefore, reduce
    its inherent bias and increase its variance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么某些算法在大数据环境下比其他算法表现更好，以及“没有免费的午餐”原则为何仍然适用，我们首先必须考虑数据的大小和复杂性、模型学习复杂模式的能力以及过拟合的风险之间的关系。让我们从一个案例开始，在这个案例中，数据的大小和复杂性保持不变，但我们增加学习算法的复杂性，使其更接近训练数据中观察到的模式。例如，我们可能将决策树扩展到过大的规模，增加回归模型中的预测变量数量，或者在神经网络中添加隐藏节点。这种关系与偏差-方差权衡的概念紧密相关；通过增加模型复杂性，我们允许模型更紧密地符合训练数据，因此减少其固有的偏差并增加其方差。
- en: '*Figure 12.4* illustrates the typical pattern that occurs as models increase
    in complexity. Initially, when the model is underfitted to the training dataset,
    increases in model complexity lead to reductions in model error and increases
    in model performance. However, there is a point at which increases in model complexity
    contribute to overfitting the training dataset. Beyond this point, although the
    model’s error rate on the training dataset continues to be reduced, the test set
    error rate increases, as the model’s ability to generalize beyond training is
    dramatically hindered. Again, this assumes a limit to the dataset’s ability to
    support the model’s increasing complexity.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.4*展示了随着模型复杂性增加时出现的典型模式。最初，当模型对训练数据集欠拟合时，模型复杂性的增加会导致模型错误减少和模型性能提高。然而，存在一个点，在这个点上，模型复杂性的增加会导致训练数据集过拟合。超过这个点，尽管模型在训练数据集上的错误率继续降低，但测试集错误率增加，因为模型泛化到训练数据之外的能力受到严重影响。再次强调，这假设数据集支持模型增加复杂性的能力有限。'
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片，描述自动生成](img/B17290_12_04.png)'
- en: 'Figure 12.4: For many training datasets, increasing the complexity of the learning
    algorithm runs the risk of overfitting and increased test set error'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：对于许多训练数据集，增加学习算法的复杂性可能会带来过拟合和测试集错误增加的风险
- en: If we can increase the size and scope of the training dataset, the big data
    regime may unlock a second tier of machine learning performance, but only if the
    learning algorithm is likewise capable of increasing its complexity to make use
    of the additional data. Many traditional algorithms, such as those covered so
    far in this book, are incapable of making such an evolutionary leap—at least not
    without some extra help.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以增加训练数据集的大小和范围，大数据环境可能会解锁机器学习性能的第二层次，但前提是学习算法同样能够增加其复杂性以利用额外的数据。许多传统算法，如本书中迄今为止所涵盖的，无法实现这样的进化飞跃——至少不是没有一些额外帮助。
- en: The missing link between the traditional machine learning algorithms and those
    capable of making this leap has to do with the number of parameters that the algorithms
    attempt to learn about the data. Recall that in *Chapter 11*, *Being Successful
    with Machine Learning*, parameters were described as the learner’s internal values
    that represent its abstraction of the data. Traditionally, for a variety of reasons,
    including the bias-variance trade-off depicted above, as well as the belief that
    simpler, more parsimonious models should be favored over more complex ones, models
    with fewer parameters have been favored. It was assumed that increasing the number
    of parameters too high would allow the dataset to simply memorize the training
    data, leading to severe overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 传统机器学习算法与能够实现这一飞跃的算法之间的缺失环节，与算法试图学习的数据的参数数量有关。回想一下，在*第11章*中，*成功运用机器学习*，参数被描述为学习者内部值，代表其对数据的抽象。传统上，由于各种原因，包括上面所示的偏差-方差权衡，以及认为应该优先考虑简单、更节俭的模型而不是更复杂的模型，因此更倾向于选择参数较少的模型。人们认为，参数数量过高会增加数据集简单地记住训练数据，从而导致严重的过拟合。
- en: Interestingly, this is true, but only to a point, as *Figure 12.5* depicts.
    As model complexity—that is, the number of parameters—increases, the test set
    error follows the same U-shaped pattern as before. However, a new pattern emerges
    once complexity and parameterization have reached the **interpolation threshold**,
    or the point at which there are enough parameters to memorize and accurately classify
    virtually all the training set examples. At this threshold, generalization error
    is at its maximum, as the model has been greatly overfitted to the training data.
    However, as model complexity increases even further, test set error once again
    begins to drop. With sufficient additional complexity, a heavily overfitted model
    may even surpass the performance of a well-tuned traditional model, at least according
    to our existing notion of “overfitted.”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这在某种程度上是正确的，但仅限于此，如图12.5所示。随着模型复杂性的增加——即参数数量——测试集错误遵循与之前相同的U形模式。然而，一旦复杂性和参数化达到**插值阈值**，或者说有足够的参数来记住并准确分类几乎所有的训练集示例，就会出现新的模式。在这个阈值，泛化误差达到最大，因为模型已经极大地过拟合了训练数据。然而，随着模型复杂性的进一步增加，测试集错误再次开始下降。在足够额外的复杂性下，一个严重过拟合的模型甚至可能超越一个调优良好的传统模型的表现，至少根据我们现有的“过拟合”概念来看。
- en: '![Chart  Description automatically generated with low confidence](img/B17290_12_05.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成，置信度低](img/B17290_12_05.png)'
- en: 'Figure 12.5: Some algorithms are able to make use of big data to generalize
    well even after they seemingly overfit the training data'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：一些算法能够在看似过度拟合训练数据后，仍然能够利用大数据进行很好的泛化
- en: 'For more information on the apparent contradiction of the “double descent”
    curve depicted here, see this groundbreaking paper: *Reconciling modern machine-learning
    practice and the classical bias–variance trade-off, Belkin M, Hsu D, Ma S, and
    Mandal S, 2019, Proceedings of the National Academy of Sciences, Vol. 116(32),
    pp. 15,849-15,854*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此处所绘“双下降”曲线的明显矛盾，更多信息请参阅这篇开创性的论文：*《调和现代机器学习实践与经典偏差-方差权衡，Belkin M, Hsu D, Ma
    S, 和 Mandal S, 2019, 美国国家科学院院刊，第116卷第32期，第15,849-15,854页》*。
- en: The mechanism that explains this unexpected result has to do with an interesting
    and perhaps even magical transformation that occurs in models capable of additional
    parameterization beyond the interpolation threshold. Once a learner has sufficient
    parameters to interpolate (to sufficiently conform to) the training data, additional
    parameters lead to a state of **overparameterization**, in which the additional
    complexity enables higher levels of thinking and abstraction. In essence, an overparameterized
    learner is capable of learning higher-order concepts; in practice, this means
    it is capable of learning how to engineer features or learning how to learn. A
    significant jump in model complexity beyond the interpolation threshold is likely
    to lead to a significant leap in the way the algorithm approaches the problem,
    but of course, not every algorithm is capable of this leap.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这一意外结果的机制与模型中发生的一种有趣甚至可能是神奇的变化有关，这种变化发生在能够进行额外参数化的模型中，而这种参数化超出了插值阈值。一旦学习者拥有足够的参数来插值（足够地符合）训练数据，额外的参数就会导致**过参数化**的状态，在这种状态下，额外的复杂性使得能够达到更高的思维和抽象水平。本质上，一个过参数化的学习者能够学习更高阶的概念；在实践中，这意味着它能够学习如何工程化特征或学习如何学习。模型复杂性的显著跳跃，超出插值阈值，可能会在算法处理问题的方式上带来显著的飞跃，但当然，并非每个算法都具备这种飞跃的能力。
- en: Deep neural networks, which can add additional complexity endlessly and trivially
    via the addition of hidden nodes arranged in layers, are the ideal candidate for
    consuming big data. As you will learn in *Chapter 15*, *Making Use of Big Data*,
    a cleverly designed neural network can engineer its own features out of unstructured
    data such as images, text, or audio. Similarly, its designation as a universal
    function approximator implies that it can identify the best functional form to
    model any pattern it identifies in the data. Thus, we must once again revisit
    the early question of how exactly this doesn’t violate the principle of “no free
    lunch.” It would appear that for datasets of sufficient size, deep learning neural
    networks are the single best approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络，可以通过添加层中排列的隐藏节点无限且轻易地增加额外的复杂性，是消耗大数据的理想候选者。正如你将在第15章*利用大数据*中了解到的那样，一个巧妙设计的神经网络可以从非结构化数据，如图像、文本或音频中，构建自己的特征。同样，它被指定为通用函数逼近器意味着它可以识别出最佳函数形式来模拟数据中识别出的任何模式。因此，我们必须再次回顾早期的问题，即这种做法如何不违反“没有免费的午餐”的原则。似乎对于足够大的数据集，深度学习神经网络是唯一最佳的方法。
- en: 'Putting a couple of practical issues aside—notably, the fact that most real-world
    projects reside in the small data regime and the fact that deep neural networks
    are computationally expensive and difficult to train—a key reason that deep learning
    doesn’t violate the “no free lunch” principle is based on the fact that once the
    neural network becomes large and substantially overparameterized, and assuming
    it has access to a sufficiently large and complex training dataset, it ceases
    to be a single learning *algorithm* and instead becomes a generalized learning
    *process*. If this seems like a distinction without a difference, perhaps a metaphor
    will help: rather than providing us with a free lunch, the process of deep learning
    provides an opportunity to teach the algorithm how to make its own lunch. Given
    the limited availability of truly big data and the limited applicability of deep
    learning to most business tasks, to produce the strongest models, it is still
    necessary for the machine learning practitioner to assist in the feature engineering
    process.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将一些实际问题暂且放在一边——特别是，大多数现实世界项目位于小数据领域的事实，以及深度神经网络计算成本高且难以训练的事实——深度学习不违反“没有免费的午餐”原则的一个关键原因是，一旦神经网络变得很大并且过度参数化，并且假设它能够访问足够大且复杂的训练数据集，它就不再是一个单一的学习**算法**，而变成了一个通用的学习**过程**。如果这听起来像是一个没有区别的区别，也许一个比喻会有所帮助：深度学习的过程不是提供免费的午餐，而是为算法提供如何自己制作午餐的机会。鉴于真正大数据的有限可用性和深度学习在大多数商业任务中的有限适用性，为了产生最强大的模型，机器学习从业者仍然需要在特征工程过程中提供帮助。
- en: Feature engineering in practice
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的特征工程
- en: Depending on the project or circumstances, the practice of feature engineering
    may look very different. Some large, technology-focused companies employ one or
    more data engineers per data scientist, which allows machine learning practitioners
    to focus less on data preparation and more on model building and iteration. Certain
    projects may rely on very small or very massive quantities of data, which may
    preclude or necessitate the use of deep learning methods or automated feature
    engineering techniques. Even projects requiring little initial feature engineering
    effort may suffer from the so-called “last mile problem,” which describes the
    tendency for costs and complexity to be disproportionally high for the small distances
    to be traveled for the “last mile” of distribution. Relating this concept to feature
    engineering implies that even if most of the work is taken care of by other teams
    or automation, a surprising amount of effort may still be required for the final
    steps of preparing the data for the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据项目或情况的不同，特征工程的实际做法可能会有很大差异。一些大型、技术导向的公司为每位数据科学家配备一名或多名数据工程师，这使得机器学习从业者可以减少对数据准备的关注，更多地专注于模型构建和迭代。某些项目可能依赖于非常小或非常大的数据量，这可能会排除或需要使用深度学习方法或自动特征工程技术。即使是那些初始特征工程工作量较小的项目，也可能遭受所谓的“最后一公里问题”，这描述了在分销的“最后一公里”中，成本和复杂度与需要走过的短距离不成比例地高。将这一概念与特征工程联系起来意味着，即使大部分工作由其他团队或自动化完成，准备模型数据最终步骤所需的努力可能仍然相当可观。
- en: It is likely that the bulk of real-world machine learning projects today require
    a substantial amount of feature engineering. Most companies have yet to achieve
    the level of analytics maturity at the organizational level needed to allow data
    scientists to focus solely on model building. Many companies and projects will
    never achieve this level due to their small size or limited scope. For many small-to-mid-sized
    companies and small-to-mid-sized projects, data scientists must take the lead
    on all aspects of the project from start to finish. Consequently, it is necessary
    for data scientists to understand the role of the feature engineer and prepare
    to perform this role if needed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，今天的大多数现实世界机器学习项目都需要大量的特征工程。大多数公司尚未达到组织层面的分析成熟度，这允许数据科学家专注于模型构建。许多公司和项目可能永远不会达到这一水平，因为它们的规模较小或范围有限。对于许多中小型公司和中小型项目，数据科学家必须从项目的开始到结束负责所有方面。因此，数据科学家有必要了解特征工程师的角色，并准备在需要时执行这一角色。
- en: 'As stated previously, feature engineering is more art than science and requires
    as much imagination as it does programming skills. In a nutshell, the three main
    goals of feature engineering might be described as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，特征工程更多的是艺术而非科学，它需要的想象力与编程技能一样多。简而言之，特征工程的主要目标可能可以描述为：
- en: Supplementing what data is already available with additional external sources
    of information
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补充现有数据，利用额外的外部信息来源
- en: Transforming the data to conform to the machine learning algorithm’s requirements
    and to assist the model with its learning
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为符合机器学习算法的要求，并协助模型进行学习
- en: Eliminating the noise while minimizing the loss of useful information— conversely,
    maximizing the use of available information
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最小化有用信息损失的同时消除噪声——相反，最大化利用可用信息
- en: An overall mantra to keep in mind when practicing feature engineering is “be
    clever.” One should strive to be a clever, frugal data miner and try to think
    about the subtle insights that you might find in every single feature, working
    systematically, and avoiding letting any data go to waste. Applying this rule
    serves as a reminder of the requisite creativity and helps to inspire the competitive
    spirit needed to build the strongest-performing learners.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践特征工程时，要记住的一个总体格言是“要聪明”。一个人应该努力成为一个聪明、节俭的数据挖掘者，并尝试思考你可能在每个特征中找到的微妙见解，系统地工作，避免让任何数据浪费。应用这一规则既是创造力的提醒，也有助于激发构建最强性能学习者的竞争精神。
- en: Although each project will require you to apply these skills in a unique way,
    experience will reveal certain patterns that emerge in many types of projects.
    The sections that follow, which provide seven “hints” for the art of feature engineering,
    are not intended to be exhaustive but, rather, provide a spark of inspiration
    on how to think creatively about making data more useful.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个项目都需要你以独特的方式应用这些技能，但经验将揭示在许多类型的项目中出现的某些模式。以下各节，提供了七个关于特征工程艺术的“提示”，并非旨在详尽无遗，而是提供一些灵感，以激发如何创造性地思考使数据更有用。
- en: 'There has been an unfortunate dearth of feature engineering books on the market,
    until recently, when a number have been published. Two of the earliest books on
    this subject are Packt Publishing’s *Feature Engineering Made Easy* (Ozdemir &
    Susara, 2018) and O’Reilly’s *Feature Engineering for Machine Learning* (Zheng
    & Casari, 2018). The book *Feature Engineering and Selection* (Kuhn & Johnson,
    2019) is also a standout and even has a free version, available on the web at:
    [http://www.feat.engineering](http://www.feat.engineering).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上关于特征工程的书籍一直很少，直到最近，才出版了一些。关于这个主题的两本早期书籍是Packt Publishing的《Feature Engineering
    Made Easy》（Ozdemir & Susara，2018）和O’Reilly的《Feature Engineering for Machine Learning》（Zheng
    & Casari，2018）。《Feature Engineering and Selection》（Kuhn & Johnson，2019）也是一本杰出的书籍，甚至还有一个免费版本，可在网上找到：[http://www.feat.engineering](http://www.feat.engineering)。
- en: 'Hint 1: Brainstorm new features'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示1：头脑风暴新特征
- en: The choice of topic for a new machine learning project is typically motivated
    by an unfulfilled need. It may be motivated by a desire for more profit, to save
    lives, or even simple curiosity, but in any case, the topic is almost surely not
    selected at random. Instead, it relates to an issue at the core of the company,
    or a topic held dear by the curious, both of which suggest a fundamental interest
    in the work. The company or individual pursuing the project is likely to already
    know a great deal about the subject and the important factors that contribute
    to the outcome of interest. With this domain experience and subject-matter expertise,
    the company, team, or individual that commissioned the project is likely to hold
    proprietary insights about the task that they alone can bring.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 新机器学习项目主题的选择通常是由未满足的需求驱动的。它可能是由追求更多利润、拯救生命或简单的好奇心驱动的，但无论如何，主题几乎肯定不是随机选择的。相反，它与公司核心问题或好奇者珍视的主题相关，这都表明对这项工作的根本兴趣。追求这个项目的公司或个人可能已经对该主题及其对感兴趣结果有重要贡献的因素有深入了解。有了这种领域经验和专业知识，委托项目的公司、团队或个人可能对任务有专有的见解，他们可以独自带来。
- en: To capitalize on these insights, at the beginning of a machine learning project,
    just prior to feature engineering, it can be helpful to conduct a brainstorming
    session in which stakeholders are gathered and ideas are generated about the potential
    factors that are associated with the outcome of interest. During this process,
    it is important to avoid limiting yourself to what is readily available in existing
    datasets. Instead, consider the process of cause-and-effect at a more abstract
    level, imagining the various metaphorical “levers” that can be pulled in order
    to impact the outcome in a positive or negative direction. Be as thorough as possible
    and exhaust all ideas during this session. If you could have literally anything
    you wanted in the model, what would be most useful?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用这些见解，在机器学习项目的开始阶段，在特征工程之前，进行一次头脑风暴会议可能会有所帮助，在这个会议上，利益相关者聚集在一起，就与感兴趣的结果相关的潜在因素提出想法。在这个过程中，重要的是避免将自己限制在现有数据集中容易获得的内容。相反，考虑因果关系的更抽象层次，想象可以拉动以影响结果的正向或负向方向的各个隐喻性“杠杆”。尽可能彻底，并在这次会议中耗尽所有想法。如果你可以在模型中
    literally 任何你想要的东西，什么最有用？
- en: The culmination of a brainstorming session may be a **mind map**, which is a
    method of diagramming ideas around a central topic. Placing the outcome of interest
    at the center of the mind map, the various potential predictors radiate out from
    the central theme, as shown in the following example of a mind mapping session
    designing a model to predict heart disease mortality.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 头脑风暴会议的成果可能是一个**心智图**，这是一种围绕中心主题绘制想法的方法。将感兴趣的结果置于心智图的中心，各种潜在预测因子从中心主题辐射出来，如下面一个心智图会议设计预测心脏病死亡率的示例所示。
- en: 'A mind map diagram may use a hierarchy to link associated concepts or group
    factors that are related in a similar data source:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 心智图图可能使用层次结构来链接相关的概念或分组在类似数据源中相关的因素：
- en: '![Diagram  Description automatically generated](img/B17290_12_06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_12_06.png)'
- en: 'Figure 12.6: Mind maps can be useful to help imagine the factors that contribute
    to an outcome'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：心智图有助于想象导致结果的各个因素
- en: While constructing the mind map, you may determine that some of the desired
    features are unavailable in the existing data sources. Perhaps the brainstorming
    group can help identify alternative sources of these data elements or find someone
    willing to help gather them. Alternatively, it may be possible to develop a **proxy
    measure** that effectively measures the same concept using a different method.
    For example, it may be impossible or practically infeasible to directly measure
    someone’s diet, but it may be possible to use their social media activity as a
    proxy by counting the number of fast-food restaurants they follow. This is not
    perfect, but it is something, and is certainly better than nothing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建心智图的过程中，你可能会发现一些期望的特征在现有数据源中不可用。也许头脑风暴小组可以帮助识别这些数据元素的替代来源，或者找到愿意帮助收集它们的人。或者，可能有可能开发一个**代理度量**，通过不同的方法有效地衡量相同的概念。例如，可能无法直接测量某人的饮食，但可能可以通过计算他们关注的快餐店数量来使用他们的社交媒体活动作为代理。这并不完美，但总比没有好。
- en: A mind mapping session can also help reveal potential interactions between features
    in which two or more factors have a disproportionate impact on the outcome; a
    joint effect may be greater (or lesser) than the sum of its parts. In the heart
    disease example, one might hypothesize that the combined effect of stress and
    obesity is substantially more likely to cause heart disease than the sum of their
    separate effects. Algorithms such as decision trees and neural networks can find
    these interaction effects automatically, but many others cannot, and in either
    case, it may benefit the learning process or result in a simpler, more interpretable
    model if these combinations are coded explicitly in the data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 心智图会议还可以帮助揭示特征之间的潜在相互作用，在这些相互作用中，两个或更多因素对结果的影响不成比例；整体效应可能大于（或小于）其各部分之和。在心脏病例中，有人可能会假设压力和肥胖的联合效应比它们各自效应的总和更有可能引起心脏病。决策树和神经网络等算法可以自动找到这些相互作用效应，但许多其他算法不能，而且在任何情况下，如果这些组合在数据中明确编码，可能有助于学习过程，或者导致一个更简单、更可解释的模型。
- en: 'Hint 2: Find insights hidden in text'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示2：在文本中寻找隐藏的见解
- en: One of the richest sources of hidden data and, therefore, one of the most fruitful
    areas for feature engineering is text data. Machine learning algorithms are generally
    not very good at realizing the full value of text data because they lack the external
    knowledge of semantic meaning that a human has gained over a lifetime of language
    use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是隐藏数据最丰富的来源之一，因此也是特征工程最有成效的领域之一。机器学习算法通常不太擅长实现文本数据的全部价值，因为它们缺乏人类在一生语言使用中获得的语义意义的外部知识。
- en: Of course, given a tremendous amount of text data, a computer may be able to
    learn the same thing, but this is not feasible for many projects, and would greatly
    add to a project’s complexity. Furthermore, text data cannot be used as-is, as
    it suffers from the curse of dimensionality; each block of text is unique and,
    therefore, serves as a form of fingerprint linking the text to an outcome. If
    used in the learning process, the algorithm will severely overfit or ignore the
    text data altogether.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，给定大量的文本数据，计算机可能能够学习到相同的东西，但这对于许多项目来说并不可行，并且会极大地增加项目的复杂性。此外，文本数据不能直接使用，因为它受到维度灾难的困扰；每一块文本都是独特的，因此它作为一种指纹将文本与结果联系起来。如果在学习过程中使用，算法可能会严重过拟合或完全忽略文本数据。
- en: The curse of dimensionality applies to unstructured “big” data more generally
    as image and audio data are likewise difficult to use directly in machine learning
    models. *Chapter 15*, *Making Use of Big Data*, covers some methods that allow
    these types of data sources to be used with traditional machine learning approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难普遍适用于非结构化的“大数据”，因为图像和音频数据同样难以直接在机器学习模型中使用。第15章“利用大数据”介绍了允许这些类型的数据源与传统机器学习方法一起使用的一些方法。
- en: The humans in charge of constructing features for the learning algorithm can
    add insight to the text data by coding reduced-dimensionality features derived
    from the interpretation of the text. In selecting a small number of categories,
    the implicit meaning is made explicit. For example, in a customer churn analysis,
    suppose a company has access to the public Twitter timeline for its customers.
    Each customer’s tweets are unique, but a human may be able to code them into three
    categories of positive, negative, and neutral. This is a simple form of **sentiment
    analysis**, which analyzes the emotion of language. Computer software, including
    some R packages, may be able to help automate this process using models or rules
    designed to understand simple semantics. In addition to sentiment analysis, it
    may be possible to categorize text data by topic; in the churn example, perhaps
    customers tweeting about customer service are more likely to switch to another
    company than customers tweeting about price.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 负责为学习算法构建特征的负责人可以通过编码从文本解释中得出的降维特征来为文本数据增加洞察力。在选择少量类别时，隐含的意义被明确化。例如，在客户流失分析中，假设一家公司可以访问其客户的公共Twitter时间线。每位客户的推文都是独特的，但人类可能能够将它们编码为三个类别：正面、负面和中性。这是一种简单的**情感分析**形式，它分析语言的情感。计算机软件，包括一些R包，可能能够通过使用旨在理解简单语义的模型或规则来自动化这一过程。除了情感分析之外，还可能根据主题对文本数据进行分类；在流失的例子中，也许提到客户服务的推文的客户比提到价格的客户更有可能转向另一家公司。
- en: 'There are many R packages that can perform sentiment analysis, some of which
    require subscriptions to paid services. To get started quickly and easily, check
    out the aptly named `SentimentAnalysis` and `RSentiment` packages, as well as
    the `Syuzhet` package. All of these can classify sentences as positive or negative
    with just a couple of lines of R code. For a deeper dive into text mining and
    sentiment analysis, see the book *Text Mining with R: A Tidy Approach, 2017, Silge
    J and Robinson D*, which is available on the web at [https://www.tidytextmining.com](https://www.tidytextmining.com).
    Additionally, see *Text Mining in Practice with R, 2017, Kwartler T*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多R包可以进行情感分析，其中一些需要订阅付费服务。为了快速轻松地开始，请查看名为`SentimentAnalysis`和`RSentiment`的包，以及`Syuzhet`包。所有这些都可以用几行R代码将句子分类为正面或负面。对于更深入的文本挖掘和情感分析，请参阅2017年出版的书籍《使用R进行文本挖掘：整洁方法，Silge
    J和Robinson D》，可在网上找到[https://www.tidytextmining.com](https://www.tidytextmining.com)。此外，还可以参考2017年出版的《使用R进行实践文本挖掘，Kwartler
    T》。
- en: Beyond coding the overt meaning of text, one of the subtle arts of feature engineering
    involves finding the covert insights hidden in the text data. In particular, there
    may be useful information encoded in the text that is not related to the direct
    interpretation of the text, but it appears in the text coincidentally or accidentally,
    like a “tell” in the game of poker—a micro-expression that reveals the player’s
    secret intentions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了编码文本的明显意义之外，特征工程中的一项微妙艺术在于寻找隐藏在文本数据中的隐蔽洞察。特别是，文本中可能包含与文本的直接解释无关的有用信息，但它可能偶然或意外地出现在文本中，就像扑克游戏中的“提示”——一种揭示玩家秘密意图的微小表情。
- en: 'Hidden text data may help reveal aspects of a person’s identity, such as age,
    gender, career level, location, wealth, or socioeconomic status. Some examples
    include:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的文本数据可能有助于揭示一个人的身份特征，如年龄、性别、职业级别、位置、财富或社会经济地位。以下是一些例子：
- en: Names and salutations such as Mr. and Mrs., or Jr. and Sr., traditional and
    modern names, male and female names, or names associated with wealth
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称和称呼，如先生和夫人，或Jr.和Sr.，传统和现代名称，男性或女性名称，或与财富相关的名称
- en: Job titles and categories such as CEO, president, assistant, senior, or director
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 职位名称和类别，如CEO、总裁、助理、高级或总监
- en: Geographic and spatial codes such as postal codes, building floor numbers, foreign
    and domestic regions, first-class tickets, PO boxes, and similar
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理和空间代码，如邮政编码、建筑楼层号、国内外地区、一等舱票、邮政信箱等
- en: Linguistic markers such as slang or other expressions that may reveal pertinent
    aspects of identities
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言标记，如俚语或其他可能揭示身份相关方面的表达
- en: To begin searching for these types of hidden insights, keep the outcome of interest
    in mind while systematically reviewing the text data. Read as many of the texts
    as possible while thinking about any way in which the text might reveal a subtle
    clue that could impact the outcome. When a pattern emerges, construct a feature
    based on the insight. For instance, if the text data commonly includes job titles,
    create rules to classify the jobs into career levels such as entry-level, mid-career,
    and executive. These career levels could then be used to predict outcomes such
    as loan default or churn likelihood.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始寻找这些类型的隐藏洞察，在系统地审查文本数据时，要牢记感兴趣的结果。尽可能多地阅读文本，同时思考文本可能以任何方式揭示可能影响结果微妙线索的方法。当出现模式时，基于洞察构建一个特征。例如，如果文本数据通常包括职位名称，可以创建规则将工作分类为职业级别，如初级、中级和高级。然后可以使用这些职业级别来预测结果，如贷款违约或流失可能性。
- en: 'Hint 3: Transform numeric ranges'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示3：转换数值范围
- en: Certain learning algorithms are more capable than others of learning from numeric
    data. Among algorithms that can utilize numeric data at all, some are better at
    learning the important cut points in the range of numeric values or are better
    at handling severely skewed data. Even a method like decision trees, which is
    certainly apt at using numeric features, has a tendency to overfit on numeric
    data and, thus, may benefit from a transformation that reduces the numeric range
    into a smaller number of potential cut points. Other methods like regression and
    neural networks may benefit from nonlinear transformations of numeric data, such
    as log scaling, normalization, and step functions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 某些学习算法比其他算法更有能力从数值数据中学习。在所有可以利用数值数据的算法中，一些算法在学习数值值范围内的重要切割点方面表现更好，或者更擅长处理严重倾斜的数据。即使是像决策树这样的方法，虽然确实擅长使用数值特征，但也倾向于在数值数据上过度拟合，因此可能从将数值范围减少到更少的潜在切割点的转换中受益。其他方法，如回归和神经网络，可能从数值数据的非线性转换中受益，例如对数缩放、归一化和阶梯函数。
- en: Many of these methods have been covered and applied in prior chapters. For example,
    in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*, we
    considered the technique of discretization (also known as “binning” or “bucketing”)
    as a means of transforming numeric data into categorical data so that it could
    be used by the naive Bayes algorithm. This technique is also sometimes useful
    for learners that can handle numeric data natively, as it can help clarify a decision
    boundary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些方法已在之前的章节中介绍并应用。例如，在*第4章*，*概率学习 – 使用朴素贝叶斯进行分类*中，我们考虑了离散化（也称为“分箱”或“桶化”）技术，作为将数值数据转换为分类数据的一种手段，以便朴素贝叶斯算法可以使用。这种技术对于可以原生处理数值数据的学习者来说有时也很有用，因为它可以帮助明确决策边界。
- en: The following figure illustrates this process for a hypothetical model predicting
    heart disease, using a numeric age predictor. On the left, we see that as the
    numeric age increases, the darker the color becomes, indicating a greater prevalence
    of heart disease with increasing age. Despite this seemingly clear trend, a decision
    tree model may struggle to identify an appropriate cut point and it may do so
    arbitrarily, or it may choose numerous small cut points; both of these are likely
    to be overfitted to the training data. Instead of leaving this choice to the model,
    it may be better to use *a priori* knowledge to create predefined groups for “young”
    and “old” patients. Although this loses some of the nuances of the true underlying
    gradient, it may help the model generalize better to future data by trading the
    decision tree’s “high variance” approach for a “high bias” approach of theory-driven
    discretization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了使用数值年龄预测器预测心脏病的一个假设模型的过程。在左侧，我们看到随着数值年龄的增加，颜色变得越来越深，这表明随着年龄的增长，心脏病的发病率也在增加。尽管这个趋势看似明显，但决策树模型可能难以确定合适的分割点，它可能随意分割，或者选择多个小的分割点；这两种情况都可能导致对训练数据的过度拟合。与其让模型自行选择，不如使用**先验**知识为“年轻”和“老年”患者创建预定义的群体。虽然这会失去一些真实潜在梯度的细微差别，但它可能通过将决策树的“高方差”方法替换为理论驱动的离散化“高偏差”方法，帮助模型更好地泛化到未来的数据。
- en: '![](img/B17290_12_07.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_12_07.png)'
- en: 'Figure 12.7: Discretization and other numeric transformations can help learners
    identify patterns more easily'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：离散化和其他数值变换可以帮助学习者更容易地识别模式
- en: In general, for datasets containing numeric features, it may be worth exploring
    each feature systematically, while also considering the learning algorithm’s approach
    to numeric data, to determine whether a transformation is necessary. Apply any
    domain or subject-matter expertise you may have to inform the creation of bins,
    buckets, step points, or nonlinear transformations in the final version of the
    feature. Even though many algorithms are capable of handling numeric data without
    recoding or transformation, additional human intelligence may help guide the model
    to a better overall fit.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于包含数值特征的数据集，系统地探索每个特征可能值得考虑，同时也要考虑学习算法对数值数据的方法，以确定是否需要进行转换。将您可能拥有的任何领域或主题专业知识应用于创建最终特征版本中的桶、桶、步长点或非线性变换。尽管许多算法能够处理数值数据而无需重新编码或转换，但额外的人类智慧可能有助于引导模型达到更好的整体拟合。
- en: 'Hint 4: Observe neighbors’ behavior'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示4：观察邻居的行为
- en: One of the lesser-known methods of surfacing hidden insights during feature
    engineering is to apply the common knowledge that “birds of a feather flock together.”
    We applied this principle to prediction in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*, but it is also a useful mindset for identifying useful
    predictors. The idea hinges on the fact that there may be explicit or implicit
    groupings across the dataset’s rows, and there may be insights found by examining
    how one example relates to the others in its neighborhood or grouping.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程过程中揭示隐藏洞察力的一种不太为人所知的方法是应用常识“物以类聚”。我们在**第3章**中应用了这一原则，即使用最近邻进行分类的**懒惰学习**，但这也是识别有用预测因子的一种有用心态。这个想法基于这样一个事实，即数据集中可能存在显式或隐式的分组，通过检查一个示例与其邻近或分组中的其他示例的关系，可能会发现一些洞察力。
- en: An example of an explicit grouping found often in real-world data is households.
    Many datasets include not only rows based on individuals but also a household
    identifier, which allows you to link rows into household groups and, thus, create
    new features based on the groups’ compositions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中数据中经常发现的显式分组的一个例子是家庭。许多数据集不仅包括基于个人的行，还包括家庭标识符，这允许您将行链接到家庭组中，从而根据组的组成创建新的特征。
- en: For instance, knowing that someone is in a household may provide an indication
    of marital status and the number of children or dependents, even if these features
    were not included in the original individual-level dataset. Simply counting or
    aggregating some of the group’s features can result in highly useful predictors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，知道某人在一个家庭中可能表明婚姻状况以及子女或依赖人数，即使这些特征没有包含在原始的个人层级数据集中。简单地计数或聚合一些群体的特征可以产生非常有用的预测因子。
- en: From here, it is also possible to share information among records in the groups.
    For instance, knowing one spouse’s income is helpful, but knowing both provides
    a better indication of the total available income. Measures of variance within
    a group can also be enlightening. There may be aspects of households that provide
    a bonus effect if the partners match or disagree on certain attributes; for example,
    if both partners report satisfaction with a particular telephone company, they
    may be especially loyal compared to households where only one member is satisfied.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，也可以在组内的记录之间共享信息。例如，知道一位配偶的收入是有帮助的，但知道两位配偶可以更好地表明可用的总收入。组内差异的度量也可能是有启发性的。如果伴侣在某些属性上匹配或不同意，家庭的一些方面可能会提供额外的效果；例如，如果两位伴侣都表示对某家电话公司满意，他们可能比只有一位成员满意的户主更加忠诚。
- en: These principles also apply to less obvious but still explicit groupings, like
    postal codes or geographic regions. By collecting the rows falling into the group,
    one can count, sum, average, take the maximum or minimum value, or examine the
    diversity within the group to construct new and potentially useful predictors.
    Groups with more or less agreement or diversity may be more or less robust to
    certain outcomes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则也适用于不那么明显但仍很明确的分组，例如邮政编码或地理区域。通过收集属于该组的行，可以计数、求和、计算平均值、取最大值或最小值，或者检查组内的多样性以构建新的、可能有用的预测因子。在更多或更少的共识或多样性方面的组可能对某些结果更稳健或更不稳健。
- en: There may be value in identifying implicit groupings as well—that is, a grouping
    not directly coded in the dataset. Clustering methods, such as those described
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, are one potential
    method of finding these types of groupings, and the resulting clusters can be
    used directly as a predictor in the model. For example, in a churn project, using
    clusters as features for the model may reveal that some clusters are more likely
    to churn than others. This may imply that churn is related to the cluster’s underlying
    demographics, or that churn is somewhat contagious among cluster members.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 识别隐含的分组也可能有价值——也就是说，这种分组在数据集中没有直接编码。例如，在第 9 章 *寻找数据组 - 使用 k-means 进行聚类* 中描述的聚类方法是一种潜在的寻找这些类型分组的方法，并且这些结果簇可以直接用作模型中的预测因子。例如，在一个客户流失项目中，使用簇作为模型的特征可能会揭示某些簇比其他簇更有可能流失。这可能意味着流失与簇的潜在人口统计特征有关，或者流失在簇成员之间有一定的传染性。
- en: In other words, if birds of a feather flock together, it makes sense to borrow
    leading indicators from the experiences of similar neighbors—they may have a similar
    reaction to some external factor or may directly influence one another. Implicit
    groups that exhibit rare or unique traits may be interesting in themselves; perhaps
    some are the bellwether or “canary in the coal mine”—trendsetters that respond
    to change earlier than other groups. Observing their behavior and coding these
    groups explicitly into the model may improve the model’s predictive ability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果物以类聚，那么从类似邻居的经验中借用领先指标是有意义的——他们可能对某些外部因素有类似的反应，或者可能直接相互影响。表现出罕见或独特特征的隐含组本身可能很有趣；也许其中一些是趋势领导者或“煤矿中的金丝雀”——比其他群体更早对变化做出反应的趋势领导者。观察他们的行为并将这些群体明确编码到模型中可能会提高模型的预测能力。
- en: If you do use information from neighbors (or from related rows, as described
    in the next section), beware of the problem of data leakage, which was described
    in *Chapter 11*, *Being Successful with Machine Learning*. Be sure to only engineer
    features using information that will be available at the time of prediction when
    the model is deployed. For example, it would be unwise to use both spouses’ data
    for a credit scoring model if only one household member completes the loan application
    and the other spouse’s data is added after the loan is approved.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实使用了邻居（或下一节中描述的相关行）的信息，请注意数据泄露的问题，这在第 11 章 *用机器学习取得成功* 中已有描述。确保只使用在模型部署时将可用的信息来构建特征。例如，如果只有一位家庭成员完成贷款申请，而另一位配偶的数据是在贷款批准后添加的，那么在信用评分模型中使用两位配偶的数据是不明智的。
- en: 'Hint 5: Utilize related rows'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示 5：利用相关行
- en: The practice of utilizing “follow the leader” behavior as hinted in the previous
    section can be especially powerful given the related rows of time series datasets,
    where the same attribute is measured repeatedly at different points in time. Data
    that contains repeated measures offers many such additional opportunities to construct
    useful predictors. Whereas the previous section considered grouping related data
    *across* the unit of analysis, the current section considers the value of grouping
    related observations *within* the unit of analysis. Essentially, by observing
    the same units of analysis repeatedly, we can examine their prior trends and make
    better predictions of the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中提到的“跟随领导者”行为的实践，考虑到时间序列数据集中相关行，在时间不同点重复测量同一属性，可以特别强大。包含重复测量的数据提供了许多构建有用预测因子的额外机会。而前一部分考虑了在分析单位“跨”分组相关数据，当前部分考虑了在分析单位“内”分组相关观察的价值。本质上，通过重复观察相同的分析单位，我们可以检查它们的历史趋势并更好地预测未来。
- en: 'Revisiting the hypothetical churn example, suppose we have access to the past
    24 months of data from subscribers to an online video streaming service. The unit
    of observation is the customer-month (one row per customer per month), while our
    unit of analysis is the customer. Our goal is to predict which customers are most
    likely to churn so that we might intervene. To construct a dataset for machine
    learning, we must collect the units of observation and aggregate them into one
    row per customer. Here is where feature engineering is especially needed. In the
    process of “rolling up” the historical data into a single row for analysis, we
    can construct features that examine trends and loyalty, asking questions such
    as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾假设的流失率示例，假设我们能够访问过去24个月来自在线视频流媒体服务订阅者的数据。观察的单位是客户-月（每位客户每月一行），而我们的分析单位是客户。我们的目标是预测哪些客户最有可能流失，以便我们可能进行干预。为了构建机器学习的数据集，我们必须收集观察单位并将它们聚合为每位客户一行。这正是特征工程特别需要的地方。在将历史数据“汇总”为分析的单行过程中，我们可以构建检查趋势和忠诚度的特征，提出如下问题：
- en: Is the customer’s average monthly activity greater than or less than their peers?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的平均月度活动是否高于或低于他们的同龄人？
- en: What is the customer’s monthly activity over time? Is it up, down, or stable?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户随时间推移的月度活动是什么？它是上升、下降还是稳定的？
- en: How frequent is their activity? Are they loyal? Is their loyalty stable across
    months?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的活动频率如何？他们是忠诚的吗？他们的忠诚度在月份间是否稳定？
- en: How consistent is the customer’s behavior? Does the behavior vary a lot from
    month to month?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的行为一致性如何？行为是否每月都有很大变化？
- en: If you are familiar with basic calculus, it may help to reflect on the concept
    of the first and second derivative, as both can be useful features in a time series
    model. The first derivative here refers to the velocity of the behavior—that is,
    the behavior count over a unit of time. For example, we may compute the number
    of dollars spent per month on the streaming service, or the number of television
    shows and movies streamed per month. These are useful predictors alone, but they
    can be made even more useful in the context of the second derivative, which is
    the acceleration (or deceleration) of the behavior. The acceleration is the change
    in velocity over time, such as the change in monthly spending or the change in
    the shows streamed per month. High-velocity customers with high spending and usage
    might be less likely to churn, but a rapid deceleration (that is, a large reduction
    in usage or spending) from these same customers might indicate an impending churn.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉基本的微积分，反思一阶和二阶导数的概念可能会有所帮助，因为它们都可以作为时间序列模型中的有用特征。这里的一阶导数指的是行为的速度——即单位时间内的行为计数。例如，我们可能计算每月在流媒体服务上花费的美元数，或者每月流过的电视节目和电影的数量。这些单独就是有用的预测因子，但在二阶导数的背景下，它们可以变得更加有用。二阶导数是行为的加速度（或减速度），即速度随时间的变化，如每月支出的变化或每月流过的节目数量的变化。高消费和高使用率的客户可能不太可能流失，但来自这些客户的快速减速度（即使用或支出的大幅减少）可能表明即将流失。
- en: In addition to velocity and acceleration, measures of consistency, reliability,
    and variability can be constructed to further enhance predictive ability. A very
    consistent behavior that suddenly changes may be more concerning than a wildly
    varying behavior that changes similarly. Calculating the proportion of recent
    months with a purchase, or with spending or behavior meeting a given threshold,
    provides a simple loyalty metric, but more sophisticated measures using variance
    are also possible.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了速度和加速度之外，还可以构建一致性、可靠性和变异性的度量，以进一步增强预测能力。一种突然改变的行为可能比类似变化但波动剧烈的行为更令人担忧。计算最近几个月有购买、支出或行为达到给定阈值的比例，可以提供一个简单的忠诚度指标，但使用方差等更复杂的度量也是可能的。
- en: 'Hint 6: Decompose time series'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示6：分解时间序列
- en: 'The repeated measures time series data described in the previous section, with
    multiple related rows per unit of analysis, is said to be in the **long format**.
    This contrasts with the type of data required for most R-based machine learning
    methods. Unless a learning algorithm is designed to understand the related rows
    of repeated measures data, it will require time series data to be specified in
    the **wide format**, which transposes the repeated rows of data into repeated
    columns. For example, if a weight measurement is recorded monthly for 3 months
    for 1,000 patients, the long-format dataset will have 3 * 1,000 = 3,000 rows and
    3 columns (patient identifier, month, and weight). As depicted in *Figure 12.8*,
    the same dataset in wide format would contain only 1,000 rows but 4 columns: 1
    column for the patient identifier, and 3 columns for the monthly weight readings:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上文所述的重复测量时间序列数据，每个分析单位有多个相关行，被称为**长格式**。这与大多数基于R的机器学习方法所需的数据类型形成对比。除非学习算法被设计成能够理解重复测量数据的关联行，否则它将需要以**宽格式**指定时间序列数据，即将数据的重复行转换为重复的列。例如，如果一个体重测量值每月记录3个月，针对1,000名患者，长格式数据集将包含3
    * 1,000 = 3,000行和3列（患者标识符、月份和体重）。如图12.8所示，同样的数据集在宽格式下将只有1,000行但4列：1列用于患者标识符，3列用于每月的体重读数：
- en: '![Table  Description automatically generated](img/B17290_12_08.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![表描述自动生成](img/B17290_12_08.png)'
- en: 'Figure 12.8: Most machine learning models require long-format time series data
    to be transformed into a wide format'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：大多数机器学习模型都需要将长格式时间序列数据转换为宽格式
- en: To construct a wide format dataset, one must first determine how much history
    will be useful for prediction. The more history that is needed, the more columns
    that will need to be added to the wide dataset. For example, if we wanted to forecast
    a customer’s energy usage 1 month into the future, we may decide to use their
    prior 12 months of energy use as predictors so that a full year of seasonality
    would be covered. Therefore, to build a model forecasting energy usage in June
    2023, we might create 12 predictor columns measuring energy use in May 2023, April
    2023, March 2023, and so on, for each of the 12 months prior to June 2023\. A
    13th column would be the target or dependent variable, recording the actual energy
    usage in June 2023\. Note that a model trained upon this dataset would learn to
    predict energy use in June 2023 based on data in the months from June 2022 to
    May 2023, but it would not be able to predict other future months because the
    target and predictors are linked to specific months.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个宽格式数据集，首先必须确定多少历史数据对预测是有用的。所需的历史数据越多，就需要在宽数据集中添加更多的列。例如，如果我们想要预测客户未来一个月的能源使用情况，我们可能会决定使用他们前12个月的能源使用数据作为预测因子，以便覆盖整个一年的季节性变化。因此，为了构建预测2023年6月能源使用的模型，我们可能会创建12个预测因子列，分别测量2023年5月、4月、3月的能源使用情况，以及6月之前每个月的能源使用情况。第13列将是目标或因变量，记录2023年6月的实际能源使用情况。请注意，基于此数据集训练的模型将学会根据2022年6月至2023年5月的数据来预测2023年6月的能源使用情况，但它无法预测其他未来的月份，因为目标和预测因子与特定的月份相关联。
- en: Instead, a better approach is to construct **lagged variables**, which are computed
    relative to the target month. The lagged variables are essentially measures that
    are delayed in time to be carried forward to a later, more recent row in the dataset.
    A model using lagged variables can be retrained on a rolling, monthly basis as
    additional months of data become available over time. Rather than having column
    names like `energy_june2023` and `energy_may2023`, the resulting dataset will
    have names that indicate the relative nature of the measurements, such as `energy_lag0`,
    `energy_lag1`, and `energy_lag2`, which indicate the energy use in the current
    month, the prior month, and 2 months ago. This model will always be applied to
    the most recent data to predict the forthcoming time period.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，更好的方法是构建**滞后变量**，这些变量相对于目标月份进行计算。滞后变量基本上是延迟一段时间以传递到数据集中更晚、更近的行的度量。使用滞后变量的模型可以在有更多月份数据可用时按滚动、每月重新训练。而不是有像`energy_june2023`和`energy_may2023`这样的列名，生成的数据集将具有表示测量相对性质的名称，如`energy_lag0`、`energy_lag1`和`energy_lag2`，这表示当前月份、上个月和两个月前的能源消耗。此模型将始终应用于最新数据以预测即将到来的时间段。
- en: '*Figure 12.9* visualizes this approach. Each month, a model is trained on the
    past 13 months of data; the most recent month is used for the target or dependent
    variable (denoted as DV) while the earlier 12 months are used as lagged predictors.
    The model can then be used to predict the future month, which has not yet been
    observed. Each successive month following the first shifts the rolling window
    1 month forward, such that data older than 13 months is unused in the model. A
    model trained using data constructed in this way does not learn the relationship
    between specific calendar months, as was the case with the non-lagged variables;
    rather, it learns how prior behavior relates to future behavior, regardless of
    the calendar month.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.9* 展示了这种方法。每个月，模型都会在过去的13个月数据上训练；最近的一个月被用作目标或因变量（表示为DV），而早先的12个月则用作滞后预测变量。然后，模型可以用来预测尚未观察到的未来月份。第一个月之后的每个月都会将滚动窗口向前移动1个月，这样超过13个月的数据就不会在模型中使用。使用这种方式构建的数据训练的模型不会学习特定日历月份之间的关系，正如非滞后变量那样；相反，它学习的是先前行为与未来行为之间的关系，而不考虑日历月份。'
- en: '![Timeline  Description automatically generated](img/B17290_12_09.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![时间线 描述自动生成](img/B17290_12_09.png)'
- en: 'Figure 12.9: Constructing lagged predictors is one method to model time series
    data'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：构建滞后预测变量是建模时间序列数据的一种方法
- en: A problem with this approach, however, is that this method has disregarded calendar
    time, yet certain calendar months may have an important impact on the target variable.
    For example, energy use may be higher in winter and summer than in spring and
    fall, and thus, it would be beneficial for the model to know not only the relationship
    between past and future behavior but also to gain a sense of seasonal effects,
    or other patterns broader than the local patterns, within the rows related to
    the unit of analysis.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的一个问题是，它忽略了日历时间，而某些日历月份可能对目标变量有重要影响。例如，能源消耗在冬季和夏季可能比春季和秋季高，因此，模型不仅要知道过去和未来的行为之间的关系，还要了解季节效应或其他比局部模式更广泛的模式，这些模式与分析单位相关。
- en: 'One might imagine that the value of the target to be predicted is composed
    of three sources of variation, which we would like to decompose into features
    for the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象，要预测的目标值由三个来源的变异性组成，我们希望将其分解为模型的特征：
- en: There is the local or internal variation, which is based on the attributes unique
    to the unit of analysis. In the example of forecasting energy demand, the local
    variation may be related to the size and construction of the household, the residents’
    energy needs, where the house is located, and so on.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有局部或内部变异性，这是基于分析单位的独特属性。在预测能源需求的例子中，局部变异性可能与家庭的大小和结构、居民的能源需求、房屋的位置等因素有关。
- en: There may be broader global trends, such as fuel prices or weather patterns,
    that affect the energy use of most households.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能存在更广泛的全球趋势，如燃料价格或天气模式，这些都会影响大多数家庭的能源消耗。
- en: There may be seasonal effects, independent of the local and global effects,
    that explain changes in the target. This is not limited to the annual weather
    patterns mentioned before, but any cyclical or predictable pattern can be considered
    a seasonal effect.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能存在季节性影响，独立于本地和全球影响，可以解释目标的变化。这不仅仅限于之前提到的年度天气模式，任何周期性或可预测的模式都可以被认为是季节性影响。
- en: 'Some specific examples relevant to the energy forecasting project may include
    higher or lower demand on:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与能源预测项目相关的某些具体例子可能包括对以下方面的需求更高或更低：
- en: Different days of the week, particularly weekdays versus weekends
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一周中的不同日子，尤其是工作日与周末
- en: Religious or government holidays
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宗教或政府假日
- en: Traditional school or business vacation periods
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统学校或商务假期期间
- en: Mass gatherings such as sporting events, concerts, and elections
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像体育赛事、音乐会和选举这样的大规模集会
- en: 'If local, global, and seasonal features can be incorporated into the training
    dataset as predictors, the model can learn their effect on the outcome. The challenge
    thereafter is twofold: subject-matter knowledge or data exploration is required
    to identify the important seasonal factors, and there must be ample training data
    for the target to be observed in each of the included seasons. The latter implies
    that the training data should be composed of more than a single month cross-section
    of time; lacking this, the learning algorithm will obviously be unable to discover
    the relationship between the seasons and the target!'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以将本地、全球和季节性特征作为预测因子纳入训练数据集中，模型就可以学习它们对结果的影响。接下来的挑战有两个方面：需要专业知识或数据探索来识别重要的季节性因素，并且必须有足够的训练数据，以便在每个包含的季节中观察到目标。后者意味着训练数据应该由超过一个月的时间横截面组成；如果没有这一点，学习算法显然将无法发现季节与目标之间的关联！
- en: Though it would seem to follow that we should revert to the original long-format
    data, this is actually not the case. In fact, the wide data with lagged variables
    from each month can be stacked in a single unified dataset with multiple rows
    per unit of analysis. Each row indicates an individual at a particular moment
    in time, with a target variable measuring the outcome at that moment, and a wide
    set of columns that have been constructed as lagged variables for periods of time
    prior to the target. Additional columns can also be added to further widen the
    matrix and decompose the various components of time variance, such as indicators
    for seasons, days of the week, and holidays; these columns will indicate whether
    the given row falls within one of these periods of interest.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来我们应该回归到原始的长格式数据，但实际上并非如此。实际上，包含每月滞后变量的宽数据可以堆叠在一个单一统一的数据集中，每个分析单位有多行。每一行表示在特定时间点的个人，目标变量衡量该时刻的结果，以及一组作为目标之前时间段滞后变量的宽列。还可以添加额外的列来进一步拓宽矩阵并分解时间变化的各个组成部分，例如季节、星期几和假日的指标；这些列将指示给定的行是否属于这些感兴趣的时期之一。
- en: The figure that follows depicts a hypothetical dataset using this approach.
    Each household (denoted by the `household_id` column) can appear repeatedly with
    different values of the target (`energy_use`) and predictors (`season`, `holiday_month`,
    `energy_lag1`, and so on). Note that the lag variables are missing (as indicated
    by the `NA` values) for the first few rows of the dataset, which means that these
    rows cannot be used for training or prediction. The remaining rows, however, can
    be used with any machine learning method capable of numeric prediction, and the
    trained model will readily forecast next month’s energy use given the row of data
    for the current month.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表展示了使用这种方法的一个假设数据集。每个家庭（由`household_id`列表示）可以重复出现，具有不同的目标值（`energy_use`）和预测因子值（`season`、`holiday_month`、`energy_lag1`等）。请注意，数据集的前几行中缺失了滞后变量（如`NA`值所示），这意味着这些行不能用于训练或预测。然而，剩余的行可以使用任何能够进行数值预测的机器学习方法，并且训练好的模型将能够根据当前月份的数据行预测下个月的能源使用。
- en: '![Table  Description automatically generated](img/B17290_12_10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_12_10.png)'
- en: 'Figure 12.10: Datasets including historical data may include both seasonal
    effects and lagged predictors'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：包括历史数据的数据集可能包括季节性效应和滞后预测因子
- en: 'Before rushing into modeling time series data, it is crucial to understand
    an important caveat about the data preparation methods described here: because
    the rows from repeated observations from the same unit of analysis are related
    to one another, including them in the training data violates the assumption of
    independent observations for methods like regression. While models built upon
    such data may still be useful, other methods for formal time series modeling may
    be more appropriate, and it is best to consider the methods described here as
    a workaround to perform forecasting with the machine learning methods previously
    covered. Linear mixed models and recurrent neural networks are two potential approaches
    that can handle this type of data natively, although both methods are outside
    the scope of this book.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在匆忙进行时间序列数据建模之前，理解这里描述的数据准备方法的一个重要注意事项至关重要：由于来自同一分析单元的重复观察的行彼此相关，将它们包含在训练数据中违反了回归等方法的独立观察假设。尽管基于此类数据构建的模型可能仍然有用，但其他正式时间序列建模方法可能更合适，最好将这里描述的方法视为使用之前介绍的机器学习方法的替代方案来进行预测。线性混合模型和循环神经网络是两种可以原生处理此类数据的潜在方法，尽管这两种方法都不在本书的范围之内。
- en: The `lme4` package is used to build mixed models in R, but it would be unwise
    to jump in without understanding the statistical underpinnings of these types
    of models; they are a significant step up in complexity over traditional regression
    modeling. The book *Linear Mixed-Effects Models Using R* (Gałecki & Burzykowski,
    2013) provides the theoretical background needed to build this type of model.
    To build recurrent neural networks, R may not be the right tool for the job, as
    specialized tools exist for this purpose. However, the `rnn` package can build
    simple RNN models for time series forecasting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: R中的`lme4`包用于构建混合模型，但如果不了解这些类型模型的统计基础就贸然进入，那将是不明智的；与传统的回归建模相比，它们在复杂性上有了显著的提升。《使用R进行线性混合效应模型》（Gałecki
    & Burzykowski，2013）一书提供了构建此类模型所需的理论背景。然而，对于构建循环神经网络，R可能不是完成这项工作的最佳工具，因为存在专门为此目的而设计的工具。不过，`rnn`包可以构建简单的循环神经网络模型用于时间序列预测。
- en: 'Hint 7: Append external data'
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示7：附加外部数据
- en: Unlike the teaching examples in this book, when a machine learning project begins
    in the real world, a dataset cannot simply be downloaded from the internet with
    prebuilt features and examples describing the topic of interest. It is unfortunate
    how many deeply interesting projects are killed before they begin for this simple
    reason. Businesses hoping to predict customer churn realize they have no historical
    data from which a model can be built; students hoping to optimize food distribution
    in poverty-stricken areas are limited by the scarce amounts of data from these
    areas; and countless projects that might increase profits or change the world
    for the better are stunted before they start. What begins as excitement around
    a machine learning project soon fizzles out due to the lack of data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的教学示例不同，当机器学习项目在现实世界中开始时，数据集不能简单地从互联网上下载，其中包含预先构建的特征和描述感兴趣主题的示例。遗憾的是，许多极具趣味的项目因为这一简单原因在开始之前就被扼杀。希望预测客户流失的企业意识到他们没有可用于构建模型的历史数据；希望优化贫困地区食品分配的学生受限于这些地区稀缺的数据量；以及无数可能增加利润或改善世界的项目在开始之前就被阻碍。原本围绕机器学习项目的兴奋很快因为数据不足而消散。
- en: Rather than ending with discouragement, it is better to channel this energy
    into an effort to create the necessary data from scratch. This may mean dialing
    colleagues on the telephone or firing off a series of email messages to connect
    with those that can grant access to databases containing relevant pieces of data.
    It may also require rolling up your sleeves and getting your hands dirty. After
    all, we live in the so-called big data era where data is not only plentiful but
    easily recorded, with assistance from electronic sensors and automated data entry
    tools.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不应以失望告终，而应将这种能量转化为从头开始创建必要数据的努力。这可能意味着打电话给同事或发送一系列电子邮件以联系那些可以提供访问包含相关数据片段的数据库的人。这也可能需要你亲自动手，因为毕竟我们生活在这个所谓的“大数据时代”，数据不仅丰富，而且易于记录，得益于电子传感器和自动数据录入工具。
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片 描述由自动生成](img/B17290_12_11.png)'
- en: 'Figure 12.11: Little effort is often sufficient to generate datasets useful
    for machine learning'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：很少的努力就足以生成对机器学习有用的数据集
- en: In the worst case, an investment of time, effort, and imagination can build
    useful datasets from nothing. Typically, this is easier than one might think.
    The previous figure illustrates several cases in which I created datasets to satisfy
    my own curiosity.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，投入时间、精力和想象力可以从无到有地构建有用的数据集。通常，这比人们想象的要容易。前面的图示说明了几个我创建数据集以满足自己好奇心的情况。
- en: Fascinated by autonomous vehicles, I drove around my neighborhood and took pictures
    of road signs to build a stop sign classification algorithm. To predict used car
    prices, I copied and pasted hundreds of listings from used car websites. And,
    to understand exactly when and why names rhyming with “Aiden” became so popular
    in the United States, I gathered dozens of years of data from the Social Security
    baby name database. None of these projects required more than a few hours of effort,
    but enrolling friends, colleagues, or internet forums as a form of crowdsourcing
    the effort or even paying for data entry assistance could have parallelized the
    task and helped my database grow larger or faster. Paid services like Amazon Mechanical
    Turk ([https://www.mturk.com](https://www.mturk.com)) provide an affordable means
    of distributing large and tedious data entry or collection tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对自动驾驶汽车着迷，我在我的社区周围开车并拍摄路标照片，以构建一个停车标志分类算法。为了预测二手车价格，我从二手车网站上复制粘贴了数百个列表。而且，为了确切了解为什么与“Aiden”押韵的名字在美国变得如此流行，我从社会保障婴儿名字数据库中收集了几十年的数据。这些项目中的任何一个都不需要超过几个小时的努力，但通过将朋友、同事或互联网论坛作为众包努力的一种形式，甚至支付数据录入助手，可以并行化任务并帮助我的数据库更大或更快地增长。像Amazon
    Mechanical Turk ([https://www.mturk.com](https://www.mturk.com))这样的付费服务提供了一种经济实惠的方式，用于分配大量且繁琐的数据录入或收集任务。
- en: To further enrich existing datasets, there is often the potential to append
    additional features from external sources. This is especially true when the main
    dataset of interest includes geographic identifiers such as postal codes, as many
    publicly available databases measure attributes for these regions. Of course,
    a postal code-level dataset will not reveal a specific individual’s exact characteristics;
    however, it may provide insight into whether the average person in the area is
    wealthier, healthier, younger, or more likely to have kids, among numerous other
    factors that may help improve the quality of a predictive model. These types of
    data can be readily found on many governmental agency websites and downloaded
    at no charge; simply merge them onto the main dataset for additional possible
    predictors.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步丰富现有的数据集，通常有从外部来源附加额外特征的可能性。这尤其适用于感兴趣的原始数据集包括地理标识符，如邮政编码，因为许多公开可用的数据库测量这些地区的属性。当然，邮政编码级别的数据集不会揭示特定个人的确切特征；然而，它可能提供有关该地区平均个人是否更富有、更健康、更年轻或更有可能拥有孩子的见解，以及其他许多可能有助于提高预测模型质量的因素。这些类型的数据可以在许多政府机构网站上轻松找到，并且免费下载；只需将它们合并到主数据集，以添加可能的预测因子。
- en: Lastly, many social media companies and data aggregator services like Facebook,
    Zillow, and LinkedIn provide free access to limited portions of their data. Zillow,
    for example, provides home value estimates for postal code regions. In some cases,
    these companies or other vendors may sell access to these datasets, which can
    be a powerful means of augmenting a predictive model. In addition to the financial
    cost of such acquisitions, they often pose a significant challenge in terms of
    **record linkage**, which involves matching entities across datasets that share
    no common unique identifier. Solving this problem involves building a **crosswalk**
    table, which maps each row in one source to the corresponding row in the other
    source. For instance, the crosswalk may link a person identified by a customer
    identification number in the main dataset to a unique website URL in an external
    social media dataset. Although there are R packages such as `RecordLinkage` that
    can help perform such matching across sources, these rely on heuristics that may
    not perform as well as human intelligence and require significant computational
    expense, particularly for large databases. In general, it is safe to assume that
    record linkage is often costly from human resource and computational expense perspectives.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多社交媒体公司和数据聚合服务，如 Facebook、Zillow 和 LinkedIn，提供对其数据有限部分的免费访问。例如，Zillow 提供邮政编码地区的房屋价值估算。在某些情况下，这些公司或其他供应商可能会出售对这些数据集的访问权限，这可以是一种强大的增强预测模型的方法。除了这些收购的财务成本外，它们通常在
    **记录链接** 方面也提出了重大挑战，这涉及到在没有任何共同唯一标识符的跨数据集中匹配实体。解决此问题需要构建一个 **映射表**，它将一个来源中的每一行映射到另一个来源中相应的行。例如，映射可能将主数据集中通过客户识别号识别的个人与外部社交媒体数据集中唯一的网站
    URL 链接起来。尽管存在像 `RecordLinkage` 这样的 R 包可以帮助在来源之间执行此类匹配，但这些包依赖于启发式方法，可能不如人类智能表现得好，并且需要大量的计算成本，尤其是在大型数据库中。总的来说，从人力资源和计算成本的角度来看，可以安全地假设记录链接通常成本高昂。
- en: When considering whether to acquire external data, be sure to research the source’s
    terms of use, as well as your region’s laws and organizational rules around using
    such sources. Some jurisdictions are stricter than others, and many rules are
    becoming stricter over time, so it is important to keep up to date on the legality
    and liability associated with outside data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑是否获取外部数据时，务必研究数据源的使用条款，以及您所在地区关于使用此类数据源的法律和组织规则。一些司法管辖区比其他地区更为严格，许多规则随着时间的推移也在变得更加严格，因此了解与外部数据相关的合法性和责任至关重要。
- en: Given the work involved in advanced data preparation, R itself has evolved to
    keep up with the new demands. Historically, R was notorious for struggling with
    very large and complex datasets, but over time, new packages have been developed
    to address these shortcomings and make it easier to perform the types of operations
    described so far in this chapter. In the remainder of this chapter, you will learn
    about these packages, which modernize the R syntax for real-world data challenges.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高级数据准备涉及的工作量，R 本身也在不断进化以跟上新的需求。历史上，R 以处理非常大的和复杂的数据集而闻名，但随着时间的推移，已经开发出新的包来解决这些不足，并使执行本章中迄今为止描述的操作变得更加容易。在本章的剩余部分，您将了解这些包，它们使
    R 语法现代化，以应对现实世界的数据挑战。
- en: Exploring R’s tidyverse
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 R 的 tidyverse
- en: A new approach has rapidly taken shape as the dominant paradigm for working
    with data in R. Championed by Hadley Wickham—the mind behind many of the packages
    that drove much of R’s initial surge in popularity—this new wave is now backed
    by a much larger team at Posit (formerly known as RStudio). The company’s user-friendly
    RStudio Desktop application integrates nicely into this new ecosystem, known as
    the **tidyverse**, because it provides a universe of packages devoted to tidy
    data. The entire suite of tidyverse packages can be installed with the `install.packages("tidyverse")`
    command.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一种新的方法迅速成为在 R 中处理数据的占主导地位的模式。由 Hadley Wickham（许多推动 R 初始流行潮的包背后的思想）倡导，这一新趋势现在得到了
    Posit（原名 RStudio）的一个更大团队的支撑。该公司的用户友好的 RStudio 桌面应用程序很好地整合到了这个新生态系统中，称为 **tidyverse**，因为它提供了一系列致力于整洁数据的包。整个
    tidyverse 包集可以通过 `install.packages("tidyverse")` 命令安装。
- en: A growing number of resources are available online to learn more about the tidyverse,
    starting with its homepage at [https://www.tidyverse.org](https://www.tidyverse.org).
    Here, you can learn about the various packages included in the set, a few of which
    will be described in this chapter. Additionally, the book *R for Data Science*
    by Hadley Wickham and Garrett Grolemund is available freely online at [https://r4ds.hadley.nz](https://r4ds.hadley.nz)
    and illustrates how the tidyverse’s self-proclaimed “opinionated” approach simplifies
    data science projects.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在线有越来越多的资源可以帮助你了解tidyverse，从其主页[https://www.tidyverse.org](https://www.tidyverse.org)开始。在这里，你可以了解包含在该套件中的各种包，其中一些将在本章中描述。此外，Hadley
    Wickham和Garrett Grolemund合著的《R for Data Science》一书可在[https://r4ds.hadley.nz](https://r4ds.hadley.nz)免费在线阅读，并展示了tidyverse自诩的“有见地”的方法如何简化数据科学项目。
- en: I am often asked the question of how R compares to Python for data science and
    machine learning. RStudio and the tidyverse are perhaps R’s greatest asset and
    point of distinction. There is arguably no easier way to begin a data science
    journey. Once you’ve learned the “tidy” way of doing data analysis, you are likely
    to wish the tidyverse functionality existed everywhere!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常被问到R与Python在数据科学和机器学习方面的比较问题。RStudio和tidyverse可能是R最大的资产和区别点。可以说，没有比开始数据科学之旅更容易的方法了。一旦你学会了“tidy”的数据分析方法，你很可能会希望tidyverse的功能无处不在！
- en: Making tidy table structures with tibbles
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tibbles制作整洁的表格结构
- en: Whereas the data frame is the center of the base R universe, the data structure
    at the heart of the tidyverse is found in the `tibble` package ([https://tibble.tidyverse.org](https://tibble.tidyverse.org)),
    the name of which is a pun on the word “table” as well as a nod to the infamous
    “tribble” in *Star Trek* lore. A **tibble** acts almost exactly like a data frame
    but includes additional modern functionality for convenience and simplicity. Tibbles
    can be used almost everywhere a data frame can be used. Detailed information about
    tibbles can be found by typing the command `vignette("tibble")` in R.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据框是基础R宇宙的中心，但tidyverse的核心数据结构位于`tibble`包中（[https://tibble.tidyverse.org](https://tibble.tidyverse.org)），其名称是对“table”一词的双关语，同时也是对《星际迷航》传说中著名的“tribble”的致敬。一个**tibble**几乎与数据框完全相同，但为了方便和简洁，还包括了额外的现代功能。tibbles几乎可以在数据框可以使用的任何地方使用。有关tibbles的详细信息，可以在R中输入命令`vignette("tibble")`来获取。
- en: 'Most of the time, using tibbles will be transparent and seamless, as tibbles
    can pass as a data frame in most R packages. However, in the rare case where you
    need to convert a tibble to a data frame, use the `as.data.frame()` function.
    To go in the other direction and convert a data frame in to a tibble, use the
    `as_tibble()` function. Here, we’ll create a tibble from the Titanic dataset first
    introduced in the previous chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，使用tibbles将是透明和无缝的，因为tibbles可以在大多数R包中充当数据框。然而，在极少数需要将tibble转换为数据框的情况下，请使用`as.data.frame()`函数。要反向操作并将数据框转换为tibble，请使用`as_tibble()`函数。在这里，我们将首先从上一章中首次介绍的Titanic数据集中创建一个tibble：
- en: '[PRE0]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Typing the name of this object demonstrates the tibble’s cleaner and more informative
    output than a standard data frame:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输入这个对象的名称展示了tibble比标准数据框具有更简洁和更丰富的输出：
- en: '![Table  Description automatically generated with medium confidence](img/B17290_12_12.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成，置信度中等](img/B17290_12_12.png)'
- en: 'Figure 12.12: Displaying a tibble object results in more informative output
    than a standard data frame'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：显示tibble对象的结果比标准数据框更具有信息量
- en: It is important to note the distinctions between tibbles and data frames, as
    the tidyverse will automatically create a tibble object for many of its operations.
    Overall, you are likely to find that tibbles are faster and easier to work with
    than data frames. They generally make smarter assumptions about the data, which
    means you will spend less time redoing R’s work—like recoding strings as factors
    or vice versa.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意tibbles和数据框之间的区别，因为tidyverse将为许多操作自动创建一个tibble对象。总的来说，你可能会发现tibbles比数据框更快、更容易处理。它们通常对数据的假设更智能，这意味着你将花费更少的时间重做R的工作——比如将字符串重新编码为因子或反之亦然。
- en: Indeed, one simple distinction between tibbles and data frames is that a tibble
    never assumes `stringsAsFactors = TRUE`, which was the default behavior in base
    R until relatively recently with the release of R version 4.0\. As described in
    previous chapters, R’s `stringsAsFactors` setting sometimes led to confusion or
    programming bugs when character columns were automatically converted in to factors
    by default. Another distinction between tibbles and data frames is that, as long
    as the name is surrounded by the backtick (`` ` ``) character, a tibble can use
    non-standard column names like `` `my var` `` that violate base R’s object naming
    rules. Other benefits of tibbles are unlocked by complementary tidyverse packages,
    as described in the sections that follow.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，tibbles 和数据框之间一个简单的区别是，tibble 从不假设 `stringsAsFactors = TRUE`，这是 R 版本 4.0
    之前在基础 R 中的默认行为。正如前几章所述，R 的 `stringsAsFactors` 设置有时会导致混淆或编程错误，因为字符列默认自动转换为因子。tibbles
    和数据框之间的另一个区别是，只要名称被反引号（`` ` ``）包围，tibble 就可以使用非标准列名，如 `` `my var` ``，这违反了基础 R
    的对象命名规则。其他 tibbles 的好处可以通过后续章节中描述的互补 tidyverse 包来解锁。
- en: Reading rectangular files faster with readr and readxl
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 readr 和 readxl 更快地读取矩形文件
- en: Nearly every chapter so far has used the `read.csv()` function to load data
    into R data frames. Although we could convert these data frames in to tibbles,
    there is a faster and more direct path to get data into the tibble format. The
    tidyverse includes the `readr` package ([https://readr.tidyverse.org](https://readr.tidyverse.org))
    for loading tabular data. This is described in the data import chapter in *R for
    Data Science* at [https://r4ds.hadley.nz/data-import.html](https://r4ds.hadley.nz/data-import.html),
    but the basic functionality is simple.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎到目前为止的每一章都使用了 `read.csv()` 函数将数据加载到 R 数据框中。虽然我们可以将这些数据框转换为 tibbles，但有一条更快、更直接的方法将数据导入
    tibble 格式。tidyverse 包含了用于加载表格数据的 `readr` 包（[https://readr.tidyverse.org](https://readr.tidyverse.org)）。这在本节
    *R for Data Science* 的数据导入章节中有描述（[https://r4ds.hadley.nz/data-import.html](https://r4ds.hadley.nz/data-import.html)），但基本功能很简单。
- en: The `readr` package provides a `read_csv()` function that loads data from CSV
    files much like base R’s `read.csv()` function. A key difference, aside from the
    subtle difference in their function names, is that the tidyverse’s version is
    much speedier—and not merely because it automatically converts the data into a
    tibble. It is about 10x faster at reading data according to the package authors.
    It is also smarter about the format of the columns to be loaded. For example,
    it has the capability to handle numbers with currency characters, parse date columns,
    and is better at handling international data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`readr` 包提供了一个 `read_csv()` 函数，它从 CSV 文件加载数据，类似于基础 R 的 `read.csv()` 函数。除了它们函数名之间的细微差别之外，tidyverse
    的版本要快得多——不仅仅是因为它自动将数据转换为 tibble。根据包作者的说明，它在读取数据方面大约快 10 倍。它还对要加载的列的格式更智能。例如，它具有处理带货币字符的数字、解析日期列的能力，并且在国际数据方面处理得更好。'
- en: 'To create a tibble from a CSV file, simply use the `read_csv()` function as
    follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 CSV 文件创建 tibble，只需使用以下 `read_csv()` 函数：
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will use the default parsing settings, which attempt to infer the correct
    data type (that is, character or numeric) for each column. The column specification
    will be displayed in the R output upon completion of the file read. The inferred
    data types may be overridden by providing the correct column specifications via
    a `col()` function call passed to the `read_csv()` function. For more information
    on the syntax, view the documentation using the `vignette("readr")` command.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用默认的解析设置，尝试为每一列推断正确的数据类型（即字符或数值）。在完成文件读取后，列规范将在 R 输出中显示。可以通过向 `read_csv()`
    函数传递一个 `col()` 函数调用来提供正确的列规范，从而覆盖推断的数据类型。有关语法的更多信息，请使用 `vignette("readr")` 命令查看文档。
- en: 'The `readxl` package ([https://readxl.tidyverse.org](https://readxl.tidyverse.org))
    provides a method to read data directly from the Microsoft Excel spreadsheet format.
    To create a tibble from an XLSX file, simply use the `read_excel()` function as
    follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`readxl` 包（[https://readxl.tidyverse.org](https://readxl.tidyverse.org)）提供了一种直接从
    Microsoft Excel 电子表格格式读取数据的方法。要从 XLSX 文件创建 tibble，只需使用以下 `read_excel()` 函数：'
- en: '[PRE2]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, as first introduced in *Chapter 2*, *Managing and Understanding
    Data*, the RStudio desktop application can write the data import code for you.
    In the upper-right of the interface, under the **Environment** tab, there is an
    **Import Dataset** button. This menu reveals a list of data import options, including
    plaintext formats like CSV files (using base R or the `readr` package), as well
    as Excel and the SPSS, SAS, and Stata formats created by other statistical computing
    software tools. Using the **From Text (readr)** option reveals the following graphical
    interface, allowing the import process to be easily customized:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如第 2 章中首次介绍的那样，*管理和理解数据*，RStudio 桌面应用程序可以为您编写数据导入代码。在界面的右上角，在**环境**选项卡下，有一个**导入数据集**按钮。此菜单显示一系列数据导入选项，包括
    CSV 文件等纯文本格式（使用基础 R 或 `readr` 包），以及由其他统计计算软件工具创建的 Excel、SPSS、SAS 和 Stata 格式。使用**从文本（readr）**选项显示以下图形界面，允许轻松定制导入过程：
- en: '![](img/B17290_12_13.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_12_13.png)'
- en: 'Figure 12.13: RStudio’s Import Dataset feature automatically writes R code
    to easily import a variety of data formats'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：RStudio 的导入数据集功能自动编写 R 代码，以便轻松导入各种数据格式
- en: The interface displays a preview of the data that updates as the import parameters
    are customized. The default column data types can be customized by clicking on
    the drop-down menu in the column header, and the code preview in the lower-right
    will update accordingly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 界面显示数据的预览，随着导入参数的定制而更新。默认列数据类型可以通过点击列标题中的下拉菜单进行定制，右下角的代码预览将相应更新。
- en: Clicking the **Import** button will immediately execute the code, but a better
    practice is to copy and paste the code into your R source code file so that the
    import process can be easily run again in the future.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**导入**按钮将立即执行代码，但更好的做法是将代码复制并粘贴到您的 R 源代码文件中，以便将来可以轻松再次运行导入过程。
- en: Preparing and piping data with dplyr
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 dplyr 准备和管道数据
- en: The `dplyr` package ([https://dplyr.tidyverse.org](https://dplyr.tidyverse.org))
    provides the infrastructure for the tidyverse, as it includes the basic functionality
    that allows data to be transformed and manipulated. It also provides a straightforward
    way to begin working with larger datasets in R. Though there are other packages
    that have greater raw speed or are capable of handling even more massive datasets,
    dplyr is still quite capable and a good first step to take if you run into speed
    or memory limitations with base R.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`dplyr`包([https://dplyr.tidyverse.org](https://dplyr.tidyverse.org))为 tidyverse
    提供了基础设施，因为它包括允许数据转换和操作的基本功能。它还提供了一种简单的方法来开始使用 R 中的大型数据集。尽管有其他包具有更高的原始速度或能够处理更大的数据集，但
    dplyr 仍然非常强大，如果您在基础 R 中遇到速度或内存限制时，它是一个很好的第一步。'
- en: 'When used with tibble objects, dplyr unlocks some impressive functionality:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 tibble 对象一起使用时，dplyr 解锁了一些令人印象深刻的功能：
- en: Because dplyr focuses on data frames rather than vectors, new operators are
    introduced that allow common data transformations to be performed with much less
    code while remaining highly readable.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 dplyr 专注于数据框而不是向量，因此引入了新的运算符，允许以更少的代码执行常见的数据处理转换，同时保持高度可读性。
- en: The package makes reasonable assumptions about data frames, which optimizes
    your effort as well as memory use. If possible, it avoids making copies of data
    by pointing to the original value instead.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该包对数据框做出了合理的假设，从而优化了您的努力以及内存使用。如果可能的话，它通过指向原始值而不是创建副本来避免复制数据。
- en: Key portions of the code are written in C++, which, according to the authors,
    yields a 20x to 1,000x performance increase over base R for many operations.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码的关键部分是用 C++编写的，据作者称，这使许多操作的性能比基础 R 提高了 20 倍到 1,000 倍。
- en: R data frames are limited by available memory. With dplyr, tibbles can be linked
    transparently to disk-based databases exceeding what can be stored in memory.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 数据框受可用内存的限制。使用 dplyr，tibbles 可以透明地链接到基于磁盘的数据库，其容量超过内存可以存储的内容。
- en: 'The dplyr grammar of working with data becomes second nature after the initial
    learning curve has been passed. There are five key verbs in the grammar, which
    perform many of the most common transformations to data tables. Beginning with
    a tibble, one may choose to:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在克服了初始的学习曲线之后，dplyr 的数据处理语法变得自然而然。语法中有五个关键动词，执行了数据表中最常见的许多转换。从 tibble 开始，可以选择：
- en: '`filter()` rows of data by values of the columns'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过列的值`filter()`筛选数据行
- en: '`select()` columns of data by name'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过名称`select()`选择数据列
- en: '`arrange()` rows of data by sorting the values'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`arrange()`通过排序值来排序数据行'
- en: '`mutate()` columns into new columns by transforming the values'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mutate()`通过变换值将列转换为新列'
- en: '`summarize()` rows of data by aggregating values into a summary'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summarize()`通过聚合值来汇总数据行'
- en: These five dplyr verbs are brought together in sequences using a **pipe operator**,
    which is natively supported in R as of version 4.1 or later. Represented by the
    `|>` symbols, which vaguely resembles an arrowhead pointing to the right, the
    pipe operator “pipes” data by moving it from one function to another. The use
    of pipes allows you to create powerful chains of functions to sequentially process
    datasets.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个dplyr动词通过**管道操作符**串联起来，该操作符从R 4.1或更高版本开始原生支持。管道操作符由`|>`符号表示，其形状略似一个指向右方的箭头，通过“管道”将数据从一个函数移动到另一个函数。使用管道操作符可以创建强大的函数链，以顺序处理数据集。
- en: In versions of R prior to 4.1.0 update, the pipe operator was denoted by the
    `%>%` character sequence and required the `magrittr` package. The differences
    between the old and new pipe functionality are relatively minor, but as a native
    operator the new pipe may have a small speed advantage. For a shortcut to typing
    the pipe operator, the RStudio Desktop IDE, the key combination *ctrl* + *shift*
    + *m* will insert the character sequence. Note that for this shortcut to produce
    the updated pipe, you may need to change the setting to “**Use the native pipe
    operator**, **|>**” in the RStudio “**Global Options**” menu under the “**Code**”
    heading.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在R 4.1.0更新之前的版本中，管道操作符由`%>%`字符序列表示，并需要`magrittr`包。新旧管道功能之间的差异相对较小，但作为原生操作符，新的管道可能具有轻微的速度优势。为了快速输入管道操作符，RStudio桌面IDE中，*ctrl*
    + *shift* + *m*快捷键可以插入字符序列。请注意，为了使此快捷键生成更新的管道，您可能需要将RStudio“**代码**”标题下的“**全局选项**”菜单中的设置更改为“**使用原生管道操作符**，**|>**”。
- en: 'After loading the package with the `library(dplyr)` command, data transformations
    begin with a tibble being piped into one of the package’s verbs. For example,
    one might `filter()` rows of the Titanic dataset to limit rows to women:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`library(dplyr)`命令加载包之后，数据转换从将tibble通过管道操作符传入包中的一个动词开始。例如，有人可能`filter()`泰坦尼克号数据集的行，以限制行只包含女性：
- en: '[PRE3]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, one might `select()` only the name, sex, and age columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，有人可能只`select()`名称、性别和年龄列：
- en: '[PRE4]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Where dplyr starts to shine is through its ability to chain together verbs
    in a sequence with pipes. For example, we can combine the prior two verbs, sort
    alphabetically using the verb `arrange()`, and save the output to a tibble as
    follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: dplyr开始发光之处在于其能够通过管道操作符串联一系列动词。例如，我们可以结合前两个动词，使用`arrange()`动词按字母顺序排序，并将输出保存到tibble中，如下所示：
- en: '[PRE5]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Although this may not seem like a revelation just yet, when combined with the
    `mutate()` verb, we can perform complex data transformations with simpler, more
    readable code than in the base R language. We will see several examples of `mutate()`
    later on, but for now, the important thing to remember is that it is used to create
    new columns in the tibble. For example, we might create a binary `elderly` feature
    that indicates whether a passenger is at least 65 years old.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这目前可能看起来并不像是一个重大的发现，但当与`mutate()`动词结合使用时，我们可以使用比基础R语言更简单、更易读的代码执行复杂的数据转换。我们将在稍后看到几个`mutate()`的例子，但现阶段，重要的是要记住它用于在tibble中创建新列。例如，我们可能创建一个表示乘客是否至少65岁的二进制`elderly`特征。
- en: 'This uses the dplyr package’s `if_else()` function to assign a value of `1`
    if the passenger is elderly, and `0` otherwise:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用了dplyr包的`if_else()`函数，如果乘客是老年人，则分配值为`1`，否则为`0`：
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By separating the statements by commas, multiple columns can be created within
    a single `mutate()` statement. This is demonstrated here to create an additional
    `child` feature that indicates whether the passenger is less than 18 years old:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逗号分隔语句，可以在单个`mutate()`语句中创建多个列。这里展示了如何创建一个额外的`child`特征，表示乘客是否小于18岁：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The remaining `dplyr` verb, `summarize()`, allows us to create aggregated or
    summarized metrics by grouping rows in the tibble. For example, suppose we would
    like to compute the survival rate by age or sex. We’ll begin with sex, as it is
    the easier of the two cases. We simply pipe the data into the `group_by(Sex)`
    function to create the male and female groups, then follow this with a `summarize()`
    statement to create a `survival_rate` feature that computes the average survival
    by group:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的`dplyr`动词`summarize()`允许我们通过分组`tibble`中的行来创建汇总或汇总度量。例如，假设我们想要计算按年龄或性别计算的存活率。我们将从性别开始，因为它比另一个案例更容易。我们只需将数据通过`group_by(Sex)`函数管道化以创建男性和女性组，然后跟随一个`summarize()`语句来创建一个`survival_rate`特征，该特征计算按组的平均存活率：
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As shown in the output, females were substantially more likely to survive than
    males.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，女性比男性更有可能存活。
- en: 'To compute survival by age, things are slightly more complicated due to the
    missing age values. We’ll need to filter out these rows and use the `group_by()`
    function to compare children (less than 18 years old) to adults as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要按年龄计算存活率，由于缺少年龄值，事情稍微复杂一些。我们需要过滤掉这些行，并使用`group_by()`函数来比较儿童（18岁以下）和成人，如下所示：
- en: '[PRE10]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The results suggest that children were about 40% more likely to survive than
    adults. When combined with the comparison between males and females, this provides
    strong evidence of the hypothesized “women and children first” policy for evacuation
    of the sinking ship.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，儿童比成人有大约40%的存活率。当与男性和女性的比较结合起来时，这为假设的“妇女和儿童优先”的撤离沉船政策提供了强有力的证据。
- en: Because summary statistics by group can be computed using other methods in base
    R (including the `ave()` and `aggregate()` functions described in previous chapters),
    it is important to note that the `summarize()` command is also capable of much
    more than this. In particular, one might use it for the feature engineering hints
    described earlier in this chapter, such as observing neighbors’ behavior, utilizing
    related rows, and decomposing time series. All three of these cases involve `group_by()`
    options like households, zip codes, or units of time. Using `dplyr` to perform
    the aggregation for these data preparation operations is much easier than attempting
    to do so in base R.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以使用R的基础方法（包括前几章中描述的`ave()`和`aggregate()`函数）计算按组汇总的统计量，因此值得注意的是`summarize()`命令也具有更多功能。特别是，人们可能使用它来完成本章前面描述的特征工程提示，例如观察邻居的行为、利用相关行和时间序列分解。这三种情况都涉及`group_by()`选项，如家庭、邮政编码或时间单位。使用`dplyr`对这些数据准备操作进行聚合比在基础R中尝试这样做要容易得多。
- en: 'To put together what we’ve learned so far and provide one more example using
    pipes, let’s build a decision tree model of the Titanic dataset. We’ll `filter()`
    missing age values, use `mutate()` to create a new `AgeGroup` feature, and `select()`
    only the columns of interest for the decision tree model. The resulting dataset
    is piped to the `rpart()` decision tree algorithm, which illustrates the ability
    to pipe data to functions outside of the tidyverse:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们迄今为止学到的知识结合起来，并使用管道提供一个更多示例，让我们构建泰坦尼克号数据集的决策树模型。我们将`filter()`缺失的年龄值，使用`mutate()`创建一个新的`AgeGroup`特征，并`select()`决策树模型感兴趣的列。结果数据集通过管道传递到`rpart()`决策树算法，这展示了将数据传递到tidyverse之外的函数的能力：
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that the series of steps reads almost like plain-language pseudocode. It
    is also worth noting the arguments within the `rpart()` function call. The `formula
    = Survived ~ .` argument uses R’s formula interface to model survival as a function
    of all predictors; the dot here represents the other features in the dataset not
    explicitly listed. The `data = _` argument uses the underscore (_) as a placeholder
    to represent the data being fed to `rpart()` by the pipe. The underscore can be
    used in this way to indicate the function parameter to which the data should be
    piped.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这一系列步骤几乎就像伪代码。还值得注意的是`rpart()`函数调用中的参数。`formula = Survived ~ .`参数使用R的公式接口将存活率建模为所有预测器的函数；这里的点代表数据集中未明确列出的其他特征。`data
    = _`参数使用下划线（_）作为占位符来表示通过管道传递给`rpart()`的数据。下划线可以以这种方式使用，以指示数据应传递到的函数参数。
- en: This is usually unnecessary for dplyr’s built-in functions, because they look
    for the piped data as the first parameter by default, but functions outside the
    tidyverse may require the pipe to target a specific function parameter in this
    way.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于dplyr的内置函数来说，这通常是不必要的，因为它们默认将管道数据作为第一个参数查找，但tidyverse之外的函数可能需要以这种方式将管道目标特定函数参数。
- en: It is important to note that the underscore placeholder character is new as
    of R version 4.2 and will not work in prior versions! In older code that uses
    the `magrittr` package, the dot character (.) was used as the placeholder.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，下划线占位符字符是R版本4.2中引入的，在之前的版本中不会工作！在旧代码中使用`magrittr`包时，点字符（.）被用作占位符。
- en: 'For fun, we can visualize the resulting decision tree, which shows that women
    and children are more likely to survive than adults, men, and those in third passenger
    class:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了娱乐，我们可以可视化生成的决策树，该树显示妇女和儿童比成年人、男性和第三乘客舱的人更有可能生存：
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following decision tree diagram:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下决策树图：
- en: '![Timeline  Description automatically generated](img/B17290_12_14.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![时间线描述自动生成](img/B17290_12_14.png)'
- en: 'Figure 12.14: A decision tree predicting Titanic survival, which was built
    using a series of pipes'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：使用一系列管道构建的预测泰坦尼克号生还的决策树
- en: These are just a few small examples of how sequences of dplyr commands can make
    complex data manipulation tasks simpler. This is on top of the fact that, due
    to dplyr’s more efficient code, the steps often execute more quickly than the
    equivalent commands in base R! Providing a complete dplyr tutorial is beyond the
    scope of this book, but there are many learning resources available online, including
    the *R for Data Science* chapter at `https://r4ds.hadley.nz/transform.html`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是几个简单的例子，说明了dplyr命令序列如何使复杂的数据操作任务变得简单。实际上，由于dplyr代码的效率更高，这些步骤通常比R基础命令执行得更快！提供完整的dplyr教程超出了本书的范围，但网上有许多学习资源，包括`https://r4ds.hadley.nz/transform.html`上的*R
    for Data Science*章节。
- en: Transforming text with stringr
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用stringr转换文本
- en: The `stringr` package (`https://stringr.tidyverse.org`) adds functions to analyze
    and transform character strings. Base R, of course, can do this too, but the functions
    are inconsistent in how they work on vectors and are relatively slow; `stringr`
    implements these functions in a form more attuned to the tidyverse workflow. The
    free resource *R for Data Science* has a tutorial that introduces the package’s
    complete set of capabilities, at [https://r4ds.hadley.nz/strings.html](https://r4ds.hadley.nz/strings.html),
    but here, we’ll examine some of the aspects most relevant to feature engineering.
    If you’d like to follow along, be sure to load the Titanic dataset and install
    and load the `stringr` package before proceeding.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`stringr`包（`https://stringr.tidyverse.org`）添加了分析并转换字符字符串的功能。当然，基础R也可以这样做，但函数在处理向量时的表现不一致，而且相对较慢；`stringr`以更适合tidyverse工作流程的形式实现了这些函数。《R
    for Data Science》这个免费资源有一个教程，介绍了该包的全部功能，可在[https://r4ds.hadley.nz/strings.html](https://r4ds.hadley.nz/strings.html)找到，但在这里，我们将检查与特征工程最相关的方面。如果您想跟上来，请确保在继续之前加载Titanic数据集并安装并加载`stringr`包。'
- en: 'Earlier in this chapter, the second tip for feature engineering was to “find
    insights hidden in text.” The `stringr` package can assist with this effort by
    providing functions to slice strings and detect patterns within text. All `stringr`
    functions begin with the prefix `str_`, and a few relevant examples are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，关于特征工程的第二个提示是“在文本中找到隐藏的见解。”`stringr`包可以通过提供切片字符串和检测文本中模式的函数来帮助这一努力。所有`stringr`函数都以前缀`str_`开头，以下是一些相关示例：
- en: '`str_detect()` determines whether a search term is found in a string'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str_detect()`确定一个搜索词是否在字符串中找到'
- en: '`str_sub()` slices a string by position and returns a substring'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str_sub()`通过位置切片字符串并返回子字符串'
- en: '`str_extract()` searches for a string and returns the matching pattern'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str_extract()`搜索字符串并返回匹配的模式'
- en: '`str_replace()` replaces characters in a string with something else'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`str_replace()`将字符串中的字符替换为其他内容'
- en: Although these functions seem quite similar, they are used for quite different
    purposes. To demonstrate these purposes, we’ll begin by examining the `Cabin`
    feature to determine whether certain rooms on the *Titanic* are linked to greater
    survival. We cannot use this feature as-is, because each cabin code is unique.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些函数看起来相当相似，但它们用于完全不同的目的。为了演示这些目的，我们将首先检查`Cabin`特征，以确定某些`泰坦尼克号`上的房间是否与更高的生存率相关。我们不能直接使用这个特征，因为每个舱位代码都是唯一的。
- en: 'However, because the codes are in forms like `A10`, `B101`, or `E67`, perhaps
    the alphabetical prefix indicates a position on the ship, and perhaps passengers
    in some of these locations may have been more able to escape the disaster. We’ll
    use the `str_sub()` function to take a 1-character substring beginning and ending
    at position 1, and save this to a `CabinCode` feature as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于代码的形式类似于`A10`、`B101`或`E67`，也许字母前缀表示船上的一个位置，也许在这些位置的一些乘客可能更有能力逃离灾难。我们将使用`str_sub()`函数来提取从位置1开始和结束的1个字符子串，并将其保存为`CabinCode`特征，如下所示：
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To confirm that the cabin code is meaningful, we can use the `table()` function
    to see a clear relationship between it and the passenger class. The `useNA` parameter
    is set to `"ifany"` to display the `NA` values caused by missing cabin codes for
    some passengers:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认舱位代码是有意义的，我们可以使用`table()`函数来查看它和乘客等级之间的清晰关系。`useNA`参数设置为`"ifany"`以显示由一些乘客缺失舱位代码而引起的`NA`值：
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `NA` values appear to be more common in the lower ticket classes, so it
    seems plausible that cheaper fares may have not received a cabin code.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`NA`值似乎在较低票价的等级中更为常见，因此似乎合理的推测是较便宜的票价可能没有收到舱位代码。'
- en: 'We can also plot the survival probability by cabin code by piping the file
    into a `ggplot()` function:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过将文件管道输入到`ggplot()`函数中来绘制按舱位代码划分的生存概率图：
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting figure shows that even within the first-class cabin types (codes
    A, B, and C) there are differences in survival rate; additionally, the passengers
    without a cabin code are the least likely to survive:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示，即使在头等舱类型（代码A、B和C）中，生存率也存在差异；此外，没有舱位代码的乘客最不可能生存：
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_12_15.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图表，条形图，直方图 描述自动生成](img/B17290_12_15.png)'
- en: 'Figure 12.15: The cabin code feature seems related to survival, even within
    first-class cabins (A, B, and C)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15：舱位代码特征似乎与生存相关，即使在头等舱（A、B和C）中也是如此
- en: Without processing the `Cabin` text data first, a learning algorithm would be
    unable to use the feature as the codes are unique to each cabin. Yet by applying
    a simple text transformation, we’ve decoded the cabin codes into something that
    can be used to improve the model’s survival predictions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有首先处理`Cabin`文本数据的情况下，学习算法将无法使用该特征，因为代码对每个舱位都是唯一的。然而，通过应用简单的文本转换，我们已经将舱位代码解码为可以用来提高模型生存预测的东西。
- en: 'With this success in mind, let’s examine another potential source of hidden
    data: the `Name` column. One might assume that this is unusable in a model, because
    the name is a unique identifier per row and training a model on this data will
    inevitably lead to overfitting. Although this is true, there is useful information
    hiding within the names. Looking at the first few rows reveals some potentially
    useful text strings:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个成功的基础上，让我们检查另一个潜在的数据来源：`Name`列。有人可能会认为这在一个模型中是不可用的，因为名字是每行的唯一标识符，在这个数据上训练模型不可避免地会导致过拟合。尽管这是真的，但名字中隐藏着有用的信息。查看前几行揭示了一些可能有用的文本字符串：
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For one, the salutation (Mr., Mrs., and Miss.) might be helpful for prediction.
    The problem is that these titles are located at different positions within the
    name strings, so we cannot simply use the `str_sub()` function to extract them.
    The correct tool for this job is `str_extract()`, which is used to match and extract
    shorter patterns from longer strings. The trick with working with this function
    is knowing how to express a text pattern rather than typing each potential salutation
    separately.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，问候语（先生、夫人、小姐）可能有助于预测。问题是这些头衔位于姓名字符串的不同位置，所以我们不能简单地使用`str_sub()`函数来提取它们。这项工作的正确工具是`str_extract()`，它用于从较长的字符串中匹配和提取较短的模式。使用这个函数的技巧在于知道如何表达一个文本模式，而不是单独地输入每个可能的问候语。
- en: The shorthand used to express a text search pattern is called a **regular expression**,
    or **regex** for short. Knowing how to create regular expressions is an incredibly
    useful skill, as they are used for the advanced find-and-replace features in many
    text editors, in addition to being useful for feature engineering in R. We’ll
    create a simple regex to extract the salutations from the name strings.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表达文本搜索模式的简写称为**正则表达式**，或简称**regex**。了解如何创建正则表达式是一项极其有用的技能，因为它们被用于许多文本编辑器的高级查找和替换功能，此外，在R中的特征工程中也很有用。我们将创建一个简单的正则表达式来从姓名字符串中提取称呼。
- en: 'The first step in using regular expressions is to identify the common elements
    across all the desired target strings. In the case of the Titanic names, it looks
    like each salutation is preceded by a comma followed by a blank space, then has
    a series of letters before ending with a period. This can be coded as the following
    regex string:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式的第一步是确定所有目标字符串中的共同元素。在泰坦尼克号姓名的情况下，看起来每个称呼都由一个逗号和一个空格开头，然后是一系列字母，最后以句号结束。这可以编码为以下正则表达式字符串：
- en: '[PRE20]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This seems to be nonsense but can be understood as a sequence that attempts
    to match a pattern character by character. The matching process begins with a
    comma and a blank space, as expected. Next, the square brackets tell the search
    function to look for any of the characters inside the brackets. For instance,
    `[AB]` would search for `A` or `B`, and `[ABC]` would search for `A`, `B`, or
    `C`. In our usage, the dash is used to search for any characters within the range
    between `A` and `z`. Note that capitalization is important—that is, `[A-Z]` is
    different from `[A-z]`. The former will search 26 characters comprising the uppercase
    alphabet while the latter will search 52 characters, including uppercase and lowercase.
    Keep in mind that `[A-z]` only matches a single character.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来似乎是胡言乱语，但可以理解为尝试逐个字符匹配模式的序列。匹配过程从预期的逗号和空格开始。接下来，方括号告诉搜索函数在括号内寻找任何字符。例如，`[AB]`会搜索`A`或`B`，而`[ABC]`会搜索`A`、`B`或`C`。在我们的用法中，破折号用于搜索`A`和`z`之间的任何字符。请注意，大小写很重要——即`[A-Z]`与`[A-z]`不同。前者将搜索包含大写字母的26个字符，而后者将搜索包括大小写字母在内的52个字符。请记住，`[A-z]`只匹配单个字符。
- en: To have the expression match more characters, we follow the brackets with a
    `+` symbol to tell the algorithm to continue matching characters until it reaches
    something not inside the brackets. Then, it checks to see whether the remaining
    part of the regex matches.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要使表达式匹配更多字符，我们可以在括号后跟一个`+`符号来告诉算法继续匹配字符，直到它遇到括号内的内容为止。然后，它检查正则表达式的剩余部分是否匹配。
- en: The remaining piece is the `\\.` sequence, which is three characters that represent
    the single period character at the end of our search pattern. Because the dot
    is a special term that represents an arbitrary character, we must escape the dot
    by prefixing it with a slash. Unfortunately, the slash is also a special character
    in R, so we must escape it as well by prefixing it with yet another slash.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的部分是`\\.`序列，这是代表我们搜索模式末尾的单个句点字符的三个字符。因为点是一个特殊术语，代表任意字符，我们必须通过在前面加上斜杠来转义点。不幸的是，斜杠在R中也是一个特殊字符，因此我们必须通过在前面加上另一个斜杠来转义它。
- en: Regular expressions can be tricky to learn but are well worth the effort. You
    can find a deep dive into understanding how they work at [https://www.regular-expressions.info](https://www.regular-expressions.info).
    Alternatively, there are many text editors and web applications that demonstrate
    matching in real time. These can be hugely helpful to understand how to develop
    the regex search patterns and diagnose errors. One of the best such tools is found
    at [https://regexr.com](https://regexr.com).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式可能难以学习，但付出努力是值得的。您可以在[https://www.regular-expressions.info](https://www.regular-expressions.info)深入了解它们是如何工作的。或者，有许多文本编辑器和网络应用程序可以实时演示匹配。这些工具对于理解如何开发正则表达式搜索模式和诊断错误非常有帮助。其中最好的工具之一可以在[https://regexr.com](https://regexr.com)找到。
- en: 'We can put this expression to work on the Titanic name data by combining it
    in a `mutate()` function with `str_extract()`, as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`mutate()`函数与`str_extract()`结合来将这个表达式应用于泰坦尼克号姓名数据，如下所示：
- en: '[PRE21]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Looking at the first few examples, it looks like these need to be cleaned up
    a bit:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下前几个例子，这些似乎需要稍微清理一下：
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s use the `str_replace()` function to eliminate the punctuation and blank
    spaces in these titles. We begin by constructing a regex to match the punctuation
    and empty space. One way to do this is to match the comma, blank space, and period
    using the `"[, \\.]"` search string. Used with `str_replace()` as shown here,
    any comma, blank space, and period characters in `Title` will be replaced by the
    empty (null) string:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the `str_replace_all()` variant of the replace function was used
    due to the fact that multiple characters needed replacement; the basic `str_replace()`
    would have only replaced the first instance of a matching character. Many of `stringr`''s
    functions have “all” variants for this use case. Let’s see the result of our effort:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Given the small counts for some of these titles and salutations, it may make
    sense to group them together. To this end, we can use dplyr’s `recode()` function
    to change the categories. We’ll keep several of the high-count levels the same,
    while grouping the rest into variants of `Miss` and a catch-all bucket, using
    the `.missing` and `.default` values to assign the `Other` label to `NA` values
    and anything else not already coded:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Checking our work, we see that our cleanup worked as planned:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also see that the title is meaningful by examining a plot of survival
    rates by title:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This produces the following bar chart:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_16.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: The constructed salutation captures the impact of both age and
    gender on survival likelihood'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The creation of `CabinCode` and `TitleGroup` features exemplifies the feature
    engineering technique of finding hidden information in text data. These new features
    are likely to provide additional information beyond the base features in the Titanic
    dataset, which learning algorithms can use to improve performance. A bit of creativity
    combined with `stringr` and knowledge of regular expressions may provide the edge
    needed to surpass the competition.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning dates with lubridate
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lubridate` package ([https://lubridate.tidyverse.org](https://lubridate.tidyverse.org))
    is an important tool for working with date and time data. It may not be needed
    for every analysis, but when it is needed, it can save a lot of grief. With dates
    and times, seemingly simple tasks can quickly turn into adventures, due to unforeseen
    subtleties like leap years and time zones—just ask anyone who has worked on birthday
    calculations, billing cycles, or similar date-sensitive tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the other tidyverse packages, the *R for Data Science* resource has
    an in-depth lubridate tutorial at `https://r4ds.hadley.nz/datetimes.html`, but
    we’ll briefly cover three of its most important feature engineering strengths
    here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring date and time data is loaded into R correctly while accounting for
    regional differences in how dates and times are expressed
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accurately calculating differences between dates and times while accounting
    for time zones and leap years
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for differences in how increments in time are understood in the real
    world, such as the fact that people become “1 year older” on their birthday
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reading dates into R is challenge number one, because dates are presented in
    many different formats. For example, the publication date of the first edition
    of *Machine Learning with R* can be expressed as:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: October 25, 2013 (a common longhand format in the United States)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10/25/13 (a common shorthand format in the United States)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 October 2013 (a common longhand format in Europe)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25.10.13 (a common shorthand format in Europe)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2013-10-25 (the international standard)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these diverse formats, lubridate is incapable of determining the correct
    format without help because months, days, and years can all fall in the range
    from 1 to 12\. Instead, we provide it the correct date constructor—either `mdy()`,
    `dmy()`, or `ymd()`, depending on the order of the month (`m`), day (`d`), and
    year (`y`) components of the input data. Given the order of the date components,
    the functions will automatically parse longhand and shorthand variants, and will
    handle leading zeros and two- or four-digit years. To demonstrate this, the dates
    expressed previously can be handled with the appropriate lubridate function, as
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Notice that in each case, the resulting `Date` object is exactly the same.
    Let’s create a similar object for each of the three previous editions of this
    book:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can do simple math to compute the difference between two dates:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice that by default, the difference between the two dates is returned as
    days. What if we hope to have an answer in years? Unfortunately, because these
    differences are a special lubridate `difftime` object, we cannot simply divide
    these numbers by 365 days to perform the obvious calculation. One option is to
    convert them into a **duration**, which is one of the ways lubridate computes
    date differences, and in particular, tracks the passage of physical time—imagine
    it acting much like a stopwatch. The `as.duration()` function performs the needed
    conversion:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can see here that the gap between the 2nd and 3rd editions of *Machine Learning
    with R* was almost twice as long as the difference between the 1st and 2nd editions.
    We can also see that the duration seems to default to seconds while also providing
    the approximate number of years. To obtain only years, we can divide the duration
    by the duration of 1 year, which lubridate provides as a `dyears()` function:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You may find it more convenient or easier to remember the `time_length()` function,
    which can perform the same calculation:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `unit` argument can be set to units like days, months, and years, depending
    on the desired result. Notice, however, that these durations are exact to the
    second like a stopwatch, which is not always how people think about dates.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: In particular, for birthdays and anniversaries, people tend to think in terms
    of calendar time—that is, the number of times the calendar has reached a particular
    milestone. In lubridate, this approach is called an **interval**, which implies
    a timeline-or calendar-based view of date differences, rather than the stopwatch-based
    approach of the duration methods discussed previously.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine we’d like to compute the age of the United States, which was
    born, so to speak, on July 4, 1776\. This means that on July 3, 2023, the country
    will be 246 birthdays old, and on July 5, 2023, it will be 247\. Using durations,
    we don’t get quite the right answers:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The problem has to do with the fact that durations deviate from calendar time
    due to calendar irregularities such as leap years and time changes. By explicitly
    converting the date difference into an interval with the `interval()` function,
    and then dividing by the `years()` function, we get closer to the right answer:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Before going any further, be sure to notice the fact that the `interval()` uses
    `start, end` syntax, in contrast to the date difference, which used `end - start`.
    Also note that the `years()` function returns a lubridate **period**, which is
    yet another way to understand differences between dates and times. Periods are
    always relative to their position on a calendar, which means that a 1-hour period
    can be a 2-hour duration during a time change, and a 1-year period can include
    365 or 366 1-day periods, depending on the calendar year—these are the types of
    challenging subtleties when working with dates that were mentioned in this section’s
    opening paragraph!
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our final age calculation, we’ll use the `%--%` interval construction
    operator as shorthand, and use the integer division operator `%/%` to return only
    the integer component of the age. These return the expected age values:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Generalizing this work, we can create a function to compute the calendar-based
    age for a given date of birth as of today:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To prove that it works, we’ll check the ages of a few famous tech billionaires:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: If you are following along in R, be aware that your results may vary depending
    on when you run the code—we’re all, unfortunately, still getting older by the
    day!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated the importance of data preparation. Because the tools
    and algorithms used to build machine learning models are the same across projects,
    data preparation is a key that unlocks the highest levels of model performance.
    This allows some aspects of human intelligence and creativity to have a large
    impact on the machine’s learning process, although clever practitioners use their
    strengths in concert with the machine’s by developing automated data engineering
    pipelines that take advantage of the computer’s ability to tirelessly search for
    useful insights in the data. These pipelines are especially important in the so-called
    “big data regime,” where data-hungry approaches like deep learning must be fed
    large amounts of data to avoid overfitting.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: In traditional small and medium data regimes, feature engineering by hand still
    reigns supreme. Using intuition and subject matter expertise, one can guide the
    model to the most useful signal in the training dataset. As this is more art than
    science, tips and tricks are learned on the job, or passed along second-hand from
    one data scientist to another. This chapter provided seven hints to help guide
    you on the journey, but the only way to truly become skilled at feature engineering
    is through practice.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Tools like the tidyverse suite of R packages make it much less laborious than
    in years past to gain the necessary experience to perform feature engineering
    tasks. This chapter demonstrated how the tidyverse packages can be used to turn
    data into more useful predictors, and how information hidden in text data can
    be extracted to turn what seem like useless features into important predictors.
    The tidyverse packages are much more capable of handling large and ever-growing
    datasets than the base R functions, and they make R a pleasure to use even as
    datasets grow in size and complexity.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: The skills developed in this chapter will provide a foundation for the work
    to come. In the next chapter, you will add new tidyverse packages to your toolkit
    and see even more examples of how it integrates into the machine learning workflow.
    You will continue to see the importance of data preparation skills as you explore
    data issues that begin as relatively minor challenges but quickly grow into massive
    problems if taken to an extreme.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
