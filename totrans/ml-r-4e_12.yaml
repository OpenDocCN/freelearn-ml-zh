- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The truism that 80 percent of the time invested in real-world machine learning
    projects is spent on data preparation is so widely cited that it is mostly accepted
    without question. Earlier chapters of this book helped perpetuate the cliché by
    stating it as a matter of fact without qualification, and although it is certainly
    a common experience and perception, it is also an oversimplification, as tends
    to be the case when generalizing from a statistic. In reality, there is no single,
    uniform experience for data preparation. Yet, it is indeed true that data prep
    work almost always involves more effort than anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: Rare is the case in which you will be provided a single CSV formatted text file,
    which can be easily read into R and processed with just a few lines of R code,
    as was the case in previous chapters. Instead, necessary data elements are often
    distributed across databases, which must then be gathered, filtered, reformatted,
    and combined before the features can be used with machine learning. This can require
    significant effort even before considering the time expended gaining access to
    the data from stakeholders, as well as exploring and understanding the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is intended to prepare you (pun intended!) for the larger and
    more complex datasets that you’ll be preparing in the real world. You will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Why data preparation is crucial to building better models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and tricks for transforming data into more useful predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized R packages for efficiently preparing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different teams and different projects require their data scientists to invest
    different amounts of time preparing data for the machine learning process, and
    thus, the 80 percent statistic may overstate or understate the effort needed for
    any given project or from any single contributor.
  prefs: []
  type: TYPE_NORMAL
- en: Still, whether it is you or someone else performing this work, you will soon
    discover the undeniable fact that advanced data preparation is a necessary step
    in the process of building strong machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: Performing feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time, effort, and imagination are central to the process of **feature engineering**,
    which involves applying subject-matter expertise to create new features for prediction.
    In simple terms, it might be described as the art of making data more useful.
    In more complex terms, it involves a combination of domain expertise and data
    transformations. One needs to know not just what data will be useful to gather
    for the machine learning project, but also how to merge, code, and clean the data
    to conform to the algorithm’s expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is closely interrelated with data exploration, as described
    in *Chapter 11*, *Being Successful with Machine Learning*. Both involve interrogating
    data through the generation and testing of hypotheses. Exploring and brainstorming
    are likely to lead to insights about which features will be useful for prediction,
    and the act of engineering the features may lead to new questions to explore.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Feature engineering is part of a cycle that helps the model and
    data work together'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature engineering is part of a cycle within a cycle in which effort is invested
    to help the model and data work better together. A round of data exploration and
    feature engineering leads to improvements to the data, which leads to iterations
    of training better models, which then informs another round of potential improvements
    to the data. These potential improvements are not only the bare minimum cleaning
    and preparation tasks needed to address simple data issues and allow the algorithm
    to run in R, but also the steps that lead an algorithm to learn more effectively.
    These may include:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing complex data transformations that help the algorithm to learn faster
    or to learn a simpler representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features that are easier to interpret or better represent the underlying
    theoretical concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing unstructured data or merging additional features onto the main source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three of these require both intense thought and creativity, and are improvisational
    and domain-specific rather than formulaic. This being said, the computer and the
    practitioner can share this work using complementary strengths. What the computer
    lacks in creativity and ability to improvise, it may be able to address with computational
    horsepower, brute force, and unwavering persistence.
  prefs: []
  type: TYPE_NORMAL
- en: The role of human and machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature engineering can be viewed as a collaboration between the human and the
    machine during the learning process stage of abstraction. Recall that in *Chapter
    1*, *Introducing Machine Learning*, the abstraction step was defined as the translation
    of stored data into broader concepts and representations. In other words, during
    abstraction, connections are made between elements of raw data, which will represent
    important concepts for the learning objective. These relationships are generally
    defined by a model, which links the learned concepts to an outcome of interest.
    During feature engineering, the human gently guides or nudges the abstraction
    process in a specific direction, with the goal of producing a better-performing
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine it this way: recall an instance in your past where you attempted to
    learn a difficult concept—possibly even while reading this very textbook! Reading
    and later re-reading the text proves to be of no help to understanding the concept,
    and frustrated, you contact a friend or colleague for help. Perhaps this friend
    explains the concept in a different way, using analogies or examples that help
    connect the concept to your prior experience, and in doing so, it leads you to
    a moment of enlightenment: “Eureka!” All is suddenly clear, and you wonder how
    you couldn’t understand the concept in the first place. Such is the power of abstractions,
    which can be transferred from one learner to another to aid the learning process.
    The process of feature engineering allows the human to transfer their intuitive
    knowledge or subject-matter expertise to the machine through intentionally and
    purposefully designed input data.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the fact that abstraction is the cornerstone of the learning process,
    it can be argued that machine learning is fundamentally feature engineering. The
    renowned computer scientist and artificial intelligence pioneer Andrew Ng said,
    “*Coming up with features is difficult, time-consuming, and requires expert knowledge.
    Applied machine learning is basically feature engineering*.” Pedro Domingos, a
    professor of computer science and author of the machine learning book *The Master
    Algorithm*, said that “*some machine learning projects succeed and some fail.
    What makes the difference? Easily the most important factor is the features used*.”
  prefs: []
  type: TYPE_NORMAL
- en: Andrew Ng’s quote appears in a lecture titled *Machine Learning and AI via Brain
    simulations*, which is available online via web search. In addition to Pedro Domingos’
    book *The Master Algorithm* (2015), see also his excellent paper “A few useful
    things to know about machine learning” in *Communications of the ACM* (2012).
    [https://doi.org/10.1145/2347736.2347755](https://doi.org/10.1145/2347736.2347755).
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering performed well can turn weaker learners into much stronger
    learners. Many machine learning and artificial intelligence problems can be solved
    with simple linear regression methods, assuming that the data has been sufficiently
    cleaned. Even very complex machine learning methods can be replicated in standard
    linear regression given sufficient feature engineering. Linear regression can
    be adapted to model nonlinear patterns, using splines and quadratic terms, and
    can approach the performance of even the most complex neural networks, given a
    sufficient number of new features that have been engineered as carefully designed
    interactions or transformations of the original input data.
  prefs: []
  type: TYPE_NORMAL
- en: The idea that simple learning algorithms can be adapted to more complex problems
    is not limited to regression. For example, decision trees can work around their
    axis-parallel decision boundaries by rotations of the input data, while hyperplane-based
    support vector machines can model complex nonlinear patterns with a well-chosen
    kernel trick. A method as simple as k-NN could be used to mimic regression or
    perhaps even more complex methods, given enough effort and sufficient understanding
    of the input data and learning problem, but herein lies the catch. Why invest
    large amounts of time performing feature engineering to employ a simple method
    when a more complex algorithm will perform just as well or better, while also
    performing the feature engineering for us automatically?
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it is probably best to match the complexity of the data’s underlying
    patterns with a learning algorithm capable of handling them readily. Performing
    feature engineering by hand when a computer can do it automatically is not only
    wasted effort but also prone to mistakes and missing important patterns. Algorithms
    like decision trees and neural networks with a sufficiently large number of hidden
    nodes—and especially, deep learning neural networks—are particularly capable of
    doing their own form of feature engineering, which is likely to be more rigorous
    and thorough than what can be done by hand. Unfortunately, this does not mean
    we can blindly apply these same methods to every task—after all, there is no free
    lunch in machine learning!
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same algorithm to every problem suggests that there is a one-size-fits-all
    approach to feature engineering, when we know that it is as much an art as it
    is a science. Consequently, if all practitioners apply the same method to all
    tasks, they will have no way of knowing whether better performance is possible.
    Perhaps a slightly different feature engineering approach could have resulted
    in a model that more accurately predicted churn or cancer, and would have led
    to greater profits or more lives saved. This is clearly a problem in the real
    world, where even a small performance boost can mean a substantial edge over the
    competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a high-stakes competition environment, such as the machine learning competitions
    on Kaggle, each team has access to the same learning algorithms and is readily
    capable of rapidly applying each of them to identify which one performs best.
    It is no surprise, then, that a theme emerges while reading interviews with Kaggle
    champions: they often invest significant effort into feature engineering. Xavier
    Conort, who was the top-rated data scientist on Kaggle in 2012–2013, said in an
    interview that:'
  prefs: []
  type: TYPE_NORMAL
- en: ”The algorithms we used are very standard for Kagglers… We spent most of our
    efforts on feature engineering… We were also very careful to discard features
    likely to expose us to the risk of overfitting.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because feature engineering is one of the few proprietary aspects of machine
    learning, it is one of the few points of distinction across teams. In other words,
    teams that perform feature engineering well tend to outperform the competition.
  prefs: []
  type: TYPE_NORMAL
- en: To read the full interview with Xavier Conort, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/](https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/).
    Interviews with other Kaggle champions are available at [https://medium.com/kaggle-blog/tagged/kaggle-competition](https://medium.com/kaggle-blog/tagged/kaggle-competition).
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Conort’s statement, it would be easy to assume that the need to invest
    in feature engineering necessitates greater investment in human intelligence and
    the application of subject-matter expertise, but this is not always true. Jeremy
    Achin, a member of a top-performing “DataRobot” team on Kaggle, remarked on the
    surprisingly limited utility of human expertise. Commenting on his team’s time
    spent on feature engineering, he said in an interview that:'
  prefs: []
  type: TYPE_NORMAL
- en: ”The most surprising thing was that almost all attempts to use subject matter
    knowledge or insights drawn from data visualization led to drastically worse results.
    We actually arranged a 2-hour whiteboard lecture from a very talented biochemist
    and came up with some ideas based on what we learned, but none of them worked
    out.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeremy Achin, along with Xavier Conort and several other high-profile Kaggle
    Grand Masters, bootstrapped their Kaggle competition successes into an artificial
    intelligence company called DataRobot, which is now worth billions of dollars.
    Their software performs machine learning automatically, suggesting that a key
    lesson learned from their Kaggle work was that computers can perform many steps
    in the machine learning process just as well as humans, if not better.
  prefs: []
  type: TYPE_NORMAL
- en: To read the full interview with Jeremy Achin, which was originally posted on
    the Kaggle “No Free Hunch” blog, visit [https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/](https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/).
    The DataRobot company is found on the web at [https://www.datarobot.com](https://www.datarobot.com).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is a balance between building models piece by piece using subject-matter
    expertise and throwing everything at the machine to see what sticks. Although
    feature engineering today is largely still a manual process, the future of the
    field seems to be headed toward the scattershot “see what sticks” approach, as
    **automated feature engineering** is a rapidly growing area of research. The foundation
    of automated feature engineering tools is the idea that a computer can make up
    for its lack of creativity and domain knowledge by testing many more combinations
    of features than a human would ever have time to attempt. Automated feature engineering
    exchanges narrow-but-guided human thought for broad-and-systematic computer thought,
    with the potential upside of finding a more optimal solution and potential downsides
    including loss of interpretability and greater likelihood of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Before getting too excited about the potential for automation, it is worth noting
    that while such tools may allow a human to outsource certain parts of *thinking*
    about feature engineering, effort must still be invested in the *coding* part
    of the task. That is to say, time that was once spent hand-coding features one
    by one is instead spent coding functions that systematically find or construct
    useful features.
  prefs: []
  type: TYPE_NORMAL
- en: There are promising algorithms in development, such as the Python-based `Featuretools`
    package (and corresponding R package `featuretoolsR`, which interacts with the
    Python code), that may help automate the feature-building process, but the use
    of such tools is not yet widespread. Additionally, such methods must be fed by
    data and computing time, both of which may be limiting factors in many machine
    learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on Featuretools, visit: [https://www.featuretools.com](https://www.featuretools.com).'
  prefs: []
  type: TYPE_NORMAL
- en: The impact of big data and deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whether feature engineering is performed by a human or by automated machine
    methods, a point is inevitably reached at which additional invested effort leads
    to little or no boost to the learning algorithm’s performance. The application
    of more sophisticated learning algorithms may also improve the model’s performance
    somewhat, but this is also subject to diminishing returns, as there exists only
    a finite number of potential methods to apply and their performance differences
    tend to be relatively minor. Consequently, if additional performance gains are
    truly necessary, we are left with one remaining option: increasing the size of
    the training dataset with additional features or examples. Moreover, because adding
    additional columns would require revising data generated by past business processes,
    in many cases, collecting more rows is the easier option of the two.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, there is a relatively low ceiling on the performance gains achievable
    via the inclusion of more rows of data. Most algorithms described in this book
    plateau quickly and will perform little better on a dataset of 1 million rows
    than on a dataset containing a few thousand. You may have already observed this
    firsthand if you’ve applied machine learning methods to real-world projects in
    your own areas of interest. Once a dataset is big enough—often just a few thousand
    rows for many real-world applications—additional examples merely cause additional
    problems, such as extended computation time and running out of memory. If more
    data causes more problems, then the natural follow-up question is, why is there
    so much hype around the so-called “big data” era?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we must first begin by making a philosophical distinction
    between datasets of various sizes. To be clear, “big data” does not merely imply
    a large number of rows or a large amount of storage consumed in a database or
    filesystem. In fact, it comprises both of these and more, as size is just one
    of four elements that may indicate the presence of big data.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the so-called **four V’s of big data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: The literal size of the data, whether it be more rows, more columns,
    or more storage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: The speed at which data accumulates, which impacts not only the
    volume but also the complexity of data processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: The differences in types or definitions of data across different
    systems, particularly the addition of unstructured sources such as text, images,
    and audio data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: The trustworthiness of the input data and the ability to match
    data across sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reading this list from top to bottom, the elements become less intuitively
    obvious, yet are more challenging to handle when encountered. The first two elements,
    volume and velocity, are the basis of what might be dubbed the **medium data**
    space. While this is not to say that there aren’t challenges working with high-volume,
    high-velocity data, these challenges can often be solved by scaling up what we
    are already doing. For example, it may be possible to use faster computers with
    more memory or apply a more computationally efficient algorithm. The presence
    of a greater variety and reduced veracity of data requires a completely different
    approach for use in machine learning projects, especially at high-velocity and
    high-volume scales. The following table lists some of the distinctions between
    the small, medium, and big data spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Most machine learning projects are on the scale of “medium data”
    while additional skills and tools are required to make use of “big data”'
  prefs: []
  type: TYPE_NORMAL
- en: Moving from small to medium and then from medium to big data requires exponential
    investment. As datasets increase in size and complexity, the required infrastructure
    becomes much more complex, adding increasingly specialized databases, computing
    hardware, and analysis tools, some of which will be covered in *Chapter 15*, *Making
    Use of Big Data*. These tools are rapidly changing, which necessitates constant
    training and re-skilling. With the increased scale of data, time becomes a more
    significant constraint; not only are the projects more complex with many more
    moving pieces, requiring more cycles of iteration and refinement, but the work
    simply takes longer to complete—literally! A machine learning algorithm that runs
    in minutes on a medium-sized dataset may take hours or days on a much larger dataset,
    even with the benefit of cloud computing power.
  prefs: []
  type: TYPE_NORMAL
- en: Given the high stakes of big data, there is often an order of magnitude difference
    in how such projects are staffed and resourced—it is simply considered part of
    “the cost of doing business.” There may be dozens of data scientists, with matching
    numbers of IT professionals supporting the required infrastructure and data processing
    pipeline. Typical big data solutions require numerous tools and technologies to
    work together. This creates an opportunity for **data architects** to plan and
    structure the various computing resources and to monitor their security, performance,
    and cloud hosting costs. Similarly, data scientists are often matched by an equal
    or greater number of **data engineers**, who are responsible for piping data between
    sources and doing the most complex programming work. Their efforts in processing
    large datasets allow data scientists to focus on analysis and machine learning
    model building.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of those working on the largest and most challenging machine
    learning projects today, many everyday projects, including nearly all the examples
    covered in this book, fall squarely into what has been called a **small data regime**.
    In this paradigm, datasets can grow to be “large” in terms of the number of rows
    or in sheer storage volume, but they will never truly be “big data.” Computer
    science and machine learning expert Andrew Ng has noted that in the realm of small
    data, the role of the human is still impactful; the human can greatly impact a
    project’s performance via hand-engineering features or by the selection of the
    most performant learning algorithm. However, as a dataset grows beyond “large”
    and into “huge” sizes and into the **big data regime**, a different class of algorithms
    breaks through the performance plateau to surpass the small gains of manual tweaks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.3*, which is adapted from Ng’s work, illustrates this phenomenon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: In the small data regime, traditional machine learning algorithms
    are competitive with and may even perform better than more complex methods, which
    perform much better as the size of data increases'
  prefs: []
  type: TYPE_NORMAL
- en: Within the confines of the small data regime, no single algorithm or class of
    algorithms performs predictably better than the others. Here, clever feature engineering
    including subject-matter expertise and hand-coded features may allow simpler algorithms
    to outperform much more sophisticated approaches or deep learning neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: As the size of data increases to the medium data regime, ensemble approaches
    (described in *Chapter 14*, *Building Better Learners*) tend to perform better
    than even a carefully handcrafted model that uses traditional machine learning
    algorithms. For the largest datasets found in the big data regime, only deep learning
    neural networks (introduced in *Chapter 7*, *Black-Box Methods – Neural Networks
    and Support Vector Machines*, and to be covered in more detail in *Chapter 15*,
    *Making Use of Big Data*) appear to be capable of the utmost performance, as their
    capability to learn from additional data practically never plateaus. Does this
    imply that the “no free lunch” theorem is incorrect and there truly is one learning
    algorithm to rule them all?
  prefs: []
  type: TYPE_NORMAL
- en: The visualization of the performance of different learning algorithms under
    the small and big data regimes can be found described by Andrew Ng in his own
    words. To find these, simply search YouTube for “Nuts and Bolts of Applying Deep
    Learning” (appears 3 minutes into the video) or “Artificial Intelligence is the
    New Electricity” (appears 20 minutes into the video).
  prefs: []
  type: TYPE_NORMAL
- en: To understand why certain algorithms perform better than others under the big
    data regime and why the “no free lunch” principle still applies, we must first
    consider the relationship between the size and complexity of the data, the capability
    of a model to learn a complex pattern, and the risk of overfitting. Let’s begin
    by considering a case in which the size and complexity of the data is held constant,
    but we increase the complexity of the learning algorithm to more closely model
    what is observed in the training data. For example, we may grow a decision tree
    to an overly large size, increase the number of predictors in a regression model,
    or add hidden nodes in a neural network. This relationship is closely linked to
    the idea of the bias-variance trade-off; by increasing the model complexity, we
    allow the model to conform more closely to the training data and, therefore, reduce
    its inherent bias and increase its variance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.4* illustrates the typical pattern that occurs as models increase
    in complexity. Initially, when the model is underfitted to the training dataset,
    increases in model complexity lead to reductions in model error and increases
    in model performance. However, there is a point at which increases in model complexity
    contribute to overfitting the training dataset. Beyond this point, although the
    model’s error rate on the training dataset continues to be reduced, the test set
    error rate increases, as the model’s ability to generalize beyond training is
    dramatically hindered. Again, this assumes a limit to the dataset’s ability to
    support the model’s increasing complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: For many training datasets, increasing the complexity of the learning
    algorithm runs the risk of overfitting and increased test set error'
  prefs: []
  type: TYPE_NORMAL
- en: If we can increase the size and scope of the training dataset, the big data
    regime may unlock a second tier of machine learning performance, but only if the
    learning algorithm is likewise capable of increasing its complexity to make use
    of the additional data. Many traditional algorithms, such as those covered so
    far in this book, are incapable of making such an evolutionary leap—at least not
    without some extra help.
  prefs: []
  type: TYPE_NORMAL
- en: The missing link between the traditional machine learning algorithms and those
    capable of making this leap has to do with the number of parameters that the algorithms
    attempt to learn about the data. Recall that in *Chapter 11*, *Being Successful
    with Machine Learning*, parameters were described as the learner’s internal values
    that represent its abstraction of the data. Traditionally, for a variety of reasons,
    including the bias-variance trade-off depicted above, as well as the belief that
    simpler, more parsimonious models should be favored over more complex ones, models
    with fewer parameters have been favored. It was assumed that increasing the number
    of parameters too high would allow the dataset to simply memorize the training
    data, leading to severe overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this is true, but only to a point, as *Figure 12.5* depicts.
    As model complexity—that is, the number of parameters—increases, the test set
    error follows the same U-shaped pattern as before. However, a new pattern emerges
    once complexity and parameterization have reached the **interpolation threshold**,
    or the point at which there are enough parameters to memorize and accurately classify
    virtually all the training set examples. At this threshold, generalization error
    is at its maximum, as the model has been greatly overfitted to the training data.
    However, as model complexity increases even further, test set error once again
    begins to drop. With sufficient additional complexity, a heavily overfitted model
    may even surpass the performance of a well-tuned traditional model, at least according
    to our existing notion of “overfitted.”
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B17290_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Some algorithms are able to make use of big data to generalize
    well even after they seemingly overfit the training data'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the apparent contradiction of the “double descent”
    curve depicted here, see this groundbreaking paper: *Reconciling modern machine-learning
    practice and the classical bias–variance trade-off, Belkin M, Hsu D, Ma S, and
    Mandal S, 2019, Proceedings of the National Academy of Sciences, Vol. 116(32),
    pp. 15,849-15,854*.'
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism that explains this unexpected result has to do with an interesting
    and perhaps even magical transformation that occurs in models capable of additional
    parameterization beyond the interpolation threshold. Once a learner has sufficient
    parameters to interpolate (to sufficiently conform to) the training data, additional
    parameters lead to a state of **overparameterization**, in which the additional
    complexity enables higher levels of thinking and abstraction. In essence, an overparameterized
    learner is capable of learning higher-order concepts; in practice, this means
    it is capable of learning how to engineer features or learning how to learn. A
    significant jump in model complexity beyond the interpolation threshold is likely
    to lead to a significant leap in the way the algorithm approaches the problem,
    but of course, not every algorithm is capable of this leap.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks, which can add additional complexity endlessly and trivially
    via the addition of hidden nodes arranged in layers, are the ideal candidate for
    consuming big data. As you will learn in *Chapter 15*, *Making Use of Big Data*,
    a cleverly designed neural network can engineer its own features out of unstructured
    data such as images, text, or audio. Similarly, its designation as a universal
    function approximator implies that it can identify the best functional form to
    model any pattern it identifies in the data. Thus, we must once again revisit
    the early question of how exactly this doesn’t violate the principle of “no free
    lunch.” It would appear that for datasets of sufficient size, deep learning neural
    networks are the single best approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting a couple of practical issues aside—notably, the fact that most real-world
    projects reside in the small data regime and the fact that deep neural networks
    are computationally expensive and difficult to train—a key reason that deep learning
    doesn’t violate the “no free lunch” principle is based on the fact that once the
    neural network becomes large and substantially overparameterized, and assuming
    it has access to a sufficiently large and complex training dataset, it ceases
    to be a single learning *algorithm* and instead becomes a generalized learning
    *process*. If this seems like a distinction without a difference, perhaps a metaphor
    will help: rather than providing us with a free lunch, the process of deep learning
    provides an opportunity to teach the algorithm how to make its own lunch. Given
    the limited availability of truly big data and the limited applicability of deep
    learning to most business tasks, to produce the strongest models, it is still
    necessary for the machine learning practitioner to assist in the feature engineering
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the project or circumstances, the practice of feature engineering
    may look very different. Some large, technology-focused companies employ one or
    more data engineers per data scientist, which allows machine learning practitioners
    to focus less on data preparation and more on model building and iteration. Certain
    projects may rely on very small or very massive quantities of data, which may
    preclude or necessitate the use of deep learning methods or automated feature
    engineering techniques. Even projects requiring little initial feature engineering
    effort may suffer from the so-called “last mile problem,” which describes the
    tendency for costs and complexity to be disproportionally high for the small distances
    to be traveled for the “last mile” of distribution. Relating this concept to feature
    engineering implies that even if most of the work is taken care of by other teams
    or automation, a surprising amount of effort may still be required for the final
    steps of preparing the data for the model.
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that the bulk of real-world machine learning projects today require
    a substantial amount of feature engineering. Most companies have yet to achieve
    the level of analytics maturity at the organizational level needed to allow data
    scientists to focus solely on model building. Many companies and projects will
    never achieve this level due to their small size or limited scope. For many small-to-mid-sized
    companies and small-to-mid-sized projects, data scientists must take the lead
    on all aspects of the project from start to finish. Consequently, it is necessary
    for data scientists to understand the role of the feature engineer and prepare
    to perform this role if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, feature engineering is more art than science and requires
    as much imagination as it does programming skills. In a nutshell, the three main
    goals of feature engineering might be described as:'
  prefs: []
  type: TYPE_NORMAL
- en: Supplementing what data is already available with additional external sources
    of information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the data to conform to the machine learning algorithm’s requirements
    and to assist the model with its learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating the noise while minimizing the loss of useful information— conversely,
    maximizing the use of available information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overall mantra to keep in mind when practicing feature engineering is “be
    clever.” One should strive to be a clever, frugal data miner and try to think
    about the subtle insights that you might find in every single feature, working
    systematically, and avoiding letting any data go to waste. Applying this rule
    serves as a reminder of the requisite creativity and helps to inspire the competitive
    spirit needed to build the strongest-performing learners.
  prefs: []
  type: TYPE_NORMAL
- en: Although each project will require you to apply these skills in a unique way,
    experience will reveal certain patterns that emerge in many types of projects.
    The sections that follow, which provide seven “hints” for the art of feature engineering,
    are not intended to be exhaustive but, rather, provide a spark of inspiration
    on how to think creatively about making data more useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'There has been an unfortunate dearth of feature engineering books on the market,
    until recently, when a number have been published. Two of the earliest books on
    this subject are Packt Publishing’s *Feature Engineering Made Easy* (Ozdemir &
    Susara, 2018) and O’Reilly’s *Feature Engineering for Machine Learning* (Zheng
    & Casari, 2018). The book *Feature Engineering and Selection* (Kuhn & Johnson,
    2019) is also a standout and even has a free version, available on the web at:
    [http://www.feat.engineering](http://www.feat.engineering).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 1: Brainstorm new features'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The choice of topic for a new machine learning project is typically motivated
    by an unfulfilled need. It may be motivated by a desire for more profit, to save
    lives, or even simple curiosity, but in any case, the topic is almost surely not
    selected at random. Instead, it relates to an issue at the core of the company,
    or a topic held dear by the curious, both of which suggest a fundamental interest
    in the work. The company or individual pursuing the project is likely to already
    know a great deal about the subject and the important factors that contribute
    to the outcome of interest. With this domain experience and subject-matter expertise,
    the company, team, or individual that commissioned the project is likely to hold
    proprietary insights about the task that they alone can bring.
  prefs: []
  type: TYPE_NORMAL
- en: To capitalize on these insights, at the beginning of a machine learning project,
    just prior to feature engineering, it can be helpful to conduct a brainstorming
    session in which stakeholders are gathered and ideas are generated about the potential
    factors that are associated with the outcome of interest. During this process,
    it is important to avoid limiting yourself to what is readily available in existing
    datasets. Instead, consider the process of cause-and-effect at a more abstract
    level, imagining the various metaphorical “levers” that can be pulled in order
    to impact the outcome in a positive or negative direction. Be as thorough as possible
    and exhaust all ideas during this session. If you could have literally anything
    you wanted in the model, what would be most useful?
  prefs: []
  type: TYPE_NORMAL
- en: The culmination of a brainstorming session may be a **mind map**, which is a
    method of diagramming ideas around a central topic. Placing the outcome of interest
    at the center of the mind map, the various potential predictors radiate out from
    the central theme, as shown in the following example of a mind mapping session
    designing a model to predict heart disease mortality.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mind map diagram may use a hierarchy to link associated concepts or group
    factors that are related in a similar data source:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Mind maps can be useful to help imagine the factors that contribute
    to an outcome'
  prefs: []
  type: TYPE_NORMAL
- en: While constructing the mind map, you may determine that some of the desired
    features are unavailable in the existing data sources. Perhaps the brainstorming
    group can help identify alternative sources of these data elements or find someone
    willing to help gather them. Alternatively, it may be possible to develop a **proxy
    measure** that effectively measures the same concept using a different method.
    For example, it may be impossible or practically infeasible to directly measure
    someone’s diet, but it may be possible to use their social media activity as a
    proxy by counting the number of fast-food restaurants they follow. This is not
    perfect, but it is something, and is certainly better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: A mind mapping session can also help reveal potential interactions between features
    in which two or more factors have a disproportionate impact on the outcome; a
    joint effect may be greater (or lesser) than the sum of its parts. In the heart
    disease example, one might hypothesize that the combined effect of stress and
    obesity is substantially more likely to cause heart disease than the sum of their
    separate effects. Algorithms such as decision trees and neural networks can find
    these interaction effects automatically, but many others cannot, and in either
    case, it may benefit the learning process or result in a simpler, more interpretable
    model if these combinations are coded explicitly in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 2: Find insights hidden in text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the richest sources of hidden data and, therefore, one of the most fruitful
    areas for feature engineering is text data. Machine learning algorithms are generally
    not very good at realizing the full value of text data because they lack the external
    knowledge of semantic meaning that a human has gained over a lifetime of language
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, given a tremendous amount of text data, a computer may be able to
    learn the same thing, but this is not feasible for many projects, and would greatly
    add to a project’s complexity. Furthermore, text data cannot be used as-is, as
    it suffers from the curse of dimensionality; each block of text is unique and,
    therefore, serves as a form of fingerprint linking the text to an outcome. If
    used in the learning process, the algorithm will severely overfit or ignore the
    text data altogether.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality applies to unstructured “big” data more generally
    as image and audio data are likewise difficult to use directly in machine learning
    models. *Chapter 15*, *Making Use of Big Data*, covers some methods that allow
    these types of data sources to be used with traditional machine learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The humans in charge of constructing features for the learning algorithm can
    add insight to the text data by coding reduced-dimensionality features derived
    from the interpretation of the text. In selecting a small number of categories,
    the implicit meaning is made explicit. For example, in a customer churn analysis,
    suppose a company has access to the public Twitter timeline for its customers.
    Each customer’s tweets are unique, but a human may be able to code them into three
    categories of positive, negative, and neutral. This is a simple form of **sentiment
    analysis**, which analyzes the emotion of language. Computer software, including
    some R packages, may be able to help automate this process using models or rules
    designed to understand simple semantics. In addition to sentiment analysis, it
    may be possible to categorize text data by topic; in the churn example, perhaps
    customers tweeting about customer service are more likely to switch to another
    company than customers tweeting about price.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many R packages that can perform sentiment analysis, some of which
    require subscriptions to paid services. To get started quickly and easily, check
    out the aptly named `SentimentAnalysis` and `RSentiment` packages, as well as
    the `Syuzhet` package. All of these can classify sentences as positive or negative
    with just a couple of lines of R code. For a deeper dive into text mining and
    sentiment analysis, see the book *Text Mining with R: A Tidy Approach, 2017, Silge
    J and Robinson D*, which is available on the web at [https://www.tidytextmining.com](https://www.tidytextmining.com).
    Additionally, see *Text Mining in Practice with R, 2017, Kwartler T*.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond coding the overt meaning of text, one of the subtle arts of feature engineering
    involves finding the covert insights hidden in the text data. In particular, there
    may be useful information encoded in the text that is not related to the direct
    interpretation of the text, but it appears in the text coincidentally or accidentally,
    like a “tell” in the game of poker—a micro-expression that reveals the player’s
    secret intentions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden text data may help reveal aspects of a person’s identity, such as age,
    gender, career level, location, wealth, or socioeconomic status. Some examples
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Names and salutations such as Mr. and Mrs., or Jr. and Sr., traditional and
    modern names, male and female names, or names associated with wealth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job titles and categories such as CEO, president, assistant, senior, or director
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographic and spatial codes such as postal codes, building floor numbers, foreign
    and domestic regions, first-class tickets, PO boxes, and similar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic markers such as slang or other expressions that may reveal pertinent
    aspects of identities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To begin searching for these types of hidden insights, keep the outcome of interest
    in mind while systematically reviewing the text data. Read as many of the texts
    as possible while thinking about any way in which the text might reveal a subtle
    clue that could impact the outcome. When a pattern emerges, construct a feature
    based on the insight. For instance, if the text data commonly includes job titles,
    create rules to classify the jobs into career levels such as entry-level, mid-career,
    and executive. These career levels could then be used to predict outcomes such
    as loan default or churn likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 3: Transform numeric ranges'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain learning algorithms are more capable than others of learning from numeric
    data. Among algorithms that can utilize numeric data at all, some are better at
    learning the important cut points in the range of numeric values or are better
    at handling severely skewed data. Even a method like decision trees, which is
    certainly apt at using numeric features, has a tendency to overfit on numeric
    data and, thus, may benefit from a transformation that reduces the numeric range
    into a smaller number of potential cut points. Other methods like regression and
    neural networks may benefit from nonlinear transformations of numeric data, such
    as log scaling, normalization, and step functions.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these methods have been covered and applied in prior chapters. For example,
    in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*, we
    considered the technique of discretization (also known as “binning” or “bucketing”)
    as a means of transforming numeric data into categorical data so that it could
    be used by the naive Bayes algorithm. This technique is also sometimes useful
    for learners that can handle numeric data natively, as it can help clarify a decision
    boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates this process for a hypothetical model predicting
    heart disease, using a numeric age predictor. On the left, we see that as the
    numeric age increases, the darker the color becomes, indicating a greater prevalence
    of heart disease with increasing age. Despite this seemingly clear trend, a decision
    tree model may struggle to identify an appropriate cut point and it may do so
    arbitrarily, or it may choose numerous small cut points; both of these are likely
    to be overfitted to the training data. Instead of leaving this choice to the model,
    it may be better to use *a priori* knowledge to create predefined groups for “young”
    and “old” patients. Although this loses some of the nuances of the true underlying
    gradient, it may help the model generalize better to future data by trading the
    decision tree’s “high variance” approach for a “high bias” approach of theory-driven
    discretization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Discretization and other numeric transformations can help learners
    identify patterns more easily'
  prefs: []
  type: TYPE_NORMAL
- en: In general, for datasets containing numeric features, it may be worth exploring
    each feature systematically, while also considering the learning algorithm’s approach
    to numeric data, to determine whether a transformation is necessary. Apply any
    domain or subject-matter expertise you may have to inform the creation of bins,
    buckets, step points, or nonlinear transformations in the final version of the
    feature. Even though many algorithms are capable of handling numeric data without
    recoding or transformation, additional human intelligence may help guide the model
    to a better overall fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 4: Observe neighbors’ behavior'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the lesser-known methods of surfacing hidden insights during feature
    engineering is to apply the common knowledge that “birds of a feather flock together.”
    We applied this principle to prediction in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*, but it is also a useful mindset for identifying useful
    predictors. The idea hinges on the fact that there may be explicit or implicit
    groupings across the dataset’s rows, and there may be insights found by examining
    how one example relates to the others in its neighborhood or grouping.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an explicit grouping found often in real-world data is households.
    Many datasets include not only rows based on individuals but also a household
    identifier, which allows you to link rows into household groups and, thus, create
    new features based on the groups’ compositions.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, knowing that someone is in a household may provide an indication
    of marital status and the number of children or dependents, even if these features
    were not included in the original individual-level dataset. Simply counting or
    aggregating some of the group’s features can result in highly useful predictors.
  prefs: []
  type: TYPE_NORMAL
- en: From here, it is also possible to share information among records in the groups.
    For instance, knowing one spouse’s income is helpful, but knowing both provides
    a better indication of the total available income. Measures of variance within
    a group can also be enlightening. There may be aspects of households that provide
    a bonus effect if the partners match or disagree on certain attributes; for example,
    if both partners report satisfaction with a particular telephone company, they
    may be especially loyal compared to households where only one member is satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: These principles also apply to less obvious but still explicit groupings, like
    postal codes or geographic regions. By collecting the rows falling into the group,
    one can count, sum, average, take the maximum or minimum value, or examine the
    diversity within the group to construct new and potentially useful predictors.
    Groups with more or less agreement or diversity may be more or less robust to
    certain outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: There may be value in identifying implicit groupings as well—that is, a grouping
    not directly coded in the dataset. Clustering methods, such as those described
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, are one potential
    method of finding these types of groupings, and the resulting clusters can be
    used directly as a predictor in the model. For example, in a churn project, using
    clusters as features for the model may reveal that some clusters are more likely
    to churn than others. This may imply that churn is related to the cluster’s underlying
    demographics, or that churn is somewhat contagious among cluster members.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if birds of a feather flock together, it makes sense to borrow
    leading indicators from the experiences of similar neighbors—they may have a similar
    reaction to some external factor or may directly influence one another. Implicit
    groups that exhibit rare or unique traits may be interesting in themselves; perhaps
    some are the bellwether or “canary in the coal mine”—trendsetters that respond
    to change earlier than other groups. Observing their behavior and coding these
    groups explicitly into the model may improve the model’s predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: If you do use information from neighbors (or from related rows, as described
    in the next section), beware of the problem of data leakage, which was described
    in *Chapter 11*, *Being Successful with Machine Learning*. Be sure to only engineer
    features using information that will be available at the time of prediction when
    the model is deployed. For example, it would be unwise to use both spouses’ data
    for a credit scoring model if only one household member completes the loan application
    and the other spouse’s data is added after the loan is approved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 5: Utilize related rows'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The practice of utilizing “follow the leader” behavior as hinted in the previous
    section can be especially powerful given the related rows of time series datasets,
    where the same attribute is measured repeatedly at different points in time. Data
    that contains repeated measures offers many such additional opportunities to construct
    useful predictors. Whereas the previous section considered grouping related data
    *across* the unit of analysis, the current section considers the value of grouping
    related observations *within* the unit of analysis. Essentially, by observing
    the same units of analysis repeatedly, we can examine their prior trends and make
    better predictions of the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting the hypothetical churn example, suppose we have access to the past
    24 months of data from subscribers to an online video streaming service. The unit
    of observation is the customer-month (one row per customer per month), while our
    unit of analysis is the customer. Our goal is to predict which customers are most
    likely to churn so that we might intervene. To construct a dataset for machine
    learning, we must collect the units of observation and aggregate them into one
    row per customer. Here is where feature engineering is especially needed. In the
    process of “rolling up” the historical data into a single row for analysis, we
    can construct features that examine trends and loyalty, asking questions such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the customer’s average monthly activity greater than or less than their peers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the customer’s monthly activity over time? Is it up, down, or stable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How frequent is their activity? Are they loyal? Is their loyalty stable across
    months?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How consistent is the customer’s behavior? Does the behavior vary a lot from
    month to month?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are familiar with basic calculus, it may help to reflect on the concept
    of the first and second derivative, as both can be useful features in a time series
    model. The first derivative here refers to the velocity of the behavior—that is,
    the behavior count over a unit of time. For example, we may compute the number
    of dollars spent per month on the streaming service, or the number of television
    shows and movies streamed per month. These are useful predictors alone, but they
    can be made even more useful in the context of the second derivative, which is
    the acceleration (or deceleration) of the behavior. The acceleration is the change
    in velocity over time, such as the change in monthly spending or the change in
    the shows streamed per month. High-velocity customers with high spending and usage
    might be less likely to churn, but a rapid deceleration (that is, a large reduction
    in usage or spending) from these same customers might indicate an impending churn.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to velocity and acceleration, measures of consistency, reliability,
    and variability can be constructed to further enhance predictive ability. A very
    consistent behavior that suddenly changes may be more concerning than a wildly
    varying behavior that changes similarly. Calculating the proportion of recent
    months with a purchase, or with spending or behavior meeting a given threshold,
    provides a simple loyalty metric, but more sophisticated measures using variance
    are also possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 6: Decompose time series'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The repeated measures time series data described in the previous section, with
    multiple related rows per unit of analysis, is said to be in the **long format**.
    This contrasts with the type of data required for most R-based machine learning
    methods. Unless a learning algorithm is designed to understand the related rows
    of repeated measures data, it will require time series data to be specified in
    the **wide format**, which transposes the repeated rows of data into repeated
    columns. For example, if a weight measurement is recorded monthly for 3 months
    for 1,000 patients, the long-format dataset will have 3 * 1,000 = 3,000 rows and
    3 columns (patient identifier, month, and weight). As depicted in *Figure 12.8*,
    the same dataset in wide format would contain only 1,000 rows but 4 columns: 1
    column for the patient identifier, and 3 columns for the monthly weight readings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Most machine learning models require long-format time series data
    to be transformed into a wide format'
  prefs: []
  type: TYPE_NORMAL
- en: To construct a wide format dataset, one must first determine how much history
    will be useful for prediction. The more history that is needed, the more columns
    that will need to be added to the wide dataset. For example, if we wanted to forecast
    a customer’s energy usage 1 month into the future, we may decide to use their
    prior 12 months of energy use as predictors so that a full year of seasonality
    would be covered. Therefore, to build a model forecasting energy usage in June
    2023, we might create 12 predictor columns measuring energy use in May 2023, April
    2023, March 2023, and so on, for each of the 12 months prior to June 2023\. A
    13th column would be the target or dependent variable, recording the actual energy
    usage in June 2023\. Note that a model trained upon this dataset would learn to
    predict energy use in June 2023 based on data in the months from June 2022 to
    May 2023, but it would not be able to predict other future months because the
    target and predictors are linked to specific months.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a better approach is to construct **lagged variables**, which are computed
    relative to the target month. The lagged variables are essentially measures that
    are delayed in time to be carried forward to a later, more recent row in the dataset.
    A model using lagged variables can be retrained on a rolling, monthly basis as
    additional months of data become available over time. Rather than having column
    names like `energy_june2023` and `energy_may2023`, the resulting dataset will
    have names that indicate the relative nature of the measurements, such as `energy_lag0`,
    `energy_lag1`, and `energy_lag2`, which indicate the energy use in the current
    month, the prior month, and 2 months ago. This model will always be applied to
    the most recent data to predict the forthcoming time period.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.9* visualizes this approach. Each month, a model is trained on the
    past 13 months of data; the most recent month is used for the target or dependent
    variable (denoted as DV) while the earlier 12 months are used as lagged predictors.
    The model can then be used to predict the future month, which has not yet been
    observed. Each successive month following the first shifts the rolling window
    1 month forward, such that data older than 13 months is unused in the model. A
    model trained using data constructed in this way does not learn the relationship
    between specific calendar months, as was the case with the non-lagged variables;
    rather, it learns how prior behavior relates to future behavior, regardless of
    the calendar month.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B17290_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Constructing lagged predictors is one method to model time series
    data'
  prefs: []
  type: TYPE_NORMAL
- en: A problem with this approach, however, is that this method has disregarded calendar
    time, yet certain calendar months may have an important impact on the target variable.
    For example, energy use may be higher in winter and summer than in spring and
    fall, and thus, it would be beneficial for the model to know not only the relationship
    between past and future behavior but also to gain a sense of seasonal effects,
    or other patterns broader than the local patterns, within the rows related to
    the unit of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might imagine that the value of the target to be predicted is composed
    of three sources of variation, which we would like to decompose into features
    for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: There is the local or internal variation, which is based on the attributes unique
    to the unit of analysis. In the example of forecasting energy demand, the local
    variation may be related to the size and construction of the household, the residents’
    energy needs, where the house is located, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There may be broader global trends, such as fuel prices or weather patterns,
    that affect the energy use of most households.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There may be seasonal effects, independent of the local and global effects,
    that explain changes in the target. This is not limited to the annual weather
    patterns mentioned before, but any cyclical or predictable pattern can be considered
    a seasonal effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some specific examples relevant to the energy forecasting project may include
    higher or lower demand on:'
  prefs: []
  type: TYPE_NORMAL
- en: Different days of the week, particularly weekdays versus weekends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Religious or government holidays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional school or business vacation periods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mass gatherings such as sporting events, concerts, and elections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If local, global, and seasonal features can be incorporated into the training
    dataset as predictors, the model can learn their effect on the outcome. The challenge
    thereafter is twofold: subject-matter knowledge or data exploration is required
    to identify the important seasonal factors, and there must be ample training data
    for the target to be observed in each of the included seasons. The latter implies
    that the training data should be composed of more than a single month cross-section
    of time; lacking this, the learning algorithm will obviously be unable to discover
    the relationship between the seasons and the target!'
  prefs: []
  type: TYPE_NORMAL
- en: Though it would seem to follow that we should revert to the original long-format
    data, this is actually not the case. In fact, the wide data with lagged variables
    from each month can be stacked in a single unified dataset with multiple rows
    per unit of analysis. Each row indicates an individual at a particular moment
    in time, with a target variable measuring the outcome at that moment, and a wide
    set of columns that have been constructed as lagged variables for periods of time
    prior to the target. Additional columns can also be added to further widen the
    matrix and decompose the various components of time variance, such as indicators
    for seasons, days of the week, and holidays; these columns will indicate whether
    the given row falls within one of these periods of interest.
  prefs: []
  type: TYPE_NORMAL
- en: The figure that follows depicts a hypothetical dataset using this approach.
    Each household (denoted by the `household_id` column) can appear repeatedly with
    different values of the target (`energy_use`) and predictors (`season`, `holiday_month`,
    `energy_lag1`, and so on). Note that the lag variables are missing (as indicated
    by the `NA` values) for the first few rows of the dataset, which means that these
    rows cannot be used for training or prediction. The remaining rows, however, can
    be used with any machine learning method capable of numeric prediction, and the
    trained model will readily forecast next month’s energy use given the row of data
    for the current month.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Datasets including historical data may include both seasonal
    effects and lagged predictors'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before rushing into modeling time series data, it is crucial to understand
    an important caveat about the data preparation methods described here: because
    the rows from repeated observations from the same unit of analysis are related
    to one another, including them in the training data violates the assumption of
    independent observations for methods like regression. While models built upon
    such data may still be useful, other methods for formal time series modeling may
    be more appropriate, and it is best to consider the methods described here as
    a workaround to perform forecasting with the machine learning methods previously
    covered. Linear mixed models and recurrent neural networks are two potential approaches
    that can handle this type of data natively, although both methods are outside
    the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: The `lme4` package is used to build mixed models in R, but it would be unwise
    to jump in without understanding the statistical underpinnings of these types
    of models; they are a significant step up in complexity over traditional regression
    modeling. The book *Linear Mixed-Effects Models Using R* (Gałecki & Burzykowski,
    2013) provides the theoretical background needed to build this type of model.
    To build recurrent neural networks, R may not be the right tool for the job, as
    specialized tools exist for this purpose. However, the `rnn` package can build
    simple RNN models for time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint 7: Append external data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the teaching examples in this book, when a machine learning project begins
    in the real world, a dataset cannot simply be downloaded from the internet with
    prebuilt features and examples describing the topic of interest. It is unfortunate
    how many deeply interesting projects are killed before they begin for this simple
    reason. Businesses hoping to predict customer churn realize they have no historical
    data from which a model can be built; students hoping to optimize food distribution
    in poverty-stricken areas are limited by the scarce amounts of data from these
    areas; and countless projects that might increase profits or change the world
    for the better are stunted before they start. What begins as excitement around
    a machine learning project soon fizzles out due to the lack of data.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than ending with discouragement, it is better to channel this energy
    into an effort to create the necessary data from scratch. This may mean dialing
    colleagues on the telephone or firing off a series of email messages to connect
    with those that can grant access to databases containing relevant pieces of data.
    It may also require rolling up your sleeves and getting your hands dirty. After
    all, we live in the so-called big data era where data is not only plentiful but
    easily recorded, with assistance from electronic sensors and automated data entry
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17290_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Little effort is often sufficient to generate datasets useful
    for machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: In the worst case, an investment of time, effort, and imagination can build
    useful datasets from nothing. Typically, this is easier than one might think.
    The previous figure illustrates several cases in which I created datasets to satisfy
    my own curiosity.
  prefs: []
  type: TYPE_NORMAL
- en: Fascinated by autonomous vehicles, I drove around my neighborhood and took pictures
    of road signs to build a stop sign classification algorithm. To predict used car
    prices, I copied and pasted hundreds of listings from used car websites. And,
    to understand exactly when and why names rhyming with “Aiden” became so popular
    in the United States, I gathered dozens of years of data from the Social Security
    baby name database. None of these projects required more than a few hours of effort,
    but enrolling friends, colleagues, or internet forums as a form of crowdsourcing
    the effort or even paying for data entry assistance could have parallelized the
    task and helped my database grow larger or faster. Paid services like Amazon Mechanical
    Turk ([https://www.mturk.com](https://www.mturk.com)) provide an affordable means
    of distributing large and tedious data entry or collection tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To further enrich existing datasets, there is often the potential to append
    additional features from external sources. This is especially true when the main
    dataset of interest includes geographic identifiers such as postal codes, as many
    publicly available databases measure attributes for these regions. Of course,
    a postal code-level dataset will not reveal a specific individual’s exact characteristics;
    however, it may provide insight into whether the average person in the area is
    wealthier, healthier, younger, or more likely to have kids, among numerous other
    factors that may help improve the quality of a predictive model. These types of
    data can be readily found on many governmental agency websites and downloaded
    at no charge; simply merge them onto the main dataset for additional possible
    predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, many social media companies and data aggregator services like Facebook,
    Zillow, and LinkedIn provide free access to limited portions of their data. Zillow,
    for example, provides home value estimates for postal code regions. In some cases,
    these companies or other vendors may sell access to these datasets, which can
    be a powerful means of augmenting a predictive model. In addition to the financial
    cost of such acquisitions, they often pose a significant challenge in terms of
    **record linkage**, which involves matching entities across datasets that share
    no common unique identifier. Solving this problem involves building a **crosswalk**
    table, which maps each row in one source to the corresponding row in the other
    source. For instance, the crosswalk may link a person identified by a customer
    identification number in the main dataset to a unique website URL in an external
    social media dataset. Although there are R packages such as `RecordLinkage` that
    can help perform such matching across sources, these rely on heuristics that may
    not perform as well as human intelligence and require significant computational
    expense, particularly for large databases. In general, it is safe to assume that
    record linkage is often costly from human resource and computational expense perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: When considering whether to acquire external data, be sure to research the source’s
    terms of use, as well as your region’s laws and organizational rules around using
    such sources. Some jurisdictions are stricter than others, and many rules are
    becoming stricter over time, so it is important to keep up to date on the legality
    and liability associated with outside data.
  prefs: []
  type: TYPE_NORMAL
- en: Given the work involved in advanced data preparation, R itself has evolved to
    keep up with the new demands. Historically, R was notorious for struggling with
    very large and complex datasets, but over time, new packages have been developed
    to address these shortcomings and make it easier to perform the types of operations
    described so far in this chapter. In the remainder of this chapter, you will learn
    about these packages, which modernize the R syntax for real-world data challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring R’s tidyverse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new approach has rapidly taken shape as the dominant paradigm for working
    with data in R. Championed by Hadley Wickham—the mind behind many of the packages
    that drove much of R’s initial surge in popularity—this new wave is now backed
    by a much larger team at Posit (formerly known as RStudio). The company’s user-friendly
    RStudio Desktop application integrates nicely into this new ecosystem, known as
    the **tidyverse**, because it provides a universe of packages devoted to tidy
    data. The entire suite of tidyverse packages can be installed with the `install.packages("tidyverse")`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: A growing number of resources are available online to learn more about the tidyverse,
    starting with its homepage at [https://www.tidyverse.org](https://www.tidyverse.org).
    Here, you can learn about the various packages included in the set, a few of which
    will be described in this chapter. Additionally, the book *R for Data Science*
    by Hadley Wickham and Garrett Grolemund is available freely online at [https://r4ds.hadley.nz](https://r4ds.hadley.nz)
    and illustrates how the tidyverse’s self-proclaimed “opinionated” approach simplifies
    data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: I am often asked the question of how R compares to Python for data science and
    machine learning. RStudio and the tidyverse are perhaps R’s greatest asset and
    point of distinction. There is arguably no easier way to begin a data science
    journey. Once you’ve learned the “tidy” way of doing data analysis, you are likely
    to wish the tidyverse functionality existed everywhere!
  prefs: []
  type: TYPE_NORMAL
- en: Making tidy table structures with tibbles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whereas the data frame is the center of the base R universe, the data structure
    at the heart of the tidyverse is found in the `tibble` package ([https://tibble.tidyverse.org](https://tibble.tidyverse.org)),
    the name of which is a pun on the word “table” as well as a nod to the infamous
    “tribble” in *Star Trek* lore. A **tibble** acts almost exactly like a data frame
    but includes additional modern functionality for convenience and simplicity. Tibbles
    can be used almost everywhere a data frame can be used. Detailed information about
    tibbles can be found by typing the command `vignette("tibble")` in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, using tibbles will be transparent and seamless, as tibbles
    can pass as a data frame in most R packages. However, in the rare case where you
    need to convert a tibble to a data frame, use the `as.data.frame()` function.
    To go in the other direction and convert a data frame in to a tibble, use the
    `as_tibble()` function. Here, we’ll create a tibble from the Titanic dataset first
    introduced in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Typing the name of this object demonstrates the tibble’s cleaner and more informative
    output than a standard data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with medium confidence](img/B17290_12_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Displaying a tibble object results in more informative output
    than a standard data frame'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note the distinctions between tibbles and data frames, as
    the tidyverse will automatically create a tibble object for many of its operations.
    Overall, you are likely to find that tibbles are faster and easier to work with
    than data frames. They generally make smarter assumptions about the data, which
    means you will spend less time redoing R’s work—like recoding strings as factors
    or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, one simple distinction between tibbles and data frames is that a tibble
    never assumes `stringsAsFactors = TRUE`, which was the default behavior in base
    R until relatively recently with the release of R version 4.0\. As described in
    previous chapters, R’s `stringsAsFactors` setting sometimes led to confusion or
    programming bugs when character columns were automatically converted in to factors
    by default. Another distinction between tibbles and data frames is that, as long
    as the name is surrounded by the backtick (`` ` ``) character, a tibble can use
    non-standard column names like `` `my var` `` that violate base R’s object naming
    rules. Other benefits of tibbles are unlocked by complementary tidyverse packages,
    as described in the sections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Reading rectangular files faster with readr and readxl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nearly every chapter so far has used the `read.csv()` function to load data
    into R data frames. Although we could convert these data frames in to tibbles,
    there is a faster and more direct path to get data into the tibble format. The
    tidyverse includes the `readr` package ([https://readr.tidyverse.org](https://readr.tidyverse.org))
    for loading tabular data. This is described in the data import chapter in *R for
    Data Science* at [https://r4ds.hadley.nz/data-import.html](https://r4ds.hadley.nz/data-import.html),
    but the basic functionality is simple.
  prefs: []
  type: TYPE_NORMAL
- en: The `readr` package provides a `read_csv()` function that loads data from CSV
    files much like base R’s `read.csv()` function. A key difference, aside from the
    subtle difference in their function names, is that the tidyverse’s version is
    much speedier—and not merely because it automatically converts the data into a
    tibble. It is about 10x faster at reading data according to the package authors.
    It is also smarter about the format of the columns to be loaded. For example,
    it has the capability to handle numbers with currency characters, parse date columns,
    and is better at handling international data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a tibble from a CSV file, simply use the `read_csv()` function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will use the default parsing settings, which attempt to infer the correct
    data type (that is, character or numeric) for each column. The column specification
    will be displayed in the R output upon completion of the file read. The inferred
    data types may be overridden by providing the correct column specifications via
    a `col()` function call passed to the `read_csv()` function. For more information
    on the syntax, view the documentation using the `vignette("readr")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `readxl` package ([https://readxl.tidyverse.org](https://readxl.tidyverse.org))
    provides a method to read data directly from the Microsoft Excel spreadsheet format.
    To create a tibble from an XLSX file, simply use the `read_excel()` function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, as first introduced in *Chapter 2*, *Managing and Understanding
    Data*, the RStudio desktop application can write the data import code for you.
    In the upper-right of the interface, under the **Environment** tab, there is an
    **Import Dataset** button. This menu reveals a list of data import options, including
    plaintext formats like CSV files (using base R or the `readr` package), as well
    as Excel and the SPSS, SAS, and Stata formats created by other statistical computing
    software tools. Using the **From Text (readr)** option reveals the following graphical
    interface, allowing the import process to be easily customized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: RStudio’s Import Dataset feature automatically writes R code
    to easily import a variety of data formats'
  prefs: []
  type: TYPE_NORMAL
- en: The interface displays a preview of the data that updates as the import parameters
    are customized. The default column data types can be customized by clicking on
    the drop-down menu in the column header, and the code preview in the lower-right
    will update accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Import** button will immediately execute the code, but a better
    practice is to copy and paste the code into your R source code file so that the
    import process can be easily run again in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and piping data with dplyr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `dplyr` package ([https://dplyr.tidyverse.org](https://dplyr.tidyverse.org))
    provides the infrastructure for the tidyverse, as it includes the basic functionality
    that allows data to be transformed and manipulated. It also provides a straightforward
    way to begin working with larger datasets in R. Though there are other packages
    that have greater raw speed or are capable of handling even more massive datasets,
    dplyr is still quite capable and a good first step to take if you run into speed
    or memory limitations with base R.
  prefs: []
  type: TYPE_NORMAL
- en: 'When used with tibble objects, dplyr unlocks some impressive functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: Because dplyr focuses on data frames rather than vectors, new operators are
    introduced that allow common data transformations to be performed with much less
    code while remaining highly readable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The package makes reasonable assumptions about data frames, which optimizes
    your effort as well as memory use. If possible, it avoids making copies of data
    by pointing to the original value instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key portions of the code are written in C++, which, according to the authors,
    yields a 20x to 1,000x performance increase over base R for many operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R data frames are limited by available memory. With dplyr, tibbles can be linked
    transparently to disk-based databases exceeding what can be stored in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dplyr grammar of working with data becomes second nature after the initial
    learning curve has been passed. There are five key verbs in the grammar, which
    perform many of the most common transformations to data tables. Beginning with
    a tibble, one may choose to:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter()` rows of data by values of the columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select()` columns of data by name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arrange()` rows of data by sorting the values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mutate()` columns into new columns by transforming the values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summarize()` rows of data by aggregating values into a summary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These five dplyr verbs are brought together in sequences using a **pipe operator**,
    which is natively supported in R as of version 4.1 or later. Represented by the
    `|>` symbols, which vaguely resembles an arrowhead pointing to the right, the
    pipe operator “pipes” data by moving it from one function to another. The use
    of pipes allows you to create powerful chains of functions to sequentially process
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In versions of R prior to 4.1.0 update, the pipe operator was denoted by the
    `%>%` character sequence and required the `magrittr` package. The differences
    between the old and new pipe functionality are relatively minor, but as a native
    operator the new pipe may have a small speed advantage. For a shortcut to typing
    the pipe operator, the RStudio Desktop IDE, the key combination *ctrl* + *shift*
    + *m* will insert the character sequence. Note that for this shortcut to produce
    the updated pipe, you may need to change the setting to “**Use the native pipe
    operator**, **|>**” in the RStudio “**Global Options**” menu under the “**Code**”
    heading.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the package with the `library(dplyr)` command, data transformations
    begin with a tibble being piped into one of the package’s verbs. For example,
    one might `filter()` rows of the Titanic dataset to limit rows to women:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, one might `select()` only the name, sex, and age columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Where dplyr starts to shine is through its ability to chain together verbs
    in a sequence with pipes. For example, we can combine the prior two verbs, sort
    alphabetically using the verb `arrange()`, and save the output to a tibble as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Although this may not seem like a revelation just yet, when combined with the
    `mutate()` verb, we can perform complex data transformations with simpler, more
    readable code than in the base R language. We will see several examples of `mutate()`
    later on, but for now, the important thing to remember is that it is used to create
    new columns in the tibble. For example, we might create a binary `elderly` feature
    that indicates whether a passenger is at least 65 years old.
  prefs: []
  type: TYPE_NORMAL
- en: 'This uses the dplyr package’s `if_else()` function to assign a value of `1`
    if the passenger is elderly, and `0` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'By separating the statements by commas, multiple columns can be created within
    a single `mutate()` statement. This is demonstrated here to create an additional
    `child` feature that indicates whether the passenger is less than 18 years old:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining `dplyr` verb, `summarize()`, allows us to create aggregated or
    summarized metrics by grouping rows in the tibble. For example, suppose we would
    like to compute the survival rate by age or sex. We’ll begin with sex, as it is
    the easier of the two cases. We simply pipe the data into the `group_by(Sex)`
    function to create the male and female groups, then follow this with a `summarize()`
    statement to create a `survival_rate` feature that computes the average survival
    by group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the output, females were substantially more likely to survive than
    males.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute survival by age, things are slightly more complicated due to the
    missing age values. We’ll need to filter out these rows and use the `group_by()`
    function to compare children (less than 18 years old) to adults as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The results suggest that children were about 40% more likely to survive than
    adults. When combined with the comparison between males and females, this provides
    strong evidence of the hypothesized “women and children first” policy for evacuation
    of the sinking ship.
  prefs: []
  type: TYPE_NORMAL
- en: Because summary statistics by group can be computed using other methods in base
    R (including the `ave()` and `aggregate()` functions described in previous chapters),
    it is important to note that the `summarize()` command is also capable of much
    more than this. In particular, one might use it for the feature engineering hints
    described earlier in this chapter, such as observing neighbors’ behavior, utilizing
    related rows, and decomposing time series. All three of these cases involve `group_by()`
    options like households, zip codes, or units of time. Using `dplyr` to perform
    the aggregation for these data preparation operations is much easier than attempting
    to do so in base R.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put together what we’ve learned so far and provide one more example using
    pipes, let’s build a decision tree model of the Titanic dataset. We’ll `filter()`
    missing age values, use `mutate()` to create a new `AgeGroup` feature, and `select()`
    only the columns of interest for the decision tree model. The resulting dataset
    is piped to the `rpart()` decision tree algorithm, which illustrates the ability
    to pipe data to functions outside of the tidyverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that the series of steps reads almost like plain-language pseudocode. It
    is also worth noting the arguments within the `rpart()` function call. The `formula
    = Survived ~ .` argument uses R’s formula interface to model survival as a function
    of all predictors; the dot here represents the other features in the dataset not
    explicitly listed. The `data = _` argument uses the underscore (_) as a placeholder
    to represent the data being fed to `rpart()` by the pipe. The underscore can be
    used in this way to indicate the function parameter to which the data should be
    piped.
  prefs: []
  type: TYPE_NORMAL
- en: This is usually unnecessary for dplyr’s built-in functions, because they look
    for the piped data as the first parameter by default, but functions outside the
    tidyverse may require the pipe to target a specific function parameter in this
    way.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the underscore placeholder character is new as
    of R version 4.2 and will not work in prior versions! In older code that uses
    the `magrittr` package, the dot character (.) was used as the placeholder.
  prefs: []
  type: TYPE_NORMAL
- en: 'For fun, we can visualize the resulting decision tree, which shows that women
    and children are more likely to survive than adults, men, and those in third passenger
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following decision tree diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B17290_12_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: A decision tree predicting Titanic survival, which was built
    using a series of pipes'
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few small examples of how sequences of dplyr commands can make
    complex data manipulation tasks simpler. This is on top of the fact that, due
    to dplyr’s more efficient code, the steps often execute more quickly than the
    equivalent commands in base R! Providing a complete dplyr tutorial is beyond the
    scope of this book, but there are many learning resources available online, including
    the *R for Data Science* chapter at `https://r4ds.hadley.nz/transform.html`.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming text with stringr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `stringr` package (`https://stringr.tidyverse.org`) adds functions to analyze
    and transform character strings. Base R, of course, can do this too, but the functions
    are inconsistent in how they work on vectors and are relatively slow; `stringr`
    implements these functions in a form more attuned to the tidyverse workflow. The
    free resource *R for Data Science* has a tutorial that introduces the package’s
    complete set of capabilities, at [https://r4ds.hadley.nz/strings.html](https://r4ds.hadley.nz/strings.html),
    but here, we’ll examine some of the aspects most relevant to feature engineering.
    If you’d like to follow along, be sure to load the Titanic dataset and install
    and load the `stringr` package before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, the second tip for feature engineering was to “find
    insights hidden in text.” The `stringr` package can assist with this effort by
    providing functions to slice strings and detect patterns within text. All `stringr`
    functions begin with the prefix `str_`, and a few relevant examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`str_detect()` determines whether a search term is found in a string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_sub()` slices a string by position and returns a substring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_extract()` searches for a string and returns the matching pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str_replace()` replaces characters in a string with something else'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these functions seem quite similar, they are used for quite different
    purposes. To demonstrate these purposes, we’ll begin by examining the `Cabin`
    feature to determine whether certain rooms on the *Titanic* are linked to greater
    survival. We cannot use this feature as-is, because each cabin code is unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, because the codes are in forms like `A10`, `B101`, or `E67`, perhaps
    the alphabetical prefix indicates a position on the ship, and perhaps passengers
    in some of these locations may have been more able to escape the disaster. We’ll
    use the `str_sub()` function to take a 1-character substring beginning and ending
    at position 1, and save this to a `CabinCode` feature as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the cabin code is meaningful, we can use the `table()` function
    to see a clear relationship between it and the passenger class. The `useNA` parameter
    is set to `"ifany"` to display the `NA` values caused by missing cabin codes for
    some passengers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `NA` values appear to be more common in the lower ticket classes, so it
    seems plausible that cheaper fares may have not received a cabin code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot the survival probability by cabin code by piping the file
    into a `ggplot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting figure shows that even within the first-class cabin types (codes
    A, B, and C) there are differences in survival rate; additionally, the passengers
    without a cabin code are the least likely to survive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_12_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: The cabin code feature seems related to survival, even within
    first-class cabins (A, B, and C)'
  prefs: []
  type: TYPE_NORMAL
- en: Without processing the `Cabin` text data first, a learning algorithm would be
    unable to use the feature as the codes are unique to each cabin. Yet by applying
    a simple text transformation, we’ve decoded the cabin codes into something that
    can be used to improve the model’s survival predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this success in mind, let’s examine another potential source of hidden
    data: the `Name` column. One might assume that this is unusable in a model, because
    the name is a unique identifier per row and training a model on this data will
    inevitably lead to overfitting. Although this is true, there is useful information
    hiding within the names. Looking at the first few rows reveals some potentially
    useful text strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For one, the salutation (Mr., Mrs., and Miss.) might be helpful for prediction.
    The problem is that these titles are located at different positions within the
    name strings, so we cannot simply use the `str_sub()` function to extract them.
    The correct tool for this job is `str_extract()`, which is used to match and extract
    shorter patterns from longer strings. The trick with working with this function
    is knowing how to express a text pattern rather than typing each potential salutation
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: The shorthand used to express a text search pattern is called a **regular expression**,
    or **regex** for short. Knowing how to create regular expressions is an incredibly
    useful skill, as they are used for the advanced find-and-replace features in many
    text editors, in addition to being useful for feature engineering in R. We’ll
    create a simple regex to extract the salutations from the name strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in using regular expressions is to identify the common elements
    across all the desired target strings. In the case of the Titanic names, it looks
    like each salutation is preceded by a comma followed by a blank space, then has
    a series of letters before ending with a period. This can be coded as the following
    regex string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This seems to be nonsense but can be understood as a sequence that attempts
    to match a pattern character by character. The matching process begins with a
    comma and a blank space, as expected. Next, the square brackets tell the search
    function to look for any of the characters inside the brackets. For instance,
    `[AB]` would search for `A` or `B`, and `[ABC]` would search for `A`, `B`, or
    `C`. In our usage, the dash is used to search for any characters within the range
    between `A` and `z`. Note that capitalization is important—that is, `[A-Z]` is
    different from `[A-z]`. The former will search 26 characters comprising the uppercase
    alphabet while the latter will search 52 characters, including uppercase and lowercase.
    Keep in mind that `[A-z]` only matches a single character.
  prefs: []
  type: TYPE_NORMAL
- en: To have the expression match more characters, we follow the brackets with a
    `+` symbol to tell the algorithm to continue matching characters until it reaches
    something not inside the brackets. Then, it checks to see whether the remaining
    part of the regex matches.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining piece is the `\\.` sequence, which is three characters that represent
    the single period character at the end of our search pattern. Because the dot
    is a special term that represents an arbitrary character, we must escape the dot
    by prefixing it with a slash. Unfortunately, the slash is also a special character
    in R, so we must escape it as well by prefixing it with yet another slash.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions can be tricky to learn but are well worth the effort. You
    can find a deep dive into understanding how they work at [https://www.regular-expressions.info](https://www.regular-expressions.info).
    Alternatively, there are many text editors and web applications that demonstrate
    matching in real time. These can be hugely helpful to understand how to develop
    the regex search patterns and diagnose errors. One of the best such tools is found
    at [https://regexr.com](https://regexr.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can put this expression to work on the Titanic name data by combining it
    in a `mutate()` function with `str_extract()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the first few examples, it looks like these need to be cleaned up
    a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use the `str_replace()` function to eliminate the punctuation and blank
    spaces in these titles. We begin by constructing a regex to match the punctuation
    and empty space. One way to do this is to match the comma, blank space, and period
    using the `"[, \\.]"` search string. Used with `str_replace()` as shown here,
    any comma, blank space, and period characters in `Title` will be replaced by the
    empty (null) string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `str_replace_all()` variant of the replace function was used
    due to the fact that multiple characters needed replacement; the basic `str_replace()`
    would have only replaced the first instance of a matching character. Many of `stringr`''s
    functions have “all” variants for this use case. Let’s see the result of our effort:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the small counts for some of these titles and salutations, it may make
    sense to group them together. To this end, we can use dplyr’s `recode()` function
    to change the categories. We’ll keep several of the high-count levels the same,
    while grouping the rest into variants of `Miss` and a catch-all bucket, using
    the `.missing` and `.default` values to assign the `Other` label to `NA` values
    and anything else not already coded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking our work, we see that our cleanup worked as planned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see that the title is meaningful by examining a plot of survival
    rates by title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_12_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: The constructed salutation captures the impact of both age and
    gender on survival likelihood'
  prefs: []
  type: TYPE_NORMAL
- en: The creation of `CabinCode` and `TitleGroup` features exemplifies the feature
    engineering technique of finding hidden information in text data. These new features
    are likely to provide additional information beyond the base features in the Titanic
    dataset, which learning algorithms can use to improve performance. A bit of creativity
    combined with `stringr` and knowledge of regular expressions may provide the edge
    needed to surpass the competition.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning dates with lubridate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lubridate` package ([https://lubridate.tidyverse.org](https://lubridate.tidyverse.org))
    is an important tool for working with date and time data. It may not be needed
    for every analysis, but when it is needed, it can save a lot of grief. With dates
    and times, seemingly simple tasks can quickly turn into adventures, due to unforeseen
    subtleties like leap years and time zones—just ask anyone who has worked on birthday
    calculations, billing cycles, or similar date-sensitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the other tidyverse packages, the *R for Data Science* resource has
    an in-depth lubridate tutorial at `https://r4ds.hadley.nz/datetimes.html`, but
    we’ll briefly cover three of its most important feature engineering strengths
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring date and time data is loaded into R correctly while accounting for
    regional differences in how dates and times are expressed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accurately calculating differences between dates and times while accounting
    for time zones and leap years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for differences in how increments in time are understood in the real
    world, such as the fact that people become “1 year older” on their birthday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reading dates into R is challenge number one, because dates are presented in
    many different formats. For example, the publication date of the first edition
    of *Machine Learning with R* can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: October 25, 2013 (a common longhand format in the United States)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10/25/13 (a common shorthand format in the United States)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 October 2013 (a common longhand format in Europe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25.10.13 (a common shorthand format in Europe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2013-10-25 (the international standard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these diverse formats, lubridate is incapable of determining the correct
    format without help because months, days, and years can all fall in the range
    from 1 to 12\. Instead, we provide it the correct date constructor—either `mdy()`,
    `dmy()`, or `ymd()`, depending on the order of the month (`m`), day (`d`), and
    year (`y`) components of the input data. Given the order of the date components,
    the functions will automatically parse longhand and shorthand variants, and will
    handle leading zeros and two- or four-digit years. To demonstrate this, the dates
    expressed previously can be handled with the appropriate lubridate function, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that in each case, the resulting `Date` object is exactly the same.
    Let’s create a similar object for each of the three previous editions of this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do simple math to compute the difference between two dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that by default, the difference between the two dates is returned as
    days. What if we hope to have an answer in years? Unfortunately, because these
    differences are a special lubridate `difftime` object, we cannot simply divide
    these numbers by 365 days to perform the obvious calculation. One option is to
    convert them into a **duration**, which is one of the ways lubridate computes
    date differences, and in particular, tracks the passage of physical time—imagine
    it acting much like a stopwatch. The `as.duration()` function performs the needed
    conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see here that the gap between the 2nd and 3rd editions of *Machine Learning
    with R* was almost twice as long as the difference between the 1st and 2nd editions.
    We can also see that the duration seems to default to seconds while also providing
    the approximate number of years. To obtain only years, we can divide the duration
    by the duration of 1 year, which lubridate provides as a `dyears()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You may find it more convenient or easier to remember the `time_length()` function,
    which can perform the same calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `unit` argument can be set to units like days, months, and years, depending
    on the desired result. Notice, however, that these durations are exact to the
    second like a stopwatch, which is not always how people think about dates.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, for birthdays and anniversaries, people tend to think in terms
    of calendar time—that is, the number of times the calendar has reached a particular
    milestone. In lubridate, this approach is called an **interval**, which implies
    a timeline-or calendar-based view of date differences, rather than the stopwatch-based
    approach of the duration methods discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine we’d like to compute the age of the United States, which was
    born, so to speak, on July 4, 1776\. This means that on July 3, 2023, the country
    will be 246 birthdays old, and on July 5, 2023, it will be 247\. Using durations,
    we don’t get quite the right answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem has to do with the fact that durations deviate from calendar time
    due to calendar irregularities such as leap years and time changes. By explicitly
    converting the date difference into an interval with the `interval()` function,
    and then dividing by the `years()` function, we get closer to the right answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Before going any further, be sure to notice the fact that the `interval()` uses
    `start, end` syntax, in contrast to the date difference, which used `end - start`.
    Also note that the `years()` function returns a lubridate **period**, which is
    yet another way to understand differences between dates and times. Periods are
    always relative to their position on a calendar, which means that a 1-hour period
    can be a 2-hour duration during a time change, and a 1-year period can include
    365 or 366 1-day periods, depending on the calendar year—these are the types of
    challenging subtleties when working with dates that were mentioned in this section’s
    opening paragraph!
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our final age calculation, we’ll use the `%--%` interval construction
    operator as shorthand, and use the integer division operator `%/%` to return only
    the integer component of the age. These return the expected age values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Generalizing this work, we can create a function to compute the calendar-based
    age for a given date of birth as of today:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'To prove that it works, we’ll check the ages of a few famous tech billionaires:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: If you are following along in R, be aware that your results may vary depending
    on when you run the code—we’re all, unfortunately, still getting older by the
    day!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated the importance of data preparation. Because the tools
    and algorithms used to build machine learning models are the same across projects,
    data preparation is a key that unlocks the highest levels of model performance.
    This allows some aspects of human intelligence and creativity to have a large
    impact on the machine’s learning process, although clever practitioners use their
    strengths in concert with the machine’s by developing automated data engineering
    pipelines that take advantage of the computer’s ability to tirelessly search for
    useful insights in the data. These pipelines are especially important in the so-called
    “big data regime,” where data-hungry approaches like deep learning must be fed
    large amounts of data to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional small and medium data regimes, feature engineering by hand still
    reigns supreme. Using intuition and subject matter expertise, one can guide the
    model to the most useful signal in the training dataset. As this is more art than
    science, tips and tricks are learned on the job, or passed along second-hand from
    one data scientist to another. This chapter provided seven hints to help guide
    you on the journey, but the only way to truly become skilled at feature engineering
    is through practice.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like the tidyverse suite of R packages make it much less laborious than
    in years past to gain the necessary experience to perform feature engineering
    tasks. This chapter demonstrated how the tidyverse packages can be used to turn
    data into more useful predictors, and how information hidden in text data can
    be extracted to turn what seem like useless features into important predictors.
    The tidyverse packages are much more capable of handling large and ever-growing
    datasets than the base R functions, and they make R a pleasure to use even as
    datasets grow in size and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The skills developed in this chapter will provide a foundation for the work
    to come. In the next chapter, you will add new tidyverse packages to your toolkit
    and see even more examples of how it integrates into the machine learning workflow.
    You will continue to see the importance of data preparation skills as you explore
    data issues that begin as relatively minor challenges but quickly grow into massive
    problems if taken to an extreme.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
