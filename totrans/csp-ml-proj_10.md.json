["```py\n// Read in the Credit Card Fraud dataset\n// TODO: change the path to point to your data directory\nstring dataDirPath = @\"<path-to-your-dir>\";\n\n// Load the data into a data frame\nstring dataPath = Path.Combine(dataDirPath, \"creditcard.csv\");\nConsole.WriteLine(\"Loading {0}\\n\\n\", dataPath);\nvar df = Frame.ReadCsv(\n    dataPath,\n    hasHeaders: true,\n    inferTypes: true\n);\n```", "```py\n// Target variable distribution\nvar targetVarCount = df.AggregateRowsBy<string, int>(\n    new string[] { \"Class\" },\n    new string[] { \"V1\" },\n    x => x.ValueCount\n).SortRows(\"V1\");\ntargetVarCount.RenameColumns(new string[] { \"is_fraud\", \"count\" });\n\ntargetVarCount.Print();\n\nDataBarBox.Show(\n    targetVarCount.GetColumn<string>(\"is_fraud\").Values.ToArray(),\n    targetVarCount[\"count\"].Values.ToArray()\n).SetTitle(\n    \"Counts by Target Class\"\n);\n```", "```py\n// Feature distributions\nforeach (string col in df.ColumnKeys)\n{\n    if (col.Equals(\"Class\") || col.Equals(\"Time\"))\n    {\n        continue;\n    }\n\n    double[] values = df[col].DropMissing().ValuesAll.ToArray();\n\n    Console.WriteLine(String.Format(\"\\n\\n-- {0} Distribution -- \", col));\n    double[] quartiles = Accord.Statistics.Measures.Quantiles(\n        values,\n        new double[] { 0, 0.25, 0.5, 0.75, 1.0 }\n    );\n    Console.WriteLine(\n        \"Min: \\t\\t\\t{0:0.00}\\nQ1 (25% Percentile): \\t{1:0.00}\\nQ2 (Median): \\t\\t{2:0.00}\\nQ3 (75% Percentile): \\t{3:0.00}\\nMax: \\t\\t\\t{4:0.00}\",\n        quartiles[0], quartiles[1], quartiles[2], quartiles[3], quartiles[4]\n    );\n\n    HistogramBox.Show(\n        values,\n        title: col\n    )\n    .SetNumberOfBins(50);\n}\n```", "```py\n// Target Var Distributions on 2-dimensional feature space\ndouble[][] data = BuildJaggedArray(\n    df.ToArray2D<double>(), df.RowCount, df.ColumnCount\n);\nint[] labels = df.GetColumn<int>(\"Class\").ValuesAll.ToArray();\n\ndouble[][] first2Components = data.Select(\n    x => x.Where((y, i) => i < 2\n).ToArray()).ToArray();\nScatterplotBox.Show(\"Feature #1 vs. Feature #2\", first2Components, labels);\n\ndouble[][] next2Components = data.Select(\n    x => x.Where((y, i) => i >= 1 && i <= 2).ToArray()\n).ToArray();\nScatterplotBox.Show(\"Feature #2 vs. Feature #3\", next2Components, labels);\n\nnext2Components = data.Select(\n    x => x.Where((y, i) => i >= 2 && i <= 3).ToArray()\n).ToArray();\nScatterplotBox.Show(\"Feature #3 vs. Feature #4\", next2Components, labels);\n```", "```py\n// Read in the Credit Card Fraud dataset\n// TODO: change the path to point to your data directory\nstring dataDirPath = @\"<path-to-dir>\";\n\n// Load the data into a data frame\nstring dataPath = Path.Combine(dataDirPath, \"creditcard.csv\");\nConsole.WriteLine(\"Loading {0}\\n\\n\", dataPath);\nvar df = Frame.ReadCsv(\n    dataPath,\n    hasHeaders: true,\n    inferTypes: true\n);\n\nConsole.WriteLine(\"* Shape: {0}, {1}\\n\\n\", df.RowCount, df.ColumnCount);\n```", "```py\nstring[] featureCols = df.ColumnKeys.Where(\n    x => !x.Equals(\"Time\") && !x.Equals(\"Class\")\n).ToArray();\n\nvar noFraudData = df.Rows[\n    df[\"Class\"].Where(x => x.Value == 0.0).Keys\n].Columns[featureCols];\ndouble[][] data = BuildJaggedArray(\n    noFraudData.ToArray2D<double>(), noFraudData.RowCount, featureCols.Length\n);\n```", "```py\nprivate static double[][] BuildJaggedArray(double[,] ary2d, int rowCount, int colCount)\n{\n    double[][] matrix = new double[rowCount][];\n    for (int i = 0; i < rowCount; i++)\n    {\n        matrix[i] = new double[colCount];\n        for (int j = 0; j < colCount; j++)\n        {\n            matrix[i][j] = double.IsNaN(ary2d[i, j]) ? 0.0 : ary2d[i, j];\n        }\n    }\n    return matrix;\n}\n```", "```py\ndouble[][] wholeData = BuildJaggedArray(\n    df.Columns[featureCols].ToArray2D<double>(), df.RowCount, featureCols.Length\n);\nint[] labels = df.GetColumn<int>(\"Class\").ValuesAll.ToArray();\ndf, into a two-dimensional array, wholeData, by using the BuildJaggedArray function.\n```", "```py\nvar pca = new PrincipalComponentAnalysis(\n    PrincipalComponentMethod.Standardize\n);\npca.Learn(data);\n```", "```py\ndouble[][] transformed = pca.Transform(wholeData);\n```", "```py\ndouble[][] first2Components = transformed.Select(x => x.Where((y, i) => i < 2).ToArray()).ToArray();\nScatterplotBox.Show(\"Component #1 vs. Component #2\", first2Components, labels);\n\ndouble[][] next2Components = transformed.Select(\n    x => x.Where((y, i) => i >= 1 && i <= 2).ToArray()\n).ToArray();\nScatterplotBox.Show(\"Component #2 vs. Component #3\", next2Components, labels);\n\nnext2Components = transformed.Select(\n    x => x.Where((y, i) => i >= 2 && i <= 3).ToArray()\n).ToArray();\nScatterplotBox.Show(\"Component #3 vs. Component #4\", next2Components, labels);\n\nnext2Components = transformed.Select(\n    x => x.Where((y, i) => i >= 3 && i <= 4).ToArray()\n).ToArray();\nScatterplotBox.Show(\"Component #4 vs. Component #5\", next2Components, labels);\n```", "```py\nDataSeriesBox.Show(\n    pca.Components.Select((x, i) => (double)i),\n    pca.Components.Select(x => x.CumulativeProportion)\n).SetTitle(\"Explained Variance\");\nSystem.IO.File.WriteAllLines(\n    Path.Combine(dataDirPath, \"explained-variance.csv\"),\n    pca.Components.Select((x, i) => String.Format(\"{0},{1:0.0000}\", i + 1, x.CumulativeProportion))\n);\n```", "```py\nConsole.WriteLine(\"exporting train set...\");\n\nSystem.IO.File.WriteAllLines(\n    Path.Combine(dataDirPath, \"pca-features.csv\"),\n    transformed.Select((x, i) => String.Format(\"{0},{1}\", String.Join(\",\", x), labels[i]))\n);\npca-features.csv. We will use this data to build anomaly detection models for credit card fraud detection in the following step.\n```", "```py\n// Read in the Credit Card Fraud dataset\n// TODO: change the path to point to your data directory\nstring dataDirPath = @\"<path-to-dir>\";\n\n// Load the data into a data frame\nstring dataPath = Path.Combine(dataDirPath, \"pca-features.csv\");\nConsole.WriteLine(\"Loading {0}\\n\\n\", dataPath);\nvar featuresDF = Frame.ReadCsv(\n    dataPath,\n    hasHeaders: false,\n    inferTypes: true\n);\nfeaturesDF.RenameColumns(\n    featuresDF.ColumnKeys\n        .Select((x, i) => i == featuresDF.ColumnCount - 1 ? \"is_fraud\" : String.Format(\"component-{0}\", i + 1))\n);\n```", "```py\nConsole.WriteLine(\"* Shape: ({0}, {1})\", featuresDF.RowCount, featuresDF.ColumnCount);\n\nvar count = featuresDF.AggregateRowsBy<string, int>(\n    new string[] { \"is_fraud\" },\n    new string[] { \"component-1\" },\n    x => x.ValueCount\n).SortRows(\"component-1\");\ncount.RenameColumns(new string[] { \"is_fraud\", \"count\" });\ncount.Print();\n```", "```py\n// First 13 components explain about 50% of the variance\nint numComponents = 13;\nstring[] cols = featuresDF.ColumnKeys.Where((x, i) => i < numComponents).ToArray();\n\n// First, compute distances from the center/mean among normal events\nvar normalDF = featuresDF.Rows[\n    featuresDF[\"is_fraud\"].Where(x => x.Value == 0).Keys\n].Columns[cols];\n\ndouble[][] normalData = BuildJaggedArray(\n    normalDF.ToArray2D<double>(), normalDF.RowCount, cols.Length\n);\n```", "```py\ndouble[] normalVariances = ComputeVariances(normalData);\ndouble[] rawDistances = ComputeDistances(normalData, normalVariances);\n\ndouble[] distances = rawDistances.ToArray();\n\ndouble meanDistance = distances.Average();\ndouble stdDistance = Math.Sqrt(\n    distances\n    .Select(x => Math.Pow(x - meanDistance, 2))\n    .Sum() / distances.Length\n);\n\nConsole.WriteLine(\n    \"* Normal - mean: {0:0.0000}, std: {1:0.0000}\",\n    meanDistance, stdDistance\n);\nComputeVariances and ComputeDistances, to compute the variances of feature values and the distances. The following is the code for the ComputeVariances method:\n```", "```py\nprivate static double[] ComputeVariances(double[][] data)\n{\n    double[] componentVariances = new double[data[0].Length];\n\n    for (int j = 0; j < data[0].Length; j++)\n    {\n        componentVariances[j] = data\n            .Select((x, i) => Math.Pow(data[i][j], 2))\n            .Sum() / data.Length;\n    }\n\n    return componentVariances;\n}\n```", "```py\nprivate static double[] ComputeDistances(double[][] data, double[] componentVariances)\n{\n\n    double[] distances = data.Select(\n        (row, i) => Math.Sqrt(\n            row.Select(\n                (x, j) => Math.Pow(x, 2) / componentVariances[j]\n            ).Sum()\n        )\n    ).ToArray();\n\n    return distances;\n}\n```", "```py\n// Detection\nvar fraudDF = featuresDF.Rows[\n    featuresDF[\"is_fraud\"].Where(x => x.Value > 0).Keys\n].Columns[cols];\n\ndouble[][] fraudData = BuildJaggedArray(\n    fraudDF.ToArray2D<double>(), fraudDF.RowCount, cols.Length\n);\ndouble[] fraudDistances = ComputeDistances(fraudData, normalVariances);\nfraudData, which we use for distance-measuring calculations. Then, using the ComputeDistances function that we wrote, we can compute the distances between the fraudulent credit card transactions and the distribution of non-fraudulent transactions. With these distances measures, we then start analyzing the fraud detection rates for each of the target false-alarm rates. Take a look at the following code snippet:\n```", "```py\n// 5-10% false alarm rate\nfor (int i = 0; i < 4; i++)\n{\n    double targetFalseAlarmRate = 0.05 * (i + 1);\n    double threshold = Accord.Statistics.Measures.Quantile(\n        distances,\n        1 - targetFalseAlarmRate\n    );\n\n    int[] detected = fraudDistances.Select(x => x > threshold ? 1 : 0).ToArray();\n\n    Console.WriteLine(\"\\n\\n---- {0:0.0}% False Alarm Rate ----\", targetFalseAlarmRate * 100.0);\n    double overallRecall = (double)detected.Sum() / detected.Length;\n    Console.WriteLine(\"* Overall Fraud Detection: {0:0.00}%\", overallRecall * 100.0);\n}\nChapter 9, *Cyber Attack Detection*. One thing that is different here, however, is the fact that we only have two target classes (fraud versus non-fraud), whereas we had five target classes (normal versus four different types of cyber attack) in Chapter 9, *Cyber Attack Detection*. As you can see from this code, we experiment with five different target false alarm rates from 5% to 10%, and analyze the fraud detection rates for the given target false alarm rate. We will take a deeper look at this code in the following model evaluation step.\n```", "```py\n// First 13 components explain about 50% of the variance\nint numComponents = 13;\nstring[] cols = featuresDF.ColumnKeys.Where((x, i) => i < numComponents).ToArray();\n\nvar rnd = new Random(1);\nint[] trainIdx = featuresDF[\"is_fraud\"]\n    .Where(x => x.Value == 0)\n    .Keys\n    .OrderBy(x => rnd.Next())\n    .Take(15000)\n    .ToArray();\nvar normalDF = featuresDF.Rows[\n    trainIdx\n].Columns[cols];\n\ndouble[][] normalData = BuildJaggedArray(\n    normalDF.ToArray2D<double>(), normalDF.RowCount, cols.Length\n);\n```", "```py\nvar teacher = new OneclassSupportVectorLearning<Gaussian>();\nvar model = teacher.Learn(normalData);\n```", "```py\nint[] testIdx = featuresDF[\"is_fraud\"]\n    .Where(x => x.Value > 0)\n    .Keys\n    .Concat(\n        featuresDF[\"is_fraud\"]\n        .Where(x => x.Value == 0 && !trainIdx.Contains(x.Key))\n        .Keys\n        .OrderBy(x => rnd.Next())\n        .Take(5000)\n        .ToArray()\n    ).ToArray();\n\nvar fraudDF = featuresDF.Rows[\n    testIdx\n].Columns[cols];\n\ndouble[][] fraudData = BuildJaggedArray(\n    fraudDF.ToArray2D<double>(), fraudDF.RowCount, cols.Length\n);\n\nint[] fraudLabels = featuresDF.Rows[\n    testIdx\n].GetColumn<int>(\"is_fraud\").ValuesAll.ToArray();\n```", "```py\nfor(int j = 0; j <= 10; j++)\n{\n    model.Threshold = -1 + j/10.0; \n\n    int[] detected = new int[fraudData.Length];\n    double[] probs = new double[fraudData.Length];\n    for (int i = 0; i < fraudData.Length; i++)\n    {\n        bool isNormal = model.Decide(fraudData[i]);\n        detected[i] = isNormal ? 0 : 1;\n    }\n\n    Console.WriteLine(\"\\n\\n---- One-Class SVM Results ----\");\n    Console.WriteLine(\"* Threshold: {0:0.00000}\", model.Threshold);\n    double correctPreds = fraudLabels\n        .Select((x, i) => detected[i] == 1 && x == 1 ? 1 : 0)\n        .Sum();\n    double precision = correctPreds / detected.Sum();\n    double overallRecall = correctPreds / fraudLabels.Sum();\n    Console.WriteLine(\"* Overall Fraud Detection: {0:0.00}%\", overallRecall * 100.0);\n    Console.WriteLine(\"* False Alarm Rate: {0:0.00}%\", (1 - precision) * 100.0);\n}\n```", "```py\n// 5-10% false alarm rate\nfor (int i = 0; i < 4; i++)\n{\n    double targetFalseAlarmRate = 0.05 * (i + 1);\n    double threshold = Accord.Statistics.Measures.Quantile(\n        distances,\n        1 - targetFalseAlarmRate\n    );\n\n    int[] detected = fraudDistances.Select(x => x > threshold ? 1 : 0).ToArray();\n\n    Console.WriteLine(\"\\n\\n---- {0:0.0}% False Alarm Rate ----\", targetFalseAlarmRate * 100.0);\n    double overallRecall = (double)detected.Sum() / detected.Length;\n    Console.WriteLine(\"* Overall Fraud Detection: {0:0.00}%\", overallRecall * 100.0);\n}\n```", "```py\nfor(int j = 0; j <= 10; j++)\n{\n    model.Threshold = -1 + j/10.0; \n\n    int[] detected = new int[fraudData.Length];\n    double[] probs = new double[fraudData.Length];\n    for (int i = 0; i < fraudData.Length; i++)\n    {\n        bool isNormal = model.Decide(fraudData[i]);\n        detected[i] = isNormal ? 0 : 1;\n    }\n\n    Console.WriteLine(\"\\n\\n---- One-Class SVM Results ----\");\n    Console.WriteLine(\"* Threshold: {0:0.00000}\", model.Threshold);\n    double correctPreds = fraudLabels\n        .Select((x, i) => detected[i] == 1 && x == 1 ? 1 : 0)\n        .Sum();\n    double precision = correctPreds / detected.Sum();\n    double overallRecall = correctPreds / fraudLabels.Sum();\n    Console.WriteLine(\"* Overall Fraud Detection: {0:0.00}%\", overallRecall * 100.0);\n    Console.WriteLine(\"* False Alarm Rate: {0:0.00}%\", (1 - precision) * 100.0);\n}\n```"]