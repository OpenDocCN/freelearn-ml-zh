<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Recognizing Objects in the World</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this <span>chapter,</span> we will immerse ourselves in the world of <strong>machine learning</strong> (<strong>ML</strong>) and Core ML by working through what could be considered the 101 Core ML application. We will be using an image classification model to allow the user to point their iPhone at anything and have the app classify the most dominant object in the view. </p>
<p class="mce-root">We will start off by first discussing the concept of <strong>convolutional neural networks</strong> (<strong>ConvNets</strong> or <strong>CNNs</strong>), a category of <span>neural networks well suited to image classification,</span> before jumping into implementation. Starting from a skeleton project, you will soon discover just how easy it is to integrate ML into your apps with the help of Core ML.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Gaining some intuition on how machines understand images</li>
<li>Building out the example application for this chapter</li>
<li>Capturing photo frames and preprocessing them before passing them to the Core ML model</li>
<li>Using the Core ML model to perform inference and interpreting the result </li>
</ul>
<div class="packt_infobox">Convolutional neural networks are commonly referred to as either CNNs or ConvNets, and these terms are used interchangeably throughout this book. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding images</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, it's not my intention to give you a theoretical or deep understanding of any particular ML algorithm, but rather gently introduce you to some of the main concepts. This will help you to gain an intuitive understanding of how they work so that you know where and how to apply them, as well as give you a platform to dive deeper into the particular subject, which I strongly encourage you to do. </p>
<div class="packt_infobox">For a good introductory text on deep learning, I strongly recommend Andrew Trask's book <em>Grokking Deep Learning</em>. For a general introduction to ML, I would recommend Toby Segaran's book <em>Programming Collective Intelligence: Building Smart Web 2.0 Applications</em>.</div>
<p>In this section, we will be introducing <span>CNNs, specifically introducing what they are and why they are well suited for spatial data, that is, images. But before discussing CNNs, we will start by inspecting the data; then we'll see why CNNs perform better than their counterpart, fully connected neural networks (or just neural networks).</span></p>
<p><span>For the purpose of illustrating these concepts, consider the task of classifying the following digits, where each digit is represented as a 5 x 5 matrix of pixels. The dark gray pixels have a value of 1 and light gray pixels have a value of 0:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span><img src="assets/9b143785-9e1f-4f17-b091-8bffa35774ab.png" style="width:42.92em;height:14.17em;"/></span></div>
<p>Using a fully connected neural network <span>(single hidden layer)</span>, our model would learn the joint probability of each pixel with respect to their associated label; that is, the model will assign positive weights to pixels that correlate with the label and using the output with the highest likelihood to be the most probable label. During training, we take each image and flatten it before feeding into our network, as shown in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7c2c3b67-caff-406a-94c2-042feea11f35.png" style="width:37.33em;height:28.92em;"/></div>
<p>This works remarkably well, and if you have experience with ML, particularly deep learning, you would have likely come across the MNIST dataset. It's a dataset consisting of labeled <span>handwritten digits, where each digit is centrally rendered to a 28 x 28 gray scale (single channel with the pixel value ranging from 0-255) image. Using a single-layer fully connected network will likely result in a validation accuracy close to 90%. But what happens if we introduce some complexities such as moving the image around a larger space, as illustrated in the following diagram?<br/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png" style="width:28.08em;height:14.25em;"/></div>
<p>The fully connected network has no concept of space or local relationships; in this case, the model would need to learn all variants of each digit at each possible location. To further emphasize the importance of being able to capture the relationship of spatial data, consider the need to learn more complex images, such as classifying dogs and cats using a network that discards 2D information. Individual pixels alone are unable to portray complex shapes such as eyes, a nose, or ears; it's only when you consider neighboring pixels that you can describe these more complex shapes:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png" style="width:47.08em;height:19.00em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Images taken from the Kaggle competition cats vs dogs (https://www.kaggle.com/c/dogs-vs-cats)</div>
<p>We need something that can abstract away from the raw pixels, something that can describe images using high-level features. Let's return to our digits dataset and investigate how we might go about extracting higher-level features for the task of classification. As alluded to in an earlier example, we need a set of features that abstracts away from the raw pixels, is <span>unaffected by</span><span> position, and preserves 2D spatial information. If you're familiar with image processing, or even image processing tools, you would have most probably come across the idea and results of </span><strong>edge detection</strong> <span>or</span> <strong>edge filters</strong><span>; in simplest terms, these work by passing a set of kernels across the whole image, where the output is the image with its edges emphasized. Let's see how this looks diagrammatically. First, we have our set of kernels; each one extracts a specific feature of the image, such as the presence of horizontal edges, vertical edges, or edges at a 45 degree angle:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/8861c305-175e-4535-ba4e-b42c9f240a96.png"/></div>
<p>For each of these filters, we pass them over our image, extracting each of the features; to help illustrate this, let's take one digit and pass the vertical kernel over it:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c152549c-5150-4a50-a20d-f08a457765c8.png"/></div>
<p>As illustrated in the previous diagram, we slide the horizontal kernel across the image, producing a new image using the values of the image and kernel. We continue until we have reached the bounds of the image, as shown in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e2d32e91-c97a-42ac-901c-9d35a04a902a.png"/></div>
<p class="mce-root">The output of this is a map showing the presence of vertical lines detected within the image. Using this and the other kernels, we can now describe each class by its dominant gradients rather than using pixel positions. This higher level abstraction allows us to recognize classes independent of their location as well as describe more complex objects.</p>
<div class="packt_infobox">Two useful things to be aware of when dealing with kernels are the <strong>stride</strong> <strong>value</strong> and <strong>padding</strong>. Strides determines how large your step size is when sliding your kernel across the image. In the preceding example, our stride is set to 1; that is, we're sliding only by a single value. Padding refers to how you deal with the boundaries; here, we are using <strong>valid</strong>, where we only process pixels within valid ranges. <strong>same </strong>would mean adding a border around the image to ensure that the output remains the same size as the input.</div>
<p>What we have performed here is known as <strong>feature engineering</strong> and something neural networks perform automatically; in particular, this is what <span>CNNs do.</span> They create a series of kernels (or convolution matrices) that are used to convolve the image to extract local features from neighboring pixels. <span>Unlike our previous engineered example, these kernels are learned during training. Because they are learned automatically, we can afford to create many filters that can extract granular nuances of the image as well, allowing us to effectively stack convolution layers on top of each other. This allows for increasingly higher levels of abstraction to learn. For example, your first layer may learn to detect simple edges, and your second layer (operating on the previous extracted features) may learn to extract simple shapes. The deeper we go, the higher the level achieved by our features, as illustrated in the diagram:<br/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png" style="width:24.25em;height:26.00em;"/></div>
<p>And there we have it! An architecture capable of understanding the world by learning features and layers of abstraction to efficiently describe it. Let's now put this into practice using a pretrained model and Core ML to get our phone to recognize the objects it sees. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recognizing objects in the world</h1>
                </header>
            
            <article>
                
<p>To recap, our goal in this chapter is to create an application that will <span>recognize what it sees. We will start by first capturing video frames, prepare these frames for our model, and finally feed them into a Core ML model to perform inference. Let's get started.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capturing data </h1>
                </header>
            
            <article>
                
<p>If you haven't done it already, <span>download</span> the latest code from the accompanying repository: <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the directory <kbd>Chapter3/Start/ObjectRecognition/</kbd> and open the project <kbd>ObjectRecognition.xcodeproj</kbd>. Once loaded, you will see the skeleton project for this chapter, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f875374a-88c0-44b2-a401-c61345016010.png"/></div>
<p>To help you navigate around the project, here is a list of core files/classes and their main functions: </p>
<ul>
<li><kbd>VideoCapture</kbd> will be responsible for the management and handling of the camera, including capturing video frames </li>
<li><kbd>CaptureVideoPreviewView.swift</kbd> contains the class <kbd>CapturePreviewView</kbd>, which will be used to present the captured frames </li>
<li><kbd>CIImage</kbd> provides convenient extensions to the class <kbd>CIImage</kbd>, used for preparing the frame for the Core ML model</li>
<li><kbd>VideoController</kbd>, as you would expect, is the controller for the application and is responsible for interfacing with the imported Core ML model </li>
</ul>
<p>We will be making changes to each of these in the following sections in order to realize the desired functionality. Our first task will be to get access to the camera and start capturing frames; to do this, we will be making use of Apple's iOS frameworks <strong>AVFoundation</strong> and <strong>CoreVideo</strong>.</p>
<p class="mce-root">The AVFoundation framework encompasses classes for handing capturing, processing, synthesizing, controlling, importing, and exporting of audiovisual media on iOS and other platforms. In this chapter, we are most interested in a subset of this framework for dealing with cameras and media capture, but you can learn more about the AVFoundation framework on Apple's official documentation site at <a href="https://developer.apple.com/documentation/avfoundation">https://developer.apple.com/documentation/avfoundation</a>.</p>
<p class="mce-root">CoreVideo provides a pipeline-based API for manipulating digital videos, capable of accelerating the process using support from both Metal and OpenGL.</p>
<p>We will designate the responsibility of setting up and capturing frames from the camera to the class <kbd>VideoCapture</kbd>; let's jump into the code now. Select <kbd>VideoCapture.swift</kbd> from the left-hand side panel to open in the editing window. Before making amendments, let's inspect what is already there and what's left to do. </p>
<p>At the top of the class, we have the protocol <kbd>VideoCaptureDelegate</kbd> defined:</p>
<pre>public protocol VideoCaptureDelegate: class {<br/>    func onFrameCaptured(<br/>      videoCapture: VideoCapture, <br/>      pixelBuffer:CVPixelBuffer?, <br/>      timestamp:CMTime)<br/>}</pre>
<p><kbd>VideoCapture</kbd> will pass through the captured frames to a registered delegate, thus allowing the <kbd>VideoCapture</kbd> class to focus solely on the task of capturing the frames. What we pass to the delegate is a reference to itself, the image data (captured frame) of type <kbd>CVPixelBuffer</kbd> and the timestamp as type <kbd>CMTime</kbd>. <kbd>CVPixelBuffer</kbd> is a CoreVideo data structure specifically for holding pixel data, and the data structure our Core ML model is expecting (which we'll see in a short while). <kbd>CMTTime</kbd> is just a struct for encapsulating a timestamp, which we'll obtain directly from the video frame.</p>
<p>Under the protocol, we have the skeleton of our <span><kbd>VideoCapture</kbd> </span>class. We will be walking through it in this section, along with an extension to implement the <kbd>AVCaptureVideoDataOutputSampleBufferDelegate</kbd> protocol, which we will use to capture frames:</p>
<pre>public class VideoCapture : NSObject{<br/>    public weak var delegate: VideoCaptureDelegate?<br/>    public var fps = 15<br/>    var lastTimestamp = CMTime()<br/>    override init() {<br/>        super.init()<br/>    }<br/>    private func initCamera() -&gt; Bool<br/>    {<br/>        return true<br/>    }<br/>    public func asyncStartCapturing(<br/>        completion: (() -&gt; Void)? = nil)<br/>        {<br/>         }<br/>    public func asyncStopCapturing(<br/>        completion: (() -&gt; Void)? = nil)<br/>        {<br/>        }<br/>}</pre>
<p>Most of this should be self-explanatory, so I will only highlight the not-so-obvious parts, starting with the variables <kbd>fps</kbd> and <kbd>lastTimestamp</kbd>. We use these together to throttle how quickly we pass frames back to the delegate; we do this as it's our assumption that we capture frames far quicker than we can process them. And to avoid having our camera lag or jump, we explicitly limit how quickly we pass frames to the delegate. <strong>Frames per second</strong> (<span><strong>fps</strong>) </span>sets this frequency while <kbd>lastTimestamp</kbd> is used in conjunction to calculate the elapsed time since the last processing of a frame. </p>
<p>The only other part of the code I will highlight here is the <kbd>asyncStartCapturing</kbd> and <kbd>asyncStopCapturing</kbd> methods; these methods, as the names imply, are responsible for starting and stopping the capture session respectively. Because they both will be using blocking methods, which can take some time, we will dispatch the task off the main thread to avoid blocking it and affecting the user's experience.</p>
<p>Finally, we have the extension; it implements the <kbd>AVCaptureVideoDataOutputSampleBufferDelegate</kbd> protocol:</p>
<pre>extension VideoCapture : AVCaptureVideoDataOutputSampleBufferDelegate{<br/>    public func captureOutput(_ output: AVCaptureOutput,<br/>                              didOutput sampleBuffer: CMSampleBuffer,<br/>                              from connection: AVCaptureConnection)<br/>    {<br/>    }<br/>}</pre>
<p>We will discuss the details shortly, but essentially it is the delegate that we assign to the camera for handling incoming frames of the camera. We will then proxy it through to the <span><kbd>VideoCaptureDelegate</kbd> </span>delegate assigned to this class.</p>
<p>Let's now walk through implementing the methods of this class, starting with <kbd>initCamera</kbd>. In this method, we want to set up the pipeline that will grab the frames from the physical camera of the device and pass them onto our delegate method. We do this by first getting a reference to the physical camera and then wrapping it in an instance of the <kbd>AVCaptureDeviceInput</kbd> class, which takes care of managing the connection and communication with the physical camera. Finally, we add a destination for the frames, which is where we use an instance of <kbd>AVCaptureVideoDataOutput</kbd>, assigning ourselves as the delegate for receiving these frames. This pipeline is wrapped in something called <kbd>AVCaptureSession</kbd>, which is responsible for coordinating and managing this pipeline. </p>
<p>Let's now define some instance variables we'll need; inside the class <kbd>VideoCapture</kbd>, add the following variables:</p>
<pre>let captureSession = AVCaptureSession()<br/>let sessionQueue = DispatchQueue(label: "session queue")</pre>
<p>We mentioned the purpose of <kbd>captureSession</kbd> previously, but also introduced a <kbd>DispatchQueue</kbd>. When adding a delegate to <kbd>AVCaptureVideoDataOutput</kbd> (for handling the arrival of new frames), you also pass in a <kbd>DispatchQueue</kbd>; this allows you to control which queue the frames are managed on. For our example, we will be handling the processing of the images off the main thread so as to avoid impacting the performance of the user interface. </p>
<p>With our instance variables now declared, we will turn our attention to the <kbd>initCamera</kbd> method, breaking it down into small snippets of code. Add the following within the body of the method:</p>
<pre>captureSession.beginConfiguration()       <br/>captureSession.sessionPreset = AVCaptureSession.Preset.medium </pre>
<p>We signal to the <kbd>captureSession</kbd> that we want to batch multiple configurations by calling the method <kbd>beginConfiguration</kbd>; these changes won't be made until we commit them by calling the session's <kbd>commitConfiguration</kbd> method. Then, in the next line of code, we set the desired quality level:</p>
<pre>guard let captureDevice = AVCaptureDevice.default(for: AVMediaType.video) else {<br/>    print("ERROR: no video devices available")<br/>    return false<br/>}<br/> <br/>guard let videoInput = try? AVCaptureDeviceInput(device: captureDevice) else {<br/>    print("ERROR: could not create AVCaptureDeviceInput")<br/>    return false<br/>}<br/> <br/>if captureSession.canAddInput(videoInput) {<br/>    captureSession.addInput(videoInput)<br/>}</pre>
<p>In the next snippet, we obtain the physical device; here, we are obtaining the default device capable of recording video, but you can just as easily search for one with specific capabilities, such as the front camera. After successfully obtaining the device, we wrap it in an instance of <kbd>AVCaptureDeviceInput</kbd> that will be responsible for capturing data from the physical camera and finally adding it to the session. </p>
<p>We now have to add the destination for these frames; again, add the following snippet to the <kbd>initCamera</kbd> method where you left off:</p>
<pre>let videoOutput = AVCaptureVideoDataOutput()<br/> <br/>let settings: [String : Any] = [<br/>    kCVPixelBufferPixelFormatTypeKey as String: NSNumber(value: kCVPixelFormatType_32BGRA)<br/>]<br/>videoOutput.videoSettings = settings<br/>videoOutput.alwaysDiscardsLateVideoFrames = true<br/>videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)<br/> <br/>if captureSession.canAddOutput(videoOutput) {<br/>    captureSession.addOutput(videoOutput)<br/>}<br/> <br/>videoOutput.connection(with: AVMediaType.video)?.videoOrientation = .portrait</pre>
<p>In the previous code snippet, we create, set up, and added our output. We start by instantiating an instance of <kbd>AVCaptureVideoDataOutput</kbd>, before defining what data we want. Here, we are requesting full color (<kbd>kCVPixelFormatType_32BGRA</kbd>), but depending on your model, it may be more efficient to request images in grayscale (<kbd>kCVPixelFormatType_8IndexedGray_WhiteIsZero</kbd>).</p>
<p>Setting <kbd>alwaysDiscardsLateVideoFrames</kbd> to true means any frames that arrive while the dispatch queue is busy will be discarded—a desirable feature for our example. We then assign ourselves along with our dedicated dispatch queue as the delegate for handing incoming frames using the method <kbd>videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)</kbd>. Once we have configured our output, we are ready to add it to our session as part of our configuration request. To prevent our images from being rotated by 90 degrees, we then request that our images are in portrait orientation. </p>
<p>Add the final statement to commit these configurations; it's only after we do this that these changes will take effect:</p>
<pre>captureSession.commitConfiguration()</pre>
<p>This now completes our <kbd>initCamera</kbd> method; let's swiftly (excuse the pun) move onto the methods responsible for starting and stopping this session. Add the following code to the body of the <kbd>asyncStartCapturing</kbd> method:</p>
<pre>sessionQueue.async {<br/>    if !self.captureSession.isRunning{<br/>       self.captureSession.startRunning()<br/>    }<br/> <br/>    if let completion = completion{<br/>        DispatchQueue.main.async {<br/>            completion()<br/>        }<br/>    }<br/> }</pre>
<p>As mentioned previously, the <kbd>startRunning</kbd> and <kbd>stopRunning</kbd> methods both block the main thread and can take some time to complete; for this reason, we execute them off the main thread, again to avoid affecting the responsiveness of the user interface. Invoking <kbd>startRunning</kbd> will start the flow of data from the subscribed inputs (camera) to the subscribed outputs (delegate). </p>
<div class="packt_tip">Errors, if any, are reported through the notification <kbd>AVCaptureSessionRuntimeError</kbd>. You can subscribe to listen to it using the default <kbd>NotificationCenter</kbd>. Similarly, you can subscribe to listen when the session starts and stops with the notifications <kbd>AVCaptureSessionDidStartRunning</kbd> and <kbd>AVCaptureSessionDidStopRunning</kbd>, respectively.</div>
<p>Similarly, add the following code to the method <kbd>asyncStopCapturing</kbd>, which will be responsible for stopping the current session:</p>
<pre>sessionQueue.async {<br/>    if self.captureSession.isRunning{<br/>        self.captureSession.stopRunning()<br/>    }<br/> <br/>    if let completion = completion{<br/>        DispatchQueue.main.async {<br/>            completion()<br/>         }<br/>     }<br/> }</pre>
<p>Within the <kbd>initCamera</kbd> method, we subscribed ourselves as the delegate to handle arriving <span>frames</span> using the statement <kbd>videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)</kbd>; let's now turn our attention to handling this. As you may recall, we included an extension of the <kbd>VideoCapture</kbd> class to implement the <kbd>AVCaptureVideoDataOutputSampleBufferDelegate</kbd> protocol within the <kbd>captureOutput</kbd> method. Add the following code:</p>
<pre>guard let delegate = self.delegate else{ return }<br/> <br/> let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)<br/> <br/> let elapsedTime = timestamp - lastTimestamp<br/> if elapsedTime &gt;= CMTimeMake(1, Int32(fps)) {<br/> <br/> lastTimestamp = timestamp<br/> <br/> let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)<br/> <br/> delegate.onFrameCaptured(videoCapture: self,<br/> pixelBuffer:imageBuffer,<br/> timestamp: timestamp)<br/> }</pre>
<p>Before walking through this code snippet, it's worth mentioning what parameters this method is passed and how we use them. The first parameter, <kbd>output</kbd>, is of the type <kbd>AVCaptureVideoDataOutput</kbd> and references the associated output that this frame originated from. The next parameter, <kbd>sampleBuffer</kbd>, is of the type <kbd>CMSampleBuffer</kbd> and this is what we will use to access data of the current frame. Along with the frames, the duration, format, and timestamp associated with each frame can also be obtained. The final parameter, <kbd>connection</kbd>, is of the type <kbd>AVCaptureConnection</kbd> and provides a reference to the connection associated with the received frame.</p>
<p>Now, walking through the code, we start by guarding against any occurrences where no delegate is assigned, and returning early if so. Then we determine whether enough time has elapsed since the last time we processed a frame, remembering that we are throttling how frequently we process a frame to ensure a seamless experience. Here, instead of using the systems clock, we obtain the time associated with the latest frame via the statement <kbd>let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)</kbd>; this ensures that we are measuring against the relative time with respect to the frame rather than absolute time of the system. Given that enough time has passed, we proceed to get a reference to the sample's image buffer via the statement <kbd>CMSampleBufferGetImageBuffer(sampleBuffer)</kbd>, finally passing it over to the assigned delegate. </p>
<p>This now completes our <kbd>VideoCapture</kbd> class; let's move on to hooking it up to our view using the <kbd>ViewController</kbd>. But before jumping into the code, let's inspect the interface via the storyboard to better understand where we'll be presenting the video stream. Within Xcode, select <kbd>Main.storyboard</kbd> from the <span class="packt_screen">Project Navigator</span> panel on the left to open up interface builder; when opened, you will be presented with a layout similar to the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png"/></div>
<p>Nothing complicated; we have a label to present our results and a view to render our video frames onto. If you select the <span class="packt_screen">VideoPreview</span> view and inspect the class assigned to it, you will see we have a custom class to handle the rendering called, appropriately, <span class="packt_screen">CapturePreviewView</span>. Let's jump into the code for this class and make the necessary changes:</p>
<pre>import AVFoundation<br/> import UIKit<br/> <br/> class CapturePreviewView: UIView {<br/> <br/> }</pre>
<p>Fortunately, <kbd>AVFoundation</kbd> makes available a subclass of <kbd>CALayer</kbd> specifically for rendering frames from the camera; all that remains for us to do is to override the view's <kbd>layerClass</kbd> property and return the appropriate class. Add the following code to the <kbd>CapturePreviewView</kbd> class:</p>
<pre>override class var layerClass: AnyClass {<br/>    return AVCaptureVideoPreviewLayer.self<br/>}</pre>
<p>This method is called early during the creation of the view and is used to determine what <kbd>CALayer</kbd> to instantiate and associate with this view. As previously mentioned, the <kbd>AVCaptureVideoPreviewLayer</kbd> is—as the name suggests<span>—</span>specifically for handling video frames. In order to get the frames rendered, we simply assign <kbd>AVCaptureSession</kbd> with the <kbd>AVCaptureVideoPreviewLayer.session</kbd> property. Let's do that now; first open up the <kbd>ViewController</kbd> class in <span class="packt_screen">Xcode</span> and add the following variable (in bold):</p>
<pre>@IBOutlet var previewView:CapturePreviewView!<br/>@IBOutlet var classifiedLabel:UILabel!<br/> <br/> <strong>let videoCapture : VideoCapture = VideoCapture()</strong></pre>
<p>The <kbd>previewView</kbd> and <kbd>classifiedLabel</kbd> are existing variables associated with the interface via the <span class="packt_screen">Interface Builder</span>. Here, we are creating an instance of <kbd>VideoCapture</kbd>, which we had implemented earlier. Next, we will set up and start the camera using the <kbd>VideoCapture</kbd> instance, before assigning the session to our <kbd>previewView</kbd> layer. Add the following code within the <kbd>ViewDidLoad</kbd> method under the statement <kbd>super.viewDidLoad()</kbd>:</p>
<pre>if self.videoCapture.initCamera(){<br/> (self.previewView.layer as! AVCaptureVideoPreviewLayer).session = self.videoCapture.captureSession<br/> <br/> (self.previewView.layer as! AVCaptureVideoPreviewLayer).videoGravity = AVLayerVideoGravity.resizeAspectFill<br/> <br/> self.videoCapture.asyncStartCapturing()<br/> } else{<br/> fatalError("Failed to init VideoCapture")<br/> }</pre>
<p>Most of the code should look familiar to you as a lot of it is using the methods we have just implemented. First we initialize the camera, calling the <kbd>initCamera</kbd> method of the <kbd>VideoCamera</kbd> class. Then, if successful, we assign the created <kbd>AVCaptureSession</kbd> to the layer's session. We also hint to the layer how we want it to handle the content, in this case filling the screen whilst respecting its aspect ratio. Finally, we start the camera by calling <kbd>videoCapture.asyncStartCapturing()</kbd>.</p>
<p>With that now completed, it's a good time to test that everything is working correctly. If you build and deploy on an iOS 11+ device, you should see the video frames being rendered on your phone's screen.</p>
<p>In the next section, we will walk through how to capture and process them for our model before performing inference (recognition).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing the data</h1>
                </header>
            
            <article>
                
<p><span>At this stage, we have the app rendering the frames from the camera, but we are not yet receiving any frames. To do this, we will assign ourselves to receive these frames, as implemented in the previous section. The existing <kbd>ViewController</kbd> class already has an extension implementing the <kbd>VideoCaptureDelegate</kbd> protocol. What's left to do is to assign ourselves as the delegate of the <kbd>VideoCapture</kbd> instance and implement the details of the callback method; the following is the code for <kbd>extension</kbd>:</span></p>
<pre>extension ViewController : VideoCaptureDelegate{<br/>    func onFrameCaptured(videoCapture: VideoCapture,<br/>     pixelBuffer:CVPixelBuffer?,<br/>     timestamp:CMTime){<br/>     }<br/> }</pre>
<div class="packt_infobox">Depending on your coding style, you can just as easily implement the protocols inside the main class. I tend to make use of extensions to implement the protocols—a personal preference.</div>
<p>First, let's assign ourselves as the delegate to start receiving the frames; within the <kbd>ViewDidLoad</kbd> method of the <kbd>ViewController</kbd> class, we add the following statement just before we initialize the camera:</p>
<pre>self.videoCapture.delegate = self</pre>
<p>Now that we have assigned ourselves as the delegate, we will receive frames (at the defined frame rate) via the callback:</p>
<pre>func onFrameCaptured(videoCapture: VideoCapture,<br/> pixelBuffer:CVPixelBuffer?,<br/> timestamp:CMTime){<br/> <strong>// TODO</strong><br/> }</pre>
<p>It's within this method that we will prepare and feed the data to the model to classify the dominant object within the frame. What the model is expecting is dependent on the model, so to get a better idea of what we need to pass it, let's download the trained model we will be using for this example and import it into our project. </p>
<p>Trained models can be obtained from a variety of sources; in some instances, you will need to convert them, and in other cases, you will need to train the model yourself. But in this instance, we can make use of the models Apple has made available; open up your web browser and navigate to <a href="https://developer.apple.com/machine-learning/" target="_blank">https://developer.apple.com/machine-learning/</a>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b03ed353-ba5c-4d37-aa90-c54468fc8831.png"/></div>
<p class="mce-root">You will be taken to a web page where Apple has made available a range of pretrained and converted models. Conveniently, most of the available models are specifically for object classification; <span>given our use case, </span>we're particularly interested in the models trained on a large array of objects. Our options include <span class="packt_screen">MobileNet</span>, <span class="packt_screen">SqueezeNet</span>, <span class="packt_screen">ResNet50</span>, <span class="packt_screen">Inception v3</span>, and <span class="packt_screen">VGG16</span>. Most of these have been trained on the ImageNet dataset, a dataset with reference to over 10 million URLs' images that have been manually assigned to one of 1,000 classes. References to the original research papers and performance can be obtained via the <span class="packt_screen">View original model details</span> link. For this example, we'll use <span class="packt_screen">Inception v3</span>, a good balance between size and accuracy.</p>
<div class="packt_infobox">Here, we are using the <span class="packt_screen">Inception v3</span> model, but the effort to swap the model is minimal; it requires updating the references as the generated classes are prefixed with the model's name, as you will soon see, and ensuring that you are conforming to the expected inputs of the model (which can be alleviated by using the Vision framework, as you will see in future chapters).</div>
<p class="mce-root">Click on the <span class="packt_screen">Download Core ML Model link</span> to proceed to download and, once downloaded, drag the <kbd>Inceptionv3.mlmodel</kbd> file onto the <span class="packt_screen">Project Navigator</span><span> panel on the left of Xcode, checking <span class="packt_screen">Copy items if needed</span> if desired or else leaving everything as default. Select the </span><kbd>Inceptionv3.mlmodel</kbd> <span>file from the <span class="packt_screen">Project Navigator</span> panel on the left to bring up the details within the <span class="packt_screen">Editor area</span>, as shown in the following screenshot:<br/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/d7672f9b-67a1-4755-befe-e69d230b3937.png"/></div>
<p>It is important to ensure that the model is correctly assigned to the appropriate target; in this example, this means verifying that the <span class="packt_screen">ObjectRecognition</span> target is checked, as seen here on the <span class="packt_screen">Utilities panel</span> to the right. Also worth noting are the expected inputs and outputs of the model. Here, the model is expecting a color image of size 299 x 299 for its input, and it returns a single class label as a string and a dictionary of string-double pairs of probabilities of all the classes. </p>
<p>When a <kbd>.mlmodel</kbd> file is imported, Xcode will generate a wrapper for the model itself and the input and output parameters to interface with the model; this is illustrated here: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/db3c0a61-38ff-4a49-9f2c-66eb98f65aa9.png" style="width:24.25em;height:31.50em;"/></div>
<p>You can easily access this by tapping on the arrow button next to the <kbd>Inceptionv3</kbd> label within the <span class="packt_screen">Model Class</span> section; when tapped, you will see the following code (separated into three distinct blocks to make it more legible):</p>
<pre>@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)<br/> class Inceptionv3Input : MLFeatureProvider {<br/> <br/> /// Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 299 pixels wide by 299     pixels high<br/> var image: CVPixelBuffer<br/> <br/> var featureNames: Set&lt;String&gt; {<br/>     get {<br/>         return ["image"]<br/>     }<br/> }<br/> <br/> func featureValue(for featureName: String) -&gt; MLFeatureValue? {<br/>     if (featureName == "image") {<br/>         return MLFeatureValue(pixelBuffer: image)<br/>     }<br/>     return nil<br/> }<br/> <br/> init(image: CVPixelBuffer) {<br/>     self.image = image<br/>     }<br/> }</pre>
<p>The first block of the preceding code is the input for our model. This class implements the <kbd>MLFeatureProvider</kbd> protocol, a protocol representing a collection of feature values for the model, in this case, the image feature. Here, you can see the expected data structure, <kbd>CVPixelBuffer</kbd>, along with the specifics declared (handily) in the comments. Let's continue on with our inspection of the generated classes by looking at the binding for the output:</p>
<pre>@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)<br/> class Inceptionv3Output : MLFeatureProvider {<br/> <br/> /// Probability of each category as dictionary of strings to doubles<br/> let classLabelProbs: [String : Double]<br/> <br/> /// Most likely image category as string value<br/> let classLabel: String<br/> <br/> var featureNames: Set&lt;String&gt; {<br/>     get {<br/>         return ["classLabelProbs", "classLabel"]<br/>     }<br/> }<br/> <br/> func featureValue(for featureName: String) -&gt; MLFeatureValue? {<br/>     if (featureName == "classLabelProbs") {<br/>         return try! MLFeatureValue(dictionary: classLabelProbs as [NSObject : NSNumber])<br/>     }<br/>     if (featureName == "classLabel") {<br/>         return MLFeatureValue(string: classLabel)<br/>     }<br/>     return nil<br/>}<br/> <br/> init(classLabelProbs: [String : Double], classLabel: String) {<br/>     self.classLabelProbs = classLabelProbs<br/>     self.classLabel = classLabel<br/>     }<br/> }</pre>
<p>As previously mentioned, the output exposes a directory of probabilities and a string for the dominated class, each exposed as properties or accessible using the getter method <kbd>featureValue(for featureName: String)</kbd> by passing in the feature's name. Our final extract for the generated code is the model itself; let's inspect that now:</p>
<pre>@available(macOS 10.13, iOS 11.0, tvOS 11.0, watchOS 4.0, *)<br/> class Inceptionv3 {<br/> var model: MLModel<br/> <br/> /**<br/> Construct a model with explicit path to mlmodel file<br/> - parameters:<br/> - url: the file url of the model<br/> - throws: an NSError object that describes the problem<br/> */<br/> init(contentsOf url: URL) throws {<br/> self.model = try MLModel(contentsOf: url)<br/> }<br/> <br/> /// Construct a model that automatically loads the model from the app's bundle<br/> convenience init() {<br/> let bundle = Bundle(for: Inceptionv3.self)<br/> let assetPath = bundle.url(forResource: "Inceptionv3", withExtension:"mlmodelc")<br/> try! self.init(contentsOf: assetPath!)<br/> }<br/> <br/> /**<br/> Make a prediction using the structured interface<br/> - parameters:<br/> - input: the input to the prediction as Inceptionv3Input<br/> - throws: an NSError object that describes the problem<br/> - returns: the result of the prediction as Inceptionv3Output<br/> */<br/> func prediction(input: Inceptionv3Input) throws -&gt; Inceptionv3Output {<br/> let outFeatures = try model.prediction(from: input)<br/> let result = Inceptionv3Output(classLabelProbs: outFeatures.featureValue(for: "classLabelProbs")!.dictionaryValue as! [String : Double], classLabel: outFeatures.featureValue(for: "classLabel")!.stringValue)<br/> return result<br/> }<br/> <br/> /**<br/> Make a prediction using the convenience interface<br/> - parameters:<br/> - image: Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 299 pixels wide by 299 pixels high<br/> - throws: an NSError object that describes the problem<br/> - returns: the result of the prediction as Inceptionv3Output<br/> */<br/> func prediction(image: CVPixelBuffer) throws -&gt; Inceptionv3Output {<br/> let input_ = Inceptionv3Input(image: image)<br/> return try self.prediction(input: input_)<br/> }<br/> }</pre>
<p>This class wraps the model class and provides strongly typed methods for performing inference via the <kbd>prediction(input: Inceptionv3Input)</kbd> and <kbd>prediction(image: CVPixelBuffer)</kbd> methods, each returning the output class we saw previously—<kbd>Inceptionv3Output</kbd>. Now, knowing what our model is expecting, let's continue to implement the preprocessing functionality required for the captured frames in order to feed them into the model.</p>
<div class="packt_infobox">Core ML 2 introduced a the ability to work with batches; if your model was compiled with Xcode 10+ then you will also see the additional method <kbd>&lt;CODE&gt;func predictions(from: MLBatchProvider, options: MLPredictionOptions)&lt;/CODE&gt;</kbd> allowing you to perform inference on a batch of inputs.</div>
<p>At this stage, we know that we are receiving the correct data type (<kbd>CVPixelBuffer</kbd>) and image format (explicitly defined in the settings when configuring the capture video output instance <kbd>kCVPixelFormatType_32BGRA</kbd>) from the camera. But we are receiving an image significantly larger than the expected size of 299 x 299. Our next task will be to create some utility methods to perform resizing and cropping. </p>
<p>For this, we will be extending <kbd>CIImage</kbd> to wrap and process the pixel data we receive along with making use of <kbd>CIContext</kbd> to obtain the raw pixels again. If you're unfamiliar with the CoreImage framework, then it suffices to say that it is a framework dedicated to efficiently processing and analyzing images. <kbd>CIImage</kbd> can be considered the base data object of this framework that is often used in conjunction with other CoreImage classes such as <kbd>CIFilter</kbd>, <kbd>CIContext</kbd>, <kbd>CIVector</kbd>, and <kbd>CIColor</kbd>. Here, we are interested in <kbd>CIImage</kbd> as it provides convenient methods for manipulating images along with <kbd>CIContext</kbd> to extract the raw pixel data from <kbd>CIImage</kbd> (<kbd>CVPixelBuffer</kbd>). </p>
<p>Back in Xcode, select the <kbd>CIImage.swift</kbd> file from the <span class="packt_screen">Project navigator</span> to open it up in the <span class="packt_screen">Editor area</span>. In this file, we have extended the <kbd>CIImage</kbd> class with a method responsible for rescaling and another for returning the raw pixels <span>(</span><kbd>CVPixelBuffer</kbd><span>), a format required for our Core ML model:</span></p>
<pre>extension CIImage{<br/> <br/> func resize(size: CGSize) -&gt; CIImage {<br/>     fatalError("Not implemented")<br/> }<br/> <br/> func toPixelBuffer(context:CIContext,<br/> size insize:CGSize? = nil,<br/>     gray:Bool=true) -&gt; CVPixelBuffer?{<br/>         fatalError("Not implemented")<br/>     }<br/> }</pre>
<p>Let's start by implementing the <kbd>resize</kbd> method; this method is passed in the desired size, which we'll use to calculate the relative scale; then we'll use this to scale the image uniformly. Add the following code snippet to the <kbd>resize</kbd> method, replacing the <kbd>fatalError("Not implemented")</kbd> statement:</p>
<pre>let scale = min(size.width,size.height) / min(self.extent.size.width, self.extent.size.height)<br/> <br/> let resizedImage = self.transformed(<br/> by: CGAffineTransform(<br/> scaleX: scale,<br/> y: scale))</pre>
<p>Unless the image is a square, we are likely to have an overflow either vertically or horizontally. To handle this, we will simply center the image and crop it to the desired size; do this by appending the following code to the <kbd>resize</kbd> method (beneath the code written in the preceding snippet): </p>
<pre>let width = resizedImage.extent.width<br/> let height = resizedImage.extent.height<br/> let xOffset = (CGFloat(width) - size.width) / 2.0<br/> let yOffset = (CGFloat(height) - size.height) / 2.0<br/> let rect = CGRect(x: xOffset,<br/> y: yOffset,<br/> width: size.width,<br/> height: size.height)<br/> <br/> return resizedImage<br/> .clamped(to: rect)<br/> .cropped(to: CGRect(<br/> x: 0, y: 0,<br/> width: size.width,<br/> height: size.height))</pre>
<p>We now have the functionality to rescale the image; our next piece of functionality is to obtain a <kbd>CVPixelBuffer</kbd> from the <kbd>CIImage</kbd>. Let's do that by implementing the body of the <kbd>toPixelBuffer</kbd> method. Let's first review the method's signature and then briefly talk about the functionality required:</p>
<pre>func toPixelBuffer(context:CIContext, gray:Bool=true) -&gt; CVPixelBuffer?{<br/>     fatalError("Not implemented")<br/> }</pre>
<p>This method is expecting a <kbd>CIContext</kbd> and flag indicating whether the image should be grayscale (single channel) or full color; <kbd>CIContext</kbd> will be used to render the image to a pixel buffer (our <kbd>CVPixelBuffer</kbd>). Let's now flesh out the implementation for <kbd>toPixelBuffer</kbd> piece by piece. </p>
<div class="packt_tip">The preprocessing required on the image (resizing, grayscaling, and normalization) is dependent on the Core ML model and the data it was trained on. You can get a sense of these parameters by inspecting the Core ML model in Xcode. If you recall, the expected input to our model is (image color 299 x 299); this tells us that the Core ML model is expecting the image to be color (three channels) and 299 x 299 in size. </div>
<p>We start by creating the pixel buffer we will be rendering our image to; a<span>dd the following code snippet to the body of the</span> <kbd>toPixelBuffer</kbd> <span>method, replacing the</span> <kbd>fatalError("Not implemented")</kbd> <span>statement:</span></p>
<pre>let attributes = [<br/> kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,<br/> kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue<br/> ] as CFDictionary<br/> <br/> var nullablePixelBuffer: CVPixelBuffer? = nil<br/> let status = CVPixelBufferCreate(kCFAllocatorDefault,<br/> Int(self.extent.size.width),<br/> Int(self.extent.size.height),<br/> gray ? kCVPixelFormatType_OneComponent8 : kCVPixelFormatType_32ARGB,<br/> attributes,<br/> &amp;nullablePixelBuffer)<br/> <br/> guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer<br/> else { return nil }</pre>
<p>We first create an array to hold the attributes defining the compatibility requirements for our pixel buffer; here, we specify that we want our pixel buffer to be compatible with <kbd>CGImage</kbd> types (<kbd>kCVPixelBufferCGImageCompatibilityKey</kbd>) and compatible with CoreGraphics bitmap contexts (<kbd>kCVPixelBufferCGBitmapContextCompatibilityKey</kbd>).</p>
<p>We then proceed to create a pixel buffer, passing in our compatibility attributes, the format (either grayscale or full color depending on the value of <kbd>gray</kbd>), width, height, and pointer to the variable. Next, we unwrap the nullable pixel buffer as well as ensure that the call was successful; if either of these is <kbd>false</kbd>, we return <kbd>NULL</kbd>. Otherwise, we're ready to render our <kbd>CIImage</kbd> into the newly created pixel buffer. Append the following code to the <kbd>toPixelBuffer</kbd> method:</p>
<pre>CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))<br/> <br/> context.render(self,<br/> to: pixelBuffer,<br/> bounds: CGRect(x: 0,<br/> y: 0,<br/> width: self.extent.size.width,<br/> height: self.extent.size.height),<br/> colorSpace:gray ?<br/> CGColorSpaceCreateDeviceGray() :<br/> self.colorSpace)<br/> <br/> CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))<br/> <br/> return pixelBuffer</pre>
<p>Before drawing, we lock the address of the pixel buffer via <kbd>CVPixelBufferLockBaseAddress</kbd> and then unlock once we've finished using the <kbd>CVPixelBufferUnlockBaseAddress</kbd> method. We are required to do this when accessing pixel data from the CPU, which we are doing here. </p>
<p>Once locked, we simply use the <kbd>CIContext</kbd> to render the scaled image to the buffer, passing in the destination rectangle (in this case, the full size of the pixel buffer) and destination color space, which is full color or grayscale depending on the value of <kbd>gray</kbd> as mentioned previously. After unlocking the pixel buffer, as described earlier, we return our newly created pixel buffer.</p>
<p>We have now extended the <kbd>CIImage</kbd> with two convenient methods, one responsible for rescaling and the other for creating a pixel buffer representation of itself. We will now return to the <kbd>ViewController</kbd> class to handle the preprocessing steps required before passing our data into the model. Select the <kbd>ViewController.swift</kbd> file from the <span class="packt_screen">Projector navigator</span> panel within Xcode to bring up the source code, and within the body of the <kbd>ViewController</kbd> class, add the following variable:</p>
<pre>let context = CIContext()</pre>
<p>As previously discussed, we will be passing this to our <kbd>CIImage.toPixelBuffer</kbd> method for rendering the image to the pixel buffer. Now return to the <kbd>onFrameCaptured</kbd> method and add the following code, to make use of the methods we've just created for preprocessing:</p>
<pre>guard let pixelBuffer = pixelBuffer else{ return }<br/> <br/> // Prepare our image for our model (resizing)<br/> guard let scaledPixelBuffer = CIImage(cvImageBuffer: pixelBuffer)<br/> .resize(size: CGSize(width: 299, height: 299))<br/> .toPixelBuffer(context: context) else{ return }</pre>
<p>We first unwrap the <kbd>pixelBuffer</kbd>, returning if it is <kbd>NULL</kbd>; then we create an instance of <kbd>CIImage</kbd>, passing in the current frame and then chaining our extension methods to perform rescaling (299 x 299) and rendering out to a pixel buffer (setting the gray parameter to false as the model is expecting full color images). If successful, we are returned a image ready to be passed to our model for inference, the focus of the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing inference </h1>
                </header>
            
            <article>
                
<p>This may come as a bit of an anticlimax for someone expecting some hardcore coding, but its simplicity definitely pays tribute to the effort of Apple's engineers in making this framework one of the most accessible ways to work with a ML model. Without further ado, let's put the final pieces together; we start by instantiating an instance of our model we had imported in the previous section.</p>
<p>Near the top, but within the body of the <kbd>ViewController</kbd> class, add the following line:</p>
<pre>let model = Inceptionv3()</pre>
<p>Our model is now ready; we return to the <kbd>onFrameCaptured</kbd> method, starting from where we previously left off, and add the following code snippet:</p>
<pre><strong>let prediction = try? self.model.prediction(image:scaledPixelBuffer)</strong><br/> <br/> // Update label<br/> DispatchQueue.main.sync {<br/> classifiedLabel.text = prediction?.classLabel ?? "Unknown"<br/> }</pre>
<p>In case you have missed it, I have made the statement performing inference in bold. That's it!</p>
<p>After performing inference, we simply assign the <kbd>classLabel</kbd> property (the class with the highest probability) to our <kbd>UILabel</kbd>, <kbd>classifiedLabel</kbd>.</p>
<p>With the final piece put in place, we build and deploy. And see how well our app performs, recognizing some objects we have lying nearby. Once you're done surveying your space, return here, where we will wrap up this chapter and move on to greater and more impressive examples. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we introduced object recognition, the 101 project for ML with Core ML. We spent some time introducing CNNs or ConvNets, a category of neural networks well suited for extracting patterns from images. We discussed how they build increasing levels of abstraction with each c<span>onvolutional layer. We then proceeded to make use of our newfound knowledge by implementing the functionality that allowed our application to recognize the physical world through its camera. We saw firsthand that the majority of the work wasn't performing inference but rather implementing the functionality to facilitate and make use of it. This is the take-away; intelligence by itself is not useful. What we are interested in exploring in this book is the application of trained models to deliver intuitive and intelligent experiences. For instance, this example can easily be turned into a language tutor assistant, allowing the user to learn a new language by observing the world around them. </span></p>
<p class="mce-root">In the next chapter, we will continue our journey into the world of computer vision with Core ML by looking at how we can infer the emotional state of someone by recognizing their facial expressions. Let's get to it. </p>


            </article>

            
        </section>
    </body></html>