- en: '*Chapter 12*: Introducing Hyperparameter Tuning Decision Map'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting too much information can sometimes lead to confusion, which can, in
    turn, lead back to adopting the simplest option. We learned about numerous hyperparameter
    tuning methods in the previous chapters. Although we have discussed the ins and
    outs of each method, it will be very useful for us to have a single source of
    truth that can be used to help us decide which method to use in which situation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll be introduced to the **Hyperparameter Tuning Decision
    Map** (**HTDM**), which summarizes all of the discussed hyperparameter tuning
    methods into a simple decision map based on many aspects, including the properties
    of the hyperparameter space, the complexity of the objective function, training
    data size, computational resources availability, prior knowledge availability,
    and the types of ML algorithms we are working with. There will be also three study
    cases that show how to utilize HTDM in practice.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to utilize HTDM in practice to help
    you decide which hyperparameter tuning method to be adopted in your specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with HTDM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study 1 – using HTDM with a CatBoost classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study 2 – using HTDM with a conditional hyperparameter space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study 3 – using HTDM with prior knowledge of the hyperparameter values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with HTDM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HTDM is designed to help you decide which hyperparameter tuning method should
    be adopted in a particular situation (see *Figure 12.1*). Here, the situation
    is defined based on six aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter space properties, including the size of the space, types of hyperparameter
    values (numerical only or mixed), and whether it contains conditional hyperparameters
    or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Objective function complexity: whether it is a cheap or expensive objective
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computational resource availability: whether or not you have enough parallel
    computational resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data size: whether you have a few, moderate, or a large number of
    training samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prior knowledge availability: whether you have prior knowledge of the good
    range of hyperparameter values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Types of ML algorithms: whether you are working with a small, medium, or large-sized
    model, and whether you are working with a traditional ML or deep learning type
    of algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Hyperparameter Tuning Decision Map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – HTDM
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of *Small*, *Medium*, and *Large* in HTDM is very subjective.
    However, you can refer to the following table as a rule of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Rule of thumb of size definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Rule of thumb of size definition
  prefs: []
  type: TYPE_NORMAL
- en: 'The following important notes may also help us decide which hyperparameter
    tuning method we should adopt in a particular situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Important notes for each hyperparameter tuning method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_003_(a).jpg)![Figure 12.3 – Important notes for each hyperparameter
    tuning method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_003_(b).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Important notes for each hyperparameter tuning method
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed HTDM, along with several additional important
    notes to help you decide which hyperparameter tuning method you should adopt in
    a particular situation. In the next few sections, we will learn how to utilize
    HTDM in practice through several interesting study cases.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 1 – using HTDM with a CatBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say we are training a classifier based on the marketing campaign data
    that was introduced in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via scikit*. Here, we are utilizing CatBoost (see [*Chapter
    11*](B18753_11_ePub.xhtml#_idTextAnchor110)*, Understanding Hyperparameters of
    Popular Algorithms*) as the classifier. This is our first time working with the
    given data. The laptop we are using only has a single-core CPU and the hyperparameter
    space is defined as follows. Note that we are not working with a conditional hyperparameter
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iterations`: `randint(5,200)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth`: `randint(3,10)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: `np.linspace(1e-5,1e-3,20)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_leaf_reg`: `np.linspace(1,30,30)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`one_hot_max_size`: `randint(2,15)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on the given case description, we can try to utilize HTDM to help us
    choose which hyperparameter tuning suits the condition the best. First of all,
    we know that we do not have any prior knowledge or meta-learning results of the
    good hyperparameter values on the given data. This means we will only focus on
    the right branch of the first node in HTDM, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Case study 1, no prior knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Case study 1, no prior knowledge
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that we are not working with a conditional hyperparameter space. This
    means we will only focus on the right branch of the second node, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Case study 1, not a conditional hyperparameter space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Case study 1, not a conditional hyperparameter space
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on a rough estimation, our CatBoost model’s size should be in the range
    of small to medium-sized. This means we will only focus on the left and bottom
    branches of the third node, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Case study 1, small to medium model size'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Case study 1, small to medium model size
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have a medium-sized hyperparameter space that consists of only numerical
    values. This means our options are Coarse-to-Fine, Random Search, PSO, Simulated
    Annealing, and Genetic Algorithm. It is worth noting that even though our hyperparameter
    space consists of only numerical values, we can still utilize hyperparameter tuning
    methods that work with mixed types of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Case study 1, medium-sized hyperparameter space with only numerical
    values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Case study 1, medium-sized hyperparameter space with only numerical
    values
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we choose a hyperparameter tuning method from the selected options?
    First, we know that PSO only works very well on the continuous type of hyperparameter
    values while we also have integers in the hyperparameter space. Thus, we can remove
    PSO from our options. This leaves us with the remaining four options. One easy
    and effective way to choose the best hyperparameter tuning method is by choosing
    the simplest method, which is the Random Search method.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the first case study on how to utilize HTDM in
    practice. In the next section, we will do the same using another interesting case
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 2 – using HTDM with a conditional hyperparameter space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say we are faced with a similar condition as in the previous section
    but now, we are working with a conditional hyperparameter space, as defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the given case description, we can try to utilize HTDM again to help
    us choose which hyperparameter tuning method suits the condition the best. Here,
    similar to the previous study case, we know that we do not have any prior knowledge
    or meta-learning results of the good hyperparameter values on the given data.
    This means we will only focus on the right branch of the first node in HTDM (see
    *Figure 12.4*). However, in this case, we are now working with a conditional hyperparameter
    space. This means we will only focus on the left branch of the second node, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Case study 2, a conditional hyperparameter space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Case study 2, a conditional hyperparameter space
  prefs: []
  type: TYPE_NORMAL
- en: Since we have more than 10,000 samples of training data (see [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via scikit*), we only have two hyperparameter tuning methods
    to choose from based on HTDM, namely the BOHB or Random Search method (see *Figure
    12.9*). Choosing Random Search over BOHB surely is a wise choice if we only compare
    them based on the simplicity of the implementation since we need to install the
    Microsoft NNI package just to adopt the BOHB method (see *Figure 12.3*).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we know that we are working with a model that is not very small, and
    BOHB can decide which subspace needs to be searched based on previous experiences,
    not based on luck. Thus, in theory, BOHB will be a better choice to save us time
    searching for the best set of hyperparameters. So, which method should we pick?
    It’s up to your discretion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Case study 2, large training data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Case study 2, large training data
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the second case study on how to utilize HTDM in
    practice. In the next section, we will do the same using another interesting case
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 3 – using HTDM with prior knowledge of the hyperparameter values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say, in this case, we are also faced with a similar condition as in the
    previous case study, but this time, we have prior knowledge of the good hyperparameter
    values for the given data since one of the data scientists in our team has worked
    with the same data previously. This means we will only focus on the left branch
    of the first node in HTDM, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Case study 3, have prior knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Case study 3, have prior knowledge
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the given case description, we know that we do not have enough parallel
    computational resources since we only have a single-core CPU. This means we will
    only focus on the right branch of the second node, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Case study 3, not enough parallel computational resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – Case study 3, not enough parallel computational resources
  prefs: []
  type: TYPE_NORMAL
- en: 'We also know that we have a medium-sized hyperparameter space that only consists
    of numerical types of values. This means our options are SMAC, TPE, and Metis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Case study 3, medium-sized hyperparameter space with only
    numerical values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_12_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Case study 3, medium-sized hyperparameter space with only numerical
    values
  prefs: []
  type: TYPE_NORMAL
- en: Based on the preceding diagram, we know that SMAC works best when the hyperparameter
    space is dominated by categorical hyperparameters, which is not the case here.
    Thus, we can remove SMAC from our options. If we try to decide based on the implementation
    popularity, then TPE is the one we should choose since it’s implemented in Hyperopt,
    Optuna, and NNI, while Metis is only implemented in NNI. However, one of the main
    selling points of Metis is its ability to suggest the set of hyperparameters we
    should test in our next trial. So, which method should we pick? It’s up to you.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the third case study on how to utilize HTDM in
    practice. Now, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we summarized all of the hyperparameter tuning methods we’ve
    discussed so far in a simple decision map called HTDM. This can help you to choose
    which method is the most suitable for your specific problem. We also discussed
    several important notes for each of the hyperparameter tuning methods and saw
    how to utilize the HTDM in practice. From now on, you’ll be able to utilize HTDM
    in practice to help you decide which hyperparameter tuning method to adopt in
    your specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll discuss the need to track hyperparameter tuning experiments
    and learn how to do so using several open source packages.
  prefs: []
  type: TYPE_NORMAL
