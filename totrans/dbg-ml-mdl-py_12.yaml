- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going Beyond ML Debugging with Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most recent advancements in machine learning have been achieved through
    deep learning modeling. In this chapter, we will introduce deep learning and PyTorch
    as a framework to use for deep learning modeling. As the focus of this book is
    not on introducing different machine learning and deep learning algorithms in
    detail, we will focus on opportunities that deep learning provides for you to
    develop high-performance models, or use available ones, that can be built on top
    of the techniques reviewed in this chapter and the next two.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frameworks for neural network modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about some theoretical aspects
    of deep learning focusing on fully connected neural networks. You will have also
    practiced with PyTorch, a widely used deep learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` >= 2.0.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchvision` >= 0.15.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also require basic knowledge of the difference between different types
    of machine learning models, such as classification, regression, and clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our natural networks of neurons work as decision-making systems with information
    processing units called neurons that help us with, for example, recognizing the
    faces of our friends. **Artificial neural networks** (**ANNs**) work similarly.
    Dissimilar to having a giant network of neurons, as in our bodies, that take care
    of all decision-making, active or reactive, ANNs are designed to be problem-specific.
    For example, we have ANNs for image classification, credit risk estimation, object
    detection, and more. We will use neural networks instead of ANNs for simplicity
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to focus on **fully connected neural networks** (**FCNNs**),
    which work on tabular data (*Figure 12**.1*). FCNNs and **multi-layer perceptrons**
    (**MLPs**) are used interchangeably in many resources. To be able to better compare
    different types of neural networks, we will use FCNNs instead of MLPs in this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Schematic illustration of an FCNN and an individual neuron](img/B16369_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Schematic illustration of an FCNN and an individual neuron
  prefs: []
  type: TYPE_NORMAL
- en: FCNNs for supervised learning have one input, one output, and one or multiple
    hidden (middle) layers. A neural network with more than three layers, inclusive
    of the input and the output layers in supervised models, is called a deep neural
    network, and deep learning refers to modeling with such networks (Hinton and Salakhutdinov,
    2006).
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is nothing other than the features of data points used for modeling.
    The number of neurons in the output layer is also determined based on the problem
    at hand. For example, in the case of binary classification, two neurons in the
    output layer represent two classes. The number and size of hidden layers are among
    the hyperparameters of an FCNN and can be optimized to improve FCNN performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each neuron in an FCNN receives a weighted sum of output values from neurons
    in the previous layer, applies a linear or nonlinear transformation to the received
    sum of values, and then outputs the resulting value to other neurons of the next
    layer. The weights used in the input value calculation of each neuron are the
    learned weights (parameters) in the training process. The nonlinear transformations
    are applied through predetermined activation functions (*Figure 12**.2*). FCNNs
    are known for coming up with complicated nonlinear relationships between input
    feature values and outputs, which makes them flexible in figuring out (maybe)
    different kinds of relationships between inputs and outputs. In FCNNs, activation
    functions that are applied to information that’s been received in neurons are
    responsible for that complexity or flexibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Widely used activation functions in neural network modeling](img/B16369_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Widely used activation functions in neural network modeling
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these activation functions, such as `sigmoid` and `softmax` functions
    are commonly used in output layers to transform the scores of the output neurons
    into values between zero and one for classification models; these are known as
    probabilities of predictions. There are also other activation functions such as
    **Gaussian error linear unit** (**GELU**) (Hendrycks and Gimpel, 2016) that have
    been used in more recent models such as **generative pre-trained transformer**
    (**GPT**), which will be explained in the next chapter. Here is the formula for
    GELU:'
  prefs: []
  type: TYPE_NORMAL
- en: GELU(z) = 0.5z(1 + tanh(√ _  2 _ π  (z + 0.044715 z 3)))
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning has two main processes: predicting outputs and learning
    from the incorrectness or correctness of predictions. In FCNNs, predictions happen
    in forward propagation. The weights of the FCNNs between the input and first hidden
    layer are used to calculate the input values of the neurons of the first hidden
    layer and similarly for other layers in the FCNN (*Figure 12**.3*). Going from
    input to output is called forward propagation or forward pass, which generates
    the output values (predictions) for each data point. Then, in the backward propagation
    (backpropagation) or backward pass, FCNN uses the predicted outputs and their
    differences with actual outputs to adjust its weights, resulting in better predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Schematic illustration of forward propagation and backpropagation
    for output prediction and parameter update, respectively](img/B16369_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Schematic illustration of forward propagation and backpropagation
    for output prediction and parameter update, respectively
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of a neural network get determined in the training process using
    an optimization algorithm. Now, we will review some widely used optimization algorithms
    in neural network settings.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization algorithms work behind the scenes, trying to minimize the loss
    function to identify the optimal parameters when you train a machine learning
    model. At each step in the training process, an optimization algorithm decides
    how to update each of the weights or parameters in a neural network, or other
    machine learning models. Most optimization algorithms rely on the gradient vector
    of the cost function to update the weights. The main difference is how the gradient
    vector is used and what data points are used to calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In gradient descent, all the data points are used to calculate the gradient
    of the cost function; then, the weights of the model get updated in the direction
    of maximum decrease of cost. Despite the effectiveness of this method for small
    datasets, it can become computationally expensive and unsuitable for large datasets
    as for every iteration of learning, the cost needs to be calculated for all the
    data points simultaneously. The alternative approach is **stochastic gradient
    descent** (**SGD**); instead of all data points, one data point gets selected
    in each iteration to calculate the cost and update the weights. But using one
    data point at a time causes a highly oscillating behavior in updating weights.
    Instead, we can use mini-batch gradient descent, which is commonly called SGD
    in tutorials and tools, in which instead of all data points or only one in each
    iteration, it will use a batch of data points to update the weights. The mathematics
    behind these three approaches is shown in *Figure 12**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch
    gradient descent optimization algorithms](img/B16369_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch
    gradient descent optimization algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Other optimization algorithms have been suggested in recent years to improve
    the performance of neural network models across a variety of applications, such
    as the Adam optimizer (Kingma and Ba, 2014). One of the intuitions behind this
    approach is to avoid diminishing gradients in the optimization process. Getting
    further into the details of different optimization algorithms is beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In neural network modeling, there are two important terms that you need to
    know the definition of: *epoch* and *batch size*. When training a neural network
    model using different frameworks, which we will review in the next section, you
    need to specify the *batch size* and the number of *epochs*. In each iteration
    of optimization, a subset of data points, or a mini-batch as in mini-batch gradient
    descent (*Figure 12**.4*), gets used to calculate loss; then, the parameters of
    the model get updated using backpropagation. This process gets repeated to cover
    all the data points in the training data. Epoch is a term we use to specify how
    many times all the training data is used during the optimization process. For
    example, specifying an epoch of 5 means that the model gets trained until all
    the data points in the training process are used five times in the optimization
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the basics of neural network modeling, we are ready to introduce
    frameworks for neural network modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks for neural network modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multiple frameworks have been used for neural network modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch ([https://pytorch.org/](https://pytorch.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow ([https://www.tensorflow.org/learn](https://www.tensorflow.org/learn))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras ([https://keras.io/](https://keras.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe ([https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXNet ([https://mxnet.apache.org/versions/1.9.1/](https://mxnet.apache.org/versions/1.9.1/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we will focus on PyTorch in practicing deep learning, but the
    concepts we’ll introduce are independent of the framework you use in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch for deep learning modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch is an open source deep learning framework, based on the `Torch` library,
    developed by Meta AI. You can easily integrate PyTorch with Python’s scientific
    computing libraries in your deep learning projects. Here, we will practice using
    PyTorch by looking at a simple example of building an FCNN model using the MNIST
    digit dataset. It is a commonly used example and the objective is solely to understand
    how to train and test a deep learning model using PyTorch if you don’t have experience
    with that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the required libraries and load the dataset for training
    and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will determine the hyperparameters of the model and its `input_size`,
    which is the number of neurons in the input layer; this is the same as the number
    of features in our data. In this example, it is equal to the number of pixels
    in each image as we are considering each pixel as one feature to build an FCNN
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will import `torch.nn`, from which we can add linear neural network
    layers for our FCNN model and write a class to determine the architecture of our
    network, which is a network with one hidden layer whose size is 256 (with 256
    neurons):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `torch.nn.Linear()` class adds a linear layer and has two input arguments:
    the number of neurons in the current and next layer, respectively. For the first,
    `nn.Linear()`, the first argument has to be equal to the number of features, while
    the second argument of the last `nn.Linear()` input argument in the network initialization
    class needs to be equal to the number of classes in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must define our cross-entropy loss function and our optimizer object
    using the Adam optimizer from `torch.optim()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train our model. As you can see in the following code block,
    we have a loop over epochs and another internal loop over each batch. Within the
    internal loop, we have three important steps that are common across most supervised
    models that use PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the output of the model for the data points within the batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss using the true labels and the predicted output for the data
    points of that batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate and update the parameters of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we must train the model on the MNIST training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of epoch 10, we have a model with a loss of 0.0214 in the training
    set. Now, we can use the following code to calculate the accuracy of the model
    in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This results in 98.4% for the model in the MNIST test set.
  prefs: []
  type: TYPE_NORMAL
- en: There are more than 10 different optimization algorithms, including the Adam
    optimization algorithm, available in PyTorch ([https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)),
    which helps you in training your deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss hyperparameter tuning, model interpretability, and fairness
    in deep learning settings. We will also introduce PyTorch Lightning, which will
    help you in your deep learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning for deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In deep learning modeling, hyperparameters are key factors in determining its
    performance. Here are some of the hyperparameters of FCNNs you can work with to
    improve the performance of your deep learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture**: The architecture of an FCNN refers to the number of hidden
    layers and their sizes, or the number of neurons. More layers result in higher
    depth in a deep learning model and could result in more complex models. Although
    the depth of neural network models has been shown to improve performance on large
    datasets in many cases (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014;
    Szegedy et al., 2015; He et al., 2016), the majority of the success stories behind
    the positive effect of higher depth on performance are outside of FCNNs. But architecture
    is still an important hyperparameter that needs to be optimized to find a high-performance
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation functions**: Despite commonly used activation functions in each
    field and problem, you can still identify the best one for your problem. Remember
    that you don’t have to use the same function across all layers, although we usually
    stick to one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: Changing batch size changes both the performance and speed
    of convergence of your models. But usually, it doesn’t have a significant effect
    on performance, except in the steep part of the learning curve in the first few
    epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: The learning rate determines the speed of convergence. A
    higher learning rate causes faster convergence but it might also cause oscillation
    around the local optimum point or even divergence. Algorithms such as the Adam
    optimizer control the diminishing convergence rate when we get closer to the local
    optima during the optimization process, but we can still play with the learning
    rate as a hyperparameter in deep learning modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of epochs**: Deep learning models have a steep learning curve for
    the first few epochs, depending on the learning rate and batch size, and then
    start plateauing on performance. Using enough epochs is important to make sure
    you get the best possible model out of your training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: We talked about the importance of regulations in controlling
    overfitting and improving generalizability in [*Chapter 5*](B16369_05.xhtml#_idTextAnchor183),
    *Improving the Performance of Machine Learning Models*, by preventing the model
    from heavily relying on individual neurons and potentially improving generalizability.
    For example, if dropout is set to 0.2, each neuron has a 20% chance of getting
    zero out during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight decay**: This is a form of L2 regularization that adds a penalty to
    the weights of the neural network. We introduced L2 regularization in [*Chapter
    5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance of Machine*
    *Learning Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can use different hyperparameter optimization tools such as Ray Tune alongside
    PyTorch to train your deep learning models and optimize their hyperparameters.
    You can read more about it in this tutorial available on the PyTorch website:
    [https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to hyperparameter tuning, PyTorch has different functionalities
    and associated libraries for tasks such as model interpretability and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced multiple explainability techniques and libraries in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine Learning Modeling*, that can help you in explaining complex machine learning
    and deep learning models. Captum AI ([https://captum.ai/](https://captum.ai/))
    is another open source model interpretability library developed by Meta AI for
    deep learning projects using PyTorch. You can easily integrate Captum into your
    existing or future PyTorch-based machine learning pipelines. You can benefit from
    different explainability and interpretability techniques such as integrated gradients,
    GradientSHAP, DeepLIFT, and saliency maps through Captum.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness in deep learning models developed by PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discussed the importance of fairness and introduced different notions, statistical
    measures, and techniques to help you in assessing and eliminating bias in your
    models as part of [*Chapter 7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing
    Bias and Achieving Fairness*. `FairTorch` ([https://github.com/wbawakate/fairtorch](https://github.com/wbawakate/fairtorch))
    and `inFairness` ([https://github.com/IBM/inFairness](https://github.com/IBM/inFairness))
    are two other libraries you can use for fairness and bias assessment for your
    deep learning modeling using PyTorch. You can benefit from `inFairness` in auditing,
    training, and post-processing your models for individual fairness. `Fairtorch`
    also provides you with tools to mitigate bias in classification and regression,
    though this is currently limited to binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Lightning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyTorch Lightning is an open source, high-level framework that simplifies the
    process of developing and training deep learning models using PyTorch for you.
    Here are some of the features of PyTorch Lightning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured code**: PyTorch Lightning organizes code into a Lightning Module
    that helps you in separating the model architecture, data handling, and training
    logic, making the code more modular and easier to maintain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training loop abstraction**: You can avoid repetitive code for the training,
    validation, and testing loops using PyTorch Lightning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed training**: PyTorch Lightning simplifies the process of scaling
    deep learning models across multiple GPUs or nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment tracking and logging**: PyTorch Lightning integrates with experiment
    tracking and logging tools such as MLflow and Weights & Biases, which make monitoring
    your deep learning model training easier for you'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic optimization**: PyTorch Lightning automatically handles the optimization
    process, manages optimizers and learning rate schedulers, and makes it easier
    to switch between different optimization algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite all these factors, there is more to deep learning modeling than FCNNs,
    as we’ll see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about deep learning modeling with FCNNs. We practiced
    using PyTorch with a simple deep learning model to help you start performing deep
    learning modeling using PyTorch if you haven’t had that experience already. You
    also learned about the important hyperparameters of FCNNs, tools for model interpretability
    and fairness that you can use in deep learning settings, and PyTorch Lightning
    as an open source high-level framework to simplify deep learning modeling for
    you. You are now ready to learn more about PyTorch, PyTorch Lightning, and deep
    learning and start benefitting from them in your problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about other more advanced types of deep
    learning models, including the convolutional neural network, transformer, and
    graph convolutional network models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do the parameters of a neural network model get updated in backpropagation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between stochastic and mini-batch gradient descent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you explain the difference between a batch and an epoch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you provide an example of where you need to use the sigmoid and softmax
    functions in your neural network models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. *Deep learning*. nature 521.7553
    (2015): 436-444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, G. E., & Salakhutdinov, R. R. (2006). *Reducing the Dimensionality of
    Data with Neural Networks*. Science, 313(5786), 504-507.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abiodun, Oludare Isaac, et al. *State-of-the-art in artificial neural network
    applications: A survey*. Heliyon 4.11 (2018): e00938.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks, D., & Gimpel, K. (2016). *Gaussian Error Linear Units (GELUs)*. arXiv
    preprint arXiv:1606.08415.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma, D. P., & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*.
    arXiv preprint arXiv:1412.6980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kadra, Arlind, et al. *Well-tuned simple nets excel on tabular datasets*. Advances
    in neural information processing systems 34 (2021): 23928-23941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems (pp. 1097-1105).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan, K., & Zisserman, A. (2014). *Very deep convolutional networks for
    large-scale image recognition*. arXiv preprint arXiv:1409.1556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep residual learning for image
    recognition*. In Proceedings of the IEEE conference on computer vision and pattern
    recognition (pp. 770-778).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich,
    A. (2015). *Going deeper with convolutions*. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (pp. 1-9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
