- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Going Beyond ML Debugging with Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习超越机器学习调试
- en: The most recent advancements in machine learning have been achieved through
    deep learning modeling. In this chapter, we will introduce deep learning and PyTorch
    as a framework to use for deep learning modeling. As the focus of this book is
    not on introducing different machine learning and deep learning algorithms in
    detail, we will focus on opportunities that deep learning provides for you to
    develop high-performance models, or use available ones, that can be built on top
    of the techniques reviewed in this chapter and the next two.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习最新的进展是通过深度学习建模实现的。在本章中，我们将介绍深度学习以及PyTorch作为深度学习建模的框架。由于本书的重点不是详细介绍不同的机器学习和深度学习算法，我们将关注深度学习为你提供的机会，以开发高性能模型，或使用可用的模型，这些模型可以建立在本章和下一章回顾的技术之上。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to artificial neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络简介
- en: Frameworks for neural network modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络建模框架
- en: By the end of this chapter, you will have learned about some theoretical aspects
    of deep learning focusing on fully connected neural networks. You will have also
    practiced with PyTorch, a widely used deep learning framework.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解深度学习的某些理论方面，重点关注全连接神经网络。你还将使用广泛使用的深度学习框架PyTorch进行实践。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要求应考虑本章，因为它们将帮助你更好地理解概念，在项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`torch` >= 2.0.0'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch` >= 2.0.0'
- en: '`torchvision` >= 0.15.1'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchvision` >= 0.15.1'
- en: You will also require basic knowledge of the difference between different types
    of machine learning models, such as classification, regression, and clustering
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还需要了解不同类型机器学习模型之间的基本区别，例如分类、回归和聚类
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，链接为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12)。
- en: Introduction to artificial neural networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络简介
- en: Our natural networks of neurons work as decision-making systems with information
    processing units called neurons that help us with, for example, recognizing the
    faces of our friends. **Artificial neural networks** (**ANNs**) work similarly.
    Dissimilar to having a giant network of neurons, as in our bodies, that take care
    of all decision-making, active or reactive, ANNs are designed to be problem-specific.
    For example, we have ANNs for image classification, credit risk estimation, object
    detection, and more. We will use neural networks instead of ANNs for simplicity
    in this book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑神经网络作为决策系统工作，其中信息处理单元称为神经元，帮助我们识别朋友的面孔等。**人工神经网络**（**ANNs**）的工作原理类似。与我们的身体中存在一个庞大的神经元网络，负责所有决策（主动或被动）不同，ANNs被设计为针对特定问题。例如，我们有用于图像分类、信用风险估计、目标检测等任务的ANNs。为了简化，本书中将使用神经网络而不是ANNs。
- en: 'First, we want to focus on **fully connected neural networks** (**FCNNs**),
    which work on tabular data (*Figure 12**.1*). FCNNs and **multi-layer perceptrons**
    (**MLPs**) are used interchangeably in many resources. To be able to better compare
    different types of neural networks, we will use FCNNs instead of MLPs in this
    book:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想专注于**全连接神经网络**（**FCNNs**），它们在表格数据上工作（*图12*）。在许多资源中，FCNNs和**多层感知器**（**MLPs**）被互换使用。为了更好地比较不同类型的神经网络，本书中将使用FCNNs而不是MLPs：
- en: '![Figure 12.1 – Schematic illustration of an FCNN and an individual neuron](img/B16369_12_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – FCNN和单个神经元的示意图](img/B16369_12_01.jpg)'
- en: Figure 12.1 – Schematic illustration of an FCNN and an individual neuron
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – FCNN和单个神经元的示意图
- en: FCNNs for supervised learning have one input, one output, and one or multiple
    hidden (middle) layers. A neural network with more than three layers, inclusive
    of the input and the output layers in supervised models, is called a deep neural
    network, and deep learning refers to modeling with such networks (Hinton and Salakhutdinov,
    2006).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用于监督学习的FCNN有一个输入、一个输出和一个或多个隐藏（中间）层。包含输入和输出层的监督模型中超过三层的神经网络称为深度神经网络，深度学习指的是使用这种网络进行建模（Hinton和Salakhutdinov，2006）。
- en: The input layer is nothing other than the features of data points used for modeling.
    The number of neurons in the output layer is also determined based on the problem
    at hand. For example, in the case of binary classification, two neurons in the
    output layer represent two classes. The number and size of hidden layers are among
    the hyperparameters of an FCNN and can be optimized to improve FCNN performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层不过是用于建模的数据点的特征。输出层神经元的数量也是根据实际问题确定的。例如，在二元分类的情况下，输出层中的两个神经元代表两个类别。隐藏层的数量和大小是FCNN的超参数之一，可以通过优化来提高FCNN的性能。
- en: 'Each neuron in an FCNN receives a weighted sum of output values from neurons
    in the previous layer, applies a linear or nonlinear transformation to the received
    sum of values, and then outputs the resulting value to other neurons of the next
    layer. The weights used in the input value calculation of each neuron are the
    learned weights (parameters) in the training process. The nonlinear transformations
    are applied through predetermined activation functions (*Figure 12**.2*). FCNNs
    are known for coming up with complicated nonlinear relationships between input
    feature values and outputs, which makes them flexible in figuring out (maybe)
    different kinds of relationships between inputs and outputs. In FCNNs, activation
    functions that are applied to information that’s been received in neurons are
    responsible for that complexity or flexibility:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: FCNN中的每个神经元接收来自前一层的神经元的加权输出值的总和，对接收到的值的总和应用线性或非线性变换，然后将结果值输出到下一层的其他神经元。每个神经元输入值计算中使用的权重是训练过程中的学习权重（参数）。非线性变换是通过预定的激活函数实现的（*图12**.2*）。FCNN以其在输入特征值和输出之间产生复杂非线性关系而闻名，这使得它们在确定（可能）输入和输出之间不同类型的关系时非常灵活。在FCNN中，应用于接收到的神经元信息的激活函数负责这种复杂性或灵活性：
- en: '![Figure 12.2 – Widely used activation functions in neural network modeling](img/B16369_12_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 神经网络建模中广泛使用的激活函数](img/B16369_12_02.jpg)'
- en: Figure 12.2 – Widely used activation functions in neural network modeling
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 神经网络建模中广泛使用的激活函数
- en: 'Each of these activation functions, such as `sigmoid` and `softmax` functions
    are commonly used in output layers to transform the scores of the output neurons
    into values between zero and one for classification models; these are known as
    probabilities of predictions. There are also other activation functions such as
    **Gaussian error linear unit** (**GELU**) (Hendrycks and Gimpel, 2016) that have
    been used in more recent models such as **generative pre-trained transformer**
    (**GPT**), which will be explained in the next chapter. Here is the formula for
    GELU:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数，例如`sigmoid`和`softmax`函数，通常用于输出层，将输出神经元的分数转换为介于零和一之间的值，用于分类模型；这些被称为预测的概率。还有其他激活函数，如**高斯误差线性单元**（**GELU**）（Hendrycks和Gimpel，2016），这些在更近期的模型中如**生成预训练转换器**（**GPT**）中已被使用，这将在下一章中解释。以下是GELU的公式：
- en: GELU(z) = 0.5z(1 + tanh(√ _  2 _ π  (z + 0.044715 z 3)))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GELU(z) = 0.5z(1 + tanh(√ _  2 _ π  (z + 0.044715 z 3)))
- en: 'Supervised learning has two main processes: predicting outputs and learning
    from the incorrectness or correctness of predictions. In FCNNs, predictions happen
    in forward propagation. The weights of the FCNNs between the input and first hidden
    layer are used to calculate the input values of the neurons of the first hidden
    layer and similarly for other layers in the FCNN (*Figure 12**.3*). Going from
    input to output is called forward propagation or forward pass, which generates
    the output values (predictions) for each data point. Then, in the backward propagation
    (backpropagation) or backward pass, FCNN uses the predicted outputs and their
    differences with actual outputs to adjust its weights, resulting in better predictions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习有两个主要过程：预测输出和从预测的不正确性或正确性中学习。在全连接神经网络（FCNNs）中，预测发生在前向传播中。输入和第一个隐藏层之间的FCNNs的权重用于计算第一个隐藏层中神经元的输入值，以及其他FCNN中的层也是如此（*图12*.3）。从输入到输出的过程称为前向传播或前向传递，它为每个数据点生成输出值（预测）。然后，在反向传播（反向传递）中，FCNN使用预测输出及其与实际输出的差异来调整其权重，从而实现更好的预测：
- en: '![Figure 12.3 – Schematic illustration of forward propagation and backpropagation
    for output prediction and parameter update, respectively](img/B16369_12_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 分别用于输出预测和参数更新的前向传播和反向传播的示意图](img/B16369_12_03.jpg)'
- en: Figure 12.3 – Schematic illustration of forward propagation and backpropagation
    for output prediction and parameter update, respectively
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 分别用于输出预测和参数更新的前向传播和反向传播的示意图
- en: The parameters of a neural network get determined in the training process using
    an optimization algorithm. Now, we will review some widely used optimization algorithms
    in neural network settings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的参数在训练过程中通过优化算法来确定。现在，我们将回顾一些在神经网络设置中广泛使用的优化算法。
- en: Optimization algorithms
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化算法
- en: Optimization algorithms work behind the scenes, trying to minimize the loss
    function to identify the optimal parameters when you train a machine learning
    model. At each step in the training process, an optimization algorithm decides
    how to update each of the weights or parameters in a neural network, or other
    machine learning models. Most optimization algorithms rely on the gradient vector
    of the cost function to update the weights. The main difference is how the gradient
    vector is used and what data points are used to calculate it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法在幕后工作，试图最小化损失函数以识别训练机器学习模型时的最佳参数。在训练过程的每一步，优化算法决定如何更新神经网络中的每个权重或参数，或其他机器学习模型。大多数优化算法依赖于成本函数的梯度向量来更新权重。主要区别在于如何使用梯度向量以及使用哪些数据点来计算它。
- en: 'In gradient descent, all the data points are used to calculate the gradient
    of the cost function; then, the weights of the model get updated in the direction
    of maximum decrease of cost. Despite the effectiveness of this method for small
    datasets, it can become computationally expensive and unsuitable for large datasets
    as for every iteration of learning, the cost needs to be calculated for all the
    data points simultaneously. The alternative approach is **stochastic gradient
    descent** (**SGD**); instead of all data points, one data point gets selected
    in each iteration to calculate the cost and update the weights. But using one
    data point at a time causes a highly oscillating behavior in updating weights.
    Instead, we can use mini-batch gradient descent, which is commonly called SGD
    in tutorials and tools, in which instead of all data points or only one in each
    iteration, it will use a batch of data points to update the weights. The mathematics
    behind these three approaches is shown in *Figure 12**.4*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，所有数据点都用于计算成本函数的梯度；然后，模型的权重在成本最大减少的方向上更新。尽管这种方法对于小数据集是有效的，但它可能变得计算成本高昂，不适用于大数据集，因为对于学习的每一次迭代，都需要同时计算所有数据点的成本。另一种方法是**随机梯度下降**（**SGD**）；在每次迭代中，不是所有数据点，而是选择一个数据点来计算成本和更新权重。但每次只使用一个数据点会导致权重更新时出现高度振荡的行为。相反，我们可以使用迷你批梯度下降，这在教程和工具中通常被称为SGD，其中不是每个迭代中所有数据点或仅一个，而是使用一批数据点来更新权重。这些三种方法背后的数学原理在*图12*.4中展示：
- en: '![Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch
    gradient descent optimization algorithms](img/B16369_12_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – 梯度下降、随机梯度下降和迷你批梯度下降优化算法](img/B16369_12_04.jpg)'
- en: Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch
    gradient descent optimization algorithms
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 梯度下降、随机梯度下降和小批量梯度下降优化算法
- en: Other optimization algorithms have been suggested in recent years to improve
    the performance of neural network models across a variety of applications, such
    as the Adam optimizer (Kingma and Ba, 2014). One of the intuitions behind this
    approach is to avoid diminishing gradients in the optimization process. Getting
    further into the details of different optimization algorithms is beyond the scope
    of this book.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已经提出了其他优化算法来提高神经网络模型在各种应用中的性能，例如 Adam 优化器（Kingma 和 Ba，2014）。这种方法的背后直觉是避免优化过程中的梯度消失。深入探讨不同优化算法的细节超出了本书的范围。
- en: 'In neural network modeling, there are two important terms that you need to
    know the definition of: *epoch* and *batch size*. When training a neural network
    model using different frameworks, which we will review in the next section, you
    need to specify the *batch size* and the number of *epochs*. In each iteration
    of optimization, a subset of data points, or a mini-batch as in mini-batch gradient
    descent (*Figure 12**.4*), gets used to calculate loss; then, the parameters of
    the model get updated using backpropagation. This process gets repeated to cover
    all the data points in the training data. Epoch is a term we use to specify how
    many times all the training data is used during the optimization process. For
    example, specifying an epoch of 5 means that the model gets trained until all
    the data points in the training process are used five times in the optimization
    process.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络建模中，有两个重要的术语你需要了解其定义：*epoch* 和 *batch size*。当使用不同的框架训练神经网络模型时，我们将在下一节中回顾，你需要指定
    *batch size* 和 *epoch* 的数量。在优化的每一次迭代中，数据点的一个子集，或者说是小批量梯度下降中的小批量（如图 12.4*），被用来计算损失；然后，使用反向传播更新模型的参数。这个过程会重复进行，以覆盖训练数据中的所有数据点。Epoch
    是我们在优化过程中用来指定所有训练数据被使用多少次的术语。例如，指定 5 个 epoch 意味着模型在优化过程中会使用训练过程中的所有数据点五次。
- en: Now that you know the basics of neural network modeling, we are ready to introduce
    frameworks for neural network modeling.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了神经网络建模的基础知识，我们将介绍用于神经网络建模的框架。
- en: Frameworks for neural network modeling
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络建模框架
- en: 'Multiple frameworks have been used for neural network modeling:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 已有多个框架被用于神经网络建模：
- en: PyTorch ([https://pytorch.org/](https://pytorch.org/))
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch ([https://pytorch.org/](https://pytorch.org/))
- en: TensorFlow ([https://www.tensorflow.org/learn](https://www.tensorflow.org/learn))
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow ([https://www.tensorflow.org/learn](https://www.tensorflow.org/learn))
- en: Keras ([https://keras.io/](https://keras.io/))
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras ([https://keras.io/](https://keras.io/))
- en: Caffe ([https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/))
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffe ([https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/))
- en: MXNet ([https://mxnet.apache.org/versions/1.9.1/](https://mxnet.apache.org/versions/1.9.1/))
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet ([https://mxnet.apache.org/versions/1.9.1/](https://mxnet.apache.org/versions/1.9.1/))
- en: In this book, we will focus on PyTorch in practicing deep learning, but the
    concepts we’ll introduce are independent of the framework you use in your projects.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将专注于 PyTorch 来实践深度学习，但我们介绍的概念与你在项目中使用的框架无关。
- en: PyTorch for deep learning modeling
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于深度学习建模的 PyTorch
- en: PyTorch is an open source deep learning framework, based on the `Torch` library,
    developed by Meta AI. You can easily integrate PyTorch with Python’s scientific
    computing libraries in your deep learning projects. Here, we will practice using
    PyTorch by looking at a simple example of building an FCNN model using the MNIST
    digit dataset. It is a commonly used example and the objective is solely to understand
    how to train and test a deep learning model using PyTorch if you don’t have experience
    with that.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个开源的深度学习框架，基于 Meta AI 开发的 `Torch` 库。你可以在深度学习项目中轻松地将 PyTorch 与 Python
    的科学计算库集成。在这里，我们将通过查看使用 MNIST 数字数据集构建 FCNN 模型的一个简单示例来练习使用 PyTorch。这是一个常用的示例，其目标仅仅是理解如果你没有相关经验，如何使用
    PyTorch 训练和测试深度学习模型。
- en: 'First, we will import the required libraries and load the dataset for training
    and testing:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的库并加载用于训练和测试的数据集：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will determine the hyperparameters of the model and its `input_size`,
    which is the number of neurons in the input layer; this is the same as the number
    of features in our data. In this example, it is equal to the number of pixels
    in each image as we are considering each pixel as one feature to build an FCNN
    model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将确定模型的超参数及其`input_size`，这是输入层中的神经元数量；这等同于我们数据中的特征数量。在这个例子中，它等于每张图像中的像素数，因为我们把每个像素视为一个特征来构建一个全连接神经网络（FCNN）模型：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we will import `torch.nn`, from which we can add linear neural network
    layers for our FCNN model and write a class to determine the architecture of our
    network, which is a network with one hidden layer whose size is 256 (with 256
    neurons):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将导入`torch.nn`，从中我们可以为我们的FCNN模型添加线性神经网络层，并编写一个类来确定我们网络的架构，这是一个包含一个大小为256（256个神经元）的隐藏层的网络：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `torch.nn.Linear()` class adds a linear layer and has two input arguments:
    the number of neurons in the current and next layer, respectively. For the first,
    `nn.Linear()`, the first argument has to be equal to the number of features, while
    the second argument of the last `nn.Linear()` input argument in the network initialization
    class needs to be equal to the number of classes in the data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Linear()`类添加一个线性层，并有两个输入参数：当前层和下一层的神经元数量。对于第一个`nn.Linear()`，第一个参数必须等于特征数量，而网络初始化类中最后一个`nn.Linear()`输入参数的第二个参数需要等于数据中的类别数量。'
- en: 'Now, we must define our cross-entropy loss function and our optimizer object
    using the Adam optimizer from `torch.optim()`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须使用`torch.optim()`中的Adam优化器定义我们的交叉熵损失函数和优化器对象：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to train our model. As you can see in the following code block,
    we have a loop over epochs and another internal loop over each batch. Within the
    internal loop, we have three important steps that are common across most supervised
    models that use PyTorch:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的模型了。正如你在下面的代码块中可以看到的，我们有一个关于epochs的循环，以及另一个关于每个批次的内部循环。在内部循环中，我们有三个在大多数使用PyTorch的监督模型中常见的步骤：
- en: Get the output of the model for the data points within the batch.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取批次内数据点的模型输出。
- en: Calculate the loss using the true labels and the predicted output for the data
    points of that batch.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该批次的真实标签和预测输出计算损失。
- en: Backpropagate and update the parameters of the model.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播并更新模型的参数。
- en: 'Next, we must train the model on the MNIST training set:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须在MNIST训练集上训练模型：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At the end of epoch 10, we have a model with a loss of 0.0214 in the training
    set. Now, we can use the following code to calculate the accuracy of the model
    in the test set:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10个epoch结束时，我们在训练集中有一个损失为0.0214的模型。现在，我们可以使用以下代码来计算测试集中模型的准确率：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This results in 98.4% for the model in the MNIST test set.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得MNIST测试集中的模型达到了98.4%的准确率。
- en: There are more than 10 different optimization algorithms, including the Adam
    optimization algorithm, available in PyTorch ([https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)),
    which helps you in training your deep learning models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch（[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)）中提供了超过10种不同的优化算法，包括Adam优化算法，这些算法可以帮助你在训练深度学习模型时。
- en: Next, we will discuss hyperparameter tuning, model interpretability, and fairness
    in deep learning settings. We will also introduce PyTorch Lightning, which will
    help you in your deep learning projects.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论深度学习设置中的超参数调整、模型可解释性和公平性。我们还将介绍PyTorch Lightning，这将有助于你在深度学习项目中。
- en: Hyperparameter tuning for deep learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习超参数调整
- en: 'In deep learning modeling, hyperparameters are key factors in determining its
    performance. Here are some of the hyperparameters of FCNNs you can work with to
    improve the performance of your deep learning models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习建模中，超参数是决定其性能的关键因素。以下是一些你可以用来提高你的深度学习模型性能的FCNN超参数：
- en: '**Architecture**: The architecture of an FCNN refers to the number of hidden
    layers and their sizes, or the number of neurons. More layers result in higher
    depth in a deep learning model and could result in more complex models. Although
    the depth of neural network models has been shown to improve performance on large
    datasets in many cases (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014;
    Szegedy et al., 2015; He et al., 2016), the majority of the success stories behind
    the positive effect of higher depth on performance are outside of FCNNs. But architecture
    is still an important hyperparameter that needs to be optimized to find a high-performance
    model.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**: FCNN的架构指的是隐藏层的数量和它们的大小，或者神经元的数量。更多的层会导致深度学习模型具有更高的深度，并可能导致更复杂的模型。尽管神经网络模型的深度在许多情况下已被证明可以提高大型数据集上的性能（Krizhevsky等人，2012年；Simonyan和Zisserman，2014年；Szegedy等人，2015年；He等人，2016年），但大多数关于更高深度对性能产生积极影响的成功故事都发生在FCNN之外。但架构仍然是一个需要优化的重要超参数，以找到高性能模型。'
- en: '**Activation functions**: Despite commonly used activation functions in each
    field and problem, you can still identify the best one for your problem. Remember
    that you don’t have to use the same function across all layers, although we usually
    stick to one.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**: 尽管每个领域和问题中都有常用的激活函数，但你仍然可以为你自己的问题找到最佳的一个。记住，你不需要在所有层中使用相同的函数，尽管我们通常坚持使用一个。'
- en: '**Batch size**: Changing batch size changes both the performance and speed
    of convergence of your models. But usually, it doesn’t have a significant effect
    on performance, except in the steep part of the learning curve in the first few
    epochs.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**: 改变批量大小会改变你模型的性能和收敛速度。但通常，它对性能没有显著影响，除了在第一个几个训练轮数学习曲线的陡峭部分。'
- en: '**Learning rate**: The learning rate determines the speed of convergence. A
    higher learning rate causes faster convergence but it might also cause oscillation
    around the local optimum point or even divergence. Algorithms such as the Adam
    optimizer control the diminishing convergence rate when we get closer to the local
    optima during the optimization process, but we can still play with the learning
    rate as a hyperparameter in deep learning modeling.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**: 学习率决定了收敛的速度。较高的学习率会导致收敛速度加快，但也可能导致在局部最优点的振荡，甚至发散。例如，Adam优化器等算法在优化过程中接近局部最优时，会控制收敛率的衰减，但我们仍然可以将学习率作为深度学习建模的超参数进行调整。'
- en: '**Number of epochs**: Deep learning models have a steep learning curve for
    the first few epochs, depending on the learning rate and batch size, and then
    start plateauing on performance. Using enough epochs is important to make sure
    you get the best possible model out of your training.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**: 深度学习模型在前几轮的训练中学习曲线陡峭，这取决于学习率和批量大小，然后性能开始趋于平稳。使用足够的训练轮数对于确保你从训练中得到最佳模型至关重要。'
- en: '**Regularization**: We talked about the importance of regulations in controlling
    overfitting and improving generalizability in [*Chapter 5*](B16369_05.xhtml#_idTextAnchor183),
    *Improving the Performance of Machine Learning Models*, by preventing the model
    from heavily relying on individual neurons and potentially improving generalizability.
    For example, if dropout is set to 0.2, each neuron has a 20% chance of getting
    zero out during training.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**: 我们在[*第五章*](B16369_05.xhtml#_idTextAnchor183)《提高机器学习模型性能》中讨论了正则化在控制过拟合和提升泛化能力方面的重要性，通过防止模型过度依赖单个神经元，从而可能提高泛化能力。例如，如果设置dropout为0.2，每个神经元在训练过程中有20%的概率被置零。'
- en: '**Weight decay**: This is a form of L2 regularization that adds a penalty to
    the weights of the neural network. We introduced L2 regularization in [*Chapter
    5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance of Machine*
    *Learning Models*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重衰减**: 这是一种L2正则化形式，它对神经网络中的权重添加惩罚。我们在[*第五章*](B16369_05.xhtml#_idTextAnchor183)《提高机器学习模型性能》中介绍了L2正则化。'
- en: 'You can use different hyperparameter optimization tools such as Ray Tune alongside
    PyTorch to train your deep learning models and optimize their hyperparameters.
    You can read more about it in this tutorial available on the PyTorch website:
    [https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用不同的超参数优化工具，如 Ray Tune，与 PyTorch 一起训练您的深度学习模型并优化其超参数。您可以在 PyTorch 网站上的这个教程中了解更多信息：[https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)。
- en: In addition to hyperparameter tuning, PyTorch has different functionalities
    and associated libraries for tasks such as model interpretability and fairness.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了超参数调整之外，PyTorch 还具有不同的功能和相关库，用于模型可解释性和公平性等任务。
- en: Model interpretability in PyTorch
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 中的模型可解释性
- en: We introduced multiple explainability techniques and libraries in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine Learning Modeling*, that can help you in explaining complex machine learning
    and deep learning models. Captum AI ([https://captum.ai/](https://captum.ai/))
    is another open source model interpretability library developed by Meta AI for
    deep learning projects using PyTorch. You can easily integrate Captum into your
    existing or future PyTorch-based machine learning pipelines. You can benefit from
    different explainability and interpretability techniques such as integrated gradients,
    GradientSHAP, DeepLIFT, and saliency maps through Captum.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B16369_06.xhtml#_idTextAnchor201)《机器学习建模中的可解释性和可解释性》中，我们介绍了多种可解释性技术和库，可以帮助您解释复杂的机器学习和深度学习模型。Captum
    AI ([https://captum.ai/](https://captum.ai/)) 是由 Meta AI 开发的一个开源模型可解释性库，用于使用 PyTorch
    的深度学习项目。您可以将 Captum 容易地集成到现有的或未来的基于 PyTorch 的机器学习管道中。您可以通过 Captum 利用不同的可解释性和可解释性技术，如集成梯度、GradientSHAP、DeepLIFT
    和显著性图。
- en: Fairness in deep learning models developed by PyTorch
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 开发的深度学习模型中的公平性
- en: We discussed the importance of fairness and introduced different notions, statistical
    measures, and techniques to help you in assessing and eliminating bias in your
    models as part of [*Chapter 7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing
    Bias and Achieving Fairness*. `FairTorch` ([https://github.com/wbawakate/fairtorch](https://github.com/wbawakate/fairtorch))
    and `inFairness` ([https://github.com/IBM/inFairness](https://github.com/IBM/inFairness))
    are two other libraries you can use for fairness and bias assessment for your
    deep learning modeling using PyTorch. You can benefit from `inFairness` in auditing,
    training, and post-processing your models for individual fairness. `Fairtorch`
    also provides you with tools to mitigate bias in classification and regression,
    though this is currently limited to binary classification.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B16369_07.xhtml#_idTextAnchor218)《减少偏差和实现公平性》中，我们讨论了公平性的重要性，并介绍了不同的概念、统计指标和技术，以帮助您评估和消除模型中的偏差。`FairTorch`
    ([https://github.com/wbawakate/fairtorch](https://github.com/wbawakate/fairtorch))
    和 `inFairness` ([https://github.com/IBM/inFairness](https://github.com/IBM/inFairness))
    是另外两个库，您可以使用它们来评估使用 PyTorch 的深度学习建模中的公平性和偏差。您可以从 `inFairness` 中受益，用于审计、训练和后处理模型以实现个体公平性。`Fairtorch`
    还为您提供了减轻分类和回归中偏差的工具，尽管目前这仅限于二元分类。
- en: PyTorch Lightning
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch Lightning
- en: 'PyTorch Lightning is an open source, high-level framework that simplifies the
    process of developing and training deep learning models using PyTorch for you.
    Here are some of the features of PyTorch Lightning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 是一个开源的高级框架，简化了使用 PyTorch 开发和训练深度学习模型的过程。以下是 PyTorch Lightning
    的一些特性：
- en: '**Structured code**: PyTorch Lightning organizes code into a Lightning Module
    that helps you in separating the model architecture, data handling, and training
    logic, making the code more modular and easier to maintain'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化代码**：PyTorch Lightning 将代码组织成 Lightning Module，这有助于您分离模型架构、数据处理和训练逻辑，使代码更加模块化且易于维护。'
- en: '**Training loop abstraction**: You can avoid repetitive code for the training,
    validation, and testing loops using PyTorch Lightning'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练循环抽象**：您可以使用 PyTorch Lightning 避免训练、验证和测试循环中的重复代码'
- en: '**Distributed training**: PyTorch Lightning simplifies the process of scaling
    deep learning models across multiple GPUs or nodes'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式训练**：PyTorch Lightning 简化了在多个 GPU 或节点上扩展深度学习模型的过程'
- en: '**Experiment tracking and logging**: PyTorch Lightning integrates with experiment
    tracking and logging tools such as MLflow and Weights & Biases, which make monitoring
    your deep learning model training easier for you'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验跟踪和日志记录**：PyTorch Lightning 与实验跟踪和日志记录工具（如 MLflow 和 Weights & Biases）集成，这使得您更容易监控深度学习模型训练。'
- en: '**Automatic optimization**: PyTorch Lightning automatically handles the optimization
    process, manages optimizers and learning rate schedulers, and makes it easier
    to switch between different optimization algorithms'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动优化**：PyTorch Lightning 自动处理优化过程，管理优化器和学习率调度器，并使得在不同优化算法之间切换更加容易。'
- en: Despite all these factors, there is more to deep learning modeling than FCNNs,
    as we’ll see in the next chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些因素，深度学习建模的内容远不止 FCNNs，正如我们在下一章中将要看到的。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about deep learning modeling with FCNNs. We practiced
    using PyTorch with a simple deep learning model to help you start performing deep
    learning modeling using PyTorch if you haven’t had that experience already. You
    also learned about the important hyperparameters of FCNNs, tools for model interpretability
    and fairness that you can use in deep learning settings, and PyTorch Lightning
    as an open source high-level framework to simplify deep learning modeling for
    you. You are now ready to learn more about PyTorch, PyTorch Lightning, and deep
    learning and start benefitting from them in your problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了使用 FCNNs 进行深度学习建模。我们通过使用 PyTorch 和一个简单的深度学习模型来练习，帮助你开始使用 PyTorch 进行深度学习建模（如果你还没有这样的经验）。你还学习了
    FCNNs 的重要超参数、可用于深度学习环境中的模型可解释性和公平性工具，以及 PyTorch Lightning 作为简化深度学习建模的开源高级框架。你现在可以学习更多关于
    PyTorch、PyTorch Lightning 和深度学习的内容，并开始从它们中受益于你的问题。
- en: In the next chapter, you will learn about other more advanced types of deep
    learning models, including the convolutional neural network, transformer, and
    graph convolutional network models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习到其他更高级的深度学习模型，包括卷积神经网络、转换器和图卷积神经网络模型。
- en: Questions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Do the parameters of a neural network model get updated in backpropagation?
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络模型的参数在反向传播中会更新吗？
- en: What is the difference between stochastic and mini-batch gradient descent?
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机梯度下降和批量梯度下降有什么区别？
- en: Can you explain the difference between a batch and an epoch?
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释一下批量（batch）和周期（epoch）之间的区别吗？
- en: Can you provide an example of where you need to use the sigmoid and softmax
    functions in your neural network models?
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能提供一个例子说明在您的神经网络模型中需要使用 sigmoid 和 softmax 函数的地方吗？
- en: References
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. *Deep learning*. nature 521.7553
    (2015): 436-444.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann, Yoshua Bengio, 和 Geoffrey Hinton. *深度学习*. 自然 521.7553 (2015):
    436-444。'
- en: Hinton, G. E., & Salakhutdinov, R. R. (2006). *Reducing the Dimensionality of
    Data with Neural Networks*. Science, 313(5786), 504-507.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G. E., & Salakhutdinov, R. R. (2006). *使用神经网络降低数据维度*. 科学，313(5786)，504-507。
- en: 'Abiodun, Oludare Isaac, et al. *State-of-the-art in artificial neural network
    applications: A survey*. Heliyon 4.11 (2018): e00938.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abiodun, Oludare Isaac, 等人. *人工神经网络应用中的最新进展：一项调查*. Heliyon 4.11 (2018): e00938。'
- en: Hendrycks, D., & Gimpel, K. (2016). *Gaussian Error Linear Units (GELUs)*. arXiv
    preprint arXiv:1606.08415.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks, D., & Gimpel, K. (2016). *高斯误差线性单元 (GELUs)*. arXiv 预印本 arXiv:1606.08415。
- en: 'Kingma, D. P., & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*.
    arXiv preprint arXiv:1412.6980.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma, D. P., & Ba, J. (2014). *Adam: 一种用于随机优化的方法*. arXiv 预印本 arXiv:1412.6980。'
- en: 'Kadra, Arlind, et al. *Well-tuned simple nets excel on tabular datasets*. Advances
    in neural information processing systems 34 (2021): 23928-23941.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kadra, Arlind, 等人. *精心调优的简单网络在表格数据集上表现出色*. 神经信息处理系统进展 34 (2021): 23928-23941。'
- en: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems (pp. 1097-1105).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *使用深度卷积神经网络进行 ImageNet
    分类*. 在神经信息处理系统进展（第 1097-1105 页）。
- en: Simonyan, K., & Zisserman, A. (2014). *Very deep convolutional networks for
    large-scale image recognition*. arXiv preprint arXiv:1409.1556.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, K., & Zisserman, A. (2014). *非常深的卷积网络在大规模图像识别中的应用*. arXiv 预印本 arXiv:1409.1556。
- en: He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep residual learning for image
    recognition*. In Proceedings of the IEEE conference on computer vision and pattern
    recognition (pp. 770-778).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, K., Zhang, X., Ren, S., & Sun, J. (2016). *深度残差学习在图像识别中的应用*. 在 IEEE 计算机视觉与模式识别会议论文集（第
    770-778 页）。
- en: Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich,
    A. (2015). *Going deeper with convolutions*. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (pp. 1-9).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich,
    A. (2015). *使用卷积进行更深入的学习*。载于IEEE计算机视觉与模式识别会议论文集（第1-9页）。
