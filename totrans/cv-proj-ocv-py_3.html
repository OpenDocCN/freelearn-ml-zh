<html><head></head><body><div><h1 class="header-title">Reading License Plates with OpenCV</h1>
                
            
            
                
<p>This chapter provides an overview of how to extract and display license plate characters in any sample photo with a license plate in it. OpenCV and its plate utility functions help us find the characters on a license plate, and give us a good taste of how computer vision and image processing work. </p>
<p>In this chapter, we will learn about the following:</p>
<ul>
<li>The steps needed to read license plates</li>
<li>Plate utility functions</li>
<li>Finding plate characters</li>
<li>Finding and reading license plates</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Identifying the license plate</h1>
                
            
            
                
<p>In this project, we are going to detect and read license plates in photos of cars. We will be performing multiple steps, from locating the license plate to displaying the characters in the located license plate.</p>
<p>Let's refer to the code in Jupyter Notebook needed to analyze our sample images:</p>
<pre>%pylab notebook<br/>figure()<br/>imshow(imread('tests/p1.jpg'))</pre>
<p>We get the following photo when we run the code:</p>
<div><img class="alignnone size-full wp-image-517 image-border" src="img/fcf031dc-bfee-453c-8480-c869cd11721f.png" style="width:30.25em;height:19.00em;" width="692" height="435"/></div>
<p>We have a photo of a car, with its license plate clearly visible and readable. The challenge is to locate the license plate, isolate it from the rest of the photo, and extract the characters from it.</p>
<p>We can now take a closer look at the license plate using the available utility functions:</p>
<div><img class="alignnone size-full wp-image-518 image-border" src="img/f8b01357-ab91-49a4-898d-bdaab043cde7.png" style="width:29.33em;height:15.00em;" width="696" height="356"/></div>
<p>There are many algorithms that can help us carry out both these tasks. For example, object detectors such as YOLO: Real-Time Object Detection can do a very good job using the relevant machine learning methods for performing such tasks.</p>
<p>However, we will be looking at a straightforward approach, using conventional image processing and computer vision techniques, instead of complex machine learning techniques such as deep learning and TensorFlow.</p>
<p>The algorithm we will be using will help us learn computer vision and image processing techniques, giving us a better understanding of the project. Let's start with our code and check the plate utility functions we will be using.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Plate utility functions</h1>
                
            
            
                
<p>Let's jump to our code in Jupyter Notebook, in order to understand plate utility functions. We will first embed the imports with our utilities.</p>
<p>We will be importing the following libraries:</p>
<ul>
<li>OpenCV (version 3.4)</li>
<li>NumPy</li>
<li>Pickle, which lets us save Python data and case functions</li>
</ul>
<p>Import the libraries as follows:</p>
<pre>import cv2<br/>import numpy as np<br/>import pickle<br/>def gray_thresh_img(input_image):<br/>     h, w, _ = input_image.shape<br/>     grayimg = cv2.cvtColor(input_image, cv2.COLOR_BGR2HSV)[:,:,2]<br/>    <br/>     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))<br/>     <br/>     tophat = cv2.morphologyEx(grayimg, cv2.MORPH_TOPHAT, kernel)<br/>     blackhat = cv2.morphologyEx(grayimg, cv2.MORPH_BLACKHAT, kernel)<br/>     graytop = cv2.add(grayimg, tophat)<br/>     contrastgray = cv2.subtract(graytop, blackhat)<br/>     blurred = cv2.GaussianBlur(contrastgray, (5,5), 0)<br/>     thesholded = cv2.adaptiveThreshold(blurred, 255.0, <br/>     cv2.ADAPTIVE_THRESH_GAUSSIAN_C, <br/>     cv2.THRESH_BINARY_INV, 19, 9)</pre>
<p>We will be using these libraries to load the k-nearest neighbors classifier for reading characters, which implicitly depends on scikit-learn.</p>
<p>We will now discuss the utilities that will be used in our code.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">The gray_thresh_img function and morphological functions</h1>
                
            
            
                
<p>The <kbd>gray_thresh_img</kbd> function takes an input image and converts it to grayscale. We need the image in grayscale, as color images may cause ambiguity, given that the color of license plates differs depending on the area. The <kbd>gray_thres_img</kbd> function gives us a binarized image.</p>
<p>We can use morphological operations for pre-processing, as this will help us reduce noise and gaps. This will de-noise our image and remove extraneous features.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Kernels</h1>
                
            
            
                
<p>A kernel is a three-by-three square on which we will be using <kbd>tophat</kbd>, <kbd>blackhat</kbd>, and <kbd>graytop</kbd> operations to create a grayscale image. This will also help us to de-noise the image—noise is usually present in natural images, and is not preferable for computer vision. The image can also be de-noised using Gaussian blur.</p>
<p>We will use adaptive thresholding, which looks at local statistics and averages in an image to check whether it is bright or dim relative to its neighborhood. This is preferred over hard thresholding, as it will binarize our images in a better way.</p>
<p>We use the <kbd>return</kbd> function to get the gray image and binarized image, as follows:</p>
<pre>return grayimg, thesholded</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">The matching character function</h1>
                
            
            
                
<p>Let's look at our next function to get the matching characters:</p>
<pre>def getmatchingchars(char_cands):<br/>    char_list = [] <br/>   <br/>    for char_cand in char_cands:<br/>        ch_matches = [] \n",<br/>        for matching_candidate in char_cands: <br/>           if matching_candidate == char_cand:<br/>              continue <br/>          chardistance = np.sqrt((abs(char_cand.x_cent  - matching_candidate.x_cent) ** 2) +  <br/>          (abs(char_cand.y_cent - matching_candidate.y_cent)**2))<br/>          x = float(abs(char_cand.x_cent - matching_candidate.x_cent))<br/>          y = float(abs(char_cand.y_cent - matching_candidate.y_cent))<br/>          angle = np.rad2deg(np.arctan(y/x) if x != 0.0 else np.pi/2)<br/>    <br/>          deltaarea = float(abs(matching_candidate.rect_area - char_cand.rect_area))\<br/>          / float(char_cand.rect_area)<br/>         deltawidth = float(abs(matching_candidate.rect_w-char_cand.rect_w))\<br/>         / float(char_cand.rect_w)<br/>         deltaheight = float(abs(matching_candidate.rect_h-char_cand.rect_h))<br/>         / float(char_cand.rect_h)<br/>   <br/>         if (chardistance &lt; (char_cand.hypotenuse * 5.0) and<br/>             angle &lt; 12.0 and deltaarea &lt; 0.5 and deltawidth &lt; 0.8 <br/>             and deltaheight &lt; 0.2):<br/>             ch_matches.append(matching_candidate) <br/>   <br/>     ch_matches.append(char_cand) <br/>     if len(ch_matches) &lt; 3: <br/>         continue <br/>   <br/>     char_list.append(ch_matches) </pre>
<p>The <kbd>getmatchingchars</kbd> function helps us find our character candidate based on the following criteria:</p>
<ul>
<li>Size</li>
<li>Relative distance</li>
<li>Angle</li>
<li>Area</li>
</ul>
<p>If the potential character is a reasonable distance from its neighbors, the angle is not too large compared to the JSON characters, and the area is not too big, we say that the possible character is a <em>character candidate</em>.</p>
<p>The following code will return a list of characters that are part of a license plate, and then create a container class that will contain objects such as the width, height, center, diagonal distance or hypotenuse, and aspect ratio of the character sub-images within our complete image:</p>
<pre>    for charlist in getmatchingchars(list(set(char_cands)-set(ch_matches))):<br/>        char_list.append(charlist) <br/>    break <br/>   <br/> return char_list<br/>    <br/># information container for possible characters in images<br/>class charclass:<br/>     def __init__(self, _contour):<br/>         self.contour = _contour<br/>         self.boundingRect = cv2.boundingRect(self.contour)<br/>         self.rect_x, self.rect_y, self.rect_w, self.rect_h = self.boundingRect<br/>         self.rect_area = self.rect_w * self.rect_h<br/>         self.x_cent = (self.rect_x + self.rect_x + self.rect_w) / 2<br/>         self.y_cent = (self.rect_y + self.rect_y + self.rect_h) / 2<br/>         self.hypotenuse = np.sqrt((self.rect_w ** 2) + (self.rect_h ** 2))<br/>         self.aspect_ratio = float(self.rect_w) / float(self.rect_h)</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">The k-nearest neighbors digit classifier</h1>
                
            
            
                
<p>The pre-trained scikit-learn <strong>k-nearest neighbors</strong> (<strong>k-nn</strong>) digit classifier also needs to be loaded, as follows:</p>
<pre># load pre-trained scikit-learn knn digit classifier<br/>    with open('knn.p', 'rb') as f:<br/>     knn = pickle.load(f) "</pre>
<p>The k-nn classifier compares a small image to a series of images already known to it, to find the closest match.</p>
<p>We are not using complex algorithms for this, because characters in a license plate are similar. This is why we can use the k-nn method, which will make a pixel-by-pixel comparison to find the closest match. The characters on a license plate are not handwritten digits where the font might differ, which would need more computation.</p>
<p>In the classifier, <kbd>p</kbd> stands for Pickle, which is how Python stores data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Finding plate characters</h1>
                
            
            
                
<p>Next, we carry out our initial search to find plate characters. First, we find characters roughly, and then find candidates based on specific criteria.</p>
<p>Let's start with the following line in our Notebook:</p>
<pre>%pylab notebook</pre>
<p>We can now execute our function cell for imports, utilities, and to load our libraries:</p>
<pre>import cv2<br/>import numpy as np<br/>import pickle<br/>def getmatchingchars(char_cands):<br/>    char_list = [] <br/>   <br/>    for char_cand in char_cands:<br/>        ch_matches = [] \n",<br/>        for matching_candidate in char_cands: <br/>           if matching_candidate == char_cand:<br/>              continue <br/>          chardistance = np.sqrt((abs(char_cand.x_cent  - matching_candidate.x_cent) ** 2) +  <br/>          (abs(char_cand.y_cent - matching_candidate.y_cent)**2))<br/>          x = float(abs(char_cand.x_cent - matching_candidate.x_cent))<br/>          y = float(abs(char_cand.y_cent - matching_candidate.y_cent))<br/>          angle = np.rad2deg(np.arctan(y/x) if x != 0.0 else np.pi/2)<br/>    <br/>          deltaarea = float(abs(matching_candidate.rect_area - char_cand.rect_area))\<br/>          / float(char_cand.rect_area)<br/>         deltawidth = float(abs(matching_candidate.rect_w-char_cand.rect_w))\<br/>         / float(char_cand.rect_w)<br/>         deltaheight = float(abs(matching_candidate.rect_h-char_cand.rect_h))<br/>         / float(char_cand.rect_h)<br/>   <br/>         if (chardistance &lt; (char_cand.hypotenuse * 5.0) and<br/>             angle &lt; 12.0 and deltaarea &lt; 0.5 and deltawidth &lt; 0.8 <br/>             and deltaheight &lt; 0.2):<br/>             ch_matches.append(matching_candidate) <br/>   <br/>     ch_matches.append(char_cand) <br/>     if len(ch_matches) &lt; 3: <br/>         continue <br/>   <br/>     char_list.append(ch_matches) <br/><br/>    for charlist in getmatchingchars(list(set(char_cands)-set(ch_matches))):<br/>        char_list.append(charlist) <br/>    break <br/>   <br/> return char_list<br/>    <br/># information container for possible characters in images<br/>class charclass:<br/>     def __init__(self, _contour):<br/>         self.contour = _contour<br/>         self.boundingRect = cv2.boundingRect(self.contour)<br/>         self.rect_x, self.rect_y, self.rect_w, self.rect_h = self.boundingRect<br/>         self.rect_area = self.rect_w * self.rect_h<br/>         self.x_cent = (self.rect_x + self.rect_x + self.rect_w) / 2<br/>         self.y_cent = (self.rect_y + self.rect_y + self.rect_h) / 2<br/>         self.hypotenuse = np.sqrt((self.rect_w ** 2) + (self.rect_h ** 2))<br/>         self.aspect_ratio = float(self.rect_w) / float(self.rect_h)</pre>
<p>We can now load our input image, which will be used for analysis. We use the <kbd>plt</kbd> function here instead of OpenCV, as OpenCV by default loads images in <strong>blue green red</strong> (<strong>BGR</strong>) format rather than <strong>red green blue</strong> (<strong>RGB</strong>) format. This is important for your custom projects, but it does not matter for our project since we will be converting the image to grayscale.</p>
<p>Let's load our image:</p>
<pre>input_image = plt.imread('tests/p5.jpg') #use cv2.imread or <br/> #import matplotlib.pyplot as plt<br/> #if running outside notebook<br/>figure()<br/>imshow(input_image)</pre>
<p>This is the output photo:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-520 image-border" src="img/644c259c-32e2-4af2-bb38-54331f147e32.png" style="width:36.83em;height:20.42em;" width="693" height="385"/></p>
<p>Let's take a closer look at the license plate of this car:</p>
<div><img class="alignnone size-full wp-image-521 image-border" src="img/baf74a28-f1b2-4e53-af3b-cad48e8a11ce.png" style="width:32.92em;height:20.17em;" width="773" height="474"/></div>
<p>We will find the characters from this image. However, we first need to remove the background, which is not important for us. Here, we need to carry out initial pre-processing on the image, using the <kbd>gray_thresh_img</kbd>, <kbd>blurred</kbd>, and <kbd>morphology</kbd> functions, which will help us get rid of the background.</p>
<p>Here is the code for the initial pre-processing: </p>
<pre>def gray_thresh_img(input_image):<br/>     h, w, _ = input_image.shape<br/>     grayimg = cv2.cvtColor(input_image, cv2.COLOR_BGR2HSV)[:,:,2]<br/>    <br/>     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))<br/>     <br/>     tophat = cv2.morphologyEx(grayimg, cv2.MORPH_TOPHAT, kernel)<br/>     blackhat = cv2.morphologyEx(grayimg, cv2.MORPH_BLACKHAT, kernel)<br/>     graytop = cv2.add(grayimg, tophat)<br/>     contrastgray = cv2.subtract(graytop, blackhat)<br/>     blurred = cv2.GaussianBlur(contrastgray, (5,5), 0)<br/>     thesholded = cv2.adaptiveThreshold(blurred, 255.0, <br/>     cv2.ADAPTIVE_THRESH_GAUSSIAN_C, <br/>     cv2.THRESH_BINARY_INV, 19, 9)</pre>
<p>Let's look at our main code:</p>
<pre>h, w = input_image.shape[:2] <br/>    <br/># We don't use color information<br/># + we need to binarize (theshold) image to find characters<br/>grayimg, thesholded = gray_thresh_img(input_image)<br/>contours = cv2.findContours(thesholded, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1]<br/>    <br/># initialize variables for possible characters/plates in image<br/>char_cands = [] <br/>plate_candidates = [] </pre>
<p>We will give the image shape, which is going to return the height, width, and RGB depth of the photo. We don't need RGB depth right now, so we will extract only <kbd>2</kbd> elements; height and width. Since we will be working on grayscale images and not colored ones, we'll call our handy <kbd>gray_thresh_img</kbd> function, which will return the gray and binarized thresholded image.</p>
<p>To find the contours, we need sub-images within the image that correspond to the character and then correspond to contours. We will use the <kbd>findContours</kbd> built-in algorithm from OpenCV to find details of complex shapes such as contours that could possibly be characters and work as our k-nn. We will then initialize our <kbd>char_cands</kbd> and <kbd>plate_candidates</kbd> variables.</p>
<p>Let's take our first pass at finding the characters:</p>
<pre>for index in range(0, len(contours)): <br/>    char_cand = charclass(contours[index])<br/>    if (char_cand.rect_area &gt; 80 and char_cand.rect_w &gt; 2 <br/>        and char_cand.rect_h &gt; 8 and 0.25 &lt; char_cand.aspect_ratio <br/>        and char_cand.aspect_ratio &lt; 1.0):<br/>    <br/>        char_cands.append(char_cand) </pre>
<p>We will be using the characters to find the license plate, which is a different approach to other machine learning algorithms. This approach will help us understand the process of finding characters better.</p>
<p>We will iterate over all the contours and use the <kbd>charclass</kbd> class (which we have already defined). This automatically extracts centers, diagonal length, and aspect ratio to determine whether the image is too big or too small, or if the aspect ratio is too skewed From this, we can infer that the character is not a letter or number that will be on the license plate. This helps us consider only contours that meet the geometric criteria.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Finding matches and groups of characters</h1>
                
            
            
                
<p>Once the first pass is done, we will refine our matches to find a group of characters that potentially could belong to a license plate. Refer to the following code:</p>
<pre>for ch_matches in getmatchingchars(char_cands): <br/>     class blank: pass<br/>     plate_candidate = blank() <br/>    <br/>     ch_matches.sort(key = lambda ch: ch.x_cent) <br/>    <br/>     plate_w = int((ch_matches[len(ch_matches) - 1].rect_x + \<br/>                    ch_matches[len(ch_matches) - 1].rect_w - ch_matches[0].rect_x) * 1.3)<br/>    <br/>     sum_char_h = 0<br/>     for ch in ch_matches:<br/>         sum_char_h += ch.rect_h<br/>    <br/>     avg_char_h = sum_char_h / len(ch_matches)<br/>     plate_h = int(avg_char_h * 1.5)<br/>    <br/>     y = ch_matches[len(ch_matches) - 1].y_cent - ch_matches[0].y_cen<br/>     r = np.sqrt((abs(ch_matches[0].x_cent <br/>                   - ch_matches[len(ch_matches) - 1].x_cent) ** 2) <br/>                + (abs(ch_matches[0].y_cent <br/>                   - ch_matches[len(ch_matches) - 1].y_cent) ** 2))<br/>     rotate_angle = np.rad2deg(np.arcsin(y / r))</pre>
<p>We will iterate over all the potential characters by calling the <kbd>getmatchingchars</kbd> function we used before, which provides additional filtering based on the criteria. It depends on the angles, trigonometry, width, and height compared to neighboring characters, and also on the kind of neighbors. These criteria help us achieve uniformity.</p>
<p>Once we have our plate candidates, we can create a <kbd>blank</kbd> object. So, we have a <kbd>blank</kbd> object with no attributes and create a list of them. We first sort from the center of those characters, which will help us sort from leftmost to rightmost going through the matches.</p>
<p>The <kbd>sum_char_h</kbd> summation will help us find the average height and width of the characters.</p>
<p>Let's look at the following code:</p>
<pre>     platex = (ch_matches[0].x_cent + ch_matches[len(ch_matches) - 1].x_cent) / 2<br/>     platey = (ch_matches[0].y_cent + ch_matches[len(ch_matches) - 1].y_cent) / 2<br/>     plate_cent = platex, platey</pre>
<p>The ideal position of the license plate is perpendicular to the camera. If the license plate is at an angle greater than a particular acceptable angle, or upside down, there is a possibility that we will not be able to read the license plate.</p>
<p>We find our <kbd>x</kbd> and <kbd>y</kbd> from the code, and correct the angle for the license plate if it is within a reasonable angle.</p>
<p>We then figure out the plate location, and store it for computation later using <kbd>rotationMatrix</kbd>. We can do this in one step, based on the angle that we found here. We want to rotate it about the center of the plate, as follows:</p>
<pre>     plate_candidate.plateloc = (tuple(plate_cent), (plate_w, plate_h), rotate_angle)<br/>     rotationMatrix = cv2.getRotationMatrix2D(tuple(plate_cent), rotate_angle, 1.0)</pre>
<p>We create our rotated image here, and the <kbd>cv2.wrapAffine</kbd> function will help us with stretching, skewing, rotating, and translation, as well as higher-order transformations such as scaling, stretching, and rotating:</p>
<pre>     rotated = cv2.warpAffine(input_image, rotationMatrix, tuple(np.flipud(input_image.shape[:2])))<br/><br/>    plate_candidate.plate_im = cv2.getRectSubPix(rotated, (plate_w, plate_h), tuple(plate_cent))<br/>    if plate_candidate.plate_im is not None:<br/>        plate_candidates.append(plate_candidate)</pre>
<p>Once we have our sub-image, which is rotated and centered around plate candidates, we save it to our plate candidates list, which we initiated earlier. We now have our characters and our initial guess for our plate candidates, using which we are ready to find and read our license plate candidates.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Finding and reading license plates with OpenCV</h1>
                
            
            
                
<p>We have already found our characters, which are license plate candidates. Now we need to determine which characters match, so that we can extract the text data and map the characters within the license plates.</p>
<p>First, we run each plate candidate through our <kbd>gray_thresh_img</kbd> function, which does our de-noising and binarization. In this case, we get a cleaner output because we are using a sub-image and not the complete image.</p>
<p>This is the extraction code we will use:</p>
<pre>for plate_candidate in plate_candidates: <br/>    <br/>    plate_candidate.grayimg, plate_candidate.thesholded = \<br/>                              gray_thresh_img(plate_candidate.plate_im) <br/>    plate_candidate.thesholded = cv2.resize(plate_candidate.thesholded, <br/>                                             (0, 0), fx = 1.6, fy = 1.6)<br/>    thresholdValue, plate_candidate.thesholded = \<br/>                               cv2.threshold(plate_candidate.thesholded, <br/>                                            0.0, 255.0, <br/>                                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)</pre>
<p>We will need our characters to be of the same size, since we will be using the k-nn approach, which is case-sensitive. If the size differs, we will receive garbage values. After we have the images sized, we need to perform thresholding, for which we will use the <kbd>OTSU</kbd> method.</p>
<p>We then need to find contours within our sub-image, and do a sanity check to make sure that the contours we found within our sub-image meet certain criteria where the size and aspect ratio are reasonable, as follows:</p>
<pre>contours = cv2.findContours(plate_candidate.thesholded, cv2.RETR_LIST, <br/> cv2.CHAIN_APPROX_SIMPLE)[1]<br/>plate_chars = [] <br/> for contour in contours: <br/> char_cand = charclass(contour)<br/> if (char_cand.rect_area &gt; 80 and char_cand.rect_w &gt; 2 <br/> and char_cand.rect_h &gt; 8 and 0.25 &lt; char_cand.aspect_ratio <br/> and char_cand.aspect_ratio &lt; 1.0):<br/> plate_chars.append(char_cand) </pre>
<p>If the contours do not meet the criteria, it means that we are either not looking at a license plate or not getting good characters.</p>
<p>Once the sanity check is complete, we run our <kbd>getmatchingchars</kbd> function, which will ensure we get a good group of characters that are roughly of the same size:</p>
<pre>plate_chars = getmatchingchars(plate_chars)<br/>  <br/>     if (len(plate_chars) == 0):<br/>     plate_candidate.chars = \"<br/>     continue<br/>for index in range(0, len(plate_chars)): <br/>    plate_chars[index].sort(key = lambda ch: ch.x_cent) <br/>    filt_matching_chars = list(plate_chars[index])</pre>
<p>This is a redundancy check, but is necessary for achieving clean and reliable results. We iterate over all the characters from left to right, in order, to check that the characters are sufficiently far apart. We do this because, conceivably, near contours that overlap each other could be characters that overlap, which would never happen in a real license plate.</p>
<p>We need to make sure that the characters are far apart, as we're not detecting the same thing over and over again; we are doing multiple <kbd>for</kbd> loops here and comparing characters to each other as follows:</p>
<pre>    for thischar in plate_chars[index]:<br/> for alt_char in plate_chars[index]:<br/> if thischar != alt_char: <br/> chardistance = np.sqrt((abs(thischar.x_cent-alt_char.x_cent)**2) <br/> + (abs(thischar.y_cent-alt_char.y_cent) ** 2))<br/> if chardistance &lt; (thischar.hypotenuse * 0.3):<br/> if thischar.rect_area &lt; alt_char.rect_area: <br/> if thischar in filt_matching_chars: <br/> filt_matching_chars.remove(thischar) <br/> else: <br/> if alt_char in filt_matching_chars: <br/> filt_matching_chars.remove(alt_char) </pre>
<p>We need to make sure that everything is centered within our region of interest, so that characters are not lost when we perform actions such as scaling, rotation, and translation while we find our k-nn.</p>
<p>In this code, we go through each character in our character list and each thresholded region, to make sure we resize the region to <kbd>20</kbd> by <kbd>30</kbd>, which matches our k-nn prediction:</p>
<pre>     charlistlen = 0<br/> char_index = 0<br/> <br/> for index in range(0, len(plate_chars)):<br/> if len(plate_chars[index]) &gt; charlistlen:<br/> charlistlen = len(plate_chars[index])<br/> char_index = index<br/> <br/> full_char_list = plate_chars[char_index]<br/> full_char_list.sort(key = lambda ch: ch.x_cent) <br/> <br/> plate_candidate.chars = \<br/> for thischar in full_char_list: <br/> roi = plate_candidate.thesholded[thischar.rect_y : <br/> thischar.rect_y + thischar.rect_h,<br/> thischar.rect_x : <br/> thischar.rect_x + thischar.rect_w]<br/><br/> resized_roi = np.float32(cv2.resize(roi, (20, 30)).reshape((1, -1)))<br/> plate_candidate.chars += str(chr(int(knn.predict(resized_roi)[0])))</pre>
<p>Now, all these regions are of length 600. NumPy's <kbd>reshape</kbd> function will map the region by some dimensions for a two-dimensional input, to get 1/600.</p>
<p>The <kbd>thischar</kbd> function is actually an empty string at the start, but will keep getting populated as we find our k-nn.</p>
<p>Also, we need to make sure that our <kbd>plate_candidates</kbd> are not blank, while we find our best candidate:</p>
<pre>if len(plate_candidates) &gt; 0:<br/>    plate_candidates.sort(key = lambda plate_candidate: <br/>                           len(plate_candidate.chars), reverse = True)<br/>    best_plate = plate_candidates[0]<br/>    print("License plate read: " + best_plate.chars + "\n") </pre>
<p>You may find multiple plate candidates for a given image, but often they're the same thing. You might have just found something with four characters, when there are actually six, or something like that. The one that has the most characters is probably right, but you can take a look at the other candidates as well.</p>
<p>We'll extract and sort by the length of the string again, find the <kbd>best_plate</kbd>, and print out the results.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Result analysis</h1>
                
            
            
                
<p>When we run our code using the best candidate code block, we get the following result:</p>
<pre>if len(plate_candidates) &gt; 0: <br/>    plate_candidates.sort(key = lambda plate_candidate: <br/>                          len(plate_candidate.chars), reverse = True)<br/>    best_plate = plate_candidates[0]<br/>    print("License plate read: " + best_plate.chars + "\n")<br/><br/>License plate read: LTLDBENZ</pre>
<p>Once we get our output, we can display our result using the following code:</p>
<pre>figure()<br/>imshow(best_plate.thesholded)</pre>
<p>The displayed image will be as follows:</p>
<div><img class="alignnone size-full wp-image-522 image-border" src="img/29224956-b4cf-4893-8da4-65719bf221b2.png" style="width:29.08em;height:10.58em;" width="691" height="250"/></div>
<p>Although there is an extra character, we can see that our displayed image is very close to the plate characters. We can, check it with our other possible plate characters to get the closest result.</p>
<p>Let's try one more license plate, to check how our code works:</p>
<pre>input_image = plt.imread('tests/p2.jpg') #use cv2.imread or <br/>                                         #import matplotlib.pyplot as plt<br/>                                         #if running outside notebook<br/>figure()<br/>imshow(input_image)</pre>
<p>And here's the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-523 image-border" src="img/57f183fe-8a9b-4ad1-8ba9-ffb6f0a7792e.png" style="width:12.00em;height:1.33em;" width="260" height="27"/></p>
<p>The photo displayed is as follows:</p>
<div><img class="alignnone size-full wp-image-524 image-border" src="img/7b28522d-8233-4afb-878e-0093dadb36d1.png" style="width:27.00em;height:17.83em;" width="774" height="510"/></div>
<p>If you just want the sub-image of the plate, you can get it using the following code:</p>
<pre>imshow(best_plate.plate_im)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-525 image-border" src="img/11243ab0-18fc-481a-bf19-2fa2ebb7ed64.png" style="width:30.42em;height:8.08em;" width="670" height="178"/></p>
<p>We can find the location of the result as well:</p>
<pre>figure()<br/># best_plate.plate_im<br/>imshow(best_plate.plate_im)<br/>best_plate.plateloc</pre>
<p>You get the following location in the output:</p>
<div><img class="alignnone size-full wp-image-526 image-border" src="img/cb45ca54-91ea-4b91-9409-287cfeeaa58e.png" style="width:35.25em;height:6.25em;" width="665" height="117"/></div>
<p>So, here we have the <kbd>x</kbd> and <kbd>y</kbd> coordinates, width, height, and some offset information.</p>
<p>We can try other available functions, such as the following:</p>
<div><img class="alignnone size-full wp-image-527 image-border" src="img/f1a092ba-ec2b-41ae-ba6e-b1bb4da83729.png" style="width:27.08em;height:11.50em;" width="478" height="202"/></div>
<p>Let's look at another example where the license plate is not clearly visible:</p>
<pre>input_image = plt.imread('tests/p3.jpg') #use cv2.imread or <br/>                                         #import matplotlib.pyplot as plt<br/>                                         #if running outside notebook<br/>figure()<br/>imshow(input_image)</pre>
<p>This gives us the following output:</p>
<div><img class="alignnone size-full wp-image-528 image-border" src="img/065f0342-9089-4974-baa9-8e318f920fbd.png" style="width:31.75em;height:18.17em;" width="800" height="458"/></div>
<p>Let's take a closer look at the license plate:</p>
<div><img class="alignnone size-full wp-image-530 image-border" src="img/450aade3-a977-4687-8bdc-380bf9ff769f.png" style="width:31.00em;height:18.42em;" width="774" height="461"/></div>
<p>Our <kbd>display</kbd> function gives us a pretty good result, as follows:</p>
<div><img class="alignnone size-full wp-image-531 image-border" src="img/3ae5a073-5b57-4c78-9103-5542367fa5d2.png" style="width:30.33em;height:10.58em;" width="694" height="242"/></div>
<p>Let's look at our final example:</p>
<pre>input_image = plt.imread('tests/p1.jpg') #use cv2.imread or <br/>                                         #import matplotlib.pyplot as plt<br/>                                         #if running outside notebook<br/>figure()<br/>imshow(input_image)</pre>
<p>Here's the view:</p>
<div><img class="alignnone size-full wp-image-532 image-border" src="img/2cfe4a47-732f-4e28-afa4-3846c3e99449.png" style="width:33.75em;height:21.33em;" width="697" height="442"/></div>
<p>The following screenshot shows the output:</p>
<div><img class="alignnone size-full wp-image-533 image-border" src="img/1ee92e1c-609b-4dbb-ab44-7b1d24bcb9e3.png" style="width:16.58em;height:1.33em;" width="268" height="22"/></div>
<p class="mce-root"/>
<p>And the resulting photo is displayed as follows:</p>
<div><img class="alignnone size-full wp-image-535 image-border" src="img/cf337289-29d2-427e-9641-7c8d1098083c.png" style="width:29.58em;height:11.17em;" width="692" height="262"/></div>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned how to perform license plate recognition using OpenCV, giving us a good taste of how computer vision and image processing work.</p>
<p>We first learned the different plate utility functions, which helped us find our plate characters. We then found possible candidates for our license plate characters using OpenCV. Finally, we analyzed our results to check the efficiency of our algorithm. </p>
<p>In the next chapter, <a href="489ca4bf-4851-4f79-9de8-0b326ab68a70.xhtml" target="_blank">Chapter 4</a>, <em>Human Pose Estimation with TensorFlow</em>, we're going to use the DeeperCut algorithm, and ArtTrack models for human pose estimation.</p>


            

            
        
    </div>



  </body></html>