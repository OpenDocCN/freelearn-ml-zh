- en: First Steps in Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习的第一步
- en: This is the moment you've been waiting for, isn't it?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您一直等待的时刻，不是吗？
- en: We have covered all of the bases—we have a functioning Python environment, we
    have OpenCV installed, and we know how to handle data in Python. Now, it's time
    to build our first machine learning system! And what better way to start off than
    to focus on one of the most common and successful types of machine learning: **supervised
    learning**?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了所有的基础——我们有一个运行的 Python 环境，我们已经安装了 OpenCV，并且我们知道如何在 Python 中处理数据。现在，是时候构建我们的第一个机器学习系统了！还有什么比专注于最常见且最成功的机器学习类型之一：**监督学习**更好的开始方式呢？
- en: From the previous chapter, we already know that supervised learning is all about
    learning regularities in training data by using the labels that come with it so
    that we can predict the labels of some new, never-seen-before test data. In this
    chapter, we want to dig a little deeper and learn how to turn our theoretical
    knowledge ...
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一章，我们已经知道监督学习主要是通过使用与之相关的标签来学习训练数据中的规律性，以便我们可以预测一些新的、从未见过的测试数据的标签。在本章中，我们想要深入一点，学习如何将我们的理论知识
    ...
- en: Technical requirements
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer to the code for this chapter at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接中找到本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03)。
- en: 'The following is globally a summary of software and hardware requirements:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对软件和硬件要求的全球性总结：
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 OpenCV 版本 4.1.x（4.1.0 或 4.1.1 都可以正常工作）。
- en: You will need Python version 3.6 (any Python 3.x version will be fine).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Python 版本 3.6（任何 Python 3.x 版本都可以）。
- en: You will need Anaconda Python 3 to install Python and the required modules.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Anaconda Python 3 来安装 Python 和所需的模块。
- en: You can use any OS—macOS, Windows, and Linux-based OSes—with this book. We recommend
    you have at least 4 GB RAM in your system.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows 和基于 Linux 的操作系统——与本书一起使用。我们建议您的系统至少有 4 GB RAM。
- en: You don't need to have a GPU to run the code provided with this book.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要 GPU 就可以运行本书提供的代码。
- en: Understanding supervised learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督学习
- en: 'We have previously established that the goal of supervised learning is always
    to predict labels (or target values) for data. However, depending on the nature
    of these labels, supervised learning can come in two distinct forms:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经确定，监督学习的目标始终是预测数据的标签（或目标值）。然而，根据这些标签的性质，监督学习可以有两种不同的形式：
- en: '**Classification**: Supervised learning is called **classification** whenever
    we use the data to predict categories. A good example of this is when we try to
    predict whether an image contains a cat or a dog. Here, the labels of the data
    are categorical, either one or the other, but never a mixture of categories. For
    example, a picture contains either a cat or a dog, never 50% cat and 50% dog (before
    you ask, no, here we do not consider pictures of the cartoon character, CatDog),
    and our job ...'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：当我们使用数据来预测类别时，监督学习被称为**分类**。一个很好的例子是我们试图预测一张图片是否包含猫或狗。在这里，数据的标签是分类的，要么是猫，要么是狗，但永远不会是类别的混合。例如，一张图片要么包含猫，要么包含狗，永远不会是50%猫和50%狗（在你问之前，不，这里我们不考虑卡通角色猫狗的图片），我们的工作
    ...'
- en: Having a look at supervised learning in OpenCV
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 OpenCV 中查看监督学习
- en: Just knowing how supervised learning works is not going to be of any use if
    we can't put it into practice. Thankfully, OpenCV provides a pretty straightforward
    interface for all its statistical learning models, which includes all supervised
    learning models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不能将其付诸实践，仅仅知道如何进行监督学习是没有用的。幸运的是，OpenCV 为其所有的统计学习模型提供了一个相当直观的接口，这包括所有监督学习模型。
- en: In OpenCV, every machine learning model derives from the `cv::ml::StatModel` base
    class. This is fancy talk for saying that if we want to use a machine learning model in
    OpenCV, we have to provide all of the functionality that `StatModel` tells us
    to. This includes a method to train the model (called `train`) and a method to
    measure the performance of the model (called `calcError`).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，每个机器学习模型都源自`cv::ml::StatModel`基类。这听起来很复杂，意思是说，如果我们想在OpenCV中使用机器学习模型，我们必须提供`StatModel`告诉我们的所有功能。这包括一个用于训练模型的方法（称为`train`）和一个用于衡量模型性能的方法（称为`calcError`）。
- en: In **Object-Oriented Programming** (**OOP**), we deal primarily with objects or classes.
    An object consists of several functions, called **methods**, as well as variables,
    called **members** or **attributes**. You can learn more about OOP in Python at [https://docs.python.org/3/tutorial/classes.html](https://docs.python.org/3/tutorial/classes.html).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在**面向对象编程**（**OOP**）中，我们主要处理对象或类。一个对象由几个函数组成，称为**方法**，以及变量，称为**成员**或**属性**。您可以在Python中了解更多关于OOP的信息，请参阅[https://docs.python.org/3/tutorial/classes.html](https://docs.python.org/3/tutorial/classes.html)。
- en: 'Thanks to this organization of the software, setting up a machine learning
    model in OpenCV always follows the same logic, as we will see later:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种软件组织方式，在OpenCV中设置机器学习模型总是遵循相同的逻辑，我们稍后会看到：
- en: '**Initialization**: We call the model by name to create an empty instance of
    the model.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化**：我们通过名称调用模型以创建一个空的模型实例。'
- en: '**Set parameters**: If the model needs some parameters, we can set them via
    setter methods, which can be different for every model. For example, for a k-NN
    algorithm to work, we need to specify its open parameter, *k* (as we will find
    out later).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置参数**：如果模型需要一些参数，我们可以通过setter方法来设置它们，这些方法对于每个模型可能都不同。例如，对于k-NN算法要工作，我们需要指定其open参数，*k*（我们稍后会了解到）。'
- en: '**Train the model**: Every model must provide a method called `train`, used
    to fit the model to some data.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型**：每个模型都必须提供一个名为`train`的方法，用于将模型拟合到某些数据。'
- en: '**Predict new labels**: Every model must provide a method called `predict`,
    used to predict the labels of new data.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测新标签**：每个模型都必须提供一个名为`predict`的方法，用于预测新数据的标签。'
- en: '**Score the model**: Every model must provide a method called `calcError`,
    used to measure performance. This calculation might be different for every model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评分模型**：每个模型都必须提供一个名为`calcError`的方法，用于衡量性能。这种计算对于每个模型可能都不同。'
- en: Because OpenCV is a vast and community-driven project, not every algorithm follows
    these rules to the extent that we as users might expect. For example, the k-NN
    algorithm does most of its work in a `findNearest` method, although `predict` still
    works. We will make sure to point out these discrepancies as we work through different
    examples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenCV是一个庞大且由社区驱动的项目，并非每个算法都像我们作为用户期望的那样遵循这些规则。例如，k-NN算法的大部分工作都在`findNearest`方法中完成，尽管`predict`仍然有效。我们将确保在处理不同示例时指出这些差异。
- en: As we will make occasional use of scikit-learn to implement some machine learning
    algorithms that OpenCV does not provide, it is worth pointing out that learning
    algorithms in scikit-learn follow an almost identical logic. The most notable
    difference is that scikit-learn sets all of the required model parameters in the
    initialization step. Also, it calls the training function, `fit`, instead of `train` and
    the scoring function `score` instead of `calcError`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将偶尔使用scikit-learn来实现OpenCV不提供的某些机器学习算法，因此指出scikit-learn中的学习算法几乎遵循相同的逻辑是值得的。最显著的区别是，scikit-learn在初始化步骤中设置所有必需的模型参数。此外，它调用训练函数`fit`而不是`train`，以及评分函数`score`而不是`calcError`。
- en: Measuring model performance with scoring functions
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用评分函数衡量模型性能
- en: One of the most important parts of building a machine learning system is to find a
    way to measure the quality of model predictions. In real-life scenarios, a model
    will rarely get everything right. From earlier chapters, we know that we are supposed
    to use data from the test set to evaluate our model. But how exactly does that
    work?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习系统最重要的部分之一是找到一种方法来衡量模型预测的质量。在现实场景中，模型很少能完全正确。从前面的章节中，我们知道我们应该使用测试集的数据来评估我们的模型。但这是如何实现的呢？
- en: The short, but not very helpful, answer is that it depends on the model. People
    have come up with all sorts of scoring functions that can be used to evaluate
    the trained model in all possible scenarios. The good news is that a lot of them
    are actually part of scikit-learn's `metrics` module.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简短但不太有帮助的回答是，这取决于模型。人们已经想出了各种各样的评分函数，可以用来评估在所有可能的场景下训练好的模型。好消息是，其中许多实际上是scikit-learn的`metrics`模块的一部分。
- en: Let's have a quick look at some of the most important scoring functions. ...
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下一些最重要的评分函数。...
- en: Scoring classifiers using accuracy, precision, and recall
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用准确度、精确度和召回率评分分类器
- en: 'In a binary classification task where there are only two different class labels,
    there are several different ways to measure classification performance. Some common
    metrics are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个只有两个不同类别标签的二分类任务中，有几种不同的方式来衡量分类性能。以下是一些常见的指标：
- en: '`accuracy_score`: Accuracy counts the number of data points in the test set
    that have been predicted correctly and returns that number as a fraction of the
    test set size. Sticking to the example of classifying pictures as cats or dogs,
    accuracy indicates the fraction of pictures that have been correctly classified
    as containing either a cat or a dog. This is the most basic scoring function for
    classifiers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accuracy_score`：准确度计算测试集中被正确预测的数据点的数量，并以测试集大小的分数形式返回该数量。坚持将图片分类为猫或狗的例子，准确度表示被正确分类为包含猫或狗的图片的比例。这是分类器的最基本评分函数。'
- en: '`precision_score`: Precision describes the ability of a classifier not to label
    as a cat a picture that contains a dog. In other words, out of all of the pictures
    in the test set that the classifier thinks contain a cat, precision is the fraction
    of pictures that actually contain a cat.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precision_score`：精确度描述了分类器不将包含狗的图片标记为猫的能力。换句话说，在测试集中，分类器认为包含猫的所有图片中，精确度是实际包含猫的图片的比例。'
- en: '`recall_score`: Recall (or sensitivity) describes the ability of a classifier to
    retrieve all of the pictures that contains a cat. In other words, out of all of
    the pictures of cats in the test set, recall is the fraction of pictures that
    has been correctly identified as pictures of cats.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recall_score`：召回率（或灵敏度）描述了分类器检索所有包含猫的图片的能力。换句话说，在测试集中所有猫的图片中，召回率是正确识别为猫的图片的比例。'
- en: 'Let''s say, we have some `ground truth` (correct according to the dataset we
    have) class labels that are either zeros or ones. We can generate them at random
    using NumPy''s random number generator. Obviously, this means that, whenever we
    rerun our code, new data points will be generated at random. However, for the
    purpose of this book, this is not very helpful, as I want you to be able to run
    the code and always get the same result as me. A nice trick to achieve that is
    to fix the seed of the random number generator. This will make sure the generator
    is initialized the same way every time you run the script:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些`ground truth`（根据我们拥有的数据集是正确的）类别标签，这些标签要么是零，要么是一。我们可以使用NumPy的随机数生成器随机生成它们。显然，这意味着，每次我们重新运行代码时，都会生成新的随机数据点。然而，为了本书的目的，这并不太有帮助，因为我希望你能运行代码并总是得到和我一样的结果。实现这一点的一个好方法是将随机数生成器的种子固定。这将确保每次运行脚本时生成器都以相同的方式初始化：
- en: 'We can fix the seed of the random number generator using the following code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来设置随机数生成器的种子：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we can generate five random labels that are either zeros or ones by picking
    random integers in the range,  `(0,2)`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过在范围`(0,2)`中随机选择整数来生成五个随机标签，这些标签要么是零，要么是一：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the literature, these two classes are sometimes also called **positives** (all
    data points with the class label, `1`) and **negatives** (all other data points).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这两个类别有时也被称为**正类**（所有具有类别标签`1`的数据点）和**负类**（所有其他数据点）。
- en: 'Let''s assume we have a classifier that tries to predict the class labels mentioned
    earlier. For the sake of argument, let''s say the classifier is not very smart
    and always predicts the label, `1`. We can mock this behavior by hardcoding the
    prediction labels:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们有一个试图预测前面提到的类别标签的分类器。为了辩论的目的，让我们假设这个分类器不是很聪明，总是预测标签`1`。我们可以通过硬编码预测标签来模拟这种行为：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: What is the accuracy of our prediction?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测准确度是多少？
- en: As mentioned earlier, accuracy counts the number of data points in the test
    set that have been predicted correctly and returns that number as a fraction of
    the test set size. We correctly predicted only the second data point (where the
    true label is `1`). In all other cases, the true label was `0`, yet we predicted
    `1`. Hence, our accuracy should be 1/5 or 0.2.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，准确率计算测试集中被正确预测的数据点的数量，并将其作为测试集大小的分数返回。我们只正确预测了第二个数据点（其中真实标签是`1`）。在其他所有情况下，真实标签是`0`，但我们预测了`1`。因此，我们的准确率应该是1/5或0.2。
- en: 'A naive implementation of an accuracy metric might sum up all occurrences where
    the predicted class label matched the true class label:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的准确率度量实现可能只是将所有预测类别标签与真实类别标签匹配的实例相加：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A smarter, and more convenient, implementation is provided by scikit-learn''s `metrics` module:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更智能且更方便的实现由scikit-learn的`metrics`模块提供：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That wasn''t too hard, was it? However, to understand precision and recall,
    we need a general understanding of type I and type II errors. Let''s recall that
    data points with the class label, `1`, are often called positives, and data points
    with the class label, `0` (or -1) are often called negatives. Then, classifying
    a specific data point can have one of four possible outcomes, as illustrated by the
    following confusion matrix:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太难，对吧？然而，要理解精确度和召回率，我们需要对I型错误和II型错误有一个一般性的理解。让我们回忆一下，具有类别标签`1`的数据点通常被称为正例，而具有类别标签`0`（或-1）的数据点通常被称为负例。然后，对特定数据点的分类可能有四种可能的结局，如下面的混淆矩阵所示：
- en: '|  | **Is truly positive** | **Is truly negative** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | **是否真正为正** | **是否真正为负** |'
- en: '| **Predicted positive** | True Positive | False Positive |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **预测为正** | 真阳性 | 假阳性 |'
- en: '| **Predicted negative** | False Negative | True Negative |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **预测为负** | 假阴性 | 真阴性 |'
- en: Let's break this down. If a data point was truly positive, and we predicted
    a positive, we got it all right! In this case, the outcome is called a **true
    positive**. If we thought the data point was a positive, but it was really a negative,
    we falsely predicted a positive (hence the term, **false positive**). Analogously,
    if we thought the data point was negative, but it was really a positive, we falsely predicted a
    negative (false negative). Finally, if we predicted a negative and the data point
    was truly a negative, we found a true negative.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下。如果一个数据点是真正正的，并且我们预测了正的，我们就完全做对了！在这种情况下，结果被称为**真阳性**。如果我们认为数据点是正的，但实际上它是负的，我们就错误地预测了正的（因此有这个术语，**假阳性**）。类似地，如果我们认为数据点是负的，但实际上它是正的，我们就错误地预测了负的（假阴性）。最后，如果我们预测了负的，而数据点确实是负的，我们就找到了一个**真阴性**。
- en: In statistical hypothesis testing, false positives are also known as **type
    I errors** and false negatives are also known as **type II errors**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计假设检验中，假阳性也被称为**I型错误**，而假阴性也被称为**II型错误**。
- en: 'Let''s quickly calculate these four metrics on our mock-up data. We have a
    true positive, where the true label is `1` and we predicted `1`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速计算这四个指标在我们的模拟数据上。我们有一个真阳性，其中真实标签是`1`，我们预测了`1`：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, a false positive is where we predicted `1` but `ground truth` was
    really `0`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，假阳性是指我们预测了`1`，但`ground truth`实际上是`0`：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I''m sure by now you''ve got the hang of it. But do we even have to do the
    math to know about predicted negatives? Our not-so-smart classifier never predicted
    `0`, so `(y_pred == 0)` should never be true:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信到现在你已经掌握了这个。但我们真的需要做数学运算来了解预测的负数吗？我们不太智能的分类器从未预测`0`，所以`(y_pred == 0)`永远不会为真：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s also draw the confusion matrix:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再绘制一下混淆矩阵：
- en: '|  | **Is truly positive** | **Is truly negative** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | **是否真正为正** | **是否真正为负** |'
- en: '| **Predicted positive** | 1 | 4 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **预测为正** | 1 | 4 |'
- en: '| **Predicted negative** | 0 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **预测为负** | 0 | 0 |'
- en: 'To make sure we did everything right, let''s calculate the accuracy one more
    time. Accuracy should be the number of true positives plus the number of true
    negatives (that is, everything we got right) divided by the total number of data
    points:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们一切都做得正确，让我们再次计算准确率。准确率应该是真阳性数加上真阴性数（即我们做对的一切）除以数据点的总数：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Success! Precision is then given as the number of true positives divided by
    the number of all true predictions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！精确度就是真阳性数除以所有真预测数：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It turns out that precision is no better than accuracy in our case. Let''s
    check our math with scikit-learn:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在我们的情况下，精确度并不比准确度更好。让我们用scikit-learn来检查我们的数学计算：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, `recall` is given as the fraction of all positives that we correctly
    classified as positives:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`recall`表示我们正确分类为正例的所有正例的比例：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Perfect recall! But, going back to our mock-up data, it should be clear that
    this excellent recall score was mere luck. Since there was only a single `1` label
    in our mock-up dataset, and we happened to correctly classify it, we got a perfect
    recall score. Does that mean our classifier is perfect? Not really! But we have
    found three useful metrics that seem to measure complementary aspects of our classification
    performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 完美回忆！但是，回到我们的模拟数据，应该很清楚，这个出色的召回分数只是运气。由于我们的模拟数据集中只有一个`1`标签，而且我们恰好正确地分类了它，所以我们得到了完美的召回分数。这意味着我们的分类器是完美的吗？不是的！但我们已经找到了三个有用的指标，它们似乎衡量了我们分类性能的互补方面。
- en: Scoring regressors using mean squared error, explained variance, and R squared
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用均方误差、解释方差和R平方评分回归器
- en: 'When it comes to regression models, our metrics, as shown earlier, don''t work
    anymore. After all, we are now predicting continuous output values, not distinct
    classification labels. Fortunately, scikit-learn provides some other useful scoring
    functions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到回归模型时，我们之前提到的指标不再适用。毕竟，我们现在预测的是连续的输出值，而不是不同的分类标签。幸运的是，scikit-learn提供了一些其他有用的评分函数：
- en: '`mean_squared_error`: The most commonly used error metric for regression problems
    is to measure the squared error between the predicted and the true target value
    for every data point in the training set, averaged across all of the data points.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean_squared_error`：回归问题中最常用的误差指标是测量训练集中每个数据点的预测值与真实目标值之间的平方误差，然后对所有数据点进行平均。'
- en: '`explained_variance_score`: A more sophisticated metric is to measure to what
    degree a model can explain the variation or dispersion of the test data. Often,
    the amount of explained ...'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explained_variance_score`：一个更复杂的指标是衡量模型在多大程度上可以解释测试数据的变异或分散程度。通常，解释的...'
- en: Using classification models to predict class labels
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类模型预测类别标签
- en: With these tools in hand, we can now take on our first real classification example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些工具在手，我们现在可以开始处理我们的第一个真正的分类示例。
- en: Consider the small town of Randomville, where people are crazy about their two
    sports teams, the Randomville Reds and the Randomville Blues. The Reds had been
    around for a long time, and people loved them. But then, some out-of-town millionaire
    came along and bought the Reds' top scorer and started a new team, the Blues.
    To the discontent of most Reds fans, the top scorer would go on to win the championship
    title with the Blues. Years later, he would return to the Reds, despite some backlash
    from fans who could never forgive him for his earlier career choices. But anyway,
    you can see why fans of the Reds don't necessarily get along with fans of the
    Blues. In fact, these two fan bases are so divided that they never even live next
    to each other. I've even heard stories where the Red fans deliberately moved away
    once Blues fans moved in next door. True story!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下Randomville这个小城镇，这里的人们疯狂地喜爱他们的两支运动队，Randomville Reds和Randomville Blues。Reds队历史悠久，深受人们喜爱。然而，后来一位外地百万富翁出现了，他买下了Reds队的最佳射手，并组建了一支新队，Blues队。让大多数Reds球迷不满的是，这位最佳射手最终带着Blues队赢得了冠军头衔。多年后，他回到了Reds队，尽管一些球迷因为他的早期职业选择而对他怀恨在心。但无论如何，你可以理解为什么Reds队的球迷并不一定和Blues队的球迷相处融洽。事实上，这两个球迷群体如此分裂，以至于他们甚至从不住在隔壁。我甚至听说过红队球迷在蓝队球迷搬进来后故意搬走的故事。这是真的！
- en: Anyway, we are new in town and are trying to sell some Blues merchandise to
    people by going from door to door. However, every now and then we come across
    a bleeding-heart Reds fan who yells at us for selling Blues stuff and chases us
    off their lawn. Not nice! It would be much less stressful, and a better use of
    our time, to avoid these houses altogether and just visit the Blues fans instead.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们刚搬到这个城镇，正试图挨家挨户地推销一些Blues队的商品。然而，时不时地我们会遇到一个狂热的Reds球迷，他们对我们卖Blues队的商品大喊大叫，并把我们赶出自己的草坪。真不友好！避免这些房子，只访问Blues队的球迷，会轻松得多，也能更好地利用我们的时间。
- en: 'Confident that we can learn to predict where the Reds fans live, we start keeping
    track of our encounters. If we come by a Reds fan''s house, we draw a red triangle
    on our handy town map; otherwise, we draw a blue square. After a while, we get
    a pretty good idea of where everyone lives:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有信心学会预测Reds球迷的居住地，于是开始记录我们的遭遇。如果我们经过一个Reds球迷的家，我们在手头的城镇地图上画一个红色三角形；否则，我们画一个蓝色正方形。过了一段时间，我们就能很好地了解每个人的居住地：
- en: '![](img/ced3ad58-fea3-4305-8752-ecebfde7cc9f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ced3ad58-fea3-4305-8752-ecebfde7cc9f.png)'
- en: However, now, we approach the house that is marked as a green circle in the
    preceding map. Should we knock on their door? We try to find some clue as to what
    team they prefer (perhaps a team flag hanging from the back porch), but we can't
    see any. How can we know if it is safe to knock on their door?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在，我们来到地图上标记为绿色圆圈的房屋。我们应该敲门吗？我们试图找到一些线索，了解他们更喜欢哪个球队（也许是在后门廊上挂着的球队旗帜），但我们什么也没看到。我们怎么知道敲门是否安全呢？
- en: What this silly example illustrates is exactly the kind of problem a supervised
    learning algorithm can solve. We have a bunch of observations (houses, their locations,
    and their colors) that make up our training data. We can use this data to learn
    from experience so that, when we face the task of predicting the color of a new
    house, we can make a well-informed estimate.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个愚蠢的例子所说明的正是监督学习算法可以解决的问题。我们有一系列观察结果（房屋、位置和颜色），构成了我们的训练数据。我们可以使用这些数据从经验中学习，以便当我们面对预测新房屋颜色的任务时，我们可以做出明智的估计。
- en: As we mentioned earlier, fans of the Reds are really passionate about their
    team, so they would never move next to a Blues fan. Couldn't we use this information
    and look at all of the neighboring houses, to find out what kind of fan lives
    in the new house?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，红队球迷对他们的球队非常热情，所以他们绝不会搬到蓝队球迷的隔壁。我们难道不能利用这些信息，查看所有相邻的房屋，以了解新房子里住的是哪种球迷吗？
- en: This is exactly what the k-NN algorithm would do.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是k-NN算法会做的事情。
- en: Understanding the k-NN algorithm
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解k-NN算法
- en: 'The k-NN algorithm is arguably one of the simplest machine learning algorithms.
    The reason for this is that we basically only need to store the training dataset.
    Then, to predict a new data point, we only need to find the closest data point
    in the training dataset: its nearest neighbor.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法可以说是最简单的机器学习算法之一。原因在于我们基本上只需要存储训练数据集。然后，为了预测一个新的数据点，我们只需要在训练数据集中找到最近的数据点：它的最近邻。
- en: 'In a nutshell, the k-NN algorithm argues that a data point probably belongs
    to the same class as its neighbors. Think about it: if our neighbor is a Reds
    fan, we''re probably Reds fans, too; otherwise, we would have moved away a long
    time ago. The same can be said for the Blues.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，k-NN算法认为一个数据点很可能属于与其邻居相同的类别。想想看：如果我们的邻居是红队球迷，我们可能也是红队球迷；否则，我们早就搬走了。对蓝队也是如此。
- en: Of course, some neighborhoods might be a little more complicated. In this case,
    we would not just consider our closest neighbor (where *k=1*), but instead ...
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一些社区可能要复杂一些。在这种情况下，我们不仅会考虑我们的最近邻（当*k=1*时），而是...
- en: Implementing k-NN in OpenCV
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中实现k-NN
- en: 'Using OpenCV, we can easily create a k-NN model via the `cv2.ml.KNearest_create()`
    function. Building the model then involves the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenCV，我们可以通过`cv2.ml.KNearest_create()`函数轻松创建k-NN模型。构建模型涉及以下步骤：
- en: Generate some training data.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一些训练数据。
- en: Create a k-NN object for a given number, *k*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为给定的数字*k*创建一个k-NN对象。
- en: Find the *k* nearest neighbors of a new data point that we want to classify.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到我们想要分类的新数据点的*k*个最近邻。
- en: Assign the class label of the new data point by majority vote.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票为新数据点分配类别标签。
- en: Plot the result.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果。
- en: 'We first import all of the necessary modules: OpenCV for the k-NN algorithm,
    NumPy for data processing, and Matplotlib for plotting. If you are working in
    a Jupyter Notebook, don''t forget to call the `%matplotlib inline` magic:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所有必要的模块：OpenCV用于k-NN算法，NumPy用于数据处理，Matplotlib用于绘图。如果你在一个Jupyter Notebook中工作，别忘了调用`%matplotlib
    inline`魔法：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Generating the training data
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成训练数据
- en: 'The first step is to generate some training data. For this, we will use NumPy''s
    random number generator. As discussed in the previous section, we will fix the
    seed of the random number generator, so that re-running the script will always
    generate the same values:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是生成一些训练数据。为此，我们将使用NumPy的随机数生成器。正如前文所述，我们将固定随机数生成器的种子，以便重新运行脚本时总是生成相同的值：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Alright, now let's get to it. What should our training data look like exactly?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们开始吧。我们的训练数据应该是什么样子呢？
- en: In the previous example, each data point is a house on the town map. Every data
    point has two features (that is, the *x* and *y* coordinates of its location on
    the town map) and a class label (that is, a blue square if a Blues fan lives there
    and a red triangle if a Reds fan lives there).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，每个数据点都是城镇地图上的一个房子。每个数据点有两个特征（即，它在城镇地图上的位置*x*和*y*坐标）和一个类别标签（即，如果住在这里的是蓝球队球迷，则显示蓝色方块；如果是红队球迷，则显示红色三角形）。
- en: The features of a single data point can, therefore, be represented ...
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单个数据点的特征可以表示为 ...
- en: Training the classifier
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分类器
- en: 'As with all other machine learning functions, the k-NN classifier is part of
    the OpenCV 3.1 `ml` module. We can create a new classifier using the following
    command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有其他机器学习函数一样，k-NN分类器是OpenCV 3.1的`ml`模块的一部分。我们可以使用以下命令创建一个新的分类器：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In older versions of OpenCV, this function might be called `cv2.KNearest()`
    instead.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV的旧版本中，此函数可能被称为`cv2.KNearest()`。
- en: 'We then pass our training data to the `train` method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将我们的训练数据传递给`train`方法：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we have to tell `knn` that our data is an *N x 2* array (that is, every
    row is a data point). Upon success, the function returns `True`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须告诉`knn`我们的数据是一个*N x 2*的数组（即，每一行是一个数据点）。成功后，该函数返回`True`。
- en: Predicting the label of a new data point
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测新数据点的标签
- en: The other really helpful method that `knn` provides is called `findNearest`.
    It can be used to predict the label of a new data point based on its nearest neighbors.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`knn`提供的另一个非常有用的方法是`findNearest`。它可以用来根据新数据点的最近邻预测其标签。'
- en: 'Thanks to our `generate_data` function, it is actually really easy to generate
    a new data point! We can think of a new data point as a dataset of size `1`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了我们的`generate_data`函数，生成新的数据点实际上非常简单！我们可以将新的数据点视为大小为`1`的数据集：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our function also returns a random label, but we are not interested in that.
    Instead, we want to predict it using our trained classifier! We can tell Python
    to ignore an output value with an underscore (`_`).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能还会返回一个随机标签，但我们对此不感兴趣。相反，我们想使用我们的训练分类器来预测它！我们可以告诉Python忽略一个输出值（下划线`_`）。
- en: Let's have a look at our town map again. We will plot the training set as we
    did earlier, but ...
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看我们的城镇地图。我们将像之前一样绘制训练集，但 ...
- en: Using regression models to predict continuous outcomes
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归模型预测连续结果
- en: Now, let's turn our attention to a regression problem. As I'm sure you can recite
    in your sleep by now, regression is all about predicting continuous outcomes rather
    than predicting discrete class labels.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向一个回归问题。我相信你现在可以倒背如流，回归完全是关于预测连续结果，而不是预测离散的类别标签。
- en: Understanding linear regression
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性回归
- en: The easiest regression model is called **linear regression**. The idea behind
    linear regression is to describe a target variable (such as Boston house pricing—recall
    the various datasets we studied in [Chapter 1](7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml),
    *A Taste of Machine Learning*) with a linear combination of features.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '最简单的回归模型称为**线性回归**。线性回归背后的思想是用特征的线性组合来描述目标变量（例如波士顿房价——回想一下我们在[第1章](7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml)，“机器学习的味道”中研究的各种数据集）。 '
- en: 'To keep things simple, let''s just focus on two features. Let''s say we want
    to predict tomorrow''s stock prices using two features: today''s stock price and
    yesterday''s stock price. We will denote today''s stock price as the first feature, *f[1]*,
    and yesterday''s stock price as *f[2]*. Then, the goal of linear regression would
    be to learn two weight coefficients, *w[1]* and *w[2]*, so that we can predict
    tomorrow''s stock price as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，让我们只关注两个特征。假设我们想使用两个特征来预测明天的股票价格：今天的股票价格和昨天的股票价格。我们将今天的股票价格表示为第一个特征，*f[1]*，将昨天的股票价格表示为*f[2]*。那么，线性回归的目标就是学习两个权重系数，*w[1]*和*w[2]*，以便我们可以按照以下方式预测明天的股票价格：
- en: Here,  is the
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，  是
- en: Linear regression in OpenCV
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenCV中的线性回归
- en: 'Before trying out linear regression on a real-life dataset, let''s understand
    how we can use the `cv2.fitLine` function to fit a line to a 2D or 3D point set:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试在真实数据集上使用线性回归之前，让我们了解如何使用`cv2.fitLine`函数将线拟合到二维或三维点集：
- en: 'Let''s start by generating some points. We will generate them by adding noise
    to the points lying on the line ![](img/25934214-92e9-4194-937c-af8f014804a7.png):'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从生成一些点开始。我们将通过向位于线上的点添加噪声来生成它们！![图片](img/25934214-92e9-4194-937c-af8f014804a7.png)：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can also visualize these points using the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码可视化这些点：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This gives us the following diagram, where the red line is the true function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图表，其中红色线是真实函数：
- en: '![](img/93344671-1736-4539-991e-8face2c9f3a6.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/93344671-1736-4539-991e-8face2c9f3a6.png)'
- en: 'Next, we will split the points into training and testing sets. Here, we will
    split the data into a 70:30 ratio, meaning, 70% of the points will be used for
    training and 30% for testing:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将点分为训练集和测试集。在这里，我们将数据分为70:30的比例，这意味着，70%的点将用于训练，30%用于测试：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s use `cv2.fitLine` to fit a line to this 2D point set. This function
    takes in the following arguments:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`cv2.fitLine`将一条线拟合到这个二维点集。这个函数接受以下参数：
- en: '`points`: This is the set of points to which a line has to be fit.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`points`：这是需要拟合直线的点的集合。'
- en: '`distType`: This is the distance used by the M-estimator.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distType`：这是M-估计器使用的距离。'
- en: '`param`: This is the numerical parameter (C), which is used in some types of
    distances. We will keep it at 0 so that an optimal value can be chosen.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`param`：这是数值参数（C），在某种类型的距离中使用。我们将将其保持为0，以便选择最佳值。'
- en: '`reps`: This is the accuracy of the distance between the origin and the line.
    `0.01` is a good default value for `reps`.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reps`：这是原点到直线的距离的精度。"0.01"是`reps`的一个很好的默认值。'
- en: '`aeps`: This is the accuracy of the angle. `0.01` is a good default value for `aeps`.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aeps`：这是角度的精度。"0.01"是`aeps`的一个很好的默认值。'
- en: For more information, have a look at the [documentation](https://docs.opencv.org/4.0.0/d3/dc0/group__imgproc__shape.html#gaf849da1fdafa67ee84b1e9a23b93f91f).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅[文档](https://docs.opencv.org/4.0.0/d3/dc0/group__imgproc__shape.html#gaf849da1fdafa67ee84b1e9a23b93f91f)。
- en: 'Let''s see what kinds of result we get using different distance type options:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看使用不同的距离类型选项会得到什么样的结果：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will also use scikit-learn''s `LinearRegression` to fit the training points
    and then use the `predict` function to predict the *y*-values for them:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将使用scikit-learn的`LinearRegression`来拟合训练点，然后使用`predict`函数预测它们的*y*-值：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We use `reshape(-1,1)` and `reshape(1,-1)` to convert the NumPy arrays into
    a column vector and then back into a row vector:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`reshape(-1,1)`和`reshape(1,-1)`将NumPy数组转换为列向量，然后再将其转换回行向量：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The only purpose of this preceding (and lengthy) code was to create a plot that
    could be used to compare the results obtained using different distance measures.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这段前面（且相当长）的代码的唯一目的就是创建一个可以用来比较使用不同距离度量获得的结果的图表。
- en: 'Let''s have a look at the plot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个图：
- en: '![](img/02196ce1-87a5-4273-b2b5-43bd4f62b56e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02196ce1-87a5-4273-b2b5-43bd4f62b56e.png)'
- en: As we can clearly see, scikit-learn's `LinearRegression` model performs much
    better than OpenCV's `fitLine` function. Now, let's use scikit-learn's API to
    predict Boston housing prices.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们清楚地看到，scikit-learn的`LinearRegression`模型比OpenCV的`fitLine`函数表现要好得多。现在，让我们使用scikit-learn的API来预测波士顿房价。
- en: Using linear regression to predict Boston housing prices
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归预测波士顿房价
- en: 'To get a better understanding of linear regression, we want to build a simple
    model that can be applied to one of the most famous machine learning datasets:
    the **Boston housing prices dataset**. Here, the goal is to predict the value
    of homes in several Boston neighborhoods in the 1970s, using information such
    as crime rate, property tax rate, distance to employment centers, and highway
    accessibility.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解线性回归，我们想要构建一个简单的模型，该模型可以应用于最著名的机器学习数据集之一：**波士顿房价数据集**。在这里，目标是使用诸如犯罪率、财产税率、到就业中心的距离和高速公路可及性等信息来预测20世纪70年代几个波士顿地区的房价。
- en: Loading the dataset
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'We can again thank scikit-learn for easy access to the dataset. We first import
    all of the necessary modules, as we did earlier:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次感谢scikit-learn为我们提供了轻松访问数据集的方式。我们首先导入所有必要的模块，就像我们之前做的那样：
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then loading the dataset is a one-liner:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集只需要一行代码：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The structure of the `boston` object is identical to the `iris` object, as
    discussed in the preceding command. We can get more information about the dataset
    in `''DESCR''` and find all data in `''data''`, all feature names in `''feature_names''`, the
    physical location of the Boston CSV dataset in `''filename''`, and all target
    values in `''target''`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`boston`对象的结构与前面命令中讨论的`iris`对象相同。我们可以在`''DESCR''`中获取更多关于数据集的信息，在`''data''`中找到所有数据，在`''feature_names''`中找到所有特征名称，在`''filename''`中找到波士顿CSV数据集的物理位置，在`''target''`中找到所有目标值：'
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The dataset contains a total of `506` data points, each of which has `13` features:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含总共`506`个数据点，每个点都有`13`个特征：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Of course, we have only a single target value, which is the housing price:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们只有一个目标值，那就是房价：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training the model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now let''s create a `LinearRegression` model that we will then train on the
    training set:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个`LinearRegression`模型，然后将在训练集上对其进行训练：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding command, we want to split the data into training and test
    sets. We are free to make the split as we see fit, but usually it is a good idea
    to reserve between 10 percent and 30 percent for testing. Here, we choose 10 percent,
    using the `test_size` argument:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们想要将数据分为训练集和测试集。我们可以自由地按照我们的意愿进行分割，但通常保留10%到30%用于测试是个好主意。在这里，我们选择10%，使用`test_size`参数：
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In scikit-learn, the `train` function is called `fit` but otherwise behaves
    exactly the same as in OpenCV:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`train`函数被称为`fit`，但除此之外的行为与OpenCV中的完全相同：
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Testing the model
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'To test the generalization performance of the model, we calculate the mean squared
    error on the test data:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试模型的泛化性能，我们在测试数据上计算均方误差：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We note that the mean squared error is a little lower on the test set than
    the training set. This is good news, as we care mostly about the test error. However,
    from these numbers it is really hard to understand how good the model really is.
    Perhaps it''s better to plot the data:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，测试集上的均方误差比训练集略低。这是一个好消息，因为我们主要关心测试误差。然而，从这些数字中很难理解模型真正有多好。也许绘制数据会更清楚：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This produces the following diagram:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/2c458025-ac75-4430-91a7-23f68787efb4.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c458025-ac75-4430-91a7-23f68787efb4.png)'
- en: 'This makes more sense! Here, we see the `ground truth` housing prices for all
    test samples in red and our predicted housing prices in blue. It''s pretty close,
    if you ask me. It is interesting to note, though, that the model tends to be off
    the most for really high or really low housing prices, such as the peak values
    of data points **12**, **18**, and **42**. We can formalize the amount of variance
    in the data that we were able to explain by calculating R squared:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这更有意义！在这里，我们可以看到所有测试样本的`ground truth`房价用红色表示，我们的预测房价用蓝色表示。如果你问我，这已经很接近了。不过，值得注意的是，模型对于极高或极低的房价往往偏离最大，例如数据点的峰值**12**、**18**和**42**。我们可以通过计算R平方来形式化我们能够解释的数据中的方差量：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will plot the `ground truth` prices, `y_test`, on the *x* axis and our
    predictions, `y_pred`, on the *y* axis. We also plot a diagonal line for reference
    (using a black dashed line, `''k--''`), as we will see soon. But we also want
    to display the R² score and mean squared error in a textbox:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在*x*轴上绘制`ground truth`价格，即`y_test`，并在*y*轴上绘制我们的预测值，即`y_pred`。我们还会绘制一条参考线（使用黑色虚线`'k--'`），很快我们就会看到。但我们还希望在文本框中显示R²分数和均方误差：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will produce the following diagram and is a professional way of plotting
    a model fit:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表，这是一种专业绘制模型拟合的方法：
- en: '![](img/139e4726-1300-4c73-aa11-c8282dba5832.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/139e4726-1300-4c73-aa11-c8282dba5832.png)'
- en: If our model was perfect, then all data points would lie on the dashed diagonal,
    since `y_pred` would always be equal to `y_true`. Deviations from the diagonal
    indicate that the model made some errors, or that there is some variance in the
    data that the model was not able to explain. Indeed, ![](img/a15dc10c-dde6-4547-ac44-71066ff64510.png)
    indicates that we were able to explain 76% of the scatter in the data, with a
    mean squared error of 14.996\. These are some performance measures we can use
    to compare the linear regression model to some more complicated ones.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型是完美的，那么所有数据点都会位于虚线对角线上，因为`y_pred`将始终等于`y_true`。与对角线偏离表示模型犯了错误，或者数据中存在模型无法解释的方差。实际上，![](img/a15dc10c-dde6-4547-ac44-71066ff64510.png)表明我们能够解释数据中的76%的散点，均方误差为14.996。这些都是我们可以用来比较线性回归模型和一些更复杂模型的性能指标。
- en: Applying Lasso and ridge regression
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用Lasso和岭回归
- en: A common problem in machine learning is that an algorithm might work really
    well on the training set but, when applied to unseen data, it makes a lot of mistakes.
    You can see how this is problematic since, often, we are most interested in how
    a model generalizes to new data. Some algorithms (such as decision trees) are
    more susceptible to this phenomenon than others, but even linear regression can
    be affected.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中一个常见的问题是，一个算法可能在训练集上表现得非常好，但当应用到未见过的数据上时，会犯很多错误。你可以看到这有多麻烦，因为我们通常最感兴趣的是模型如何泛化到新数据。一些算法（如决策树）比其他算法更容易出现这种现象，但即使是线性回归也可能受到影响。
- en: This phenomenon is also known as **overfitting**, and we will talk about it
    extensively in [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml), *Using
    Decision Trees to Make a Medical Diagnosis*, and [Chapter 11](904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml), *Selecting
    the Right Model with Hyperparameter Tuning*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象也被称为**过拟合**，我们将在第5章[使用决策树进行医疗诊断](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml)和第11章[使用超参数调整选择合适的模型](904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml)中详细讨论。
- en: A common technique for reducing overfitting is called **regularization**, which
    involves ...
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合的常见技术被称为**正则化**，它涉及...
- en: Classifying iris species using logistic regression
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归对菊花物种进行分类
- en: 'Another famous dataset in the world of machine learning is called the Iris dataset.
    The Iris dataset contains measurements of 150 iris flowers from three different
    species: Setosa, Versicolor, and Viriginica. These measurements include the length
    and width of the petals and the length and width of the sepals, all measured in
    centimeters.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上机器学习领域另一个著名的数据集被称为菊花数据集。菊花数据集包含来自三种不同物种（Setosa、Versicolor和Viriginica）的150朵菊花测量值。这些测量值包括花瓣和萼片的长度和宽度，所有测量值都以厘米为单位。
- en: Our goal is to build a machine learning model that can learn the measurements
    of these iris flowers, the species of which are known, so that we can predict
    the species for a new iris flower.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个机器学习模型，该模型可以学习这些已知物种的菊花花径测量值，以便我们可以预测新菊花的物种。
- en: Understanding logistic regression
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解逻辑回归
- en: 'Before we start off with this section, let me issue warning—logistic regression,
    despite its name, is actually a model for classification, specifically when you
    have two classes. It derives its name from the  logistic function (or sigmoid)
    it uses to convert any real-valued input *x* into a predicted output value *ŷ* that takes values
    between **0** and **1**, as shown in the following diagram:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始本节之前，我要发出警告——尽管名为逻辑回归，但实际上它是一个分类模型，特别是当你有两个类别时。它得名于它使用的**逻辑函数**（或Sigmoid函数），它将任何实值输入*x*转换为介于**0**和**1**之间的预测输出值*ŷ*，如下面的图所示：
- en: '![](img/b9b54eb8-287c-4286-a68e-d5f9496af292.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9b54eb8-287c-4286-a68e-d5f9496af292.png)'
- en: Rounding *ŷ* to the nearest integer effectively classifies the input as belonging
    either to class **0** or **1**.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将*ŷ*四舍五入到最接近的整数实际上将输入分类为属于类别**0**或**1**。
- en: Of course, most often, our problems have more than one input or feature value, *x*.
    For example, the Iris dataset provides a total ...
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的大部分问题通常有多个输入或特征值，*x*。例如，菊花数据集提供了总共...
- en: Loading the training data
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载训练数据
- en: 'The Iris dataset is included with scikit-learn. We first load all of the necessary modules,
    as we did in our earlier examples:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 菊花数据集包含在scikit-learn中。我们首先加载所有必要的模块，就像我们之前的例子一样：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then loading the dataset is a one-liner:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载数据集只需一行代码：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This function returns a dictionary we call `iris`, which contains a bunch of
    different fields:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回一个我们称为`iris`的字典，其中包含许多不同的字段：
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here, all of the data points are contained in `''data''`. There are `150` data
    points, each of which has `4` feature values:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，所有数据点都包含在`'data'`中。有`150`个数据点，每个数据点都有`4`个特征值：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These four features correspond to the sepal and petal dimensions mentioned
    earlier:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个特征对应于之前提到的萼片和花瓣尺寸：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For every data point, we have a class label stored in `target`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，我们都有一个存储在`target`中的类别标签：
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also inspect the class labels and find that there is a total of three
    classes:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查类别标签，发现总共有三个类别：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Making it a binary classification problem
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将其转换为二分类问题
- en: 'For the sake of simplicity, we want to focus on a binary classification problem for
    now, where we only have two classes. The easiest way to do this is to discard
    all data points belonging to a certain class, such as class label 2, by selecting
    all of the rows that do not belong to class `2`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们目前只想关注一个二分类问题，其中我们只有两个类别。最简单的方法是丢弃属于特定类别（例如类别标签2）的所有数据点，通过选择不属于类别`2`的所有行来实现：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, let's inspect the data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查数据。
- en: Inspecting the data
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'Before you get started with setting up a model, it is always a good idea to have a
    look at the data. We did this earlier for the town map example, so let''s repeat
    it here too. Using Matplotlib, we create a **scatter plot** where the color of
    each data point corresponds to the class label:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始设置模型之前，总是先查看数据是一个好主意。我们之前在城镇地图示例中这样做过，所以这里也重复一下。使用Matplotlib，我们创建一个**散点图**，其中每个数据点的颜色对应于类别标签：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To make plotting easier, we limit ourselves to the first two features (`iris.feature_names[0]` being
    the sepal length and `iris.feature_names[1]` being the sepal width). We can see
    a nice separation of classes in the following diagram:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使绘图更简单，我们限制自己只使用前两个特征（`iris.feature_names[0]`代表花萼长度，`iris.feature_names[1]`代表花萼宽度）。我们可以在下面的图中看到类别的良好分离：
- en: '![](img/02b2daa3-8a42-4601-b8c5-e84f87ed0cdb.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02b2daa3-8a42-4601-b8c5-e84f87ed0cdb.png)'
- en: The preceding image shows the plot for the first two features of the Iris dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图显示了Iris数据集前两个特征的绘图。
- en: Splitting data into training and test sets
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: 'We learned, in the previous chapter, that it is essential to keep training and
    test data separate. We can easily split the data using one of scikit-learn''s
    many helper functions:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到保持训练数据和测试数据分离是至关重要的。我们可以使用scikit-learn的许多辅助函数之一轻松地分割数据：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here, we want to split the data into 90% training data and 10% test data, which
    we specify with `test_size=0.1`. By inspecting the return arguments, we note that
    we ended up with exactly `90` training data points and `10` test data points:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们希望将数据分为90%的训练数据和10%的测试数据，我们通过`test_size=0.1`来指定。通过检查返回的参数，我们注意到我们最终得到了恰好`90`个训练数据点和`10`个测试数据点：
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Training the classifier
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分类器
- en: 'Creating a logistic regression classifier involves pretty much the same steps
    as setting up k-NN:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 创建逻辑回归分类器涉及的过程几乎与设置k-NN相同：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then have to specify the desired training method. Here, we can choose `cv2.ml.LogisticRegression_BATCH` or `cv2.ml.LogisticRegression_MINI_BATCH`.
    For now, all we need to know is that we want to update the model after every data
    point, which can be achieved with the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们必须指定所需的训练方法。在这里，我们可以选择`cv2.ml.LogisticRegression_BATCH`或`cv2.ml.LogisticRegression_MINI_BATCH`。目前，我们只需要知道我们希望在每次数据点之后更新模型，这可以通过以下代码实现：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We also want to specify the number of iterations the algorithm should run before
    it terminates:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望指定算法在终止前应该运行的迭代次数：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can then call the `train` method of the object (in the exact same way as
    we did earlier), which will return `True` upon success:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用对象的`train`方法（与之前完全相同），在成功时将返回`True`：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we just saw, the goal of the training phase is to find a set of weights
    that best transform the feature values into an output label. A single data point
    is given by its four feature values (*f[0]*, *f[1]*, *f[2]*, and *f[3]*). Since
    we have four features, we should also get four weights, so that *x = w[0] f[0] +
    w[1] f[1] + w[2] f[2] + w[3] f[3]*, and *ŷ=σ(x)*. However, as discussed previously,
    the algorithm adds an extra weight that acts as an offset or bias, so that *x
    = w[0] f[0] + w[1] f[1] + w[2] f[2] + w[3] f[3] + w[4]*. We can retrieve these
    weights as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才看到的，训练阶段的目标是找到一组权重，这些权重可以将特征值最佳地转换为输出标签。一个数据点由其四个特征值（*f[0]*、*f[1]*、*f[2]*和*f[3]*）给出。由于我们有四个特征，我们也应该得到四个权重，以便*x
    = w[0] f[0] + w[1] f[1] + w[2] f[2] + w[3] f[3]*，并且*ŷ=σ(x)*。然而，正如之前讨论的，算法添加了一个额外的权重，它充当偏移或偏差，因此*x
    = w[0] f[0] + w[1] f[1] + w[2] f[2] + w[3] f[3] + w[4]*。我们可以如下检索这些权重：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This means that the input to the logistic function is *x = -0.0409 f[0] - 0.0191
    f[1] - 0.163 f[2] + 0.287 f[3] + 0.119*. Then, when we feed in a new data point
    (*f[0]*, *f[1]*, *f[2]*, *f[3]*) that belongs to class 1, the output *ŷ=σ(x)* should
    be close to 1\. But how well does that actually work?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着逻辑函数的输入是*x = -0.0409 f[0] - 0.0191 f[1] - 0.163 f[2] + 0.287 f[3] + 0.119*。然后，当我们输入一个属于类别1的新数据点（*f[0]*、*f[1]*、*f[2]*、*f[3]*）时，输出*ŷ=σ(x)*应该接近1。但这实际上效果如何呢？
- en: Testing the classifier
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试分类器
- en: 'Let''s see for ourselves by calculating the accuracy score on the training
    set:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们亲自计算训练集上的准确率分数来验证：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Perfect score! However, this only means that the model was able to perfectly **memorize** the
    training dataset. This does not mean that the model would be able to classify
    a new, unseen data point. For this, we need to check the test dataset:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 完美分数！然而，这仅仅意味着模型能够完美地**记忆**训练数据集。这并不意味着模型能够对新的、未见过的数据点进行分类。为此，我们需要检查测试数据集：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Luckily, we get another perfect score! Now we can be sure that the model we
    built is truly awesome.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们又得到了一个满分！现在我们可以确信，我们构建的模型确实是出色的。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered quite a lot of ground, didn't we?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们覆盖了相当多的内容，不是吗？
- en: In short, we learned a lot about different supervised learning algorithms, how
    to apply them to real datasets, and how to implement everything in OpenCV. We
    introduced classification algorithms such as k-NN and logistic regression and
    discussed how they could be used to predict labels as two or more discrete categories.
    We introduced various variants of linear regression (such as Lasso regression
    and ridge regression) and discussed how they could be used to predict continuous
    variables. Last but not least, we got acquainted with the Iris and Boston datasets,
    two classics in the history of machine learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们学习了关于不同监督学习算法的很多知识，如何将它们应用于真实数据集，以及如何在OpenCV中实现一切。我们介绍了分类算法，如k-NN和逻辑回归，并讨论了它们如何被用来预测两个或更多离散类别标签。我们还介绍了线性回归的各种变体（如Lasso回归和岭回归），并讨论了它们如何被用来预测连续变量。最后但同样重要的是，我们熟悉了机器学习历史上的两个经典数据集：Iris和Boston。
- en: In the following chapters, we will go into much greater depth within these topics
    and explore some more interesting examples of where these concepts can be useful.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地探讨这些主题，并探索一些这些概念可以发挥作用的更有趣的例子。
- en: But first, we need to talk about another essential topic in machine learning, feature
    engineering. Often, data does not come in nicely formatted datasets, and it is
    our responsibility to represent the data in a meaningful way. Therefore, the next
    chapter will talk about representing features and engineering data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要讨论机器学习中的另一个重要主题，即特征工程。通常，数据不会以整洁的格式出现，我们有责任以有意义的方式表示数据。因此，下一章将讨论如何表示特征和工程数据。
