- en: Clean Up Your Personal Twitter Timeline by Clustering Tweets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过聚类推文清理您的个人Twitter时间线
- en: 'Here''s a little bit of gossip for you: The original project for this title
    had to do with detecting foreign influence on US elections in social media. At
    about the same time, I was also applying for a visa to the United States, to give
    a series of talks. It later transpired that I hadn''t needed the visa after all;
    ESTA covered all the things I had wanted to do in the United States. But as I
    was preparing for the visa, an attorney gave me a very stern talking-to about
    writing a book on the politics of the United States. The general advice is this—if
    I don''t want trouble with US Customs and Border Patrol, I should not write or
    say anything on social media about American politics, and especially not write
    a chapter of a book on it. So, I had to hastily rewrite this chapter. The majority
    of methods used in this chapter can be used for the original purpose, but the
    content is a lot milder.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点点八卦：这个标题的原始项目是关于检测社交媒体上外国对美国选举的影响。大约在同一时间，我还在申请美国的签证，去发表一系列演讲。后来发现，我根本不需要签证；ESTA涵盖了我在美国想要做的所有事情。但在准备签证的过程中，一位律师严厉地告诫我不要写一本关于美国政治的书。一般的建议是——如果我不想与美国海关和边境巡逻队发生麻烦，我就不应该在社交媒体上写或说任何关于美国政治的事情，更不要说写一本书的章节。所以，我不得不匆忙重写这一章。这一章中使用的多数方法都可以用于原始目的，但内容要温和得多。
- en: I use Twitter a lot. I mainly tweet and read Twitter in my downtime. I follow
    many people who share similar interests, among other things, machine learning,
    artificial intelligence, Go, linguistics, and programming languages. These people
    not only share interests with me; they also share interests with one another.
    As such, sometimes, multiple people may be tweeting about the same topic.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我大量使用Twitter。我主要在空闲时间发推文和阅读Twitter。我关注了许多有相似兴趣的人，包括机器学习、人工智能、围棋、语言学和编程语言。这些人不仅与我分享兴趣，彼此之间也分享兴趣。因此，有时可能会有多个人就同一主题发推文。
- en: As may be obvious from the fact that I use Twitter a lot, I am a novelty junkie.
    I like new things. Multiple people tweeting about the same topic is nice if I
    am interested in the differing viewpoints, but I don't use Twitter like that.
    I use Twitter as a sort of summary of interesting topics. Events X, Y, and Z happened.
    It's good enough that I know they happened. For most topics, there is no benefit
    for me to go deep and learn what the finer points are, and 140 characters is not
    a lot of characters for nuance anyway. Therefore, a shallow overview is enough
    to keep my general knowledge abreast with the rest of the population.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从我大量使用Twitter这一事实可能显而易见，我是一个新鲜事物爱好者。我喜欢新鲜事物。如果我对不同的观点感兴趣，那么多人就同一主题发推文是件好事，但我不那样使用Twitter。我使用Twitter作为一种有趣话题的总结。X、Y、Z事件发生了。知道它们发生了就足够了。对于大多数话题，深入学习和了解细节对我没有好处，而且140个字符对于细微差别来说也不算多。因此，一个浅显的概述就足以让我的一般知识跟上其他人。
- en: Thus, when multiple people tweet about the same topic, that's repetition in
    my newsfeed. That's annoying. What if, instead of that, my feed could just be
    one instance of each topic?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当多个人就同一主题发推文时，在我的新闻源中就是重复。这很烦人。如果我的新闻源中每个主题只出现一次，那会怎么样呢？
- en: I think of my Twitter-reading habit as happening in sessions. Each session is
    typically five minutes. I really only read about 100 tweets each session. If out
    of 100 tweets I read, 30% of the people I follow overlap on topics, then I really
    only have read 30 tweets of real content. That's not efficient at all! Efficiency
    means being able to cover more topics per session.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我的Twitter阅读习惯是分批进行的。每次会话通常是五分钟。我实际上每次会话只阅读大约100条推文。如果在我阅读的100条推文中，我关注的30%的人重叠在某个话题上，那么实际上我只阅读了30条真实内容的推文。这根本就不高效！效率意味着每次会话能够覆盖更多的话题。
- en: So, how do you increase efficiency in reading tweets? Well, remove the tweets
    that cover the same topic of course! There is the secondary matter of choosing
    the best tweet that summarizes the topic, but that's a subject for another day.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何提高阅读推文的效率呢？当然，移除覆盖相同主题的推文！还有选择最好的一条总结该主题的推文的问题，但这将是另一天的主题。
- en: The project
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: What we're going to do is to cluster tweets on Twitter. We will be using two
    different clustering techniques, K-means and DBSCAN. For this chapter, we're going
    to rely on some skills we built up in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml),
    *Linear Regression – House Price Prediction*. We will also be using the same libraries
    used in Chapter 2, *Linear Regression – House Price Prediction*. On top of that,
    we will also be using the clusters library by mpraski.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的就是在Twitter上对推文进行聚类。我们将使用两种不同的聚类技术，K-means和DBSCAN。对于本章，我们将依赖我们在[第二章](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml)中构建的一些技能，*线性回归
    – 房价预测*。我们还将使用第二章中使用的相同库，*线性回归 – 房价预测*。除此之外，我们还将使用mpraski的聚类库。
- en: By the end of the project, we will be able to clean up any collection of tweets
    from Twitter, and cluster them into groups. The main body of code that fulfills
    the objective is very simple, it's only about 150 lines of code in total. The
    rest of the code is for fetching and preprocessing data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到项目结束时，我们将能够清理Twitter上的任何推文集合，并将它们聚类成组。实现目标的主体代码非常简单，总共只有大约150行代码。其余的代码用于获取和预处理数据。
- en: K-means
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: '**K-means** is a method of clustering data. The problem is posed as this—given
    a dataset of N items, we wish to partition the data into K groups. How do you
    do so?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means**是一种数据聚类的方法。问题可以这样提出——给定一个包含N个项目的数据集，我们希望将数据划分为K组。你该如何做呢？'
- en: Allow me to take a side bar and explore the wonderful world of coordinates.
    No, no, don't run! It's very visual.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我稍微偏离一下主题，探索一下坐标的奇妙世界。不，不，别跑！这非常直观。
- en: '![](img/2b3bd494-5b0d-41b0-a81e-0401099e2b92.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b3bd494-5b0d-41b0-a81e-0401099e2b92.png)'
- en: Which line is longer? How do you know?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 哪条线更长？你怎么知道？
- en: '![](img/7b12adde-1086-432d-bbcc-5ad5ccd91311.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7b12adde-1086-432d-bbcc-5ad5ccd91311.png)'
- en: 'You know which line is longer because you can measure each line from points
    a, b, c, and d. Now, let''s try something different:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道哪条线更长，因为你可以从点a、b、c和d测量每条线。现在，让我们尝试一些不同的事情：
- en: '![](img/2c45dafa-dfaa-4e0b-b663-7766b5ef57ec.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c45dafa-dfaa-4e0b-b663-7766b5ef57ec.png)'
- en: Which dot is closest to X? How do you know?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个点离X最近？你怎么知道？
- en: 'You know because again, you can measure the distance between the dots. And
    now, for our final exercise:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，因为你可以测量点之间的距离。现在，让我们进行最后的练习：
- en: '![](img/6035e6fc-1038-4cbe-9bfa-15c56ba47388.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6035e6fc-1038-4cbe-9bfa-15c56ba47388.png)'
- en: 'Consider the distance between the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下距离：
- en: '**A** and **X**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **X**'
- en: '**A** and **Y**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **Y**'
- en: '**A** and **Z**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **Z**'
- en: '**B** and **X**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **X**'
- en: '**B** and **Y**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **Y**'
- en: '**B** and **Z**'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **Z**'
- en: '**C** and **X**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **X**'
- en: '**C** and **Y**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **Y**'
- en: '**C** and **Z**'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **Z**'
- en: What is the average distance between **A** and **X**, **B** and **X**, and **C**
    and **X**? What is the average distance between **A** and **Y**, **B** and **Y**
    and **C** and **Y**? What is the average distance between **A** and **Z**, **B**
    and **Z**, and **C** and **Z**?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**A** 和 **X**、**B** 和 **X**、**C** 和 **X**之间的平均距离是多少？**A** 和 **Y**、**B** 和 **Y**、**C**
    和 **Y**之间的平均距离是多少？**A** 和 **Z**、**B** 和 **Z**、**C** 和 **Z**之间的平均距离是多少？'
- en: If you had to choose one point between **X**, **Y**, and **Z** to represent
    the **A**, **B**, and **C**, which would you choose?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须在**X**、**Y**和**Z**之间选择一个点来代表**A**、**B**和**C**，你会选择哪一个？
- en: Congratulations! You just did a very simple and abbreviated version of K-means
    clustering. Specifically, you did a variant where *k = 1*. If you had to pick
    two points between **X**, **Y**, and **Z**, then that's *k = 2*. A cluster is
    therefore the set of points that make it such that the average distance of the
    group is minimal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚完成了一个非常简单和简化的K-means聚类。具体来说，你做的是一个变体，其中*k = 1*。如果你必须在**X**、**Y**和**Z**之间选择两个点，那么那将是*k
    = 2*。因此，聚类是由使组内平均距离最小化的点集组成的。
- en: That's a mouthful, but think back to what you just did. Now, instead of just
    three points, **A**, **B**, and **C**, you have many points. And you aren't given
    **X**, **Y**, or **Z**; you'd have to generate your own **X**, **Y**, and **Z**
    points. Then, you have to find the groups that minimize the distance to each possible
    points of **X**, **Y**, and **Z**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很复杂，但回想一下你刚才做了什么。现在，你不再只有三个点**A**、**B**和**C**，而是有很多点。你没有给出**X**、**Y**或**Z**；你必须生成自己的**X**、**Y**和**Z**点。然后，你必须找到使每个可能的**X**、**Y**和**Z**点距离最小的组。
- en: That is, in a nutshell, K-means. It's easy to understand, but hard to implement
    it well. It turns out K-means is NP-hard; it may not be solved in polynomial time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 概而言之，这就是K-means。它容易理解，但难以良好实现。结果证明K-means是NP难的；它可能无法在多项式时间内解决。
- en: DBSCAN
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN
- en: '**DBSCAN** inherits the idea that data can be represented as multidimensional
    points. Again, sticking with a two-dimensional example, this is in rough steps
    how DBSCAN works:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**DBSCAN**继承了数据可以表示为多维点的想法。再次，以二维为例，以下是DBSCAN大致的工作步骤：'
- en: Pick a point that has not been visited before.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个尚未访问的点。
- en: Draw a circle with the point as the center. The radius of the circle is epsilon.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以点为中心画一个圆。圆的半径是epsilon。
- en: Count how many other points fall into the circle. If there are more than a specified
    threshold, we mark all the points as being part of the same cluster.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算有多少其他点落入圆内。如果有超过指定的阈值，我们将所有点标记为属于同一个簇。
- en: Recursively do the same for each point in this cluster. Doing so expands the
    cluster.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这个簇中的每个点递归地执行相同的操作。这样做会扩大簇。
- en: Repeat these steps.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤。
- en: I highly encourage you to do this on dotted paper and try to draw this out yourself.
    Start by plotting random points, and use pencils to draw circles on paper. This
    will give you an intuition of how DBSCAN works. The picture shows my working that
    enhanced my intuition about how DBSCAN works. I found this intuition to be very
    useful.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你们在点状纸上尝试自己画出这个图。首先，绘制随机点，然后用铅笔在纸上画圆。这将给你们一个关于DBSCAN如何工作的直观感受。图片显示了我增强对DBSCAN工作原理直觉的工作。我发现这种直觉非常有用。
- en: Data acquisition
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据获取
- en: In the earlier exercises, I asked you to look at the dots and figure out the
    distance. This gives a hint as to how we need to think of our data. We need to
    think of our data as coordinates in some imaginary coordinate space. Now, our
    data won't be just two-dimensional, because it's textual. Instead, it'll be multidimensional.
    This gives us hints as to how our data will look—slices of numbers representing
    a coordinate in some arbitrarily large N-dimensional space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的练习中，我要求你们观察点并计算出距离。这为我们如何思考数据提供了一些线索。我们需要将数据视为某个想象中的坐标系中的坐标。现在，我们的数据不会仅仅是二维的，因为它是文本的。相反，它将是多维的。这为我们提供了关于数据外观的线索——代表某个任意大的N维空间中坐标的数字切片。
- en: But, first, we'll need to get the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，首先，我们需要获取数据。
- en: To acquire the tweets from the feed, we'll be using Aditya Mukherjee's excellent
    Anaconda library. To install it, simply run `go get -u github.com/ChimeraCoder/Anaconda`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取推文，我们将使用Aditya Mukherjee的出色Anaconda库。要安装它，只需运行`go get -u github.com/ChimeraCoder/Anaconda`。
- en: 'Of course, one can''t just grab data from Twitter willy-nilly. We will need
    to acquire data via the Twitter API. The documentation of Twitter''s API is the
    best source to get started: [https://developer.twitter.com/en/docs/basics/getting-started](https://developer.twitter.com/en/docs/basics/getting-started).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，不能随意从Twitter抓取数据。我们需要通过Twitter API获取数据。Twitter API的文档是开始的好资源：[https://developer.twitter.com/en/docs/basics/getting-started](https://developer.twitter.com/en/docs/basics/getting-started)。
- en: 'You will need to first apply for a Twitter developer account (if you don''t
    already have it): [https://developer.twitter.com/en/apply/user](https://developer.twitter.com/en/apply/user).
    The process is rather lengthy and requires human approval for a developer account.
    Despite this, you don''t need developer access to develop this project. I thought
    I had access to Twitter''s API when I started, but it turns out I didn''t. The
    good news is, the Twitter API documentation page does provide enough examples
    to get started with developing the necessary data structures.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你们需要首先申请一个Twitter开发者账户（如果你们还没有的话）：[https://developer.twitter.com/en/apply/user](https://developer.twitter.com/en/apply/user)。这个过程相当漫长，需要人工批准开发者账户。尽管如此，你们不需要开发者访问权限来开发这个项目。我开始时以为我有访问Twitter
    API的权限，但结果证明我没有。好消息是，Twitter API文档页面提供了足够的示例，可以帮助你们开始开发必要的数据结构。
- en: 'The specific end point that we''re interested in is this: [https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的具体终点是：[https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html)。
- en: Exploratory data analysis
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'Let''s look at the `JSON` acquired from the Twitter API endpoint. A single
    tweet looks something like this (from the Twitter API documentation example):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从Twitter API端点获取的`JSON`。单个推文看起来可能像这样（来自Twitter API文档示例）：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will be representing each individual tweet in a data structure that looks
    like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用类似这样的数据结构来表示每个单独的推文：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that we embed `anaconda.Tweet`, which is given as such in the Anaconda
    package:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们嵌入`anaconda.Tweet`，这在Anaconda包中是这样给出的：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the interest of building the program, we''ll use the example tweets supplied
    by Twitter. I saved the example responses into a file called `example.json` and
    then a `mock` function is created to mock calling the API:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建程序，我们将使用Twitter提供的示例推文。我将示例响应保存到一个名为`example.json`的文件中，然后创建了一个`mock`函数来模拟调用API：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The utility function `dieIfErr` is defined as usual:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实用函数`dieIfErr`被定义为通常：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that in `mock`, no API calls to Twitter were made. In the future, we will
    be creating a function with a similar API so we can just replace the mock version
    of this function with the real one, which acquires the timeline from the API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`mock`中，没有对Twitter进行API调用。将来，我们将创建一个具有类似API的函数，这样我们就可以用真实的版本替换这个函数的模拟版本，从API获取时间线。
- en: 'For now, we can test that this works by the following program:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们可以通过以下程序测试它是否工作：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the output I got:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的输出结果：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data massage
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理
- en: When we tested that the data structure made sense, we printed the `FullText`
    field. We wish to cluster based on the content of the tweet. What matters to us
    is that content. This can be found in the `FullText` field of the struct. Later
    on in the chapter, we will see how we may use the metadata of the tweets, such
    as location, to help cluster the tweets better.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们测试数据结构是否合理时，我们打印了`FullText`字段。我们希望根据推文的内文进行聚类。对我们来说，重要的是内容。这可以在结构的`FullText`字段中找到。在章节的后面，我们将看到我们如何可能使用推文的元数据，例如位置，来帮助更好地聚类推文。
- en: 'As mentioned in the previous sections, each individual tweet needs to be represented
    as a coordinate in some higher-dimensional space. Thus, our goal is to take all
    the tweets in a timeline and preprocess them in such a way that we can get this
    output table:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所述，每个单独的推文都需要在某个高维空间中表示为一个坐标。因此，我们的目标是获取时间线中的所有推文，并预处理它们，以便我们得到以下输出表：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each row in the table represents a tweet, indexed by the tweet ID. The columns
    that follow are words that exist in the tweet, indexed by its header. So, in the
    first row, `test` appears in the tweet, while `twitter`, `right`, and `wrong`
    do not. The slice of numbers `[0 1 0 0]` in the first row is the input we require
    for the clustering algorithms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的每一行代表一个推文，通过推文ID进行索引。接下来的列是推文中存在的单词，通过其标题进行索引。因此，在第一行中，`test`出现在推文中，而`twitter`、`right`和`wrong`没有出现。第一行中的数字切片`[0
    1 0 0]`是我们对聚类算法所需的输入。
- en: Of course, binary numbers indicating the presence of a word in a tweet isn't
    the best. It'd be more interesting if the relative importance of the word is used
    instead. Again, we turn to the familiar TF-IDF, first introduced in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml),
    *Linear Regression – House Price Prediction*, for this. More advanced techniques
    such as using word embeddings exist. But you'd be surprised how well something
    as simple as TF-IDF can perform.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，表示推文中单词存在的二进制数字并不是最好的。如果使用单词的相对重要性会更有趣。再次，我们转向熟悉的TF-IDF，它首次在[第2章](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml)中介绍，*线性回归
    – 房价预测*。更高级的技术，如使用词嵌入，也存在。但你会惊讶于像TF-IDF这样简单的东西可以表现得有多好。
- en: By now, the process should be familiar—we want to represent the text as a slice
    of numbers, not as a slice of bytes. In order to do so, we would have to require
    some sort of dictionary to convert the words in the text into IDs. From there,
    we can built the table.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，这个过程应该很熟悉了——我们希望将文本表示为数字切片，而不是字节切片。为了做到这一点，我们需要某种类型的字典来将文本中的单词转换为ID。从那里，我们可以构建表格。
- en: Again, like in [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression – House Price* Prediction, we shall approach this with a simple tokenization
    strategy. More advanced tokenizers are nice, but not necessary for our purpose.
    Instead, we'll rely on good old `strings.Field`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，就像在 [第2章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)
    中，*线性回归 – 房价预测*，我们将采用简单的标记化策略。更高级的标记化器很棒，但对我们来说不是必需的。相反，我们将依赖古老的 `strings.Field`。
- en: The processor
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理器
- en: 'Having laid out our requirements, we can combine them into a single data structure
    that contains the things we need. Here''s how the processor data structure looks:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们的需求后，我们可以将它们组合成一个包含所需内容的单一数据结构。以下是处理器数据结构的外观：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For now, ignore the `locations` field. We shall look into how metadata might
    be useful in clustering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，忽略 `locations` 字段。我们将研究元数据在聚类中的用途。
- en: 'To create a new `processor`, the following function is defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的 `processor`，定义了以下函数：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we see some interesting decisions. The corpus is constructed with a number
    of special strings—`mention`, `hashtag`, `retweet`, and `url.` These are defined
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一些有趣的决策。语料库是用一些特殊字符串构建的——`mention`、`hashtag`、`retweet` 和 `url`。这些定义如下：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Some of the designs of this is for historical reasons. A long time ago, before
    Twitter supported retweets as an action, people manually retweeted tweets by prepending
    `RT` on to tweets. If we are to analyze data far into the past (which we won't
    for this chapter), then we'd have to be aware of the historical designs of Twitter
    as well. So, you must design for that.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分设计的历史原因。很久以前，在Twitter支持转发作为动作之前，人们通过在推文前加上 `RT` 来手动转发推文。如果我们必须分析很久以前的数据（我们不会在本章中这样做），那么我们必须了解Twitter的历史设计。因此，你必须为此进行设计。
- en: But having constructed a corpus with special keywords implies something. It
    implies that when converting the text of a tweet into a bunch of IDs and numbers,
    mentions, hashtags, retweets, and URLs are all treated as the same. It implies
    we don't really want to care what the URL is, or who is mentioned. However, when
    it comes to hashtags, that's the interesting case.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，构建包含特殊关键词的语料库意味着某些事情。它意味着在将推文的文本转换为一系列ID和数字、提及、hashtag、转发和URL时，它们都被视为相同的。它意味着我们并不真正关心URL是什么，或者谁被提及。然而，当涉及到hashtag时，这是一个有趣的情况。
- en: A hashtag is typically used to denote the topic of the tweet. Think `#MeToo` or
    `#TimesUp`. A hashtag contains information. Compressing all hashtags into one
    single ID may not be useful. This is a point to note when we experiment later
    on.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 常用hashtag来表示推文的主题。例如 `#MeToo` 或 `#TimesUp`。hashtag包含信息。将所有hashtag压缩成一个单一的ID可能没有用。这是我们稍后实验时需要注意的一个点。
- en: 'Having said all that, here''s how a list of `*processedTweet` is processed.
    We will be revisiting and revising the function as the chapter goes on:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，以下是处理 `*processedTweet` 列表的方法。随着章节的进行，我们将重新访问和修改这个函数：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's go through this function line by line.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这个函数。
- en: We start by ranging over all the `*processedTweets`. `a` is `[]*processedTweet`
    for a good reason—we want to modify the structure as we go along. If `a` were
    `[]processedTweet`, then we would have to either allocate a lot more, or have
    complicated modification schemes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先遍历所有的 `*processedTweets`。`a` 是 `[]*processedTweet` 的原因——我们希望在过程中修改结构。如果
    `a` 是 `[]processedTweet`，那么我们就必须分配更多的空间，或者有复杂的修改方案。
- en: 'Each tweet is comprised of its `FullText`. We want to extract each word from
    the text, and then give each word its own ID. To do that, this is the loop:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每条推文由其 `FullText` 组成。我们想要从文本中提取每个单词，然后为每个单词分配一个ID。为此，这是循环：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Preprocessing a single word
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理单个单词
- en: 'The `p.single` processes a single word. It returns the ID of the word, and
    whether to add it to the list of words that make up the tweet. It is defined as
    follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`p.single` 处理单个单词。它返回单词的ID，以及是否将其添加到构成推文的单词列表中。它定义如下：'
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We start by making the word lowercase. This makes words such as `café` and `Café`
    equivalent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将单词转换为小写。这使得像 `café` 和 `Café` 这样的单词等效。
- en: Speaking of `café`, what would happen if there are two tweets mentioning a `café`,
    but one user writes `café` and the other writes cafe? Assume, of course, they
    both refer to the same thing. We'd need some form of normalization to tell us
    that they're the same.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 说到`café`，如果有两条推文都提到了`café`，但一个用户写成`café`，另一个用户写成cafe？当然，假设他们都指的是同一件事。我们需要某种归一化形式来告诉我们它们是相同的。
- en: Normalizing a string
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串归一化
- en: First, the word is to be normalized into `NFKC` form. In [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression–House Price Prediction*, this was introduced, but I then mentioned
    that LingSpam basically provides normalized datasets. In real-world data, which
    Twitter is, data is often dirty. Hence, we need to be able to compare them on
    an apples-to-apples basis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，单词需要被归一化为`NFKC`形式。在[第2章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)，*线性回归-房价预测*中，这被介绍过，但我随后提到LingSpam基本上提供了归一化数据集。在现实世界的数据中，比如Twitter，数据通常是杂乱的。因此，我们需要能够以苹果对苹果的方式比较它们。
- en: 'To show this, let''s write a side program:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一点，让我们写一个辅助程序：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first thing to note is that there are at least three ways of writing the
    word `café`, which for the purposes of this demonstration means coffee shop. It's
    clear from the first two comparisons that the words are not the same. But since
    they mean the same thing, a comparison should return `true`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，至少有三种方式可以写出单词`café`，在这个演示中意味着咖啡馆。从前两个比较中很明显，这两个单词是不相同的。但既然它们意味着相同的事情，比较应该返回`true`。
- en: 'To do that, we will need to transform all the text to one form, and then comapare
    it. To do so, we would need to define a transformer:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要将所有文本转换成一种形式，然后进行比较。为此，我们需要定义一个转换器：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This transformer is a chain of text transformers, applied one after another.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器是一系列文本转换器的链，一个接一个地应用。
- en: First, we convert all the text to its decomposing form, NFD. This would turn
    `café` into `cafe\u0301`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将所有文本转换为它的分解形式，NFD。这将`café`转换为`cafe\u0301`。
- en: 'Then, we remove any non-spacing mark. This turns `cafe\u0301` into `cafe`.
    This removal function is done with the `isMn` function, defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们移除任何非间隔符号。这会将`cafe\u0301`转换为`cafe`。这个移除函数是通过`isMn`函数完成的，定义如下：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Lastly, convert everything to NKFC form for maximum compatibility and space
    saving. All three strings are now equal.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将所有内容转换为NKFC形式以实现最大兼容性和节省空间。现在这三个字符串都是相等的。
- en: 'Note that this type of comparison is done with one single assumption that belies
    it all: there is one language that we''re doing our comparisons in—English. **Café**
    in French means **coffee** as well as **coffee shop**. This kind of normalization,
    where we remove diacritical marks, works so long as removing a diacritic mark
    does not change the meaning of the word. We''d have to be more careful around
    normalization when dealing with multiple languages. But for this project, this
    is a good enough assumption.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种比较是基于一个单一的假设：我们正在进行比较的语言是英语。法语中的**Café**意味着**咖啡**以及**咖啡馆**。这种去除重音符号的归一化，只要去除重音符号不会改变单词的意义，就可以工作。在处理多种语言时，我们需要在归一化方面更加小心。但在这个项目中，这是一个足够好的假设。
- en: 'With this new knowledge, we will need to update our `processor` type:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些新知识，我们需要更新我们的`processor`类型：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The first line of our `p.single` function would have to change too, from this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们`p.single`函数的第一行也需要改变，从以下内容变为：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It will change to this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它将变成这样：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you're feeling extra hard-working, try making `strings.ToLower` a `transform.Transformer`.
    It is harder than you might expect, but not as hard as it appears.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感觉特别勤奋，尝试将`strings.ToLower`转换为`transform.Transformer`。这比你想象的要难，但也没有你想象的那么难。
- en: Preprocessing stopwords
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理停用词
- en: Enough about normalization. We now turn our focus to `stopwords`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于归一化就说到这里。我们现在将注意力转向`stopwords`。
- en: Recall from [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression–House Price Prediction*, that `stopwords` are words such as **the**,
    **there**, **from**, and so on. They're connective words, useful in understanding
    the specific context of sentences, but for a naive statistical analysis, they
    often add nothing more than noise. So, we have to remove them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第二章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)，*线性回归-房价预测*，停用词是一些像**the**、**there**、**from**这样的词。它们是连接词，有助于理解句子的特定上下文，但对于简单的统计分析来说，它们通常只会增加噪音。因此，我们必须移除它们。
- en: 'A check for `stopwords` is simple. If a word matches a `stopwords`, we''ll
    return `false` for whether to add the word ID into the sentence:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对停用词的检查很简单。如果一个词匹配`stopwords`，我们将返回`false`以确定是否将词ID添加到句子中：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Where does the list of `stopwords` come from? It''s simple enough that I just
    wrote this in `stopwords.go`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词列表从哪里来？这很简单，我就在`stopwords.go`中写了这个：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And that's it! A tweet with content that looks like this—*an apple a day keeps
    the doctor away* would have the IDs for *apple*, *day*, *doctor*, and *away*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！内容看起来像这样的推文——*一天一个苹果，医生远离我*——会有*apple*、*day*、*doctor*和*away*的ID。
- en: The list of stopwords is adapted from the list that is used in the `lingo` package.
    The list of stopwords in the `lingo` package is meant to be used on lemmatized
    words. Because we're not lemmatizing, some words were manually added. It's not
    perfect but works well enough for our purpose.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词列表是从`lingo`包中使用的列表改编而来的。`lingo`包中的停用词列表是用来在词干化的单词上使用的。因为我们没有进行词干化，所以一些词是手动添加的。它并不完美，但足够满足我们的目的。
- en: Preprocessing Twitter entities
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理推特实体
- en: 'After we''ve removed the stopwords, it''s time to process the special Twitter
    entities:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们移除了停用词之后，就到了处理特殊的推特实体的时候了：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These are straightforwards enough.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都很直接。
- en: If a word begins with `"#"`, then it's a hashtag. We might want to come back
    to this later, so it's good to keep this in mind.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词以`"#"`开头，那么它是一个标签。我们可能稍后会回到这个话题，所以记住这一点是好的。
- en: Any word that begins with a `"@"` is a mention. This is a little tricky. Sometimes,
    people tweet things such as `I am @PlaceName`, indicating a location, as opposed
    to mentioning a user (indeed, one may find `@PlaceName` does not exist). Or, alternatively,
    people may tweet something such as `I am @ PlaceName`. In this case, the solo `"@"` would
    still be treated as a mention. I found that for the former (`@PlaceName`), it
    doesn't really matter if the word is treated as a mention. Twitter's API does
    indeed return a list of mentions that you may check against. But for my personal
    timeline, this was extra work that isn't necessary. So, think of this as an extra
    credit project—check against the list of mentions from the API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 任何以`"@"`开头的词都是提及。这有点棘手。有时，人们会发推文说诸如`I am @PlaceName`这样的话，表示一个地点，而不是提及一个用户（实际上，可能会发现`@PlaceName`并不存在）。或者，人们可能会发推文说`I
    am @ PlaceName`。在这种情况下，单独的`"@"`仍然会被视为提及。我发现对于前者（`@PlaceName`），将这个词视为提及并没有太大的关系。Twitter的API确实会返回一个提及列表，你可以对其进行检查。但对我来说，这只是一个不必要的额外工作。所以，把这当作一个加分项目——检查API返回的提及列表。
- en: Of course, we shan't be as lazy as to leave everything to extra credit; simple
    checks can be made—if `@` is solo, then we shouldn't treat it as a mention. It
    should be treated as `at`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不应该那么懒惰，把所有事情都留给加分项目；可以做一些简单的检查——如果`@`是单独的，那么我们不应该将其视为提及。它应该被视为`at`。
- en: Now, we check for URLs. The line `if strings.HasPrefix(word, "http://")`checks
    for a `http://` prefix. This isn't good. This doesn't account for URLs with a
    `https` scheme.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们检查URL。行`if strings.HasPrefix(word, "http://")`检查`http://`前缀。这并不好。这没有考虑到使用`https`方案的URL。
- en: 'Now we know how to modify this section of the code. It looks like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何修改这段代码。它看起来是这样的：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Lastly, a final line of code is added to handle historical tweets before retweets
    were supported by Twitter:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加了一行代码来处理在Twitter支持转发之前的历史推文：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Processing a single tweet
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理单个推文
- en: 'Consider the following snippet of code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码片段：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What it says is after we've preprocessed every single word, we simply add that
    word to the TFIDF.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它的意思是在我们预处理了每个单词之后，我们只需简单地将该单词添加到TFIDF中。
- en: Clustering
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: The purpose of this project is to clean up the amount of tweets that I have
    to read. If there is a reading budget of 100 tweets, I don't want to be reading
    50 tweets on the same topic; they may well represent different viewpoints, but
    in general for skimming purposes, are not relevant to my interests. Clustering
    provides a good solution to this problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的目的是清理我需要阅读的推文数量。如果有100条推文的阅读预算，我不想阅读50条同一主题的推文；它们可能代表不同的观点，但一般来说，对于浏览目的，它们与我感兴趣的不相关。聚类为这个问题提供了一个很好的解决方案。
- en: First, if the tweets are clustered, the 50 tweets on the same topic will be
    grouped in the same cluster. This allows me to dig in deeper if I wish. Otherwise,
    I can just skip those tweets and move on.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果推文被聚类，同一主题的50条推文将被分组在同一个聚类中。这样，如果我想深入研究，我可以这样做。否则，我可以跳过这些推文并继续。
- en: In this project, we wish to use K-means. To do so, we'll use Marcin Praski's
    `clusters` library. To install it, simply run `go get -u github.com/mpraski/clusters`.
    It's a good library, and it comes built in with multiple clustering algorithms.
    I introduced K-means before, but we're also going to be using DBSCAN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们希望使用K-means。为此，我们将使用Marcin Praski的`clusters`库。要安装它，只需运行`go get -u github.com/mpraski/clusters`。这是一个好的库，它内置了多个聚类算法。我之前介绍了K-means，但我们还将使用DBSCAN。
- en: Last, we're going to be using the DMMClust algorithm to compare against. The
    DMMClust algorithm is in a different library. To install it, simply run `go get
    -u github.com/go-nlp/dmmclust`. The purpose of DMMClust is to cluster small texts
    using an innovative process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用DMMClust算法进行比较。DMMClust算法位于不同的库中。要安装它，只需运行`go get -u github.com/go-nlp/dmmclust`。DMMClust的目的使用创新的过程对小型文本进行聚类。
- en: Clustering with K-means
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means聚类
- en: 'As a recap, here''s what we did so far—we processed each tweet in a list of
    tweets from the home timeline to be a slice of `float64`. These represent the
    coordinates in the higher-dimensional space. Now, all we need to do is the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，到目前为止我们做了什么——我们将来自主页时间线的推文列表中的每条推文处理成`float64`的切片。这些代表高维空间中的坐标。现在，我们只需要做以下事情：
- en: Create a clusterer.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个聚类器。
- en: Create a `[][]float64` representing all the tweets from the timeline.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`[][]float64`来表示时间线上的所有推文。
- en: Train the clusterer.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练聚类器。
- en: Predict which tweet belongs in which cluster.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测每条推文属于哪个聚类。
- en: 'It can be done as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样做：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Surprised? Let's break it down.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 惊讶吗？让我们来分析一下。
- en: 'The first few lines are for processing `tweets`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行是用于处理`tweets`的：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then create a clusterer:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个聚类器：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, we say we want a K-means clusterer. We'll train on the data 10,000 times,
    and we want it to find 25 clusters, using the `EuclideanDistance` method to calculate
    distances. The Euclidean distance is your bog standard distance calculation, the
    same one you'd use to calculate the distance between two points in the exercises
    in the K-means section before. There are other methods of calculating distances,
    which are more suited for textual data. Later in this chapter, I'll show you how
    to create a distance function, the Jacard distance, which is much better than
    Euclidean distance when used on text.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们说我们想要一个K-means聚类器。我们将对数据进行10,000次训练，并希望找到25个聚类，使用`EuclideanDistance`方法计算距离。欧几里得距离是标准的距离计算方法，与你在K-means部分之前的练习中计算两点之间距离的方法相同。还有其他计算距离的方法，它们更适合文本数据。在本章的后面部分，我将向你展示如何创建一个距离函数，即Jaccard距离，当用于文本时，它比欧几里得距离要好得多。
- en: 'After we''ve created a clusterer, we need to convert our list of `tweets` into
    a matrix. We then train the clusterer:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建聚类器之后，我们需要将我们的`tweets`列表转换为矩阵。然后我们训练聚类器：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And, finally, we display the `clusters`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示`clusters`：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Clustering with DBSCAN
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN聚类
- en: 'Clustering with DBSCAN using Marcin''s package is equally simple. In fact,
    you would just need to change one single line of code from this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Marcin的包进行DBSCAN聚类同样简单。实际上，你只需要更改一行代码，如下所示：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You would change it to this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将其改为这样：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, of course, the question is what values should `eps` and `minPts` be?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在的问题是`eps`和`minPts`应该取什么值？
- en: '`eps` represents the minimum distance required for two points to be considered
    a neighbor. `minPts` is the minimum number of points to form a dense cluster.
    Let''s address `eps` first.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps`代表两个点被认为是邻居所需的最小距离。`minPts`是形成密集聚类的最小点数。让我们先讨论`eps`。'
- en: How do we know what the best distance is? A good way to figure this out is usually
    to visualize the data. In fact, this is what the original inventors of the DBSCAN
    algorithm suggests. But what exactly are we to visualize?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to visualize the distance between the tweets. Given a dataset, we can
    compute a distance matrix that looks something like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To do so, we write the following function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This function takes a matrix of floats; each row represents a tweet, and finds
    the top k-nearest neighbors. Let's walk through the algorithm. As we walk though
    the algorithm, bear in mind that each row is a tweet; you can think of each row
    therefore as a very complicated coordinate.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we want to do is to find the distance between a tweet and another
    tweet, hence the following block:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Of particular note are the two expressions `for _, row := range a` and `for
    _, row2 := range a`. In a normal KNN function, you'd have two matrices, `a` and
    `b`, and you'd find the distance between a tweet in `a` and a tweet in `b`. But
    for the purposes of drawing this chart, we are going to compare tweets within
    the same dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we acquired all the distances, we want to find the closest neighbors,
    so we sort the list and then put them in the distance matrix:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This, in a very quick way, is how to do K-nearest neighbors. Of course, it's
    not the most efficient. The algorithm I've shown here is *O(n^2)*. There are better
    ways of doing things, but for the purpose of this project, this suffices.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we grab the last column of the matrix and sort the last column.
    This is what we wish to plot. The plotting code is not unlike that seen in previous
    chapters. I shall provide it here with no further elaboration on how to use it:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'When I plot the real Twitter data to figure out the ideal `eps`, I get the
    following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee722fa9-fd28-45b5-9f84-2f49e9dc4954.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: What you want to find is an `elbow` or `knee` in the picture. Unfortunately,
    as you can tell, there are many of them. This is going to make clustering with
    the DBSCAN algorithm difficult. What this means is that the data is rather noisy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: One of the things that is of particular importance is the distance function
    used. I will go into this a little further in following sections on tweaking the
    program.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with DMMClust
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having been somewhat discouraged by the distance plot of my Twitter home feed,
    I looked into another way of clustering tweets. To that end, I used the `dmmclust`
    library (of which I am the primary author). The purpose of the DMMClust algorithm
    is that it is able to handle small texts quite well. Indeed, it was written to
    handle the problem of having small text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is a small text? Most text clustering research out there is done
    on texts with large amounts of words. Twitter, up to very recently, only supported
    140 characters. As you may imagine, the amount of information that 140 characters
    to be transmitted as human language is not very much.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The DMMClust algorithm works very much like students joining high school social
    clubs. Imagine the tweets as a bunch of students. Each student randomly joins
    a social club. Within each social club, they may like their fellow members of
    the club, or they may not. If they do not like the people in the group, they are
    allowed to change social clubs. This happens until all the clubs have people who
    like each other the most, or until the amount of iterations runs out.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: This, in a nutshell, is how the DMMClust algorithm works.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Real data
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we've been working on an example `JSON` that the Twitter documentation
    provides. I assume by now you have your Twitter API access. So, let's get real
    Twitter data!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'To get your API keys from the developer portal, click on the Get Started link.
    You will come to a page such as this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba823fed-8ca0-4c68-94b9-4dceb42f62da.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Select Create an app. You will be brought to a page that looks like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bfe0179-c124-46df-b8f7-421c22ec9fea.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'I had previously created a Twitter app a long time ago (it had very similar
    features to the one we''re creating in this project); hence, I have an app there
    already. Click on the blue Create an app button at the top right. You will be
    brought to the following form:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/241b3ba0-075d-4278-91d6-9cc220c5d281.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Fill in the form then click submit. It might take a few days before you receive
    an email saying the app has been approved for development. Be sure to be truthful
    in the description. Lastly, you should then be able to click into your app, and
    get the following page, which shows your API key and secret:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02da6d9d-162e-4652-a314-42b97726412e.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Click Create to create your access token and access token secret. You'll be
    needing them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our API access key, this is how you''d access Twitter using
    the Anaconda package:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'At first glance, this snippet of code is a little weird. Let''s go through
    the code line by line. The first six lines deal with the access tokens and keys.
    Obviously, they should not be hardcoded in. A good way to handle secrets like
    these is to put them in environment variables. I''ll leave that as an exercise
    to the reader. We''ll move on to the rest of the code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'These two lines uses the Anaconda library to get the tweets found in the Home
    timeline. The `nil` being passed in may be of interest. Why would one do this?
    The `GetHomeTimeline` method takes a map of `url.Values`. The package can be found
    in the standard library as `net/url`. Values is defined thus:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'But what do the values represent? It turns out that you may pass some parameters
    to the Twitter API. The parameters and what they do are enumerated here: [https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline).
    I don''t wish to limit anything, so passing in `nil` is acceptable.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is `[]anaconda.Tweet`, all neatly packaged up for us to use. The
    following few lines are therefore quite odd:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Why would I want to save this as a `JSON` file? The answer is simple—when using
    machine learning algorithms, you may need to tune the algorithm. Saving the request
    as a `JSON` file serves two purposes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: It allows for consistency. Under active development, you would expect to tweak
    the algorithm a lot. If the JSON file keeps changing, how do you know if it's
    the tweaks that are making the improvements, and not because the JSON has changed?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being a good citizen. Twitter's API is rate limited. This means you cannot request
    the same thing over and over again too many times. While testing and tuning machine
    learning algorithms, you are likely to have to repeatedly process your data over
    and over again. Instead of hammering the Twitter servers, you should be a good
    citizen and use a locally cached copy.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We defined `load` earlier. Again, we shall see its usefulness in the context
    of tweaking the algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The program
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we''ve done that, we may move the previous `main()` into a different function,
    leaving ourselves with a blank canvas for `main()` again. We''re now ready for
    the meat of the program. This is a skeleton program. You''re encouraged to actually
    actively change the program while writing this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'There are some utility functions that I have yet to show you. Now it''s time
    to define them:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: These are some of the utility functions that may be found in `utils.go`. They
    mainly help with tweaking the program. Now run the program by typing `go run *.go`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking the program
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have been following up to this point, you may get very poor results from
    all the clustering algorithms. I'd like to remind you that the stated objective
    of this book in general is to impart an understanding of what it's like to do
    data science in Go. For the most part, I have advocated a method that can be described
    as think hard about the problem, then write the answers down. But the reality
    is that often trial and error are required.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The solution that works for me on my Twitter home timeline may not work for
    you. For example, this code works well on a friend's Twitter feed. Why is this?
    He follows a lot of similar people who talk about similar things at the same time.
    It's a little harder to cluster tweets in my Twitter home feed. I follow a diverse
    array of people. The people I follow don't have set schedules of tweeting and
    do not generally interact with other Twitter users. Therefore, the tweets are
    generally quite diverse already.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: It is with this in mind that I encourage you to experiment and tweak your program.
    In the subsections that follow, I shall outline what worked for me. It may not
    work for you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking distances
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to this point, we had been using Euclidean distance as provided by the `Marcin`
    library. The Euclidean distance is computed as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `EuclideanDistance` is a good metric to use when it comes to coordinates
    in a Cartesian space. Indeed, earlier I had drawn up an analogy of thinking of
    a tweet as a bunch of coordinates in space, to explain K-means and DBSCAN. The
    reality is that text documents aren't really in Cartesian space. You may think
    of them as being in Cartesian space, but they are not strictly so.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: So, allow me to introduce another type of distance, one that is more suited
    to dealing with textual elements in a bag-of-words-style setting that we're currently
    doing, the Jaccard distance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jaccard distance is defined as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here, `$A$` and `$B$` are sets of words in each tweet. The implementation of
    the Jaccard distance in Go is rudimentary, but it works:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tweaking the preprocessing step
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing you may note is that the preprocessing of tweets is very minimal,
    and some of the rules are odd. For example, all hashtags are treated as one, as
    are all links and mentions. When this project started, it seemed like a good reason.
    There is no other justification than it seemed like a good reason; one always
    needs a springboard from which to jump off in any project. A flimsy excuse at
    that point is as good as any other.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, I have tweaked my preprocessing steps. These are the functions
    that I finally settled on. Do observe the difference between this and the original, listed
    in previous sections:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The most notable thing that I have changed is that I now consider a hashtag
    a word. Mentions are removed. As for URLs, in one of the attempts at clustering,
    I realized that the clustering algorithms were clustering all the tweets with
    a URL into the same cluster. That realization made me remove hashtags, mentions,
    and URLs. Hashtags have the `#` removed and are treated as if they were normal
    words.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you may note that I added some quick and dirty ways to `clean`
    certain things:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, I used regular expressions to replace multiple newlines with just one,
    and to replace all HTML-encoded text with nothing. Lastly, I removed all punctuation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: In a more formal setting, I would use a proper lexer to handle my text. The
    lexer I'd use would come from Lingo (`github.com/chewxy/lingo`). But given that
    Twitter is a low value environment, there wasn't much point in doing so. A proper
    lexer like the one in lingo flags text as multiple things, allowing for easy removal.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing you might notice is that I changed the definition of what a tweet
    is mid-flight:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This block of code says if a tweet is indeed a retweeted status, replace the
    tweet with the retweeted tweet. This works for me. But it may not work for you.
    I personally consider any retweet to be the same as repeating a tweet. So, I do
    not see why they should be separate. Additionally, Twitter allows for users to
    comment on a retweet. If you want to include that, you'd have to change the logic
    a little bit more. Either way, the way I got to this was by manually inspecting
    the `JSON` file I had saved.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: It's asking these questions and then making a judgment call what is important
    in doing data science, either in Go or any other language. It's not about blindly
    applying algorithms. Rather, it's always driven by what the data tells you.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing that you may note is this curious block of code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here, I only consider English tweets. I follow many people who tweet in a variety
    of languages. At any given time, my home timeline would have about 15% of tweets
    in French, Chinese, Japanese, or German. Clustering tweets in a different language
    is a whole different ballgame, so I chose to omit them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to cluster tweets using a variety of clustering
    methods. Though frequently touted as one of the most robust algorithms, we've
    shown that DBSCAN has problems with clustering tweets due to the nature of tweets
    being noisy. Instead, we see that older, more traditional methods, as well as
    a new method of clustering, would yield better results.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: This points to a lesson—there is no one machine-learning algorithm to rule them
    all; there is no ultimate algorithm. Instead, we need to try more than one thing.
    In the chapters that follow, this theme will be more apparent, and we shall approach
    these with more rigor. In the next chapter, we will learn about basics of neural
    networks and apply them on handwriting to recognize digits.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
