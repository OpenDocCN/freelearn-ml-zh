<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Mobile in Android</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we focused on supervised learning and unsupervised learning, and learned about the different types of learning algorithms. In this chapter, we will get introduced to TensorFlow for mobile, and go through a sample program implementation using TensorFlow for mobile. In <a href="3e97f92b-a2d9-4618-9a3b-91552fa3fc3d.xhtml" target="_blank">Chapter 9</a>, <em>Neural Networks on Mobile</em>, we will be using it to implement a classification algorithm. But we need to understand how TensorFlow for mobile works and be able to write samples using it before we can implement machine learning algorithms with it. The objective of this chapter is to get introduced to TensorFlow, TensorFlow Lite, TensorFlow for mobile, and their ways of working, and to try hands-on examples using <span>TensorFlow for mobile</span> in Android.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>An introduction to TensorFlow, TensorFlow Lite, and TensorFlow for mobile</li>
<li>The components of TensorFlow for mobile</li>
<li>The architecture of a mobile machine learning application</li>
<li>Building a sample program using TensorFlow for mobile in Android</li>
</ul>
<p><span>By the end of this chapter, you will know how to build an application using TensorFlow for mobile in Android. We will walk through using it in order to implement a classification algorithm in <a href="3e97f92b-a2d9-4618-9a3b-91552fa3fc3d.xhtml" target="_blank">Chapter 9</a>, <em>Neural Networks on Mobile</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p><span>TensorFlow is a tool to implement machine learning developed by Google, and was open sourced in 2015. It is a product that can be installed on desktops and can be used to create machine learning models. Once the model has been built and trained on the desktop, the developer can transfer these models to mobile devices and start using them to predict results in mobile applications by integrating them into iOS and Android mobile applications. </span><span>There are currently two flavors of TensorFlow available for implementing machine learning solutions on mobile and embedded devices:</span></p>
<ul>
<li><span><strong>Mobile devices</strong>: TensorFlow for Mobile</span></li>
<li><span><strong>Mobile and Embedded devices</strong>: TensorFlow Lite </span></li>
</ul>
<p>The following table will help you to understand the key differences between TensorFlow for mobile and TensorFlow Lite:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 369px">
<p><strong>TensorFlow for Mobile</strong></p>
</td>
<td style="width: 437px">
<p><strong>TensorFlow Lite</strong></p>
</td>
</tr>
<tr>
<td style="width: 369px">
<p>Designed to work with larger devices.</p>
</td>
<td style="width: 437px">
<p>Designed to work with really small devices.</p>
</td>
</tr>
<tr>
<td style="width: 369px">
<p>Binary is optimized for mobile.</p>
</td>
<td style="width: 437px">
<p>Binary is really very small in size optimized for mobile and embedded devices, minimal dependencies, and enhanced performance.</p>
</td>
</tr>
<tr>
<td style="width: 369px">
<p>Enables deployment in CPU, GPU, and TPU across Android, iOS, and Raspberry Pi.</p>
</td>
<td style="width: 437px">
<p>Supports hardware acceleration. Deployment possible on iOS, Android, and Raspberry Pi.</p>
</td>
</tr>
<tr>
<td style="width: 369px">
<p>Recommended for usage now in mobile devices for production deployments.</p>
</td>
<td style="width: 437px">
<p>Still under Beta and is undergoing improvements.</p>
</td>
</tr>
<tr>
<td style="width: 369px">
<p>Wider operator and ML model support available.</p>
</td>
<td style="width: 437px">
<p>Limited operators supported, and not all ML models are supported.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Lite components</h1>
                </header>
            
            <article>
                
<p>In this section, we will go through the details of TensorFlow Lite: the overall architecture, the key components, and their functionality.</p>
<p>The following diagram provides a high-level overview of the key components and how they interact to bring machine learning to mobile devices:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-919 image-border" src="assets/628519c5-2f39-4323-a659-99e7b4efd8f1.png" style="width:37.33em;height:21.42em;"/></p>
<p>The following are the key steps to be followed when implementing ML on devices:</p>
<ol>
<li>Use the TensorFlow, or any other machine learning framework, to create the trained TensorFlow/ML models on the desktop. The trained model can also be created using any Cloud ML engine.</li>
<li>Use the TensorFlow Lite converter to convert the <span>trained ML model</span> to the TensorFlow Lite model file.</li>
<li>Write a mobile application using these files and convert it into a package for deployment and execution in mobile devices. These lite files could be interpreted and executed directly in the kernels or in the hardware accelerators, if available in the device.</li>
</ol>
<p>The following are the key components of TensorFlow Lite:</p>
<ul>
<li>Model-file format</li>
<li>Interpreter</li>
<li>Ops/kernel</li>
<li>Interface to hardware acceleration</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-file format</h1>
                </header>
            
            <article>
                
<p>The following are the highlights of the model-file format:</p>
<ul>
<li><span>It is lightweight and has very few software dependencies. </span></li>
<li><span>It supports quantization.</span>
<ul>
<li><span>This format is FlatBuffer-based and, hence, increases the speed of execution. FlatBuffer is an open source project by Google, originally designed for video games.</span></li>
</ul>
</li>
<li><span>FlatBuffer is a cross-platform serialization library and is similar to protocol buffers.</span></li>
<li><span>This format is more memory-efficient as it does not need a parsing/unpacking step to perform a secondary representation prior to data access. There is no marshaling step and, hence, it uses less code.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpreter</h1>
                </header>
            
            <article>
                
<p><span>The following are the highlights of the interpreter:</span></p>
<ul>
<li><span>It is a mobile-optimized interpreter.</span></li>
<li><span>It helps to keep mobile apps lean and fast.</span></li>
<li><span>It uses a static-graph ordering and a custom (less dynamic) memory allocator to ensure minimal load, initialization, and execution latency.</span></li>
<li><span>The interpreter has a static memory plan and a static execution plan.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ops/Kernel</h1>
                </header>
            
            <article>
                
<p><span>A set of core operators, both quantized and float, many of which have been tuned for mobile platforms. These can be used to create and run custom models. Developers can also write their own custom operators and use them in models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interface to hardware acceleration</h1>
                </header>
            
            <article>
                
<p>TensorFlow Lite has an interface to hardware accelerators; in Android, it is through the Android Neural Network API and, in iOS, it is through CoreML.</p>
<p>The following are the pretested models that are guaranteed to work out of the box with TensorFlow Lite:</p>
<ul>
<li>
<p><strong>Inception V3</strong>: A popular model for detecting the dominant objects present in an image.</p>
</li>
<li>
<p><strong>MobileNets:</strong> Computer vision models that can be used for classification, detection, and segmentation. MobileNet models are smaller, but<span> less accurate, than Inception V3.</span></p>
</li>
<li>
<p><strong>On-device smart reply</strong>: An on-device model that provides one-touch replies for an incoming text message by suggesting contextually-relevant messages. </p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The architecture of a mobile machine learning application</h1>
                </header>
            
            <article>
                
<p>Now that we understand the components of TensorFlow Lite, we'll look at how a mobile application works with the TensorFlow components to provide the mobile ML solution.</p>
<p>The mobile application should leverage the TensorFlow Lite model file to perform the inference for future data. The TensorFlow Lite model file can either be packaged with the mobile application and deployed together, or kept separate from the mobile application deployment package. The following diagram depicts the two possible deployment scenarios:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-920 image-border" src="assets/6bffeb98-43b8-4317-b06d-15857be6f603.png" style="width:28.33em;height:15.58em;"/></p>
<p>Each deployment has its pros and cons. In the first case, where both are coupled, there is more security for the model file and it can be kept safe and secured. This is a more straightforward approach. However, the application package size is increased due to the size of the model file. In the second case, where both are kept separate, it is easy to update the model file separately, without performing an application upgrade. Hence, all activities with respect to the application upgrade, deployment to the app store, and so on can be avoided for a model upgrade. The application package size can also be minimized due to this separation. However, since the model file is standalone, it should be handled with greater care, without leaving it vulnerable to security threats.</p>
<p>Having got an overview of the mobile application with the TensorFlow Lite model file, let's look at the whole picture. The mobile application is packaged with the TensorFlow Lite model file. This interaction between the mobile application written using the Android SDK and the TensorFlow Lite model file happens through the TensorFlow Lite Interpreter, which is part of the Android NDK layer. The C functions are invoked through the interfaces exposed to the SDK layer from the mobile application in order to do the prediction or inference by using the trained TensorFlow Lite model deployed with the mobile application. The following diagram provides a clear view of the layers of the SDK and NDK of the Android ecosystem that will be involved in a typical machine learning program. The execution can also be triggered on GPU or any specialized processors through the android NN layer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-921 image-border" src="assets/bf31274a-6fd3-4944-b593-0ac6b96428e8.png" style="width:37.58em;height:23.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the model concepts</h1>
                </header>
            
            <article>
                
<p>Before writing our first program using TensorFlow, we will briefly go through the concepts that will help us to understand how the TensorFlow Lite model work. We won't be going into the details, but a conceptual high level overview alone for better understanding.</p>
<p><span>MobileNet and Inception V3 are the built-in models that are based on <strong>convolutional neural networks</strong> (<strong>CNNs</strong>).</span><span> </span></p>
<p><span>At its most basic level, CNN can be thought of as a kind of neural network that uses many identical copies of the same neuron.</span><span> This allows the network to have lots of neurons and express computationally large models while keeping the number of actual parameters – the values describing how neurons behave – that need to be learned fairly low.</span></p>
<p>This concept can be understood with the analogy of a Jigsaw puzzle and how we usually solve one. The following diagram is a puzzle that needs to be solved: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d8c0e4b-9f45-45ea-a30b-32460471c42e.png" style="width:15.33em;height:15.33em;"/></div>
<p>If we have to assemble this puzzle from the pieces provided, just think about how you will start solving it. You may group all the pieces with different colors together. Then within the same color, you'd check for patterns and then assemble them. This is the same way that convolutional networks train for image classification and recognition. Hence there is only a small portion, each neuron remembers. But the parent neuron understands how the things within its scope needs to be assembled to get the big picture.</p>
<p>In the Inception V3 and the MobileNet models, both work based on the CNN concept. The model is pretty much trained and stable. All we need to do to use our set of images is retrain the model with our images. So now that we have had enough of concepts and theory, we will move on to writing our first sample program using TensorFlow Lite for Android.</p>
<p>We will be using the TensorFlow for mobile for a classification application in <a href="3e97f92b-a2d9-4618-9a3b-91552fa3fc3d.xhtml" target="_blank">Chapter 9</a>, <em>Neural Networks on Mobile</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the mobile application using the TensorFlow model</h1>
                </header>
            
            <article>
                
<p><strong>What we are going to do?</strong></p>
<p>In this section, we are going to build a small <kbd>(a+b)2</kbd> model in TensorFlow, deploy it into an android mobile application, and run it from the Android mobile device.</p>
<p><strong>What do you need to know?</strong></p>
<p>To proceed in this section, you need a working installation of Python, <span>TensorFlow</span> dependencies, and android studio, and also some knowledge of python and java android. You can find the instructions on how to install <span>TensorFlow</span> here: <a href="https://www.tensorflow.org/install/">https://www.tensorflow.org/install/</a>.</p>
<p>If you need a detailed installation procedure for Windows, please refer to the one provided with screenshots in the <a href="d7ddae2d-9276-461e-9526-73448159e26b.xhtml" target="_blank">Chapter 11</a><span>, <em>The Future of ML on Mobile Applications</em></span> of this book.</p>
<p>We saw the details of <span>TensorFlow</span> already. To put it onto a simple words TensorFlow is nothing but saving the tensor flow program written in python into a small file that can be read by the C++ native libraries what we will install in our Android app and can execute and do the inference from the mobile. To do so, JNI (Java native interface) is working as a bridge between java and C++.</p>
<p>To learn more about the idea behind tensor flow lite, check out <a href="https://www.tensorflow.org/mobile/tflite/">https://www.tensorflow.org/mobile/tflite/.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing our first program</h1>
                </header>
            
            <article>
                
<p>In order to write a TensorFlow mobile application, there are a few steps that we need to follow:</p>
<ol>
<li>Create the TF (TensorFlow) model</li>
<li>Save the model</li>
<li>Freeze the graph</li>
</ol>
<ol start="4">
<li>Optimize the model</li>
<li>Write the Android application and execute it</li>
</ol>
<p>We will go through each of the steps in detail now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and Saving the TF model</h1>
                </header>
            
            <article>
                
<p>First, we first create a simple model and save its computation graph as a serialized <kbd>GraphDef</kbd> file. After training the model, we then save the values of its variables into a checkpoint file. We have to turn these two files into an optimized standalone file, which is all we need to use inside the Android app.</p>
<p>For this tutorial, we create a very simple <span>TensorFlow</span> graph that implements a small use case that will calculate <em>(a+b)<sup>2</sup>=c</em>. Here, we are saving the input as <em>a</em> and <em>b</em>, and the output as <em>c</em>.</p>
<p>To implement this sample program, we are going to use Python. So, as a prerequisite, you need to install python in your machine and install the TensorFlow libraries on your machine using <kbd>pip</kbd>.</p>
<div class="packt_tip">Please check the software installations/appendix section of this book for instructions on how to install Python. <kbd>pip</kbd> is a python package manager that comes with Python.</div>
<p>Once you install python and set the path correctly, you can run the <kbd>pip</kbd> command from the command prompt. To install <span>TensorFlow,</span> run the following command:</p>
<pre><strong>pip install tensorflow</strong></pre>
<p>This sample might seem too simple and might not contain anything related to machine learning, but this example should be a good starting point to understand the concepts of TensorFlow and its working:</p>
<pre><span>import tensorflow as tf <br/></span><span>a = tf.placeholder(tf.int32, name='a') # input <br/></span><span>b = tf.placeholder(tf.int32, name='b') # input <br/></span><span>times = tf.Variable(name="times", dtype=tf.int32, initial_value=2) <br/></span><span>c = tf.pow(tf.add(a, b), times, name="c") <br/></span><span>saver = tf.train.Saver()<br/></span><span><br/>init_op = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init_op) tf.train.write_graph(sess.graph_def, '.', 'tfdroid.pbtxt')<br/></span><span><br/>sess.run(tf.assign(name="times", value=2, ref=times)) # save the graph <br/></span><span># save a checkpoint file, which will store the above assignment saver.save(sess, './tfdroid.ckpt')</span></pre>
<p>In the preceding program, we are creating two placeholders, named <em>a</em> and <em>b</em>, that can hold integer values. For now, just you can imagine placeholders as nodes in a tree for a decision tree. In the next line, we are creating a variable named times. We are creating this to store how many times we need to multiply the input. In this case, we are giving two as agenda is to do for <em>(a+b)</em><sup><em>2</em></sup><em>.</em></p>
<p>In the next line, we are applying addition operation on both the <em>a</em> and <em>b</em> nodes. And for that sum, we are applying power operation and saving the result in a new node called c. To run the code, first save it in a file with the <kbd>.py</kbd> extension. Then execute the program using the <kbd>python</kbd> command, as follows:</p>
<pre>python (filename)</pre>
<p>Running the previous piece of code will produce two files. First, it saves the TF computation graph in a <kbd>GraphDef</kbd> text file called <kbd>tfdroid.pbtxt</kbd>. Next, it will perform a simple assignment (which normally would be done through actual learning) and save a checkpoint of the model variables in <kbd>tfdroid.ckpt</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Freezing the graph</h1>
                </header>
            
            <article>
                
<p>Now that we have these files, we need to freeze the graph by converting the variables in the checkpoint file into <kbd>Const Ops</kbd> that contain the values of the variables, and combining them with the GraphDef in a standalone file. Using this file makes it easier to load the model inside a mobile app. <span>TensorFlow </span>provides <kbd>freeze_graph</kbd> in <kbd>tensorflow.python.tools</kbd> for this purpose:</p>
<pre>import sys import tensorflow as tf from tensorflow.python.tools <br/>import freeze_graph from tensorflow.python.tools <br/>import optimize_for_inference_lib MODEL_NAME = 'tfdroid'<br/># Freeze the graph<br/><br/>input_graph_path = MODEL_NAME+'.pbtxt' checkpoint_path = './'+MODEL_NAME+'.ckpt' input_saver_def_path = "" input_binary = False output_node_names = "c" restore_op_name = "save/restore_all" filename_tensor_name = "save/Const:0" output_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb' output_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb' clear_devices = True freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary, checkpoint_path, output_node_names, restore_op_name, filename_tensor_name, output_frozen_graph_name, clear_devices, "")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing the model file</h1>
                </header>
            
            <article>
                
<p>Once we have the frozen graph, we can further optimize the file for inference-only purposes by removing the parts of the graph that are only needed during training. According to the documentation, this includes:</p>
<ul>
<li>Removing training-only operations, such as checkpoint saving</li>
<li>Stripping out parts of the graph that are never reached</li>
<li>Removing debug operations, such as <kbd>CheckNumerics</kbd></li>
<li>Folding batch normalization ops into the pre-calculated weights</li>
<li>Fusing common operations into unified versions</li>
</ul>
<p><span>TensorFlow</span> provides <kbd>optimize_for_inference_lib</kbd> in <kbd>tensorflow.python.tools</kbd> for this purpose:</p>
<pre># Optimize for inference <br/>input_graph_def = tf.GraphDef() with tf.gfile.Open(output_frozen_graph_name, "r") as f: data = f.read() input_graph_def.ParseFromString(data) <br/>output_graph_def = optimize_for_inference_lib.optimize_for_inference( input_graph_def, ["a", "b"], <br/># an array of the input node(s) ["c"], <br/># an array of output nodes tf.int32.as_datatype_enum)<br/><br/># Save the optimized graph f = tf.gfile.FastGFile(output_optimized_graph_name, "w") f.write(output_graph_def.SerializeToString()) tf.train.write_graph(output_graph_def, './', output_optimized_graph_name)</pre>
<p>Take note of the input and output nodes in the preceding code. Our graph only has one input node, named I, and one output node, named O. These names correspond to the names you use when you define your tensors. You should adjust these based on your graph in case you are using a different one.</p>
<p>Now we have a binary file, called <kbd>optimized_tfdroid.pb</kbd>, which means we are ready to build our Android app. If you got an exception when creating <kbd>optimized_tfdroid.pb</kbd>, you can use <kbd>tfdroid.somewhat</kbd>, which is an unoptimized version of the model <span>– it</span> is fairly large.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Android app</h1>
                </header>
            
            <article>
                
<p>We need to get the <span>TensorFlow</span> libraries for Android, create an Android app, configure it to use these libraries, and then invoke the <span>TensorFlow</span> model inside the app.</p>
<p>Although you can compile the <span>TensorFlow</span> libraries from scratch, it’s easier to use the prebuilt libraries.</p>
<p>Now use Android Studio to create an Android project with an empty activity.</p>
<p>Once the project is created, add the TF Libraries to the project's <kbd>libs</kbd> folder. You can get <span><span>these libraries </span></span><span>from the GitHub repository: </span><a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/tensorflow%20simple/TensorflowSample/app/libs">https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/tensorflow%20simple/TensorflowSample/app/libs</a><span>.</span></p>
<p>Now your project's <kbd>libs/</kbd> folder should look like this:</p>
<pre>libs<br/>|____arm64-v8a<br/>| |____libtensorflow_inference.so<br/>|____armeabi-v7a<br/>| |____libtensorflow_inference.so<br/>|____libandroid_tensorflow_inference_java.jar<br/>|____x86<br/>| |____libtensorflow_inference.so<br/>|____x86_64<br/>| |____libtensorflow_inference.so</pre>
<p>You need to let your build system know where these libraries are located by putting the following lines inside of the Android block in <kbd>app/build.gradle</kbd>:</p>
<pre>sourceSets { main { jniLibs.srcDirs = ['libs'] } }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Copying the TF Model</h1>
                </header>
            
            <article>
                
<p>Create an Android Asset Folder for the app and place the <kbd>optimized_tfdroid.pb or tfdroid.pb</kbd> file that we just created inside it (<kbd>app/src/main/assets/</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an activity</h1>
                </header>
            
            <article>
                
<p>Click on the project and create an empty activity named <kbd>MainActivity</kbd>. In the layout of that activity, paste the following XML:</p>
<pre>&lt;?xml version="1.0" encoding="utf-8"?&gt;<br/>&lt;RelativeLayout <br/><br/>android:id="@+id/activity_main"<br/>android:layout_width="match_parent"<br/>android:layout_height="match_parent"<br/>android:paddingBottom="@dimen/activity_vertical_margin"<br/>android:paddingLeft="@dimen/activity_horizontal_margin"<br/>android:paddingRight="@dimen/activity_horizontal_margin"<br/>android:paddingTop="@dimen/activity_vertical_margin"<br/>tools:context="com.example.vavinash.tensorflowsample.MainActivity"&gt;<br/><br/>&lt;EditText<br/>android:id="@+id/editNum1"<br/>android:layout_width="100dp"<br/>android:layout_height="wrap_content"<br/>android:layout_alignParentTop="true"<br/>android:layout_marginEnd="13dp"<br/>android:layout_marginTop="129dp"<br/>android:layout_toStartOf="@+id/button"<br/>android:ems="10"<br/>android:hint="a"<br/>android:inputType="textPersonName"<br/>android:textAlignment="center" /&gt;<br/><br/>&lt;EditText<br/>android:id="@+id/editNum2"<br/>android:layout_width="100dp"<br/>android:layout_height="wrap_content"<br/>android:layout_alignBaseline="@+id/editNum1"<br/>android:layout_alignBottom="@+id/editNum1"<br/>android:layout_toEndOf="@+id/button"<br/>android:ems="10"<br/>android:hint="b"<br/>android:inputType="textPersonName"<br/>android:textAlignment="center" /&gt;<br/><br/>&lt;Button<br/>android:text="Run"<br/>android:layout_width="wrap_content"<br/>android:layout_height="wrap_content"<br/>android:id="@+id/button"<br/>android:layout_below="@+id/editNum2"<br/>android:layout_centerHorizontal="true"<br/>android:layout_marginTop="50dp" /&gt;<br/><br/>&lt;TextView<br/>android:layout_width="wrap_content"<br/>android:layout_height="wrap_content"<br/>android:text="Output"<br/>android:id="@+id/txtViewResult"<br/>android:layout_marginTop="85dp"<br/>android:textAlignment="center"<br/>android:layout_alignTop="@+id/button"<br/>android:layout_centerHorizontal="true" /&gt;<br/>&lt;/RelativeLayout&gt;</pre>
<p>In the <kbd>mainactivity.java</kbd> file, paste the following code:</p>
<pre>package com.example.vavinash.tensorflowsample;<br/>import android.support.v7.app.AppCompatActivity;<br/>import android.os.Bundle;<br/>import android.widget.EditText;<br/>import android.widget.TextView;<br/>import android.widget.Button;<br/>import android.view.View;<br/>import org.tensorflow.contrib.android.TensorFlowInferenceInterface;public class MainActivity extends AppCompatActivity {<br/>    //change with the file name of your own model generated in python tensorflow.<br/>    private static final String MODEL_FILE = "file:///android_asset/tfdroid.pb";<br/><br/>    //here we are using this interface to perform the inference with our generated model. It internally     uses c++ libraries and JNI.<br/>    private TensorFlowInferenceInterface inferenceInterface;<br/>    static {<br/>        System.loadLibrary("tensorflow_inference");<br/>    }<br/>    @Override<br/>    protected void onCreate(Bundle savedInstanceState) {<br/>        super.onCreate(savedInstanceState);<br/>        setContentView(R.layout.activity_main);<br/>        inferenceInterface = new TensorFlowInferenceInterface();<br/>        //instantiatind and setting our model file as input.<br/>        inferenceInterface.initializeTensorFlow(getAssets(), MODEL_FILE);<br/>        final Button button = (Button) findViewById(R.id.button);<br/>        button.setOnClickListener(new View.OnClickListener() {<br/>            public void onClick(View v) {<br/>                final EditText editNum1 = (EditText) findViewById(R.id.editNum1);<br/>                final EditText editNum2 = (EditText) findViewById(R.id.editNum2);<br/>                float num1 = Float.parseFloat(editNum1.getText().toString());<br/>                float num2 = Float.parseFloat(editNum2.getText().toString());<br/>                int[] i = {1};<br/>                int[] a = {((int) num1)};<br/>                int[] b = {((int) num2)};<br/>                //Setting input for variable a and b in our model.<br/>                inferenceInterface.fillNodeInt("a",i,a);<br/>                inferenceInterface.fillNodeInt("b",i,b);<br/>                //performing the inference and getting the output in variable c<br/>                inferenceInterface.runInference(new String[] {"c"});<br/>                //reading received output<br/>                int[] c = {0};<br/>                inferenceInterface.readNodeInt("c", c);<br/>                //projecting to user.<br/>                final TextView textViewR = (TextView) findViewById(R.id.txtViewResult);<br/>                textViewR.setText(Integer.toString(c[0]));<br/>            }<br/>        });<br/>    }<br/>}</pre>
<p>In the preceding program, we are loading the <span>TensorFlow</span> binaries using the following snippet:</p>
<pre>System.loadLibrary("tensorflow_inference");</pre>
<p>In the create Bundle method, we have the main logic. Here, we are creating the <span>TensorFlow</span> inference object by supplying the <span>TensorFlow</span> model's <kbd>.pb</kbd> file, which has been generated and we saw that in the section - create and save model</p>
<p>Then we registered a click event to the <span class="packt_screen">Run</span> button. In this, we are feeding the values to the a and b nodes in <span>TensorFlow</span> and running the inference, then we fetch the value in the C node and show it to the user.</p>
<p>Now run the app to see the results of the <kbd>(a+b)2 = c</kbd> expression:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6abc781a-f565-4d2e-8273-a38058dfb194.png" style="width:16.33em;height:29.00em;"/></p>
<p class="mce-root"><span>On the left side, it is showing the app's opening screen. In the provided text boxes, we need to give the</span> <kbd>a</kbd> <span>and</span> <kbd>b</kbd> <span>values. Once you click on the</span> <span class="packt_screen">Run</span> <span>button, you will see the result in the output area.</span></p>
<div class="packt_tip">You can get the preceding app code from the GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/tensorflow%20simple">https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/tensorflow%20simple</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we g<span>ot introduced to Google's machine learning tools for Mobile and looked at</span><span> the various flavors of the toolkit – TensorFlow for Mobile and TensorFlow Lite. We also explored </span><span>the architecture of a</span> <span>TensorFlow-</span><span>ML-enabled mobile application. Then we discussed</span><span> the architecture and details of TensorFlow Lite and its components, and even demonstrated a simple use case for an</span><span> android mobile application using TensorFlow for mobile.</span></p>
<p>In the next chapter, we will be using the TensorFlow for mobile that we discussed here to implement a classification algorithm.</p>


            </article>

            
        </section>
    </body></html>