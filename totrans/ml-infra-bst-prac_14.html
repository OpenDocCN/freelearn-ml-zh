<html><head></head><body>
		<div id="_idContainer109">
			<h1 class="chapter-number"><a id="_idTextAnchor132"/>11</h1>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor133"/>Training and Evaluation of Advanced ML Algorithms – GPT and Autoencoders</h1>
			<p>Classical <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) are very good for classical problems – prediction, classification, and recognition. As we learned in the previous chapter, training them requires a moderate amount of data, and we train them for specific tasks. However, breakthroughs in ML and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) in the late 2010s and the <a id="_idIndexMarker393"/>beginning of 2020s were about completely different types of models – <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>), <strong class="bold">Generative Pre-Trained Transformers</strong> (<strong class="bold">GPTs</strong>), and <strong class="bold">generative </strong><span class="No-Break"><strong class="bold">AI</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GenAI</strong></span><span class="No-Break">).</span></p>
			<p>GenAI models provide two <a id="_idIndexMarker394"/>advantages – they can generate new data and they can provide us with an internal representation of the data that captures the context of the data and, to some extent, its semantics. In the previous chapters, we saw how we can use existing models for inference and generating simple pieces <span class="No-Break">of text.</span></p>
			<p>In this chapter, we explore how GenAI <a id="_idIndexMarker395"/>models work based on GPT and Bidirectional Encoder Representations from Transformers (BERT) models. These models are designed to generate new data based on the patterns that they were trained on. We also look at the concept of autoencoders (AEs), where we train an AE to generate new images based on previously <span class="No-Break">trained data.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>From classical ML models <span class="No-Break">to GenAI</span></li>
				<li>The theory behind GenAI models – AEs <span class="No-Break">and transformers</span></li>
				<li>Training and<a id="_idIndexMarker396"/> evaluation of a <strong class="bold">Robustly Optimized BERT Approach</strong> (<span class="No-Break"><strong class="bold">RoBERTa</strong></span><span class="No-Break">) model</span></li>
				<li>Training and evaluation of <span class="No-Break">an AE</span></li>
				<li>Developing safety cages to prevent models from breaking the <span class="No-Break">entire system</span></li>
			</ul>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor134"/>From classical ML to GenAI</h1>
			<p>Classical AI, also known as symbolic AI or rule-based AI, emerged as one of the earliest schools of thought in the<a id="_idIndexMarker397"/> field. It is rooted in the concept of explicitly encoding knowledge and using logical rules to manipulate symbols and derive intelligent behavior. Classical AI systems are designed to follow predefined rules and algorithms, enabling them to solve well-defined problems with precision and determinism. We delve into the<a id="_idIndexMarker398"/> underlying principles of classical AI, exploring its reliance on rule-based systems, expert systems, and <span class="No-Break">logical reasoning.</span></p>
			<p>In contrast, GenAI represents a paradigm shift in AI development, capitalizing on the power of ML and NNs to create intelligent systems that can generate new content, recognize patterns, and make informed decisions. Rather than relying on explicit rules and handcrafted knowledge, GenAI leverages data-driven approaches to learn from vast amounts of information and infer patterns and relationships. We examine the core concepts of GenAI, including DL, NNs, and probabilistic models, to unravel its ability to create original content and foster <span class="No-Break">creative problem-solving.</span></p>
			<p>One of the examples of a GenAI model is the GPT-3 model. GPT-3 is a state-of-the-art language model developed by OpenAI. It is based on the transformer architecture. GPT-3 is trained using a<a id="_idIndexMarker399"/> technique called <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>), which enables it to generate coherent and contextually <span class="No-Break">relevant text.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor135"/>The theory behind advanced models – AEs and transformers</h1>
			<p>One of the large limitations of classical ML models is the access to annotated data. Large NNs contain millions (if not billions) of parameters, which means that they require equally many labeled data points to be trained correctly. Data labeling, also known as annotation, is the most <a id="_idIndexMarker400"/>expensive activity in<a id="_idIndexMarker401"/> ML, and therefore it is the labeling process that becomes the de facto limit of ML models. In the early 2010s, the solution to that problem was to <span class="No-Break">use crowdsourcing.</span></p>
			<p>Crowdsourcing, which is a process of collective data collection (among other things), means that we use users of our services to label the data. A CAPTCHA is one of the most prominent examples. A CAPTCHA is used when we need to recognize images in order to log in to a service. When we introduce new images, every time a user needs to recognize these images, we can label a lot of data in a relatively <span class="No-Break">short time.</span></p>
			<p>There is, nevertheless, an inherent problem with that process. Well, there are a few problems, but the most prominent one is that this process works mostly with images or similar kinds of data. It is also a<a id="_idIndexMarker402"/> relatively limited process – we can only ask users to recognize an image, but not add a semantic map and not draw a bounding box over an image. We cannot ask users to assess the similarity of images or any other, bit more <span class="No-Break">advanced, task.</span></p>
			<p>Here enter more advanced methods – GenAI and networks such as <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>). These <a id="_idIndexMarker403"/>networks are designed to generate data and learn which data is like the original data. These networks are very powerful and have been used in such applications as the<a id="_idIndexMarker404"/> generation of images; for example, in the so-called “<span class="No-Break">deep fakes.”</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor136"/>AEs</h2>
			<p>One of the main components of such a model is the AE, which is designed to learn a compressed<a id="_idIndexMarker405"/> representation (encoding) of the input data and then reconstruct the original data (decoding) from this <span class="No-Break">compressed representation.</span></p>
			<p>The architecture of an AE (<span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em>) consists of two main components: an encoder and a decoder. The<a id="_idIndexMarker406"/> encoder takes the input data and maps it to a lower-dimensional latent space representation, often referred to as the encoding/embedding or latent representation. The decoder takes this encoded representation and reconstructs the original <span class="No-Break">input data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer101">
					<img alt="Figure 11.1 – High-level architecture of AEs" src="image/B19548_11_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – High-level architecture of AEs</p>
			<p>The objective of an AE is to minimize reconstruction errors, which is the difference between the input data and<a id="_idIndexMarker407"/> the output of the decoder. By doing so, the AE learns to capture the most important features of the input data in the latent representation, effectively compressing the information. The most interesting part is the latent space or encoding. This part allows this model to learn the representation of a complex data point (for example, an image) in a small vector of just a few numbers. The latent representation learned by the AE can be considered a compressed representation or a low-dimensional embedding of the input data. This compressed representation can be used for various purposes, such as data visualization, dimensionality reduction, anomaly detection, or as a starting point for other <span class="No-Break">downstream tasks.</span></p>
			<p>The encoder part calculates <a id="_idIndexMarker408"/>the latent vector, and the decoder part can expand it to an image. There are different types of AEs; the most interesting one is the <strong class="bold">variational AE</strong> (<strong class="bold">VAE</strong>), which <a id="_idIndexMarker409"/>encodes the parameters of a function that can generate new data rather than the representation of the data itself. In this way, it can create new data based on the distribution. In fact, it can even create completely new types of data by combining <span class="No-Break">different functions.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor137"/>Transformers</h2>
			<p>In <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks, we usually employ a bit different type of GenAI – transformers. Transformers <a id="_idIndexMarker410"/>revolutionized the field of machine translation but have been applied to many other tasks, including language understanding and <span class="No-Break">text generation.</span></p>
			<p>At its core, a transformer employs a self-attention mechanism that allows the model to weigh the importance of different words or tokens in a sequence when processing them. This attention mechanism <a id="_idIndexMarker411"/>enables the model to capture long-range dependencies and contextual relationships between words more effectively than<a id="_idIndexMarker412"/> traditional <strong class="bold">recurrent NNs</strong> (<strong class="bold">RNNs</strong>) or <strong class="bold">convolutional </strong><span class="No-Break"><strong class="bold">NNs</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CNNs</strong></span><span class="No-Break">).</span></p>
			<p>Transformers consist<a id="_idIndexMarker413"/> of an encoder-decoder structure. The encoder processes the input sequence, such as a sentence, and the decoder generates an output sequence, often based on the input and a target sequence. Two elements are unique <span class="No-Break">for transformers:</span></p>
			<ul>
				<li><strong class="bold">Multi-head self-attention (MHSA)</strong>: A mechanism that allows the model to attend to different positions in the <a id="_idIndexMarker414"/>input sequence simultaneously, capturing different types of dependencies. This is an extension to the RNN architecture, which was able to connect neurons in the same layer, thus capturing <span class="No-Break">temporal dependencies.</span></li>
				<li><strong class="bold">Positional encoding</strong>: To incorporate positional information into the model, positional encoding vectors are<a id="_idIndexMarker415"/> added to the input embeddings. These positional encodings are based on the tokens and their relative position to one another. This mechanism allows us to capture the context of a specific token and therefore to <a id="_idIndexMarker416"/>capture the basic contextual semantics of <span class="No-Break">the text.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> presents the high-level architecture <span class="No-Break">of transformers:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer102">
					<img alt="Figure 11.2 – High-level architecture of transformers" src="image/B19548_11_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – High-level architecture of transformers</p>
			<p>In this architecture, self-attention is a key mechanism that allows the model to weigh the importance of different words or tokens in a sequence when processing them. The self-attention mechanism is applied independently to each word in the input sequence, and it helps capture contextual relationships and dependencies between words. The term <em class="italic">head</em> refers to an independent attention mechanism that operates in parallel. Multiple self-attention heads can be used in the transformer model to capture different types of relationships (although we do not know what these <span class="No-Break">relationships are).</span></p>
			<p>Each self-attention head operates by computing attention scores between query representations and key representations. These attention scores indicate the importance or relevance of each word in the sequence with respect to the others. Attention scores are obtained by taking the dot product between the query and key representations, followed by applying a softmax function to normalize <span class="No-Break">the scores.</span></p>
			<p>The attention scores are then <a id="_idIndexMarker417"/>used to weigh the value representations. The weighted values are summed together to obtain the output representation for each word in <span class="No-Break">the sequence.</span></p>
			<p>The feed-forward networks in transformers serve two main purposes: feature extraction and position-wise representation. The feature extraction extracts higher-level features from the self-attention outputs, in a way that is very similar to the word-embeddings extraction that we learned previously. By applying non-linear transformations, the model can capture complex patterns and dependencies in the input sequence. The position-wise representation ensures that the model can learn different transformations for each position. It allows the model to learn complex representations of the sentences and therefore capture the more complex context of each word <span class="No-Break">and sentence.</span></p>
			<p>The transformer architecture is the basis for modern models such as GPT-3, which is a pre-trained generative transformer; that is, a transformer that has been pre-trained on a large mass of text. However, it is based on models such as BERT and <span class="No-Break">its relatives.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor138"/>Training and evaluation of a RoBERTa model</h1>
			<p>In general, the training process for GPT-3 involves exposing the model to a massive amount of text data from <a id="_idIndexMarker418"/>diverse sources, such as books, articles, websites, and more. By analyzing the patterns, relationships, and language structures within this data, the model learns to predict the likelihood of a word or phrase appearing based on the surrounding context. This learning objective is achieved <a id="_idIndexMarker419"/>through a process known as <strong class="bold">masked language modeling</strong> (<strong class="bold">MLM</strong>), where certain words are randomly masked in the input, and the model is tasked with predicting the correct word <a id="_idIndexMarker420"/>based on <span class="No-Break">the context.</span></p>
			<p>In this chapter, we train the RoBERTa model, which is a variation of the now-classical BERT model. Instead of using generic sources such as books and <em class="italic">Wikipedia</em> articles, we use programs. To make our training task a bit more specific, let us train a model that is capable of “understanding” code from a networking domain – WolfSSL, which is an open source implementation of the SSL protocol, used in many embedded <span class="No-Break">software devices.</span></p>
			<p>Once the training is complete, BERT models are capable of generating text by leveraging their learned knowledge and the context provided in a given prompt. When a user provides a prompt or a partial sentence, the model processes the input and generates a response by probabilistically predicting the most likely next word based on the context it has learned from the <span class="No-Break">training data.</span></p>
			<p>When it comes to GPT-3 (and similar) models, it is an extension of the BERT model. The generation process in GPT-3 involves multiple layers of attention mechanisms within the transformer architecture. These <a id="_idIndexMarker421"/>attention mechanisms allow the model to focus on relevant parts of the input text and make connections between different words <a id="_idIndexMarker422"/>and phrases, ensuring coherence and contextuality in the generated output. The model generates text by sampling or selecting the most probable next word at each step, taking into account previously <span class="No-Break">generated words.</span></p>
			<p>So, let us start our training process by preparing the data for training. First, we read <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
from tokenizers import ByteLevelBPETokenizer
paths = ['source_code_wolf_ssl.txt']
print(f'Found {len(paths)} files')
print(f'First file: {paths[0]}')</pre>			<p>This provides us with the raw training set. In this set, the text file contains all the source code from the WolfSSL protocol in one file. We do not have to prepare it like this, but it certainly makes the process easier as we only deal with one source file. Now, we can train the tokenizer, very similar to what we saw in the <span class="No-Break">previous chapters:</span></p>
			<pre class="source-code">
# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()
print('Training tokenizer...')
# Customize training
# we use a large vocabulary size, but we could also do with ca. 10_000
tokenizer.train(files=paths,
                vocab_size=52_000,
                min_frequency=2,
                special_tokens=["&lt;s&gt;","&lt;pad&gt;","&lt;/s&gt;","&lt;unk&gt;","&lt;mask&gt;",])</pre>			<p>The first line initializes an instance of the <strong class="source-inline">ByteLevelBPETokenizer</strong> tokenizer class. This tokenizer is based<a id="_idIndexMarker423"/> on a byte-level version of the <strong class="bold">Byte-Pair Encoding</strong> (<strong class="bold">BPE</strong>) algorithm, which is a popular <a id="_idIndexMarker424"/>subword tokenization method. We discussed it in<a id="_idIndexMarker425"/> the <span class="No-Break">previous chapters.</span></p>
			<p>The next line prints a message indicating that the tokenizer training process <span class="No-Break">is starting.</span></p>
			<p>The <strong class="source-inline">tokenizer.train()</strong> function is called to train the tokenizer. The training process takes a <span class="No-Break">few parameters:</span></p>
			<ul>
				<li><strong class="source-inline">files=paths</strong>: This parameter specifies the input files or paths containing the text data to train the tokenizer. It expects a list of <span class="No-Break">file paths.</span></li>
				<li><strong class="source-inline">vocab_size=52_000</strong>: This parameter sets the size of the vocabulary; that is, the number of unique tokens the tokenizer will generate. In this case, the tokenizer will create a vocabulary of <span class="No-Break">52,000 tokens.</span></li>
				<li><strong class="source-inline">min_frequency=2</strong>: This parameter specifies the minimum frequency a token must have in the training data to be included in the vocabulary. Tokens that occur less frequently than this<a id="_idIndexMarker426"/> threshold will be treated as <strong class="bold">out-of-vocabulary</strong> (<span class="No-Break"><strong class="bold">OOV</strong></span><span class="No-Break">) tokens.</span></li>
				<li><strong class="source-inline">special_tokens=["&lt;s&gt;","&lt;pad&gt;","&lt;/s&gt;","&lt;unk&gt;","&lt;mask&gt;"]</strong>: This parameter defines a list of special tokens that will be added to the vocabulary. Special tokens are commonly used to represent specific meanings or special purposes. In this case, the special tokens are <strong class="source-inline">&lt;s&gt;</strong>, <strong class="source-inline">&lt;pad&gt;</strong>, <strong class="source-inline">&lt;/s&gt;</strong>,<strong class="source-inline"> &lt;unk&gt;</strong>, and <strong class="source-inline">&lt;mask&gt;</strong>. These tokens are often used in tasks such as machine translation, text generation, or <span class="No-Break">language modeling.</span></li>
			</ul>
			<p>Once the training process is completed, the tokenizer will have learned the vocabulary and will be able to encode and decode text using the trained subword units. We can now save the tokenizer using this piece <span class="No-Break">of code:</span></p>
			<pre class="source-code">
import os
# we give this model a catchy name - wolfBERTa
# because it is a RoBERTa model trained on the WolfSSL source code
token_dir = './wolfBERTa'
if not os.path.exists(token_dir):
  os.makedirs(token_dir)
tokenizer.save_model('wolfBERTa')</pre>			<p>We also test this tokenizer<a id="_idIndexMarker427"/> using the following line: <strong class="source-inline">tokenizer.encode("int main(int argc, </strong><span class="No-Break"><strong class="source-inline">void **argv)").tokens</strong></span><span class="No-Break">.</span></p>
			<p>Now, let us make sure that<a id="_idIndexMarker428"/> the tokenizer is comparable with our model in the next step. To do that, we need to make sure that the output of the tokenizer never exceeds the number of tokens that the model <span class="No-Break">can accept:</span></p>
			<pre class="source-code">
from tokenizers.processors import BertProcessing
# let's make sure that the tokenizer does not provide more tokens than we expect
# we expect 512 tokens, because we will use the BERT model
tokenizer._tokenizer.post_processor = BertProcessing(
    ("&lt;/s&gt;", tokenizer.token_to_id("&lt;/s&gt;")),
    ("&lt;s&gt;", tokenizer.token_to_id("&lt;s&gt;")),
)
tokenizer.enable_truncation(max_length=512)</pre>			<p>Now, we can move over to<a id="_idIndexMarker429"/> preparing the model. We do this by importing the <a id="_idIndexMarker430"/>predefined class from the <span class="No-Break">HuggingFace hub:</span></p>
			<pre class="source-code">
import the RoBERTa configuration
from transformers import RobertaConfig
# initialize the configuration
# please note that the vocab size is the same as the one in the tokenizer.
# if it is not, we could get exceptions that the model and the tokenizer are not compatible
config = RobertaConfig(
    vocab_size=52_000,
    max_position_embeddings=514,
    num_attention_heads=12,
    num_hidden_layers=6,
    type_vocab_size=1,
)</pre>			<p>The first line, <strong class="source-inline">from transformers import RobertaConfig</strong>, imports the <strong class="source-inline">RobertaConfig</strong> class from the <strong class="source-inline">transformers</strong> library. The <strong class="source-inline">RobertaConfig</strong> class is used to configure the RoBERTa model. Next, the code initializes the configuration of the RoBERTa model. The parameters passed to the <strong class="source-inline">RobertaConfig</strong> constructor are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">vocab_size=52_000</strong>: This parameter sets the size of the vocabulary used by the RoBERTa model. It should match the vocabulary size used during the tokenizer training. In this case, the tokenizer and the model both have a vocabulary size of 52,000, ensuring they <span class="No-Break">are compatible.</span></li>
				<li><strong class="source-inline">max_position_embeddings=514</strong>: This parameter sets the maximum sequence length that the RoBERTa model can handle. It defines the maximum number of tokens in a sequence that the model can process. Longer sequences may need to be truncated or split into smaller segments. Please note that the input is 514, not 512 as the <a id="_idIndexMarker431"/>output of the tokenizer. This is caused by the fact that we leave the place<a id="_idIndexMarker432"/> from the starting and <span class="No-Break">ending tokens.</span></li>
				<li><strong class="source-inline">num_attention_heads=12</strong>: This parameter sets the number of attention heads in the <strong class="bold">multi-head attention</strong> (<strong class="bold">MHA</strong>) mechanism of the RoBERTa model. Attention heads allow the <a id="_idIndexMarker433"/>model to focus on different parts of the input <span class="No-Break">sequence simultaneously.</span></li>
				<li><strong class="source-inline">num_hidden_layers=6</strong>: This parameter sets the number of hidden layers in the RoBERTa model. These layers contain the learnable parameters of the model and are responsible for processing the <span class="No-Break">input data.</span></li>
				<li><strong class="source-inline">type_vocab_size=1</strong>: This parameter sets the size of the token type vocabulary. In models such as RoBERTa, which do not use the token type (also called a segment) embeddings, this value is typically set <span class="No-Break">to 1.</span></li>
			</ul>
			<p>The configuration object config stores all these settings and will be used later when initializing the actual RoBERTa model. Having the same configuration parameters as the tokenizer ensures that the model and tokenizer are compatible and can be used together to process text <span class="No-Break">data properly.</span></p>
			<p>It is worth noting that this model is rather small, compared to the 175 billion parameters of GPT-3. It has (only) 85 million parameters. However, it can be trained on a laptop with a moderately powerful GPU (any NVIDIA GPU with 6 GB of VRAM will do). The model is, nevertheless, much larger than the original BERT model from 2017, which had only six attention heads and a handful of millions <span class="No-Break">of parameters.</span></p>
			<p>Once the model is created, we need to <span class="No-Break">initiate it:</span></p>
			<pre class="source-code">
# Initializing a Model From Scratch
from transformers import RobertaForMaskedLM
# initialize the model
model = RobertaForMaskedLM(config=config)
# let's print the number of parameters in the model
print(model.num_parameters())
# let's print the model
print(model)</pre>			<p>The last two lines print out the number of parameters in the model (a bit over 85 million) and then the model itself. The output of that model is quite large, so we do not present <span class="No-Break">it here.</span></p>
			<p>Now that the model is ready, we<a id="_idIndexMarker434"/> need to go back to the dataset and prepare it for<a id="_idIndexMarker435"/> training. The simplest way is to reuse the previously trained tokenizer by reading it back from the folder, but with the changed class of that tokenizer so that it fits <span class="No-Break">the model:</span></p>
			<pre class="source-code">
from transformers import RobertaTokenizer
# initialize the tokenizer from the file
tokenizer = RobertaTokenizer.from_pretrained("./wolfBERTa", max_length=512)</pre>			<p>Once this is done, we can read <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
from datasets import load_dataset
new_dataset = load_dataset("text", data_files='./source_code_wolf_ssl.txt')</pre>			<p>The previous code fragment reads the same dataset that we used to train the tokenizer. Now, we will use the tokenizer to transform the dataset into a set <span class="No-Break">of tokens:</span></p>
			<pre class="source-code">
tokenized_dataset = new_dataset.map(lambda x: tokenizer(x["text"]), num_proc=8)</pre>			<p>This takes a moment, but it gives <a id="_idIndexMarker436"/>us a moment to also reflect on the fact that this code takes advantage of the so-called map-reduce algorithm, which became a golden standard for processing large files at the beginning of the 2010s when the concept of big data was very popular. It is the <strong class="source-inline">map()</strong> function that utilizes <span class="No-Break">that algorithm.</span></p>
			<p>Now, we need to prepare <a id="_idIndexMarker437"/>the dataset for training by creating so-called masked input. Masked input is a set of sentences where words are replaced by the mask token (<strong class="source-inline">&lt;mask&gt;</strong> in our case). It can look something like the example in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer103">
					<img alt="Figure 11.3 – Masked input for MLMs" src="image/B19548_11_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Masked input for MLMs</p>
			<p>It’s easy to guess that the <strong class="source-inline">&lt;mask&gt;</strong> token can appear at any place and that it should appear several times in similar places in order for the model to actually learn the masked token’s <a id="_idIndexMarker438"/>context. It would be very cumbersome to do it manually, and therefore, the HuggingFace library has a dedicated class for it – <strong class="source-inline">DataCollatorForLanguageModeling</strong>. The following code demonstrates how to instantiate that class and how to use <span class="No-Break">its parameters:</span></p>
			<pre class="source-code">
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)</pre>			<p>The <strong class="source-inline">from transformers import DataCollatorForLanguageModeling</strong> line imports the <strong class="source-inline">DataCollatorForLanguageModeling</strong> class, which is used for preparing data for language modeling tasks. The code initializes a <strong class="source-inline">DataCollatorForLanguageModeling</strong> object named <strong class="source-inline">data_collator</strong>. This object takes <span class="No-Break">several</span><span class="No-Break"><a id="_idIndexMarker439"/></span><span class="No-Break"> parameters:</span></p>
			<ul>
				<li><strong class="source-inline">tokenizer=tokenizer</strong>: This parameter specifies the tokenizer to be used for encoding and decoding the text data. It expects an instance of a <strong class="source-inline">tokenizer</strong> object. In this case, it appears that the <strong class="source-inline">tokenizer</strong> object has been previously defined and assigned to the <span class="No-Break"><strong class="source-inline">tokenizer</strong></span><span class="No-Break"> variable.</span></li>
				<li><strong class="source-inline">mlm=True</strong>: This parameter indicates that the language modeling task is an <span class="No-Break">MLM task.</span></li>
				<li><strong class="source-inline">mlm_probability=0.15</strong>: This parameter sets the probability of masking a token in the input text. Each token has a 15% chance of being masked during <span class="No-Break">data preparation.</span></li>
			</ul>
			<p>The <strong class="source-inline">data_collator</strong> object is now ready to be used for preparing data for language modeling tasks. It takes care of tasks such as tokenization and masking of the input data to be compatible with the <a id="_idIndexMarker440"/>RoBERTa model. Now, we can instantiate<a id="_idIndexMarker441"/> another helper class – <strong class="source-inline">Trainer</strong> – which manages the training process of the <span class="No-Break">MLM model:</span></p>
			<pre class="source-code">
from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir="./wolfBERTa",
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=32,
    save_steps=10_000,
    save_total_limit=2,
)
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset['train'],
)</pre>			<p>The <strong class="source-inline">from transformers import Trainer, TrainingArguments</strong> line imports the <strong class="source-inline">Trainer</strong> class and the <strong class="source-inline">TrainingArguments</strong> class from the <strong class="source-inline">transformers</strong> library. Then it initializes a <strong class="source-inline">TrainingArguments</strong> object, <strong class="source-inline">training_args</strong>. This object takes several parameters to configure the <span class="No-Break">training process:</span></p>
			<ul>
				<li><strong class="source-inline">output_dir="./wolfBERTa"</strong>: This parameter specifies the directory where the trained model and other training artifacts will <span class="No-Break">be saved.</span></li>
				<li><strong class="source-inline">overwrite_output_dir=True</strong>: This parameter determines whether to overwrite <strong class="source-inline">output_dir</strong> if it already exists. If set to <strong class="source-inline">True</strong>, it will overwrite <span class="No-Break">the directory.</span></li>
				<li><strong class="source-inline">num_train_epochs=10</strong>: This parameter sets the number of training epochs; that is, the number of times the training data will be iterated during training. In our example, it is enough with a few epochs only, such as 10. It takes a lot of time to train these models, so that’s why we go with a small number <span class="No-Break">of epochs.</span></li>
				<li><strong class="source-inline">per_device_train_batch_size=32</strong>: This parameter sets the batch size per GPU for training. It determines how many training examples are processed together in parallel during each training step. If you do not have a lot of VRAM in your GPU, decrease <span class="No-Break">this</span><span class="No-Break"><a id="_idIndexMarker442"/></span><span class="No-Break"> number.</span></li>
				<li><strong class="source-inline">save_steps=10_000</strong>: This parameter specifies the number of training steps before saving a checkpoint of <span class="No-Break">the model.</span></li>
				<li><strong class="source-inline">save_total_limit=2</strong>: This parameter limits the total number of saved checkpoints. If the limit is <a id="_idIndexMarker443"/>exceeded, older checkpoints will <span class="No-Break">be deleted.</span></li>
			</ul>
			<p>After initializing the trainer arguments, the code initializes a <strong class="source-inline">Trainer</strong> object with the <span class="No-Break">following arguments:</span></p>
			<ul>
				<li><strong class="source-inline">model=model</strong>: This parameter specifies the model to be trained. In this case, the pre-initialized RoBERTa model from our previous steps is assigned to the <span class="No-Break">model variable.</span></li>
				<li><strong class="source-inline">args=training_args</strong>: This parameter specifies the training arguments, which we prepared in our <span class="No-Break">previous steps.</span></li>
				<li><strong class="source-inline">data_collator=data_collator</strong>: This parameter specifies the data collator to be used during training. This object was prepared previously in <span class="No-Break">our code.</span></li>
				<li><strong class="source-inline">train_dataset=tokenized_dataset['train']</strong>: This parameter specifies the training dataset. It appears that a tokenized dataset has been prepared and stored in a dictionary called <strong class="source-inline">tokenized_dataset</strong>, and the training portion of that dataset is assigned to <strong class="source-inline">train_dataset</strong>. In our case, since we did not define the train-test split, it take the <span class="No-Break">entire dataset.</span></li>
			</ul>
			<p>The <strong class="source-inline">Trainer</strong> object is now ready to be used for training the RoBERTa model using the specified training arguments, data collator, and training dataset. We do this by simply <span class="No-Break">writing </span><span class="No-Break"><strong class="source-inline">trainer.train()</strong></span><span class="No-Break">.</span></p>
			<p>Once the model finishes training, we can save it using the following command: <strong class="source-inline">trainer.save_model("./wolfBERTa")</strong>. After that, we can use the model just as we learned in <a href="B19548_10.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<p>It takes a while to train the model; on a consumer-grade GPU such as NVIDIA 4090, it can take about one day for 10 epochs, but if we want to use a larger dataset or more epochs, it can take much<a id="_idIndexMarker444"/> longer. I do not advise executing this code on a computer without a GPU as it takes ca. 5-10 times longer than on a GPU. Hence my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #57</p>
			<p class="callout">Use NVIDIA <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>; accelerated computing) for training advanced <a id="_idIndexMarker445"/>models such as BERT, GPT-3, <span class="No-Break">and AEs.</span></p>
			<p>For classical ML, and even for simple NNs, a modern CPU is more than enough. The number of calculations is large, but not extreme. However, when it comes to training BERT models, AEs, and similar, we need acceleration for handling tensors (vectors) and making calculations on entire <a id="_idIndexMarker446"/>vectors at once. CUDA is NVIDIA’s acceleration framework. It allows developers to utilize the power of NVIDIA GPUs to accelerate computational tasks, including training DL models. It provides a <span class="No-Break">few benefits:</span></p>
			<ul>
				<li><strong class="bold">GPU parallelism</strong>, designed to handle many parallel computations simultaneously. DL models, especially<a id="_idIndexMarker447"/> large models such as RoBERTa, consist of millions or even billions of parameters. Training these models involves performing numerous mathematical operations, such as matrix multiplications and convolutions, on these parameters. CUDA enables these computations to be parallelized across the thousands of cores present in a GPU, greatly speeding up the training process compared to a <span class="No-Break">traditional CPU.</span></li>
				<li><strong class="bold">Optimized tensor operations for PyTorch or TensorFlow</strong>, which are designed to work seamlessly with CUDA. These frameworks provide GPU-accelerated libraries that implement optimized tensor operations specifically designed for GPUs. Tensors are multi-dimensional arrays used to store and manipulate data in DL models. With CUDA, these tensor operations can be efficiently executed on the GPU, leveraging its high memory bandwidth and parallel <span class="No-Break">processing capabilities.</span></li>
				<li><strong class="bold">High memory bandwidth</strong>, which enables data to be transferred to and from the GPU memory at a much faster rate, enabling faster data processing during training. DL models often require large amounts of data to be loaded and processed in batches. CUDA allows these batches to be efficiently transferred and processed on the GPU, reducing <span class="No-Break">training time.</span></li>
			</ul>
			<p>By utilizing CUDA, DL frameworks <a id="_idIndexMarker448"/>can effectively leverage the parallel computing capabilities and optimized operations of NVIDIA GPUs, resulting in significant <a id="_idIndexMarker449"/>acceleration of the training process for large-scale models such <span class="No-Break">as RoBERTa.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor139"/>Training and evaluation of an AE</h1>
			<p>We mentioned AEs in <a href="B19548_07.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> when we discussed the process of feature engineering for images. AEs, however, are used to do much more than just image feature extraction. One of the major aspects of them is to be <a id="_idIndexMarker450"/>able to recreate images. This means that we can create images based on the placement of the image in the <span class="No-Break">latent space.</span></p>
			<p>So, let us train the AE model for a dataset that is pretty standard in ML – Fashion MNIST. We got to see what the dataset looks like in our previous chapters. We start our training by preparing the data in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# Transforms images to a PyTorch Tensor
tensor_transform = transforms.ToTensor()
# Download the Fashion MNIST Dataset
dataset = datasets.FashionMNIST(root = "./data",
                         train = True,
                         download = True,
                         transform = tensor_transform)
# DataLoader is used to load the dataset
# for training
loader = torch.utils.data.DataLoader(dataset = dataset,
                                     batch_size = 32,
                                     shuffle = True)</pre>			<p>It imports the necessary modules from the <span class="No-Break">PyTorch library.</span></p>
			<p>It defines a transformation called <strong class="source-inline">tensor_transform</strong> using <strong class="source-inline">transforms.ToTensor()</strong>. This transformation is used to convert images in the dataset to <span class="No-Break">PyTorch tensors.</span></p>
			<p>The code fragment downloads the dataset using the <strong class="source-inline">datasets.FashionMNIST()</strong> function. The <strong class="source-inline">train</strong> parameter is set to <strong class="source-inline">True</strong> to indicate that the downloaded dataset is for training purposes. The <strong class="source-inline">download</strong> parameter is set to <strong class="source-inline">True</strong> to automatically download the dataset if it is not already present in the <span class="No-Break">specified directory.</span></p>
			<p>Since we use the PyTorch framework with accelerated computing, we need to make sure that the image is<a id="_idIndexMarker451"/> transformed into a tensor. The <strong class="source-inline">transform</strong> parameter is set to <strong class="source-inline">tensor_transform</strong>, which is a transformer defined in the first line of the <span class="No-Break">code fragment.</span></p>
			<p>Then, we create a <strong class="source-inline">DataLoader</strong> object used to load the dataset in batches for training. The <strong class="source-inline">dataset</strong> parameter is set to the previously downloaded dataset. The <strong class="source-inline">batch_size</strong> parameter is set to <strong class="source-inline">32</strong>, indicating that each batch of the dataset will contain <span class="No-Break">32 images.</span></p>
			<p>The <strong class="source-inline">shuffle</strong> parameter is set to <strong class="source-inline">True</strong> to shuffle the order of the samples in each epoch of training, ensuring randomization and reducing any potential bias <span class="No-Break">during training.</span></p>
			<p>Once we have the dataset prepared, we can create our AE, which we do <span class="No-Break">like this:</span></p>
			<pre class="source-code">
# Creating a PyTorch class
# 28*28 ==&gt; 9 ==&gt; 28*28
class AE(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # Building an linear encoder with Linear
        # layer followed by Relu activation function
        # 784 ==&gt; 9
        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(28 * 28, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 36),
            torch.nn.ReLU(),
            torch.nn.Linear(36, 18),
            torch.nn.ReLU(),
            torch.nn.Linear(18, 9)
        )
        # Building an linear decoder with Linear
        # layer followed by Relu activation function
        # The Sigmoid activation function
        # outputs the value between 0 and 1
        # 9 ==&gt; 784
        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(9, 18),
            torch.nn.ReLU(),
            torch.nn.Linear(18, 36),
            torch.nn.ReLU(),
            torch.nn.Linear(36, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 28 * 28),
            torch.nn.Sigmoid()
        )
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded</pre>			<p>First, we define a class named <strong class="source-inline">AE</strong> that inherits from the <strong class="source-inline">torch.nn.Module</strong> class, which is the base class for all NN modules in PyTorch. The <strong class="source-inline">super().__init__()</strong> line ensures that the initialization of the base class (<strong class="source-inline">torch.nn.Module</strong>) is called. Since AEs are a special kind of NN class with backpropagation learning, we can just inherit a lot of basic functionality from <span class="No-Break">the library.</span></p>
			<p>Then, we define the encoder part of the AE. The encoder consists of several linear (fully connected) layers with<a id="_idIndexMarker452"/> ReLU activation functions. Each <strong class="source-inline">torch.nn.Linear</strong> layer represents a linear transformation of the input data followed by an activation function. In this case, the input size is 28 * 28 (which corresponds to the dimensions of an image in the Fashion MNIST dataset), and the output size gradually decreases until it reaches 9, which is our latent <span class="No-Break">vector size.</span></p>
			<p>Then, we define the decoder part of the AE. The decoder is responsible for reconstructing the input data from the encoded representation. It consists of several linear layers with ReLU activation functions, followed by a final linear layer with a sigmoid activation function. The input size of the decoder is 9, which corresponds to the size of the latent vector space in the bottleneck of the encoder. The output size is 28 * 28, which matches the dimensions of the original <span class="No-Break">input data.</span></p>
			<p>The <strong class="source-inline">forward</strong> method defines the forward pass of the AE. It takes an <strong class="source-inline">x</strong> input and passes it through the encoder to <a id="_idIndexMarker453"/>obtain an encoded representation. Then, it passes the encoded representation through the decoder to reconstruct the input data. The reconstructed output is returned as the result. We are now ready to instantiate <span class="No-Break">our AE:</span></p>
			<pre class="source-code">
# Model Initialization
model = AE()
# Validation using MSE Loss function
loss_function = torch.nn.MSELoss()
# Using an Adam Optimizer with lr = 0.1
optimizer = torch.optim.Adam(model.parameters(),
                             lr = 1e-1,
                             weight_decay = 1e-8)</pre>			<p>In this code, we first instantiate our AE as our model. Then, we create an instance of the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) loss function<a id="_idIndexMarker454"/> provided by PyTorch. MSE is a commonly used loss function for regression tasks. We need it to calculate the mean squared difference between the predicted values and the target values – which are the individual pixels in our dataset, providing a measure of how well the model is performing. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.4</em> shows the role of the learning function in the process of training <span class="No-Break">the AE:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 11.4 – Loss function (MSE) in the AE training process" src="image/B19548_11_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Loss function (MSE) in the AE training process</p>
			<p>Then, we initialize the optimizer used to update the model’s parameters during training. In this case, the code creates an <a id="_idIndexMarker455"/>Adam optimizer, which is a popular optimization algorithm for training NNs. It takes three <span class="No-Break">important arguments:</span></p>
			<ul>
				<li><strong class="source-inline">model.parameters()</strong>: This specifies the parameters that will be optimized. In this case, it includes all the parameters of the model (the AE) that we <span class="No-Break">created earlier.</span></li>
				<li><strong class="source-inline">lr=1e-1</strong>: This sets the learning rate, which determines the step size at which the optimizer updates the parameters. A higher learning rate can lead to faster convergence but may risk overshooting the optimal solution, while a lower learning rate may converge more slowly but with potentially <span class="No-Break">better accuracy.</span></li>
				<li><strong class="source-inline">weight_decay=1e-8</strong>: This parameter adds a weight decay regularization term to the optimizer. Weight decay helps prevent overfitting by adding a penalty term to the loss function that discourages large weights. The <strong class="source-inline">1e-8</strong> value represents the weight <span class="No-Break">decay coefficient.</span></li>
			</ul>
			<p>With this code, we have now an<a id="_idIndexMarker456"/> instance of an AE to train. Now, we can start the process of training. We train the model for 10 epochs, but we can try more <span class="No-Break">if needed:</span></p>
			<pre class="source-code">
epochs = 10
outputs = []
losses = []
for epoch in range(epochs):
    for (image, _) in loader:
      # Reshaping the image to (-1, 784)
      image = image.reshape(-1, 28*28)
      # Output of Autoencoder
      reconstructed = model(image)
      # Calculating the loss function
      loss = loss_function(reconstructed, image)
      # The gradients are set to zero,
      # the gradient is computed and stored.
      # .step() performs parameter update
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      # Storing the losses in a list for plotting
      losses.append(loss)
    outputs.append((epochs, image, reconstructed))</pre>			<p>We start by iterating over the specified number of epochs for the training. Within each epoch, we iterate over the <a id="_idIndexMarker457"/>loader, which provides batches of image data and their corresponding labels. We do not use the labels, because the AE is a network to recreate images and not to learn what the images show – in that sense, it is an <span class="No-Break">unsupervised model.</span></p>
			<p>For each image, we reshape the input image data by flattening each image, originally in the shape of <strong class="source-inline">(batch_size, 28, 28)</strong>, into a 2D tensor of shape <strong class="source-inline">(batch_size, 784)</strong>, where each row represents a flattened image. The flattened image is created when we take each row of pixels and concatenate it to create one large vector. It is needed as the images are<a id="_idIndexMarker458"/> two-dimensional, while our tensor input needs to be of a <span class="No-Break">single dimension.</span></p>
			<p>Then, we obtain the reconstructed image using <strong class="source-inline">reconstructed = model(image)</strong>. Once we get the reconstructed image, we can calculate the MSE loss function and use that information to manage the next step of the learning (<strong class="source-inline">optimizer.zero_grad()</strong>). In the last line, we add this information to the list of losses per iteration so that we can create a learning diagram. We do it by using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# Defining the Plot Style
plt.style.use('seaborn')
plt.xlabel('Iterations')
plt.ylabel('Loss')
# Convert the list to a PyTorch tensor
losses_tensor = torch.tensor(losses)
plt.plot(losses_tensor.detach().numpy()[::-1])</pre>			<p>This results in a learning diagram, shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 11.5 – Learning rate diagram from training our AE" src="image/B19548_11_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Learning rate diagram from training our AE</p>
			<p>The learning rate diagram shows<a id="_idIndexMarker459"/> that the AE is not really great yet and that we should train it a bit more. However, we can always check what the recreated images look like. We can do that using <span class="No-Break">this code:</span></p>
			<pre class="source-code">
for i, item in enumerate(image):
  # Reshape the array for plotting
  item = item.reshape(-1, 28, 28)
  plt.imshow(item[0])</pre>			<p>The code results in the output shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="Figure 11.6 – Recreated image from our AE" src="image/B19548_11_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Recreated image from our AE</p>
			<p>Despite the learning rate, which<a id="_idIndexMarker460"/> is OK, we still can get very good results from <span class="No-Break">our AEs.</span></p>
			<p class="callout-heading">Best practice #58</p>
			<p class="callout">In addition to monitoring the loss, make sure to visualize the actual results of <span class="No-Break">the generation.</span></p>
			<p>Monitoring the loss function is a good way to understand when the AE stabilizes. However, just the loss function is not enough. I usually plot the actual output to understand whether the AE has been <span class="No-Break">trained correctly.</span></p>
			<p>Finally, we can visualize the learning process when we use <span class="No-Break">this code:</span></p>
			<pre class="source-code">
yhat = model(image[0])
make_dot(yhat,
         params=dict(list(model.named_parameters())),
         show_attrs=True,
         show_saved=True)</pre>			<p>This code visualizes the learning process of the entire network. It creates a large image, and we can only show a small excerpt of it. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.7</em> shows <span class="No-Break">this excerpt:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 11.7 – The first three steps in training an AE, visualized as the AE architecture" src="image/B19548_11_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – The first three steps in training an AE, visualized as the AE architecture</p>
			<p>We can even visualize the <a id="_idIndexMarker461"/>entire architecture in a text form by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from torchsummary import summary
summary(model, (1, 28 * 28))</pre>			<p>This results in the <span class="No-Break">following model:</span></p>
			<pre class="source-code">
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1               [-1, 1, 128]         100,480
              ReLU-2               [-1, 1, 128]               0
            Linear-3                [-1, 1, 64]           8,256
              ReLU-4                [-1, 1, 64]               0
            Linear-5                [-1, 1, 36]           2,340
              ReLU-6                [-1, 1, 36]               0
            Linear-7                [-1, 1, 18]             666
              ReLU-8                [-1, 1, 18]               0
<strong class="bold">            Linear-9                 [-1, 1, 9]             171</strong>
           Linear-10                [-1, 1, 18]             180
             ReLU-11                [-1, 1, 18]               0
           Linear-12                [-1, 1, 36]             684
             ReLU-13                [-1, 1, 36]               0
           Linear-14                [-1, 1, 64]           2,368
             ReLU-15                [-1, 1, 64]               0
           Linear-16               [-1, 1, 128]           8,320
             ReLU-17               [-1, 1, 128]               0
           Linear-18               [-1, 1, 784]         101,136
          Sigmoid-19               [-1, 1, 784]               0
================================================================
Total params: 224,601
Trainable params: 224,601
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
----------------------------------------------------------------</pre>			<p>The bottleneck layer is in boldface, to illustrate the place where the encode and decode parts are linked to <span class="No-Break">one</span><span class="No-Break"><a id="_idIndexMarker462"/></span><span class="No-Break"> another.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor140"/>Developing safety cages to prevent models from breaking the entire system</h1>
			<p>As GenAI systems such<a id="_idIndexMarker463"/> as MLMs and AEs create new content, there is a risk that they generate content that can either break the entire software system or <span class="No-Break">become unethical.</span></p>
			<p>Therefore, software engineers often use the concept of a safety cage to guard the model itself from inappropriate input and output. For an MLM such as RoBERTa, this can be a simple preprocessor that checks whether the content generated is problematic. Conceptually, this is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 11.8 – Safety-cage concept for MLMs" src="image/B19548_11_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Safety-cage concept for MLMs</p>
			<p>In the example of the <strong class="source-inline">wolfBERTa</strong> model, this can mean that we check whether the generated code does not contain cybersecurity vulnerabilities, which can potentially allow hackers to take over our system. This means that all programs generated by the <strong class="source-inline">wolfBERTa</strong> model should be checked using tools such as SonarQube or CodeSonar to check for cybersecurity vulnerabilities, hence my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #59</p>
			<p class="callout">Check the output of GenAI models so that it does not break the entire system or provide <span class="No-Break">unethical responses.</span></p>
			<p>My recommendation to create such safety cages is to start from the requirements of the system. The first step is to understand what the system is going to do and understand which dangers and<a id="_idIndexMarker464"/> risks this task entails. The safety cage’s output processor should ensure that these dangerous situations do not occur and that they are <span class="No-Break">handled properly.</span></p>
			<p>Once we understand how to prevent dangers, we can move over to conceptualizing how to prevent these risks on the language-model level. For example, when we train the model, we can select code that is known to be secure and does not contain security vulnerabilities. Although it does not guarantee that the model generates secure code, it certainly reduces the risk <span class="No-Break">for it.</span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor141"/>Summary</h1>
			<p>In this chapter, we learned how to train advanced models and saw that their training is not much more difficult than training classical ML models, which were described in <a href="B19548_10.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. Even though the models that we trained are much more complex than the models in <a href="B19548_10.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we can use the same principles and expand this kind of activity to train even more <span class="No-Break">complex models.</span></p>
			<p>We focused on GenAI in the form of BERT models (fundamental GPT models) and AEs. Training these models is not very difficult, and we do not need huge computing power to train them. Our <strong class="source-inline">wolfBERTa</strong> model has ca. 80 million parameters, which seems like a lot, but the really good models, such as GPT-3, have billions of parameters – GPT-3 has 175 billion parameters, NVIDIA Turing has over 350 billion parameters, and GPT-4 is 1,000 times larger than GPT-3. The training process is the same, but we need a supercomputing architecture in order to train <span class="No-Break">these models.</span></p>
			<p>We have also learned that these models are only parts of larger software systems. In the next chapter, we learn how to create such a <span class="No-Break">larger system.</span></p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor142"/>References</h1>
			<ul>
				<li><em class="italic">Kratsch, W. et al., Machine learning in business process monitoring: a comparison of deep learning and classical approaches used for outcome prediction. Business &amp; Information Systems Engineering, 2021, 63: </em><span class="No-Break"><em class="italic">p. 261-276.</em></span></li>
				<li><em class="italic">Vaswani, A. et al., Attention is all you need. Advances in neural information processing systems, </em><span class="No-Break"><em class="italic">2017, 30.</em></span></li>
				<li><em class="italic">Aggarwal, A., M. Mittal, and G. Battineni, Generative adversarial network: An overview of theory and applications. International Journal of Information Management Data Insights, 2021. 1(1): </em><span class="No-Break"><em class="italic">p. 100004.</em></span></li>
				<li><em class="italic">Creswell, A., et al., Generative adversarial networks: An overview. IEEE signal processing magazine, 2018. 35(1): </em><span class="No-Break"><em class="italic">p. 53-65.</em></span></li>
			</ul>
		</div>
	</body></html>