["```py\nfrom sklearn.model_selection import train_test_split\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\n>>> lr = LogisticRegression()\n>>> lr.fit(X_train, Y_train)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n verbose=0, warm_start=False)\n\n>>> lr.score(X_test, Y_test)\n0.95199999999999996\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\n>>> cross_val_score(lr, X, Y, scoring='accuracy', cv=10)\narray([ 0.96078431,  0.92156863,  0.96      ,  0.98      ,  0.96      ,\n 0.98      ,  0.96      ,  0.96      ,  0.91836735,  0.97959184])\n\n```", "```py\n>>> lr.intercept_\narray([-0.64154943])\n\n>>> lr.coef_\narray([[ 0.34417875,  3.89362924]])\n```", "```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 500\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n```", "```py\nfrom sklearn.linear_model import SGDClassifier\n\n>>> sgd = SGDClassifier(loss='perceptron', learning_rate='optimal', n_iter=10)\n>>> cross_val_score(sgd, X, Y, scoring='accuracy', cv=10).mean()\n0.98595918367346935\n```", "```py\nfrom sklearn.linear_model import Perceptron\n\n>>> perc = Perceptron(n_iter=10)\n>>> cross_val_score(perc, X, Y, scoring='accuracy', cv=10).mean()\n0.98195918367346935\n```", "```py\nimport multiprocessing\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV\n\n>>> iris = load_iris()\n\n>>> param_grid = [\n { \n 'penalty': [ 'l1', 'l2' ],\n 'C': [ 0.5, 1.0, 1.5, 1.8, 2.0, 2.5]\n }\n]\n\n>>> gs = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid,\n scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\n>>> gs.fit(iris.data, iris.target)\nGridSearchCV(cv=10, error_score='raise',\n estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n verbose=0, warm_start=False),\n fit_params={}, iid=True, n_jobs=8,\n param_grid=[{'penalty': ['l1', 'l2'], 'C': [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5]}],\n pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n scoring='accuracy', verbose=0)\n\n>>> gs.best_estimator_\nLogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n verbose=0, warm_start=False)\n\n>>> cross_val_score(gs.best_estimator_, iris.data, iris.target, scoring='accuracy', cv=10).mean()\n0.96666666666666679\n```", "```py\nimport multiprocessing\n\nfrom sklearn.model_selection import GridSearchCV\n\n>>> param_grid = [\n { \n 'penalty': [ 'l1', 'l2', 'elasticnet' ],\n 'alpha': [ 1e-5, 1e-4, 5e-4, 1e-3, 2.3e-3, 5e-3, 1e-2],\n 'l1_ratio': [0.01, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.75, 0.8]\n }\n]\n\n>>> sgd = SGDClassifier(loss='perceptron', learning_rate='optimal')\n>>> gs = GridSearchCV(estimator=sgd, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\n>>> gs.fit(X, Y)\nGridSearchCV(cv=10, error_score='raise',\n estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n learning_rate='optimal', loss='perceptron', n_iter=5, n_jobs=1,\n penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n verbose=0, warm_start=False),\n fit_params={}, iid=True, n_jobs=8,\n param_grid=[{'penalty': ['l1', 'l2', 'elasticnet'], 'alpha': [1e-05, 0.0001, 0.0005, 0.001, 0.0023, 0.005, 0.01], 'l1_ratio': [0.01, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.75, 0.8]}],\n pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n scoring='accuracy', verbose=0)\n\n>>> gs.best_score_\n0.89400000000000002\n\n>>> gs.best_estimator_\nSGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n eta0=0.0, fit_intercept=True, l1_ratio=0.1, learning_rate='optimal',\n loss='perceptron', n_iter=5, n_jobs=1, penalty='elasticnet',\n power_t=0.5, random_state=None, shuffle=True, verbose=0,\n warm_start=False)\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\n>>> accuracy_score(Y_test, lr.predict(X_test))\n0.94399999999999995\n```", "```py\nfrom sklearn.metrics import zero_one_loss\n\n>>> zero_one_loss(Y_test, lr.predict(X_test))\n0.05600000000000005\n\n>>> zero_one_loss(Y_test, lr.predict(X_test), normalize=False)\n7L\n```", "```py\nfrom sklearn.metrics import jaccard_similarity_score\n\n>>> jaccard_similarity_score(Y_test, lr.predict(X_test))\n0.94399999999999995\n```", "```py\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n>>> lr = LogisticRegression()\n>>> lr.fit(X_train, Y_train)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n verbose=0, warm_start=False)\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\n>>> cm = confusion_matrix(y_true=Y_test, y_pred=lr.predict(X_test))\ncm[::-1, ::-1]\n[[50  5]\n [ 2 68]]\n```", "```py\nfrom sklearn.metrics import precision_score\n\n>>> precision_score(Y_test, lr.predict(X_test))\n0.96153846153846156\n```", "```py\nfrom sklearn.metrics import recall_score\n\n>>> recall_score(Y_test, lr.predict(X_test))\n0.90909090909090906\n```", "```py\nfrom sklearn.metrics import fbeta_score\n\n>>> fbeta_score(Y_test, lr.predict(X_test), beta=1)\n0.93457943925233655\n\n>>> fbeta_score(Y_test, lr.predict(X_test), beta=0.75)\n0.94197437829691033\n\n>>> fbeta_score(Y_test, lr.predict(X_test), beta=1.25)\n0.92886270956048933\n```", "```py\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n\n>>> lr = LogisticRegression()\n>>> lr.fit(X_train, Y_train)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n verbose=0, warm_start=False)\n\n>>> Y_scores = lr.decision_function(X_test)\n```", "```py\nfrom sklearn.metrics import roc_curve\n\n>>> fpr, tpr, thresholds = roc_curve(Y_test, Y_scores)\n```", "```py\nfrom sklearn.metrics import auc\n\n>>> auc(fpr, tpr)\n0.96961038961038959\n```", "```py\nimport matplotlib.pyplot as plt\n\n>>> plt.figure(figsize=(8, 8))\n>>> plt.plot(fpr, tpr, color='red', label='Logistic regression (AUC: %.2f)' % auc(fpr, tpr))\n>>> plt.plot([0, 1], [0, 1], color='blue', linestyle='--')\n>>> plt.xlim([0.0, 1.0])\n>>> plt.ylim([0.0, 1.01])\n>>> plt.title('ROC Curve')\n>>> plt.xlabel('False Positive Rate')\n>>> plt.ylabel('True Positive Rate')\n>>> plt.legend(loc=\"lower right\")\n>>> plt.show()\n```"]