- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Validity and Efficiency of Conformal Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will dive deeper into the concepts of validity and efficiency
    in the context of probabilistic prediction models, building upon the foundations
    laid in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Validity and efficiency are crucial aspects that ensure the practicality and
    robustness of prediction models across a wide range of industry applications.
    Understanding these concepts and their implications will enable you to develop
    unbiased and high-performing models that can reliably support decision-making
    and risk assessment processes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the definitions, metrics, and examples of valid
    and efficient models and discuss the automatic validity guarantees provided by
    **conformal prediction**, a cutting-edge approach to uncertainty quantification.
    By the end of this chapter, you will be equipped with the knowledge necessary
    to assess and improve the validity and efficiency of your predictive models, paving
    the way for more reliable and effective applications in your respective fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The validity of probabilistic predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficiency of probabilistic predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validity of probabilistic predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by summarizing the reasons why unbiased point prediction models are
    important across various domains and applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy and reliability**: An unbiased model ensures that the predictions
    it generates are accurate and reliable on average, meaning that the model is neither
    systematically overestimating nor underestimating the true values. This accuracy
    is crucial for making well-informed decisions, minimizing risks, and improving
    the overall performance of a system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trust and credibility**: Unbiased prediction models help build trust and
    credibility among stakeholders, as they provide a reliable basis for decision-making.
    Users can have more confidence in the outputs generated by an unbiased model,
    knowing that it is not skewed or favoring any specific outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness and equity**: In some applications, such as finance, healthcare,
    and human resources, unbiased models are essential to ensure fairness and equity
    among different groups or individuals. Biased models can inadvertently reinforce
    existing inequalities or create new ones, leading to unfair treatment or allocation
    of resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalizability**: Unbiased models are more likely to generalize well to
    new, unseen data because they accurately represent the underlying relationships
    in the data. In contrast, biased models may perform poorly when applied to new
    data or different conditions, leading to unexpected errors or suboptimal outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory compliance**: In certain industries, unbiased models are a requirement
    for regulatory compliance. For example, in finance, healthcare, and insurance,
    models must be free of bias to meet regulatory standards and ensure that customers
    are treated fairly and risks are managed effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of conformal prediction, validity refers to the ability of a
    prediction model to provide accurate, reliable, and unbiased estimates of the
    uncertainty associated with its predictions. More specifically, a valid conformal
    predictor generates prediction intervals that contain the true values of the target
    variable with a predefined coverage probability, ensuring that the model’s uncertainty
    quantification is reliable and well calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The significance of validity in conformal prediction can be understood from
    various perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence in** **predictions**: Valid conformal predictors allow users to
    have confidence in the prediction intervals they generate, as they know that these
    intervals truly reflect the uncertainty in the predictions. For instance, if a
    conformal predictor produces a 95% prediction interval for a certain data point,
    users can trust that there is a 95% probability that the true value lies within
    that interval. This confidence is crucial for decision-making and risk management
    in various applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness to model misspecification**: One of the key strengths of conformal
    prediction is its ability to provide valid uncertainty estimates even when the
    underlying prediction model is misspecified or imperfect. This robustness to model
    misspecification is particularly valuable in real-world settings, where the true
    data-generating process is often unknown or complex and the available models may
    only provide approximations of the underlying relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-parametric nature**: Conformal prediction is a non-parametric method,
    meaning that it does not rely on any specific assumptions about the data distribution
    or prediction errors. This non-parametric property further contributes to the
    validity of conformal predictors, as they can adapt to different data structures
    and provide accurate uncertainty estimates without requiring explicit knowledge
    of the underlying distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applicability across domains**: The validity of conformal prediction is a
    universal property that holds across various domains and applications. This universality
    allows practitioners to leverage conformal prediction in diverse fields, such
    as finance, healthcare, energy, and transportation, knowing that the uncertainty
    estimates provided by conformal predictors will be valid and reliable regardless
    of the specific context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic validity guarantees**: A key advantage of conformal prediction
    over traditional methods is its ability to provide automatic validity guarantees,
    meaning that the uncertainty estimates it produces are guaranteed to be valid
    under mild assumptions, such as the exchangeability of the data. This automatic
    validity ensures that conformal predictors maintain their reliability even as
    new data points are added or as the underlying relationships evolve over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conformal prediction, validity is mathematically defined in terms of the
    coverage probability of the prediction intervals or regions generated by the conformal
    predictor. A conformal predictor is valid if, for any desired confidence level
    *(1−α)*, the proportion of true target values contained within their corresponding
    prediction intervals is at least *(1−α)*, on average, across multiple instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, let’s denote the target variable as *Y* and the prediction
    interval (in regression) or set (in classification) as *I(x, α)*, where *x* represents
    the features of the test data point and *α* is the significance level *(α* ∈ *[0,
    1])*. The conformal predictor is valid if the following condition holds:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Y ∈ I(x, α)) ≥ 1 – α
  prefs: []
  type: TYPE_NORMAL
- en: This condition states that the probability that the true target value *Y* is
    within the prediction interval *I(x, α)* is at least *(1−α)* for any given input
    data point *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Validity in conformal prediction is closely related to the calibration concept
    in probabilistic prediction. A calibrated predictor generates prediction intervals
    with the correct coverage probability, ensuring that the uncertainty estimates
    it provides are well aligned with the true underlying uncertainty in the data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that validity in conformal prediction is guaranteed
    under the assumption of exchangeability, which requires that the observed data
    points are exchangeable with future, unseen data points. This assumption holds
    for **independent and identically distributed** (**IID**) data. In addition, successful
    modifications of conformal prediction have been developed to address non-exchangeable
    data, including many successful models for time series.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier calibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classifier calibration ensures that the predicted probabilities of an event
    match the true probabilities or frequencies of that event occurring. For example,
    in weather forecasting, calibration ensures that the forecasted probability of
    rain aligns with the actual occurrence of rain over a series of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of classifier calibration has been applied to weather forecasting
    since the 1950s, pioneered by Glen Brier. In the case of rain forecasting, a forecaster
    might declare an 80% chance of rain. If, on average, rain occurs 60% of the time
    following such statements, we consider the forecast well calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a weather forecaster who makes a statement that there is an *x*%
    chance of rain for a particular day. To assess the calibration of the forecaster’s
    predictions, we would collect a series of similar predictions along with their
    corresponding outcomes (whether it rained or not). For example, suppose we gather
    100 instances in which the forecaster predicted a 60% chance of rain. If the forecaster’s
    predictions are well calibrated, it should rain on approximately 60 of those 100
    days, resulting in an observed frequency of rain that matches the predicted probability.
  prefs: []
  type: TYPE_NORMAL
- en: But what would this mean in practice? Let’s consider an example to understand
    this concept better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, over time, a weather forecaster made 10 predictions of an *x*% chance
    of rain. In the following table, we show these forecasts and the actual outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Forecasted probability** **of rain** | **Actual** **outcome (rain)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 80% | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 60% | No |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 90% | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 30% | No |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 70% | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 50% | No |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 80% | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 20% | No |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 40% | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 60% | Yes |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Forecasted probability of rain versus actual outcome
  prefs: []
  type: TYPE_NORMAL
- en: To determine whether this forecast is well calibrated, we need to compare the
    forecasted rain probabilities with the actual outcomes for each probability level.
    We can group the forecasts by their probability levels and calculate the observed
    frequency of rain for each group.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Chance** **of rain** | 20% | 30% | 40% | 50% | 60% | 70% | 80% | 90% |'
  prefs: []
  type: TYPE_TB
- en: '| **Predicted** | 1 day | 1 day | 1 day | 1 day | 2 days | 1 day | 2 days |
    1 day |'
  prefs: []
  type: TYPE_TB
- en: '| **Rained** | 0 days | 0 days | 1 day | 0 days | 1 day | 1 day | 2 days |
    1 day |'
  prefs: []
  type: TYPE_TB
- en: '| **Observed** **frequency** | 0% | 0% | 100% | 0% | 50% | 100% | 100% | 100%
    |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Forecasted probability of rain versus observed frequency
  prefs: []
  type: TYPE_NORMAL
- en: Based on the observed frequencies, we can see that the forecast is not well
    calibrated. The observed frequencies do not align with the forecasted probabilities
    for most probability levels. For example, on the two days with a 60% chance of
    rain, it only rained once (50% of the predicted frequency), and on the day with
    a 50% chance of rain, it didn’t rain at all (0% the predicted frequency).
  prefs: []
  type: TYPE_NORMAL
- en: How can we aggregate these results into certain metrics? We could use the **Brier
    score**, which we encountered in previous chapters. The Brier score is a commonly
    used calibration metric for binary classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the Brier score is calculated as the mean squared difference between
    the predicted probabilities and the true binary outcomes (0 or 1):'
  prefs: []
  type: TYPE_NORMAL
- en: Brier score = (1 / N) ∑ (prediction _ i − outcome _ i) ^ 2
  prefs: []
  type: TYPE_NORMAL
- en: where *N* is the number of predictions, *prediction_i* is the predicted probability
    for the *i*-th instance, and *outcome_i* is the true binary outcome for the *i*-th
    instance (1 for rain, 0 for no rain).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can compute the Brier score:'
  prefs: []
  type: TYPE_NORMAL
- en: Brier score = (1 / 10) * (0.04 + 0.36 + 0.01 + 0.09 + 0.09 + 0.25 + 0.04 + 0.04
    + 0.36 + 0.16) = 0.144
  prefs: []
  type: TYPE_NORMAL
- en: A lower Brier score indicates better model performance and, consequently, better
    calibration. However, with a reference or comparison to other models, it is easier
    to determine whether a Brier score of 0.144 is good or not. Additionally, it is
    important to remember that this assessment is based on a limited sample size of
    only 10 days, which may not provide an accurate representation of the forecast’s
    calibration over a longer period.
  prefs: []
  type: TYPE_NORMAL
- en: We can also create a calibration diagram, also known as a reliability diagram,
    to assess the calibration of probabilistic prediction models. The diagram plots
    the predicted probabilities (grouped into bins) on the *x* axis against the observed
    frequencies of the event on the *y* axis. A well-calibrated model would have points
    along the diagonal (45-degree angle), indicating that the predicted probabilities
    match the observed frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from *Table 4.2*, the forecast is not well calibrated. The observed
    frequencies do not match the forecasted probabilities for most probability levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of miscalibration: *underconfidence* and *overconfidence*.
    When a classifier exhibits underconfidence, it underestimates its ability to distinguish
    between classes, performing better in practice than its predictions suggest. In
    contrast, an overconfident classifier overestimates its capacity to separate classes,
    performing worse than its predicted probabilities imply.'
  prefs: []
  type: TYPE_NORMAL
- en: Another metric that can be used to evaluate calibration is log loss, also known
    as logarithmic loss or cross-entropy, which is a metric used to evaluate the performance
    of classification models that produce probability estimates for each class. It
    measures the divergence between the true and predicted probability distributions,
    penalizing incorrect and uncertain predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of log loss is based on the idea that a classifier should not only
    predict the correct class but also be confident in its prediction. Log loss quantifies
    the uncertainty in the predicted probabilities by comparing them with the actual
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For binary classification, log loss is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Log loss = − (y * log(p) + (1 − y) * log(1 − p))
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, *y* stands for the true class label, which can be either 0
    or 1\. The symbol *p* signifies the predicted probability for the positive class
    (class 1). The term *log* indicates the natural logarithm. The log loss is calculated
    for each instance and then averaged across all instances to obtain the final log
    loss value.
  prefs: []
  type: TYPE_NORMAL
- en: In a calibration context, log loss can be used to assess how well the predicted
    probabilities match the true outcomes. A well-calibrated model will have a lower
    log loss, as the predicted probabilities will be closer to the actual class labels.
    Conversely, a poorly calibrated model will have a higher log loss, indicating
    a discrepancy between the predicted probabilities and the true outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that log loss alone might not be sufficient to assess
    calibration, as it also depends on the classification accuracy. However, when
    used in conjunction with other metrics, such as calibration diagrams, log loss
    can provide valuable insights into the calibration of a classification model.
    In practice, log loss is often used alongside the Brier score to evaluate a model’s
    calibration. When both metrics agree on the relative calibration of two models,
    this provides stronger evidence than relying on a single calibration metric, such
    as log loss or Brier loss, alone. A more comprehensive assessment of a model’s
    calibration can be achieved by considering both metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the rain prediction example from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the log loss for this example, we will use this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Log loss = − (y * log(p) + (1 − y) * log(1 − p))
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* is the true class label (0 or 1) and *p* is the predicted probability
    of the positive class (rain).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compute the log loss for each day:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Day 1: (1 * log(0.8) + (1 - 1) * log(1 - 0.8)) = 0.223'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 2: (0 * log(0.8) + (1 - 0) * log(1 - 0.8)) = 1.609'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 3: (1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = 0.511'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 4: (1 * log(0.7) + (1 - 1) * log(1 - 0.7)) = 0.357'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 5: (1 * log(0.9) + (1 - 1) * log(1 - 0.9)) = 0.105'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 6: (0 * log(0.7) + (1 - 0) * log(1 - 0.7)) = 1.204'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 7: (1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = 0.511'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 8: (1 * log(0.5) + (1 - 1) * log(1 - 0.5)) = 0.693'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 9: (0 * log(0.6) + (1 - 0) * log(1 - 0.6)) = 0.916'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day 10: (0 * log(0.4) + (1 - 0) * log(1 - 0.4)) = 0.511'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can compute the average log loss across all 10 days:'
  prefs: []
  type: TYPE_NORMAL
- en: Average log loss = (0.223 + 1.609 + 0.511 + 0.357 + 0.105 + 1.204 + 0.511 +
    0.693 + 0.916 + 0.511) / 10 ≈ 0.664
  prefs: []
  type: TYPE_NORMAL
- en: The average log loss for this rain prediction example is approximately 0.664.
  prefs: []
  type: TYPE_NORMAL
- en: 'A question naturally arises: of statistical, machine, and deep learning, which
    are well calibrated and which are not?'
  prefs: []
  type: TYPE_NORMAL
- en: As a general guideline, it is important to remember that most machine learning
    models are miscalibrated to some extent, with varying degrees of severity. However,
    logistic regression has its own limitations and may only be suitable for some
    applications due to its relatively simpler modeling capacity.
  prefs: []
  type: TYPE_NORMAL
- en: A classical study of calibration is the paper *Predicting Good Probabilities
    With Supervised Learning (**2005):* [https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)
    which examined the calibration properties of various supervised classification
    algorithms. The paper found that maximum margins, such as support vector machines
    and boosted trees, produced miscalibrated class scores and tended to push predictions
    close to 0 and 1, while other methods, such as naïve Bayes, pushed predictions
    in the other directions.
  prefs: []
  type: TYPE_NORMAL
- en: While it was initially thought that simple neural networks produced calibrated
    predictions, this conclusion has since been reevaluated. In a more recent paper
    titled *Are Traditional Neural Networks Well-Calibrated?* ([https://ieeexplore.ieee.org/document/8851962](https://ieeexplore.ieee.org/document/8851962)),
    the authors showed that individual multilayer perceptrons, as well as ensembles
    of multilayer perceptrons, frequently display poor calibration.
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency of probabilistic predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Efficiency is a performance metric used to evaluate probabilistic predictors.
    It measures how precise or informative the prediction intervals or regions are.
    In other words, efficiency indicates how tight or narrow the predicted probability
    distributions are. Smaller intervals or regions are considered more efficient,
    as they convey more certainty about the predicted outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: While validity focuses on ensuring that the error rate is controlled, efficiency
    assesses the usefulness and precision of the predictions. An efficient predictor
    provides more specific information about the possible outcomes, whereas a less
    efficient predictor generates wider intervals or regions, resulting in less precise
    information.
  prefs: []
  type: TYPE_NORMAL
- en: There is an inherent trade-off between validity and efficiency. A conformal
    predictor can always achieve perfect validity by outputting very wide prediction
    sets that encompass all possible outcomes. However, this lacks efficiency, as
    the predictions are too conservative and imprecise.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a model can output very narrow, precise predictions but may
    fail on the validity criteria by making erroneous predictions more than the allowed
    threshold. This results from overconfidence and unreliable probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a conformal predictor finds an optimal balance; the predictions are
    as tight as possible while still meeting the validity guarantee. This ensures
    accuracy and precision without being overly conservative or exceeding the error
    rate threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conformal prediction, efficiency is typically measured by evaluating the
    size of the prediction intervals or regions generated by the conformal predictor.
    Smaller intervals or regions are considered more efficient, as they provide more
    precise information about the possible outcomes. Here are a few common ways to
    measure efficiency in conformal prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction** **interval length**: For regression problems, the average length
    of the prediction intervals can be calculated by finding the difference between
    the upper and lower bounds of each interval and then averaging these differences
    across all instances. Smaller average lengths indicate higher efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction set size**: For classification problems, the size of the prediction
    sets can be evaluated. A smaller prediction set contains fewer class labels and
    is considered more efficient. One way to measure this is by computing the average
    size of the prediction sets across all instances. A lower average set size indicates
    better efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coverage probability**: Coverage probability measures the proportion of true
    outcomes that fall within the prediction intervals or regions. While it is mainly
    used to evaluate the validity of conformal predictors, it can also provide insights
    into efficiency. A predictor with tight intervals or regions will have a higher
    coverage probability, indicating better efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P-value histograms**: In conformal prediction, p-values are calculated for
    each instance and class label. Examining the distribution of p-values can provide
    insights into efficiency. A uniform distribution of p-values suggests that the
    predictor is valid but not necessarily efficient, while a more concentrated distribution
    (e.g., with p-values close to 0 or 1) implies greater efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already seen in the previous chapters how conformal prediction guarantees
    the automatic validity of prediction sets by constructing prediction intervals
    (for regression) or prediction sets (for classification) that come with a guaranteed
    error rate, which is determined by a user-defined confidence level. The key idea
    behind conformal prediction is to use past data and the observed behavior of a
    given machine learning model to estimate the uncertainty in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap the stages in **inductive conformal prediction,** which consists
    of two main phases: the calibration phase and the prediction phase. Here’s an
    outline of how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calibration phase**: In this phase, a machine learning model is trained on
    a dataset, and a nonconformity measure is calculated for each instance in the
    dataset. The nonconformity measure quantifies the strangeness or atypicality of
    an instance with respect to the rest of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction phase**: When a new instance needs to be predicted, the nonconformity
    measure, for instance, is calculated using the same nonconformity measure function
    used in the calibration phase. The instance’s nonconformity score is then compared
    to the nonconformity scores of the calibration instances. A p-value is computed
    for each possible outcome, reflecting the proportion of calibration instances
    with nonconformity scores higher than or equal to that of the new instance. The
    p-values can be interpreted as a measure of how likely it is for the instance
    to belong to each class (for classification) or to fall within a certain range
    (for regression).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction intervals or sets**: Based on the computed p-values and the user-defined
    confidence level, prediction intervals (for regression) or prediction sets (for
    classification) are constructed. These intervals or sets are guaranteed to contain
    the true outcome with a probability equal to the chosen confidence level. For
    instance, if the confidence level is set to 0.95, the true outcome will fall within
    the prediction interval or set 95% of the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By ensuring that the prediction intervals or sets contain the true outcomes
    with the desired probability, conformal prediction provides automatic validity
    for predictions. It is worth noting that while conformal prediction guarantees
    validity, it does not necessarily guarantee efficiency, which depends on the precision
    of the prediction intervals or sets.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B19925_04.xhtml#_idTextAnchor040), we will look at and learn
    about different types of conformal prediction with practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have deep-dived into the concepts of validity and efficiency
    in the context of probabilistic prediction models, building upon the foundations
    laid in the previous chapters. We have looked at the definitions of validity and
    efficiency and learned about various metrics that can be used to evaluate and
    compare different models in terms of validity and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about different families of conformal predictors
    and explore various approaches to quantifying uncertainty.
  prefs: []
  type: TYPE_NORMAL
