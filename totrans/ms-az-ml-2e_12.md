# *第9章*：使用Azure Machine Learning构建ML模型

在前面的章节中，我们学习了Azure Machine Learning中的数据集、预处理、特征提取和管道。在本章中，我们将利用我们迄今为止所获得的知识来创建和训练一个强大的基于树的集成分类器。

首先，我们将深入了解流行的集成分类器，如**随机森林**、**XGBoost**和**LightGBM**的幕后场景。这些分类器在实际的现实中表现极为出色，并且它们在底层都是基于决策树。通过了解它们的主要优势，你将能够轻松发现可以用集成决策树分类器解决的问题。

我们还将学习**梯度提升**和**随机森林**之间的区别，以及是什么使得这些树集成对实际应用有用。这两种技术都有助于克服决策树的主要弱点，并且可以应用于许多不同的分类和回归问题。

最后，我们将使用我们迄今为止所学的所有技术在一个样本数据集上训练一个LightGBM分类器。我们将编写一个训练脚本，该脚本可以自动记录所有参数、评估指标和图形，并且可以通过命令行参数进行配置。我们将计划在Azure Machine Learning训练集群上运行训练脚本。

在本章中，我们将涵盖以下主题：

+   使用基于树的集成分类器

+   使用LightGBM训练集成分类器模型

# 技术要求

在本章中，我们将使用以下Python库和版本来创建基于决策树的集成分类器：

+   `azureml-core 1.34.0`

+   `azureml-sdk 1.34.0`

+   `lightgbm 3.2.1`

+   `numpy 1.19.5`

+   `pandas 1.3.2`

+   `scikit-learn 0.24.2`

+   `seaborn 0.11.2`

+   `matplotlib 3.4.3`

与前面的章节类似，你可以使用本地Python解释器或Azure Machine Learning托管的工作簿环境来执行此代码。

本章中的所有代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter09](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter09)。

# 使用基于树的集成分类器

监督的基于树的集成分类和回归技术在近年来在许多实际应用中证明非常成功。因此，它们今天在包括欺诈检测、推荐引擎、标签引擎等多个应用中得到了广泛的使用。你所有的移动和桌面操作系统、Office程序以及音频或视频流服务每天都在大量使用它们。

因此，在本节中，我们将深入了解它们受欢迎和性能的主要原因，包括训练和评分。如果您是传统机器学习算法的专家，并且了解提升和袋装之间的区别，您可以直接跳到下一节，*使用 LightGBM 训练集成分类器模型*，在那里我们将理论付诸实践。

我们首先将探讨决策树，这是一种非常简单的技术，已有数十年历史。我们鼓励您即使跟随简单的步骤，因为它们构成了今天最先进经典监督机器学习方法的基石。我们还将详细探讨基于树的分类器的优势，以帮助您理解经典方法与基于深度学习的机器学习模型之间的差异。

单个决策树也存在许多缺点，因此它仅用于集成模型，而从不作为单独的模型使用。我们将在本节稍后部分更详细地探讨单个决策树的缺点。之后，我们将发现将多个弱单个树组合成一个强大的集成分类器的方法，这种分类器建立在基于树的方法的优势之上，并将它们转化为今天所拥有的——几乎集成到每个现成机器学习平台中的强大多用途监督机器学习模型。

## 理解简单的决策树

让我们先讨论一下`if/else`语句。这个函数可以是连续回归函数或决策边界函数。因此，像许多其他机器学习方法一样，决策树可以用于学习回归和分类问题。

从前面的描述中，我们可以立即发现决策树的一些重要优势：

+   一种是灵活性，可以在不同的数据分布、数据类型（例如，数值和分类数据）以及机器学习问题（如分类或回归）上工作。

+   另一个优势，也是它们与更复杂模型竞争的原因之一，是它们的可解释性。基于树的模型和集成可以可视化，甚至可以打印在纸上以解释预测的决策（输出）。

+   第三大优势在于它们在训练性能、模型大小和有效性方面的实际应用。将预训练的决策树集成到桌面、Web 或移动应用程序中，比深度学习方法要简单得多，也快得多。

    重要提示

    请注意，我们并不打算将基于树的方法作为解决每个机器学习问题的解决方案，并贬低深度学习方法的重要性。我们更希望让您在本章中意识到传统方法的优点，以便您可以为您的特定问题评估正确的方案。

下图展示了一个用于判断一个人是否健康的决策树示例：

![图 9.1 – 一个简单的决策树](img/B17928_09_01.jpg)

图 9.1 – 一个简单的决策树

*图9.1*是一个训练好的决策树的例子，我们可以通过简单地遍历每个节点并到达树的叶节点来对模型进行评分。

### 决策树的优点

基于决策树的机器学习模型因其处理现实世界应用中的数据时的优势而极为流行，这些数据形式多样、形状各异，且杂乱、有偏见和不完整。以下是决策树的关键优点：

+   它们支持广泛的应用。

+   它们需要很少的数据准备。

+   它们使模型的可解释性成为可能。

+   它们提供快速训练和快速评分。

首先，让我们关注决策树的*灵活性*，这是它们与其他许多经典/统计机器学习方法的重大优势之一。尽管其通用框架非常灵活，支持*分类*和*回归*，以及*多输出问题*，但它之所以受到广泛欢迎，是因为它能够直接处理数值和分类数据。多亏了嵌套的`if-else`树，它还可以处理名义类别以及数据中的NULL或缺失值。决策树之所以受欢迎，是因为它们不需要在事先进行大量的预处理和数据清洗。

尽管数据准备和清洗是每个机器学习管道中的重要步骤，但有一个自然支持分类输入数据的框架仍然很令人愉快。一些集成树形分类器是建立在这一优势之上的，例如，**CatBoost**——来自Yandex Research的梯度提升树实现，具有对分类数据的原生支持。

树形模型的一个重要优点，特别是从商业角度来看，是模型的*可解释性*。与其他机器学习方法不同，决策树分类器模型的输出不是一个巨大的参数化决策边界函数。训练好的深度学习模型通常会产生包含超过1亿个参数的模型，因此表现得像一个黑盒——特别是对于商业决策者来说。虽然从深度学习模型中获取见解和推理关于激活是可能的，但通常很难推理输入参数对输出变量的影响。

可解释性是树形方法的优势所在。与许多其他传统机器学习方法（如SVM、逻辑回归或深度学习）相比，决策树是一个非参数模型，因此不使用参数来描述要学习的函数。它使用可以绘制、可视化和打印在纸上的嵌套决策树。这使得决策者能够理解基于树的分类模型的每一个决策（输出）——可能需要很多纸张，但总是可能的。

当谈到可解释性时，我们需要提到决策树的另一个重要方面：决策树模型在训练过程中隐式地发展了**特征重要性**的概念。这是训练好的决策树模型的一个非常有用的输出，我们可以用它来对特征进行预处理排序，而无需首先清理数据。

重要提示

虽然特征重要性也可以用其他机器学习（ML）方法来衡量，例如线性回归，但它们通常需要输入一个清洗和归一化的数据集。许多其他机器学习方法，如支持向量机（SVM）或深度学习，不会为单个输入维度开发特征重要性的度量。

基于决策树的方法在这方面表现出色，因为它们内部根据重要性标准创建每个单个分割（决策）。这导致了对最终模型中哪些特征维度重要以及如何重要的内在理解。

让我们来看看决策树另一个巨大的优势。与源自非参数方法的传统统计模型相比，决策树具有许多实际优势。基于树的模型通常在各种输入分布上都能产生良好的结果，甚至在模型假设被违反时也能很好地工作。除此之外，与深度学习方法相比，训练好的树的大小较小，推理/评分速度快。

### 决策树的缺点

正如生活中的一切都有利有弊一样，决策树也是如此。与单个决策树相关的一些严重缺点应该让你在机器学习（ML）管道中避免使用单个决策树分类器。单个决策树的主要弱点是树是在所有训练样本上拟合的，因此很可能发生**过拟合**。这是因为模型本身倾向于构建复杂的`if-else`树来模拟连续函数。

另一个重要点是，即使对于简单概念，找到最优决策树也是一个**NP难题**（也称为**非确定性多项式时间难题**）。因此，它通过启发式方法来解决，并且得到的单个决策通常不是最优的。

过拟合很糟糕——非常糟糕——并且会导致机器学习中的严重问题。一旦模型过拟合，它就不太能泛化，因此在未见过的数据上的性能非常差。因此，对新输入的预测结果将比训练期间测量的结果更差。另一个相关问题是，训练数据或训练样本顺序的微小变化可能导致非常不同的嵌套树，因此训练收敛性不稳定。单个决策树极其容易过拟合。此外，单个决策树很可能偏向于训练数据中样本数量最多的类别。

通过Bagging和Boosting将多个决策树组合成一个**集成模型**，可以克服单棵树的缺点，如过拟合、不稳定和非最优树。还有许多基于树的优化方法，包括**树剪枝**，以提高泛化能力。使用这些技术的流行模型包括**随机森林**和**梯度提升树**，它们克服了单棵决策树的大部分问题，同时保持了大部分优点。我们将在下一节中探讨这两种方法。

重要提示

有时，即使在基于树的集成方法中，也会出现一些更基本的缺点，这些缺点值得提及。由于决策树的本质，基于树的模型在学习复杂函数，如XOR问题时存在困难。对于这些问题，最好使用非线性参数模型，如神经网络和深度学习方法。

## 将分类器与Bagging结合

单个决策树的一个关键缺点是对训练数据的过拟合，因此，从训练数据的小变化中，会导致泛化性能差和稳定性差。一个**Bagging**（也称为*自助聚合*）分类器通过将多个独立的模型组合成一个在训练数据子集上训练的**集成模型**来克服这个确切问题。这些子集是通过从训练数据集中随机抽取带有替换的样本来构建的。单个模型的输出要么通过分类中的多数投票选择，要么通过回归问题中的均值聚合。

通过结合独立的模型，我们可以降低组合模型的方差，而不会增加偏差，从而大大提高泛化能力。然而，训练多个单独模型还有一个好处：并行化。由于每个单独的模型都使用训练数据的一个随机子集，因此训练过程可以轻松并行化，并在多个计算节点上训练。因此，当在大数据集上训练大量基于树的分类器时，Bagging是一种流行的技术。

下面的*图9.2*展示了每个分类器如何独立地在相同的训练数据上训练——每个模型使用带有替换的随机子集。所有单个模型的组合构成了集成模型。

![图9.2 – Bagging](img/B17928_09_02.jpg)

图9.2 – Bagging

Bagging可以与任何机器学习模型结合使用；然而，它通常与基于树的分类器一起使用，因为它们最容易过拟合。**随机森林**的理念建立在bagging方法之上，并结合了每个分割（决策）的随机特征子集。当随机选择一个特征时，计算分割的最佳阈值，以优化某个*信息准则*（通常是**GINI**或**信息增益**）。因此，随机森林使用训练数据的随机子集、随机特征选择和分割的最佳阈值。

随机森林因其简单的基于决策树的模型以及更好的泛化能力和易于并行化而得到广泛应用。采用特征随机子集的另一个好处是，这种技术也适用于非常高维度的输入。因此，在处理经典机器学习方法时，随机森林通常用于大规模的树集成。

另一种流行的基于树的bagging技术是**extra-trees**（即**extremely randomized trees**）算法，它在维度分割上增加了一个额外的随机化步骤。对于每个分割，随机抽取阈值，并选择最佳阈值进行决策。因此，除了随机特征外，extra-trees算法还使用随机分割阈值来进一步提高泛化能力。

下面的*图9.3*展示了所有树集成技术在推理中的应用。每棵树计算一个单独的分数，而每棵树的结果被汇总以产生最终结果：

![图9.3 – 多数投票](img/B17928_09_03.jpg)

图9.3 – 多数投票

你可以在许多流行的机器学习库中找到基于树的bagging集成，例如scikit-learn、Spark MLlib、ML.NET等。

## 使用boosting轮优化分类器

在计算机科学的许多问题中，我们可以用一个更复杂但更优的方法来替换随机贪婪方法。对于树集成也是如此，并为其奠定了**boosted树集成**的基础。

增强背后的基本思想如下：

1.  我们开始在整个训练数据集上训练单个模型。

1.  然后我们在训练数据集上计算模型的预测，并开始提高产生错误结果的训练样本的权重。

1.  接下来，我们使用加权训练集训练另一棵决策树。然后我们将这两棵决策树组合成一个集成，并预测加权训练集的输出类别。然后我们在下一轮boosting中进一步增加组合模型中错误分类的训练样本的权重。

1.  我们继续执行此算法，直到达到停止标准。

下面的*图9.4*展示了使用boosting优化训练误差如何随着每次迭代（boosting轮）的增加而降低：

![图9.4 – Boosting](img/B17928_09_04.jpg)

图9.4 – Boosting

第一个提升算法是**AdaBoost**，它通过在加权训练集上拟合，将多个弱模型组合成一个集成，并通过学习率适应每一轮迭代。这种方法的概念是添加单个树，这些树专注于预测前一个树无法预测的东西。

提升的一个特别成功的技巧是**梯度提升树**（或**梯度提升**）。在梯度提升中，你将梯度下降优化技术与提升相结合，以便将提升推广到任意损失函数。现在，我们不再使用权重调整数据集样本，而是在每一轮迭代中计算损失函数的梯度，并选择最优权重——那些最小化损失函数的权重。多亏了优化技术的使用，这种方法产生了非常好的结果，增加了决策树现有的优势。

基于梯度提升树的集成在许多流行的机器学习库中都有包括，例如scikit-learn、Spark MLlib等。然而，一些个别实现，如XGBoost和LightGBM，已经获得了相当多的流行度，并且可以作为独立库以及作为scikit-learn和Spark的插件使用。

# 使用LightGBM训练集成分类器模型

由于决策树的简单性和结合多个分类器的优势，随机森林和梯度提升树都是强大的机器学习技术。在这个例子中，我们将使用来自微软的流行LightGBM库来实现这两种技术在测试数据集上的实现。LightGBM是一个梯度提升框架，它结合了多个基于树的机器学习算法。

对于本节，我们将遵循典型的最佳实践方法，使用Azure机器学习，并执行以下步骤：

1.  在Azure中注册数据集。

1.  创建一个远程计算集群。

1.  实现一个可配置的训练脚本。

1.  在计算集群上运行训练脚本。

1.  记录和收集数据集、参数和性能。

1.  注册训练好的模型。

在我们开始这个激动人心的方法之前，我们将快速看一下为什么我们选择LightGBM作为训练装袋和提升树集成工具。

## LightGBM概述

LightGBM使用了许多经典基于树的集成技术的优化，以在分类和连续特征上提供出色的性能。后者使用基于直方图的方法进行配置文件分析，并将其转换为最优分割的离散箱，这减少了内存消耗并加快了训练速度。这使得LightGBM比使用预排序算法计算分割的其他提升库更快、更节省内存，因此是大型数据集的一个很好的选择。

LightGBM的另一个优化是，树是垂直生长的，从叶子到叶子，而其他类似的库是水平生长的，一层层。在叶节点算法中，新添加的叶子节点总是有最大的损失减少。这意味着这些算法与分层算法相比，往往能实现更小的损失。然而，更大的深度也会导致过拟合，因此你必须仔细调整每个树的最大深度。总的来说，LightGBM在大量应用上使用默认参数就能产生非常好的结果。

在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)，“使用NLP的高级特征提取”中，我们了解了很多关于分类特征嵌入和从文本特征中提取语义意义的内容。我们探讨了嵌入名义分类变量的常见技术，如标签编码和独热编码，以及其他方法。然而，为了优化基于树的分类变量的分割标准，有更好的编码可以产生最优的分割。因此，在本节中，我们根本不对分类变量进行编码，而只是简单地告诉LightGBM哪些使用的变量是分类变量。

最后要提到的一点是，LightGBM可以利用GPU加速，并且可以在数据并行或模型并行的方式下进行训练。我们将在[*第12章*](B17928_12_ePub.xhtml#_idTextAnchor189)，“Azure上的分布式机器学习”中了解更多关于分布式训练的内容。

重要提示

LightGBM是一个基于树的集成模型的绝佳选择，特别是对于非常大的数据集。

在本书中，我们将使用带有`lgbm`命名空间的LightGBM。然后我们可以通过输入四个字符来直接调用命名空间中的不同方法——这是Python中数据科学家的一种最佳实践方法。让我们看一个简单的例子：

[PRE0]

值得注意的是，所有算法都是通过`lgbm.train()`方法进行训练的，我们使用不同的参数来指定算法、应用类型、损失函数，以及每个算法的额外超参数。LightGBM支持多种基于决策树的集成模型，用于袋装和提升。以下是您可以选择的算法选项，以及它们的名称，以便在提升参数中识别它们：

+   `gbdt`：传统的梯度提升决策树

+   `rf`：随机森林

+   `dart`：Dropouts meet multiple additive regression trees

+   `goss`：基于梯度的单侧采样

前两个选项，即*梯度提升决策树*（`gbdt`），这是LightGBM的默认选择，以及*随机森林*（`rf`），是提升和袋装技术的经典实现，在本章的第一节中解释，并具有LightGBM特定的优化。其他两种技术，*Dropouts Meet Multiple Additive Regression Trees*（`dart`）和*Gradient-Based One-Side Sampling*（`goss`），是LightGBM特有的，并为更好的结果提供了更多优化，以牺牲训练速度为代价。

目标参数——这是最重要的参数之一——指定了模型的适用应用程序类型，因此是您试图解决的机器学习问题。在LightGBM中，您有以下标准选项，这些选项与其他大多数基于决策树的集成算法类似：

+   `regression`: 用于预测连续目标变量

+   `binary`: 用于二分类任务

+   `multiclass`: 用于多分类问题

除了标准选择之外，您还可以选择以下更具体的目标：`regression_l1`、`huber`、`fair`、`poisson`、`quantile`、`mape`、`gamma`、`cross_entropy`以及许多其他选项。

与模型的目标参数直接相关的是选择损失函数来衡量和优化训练性能。在这里，LightGBM也为我们提供了默认选项，这些选项也大多数其他提升库中可用，我们可以通过metric参数来指定：

+   `mae`: 均值绝对误差

+   `mse`: 均方误差

+   `binary_logloss`: 二分类的损失

+   `multi_logloss`: 多分类的损失

除了这些损失度量之外，还支持其他度量，例如`rmse`、`quantile`、`mape`、`huber`、`fair`、`poisson`以及许多其他度量。在我们的分类场景中，我们将选择具有`binary`目标函数和`binary_logloss`度量的`dart`算法。

重要提示

您还可以将LightGBM用作scikit-learn估计器。为此，从`lightgbm`命名空间调用`LGBMModel`、`LGBMClassifier`或`LGBMRegressor`模型。然而，最新功能通常仅通过LightGBM接口可用。

现在，了解了如何使用LightGBM，我们可以开始实现数据准备和编写脚本。

## 准备数据

在本节中，我们将读取和准备数据，并将清洗后的数据注册为新的数据集在Azure Machine Learning中。这将使我们能够从与工作区连接的任何计算目标访问数据，而无需手动复制数据、挂载磁盘或设置与数据存储的连接。这在[*第4章*](B17928_04_ePub.xhtml#_idTextAnchor071)中进行了详细讨论，*数据摄取和管理数据集*。所有设置、调度和操作都将从作者环境——*Jupyter笔记本*中完成。

对于分类示例，我们将使用*泰坦尼克号数据集*，这是一个流行的机器学习实践者数据集，用于预测泰坦尼克号上每位乘客的二元生存概率（*生存*或*未生存*）。该数据集的特征描述了乘客，并包含以下属性：乘客ID、等级、姓名、性别、年龄、船上兄弟姐妹或配偶的数量、船上子女或父母数量、票号、票价、船舱号和登船港口。

重要提示

关于这个数据集的详细信息以及完整的预处理流程，可以在本书附带源代码中找到。

在不知道更多细节的情况下，我们将卷起袖子设置工作区并开始实验：

1.  我们从`azureml.core`导入`Workspace`和`Experiment`，并指定实验名称为`titanic-lgbm`：

    [PRE1]

1.  接下来，我们使用pandas加载数据集，并开始清理和预处理数据：

    [PRE2]

在前面的示例中，我们从CSV文件加载数据，删除未使用的列，将`Sex`特征的值替换为标签`0`和`1`，并将`Embarked`特征的分类值编码为标签。

1.  接下来，我们编写一个小的实用函数`df_to_dataset()`，它将帮助我们存储pandas DataFrame并将其注册为Azure数据集，以便在Azure机器学习环境中轻松重用：

    [PRE3]

首先，我们检索到我们机器学习工作区的默认数据存储的引用——这是我们首次设置工作区时创建的Azure Blob存储。然后，我们使用一个辅助函数将数据集上传到这个默认数据存储，并将其作为表格数据集引用。

1.  接下来，我们使用新创建的辅助函数将pandas DataFrame注册为名为`titanic_cleaned`的数据集：

    [PRE4]

1.  一旦数据集在Azure中注册，就可以在任何Azure机器学习工作区中访问。如果我们现在进入UI并点击`titanic_cleaned`数据集。在UI中，我们还可以轻松检查和预览数据，如下面的截图所示：

![图9.5 – 泰坦尼克号数据集](img/B17928_09_05.jpg)

图9.5 – 泰坦尼克号数据集

值得注意的是，我们首先将分类变量编码为整数，使用标签编码，但稍后告诉LightGBM哪些变量包含数值列中的分类信息。这将帮助LightGBM在计算直方图和最佳参数分割时对这些列进行不同的处理。

数据集注册的好处是，我们现在可以简单地将数据传递给训练脚本或从Azure机器学习中的任何Python解释器访问它。让我们继续训练示例，为LightGBM创建训练和执行环境。

## 设置计算集群和执行环境

在我们开始训练 LightGBM 分类器之前，我们需要设置我们的训练集群和一个包含所有必需 Python 库的训练环境。对于本章，我们选择了一个最多有四个节点的 `STANDARD_D2_V2` 类型的 CPU 集群：

1.  让我们编写一个小的辅助函数，使我们能够检索或创建一个具有指定名称和配置的训练集群。我们利用 `ComputeTargetException`，如果未找到指定名称的集群，则会抛出此异常：

    [PRE5]

我们已经在之前的章节中看到了这个脚本的组成部分，其中我们调用 `AmlCompute.provisioning_configuration()` 来配置一个新的集群。你可以在你的创作环境中定义所有基础设施这一点非常有帮助。

1.  让我们检索或创建一个新的训练集群：

    [PRE6]

1.  接下来，我们想要为我们的训练环境和 Python 配置做同样的事情。我们实现了一个小的 `get_run_config()` 函数，用于返回具有 Python 配置的远程执行环境。这将用于配置训练脚本所需的全部 Python 包：

    [PRE7]

在前面的脚本中，我们使用 `RunConfiguration` 定义了 Azure Machine Learning 所需的包，如 `azureml-defaults` 和自定义 Python 包。

1.  接下来，我们使用此函数配置一个包含所有必需 `pip` 包的 Python 镜像，包括 `lightgbm`：

    [PRE8]

在前面的代码片段中使用的两个函数非常有用。你使用 Azure Machine Learning 的时间越长，你将构建更多的抽象来轻松地与 Azure Machine Learning 服务交互。

使用自定义运行配置和自定义 Python 包，Azure Machine Learning 将在调度使用此运行配置的作业时立即设置 Docker 镜像并将其自动注册到 *容器注册表* 中。让我们首先构建训练脚本，然后在集群上调度它。

## 构建 LightGBM 分类器

现在我们有了数据集，并且我们已经为 LightGBM 分类模型的训练设置了环境和集群，我们可以设置训练脚本。上一节中的代码是在 Jupyter 笔记本中编写的。本节中的以下代码现在将编写并存储在一个名为 `train_lgbm.py` 的 Python 文件中。我们将按照以下步骤开始构建分类器：

1.  首先，我们配置运行并从运行中提取工作区配置。这应该已经很熟悉了，因为我们已经为到目前为止在 Azure Machine Learning 上调度的几乎所有脚本都做过这件事：

    [PRE9]

1.  接下来，我们设置一个参数解析器，将命令行参数解析为 LightGBM 参数。我们开始时只有一些参数，但可以轻松地添加所有可用参数和默认值：

    [PRE10]

    重要提示

    我们建议使您的训练脚本可配置。使用 `argparse` 定义数据集、输入参数和默认值。如果您坚持这个约定，所有模型参数都将自动跟踪在您的 Azure Machine Learning 实验中。另一个好处是，您以后可以调整超参数，而无需在训练脚本中更改一行代码。

1.  然后，我们可以引用从输入参数中清理过的数据集，并使用 `to_pandas_dataframe()` 方法将其加载到内存中：

    [PRE11]

1.  在将数据集作为 pandas DataFrame 加载之后，我们现在可以开始将训练数据分割成训练集和验证集。我们还将把目标变量 `Survived` 从训练数据集中分割成其自己的变量：

    [PRE12]

1.  接下来，我们向 LightGBM 介绍分类特征，这些特征已经被转换成数值变量，但需要特殊处理来计算最优分割值：

    [PRE13]

1.  接下来，我们创建实际的 LightGBM 训练集和测试集，从 pandas DataFrame 中：

    [PRE14]

与 scikit-learn 不同，我们无法直接在 LightGBM 中使用 pandas DataFrame，但需要使用包装类 `lgbm.Dataset`。这将使我们能够访问所有必需的优化和功能，例如分布式训练、稀疏数据优化以及关于分类特征的元信息。

1.  在解析完命令行参数后，我们将它们传递到一个参数字典中，然后将其传递给 LightGBM 训练方法：

    [PRE15]

1.  所有通过命令行参数传递的参数都会自动记录在 Azure Machine Learning 中。然而，如果您想以编程方式访问模型参数或在 Azure Machine Learning 的实验概览中显示它们，我们可以在实验中记录它们。这将把所有参数附加到每个运行实例上，并在 Azure Machine Learning 中作为参数值提供。这意味着我们可以在以后根据模型参数对实验运行进行排序和筛选：

    [PRE16]

梯度提升是一种具有可变迭代次数和可选提前停止标准的迭代优化方法。因此，我们还想记录训练脚本的每个迭代的全部指标。在这本书的整个过程中，我们将使用所有 ML 框架的类似技术——即使用一个回调函数，将所有可用的指标记录到您的 Azure Machine Learning 工作区。让我们使用 LightGBM 的自定义回调规范来编写这样一个函数。

1.  在这里，我们创建一个回调对象，它遍历所有评估结果并将它们记录在运行中：

    [PRE17]

1.  在我们为 LightGBM 预测器设置好参数之后，我们可以使用 `lgbm.train()` 方法来配置训练和验证过程。我们需要提供所有参数、参数和回调函数：

    [PRE18]

上述代码的亮点是，通过提供通用的回调函数，所有训练和验证分数将自动记录到 Azure。因此，我们可以实时跟踪训练迭代，无论是在 UI 中还是在 API 中——例如，在一个自动收集所有运行信息的 Jupyter 小部件中。

1.  为了评估最终的训练分数，我们使用训练好的分类器来预测几个默认的分类分数，例如 `accuracy`（准确率）、`precision`（精确率）和 `recall`（召回率），以及组合的 `f1` 分数：

    [PRE19]

我们已经运行了脚本并看到了所有指标以及模型在 Azure 中的性能。但这只是开始——我们想要更多！

1.  让我们在 Azure Machine Learning 中计算特征重要性并跟踪其图表，并运行它。我们可以用几行代码来完成这个任务：

    [PRE20]

一旦将此片段添加到训练脚本中，每次训练运行也将存储一个特征重要性图表。这有助于了解不同的指标如何影响特征重要性。

1.  我们还想添加一个额外的步骤。每当训练脚本运行时，我们希望将训练好的模型上传并注册到模型注册表中。通过这样做，我们可以在以后手动或自动将模型部署到容器服务中。然而，这只能通过保存每次运行的训练工件来完成：

    [PRE21]

在前面的片段中，我们使用了 `joblib` 包，该包最初是 scikit-learn 的一部分，用于将分类器保存到磁盘。然后我们将导出的模型注册为 Azure Machine Learning 中的 LightGBM 模型。

就这样——我们已经写完了整个训练脚本。它并不特别长，也不特别复杂。最棘手的部分是理解如何选择 LightGBM 的一些参数以及一般性地理解梯度提升——这就是为什么我们将章节的前半部分专门用于这个主题。现在让我们启动集群并提交训练脚本。

## 在 Azure Machine Learning 集群上安排训练脚本

我们逻辑上回到了作者环境——Jupyter 笔记本。上一节中的代码已存储为 `train_lgbm.py` 文件，我们现在将准备将其提交到集群。一件好事是我们使训练脚本可以通过命令行参数进行配置，因此我们可以使用 CLI 参数调整 LightGBM 模型的基参数。在以下步骤中，我们将配置作者脚本以执行训练过程：

1.  让我们定义此模型的参数——我们将使用 `dart`，标准学习率为 `0.01`，dropout 率为 `0.15`。我们还通过命名参数将数据集传递给训练脚本：

    [PRE22]

我们指定了提升方法，`dart`。正如我们在上一节中学到的，这项技术表现非常好，但并不特别高效，比其他选项——`gbdt`、`rf` 和 `goss`——慢一些。

重要提示

这也是Azure机器学习中的超参数调整工具`HyperOpt`将超参数传递给训练脚本的方式。我们将在[*第11章*](B17928_11_ePub.xhtml#_idTextAnchor178)中了解更多关于超参数调整和自动机器学习的内容。

1.  接下来，我们可以将参数传递给`ScriptRunConfig`并启动训练脚本：

    [PRE23]

在前面的代码中，我们指定了我们的分类器文件，该文件存储在当前编写脚本的相关位置。Azure机器学习会将训练脚本上传到默认的数据存储库，并在运行脚本的集群的所有节点上使其可用。

1.  最后，让我们提交运行配置并执行训练脚本：

    [PRE24]

`RunDetails`方法为我们提供了一个带有实时日志的交互式小部件，这些日志来自远程计算服务。我们可以看到集群正在初始化和扩展，Docker镜像正在构建和注册，最终，还包括训练脚本的日志。

提示

如果你更喜欢其他方法而不是交互式的Jupyter小部件，你也可以使用`run.wait_for_completion(show_output=True)`或`print(run.get_portal_url())`来跟踪日志，以获取在Azure中运行的实验的URL。

1.  现在，让我们切换到Azure机器学习UI，并在实验中查找运行。一旦我们点击它，我们就可以导航到**指标**部分，并找到所有已记录指标的概览。你可以在以下*图9.6*中看到，具有相同名称的多次记录的指标被转换为向量，并以折线图的形式显示：

![图9.6 – 验证损失](img/B17928_09_06.jpg)

图9.6 – 验证损失

然后，点击**图像**部分。当我们这样做时，我们会看到我们在训练脚本中创建的特征重要性图。以下*图9.7*展示了它在Azure机器学习UI中的样子：

![图9.7 – 特征重要性](img/B17928_09_07.jpg)

图9.7 – 特征重要性

我们看到了如何在Azure机器学习中训练一个LightGBM分类器，利用了自动扩展的Azure机器学习计算集群。记录指标、图表和参数将所有关于训练运行的详细信息保持在同一个地方。与保存训练脚本的快照、输出、日志和训练好的模型一起，这对任何专业的大型机器学习项目来说都是无价的。

你应该从本章记住的是，梯度提升树是一种非常高效且可扩展的经典机器学习方法，拥有许多优秀的库，并支持分布式学习和GPU加速。LightGBM是微软提供的一种替代方案，它在微软和开源生态系统中都得到了很好的整合。如果你在寻找一个经典、快速且易于理解的机器学习模型，我们的建议是选择LightGBM。

# 摘要

在本章中，你学习了如何在Azure机器学习中构建一个经典机器学习模型。

您了解了决策树，这是一种在多种分类和回归问题中流行的技术。决策树的主要优势是它们需要很少的数据准备，因为它们在分类数据和不同的数据分布上表现良好。另一个重要的好处是它们的可解释性，这对于商业决策和用户来说尤为重要。这有助于您了解何时使用基于决策树的集成预测器是合适的。

然而，我们也了解到了一些弱点，特别是在过拟合和泛化能力差方面。幸运的是，基于树的集成技术，如 bagging（自助聚合）和 boosting（提升），有助于克服这些问题。虽然 bagging 有像随机森林这样的流行方法，它能够很好地并行化，但 boosting，尤其是梯度 boosting，有高效的实现，包括 XGBoost 和 LightGBM。

您在 Azure Machine Learning 中使用 LightGBM 库实现了并训练了一个基于决策树的分类器。LightGBM 是由微软开发的，通过一些优化提供了出色的性能和训练时间。这些优化帮助 LightGBM 即使在处理大型数据集时也能保持较小的内存占用，并且通过更少的迭代次数产生更好的损失。您不仅使用 Azure Machine Learning 来执行您的训练脚本，还用它来跟踪您的模型训练性能和最终的分类器。

在下一章中，我们将探讨一些流行的深度学习技术，以及如何使用 Azure Machine Learning 来训练它们。
