["```py\nimport numpy as np\nimport pandas as pd\ncomps = pd.read_csv(\"/kaggle/input/meta-kaggle/Competitions.csv\")\nevaluation = ['EvaluationAlgorithmAbbreviation',\n              'EvaluationAlgorithmName',\n              'EvaluationAlgorithmDescription',]\ncompt = ['Title', 'EnabledDate', 'HostSegmentTitle']\ndf = comps[compt + evaluation].copy()\ndf['year'] = pd.to_datetime(df.EnabledDate).dt.year.values\ndf['comps'] = 1\ntime_select = df.year >= 2015\ncompetition_type_select = df.HostSegmentTitle.isin(\n\t\t\t\t\t\t['Featured', 'Research'])\npd.pivot_table(df[time_select&competition_type_select],\n               values='comps',\n               index=['EvaluationAlgorithmAbbreviation'],\n               columns=['year'],\n               fill_value=0.0,\n               aggfunc=np.sum,\n               margins=True\n              ).sort_values(\n                by=('All'), ascending=False).iloc[1:,:].head(20) \n```", "```py\nmetric = 'AUC'\nmetric_select = df['EvaluationAlgorithmAbbreviation']==metric\nprint(df[time_select&competition_type_select&metric_select]\n        [['Title', 'year']]) \n```", "```py\ncounts = (df[time_select&competition_type_select]\n          .groupby('EvaluationAlgorithmAbbreviation'))\ntotal_comps_per_year = (df[time_select&competition_type_select]\n                        .groupby('year').sum())\nsingle_metrics_per_year = (counts.sum()[counts.sum().comps==1]\n                           .groupby('year').sum())\nsingle_metrics_per_year\ntable = (total_comps_per_year.rename(columns={'comps': 'n_comps'})\n         .join(single_metrics_per_year / total_comps_per_year)\n         .rename(columns={'comps': 'pct_comps'}))\nprint(table) \n```", "```py\n n_comps pct_comps\nyear                   \n2015       28  0.179\n2016       19  0.158\n2017       34  0.177\n2018       35  0.229\n2019       36  0.278\n2020       43  0.302\n2021        8  0.250 \n```", "```py\nprint(counts.sum()[counts.sum().comps==1].index.values) \n```", "```py\n['AHD@{Type}' 'CVPRAutoDrivingAveragePrecision' 'CernWeightedAuc'\n'FScore_1' 'GroupMeanLogMAE' 'ImageNetObjectLocalization'\n'IndoorLocalization'  'IntersectionOverUnionObjectSegmentationBeta'\n'IntersectionOverUnionObjectSegmentationWithClassification'\n'IntersectionOverUnionObjectSegmentationWithF1' 'Jaccard'\n'JaccardDSTLParallel' 'JigsawBiasAUC' 'LaplaceLogLikelihood'\n'LevenshteinMean' 'Lyft3DObjectDetectionAP' 'M5_WRMSSE' 'MASpearmanR' 'MCRMSE' 'MCSpearmanR' 'MWCRMSE' 'MeanColumnwiseLogLoss' 'MulticlassLossOld' 'NDCG@{K}' 'NQMicroF1' 'NWRMSLE' 'PKUAutoDrivingAP' 'R2Score' 'RValue' 'RootMeanSquarePercentageError' 'SIIMDice' 'SMAPE' 'SantaResident' 'SantaRideShare' 'SantaWorkshopSchedule2019' 'TrackML'\n 'TravelingSanta2' 'TwoSigmaNews' 'WeightedAUC' 'WeightedMulticlassLoss' 'WeightedPinballLoss' 'WeightedRowwisePinballLoss' 'YT8M_MeanAveragePrecisionAtK' 'ZillowMAE' 'football' 'halite' 'mab'] \n```", "```py\nsklearn.metrics.confusion_matrix(\n    y_true, y_pred, *, labels=None, sample_weight=None,\n    normalize=None\n) \n```", "```py\nfrom scipy.misc import derivative\nimport xgboost as xgb\ndef focal_loss(alpha, gamma):\n    def loss_func(y_pred, y_true):\n        a, g = alpha, gamma\n        def get_loss(y_pred, y_true):\n            p = 1 / (1 + np.exp(-y_pred))\n            loss = (-(a * y_true + (1 - a)*(1 - y_true)) * \n                    ((1 - (y_true * p + (1 - y_true) * \n                     (1 - p)))**g) * (y_true * np.log(p) + \n                    (1 - y_true) * np.log(1 - p)))\n            return loss\n        partial_focal = lambda y_pred: get_loss(y_pred, y_true)\n        grad = derivative(partial_focal, y_pred, n=1, dx=1e-6)\n        hess = derivative(partial_focal, y_pred, n=2, dx=1e-6)\n        return grad, hess\n    return loss_func\nxgb = xgb.XGBClassifier(objective=focal_loss(alpha=0.25, gamma=1)) \nfocal_loss, which is then fed into an XGBoost instanceâ€™s object parameters. The example is worth showing because the focal loss requires the specification of some parameters in order to work properly on your problem (alpha and gamma). The more simplistic solution of having their values directly coded into the function is not ideal, since you may have to change them systematically as you are tuning your model. Instead, in the proposed function, when you input the parameters into the focal_loss function, they reside in memory and they are referenced by the loss_func function that is returned to XGBoost. The returned cost function, therefore, will work, referring to the alpha and gamma values that you have initially instantiated.\n```", "```py\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import average_precision_score\nscorer = make_scorer(average_precision_score, \naverage='weighted', greater_is_better=True, needs_proba=False) \n```", "```py\nsklearn.calibration.CalibratedClassifierCV(base_estimator=None, *,\n    method='sigmoid', cv=None, n_jobs=None, ensemble=True) \n```"]