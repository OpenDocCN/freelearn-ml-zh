- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Model Calibration
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†
- en: So far, we have explored various ways to handle the data imbalance. In this
    chapter, we will see the need to do some post-processing of the prediction scores
    that we get from the trained models. This can be helpful either during the real-time
    prediction from the model or during the offline training time evaluation of the
    model. We will also understand some ways of measuring how calibrated the model
    is and how imbalanced datasets make the model calibration inevitable.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æ¢è®¨äº†å¤„ç†æ•°æ®ä¸å¹³è¡¡çš„å„ç§æ–¹æ³•ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°éœ€è¦å¯¹ä»è®­ç»ƒæ¨¡å‹è·å¾—çš„é¢„æµ‹åˆ†æ•°è¿›è¡Œä¸€äº›åå¤„ç†çš„éœ€æ±‚ã€‚è¿™å¯ä»¥åœ¨ä»æ¨¡å‹è¿›è¡Œå®æ—¶é¢„æµ‹æˆ–åœ¨å¯¹æ¨¡å‹è¿›è¡Œç¦»çº¿è®­ç»ƒæ—¶é—´è¯„ä¼°æ—¶æœ‰æ‰€å¸®åŠ©ã€‚æˆ‘ä»¬è¿˜å°†äº†è§£ä¸€äº›è¡¡é‡æ¨¡å‹æ ¡å‡†ç¨‹åº¦çš„æ–¹æ³•ä»¥åŠä¸å¹³è¡¡æ•°æ®é›†å¦‚ä½•ä½¿æ¨¡å‹æ ¡å‡†å˜å¾—ä¸å¯é¿å…ã€‚
- en: 'The following topics will be covered in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Introduction to model calibration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†ç®€ä»‹
- en: The influence of data balancing techniques on model calibration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®å¹³è¡¡æŠ€æœ¯å¯¹æ¨¡å‹æ ¡å‡†çš„å½±å“
- en: Plotting calibration curves for a model trained on a real-world dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç»˜åˆ¶æ ¡å‡†æ›²çº¿
- en: Model calibration techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†æŠ€æœ¯
- en: The impact of calibration on a modelâ€™s performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¡å‡†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
- en: By the end of this chapter, you will have a clear understanding of what model
    calibration means, how to measure it, and when and how to apply it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†æ¸…æ¥šåœ°äº†è§£æ¨¡å‹æ ¡å‡†çš„å«ä¹‰ã€å¦‚ä½•è¡¡é‡å®ƒä»¥åŠä½•æ—¶ä»¥åŠå¦‚ä½•åº”ç”¨å®ƒã€‚
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `matplotlib`, `numpy`, `scikit-learn`, `xgboost`, and `imbalanced-learn`. The
    code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10).
    You can open the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon on the top of the chapterâ€™s notebook or by launching it from [https://colab.research.google.com](https://colab.research.google.com)
    using the GitHub URL of the notebook.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å‰å‡ ç« ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨å¸¸è§çš„åº“ï¼Œå¦‚`matplotlib`ã€`numpy`ã€`scikit-learn`ã€`xgboost`å’Œ`imbalanced-learn`ã€‚æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10)ã€‚æ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»ç« èŠ‚ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡æˆ–é€šè¿‡ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLä»[https://colab.research.google.com](https://colab.research.google.com)å¯åŠ¨å®ƒæ¥æ‰“å¼€GitHubç¬”è®°æœ¬ã€‚
- en: Introduction to model calibration
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†ç®€ä»‹
- en: What is the difference between stating â€œ*The model predicted the transaction
    as fraudulent*â€ and â€œ*The* *model estimated a 60% probability of the transaction
    being fraudulent*â€? When would one statement be more useful than the other?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: â€œ*æ¨¡å‹é¢„æµ‹äº¤æ˜“ä¸ºæ¬ºè¯ˆ*â€å’Œâ€œ*æ¨¡å‹ä¼°è®¡äº¤æ˜“æ¬ºè¯ˆçš„å¯èƒ½æ€§ä¸º60%*â€ä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿä½•æ—¶ä¸€ä¸ªé™ˆè¿°æ¯”å¦ä¸€ä¸ªæ›´æœ‰ç”¨ï¼Ÿ
- en: The difference between the two is that the second statement represents likelihood.
    This likelihood can be useful in understanding the modelâ€™s confidence, which is
    needed in many applications, such as in medical diagnosis. For example, the prediction
    that a patient is 80% likely or 80% probable to have cancer is more useful to
    the doctor than just predicting whether the patient has cancer or not.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤è€…ä¹‹é—´çš„åŒºåˆ«åœ¨äºç¬¬äºŒä¸ªé™ˆè¿°ä»£è¡¨ä¼¼ç„¶æ€§ã€‚è¿™ç§ä¼¼ç„¶æ€§åœ¨ç†è§£æ¨¡å‹çš„ç½®ä¿¡åº¦æ—¶å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œè¿™åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½æ˜¯å¿…éœ€çš„ï¼Œä¾‹å¦‚åœ¨åŒ»å­¦è¯Šæ–­ä¸­ã€‚ä¾‹å¦‚ï¼Œé¢„æµ‹ä¸€ä¸ªæ‚£è€…æœ‰80%çš„å¯èƒ½æ€§æˆ–80%çš„å¯èƒ½æ€§æ‚£æœ‰ç™Œç—‡ï¼Œå¯¹åŒ»ç”Ÿæ¥è¯´æ¯”ä»…ä»…é¢„æµ‹æ‚£è€…æ˜¯å¦æ‚£æœ‰ç™Œç—‡æ›´æœ‰ç”¨ã€‚
- en: A model is considered calibrated if there is a match between the number of positive
    classes and predicted probability. Letâ€™s try to understand this further. Letâ€™s
    say we have 10 observations, and for each of them, the model predicts a probability
    of 0.7 to be of the positive class. If the model is calibrated, then we expect
    7 out of those 10 observations to belong to the positive class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ­£ç±»åˆ«çš„æ•°é‡ä¸é¢„æµ‹æ¦‚ç‡ç›¸åŒ¹é…ï¼Œåˆ™è®¤ä¸ºæ¨¡å‹æ˜¯æ ¡å‡†çš„ã€‚è®©æˆ‘ä»¬è¿›ä¸€æ­¥ç†è§£è¿™ä¸€ç‚¹ã€‚å‡è®¾æˆ‘ä»¬æœ‰10ä¸ªè§‚å¯Ÿå€¼ï¼Œå¹¶ä¸”å¯¹äºæ¯ä¸€ä¸ªï¼Œæ¨¡å‹é¢„æµ‹æ­£ç±»åˆ«çš„æ¦‚ç‡ä¸º0.7ã€‚å¦‚æœæ¨¡å‹æ˜¯æ ¡å‡†çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„è®¡è¿™10ä¸ªè§‚å¯Ÿå€¼ä¸­æœ‰7ä¸ªå±äºæ­£ç±»åˆ«ã€‚
- en: However, surprisingly, most machine learning models are not calibrated, and
    their prediction values tend to be overconfident or underconfident. What does
    that mean? An overconfident model would predict the probability to be 0.9 (for
    example), while the actual probability might have been only 0.6\. Similarly, an
    underconfident model would predict the probability to be 0.6 (for example) while
    the actual probability might have been 0.9.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¤§å¤šæ•°æœºå™¨å­¦ä¹ æ¨¡å‹éƒ½æ²¡æœ‰æ ¡å‡†ï¼Œå®ƒä»¬çš„é¢„æµ‹å€¼å¾€å¾€è¿‡äºè‡ªä¿¡æˆ–ç¼ºä¹è‡ªä¿¡ã€‚è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿä¸€ä¸ªè¿‡äºè‡ªä¿¡çš„æ¨¡å‹å¯èƒ½ä¼šé¢„æµ‹æ¦‚ç‡ä¸º0.9ï¼ˆä¾‹å¦‚ï¼‰ï¼Œè€Œå®é™…æ¦‚ç‡å¯èƒ½åªæœ‰0.6ã€‚åŒæ ·ï¼Œä¸€ä¸ªç¼ºä¹è‡ªä¿¡çš„æ¨¡å‹å¯èƒ½ä¼šé¢„æµ‹æ¦‚ç‡ä¸º0.6ï¼ˆä¾‹å¦‚ï¼‰ï¼Œè€Œå®é™…æ¦‚ç‡å¯èƒ½æ˜¯0.9ã€‚
- en: '*Do we always need to calibrate* *model probabilities?*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬æ˜¯å¦æ€»æ˜¯éœ€è¦æ ¡å‡†* *æ¨¡å‹æ¦‚ç‡*ï¼Ÿ'
- en: Actually, it depends upon the problem at hand. If the problem inherently involves
    the ordering of certain items, say in search ranking, then all we need is relative
    scores and real probabilities donâ€™t matter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™å–å†³äºå…·ä½“é—®é¢˜ã€‚å¦‚æœé—®é¢˜æœ¬è´¨ä¸Šæ¶‰åŠæŸäº›é¡¹ç›®çš„æ’åºï¼Œæ¯”å¦‚åœ¨æœç´¢æ’åä¸­ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦ç›¸å¯¹åˆ†æ•°ï¼Œå®é™…çš„æ¦‚ç‡å¹¶ä¸é‡è¦ã€‚
- en: 'Here is an example of an overconfident model where we can see that most of
    the time, the predicted probabilities from the model are much higher than the
    fraction of actual positive examples:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªè¿‡åº¦è‡ªä¿¡æ¨¡å‹çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¤§å¤šæ•°æ—¶å€™ï¼Œæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡è¿œé«˜äºå®é™…æ­£ä¾‹çš„åˆ†æ•°ï¼š
- en: '![](img/B17259_10_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_01.jpg)'
- en: Figure 10.1 â€“ The calibration curve of an overconfident model for which predicted
    probabilities are overestimated
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.1 â€“ å¯¹é¢„æµ‹æ¦‚ç‡é«˜ä¼°çš„è¿‡åº¦è‡ªä¿¡æ¨¡å‹çš„æ ¡å‡†æ›²çº¿
- en: Why bother with model calibration
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦åœ¨ä¹æ¨¡å‹æ ¡å‡†
- en: 'As weâ€™ve discussed, model calibration may not be necessary if the primary goal
    is to obtain a relative ranking of items. However, there are several other scenarios
    where model calibration becomes crucial:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬è®¨è®ºçš„é‚£æ ·ï¼Œå¦‚æœä¸»è¦ç›®æ ‡æ˜¯è·å¾—é¡¹ç›®çš„ç›¸å¯¹æ’åï¼Œåˆ™æ¨¡å‹æ ¡å‡†å¯èƒ½ä¸æ˜¯å¿…è¦çš„ã€‚ç„¶è€Œï¼Œè¿˜æœ‰å…¶ä»–å‡ ç§åœºæ™¯ä¸‹ï¼Œæ¨¡å‹æ ¡å‡†å˜å¾—è‡³å…³é‡è¦ï¼š
- en: '**Interpreting model predictions as confidence**: Calibrated models allow the
    scores to be interpreted as the modelâ€™s confidence in its predictions. For example,
    in a spam detection system, a calibrated score of 0.9 could mean the model is
    90% confident that an email is spam.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†æ¨¡å‹é¢„æµ‹è§£é‡Šä¸ºç½®ä¿¡åº¦**ï¼šæ ¡å‡†æ¨¡å‹å…è®¸å°†åˆ†æ•°è§£é‡Šä¸ºæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨åƒåœ¾é‚®ä»¶æ£€æµ‹ç³»ç»Ÿä¸­ï¼Œæ ¡å‡†å¾—åˆ†ä¸º0.9å¯èƒ½æ„å‘³ç€æ¨¡å‹æœ‰90%çš„ç½®ä¿¡åº¦è®¤ä¸ºä¸€å°ç”µå­é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶ã€‚'
- en: '**Interpreting model predictions as probabilities**: These scores can also
    be viewed as probabilities, making them directly interpretable. In a weather prediction
    model, a calibrated score of 0.8 could be interpreted as an 80% chance of rain.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†æ¨¡å‹é¢„æµ‹è§£é‡Šä¸ºæ¦‚ç‡**ï¼šè¿™äº›åˆ†æ•°ä¹Ÿå¯ä»¥è¢«è§†ä¸ºæ¦‚ç‡ï¼Œä½¿å®ƒä»¬ç›´æ¥å¯è§£é‡Šã€‚åœ¨ä¸€ä¸ªå¤©æ°”é¢„æŠ¥æ¨¡å‹ä¸­ï¼Œæ ¡å‡†å¾—åˆ†ä¸º0.8å¯ä»¥è§£é‡Šä¸ºæœ‰80%çš„é™é›¨å¯èƒ½æ€§ã€‚'
- en: '**High-stake applications**: Such calibrated probabilities are particularly
    useful in high-stake applications such as healthcare for disease prediction or
    in fraud detection. For instance, in predicting the likelihood of a patient having
    a certain disease, a calibrated score of 0.7 could mean thereâ€™s a 70% chance the
    patient has the disease, guiding further medical tests or treatments.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜é£é™©åº”ç”¨**ï¼šè¿™ç§æ ¡å‡†æ¦‚ç‡åœ¨åŒ»ç–—ä¿å¥ç­‰é«˜é£é™©åº”ç”¨ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œä¾‹å¦‚ç–¾ç—…é¢„æµ‹æˆ–æ¬ºè¯ˆæ£€æµ‹ã€‚ä¾‹å¦‚ï¼Œåœ¨é¢„æµ‹æ‚£è€…æ‚£æœ‰æŸç§ç–¾ç—…çš„å¯èƒ½æ€§æ—¶ï¼Œæ ¡å‡†å¾—åˆ†ä¸º0.7å¯èƒ½æ„å‘³ç€æ‚£è€…æœ‰70%çš„å¯èƒ½æ€§æ‚£æœ‰è¯¥ç–¾ç—…ï¼Œä»è€ŒæŒ‡å¯¼è¿›ä¸€æ­¥çš„åŒ»å­¦æ£€æŸ¥æˆ–æ²»ç–—ã€‚'
- en: '**Enhancing human interpretability and trust**: Human interpretability and
    trust in model predictions are enhanced when the model is calibrated. For example,
    in a loan approval system, a calibrated score could help loan officers understand
    the risk associated with a loan application, thereby aiding in the decision-making
    process.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¢å¼ºäººç±»å¯è§£é‡Šæ€§å’Œä¿¡ä»»**ï¼šå½“æ¨¡å‹æ ¡å‡†æ—¶ï¼Œäººç±»å¯¹æ¨¡å‹é¢„æµ‹çš„å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦å¾—åˆ°å¢å¼ºã€‚ä¾‹å¦‚ï¼Œåœ¨è´·æ¬¾å®¡æ‰¹ç³»ç»Ÿä¸­ï¼Œæ ¡å‡†å¾—åˆ†å¯ä»¥å¸®åŠ©è´·æ¬¾å®˜å‘˜äº†è§£è´·æ¬¾ç”³è¯·çš„é£é™©ï¼Œä»è€Œæœ‰åŠ©äºå†³ç­–è¿‡ç¨‹ã€‚'
- en: 'It is particularly important to be aware of model calibration when working
    with deep learning models, as several common neural network hyperparameters can
    affect model calibration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èµ·å·¥ä½œæ—¶ï¼Œå°¤å…¶é‡è¦çš„æ˜¯è¦æ„è¯†åˆ°æ¨¡å‹æ ¡å‡†ï¼Œå› ä¸ºå‡ ä¸ªå¸¸è§çš„ç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼šå½±å“æ¨¡å‹æ ¡å‡†ï¼š
- en: '**Model capacity**: More layers (depth) and more neurons (width) usually reduce
    the classification error but have been found to lower the calibration of the model
    [1].'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å®¹é‡**ï¼šæ›´å¤šçš„å±‚ï¼ˆæ·±åº¦ï¼‰å’Œæ›´å¤šçš„ç¥ç»å…ƒï¼ˆå®½åº¦ï¼‰é€šå¸¸å¯ä»¥å‡å°‘åˆ†ç±»é”™è¯¯ï¼Œä½†å‘ç°è¿™ä¼šé™ä½æ¨¡å‹çš„æ ¡å‡†åº¦ [1]ã€‚'
- en: '**Batch norm**: Although batch norm typically improves training time, has a
    mild regularizing effect, and might even improve the accuracy of the model, it
    can also make the model more miscalibrated [1].'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹å½’ä¸€åŒ–**ï¼šå°½ç®¡æ‰¹å½’ä¸€åŒ–é€šå¸¸å¯ä»¥æé«˜è®­ç»ƒæ—¶é—´ï¼Œå…·æœ‰è½»å¾®çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œç”šè‡³å¯èƒ½æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä¹Ÿå¯èƒ½ä½¿æ¨¡å‹å‡ºç°æ›´å¤šçš„æ ¡å‡†é”™è¯¯[1]ã€‚'
- en: '**Weight decay**: Weight decay is a regularization technique, and more weight
    decay typically helps calibrate the model. So, instead, if we have less weight
    decay, then we would expect the model to be more miscalibrated [1].'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æƒé‡è¡°å‡**ï¼šæƒé‡è¡°å‡æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šå¸¸æ›´å¤šçš„æƒé‡è¡°å‡æœ‰åŠ©äºæ ¡å‡†æ¨¡å‹ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¾ƒå°‘çš„æƒé‡è¡°å‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æœŸæ¨¡å‹ä¼šæ›´å¤šåœ°å‡ºç°æ ¡å‡†é”™è¯¯[1]ã€‚'
- en: Letâ€™s see what kind of model scores typically need to be calibrated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹é€šå¸¸éœ€è¦æ ¡å‡†çš„æ¨¡å‹å¾—åˆ†ç±»å‹ã€‚
- en: Models with and without well-calibrated probabilities
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…·æœ‰å’Œæ²¡æœ‰è‰¯å¥½æ ¡å‡†æ¦‚ç‡çš„æ¨¡å‹
- en: The logistic regression model is often assumed to output calibrated probabilities,
    particularly when it is an appropriate fit for the data [2]. This assumption is
    based on the modelâ€™s optimization of the cross-entropy loss or log loss function.
    However, itâ€™s worth noting that logistic regression can produce overconfident
    predictions, and regularization techniques such as L1/L2 can help the model be
    more conservative and thus improve calibration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’æ¨¡å‹é€šå¸¸å‡è®¾è¾“å‡ºæ ¡å‡†æ¦‚ç‡ï¼Œå°¤å…¶æ˜¯åœ¨å®ƒé€‚åˆæ•°æ®æ—¶[2]ã€‚è¿™ä¸ªå‡è®¾åŸºäºæ¨¡å‹å¯¹äº¤å‰ç†µæŸå¤±æˆ–å¯¹æ•°æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€»è¾‘å›å½’å¯èƒ½ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ï¼Œè€ŒL1/L2ç­‰æ­£åˆ™åŒ–æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´åŠ ä¿å®ˆï¼Œä»è€Œæé«˜æ ¡å‡†ã€‚
- en: NaÃ¯ve Bayes models often push probabilities close to zero or one due to their
    assumption about feature independence, which can result in poor calibration [2].
    On the other hand, bagging models (such as random forests) and boosting models
    generally produce probabilities that are away from the extremes of zero and one.
    This is due to the score-averaging nature of the individual decision trees or
    stumps they use, which often leads to better calibration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¯¹ç‰¹å¾ç‹¬ç«‹æ€§çš„å‡è®¾ï¼Œæœ´ç´ è´å¶æ–¯æ¨¡å‹é€šå¸¸å°†æ¦‚ç‡æ¨å‘é›¶æˆ–ä¸€ï¼Œè¿™å¯èƒ½å¯¼è‡´æ ¡å‡†ä¸è‰¯[2]ã€‚å¦ä¸€æ–¹é¢ï¼Œè¢‹è£…æ¨¡å‹ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰å’Œæå‡æ¨¡å‹é€šå¸¸äº§ç”Ÿè¿œç¦»é›¶å’Œä¸€çš„æ¦‚ç‡ã€‚è¿™æ˜¯ç”±äºå®ƒä»¬ä½¿ç”¨çš„å•ä¸ªå†³ç­–æ ‘æˆ–æ ‘æ¡©çš„å¾—åˆ†å¹³å‡æ€§è´¨ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ›´å¥½çš„æ ¡å‡†ã€‚
- en: For neural networks, some research studies show that simple networks tend to
    give calibrated scores [2]. Still, since neural network models are getting more
    complex day by day, modern neural networks tend to be fairly uncalibrated [1]
    [3]. As *Figure 10**.2* shows, a five-layer LeNet is well calibrated since its
    confidence levels closely mirror the expected accuracy, evident by the bars roughly
    aligning along the diagonal. In contrast, while a 110-layer ResNet boasts higher
    accuracy (lower error), its confidence scores donâ€™t align as closely with this
    accuracy [1].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¥ç»ç½‘ç»œï¼Œä¸€äº›ç ”ç©¶æ˜¾ç¤ºï¼Œç®€å•çš„ç½‘ç»œå¾€å¾€ç»™å‡ºæ ¡å‡†å¾—åˆ†[2]ã€‚ç„¶è€Œï¼Œç”±äºç¥ç»ç½‘ç»œæ¨¡å‹æ¯å¤©éƒ½åœ¨å˜å¾—æ›´åŠ å¤æ‚ï¼Œç°ä»£ç¥ç»ç½‘ç»œå¾€å¾€æ ¡å‡†ä¸è¶³[1] [3]ã€‚å¦‚å›¾*10**.2*æ‰€ç¤ºï¼Œäº”å±‚LeNetæ ¡å‡†è‰¯å¥½ï¼Œå› ä¸ºå…¶ç½®ä¿¡æ°´å¹³ä¸é¢„æœŸçš„å‡†ç¡®æ€§ç´§å¯†åŒ¹é…ï¼Œè¿™ä»æ¡å½¢å›¾å¤§è‡´æ²¿å¯¹è§’çº¿å¯¹é½ä¸­å¯ä»¥çœ‹å‡ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶110å±‚ResNetå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼ˆæ›´ä½çš„é”™è¯¯ç‡ï¼‰ï¼Œä½†å…¶ç½®ä¿¡åˆ†æ•°ä¸è¿™ç§å‡†ç¡®æ€§å¹¶ä¸ç´§å¯†åŒ¹é…[1]ã€‚
- en: '![](img/B17259_10_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_10_02.jpg)'
- en: Figure 10.2 â€“ Reliability diagrams for a five-layer LeNet (left) and a 110-layer
    ResNet (right) on CIFAR-100 (adapted from Guo et al. [1])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.2 â€“ åœ¨CIFAR-100ä¸Šå¯¹äº”å±‚LeNetï¼ˆå·¦ï¼‰å’Œ110å±‚ResNetï¼ˆå³ï¼‰çš„å¯é æ€§å›¾ï¼ˆæ”¹ç¼–è‡ªGuoç­‰äºº[1]ï¼‰
- en: Next, we will learn how to measure whether a model is calibrated or not.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è¡¡é‡æ¨¡å‹æ˜¯å¦æ ¡å‡†ã€‚
- en: Calibration curves or reliability plot
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¡å‡†æ›²çº¿æˆ–å¯é æ€§å›¾
- en: Letâ€™s see how we can understand if the modelâ€™s scores are calibrated or not.
    Letâ€™s assume we have a model that predicts if an image is of a cat or not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ç†è§£æ¨¡å‹çš„å¾—åˆ†æ˜¯å¦æ ¡å‡†ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé¢„æµ‹å›¾åƒæ˜¯å¦ä¸ºçŒ«çš„æ¨¡å‹ã€‚
- en: A **calibration curve** is basically obtained by plotting the fraction of actual
    positive values (*y* axis) against predicted probability scores (*x* axis).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ ¡å‡†æ›²çº¿**åŸºæœ¬ä¸Šæ˜¯é€šè¿‡å°†å®é™…æ­£å€¼çš„æ¯”ä¾‹ï¼ˆ*y*è½´ï¼‰ä¸é¢„æµ‹æ¦‚ç‡å¾—åˆ†ï¼ˆ*x*è½´ï¼‰è¿›è¡Œç»˜å›¾æ¥è·å¾—çš„ã€‚'
- en: 'Letâ€™s see how to plot the calibration curve, also known as a **reliability
    plot**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ç»˜åˆ¶æ ¡å‡†æ›²çº¿ï¼Œä¹Ÿç§°ä¸º**å¯é æ€§å›¾**ï¼š
- en: 'Create a dataset with two columns: one with actual labels and another with
    predicted probability.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤åˆ—çš„æ•°æ®é›†ï¼šä¸€åˆ—æ˜¯å®é™…æ ‡ç­¾ï¼Œå¦ä¸€åˆ—æ˜¯é¢„æµ‹æ¦‚ç‡ã€‚
- en: Sort the data into ascending order using predicted probability.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„æµ‹æ¦‚ç‡å¯¹æ•°æ®è¿›è¡Œå‡åºæ’åºã€‚
- en: Divide the predicted probability dataset into fixed-size bins ranging from 0
    to 1\. For example, if we create 10 bins, we get 0.1, 0.2, 0.3, â€¦, 0.9, 1.0\.
    If there are too many examples in the dataset, we can use smaller-size bins and
    vice versa.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é¢„æµ‹æ¦‚ç‡æ•°æ®é›†åˆ’åˆ†ä¸ºä» 0 åˆ° 1 çš„å›ºå®šå¤§å°åŒºé—´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åˆ›å»º 10 ä¸ªåŒºé—´ï¼Œæˆ‘ä»¬å¾—åˆ° 0.1ã€0.2ã€0.3ã€â€¦â€¦ã€0.9ã€1.0ã€‚å¦‚æœæ•°æ®é›†ä¸­æœ‰å¤ªå¤šç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å°çš„åŒºé—´ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: Now compute the fraction of actual positives in each bin. These fraction values
    will be our *y* axis values. On the *x* axis, we plot the fixed bin value, i.e.,
    0.1, 0.2, 0.3, etc.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨è®¡ç®—æ¯ä¸ªåŒºé—´ä¸­å®é™…æ­£ä¾‹çš„æ¯”ä¾‹ã€‚è¿™äº›æ¯”ä¾‹å€¼å°†æ˜¯æˆ‘ä»¬çš„ *y* è½´å€¼ã€‚åœ¨ *x* è½´ä¸Šï¼Œæˆ‘ä»¬ç»˜åˆ¶å›ºå®šåŒºé—´å€¼ï¼Œå³ 0.1ã€0.2ã€0.3ã€ç­‰ç­‰ã€‚
- en: 'We get a plot like the one in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¾—åˆ°å¦‚ä¸‹å›¾ä¸­ç±»ä¼¼çš„å›¾è¡¨ï¼š
- en: '![](img/B17259_10_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_10_03.jpg)'
- en: Figure 10.3 â€“ Plotting the probability predictions of an XGBoost classifier
    against the fraction of positives
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.3 â€“ å°† XGBoost åˆ†ç±»å™¨çš„æ¦‚ç‡é¢„æµ‹ä¸æ­£ä¾‹æ¯”ä¾‹ç»˜åˆ¶æˆå›¾
- en: 'We need to be careful with the number of bins chosen because of the following
    reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å°å¿ƒé€‰æ‹©åŒºé—´çš„æ•°é‡ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›åŸå› ï¼š
- en: If we choose too few bins, the plot may look linear and well-fitted, giving
    the impression that the model is calibrated. More importantly, the real danger
    of using too few bins is that the curve wonâ€™t have enough detail; it will essentially
    be just a few points connected together.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é€‰æ‹©çš„åŒºé—´å¤ªå°‘ï¼Œå›¾è¡¨å¯èƒ½çœ‹èµ·æ¥æ˜¯çº¿æ€§çš„å¹¶ä¸”æ‹Ÿåˆå¾—å¾ˆå¥½ï¼Œç»™äººä¸€ç§æ¨¡å‹å·²ç»æ ¡å‡†çš„å°è±¡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨å¤ªå°‘åŒºé—´çš„çœŸæ­£å±é™©æ˜¯æ›²çº¿å°†æ²¡æœ‰è¶³å¤Ÿçš„ç»†èŠ‚ï¼›å®ƒæœ¬è´¨ä¸Šåªæ˜¯å‡ ä¸ªç‚¹è¿æ¥åœ¨ä¸€èµ·ã€‚
- en: Similarly, if we choose too many bins, then the plot may look noisy, and we
    may wrongly conclude the model to be uncalibrated.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹©çš„åŒºé—´å¤ªå¤šï¼Œé‚£ä¹ˆå›¾è¡¨å¯èƒ½çœ‹èµ·æ¥å¾ˆå˜ˆæ‚ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé”™è¯¯åœ°å¾—å‡ºæ¨¡å‹æœªæ ¡å‡†çš„ç»“è®ºã€‚
- en: It might become particularly difficult to identify whether a model is calibrated
    or not if we are dealing with imbalanced datasets. If our dataset is not balanced
    and has a much smaller number of examples of positive classes to plot, then the
    calibration plot may look noisy or show that the model is underconfident or overconfident.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¸å¹³è¡¡æ•°æ®é›†ï¼Œé‚£ä¹ˆç¡®å®šæ¨¡å‹æ˜¯å¦æ ¡å‡†å¯èƒ½ç‰¹åˆ«å›°éš¾ã€‚å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†ä¸å¹³è¡¡ï¼Œå¹¶ä¸”æ­£ä¾‹çš„ç¤ºä¾‹æ•°é‡å¾ˆå°‘ï¼Œé‚£ä¹ˆæ ¡å‡†å›¾å¯èƒ½çœ‹èµ·æ¥å¾ˆå˜ˆæ‚æˆ–æ˜¾ç¤ºæ¨¡å‹ä¸å¤Ÿè‡ªä¿¡æˆ–è¿‡äºè‡ªä¿¡ã€‚
- en: 'However, itâ€™s worth noting that many models are not perfectly calibrated and
    their calibration curves may deviate from the perfect calibration line:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šæ¨¡å‹å¹¶ä¸æ˜¯å®Œå…¨æ ¡å‡†çš„ï¼Œå®ƒä»¬çš„æ ¡å‡†æ›²çº¿å¯èƒ½ä¼šåç¦»å®Œç¾çš„æ ¡å‡†çº¿ï¼š
- en: '![](img/B17259_10_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_10_04.jpg)'
- en: Figure 10.4 â€“ A calibration curve plot when fitted via an XGBoost model; on
    the left is an overconfident model and on the right is an underconfident model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.4 â€“ é€šè¿‡ XGBoost æ¨¡å‹æ‹Ÿåˆçš„æ ¡å‡†æ›²çº¿å›¾ï¼›å·¦ä¾§æ˜¯ä¸€ä¸ªè¿‡äºè‡ªä¿¡çš„æ¨¡å‹ï¼Œå³ä¾§æ˜¯ä¸€ä¸ªä¸å¤Ÿè‡ªä¿¡çš„æ¨¡å‹
- en: '`scikit-learn` provides a function called `calibration_curve` to easily plot
    this curve:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` æä¾›äº†ä¸€ä¸ªåä¸º `calibration_curve` çš„å‡½æ•°ï¼Œå¯ä»¥è½»æ¾ç»˜åˆ¶æ­¤æ›²çº¿ï¼š'
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: However, visually judging and comparing various calibration plots can be error-prone,
    and we might want to use a metric that can make some kind of numerical comparison
    of the calibrations of two different models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè§†è§‰åˆ¤æ–­å’Œæ¯”è¾ƒå„ç§æ ¡å‡†å›¾å¯èƒ½ä¼šå‡ºé”™ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä½¿ç”¨ä¸€ä¸ªå¯ä»¥æ¯”è¾ƒä¸¤ä¸ªä¸åŒæ¨¡å‹æ ¡å‡†çš„æŸç§æ•°å€¼æ¯”è¾ƒæŒ‡æ ‡ã€‚
- en: Brier score
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Brier åˆ†æ•°
- en: 'There is a commonly used measure called the **Brier score**, which is basically
    the mean squared error of the predicted probability obtained from the model as
    follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªå¸¸ç”¨çš„åº¦é‡æ ‡å‡†ç§°ä¸º **Brier åˆ†æ•°**ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä»æ¨¡å‹è·å¾—çš„é¢„æµ‹æ¦‚ç‡çš„å‡æ–¹è¯¯å·®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: Brier score = Â 1Â _Â NÂ  âˆ‘ (predicted _ probability âˆ’ actual _ label)Â 2
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Brier åˆ†æ•° = 1/N âˆ‘ (é¢„æµ‹æ¦‚ç‡ - å®é™…æ ‡ç­¾)Â²
- en: where N is the number of examples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ N æ˜¯ç¤ºä¾‹æ•°é‡ã€‚
- en: 'This score varies between 0 (best possible score) and 1 (worst possible score).
    This metric is very similar to the `scikit-learn` makes our job a bit easier:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†æ•°ä»‹äº 0ï¼ˆæœ€ä½³å¯èƒ½åˆ†æ•°ï¼‰å’Œ 1ï¼ˆæœ€å·®å¯èƒ½åˆ†æ•°ï¼‰ä¹‹é—´ã€‚è¿™ä¸ªæŒ‡æ ‡ä¸ `scikit-learn` éå¸¸ç›¸ä¼¼ï¼Œå®ƒä½¿æˆ‘ä»¬çš„å·¥ä½œå˜å¾—å®¹æ˜“ä¸€äº›ï¼š
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This outputs the following Brier score loss value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¾“å‡ºä»¥ä¸‹ Brier åˆ†æ•°æŸå¤±å€¼ï¼š
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The paper *Class Probability Estimates are Unreliable for Imbalanced Data (and
    How to Fix Them)* by Wallace and Dahabreh [4] argues that a lower Brier score
    for an imbalanced dataset might just mean that the calibration is good overall
    but not necessarily for minority or rare classes. In order to track the calibration
    of individual classes, they proposed a stratified Brier score:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Wallace å’Œ Dahabreh çš„è®ºæ–‡ã€Šä¸å¹³è¡¡æ•°æ®ä¸­çš„ç±»åˆ«æ¦‚ç‡ä¼°è®¡ä¸å¯é ï¼ˆä»¥åŠå¦‚ä½•ä¿®å¤å®ƒä»¬ï¼‰ã€‹ï¼ˆ[4]ï¼‰è®¤ä¸ºï¼Œå¯¹äºä¸å¹³è¡¡æ•°æ®é›†ï¼Œè¾ƒä½çš„ Brier
    åˆ†æ•°å¯èƒ½ä»…ä»…æ„å‘³ç€æ ¡å‡†æ€»ä½“ä¸Šæ˜¯å¥½çš„ï¼Œä½†å¹¶ä¸ä¸€å®šé€‚ç”¨äºå°‘æ•°æˆ–ç¨€æœ‰ç±»åˆ«ã€‚ä¸ºäº†è·Ÿè¸ªå•ä¸ªç±»åˆ«çš„æ ¡å‡†ï¼Œä»–ä»¬æå‡ºäº†åˆ†å±‚ Brier åˆ†æ•°ï¼š
- en: '![](img/B17259_10_05.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_10_05.jpg)'
- en: Figure 10.5 â€“ Stratified Brier scores for positive and negative classes [4]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.5 â€“ æ­£è´Ÿç±»çš„åˆ†å±‚Brieråˆ†æ•° [4]
- en: where NÂ pos denotes the number of positive class examples, NÂ neg denotes the
    number of negative class examples, yÂ i is the label, and Â Ë†Â PÂ  is the model prediction
    score.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­NÂ posè¡¨ç¤ºæ­£ç±»ç¤ºä¾‹çš„æ•°é‡ï¼ŒNÂ negè¡¨ç¤ºè´Ÿç±»ç¤ºä¾‹çš„æ•°é‡ï¼ŒyÂ iæ˜¯æ ‡ç­¾ï¼ŒË†Â Pæ˜¯æ¨¡å‹é¢„æµ‹åˆ†æ•°ã€‚
- en: Letâ€™s look at an alternative metric to measure calibration that is more popular
    among deep learning models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªæ›´å—æ¬¢è¿çš„æ›¿ä»£æŒ‡æ ‡æ¥è¡¡é‡æ ¡å‡†ï¼Œè¿™åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æ›´ä¸ºå¸¸è§ã€‚
- en: Expected Calibration Error
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„æœŸæ ¡å‡†è¯¯å·®
- en: '**Expected Calibration Error** (**ECE**) [5] is another metric for measuring
    how calibrated a model is. Predicted probabilities from the model are grouped
    into M bins of equal size. Letâ€™s assume BÂ m is the set of examples whose prediction
    scores fall into the *m*th bin.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¢„æœŸæ ¡å‡†è¯¯å·®**ï¼ˆ**ECE**ï¼‰[5]æ˜¯è¡¡é‡æ¨¡å‹æ ¡å‡†ç¨‹åº¦çš„å¦ä¸€ä¸ªæŒ‡æ ‡ã€‚æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡è¢«åˆ†ç»„åˆ°Mä¸ªå¤§å°ç›¸ç­‰çš„åŒºé—´ä¸­ã€‚å‡è®¾BÂ mæ˜¯é¢„æµ‹åˆ†æ•°è½åœ¨ç¬¬*m*ä¸ªåŒºé—´çš„ç¤ºä¾‹é›†åˆã€‚'
- en: Then for each bin (BÂ m), we calculate the difference between the average predicted
    probability (that is, conf(BÂ m)) and accuracy (that is, the proportion of examples
    correctly classified). This difference is | acc(BÂ m) âˆ’ conf(BÂ m)|.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¯¹äºæ¯ä¸ªåŒºé—´ï¼ˆBÂ mï¼‰ï¼Œæˆ‘ä»¬è®¡ç®—å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼ˆå³conf(BÂ m)ï¼‰å’Œå‡†ç¡®ç‡ï¼ˆå³æ­£ç¡®åˆ†ç±»çš„ç¤ºä¾‹æ¯”ä¾‹ï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ä¸ªå·®å¼‚æ˜¯| acc(BÂ m) âˆ’
    conf(BÂ m)|ã€‚
- en: 'We also weigh these differences by the number of examples in each bin and finally
    sum them up to get the overall ECE value. This is equivalent to multiplying the
    difference by BÂ m/n, where n is the total number of examples. Finally, we sum
    this over all the bins to get the final formula:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ ¹æ®æ¯ä¸ªåŒºé—´çš„ç¤ºä¾‹æ•°é‡æƒè¡¡è¿™äº›å·®å¼‚ï¼Œæœ€åå°†å®ƒä»¬åŠ èµ·æ¥å¾—åˆ°æ€»çš„ECEå€¼ã€‚è¿™ç›¸å½“äºå°†å·®å¼‚ä¹˜ä»¥BÂ m/nï¼Œå…¶ä¸­næ˜¯ç¤ºä¾‹æ€»æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªå€¼å¯¹æ‰€æœ‰åŒºé—´æ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„å…¬å¼ï¼š
- en: ECE = âˆ‘Â m=1Â MÂ |BÂ m| /n *|acc(BÂ m) âˆ’ conf(BÂ m)|
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ECE = âˆ‘Â m=1Â MÂ |BÂ m| /n *|acc(BÂ m) âˆ’ conf(BÂ m)|
- en: 'Accuracy and confidence can be defined as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®ç‡å’Œç½®ä¿¡åº¦å¯ä»¥å®šä¹‰ä¸ºä»¥ä¸‹ï¼š
- en: 'Accuracy acc(BÂ m) is the proportion of examples in bin BÂ m correctly classified
    by the model:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡†ç¡®ç‡acc(BÂ m)æ˜¯æ¨¡å‹æ­£ç¡®åˆ†ç±»åˆ°BÂ måŒºé—´çš„ç¤ºä¾‹æ¯”ä¾‹ï¼š
- en: acc(BÂ m) = (1 / |BÂ m|)* âˆ‘Â i=1Â |BÂ m|Â I(yÂ i = Å·Â i)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: acc(BÂ m) = (1 / |BÂ m|)* âˆ‘Â i=1Â |BÂ m|Â I(yÂ i = Å·Â i)
- en: 'Confidence is the average predicted probability of the examples in bin BÂ m:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½®ä¿¡åº¦æ˜¯BÂ måŒºé—´ä¸­ç¤ºä¾‹çš„å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼š
- en: conf(BÂ m) = (1 / |BÂ m|)* âˆ‘Â i=1Â |BÂ m|Â pÂ i
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: conf(BÂ m) = (1 / |BÂ m|)* âˆ‘Â i=1Â |BÂ m|Â pÂ i
- en: 'There is an extension of the previous metric called **Maximum Calibration Error**
    (**MCE**) that measures the largest difference between the accuracy and confidence
    *across all* *the bins*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªä¹‹å‰æŒ‡æ ‡çš„æ‰©å±•ï¼Œç§°ä¸º**æœ€å¤§æ ¡å‡†è¯¯å·®**ï¼ˆ**MCE**ï¼‰ï¼Œå®ƒè¡¡é‡äº†æ‰€æœ‰åŒºé—´ä¸­å‡†ç¡®ç‡å’Œç½®ä¿¡åº¦ä¹‹é—´çš„æœ€å¤§å·®å¼‚ï¼š
- en: MCE = ma xÂ m=1Â MÂ |acc(BÂ m) âˆ’ conf(BÂ m)|
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MCE = ma xÂ m=1Â MÂ |acc(BÂ m) âˆ’ conf(BÂ m)|
- en: This can be useful in applications where it is important that the model be well
    calibrated in all bins, and MCE can then be minimized.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨æ¨¡å‹éœ€è¦åœ¨æ‰€æœ‰åŒºé—´å†…éƒ½å¾ˆå¥½åœ°æ ¡å‡†çš„åº”ç”¨ä¸­éå¸¸æœ‰ç”¨ï¼Œå¹¶ä¸”å¯ä»¥æœ€å°åŒ–MCEã€‚
- en: '*Figure 10**.6* shows a reliability diagram with the ECE and MCE values on
    the MNIST dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾10**.6*æ˜¾ç¤ºäº†MNISTæ•°æ®é›†ä¸Šçš„å¯é æ€§å›¾ï¼Œå…¶ä¸­åŒ…å«ECEå’ŒMCEå€¼ï¼š'
- en: '![](img/B17259_10_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_06.jpg)'
- en: Figure 10.6 â€“ Reliability diagram with ECE and MCE values for the MNIST dataset
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.6 â€“ MNISTæ•°æ®é›†çš„å¯é æ€§å›¾ï¼ŒåŒ…å«ECEå’ŒMCEå€¼
- en: ğŸš€ Model calibration in production at Netflix
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Netflixåœ¨ç”Ÿäº§ä¸­çš„æ¨¡å‹æ ¡å‡†
- en: '**ğŸ¯** **Problem** **being solved:**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¯** **è§£å†³çš„é—®é¢˜ï¼š**'
- en: Netflix aimed to provide recommendations [6] that were closely aligned with
    a userâ€™s varied interests rather than focusing solely on their main preferences.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Netflixæ—¨åœ¨æä¾›ä¸ç”¨æˆ·å¤šæ ·åŒ–çš„å…´è¶£ç´§å¯†ç›¸å…³çš„æ¨è[6]ï¼Œè€Œä¸æ˜¯ä»…ä»…å…³æ³¨ä»–ä»¬çš„ä¸»è¦åå¥½ã€‚
- en: '**âš–ï¸****Data imbalance:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**âš–ï¸** **æ•°æ®ä¸å¹³è¡¡ï¼š**'
- en: Traditional recommendation systems can amplify a userâ€™s primary interests, thereby
    overshadowing their secondary or tertiary preferences. This can be considered
    a form of interest imbalance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿå¯èƒ½ä¼šæ”¾å¤§ç”¨æˆ·çš„ä¸»è¦å…´è¶£ï¼Œä»è€Œæ©ç›–ä»–ä»¬çš„æ¬¡è¦æˆ–ç¬¬ä¸‰çº§åå¥½ã€‚è¿™å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§å…´è¶£ä¸å¹³è¡¡ã€‚
- en: '**ğŸ¨** **Model** **calibration strategy:**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¨** **æ¨¡å‹æ ¡å‡†ç­–ç•¥ï¼š**'
- en: Netflix employed a greedy re-ranking approach to calibration. The initial model
    ranked movies based on the predicted likelihood of a user watching them. This
    ranking is then adjusted using a greedy algorithm to ensure that the top 10 recommended
    movies match the genre distribution in the userâ€™s watch history.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Netflixé‡‡ç”¨äº†è´ªå©ªé‡æ’åºæ–¹æ³•è¿›è¡Œæ ¡å‡†ã€‚åˆå§‹æ¨¡å‹æ ¹æ®ç”¨æˆ·è§‚çœ‹ç”µå½±çš„é¢„æµ‹æ¦‚ç‡å¯¹ç”µå½±è¿›è¡Œæ’åºã€‚ç„¶åä½¿ç”¨è´ªå©ªç®—æ³•è°ƒæ•´æ’åºï¼Œä»¥ç¡®ä¿å‰10ä¸ªæ¨èç”µå½±ä¸ç”¨æˆ·è§‚çœ‹å†å²ä¸­çš„ç±»å‹åˆ†å¸ƒç›¸åŒ¹é…ã€‚
- en: '**Example**: If a userâ€™s watch history comprises 50% action, 30% comedy, and
    20% drama, the re-ranking algorithm reshuffles the top recommendations to reflect
    this distribution.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹**ï¼šå¦‚æœä¸€ä¸ªç”¨æˆ·çš„è§‚çœ‹å†å²åŒ…æ‹¬50%çš„åŠ¨ä½œç‰‡ï¼Œ30%çš„å–œå‰§ç‰‡å’Œ20%çš„å‰§æƒ…ç‰‡ï¼Œé‡æ–°æ’åºç®—æ³•ä¼šé‡æ–°æ’åˆ—é¡¶çº§æ¨èä»¥åæ˜ è¿™ç§åˆ†å¸ƒã€‚'
- en: '**ğŸ“Š****Additional points:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“Š** **é™„åŠ ç‚¹**ï¼š'
- en: The greedy re-ranking algorithm was straightforward to implement. It was empirically
    shown to improve recommendation performance across various datasets.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è´ªå©ªé‡æ–°æ’åºç®—æ³•æ˜“äºå®ç°ã€‚ç»éªŒä¸Šè¯æ˜ï¼Œå®ƒåœ¨å„ç§æ•°æ®é›†ä¸Šæé«˜äº†æ¨èæ€§èƒ½ã€‚
- en: This approach ensured that Netflixâ€™s recommendations cater to the full spectrum
    of a userâ€™s interests, preventing any single interest from dominating the suggestions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ç¡®ä¿äº†Netflixçš„æ¨èèƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·å…´è¶£çš„å…¨è°±ç³»ï¼Œé˜²æ­¢ä»»ä½•å•ä¸€å…´è¶£ä¸»å¯¼å»ºè®®ã€‚
- en: In the next section, letâ€™s try to understand how data balancing techniques can
    affect the calibration of models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œè®©æˆ‘ä»¬å°è¯•äº†è§£æ•°æ®å¹³è¡¡æŠ€æœ¯å¦‚ä½•å½±å“æ¨¡å‹çš„æ ¡å‡†ã€‚
- en: The influence of data balancing techniques on model calibration
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®å¹³è¡¡æŠ€æœ¯å¯¹æ¨¡å‹æ ¡å‡†çš„å½±å“
- en: The usual impact of applying data-level techniques, such as oversampling and
    undersampling, is that they change the distribution of the training data for the
    model. This means that the model sees an almost equal number of all the classes,
    which doesnâ€™t reflect the actual data distribution. Because of this, the model
    becomes less calibrated against the true imbalanced distribution of data. Similarly,
    algorithm-level cost-sensitive techniques that use `class_weight` to account for
    the data imbalance have a similar degraded impact on degrading the calibration
    of the model against the true data distribution. *Figure 10**.7* (log scale) from
    a recent study [7] shows the degrading calibration of a CNN-based model for pneumonia
    detection task, as `class_weight` increases from 0.5 to 0.9 to 0.99\. The model
    becomes over-confident and hence less calibrated with the increase in `class_weight`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨æ•°æ®çº§æŠ€æœ¯ï¼Œå¦‚è¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ï¼Œçš„é€šå¸¸å½±å“æ˜¯å®ƒä»¬æ”¹å˜äº†æ¨¡å‹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒã€‚è¿™æ„å‘³ç€æ¨¡å‹çœ‹åˆ°æ‰€æœ‰ç±»åˆ«çš„æ•°é‡å‡ ä¹ç›¸ç­‰ï¼Œè¿™å¹¶ä¸åæ˜ å®é™…çš„æ•°æ®åˆ†å¸ƒã€‚å› æ­¤ï¼Œæ¨¡å‹å¯¹çœŸå®ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒçš„æ ¡å‡†ç¨‹åº¦é™ä½ã€‚åŒæ ·ï¼Œä½¿ç”¨`class_weight`æ¥å¤„ç†æ•°æ®ä¸å¹³è¡¡çš„ç®—æ³•çº§æˆæœ¬æ•æ„ŸæŠ€æœ¯å¯¹æ¨¡å‹å¯¹çœŸå®æ•°æ®åˆ†å¸ƒçš„æ ¡å‡†ä¹Ÿæœ‰ç±»ä¼¼çš„è´Ÿé¢å½±å“ã€‚*å›¾10.7*ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰æ¥è‡ªæœ€è¿‘çš„ç ”ç©¶[7]ï¼Œå±•ç¤ºäº†åŸºäºCNNçš„è‚ºç‚æ£€æµ‹ä»»åŠ¡çš„æ¨¡å‹æ ¡å‡†çš„é€€åŒ–ï¼Œéšç€`class_weight`ä»0.5å¢åŠ åˆ°0.9åˆ°0.99ã€‚éšç€`class_weight`çš„å¢åŠ ï¼Œæ¨¡å‹å˜å¾—è¿‡äºè‡ªä¿¡ï¼Œå› æ­¤ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„æ ¡å‡†ç¨‹åº¦é™ä½ã€‚
- en: '![](img/B17259_10_07.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_07.jpg)'
- en: Figure 10.7 â€“ Degrading calibration of a CNN model as class_weight changes from
    0.5 to 0.9 to 0.99 (log scale) (image adapted from Caplin, et al. [7])
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.7 â€“ éšç€class_weightä»0.5å˜åŒ–åˆ°0.9åˆ°0.99ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰çš„CNNæ¨¡å‹æ ¡å‡†é€€åŒ–ï¼ˆå›¾åƒæ”¹ç¼–è‡ªCaplinç­‰äºº[7]ï¼‰
- en: Similarly, in *Figure 10**.8*, we show the calibration curve for the logistic
    regression model on the `thyroid_sick` UCI dataset. The corresponding notebook
    can be found in the GitHub repo of this book.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œåœ¨*å›¾10.8*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨`thyroid_sick` UCIæ•°æ®é›†ä¸Šé€»è¾‘å›å½’æ¨¡å‹çš„æ ¡å‡†æ›²çº¿ã€‚ç›¸åº”çš„ç¬”è®°æœ¬å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚
- en: '![](img/B17259_10_08.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_08.jpg)'
- en: Figure 10.8 â€“ A calibration curve using logistic regression with no sampling
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.8 â€“ ä½¿ç”¨æ— é‡‡æ ·çš„é€»è¾‘å›å½’æ ¡å‡†æ›²çº¿
- en: '*Figure 10**.9* and *Figure 10**.10* demonstrate how oversampling techniques
    can worsen a modelâ€™s calibration:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾10.9*å’Œ*å›¾10.10*å±•ç¤ºäº†è¿‡é‡‡æ ·æŠ€æœ¯å¦‚ä½•æ¶åŒ–æ¨¡å‹çš„æ ¡å‡†ï¼š'
- en: '![](img/B17259_10_09.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_09.jpg)'
- en: Figure 10.9 â€“ A calibration curve using logistic regression with random oversampling
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.9 â€“ ä½¿ç”¨éšæœºè¿‡é‡‡æ ·çš„é€»è¾‘å›å½’æ ¡å‡†æ›²çº¿
- en: '![](img/B17259_10_10.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_10.jpg)'
- en: Figure 10.10 â€“ A calibration curve using logistic regression with SMOTE
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.10 â€“ ä½¿ç”¨SMOTEçš„é€»è¾‘å›å½’æ ¡å‡†æ›²çº¿
- en: 'Similarly, *Figure 10**.11* shows a similar effect for undersampling techniques:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œ*å›¾10.11*å±•ç¤ºäº†æ¬ é‡‡æ ·æŠ€æœ¯äº§ç”Ÿçš„ç±»ä¼¼æ•ˆæœï¼š
- en: '![](img/B17259_10_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_11.jpg)'
- en: Figure 10.11 â€“ A calibration curve using logistic regression with random undersampling
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.11 â€“ ä½¿ç”¨éšæœºæ¬ é‡‡æ ·çš„é€»è¾‘å›å½’æ ¡å‡†æ›²çº¿
- en: '*Figure 10**.12* shows how class weighting can negatively affect the model
    calibration:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾10.12*å±•ç¤ºäº†ç±»æƒé‡å¦‚ä½•å¯¹æ¨¡å‹æ ¡å‡†äº§ç”Ÿè´Ÿé¢å½±å“ï¼š'
- en: '![](img/B17259_10_12.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_12.jpg)'
- en: Figure 10.12 â€“ A calibration curve with no sampling technique (left) and class
    weighting (right) using logistic regression
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.12 â€“ æ— é‡‡æ ·æŠ€æœ¯ï¼ˆå·¦ï¼‰å’Œç±»æƒé‡ï¼ˆå³ï¼‰ä½¿ç”¨é€»è¾‘å›å½’çš„æ ¡å‡†æ›²çº¿
- en: In the plots that we just saw, both undersampling and oversampling made the
    model over-confident. Undersampling can make the model optimistic about its ability
    to classify the minority class while oversampling can lead the model to overestimate
    the likelihood of encountering minority instances. This overconfidence arises
    because the model assumes the altered training data represents the real-world
    distribution. To elaborate, when we undersample or oversample, weâ€™re essentially
    telling the model that the minority class is more common (or less rare) than it
    actually is. The model can then generalize this skewed view to new, unseen data.
    As a result, it can become overconfident in its predictions for the minority class,
    thinking these outcomes are more likely than they actually are. This overconfidence
    doesnâ€™t extend to the majority class because the model still sees plenty of those
    examples during training. Therefore, the model ends up being miscalibrated and
    tends to be overly sure of its predictions for the minority class.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„å›¾è¡¨ä¸­ï¼Œæ— è®ºæ˜¯æ¬ é‡‡æ ·è¿˜æ˜¯è¿‡é‡‡æ ·éƒ½ä½¿æ¨¡å‹è¿‡äºè‡ªä¿¡ã€‚æ¬ é‡‡æ ·å¯èƒ½ä¼šä½¿æ¨¡å‹å¯¹å…¶åˆ†ç±»å°‘æ•°ç±»çš„èƒ½åŠ›è¿‡äºä¹è§‚ï¼Œè€Œè¿‡é‡‡æ ·å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹é«˜ä¼°é‡åˆ°å°‘æ•°å®ä¾‹çš„å¯èƒ½æ€§ã€‚è¿™ç§è¿‡è‡ªä¿¡æ˜¯ç”±äºæ¨¡å‹å‡è®¾ä¿®æ”¹åçš„è®­ç»ƒæ•°æ®ä»£è¡¨äº†çœŸå®ä¸–ç•Œçš„åˆ†å¸ƒã€‚ä¸ºäº†è¯¦ç»†è¯´æ˜ï¼Œå½“æˆ‘ä»¬è¿›è¡Œæ¬ é‡‡æ ·æˆ–è¿‡é‡‡æ ·æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨å‘Šè¯‰æ¨¡å‹å°‘æ•°ç±»çš„å‡ºç°é¢‘ç‡æ¯”å®é™…æƒ…å†µæ›´é«˜ï¼ˆæˆ–æ›´ä¸å¸¸è§ï¼‰ã€‚ç„¶åï¼Œæ¨¡å‹å¯ä»¥å°†è¿™ç§æ‰­æ›²çš„è§‚ç‚¹æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ã€‚å› æ­¤ï¼Œå®ƒå¯èƒ½ä¼šå¯¹å…¶å¯¹å°‘æ•°ç±»çš„é¢„æµ‹è¿‡äºè‡ªä¿¡ï¼Œè®¤ä¸ºè¿™äº›ç»“æœæ¯”å®é™…æƒ…å†µæ›´æœ‰å¯èƒ½ã€‚è¿™ç§è¿‡è‡ªä¿¡å¹¶ä¸é€‚ç”¨äºå¤šæ•°ç±»ï¼Œå› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ç„¶çœ‹åˆ°äº†å¤§é‡çš„è¿™äº›ä¾‹å­ã€‚å› æ­¤ï¼Œæ¨¡å‹æœ€ç»ˆä¼šæ ¡å‡†ä¸å½“ï¼Œå¹¶ä¸”å€¾å‘äºå¯¹å…¶å¯¹å°‘æ•°ç±»çš„é¢„æµ‹è¿‡äºè‡ªä¿¡ã€‚
- en: In the following section, we will utilize a real-world dataset, train a model
    using this dataset, and then determine the calibration of the model by plotting
    calibration curves.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ï¼Œä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åé€šè¿‡ç»˜åˆ¶æ ¡å‡†æ›²çº¿æ¥ç¡®å®šæ¨¡å‹çš„æ ¡å‡†æƒ…å†µã€‚
- en: Plotting calibration curves for a model trained on a real-world dataset
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶åŸºäºçœŸå®ä¸–ç•Œæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹çš„æ ¡å‡†æ›²çº¿
- en: Model calibration should ideally be done on a dataset that is separate from
    the training and test set. Why? Itâ€™s to avoid overfitting because the model can
    become too tailored to the training/test setâ€™s unique characteristics.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†ç†æƒ³æƒ…å†µä¸‹åº”è¯¥åœ¨ç‹¬ç«‹äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ•°æ®é›†ä¸Šè¿›è¡Œã€‚ä¸ºä»€ä¹ˆï¼Ÿè¿™æ˜¯ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½ä¼šå˜å¾—è¿‡äºé€‚åº”è®­ç»ƒ/æµ‹è¯•é›†çš„ç‹¬ç‰¹ç‰¹å¾ã€‚
- en: We can have a hold-out dataset that has been specifically set aside for model
    calibration. In some cases, we may have too little data to justify splitting it
    further into a separate hold-out dataset for calibration. In such cases, a practical
    compromise might be to use the test set for calibration, assuming that the test
    set has the same distribution as the dataset on which the model will be used to
    make final predictions. However, we should keep in mind that after calibrating
    on the test set, we no longer have an unbiased estimate of the final performance
    of the model, and we need to be cautious about interpreting the modelâ€™s performance
    metrics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä¿ç•™ä¸€ä¸ªä¸“é—¨ç”¨äºæ¨¡å‹æ ¡å‡†çš„ä¿ç•™æ•°æ®é›†ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½æ•°æ®å¤ªå°‘ï¼Œæ— æ³•è¯æ˜å°†å…¶è¿›ä¸€æ­¥åˆ†å‰²ä¸ºå•ç‹¬çš„ä¿ç•™æ•°æ®é›†è¿›è¡Œæ ¡å‡†çš„åˆç†æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªå®ç”¨çš„æŠ˜è¡·æ–¹æ¡ˆå¯èƒ½æ˜¯ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†ï¼Œå‡è®¾æµ‹è¯•é›†ä¸æ¨¡å‹å°†ç”¨äºæœ€ç»ˆé¢„æµ‹çš„æ•°æ®é›†å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åº”è¯¥è®°ä½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ ¡å‡†åï¼Œæˆ‘ä»¬ä¸å†æœ‰æ¨¡å‹æœ€ç»ˆæ€§èƒ½çš„æ— åä¼°è®¡ï¼Œæˆ‘ä»¬éœ€è¦è°¨æ…åœ°è§£é‡Šæ¨¡å‹æ€§èƒ½æŒ‡æ ‡ã€‚
- en: We use the `HR Data for Analytics` dataset from Kaggle ([https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics](https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics)).
    This dataset contains employee profiles of a large company, where each record
    is an employee.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ Kaggle ä¸Šçš„ `HR Data for Analytics` æ•°æ®é›†ï¼ˆ[https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics](https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics)ï¼‰ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº†ä¸€å®¶å¤§å‹å…¬å¸çš„å‘˜å·¥æ¡£æ¡ˆï¼Œå…¶ä¸­æ¯æ¡è®°å½•ä»£è¡¨ä¸€åå‘˜å·¥ã€‚
- en: 'The downloaded dataset `HR_comma_sep.csv` has been added to the GitHub repo
    of the book. Letâ€™s load the dataset into a `pandas` DataFrame:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è½½çš„æ•°æ®é›† `HR_comma_sep.csv` å·²æ·»åŠ åˆ°æœ¬ä¹¦çš„ GitHub ä»“åº“ä¸­ã€‚è®©æˆ‘ä»¬å°†æ•°æ®é›†åŠ è½½åˆ°ä¸€ä¸ª `pandas` DataFrame
    ä¸­ï¼š
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This shows some sample rows from the dataset:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†æ•°æ®é›†çš„ä¸€äº›æ ·æœ¬è¡Œï¼š
- en: '|  | `last_evaluation` | `left` | ... | `sales` | `salary` |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | `last_evaluation` | `left` | ... | `sales` | `salary` |'
- en: '| `0` | 0.53 | 1 | ... | sales | low |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `0` | 0.53 | 1 | ... | é”€å”®é¢ | ä½ |'
- en: '| `1` | 0.86 | 1 | ... | sales | medium |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `1` | 0.86 | 1 | ... | é”€å”®é¢ | ä¸­ç­‰ |'
- en: '| `2` | 0.88 | 1 | ... | sales | medium |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `2` | 0.88 | 1 | ... | é”€å”®é¢ | ä¸­ç­‰ |'
- en: '| `3` | 0.87 | 1 | ... | sales | low |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `3` | 0.87 | 1 | ... | é”€å”®é¢ | ä½ |'
- en: '| `4` | 0.52 | 1 | ... | sales | low |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| `4` | 0.52 | 1 | ... | é”€å”®é¢ | ä½ |'
- en: '| `...` | ... | ... | ... | ... | ... |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `...` | ... | ... | ... | ... | ... |'
- en: '| `14994` | 0.57 | 1 | ... | support | low |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `14994` | 0.57 | 1 | ... | æ”¯æŒåº¦ | ä½ |'
- en: '| `14995` | 0.48 | 1 | ... | support | low |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `14995` | 0.48 | 1 | ... | æ”¯æŒåº¦ | ä½ |'
- en: '| `14996` | 0.53 | 1 | ... | support | low |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `14996` | 0.53 | 1 | ... | æ”¯æŒåº¦ | ä½ |'
- en: '| `14997` | 0.96 | 1 | ... | support | low |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `14997` | 0.96 | 1 | ... | æ”¯æŒåº¦ | ä½ |'
- en: '| `14998` | 0.52 | 1 | ... | support | low |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `14998` | 0.52 | 1 | ... | æ”¯æŒåº¦ | ä½ |'
- en: Table 10.1 â€“ Sample rows from the HR Data for Analytics dataset from Kaggle
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨10.1 â€“ Kaggleçš„HR Data for Analyticsæ•°æ®é›†çš„æ ·æœ¬è¡Œ
- en: 'Itâ€™s clear that some of the columns, such as `sales` and `salary`, are categorical.
    The `left` column is our labels column. Letâ€™s get the imbalance in the dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆæ˜æ˜¾ï¼Œä¸€äº›åˆ—ï¼Œå¦‚`sales`å’Œ`salary`ï¼Œæ˜¯åˆ†ç±»çš„ã€‚`left`åˆ—æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾åˆ—ã€‚è®©æˆ‘ä»¬è·å–æ•°æ®é›†çš„ä¸å¹³è¡¡æƒ…å†µï¼š
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us the count of the labels:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº†æ ‡ç­¾çš„æ•°é‡ï¼š
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We need to convert the categorical columns into ID labels using `LabelEncoder`
    and then standardize these columns using `StandardScaler` from `sklearn`. After
    preprocessing, we split the dataset into three subsets: 80% for the training set
    and 10% each for the validation and test sets. Weâ€™ll skip the code for these steps
    and jump straight into training the model. For the complete code, please refer
    to the accompanying GitHub notebook.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä½¿ç”¨`LabelEncoder`å°†åˆ†ç±»åˆ—è½¬æ¢ä¸ºIDæ ‡ç­¾ï¼Œç„¶åä½¿ç”¨`sklearn`ä¸­çš„`StandardScaler`å¯¹è¿™äº›åˆ—è¿›è¡Œæ ‡å‡†åŒ–ã€‚é¢„å¤„ç†å®Œæˆåï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åˆ†ä¸ºä¸‰ä¸ªå­é›†ï¼š80%ç”¨äºè®­ç»ƒé›†ï¼Œ10%åˆ†åˆ«ç”¨äºéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬å°†è·³è¿‡è¿™äº›æ­¥éª¤çš„ä»£ç ï¼Œç›´æ¥è¿›å…¥æ¨¡å‹è®­ç»ƒã€‚å®Œæ•´çš„ä»£ç è¯·å‚é˜…é™„å¸¦çš„GitHubç¬”è®°æœ¬ã€‚
- en: As usual, we will train a random forest model on the training set and use the
    test set for evaluating the model. We will use a validation set for calibrating
    the model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å°†åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æµ‹è¯•é›†æ¥è¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨éªŒè¯é›†æ¥æ ¡å‡†æ¨¡å‹ã€‚
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s figure out how calibrated the model is. We print the Brierâ€™s score and
    plot the calibration curve:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹æ¨¡å‹çš„æ ¡å‡†ç¨‹åº¦ã€‚æˆ‘ä»¬æ‰“å°Brieråˆ†æ•°å¹¶ç»˜åˆ¶æ ¡å‡†æ›²çº¿ï¼š
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This gives the following output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Letâ€™s plot the calibration curve:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶æ ¡å‡†æ›²çº¿ï¼š
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B17259_10_13_New.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_10_13_New.jpg)'
- en: Figure 10.13 â€“ A calibration curve of an uncalibrated random forest model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.13 â€“ æœªæ ¡å‡†çš„éšæœºæ£®æ—æ¨¡å‹çš„æ ¡å‡†æ›²çº¿
- en: '*Figure 10**.13* shows that the model is overconfident in the initial range
    of predictions (0 to ~0.4) and is then underconfident afterward.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾10.13* æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨é¢„æµ‹èŒƒå›´çš„åˆå§‹é˜¶æ®µï¼ˆ0åˆ°çº¦0.4ï¼‰è¿‡äºè‡ªä¿¡ï¼Œéšååˆ™ä¸å¤Ÿè‡ªä¿¡ã€‚'
- en: In the next section, we will look at some techniques to improve the calibration
    of the models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€äº›æé«˜æ¨¡å‹æ ¡å‡†çš„æŠ€æœ¯ã€‚
- en: Model calibration techniques
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ ¡å‡†æŠ€æœ¯
- en: 'There are several ways to calibrate a model. There are two broad categorizations
    of the calibration techniques based on the nature of the method used to adjust
    the predicted probabilities to better align with the true probabilities: parametric
    and non-parametric:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§æ–¹æ³•å¯ä»¥æ ¡å‡†æ¨¡å‹ã€‚æ ¹æ®è°ƒæ•´é¢„æµ‹æ¦‚ç‡ä»¥æ›´å¥½åœ°ä¸çœŸå®æ¦‚ç‡å¯¹é½æ‰€ä½¿ç”¨çš„æ–¹æ³•çš„æ€§è´¨ï¼Œæ ¡å‡†æŠ€æœ¯å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼šå‚æ•°åŒ–å’Œéå‚æ•°åŒ–ï¼š
- en: '**Parametric methods**: These methods assume a specific functional form for
    the relationship between the predicted probabilities and the true probabilities.
    They have a set number of parameters that need to be estimated from the data.
    Once these parameters are estimated, the calibration function is fully specified.
    Examples include Platt scaling, which assumes a logistic function, and beta calibration,
    which assumes a beta distribution. We will also discuss temperature scaling and
    label smoothing.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚æ•°åŒ–æ–¹æ³•**ï¼šè¿™äº›æ–¹æ³•å‡è®¾é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ¦‚ç‡ä¹‹é—´çš„å…³ç³»å…·æœ‰ç‰¹å®šçš„å‡½æ•°å½¢å¼ã€‚å®ƒä»¬å…·æœ‰ä¸€ç»„éœ€è¦ä»æ•°æ®ä¸­ä¼°è®¡çš„å‚æ•°ã€‚ä¸€æ—¦è¿™äº›å‚æ•°è¢«ä¼°è®¡ï¼Œæ ¡å‡†å‡½æ•°å°±å®Œå…¨ç¡®å®šäº†ã€‚ä¾‹å¦‚ï¼ŒPlattç¼©æ”¾æ³•å‡è®¾å¯¹æ•°å‡½æ•°ï¼Œbetaæ ¡å‡†å‡è®¾betaåˆ†å¸ƒã€‚æˆ‘ä»¬è¿˜å°†è®¨è®ºæ¸©åº¦ç¼©æ”¾å’Œæ ‡ç­¾å¹³æ»‘ã€‚'
- en: '**Non-parametric methods**: These methods do not assume a specific functional
    form for the calibration function. They are more flexible and can adapt to more
    complex relationships between the predicted and true probabilities. However, they
    often require more data to produce a reliable calibration. Examples include isotonic
    regression, which fits a piece-wise constant function, and spline calibration,
    which uses spline (piecewise-defined polynomial) functions to fit the predicted
    probabilities.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éå‚æ•°åŒ–æ–¹æ³•**ï¼šè¿™äº›æ–¹æ³•ä¸å‡è®¾æ ¡å‡†å‡½æ•°å…·æœ‰ç‰¹å®šçš„å‡½æ•°å½¢å¼ã€‚å®ƒä»¬æ›´çµæ´»ï¼Œå¯ä»¥é€‚åº”é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ¦‚ç‡ä¹‹é—´æ›´å¤æ‚çš„å…³ç³»ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦æ›´å¤šçš„æ•°æ®æ¥äº§ç”Ÿå¯é çš„æ ¡å‡†ã€‚ä¾‹å¦‚ï¼Œç­‰è·å›å½’æ³•æ‹Ÿåˆåˆ†æ®µå¸¸æ•°å‡½æ•°ï¼Œæ ·æ¡æ ¡å‡†æ³•ä½¿ç”¨æ ·æ¡ï¼ˆåˆ†æ®µå®šä¹‰çš„å¤šé¡¹å¼ï¼‰å‡½æ•°æ¥æ‹Ÿåˆé¢„æµ‹æ¦‚ç‡ã€‚'
- en: First, we will explore a theoretical, formula-based method for calibrating scores
    for models trained on sampled data, specifically in the context of imbalanced
    data. Next, weâ€™ll examine popular methods such as Plattâ€™s scaling and isotonic
    regression, which are commonly used with classical machine learning models. Finally,
    weâ€™ll introduce supporting techniques such as temperature scaling and label smoothing,
    which are more prevalent among deep learning models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€ç§åŸºäºç†è®ºã€å…¬å¼çš„æ–¹æ³•ï¼Œç”¨äºæ ¡å‡†åœ¨é‡‡æ ·æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åˆ†æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ä¸å¹³è¡¡çš„èƒŒæ™¯ä¸‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥æµè¡Œçš„æ–¹æ³•ï¼Œå¦‚Plattçš„ç¼©æ”¾å’Œç­‰è°ƒå›å½’ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¸ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä»‹ç»æ”¯æŒæŠ€æœ¯ï¼Œå¦‚æ¸©åº¦ç¼©æ”¾å’Œæ ‡ç­¾å¹³æ»‘ï¼Œè¿™äº›æŠ€æœ¯åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æ›´ä¸ºå¸¸è§ã€‚
- en: The calibration of model scores to account for sampling
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æ¨¡å‹åˆ†æ•°è¿›è¡Œæ ¡å‡†ä»¥è€ƒè™‘é‡‡æ ·
- en: If we used oversampling or undersampling to balance a dataset, we can derive
    a theoretical calibration formula. As we saw in [*Chapter 2*](B17259_02.xhtml#_idTextAnchor042),
    *Oversampling Methods*, [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods* (both based on sampling), and [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, we can apply some (over/under) sampling techniques
    or data augmentation techniques to bump up the relative number of samples of the
    minority classes(s) in order to account for a data imbalance. As a result, we
    change the distribution of training data. Although downsampling enhances the modelâ€™s
    ability to distinguish between classes, it also leads to an overestimation of
    the predicted probabilities. Because of this, the model scores during inference
    (real-world prediction) time are still in the downsampled space, and we should
    bring the scores back to the real distribution.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä½¿ç”¨è¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·æ¥å¹³è¡¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºä¸€ä¸ªç†è®ºæ ¡å‡†å…¬å¼ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬2ç« *](B17259_02.xhtml#_idTextAnchor042)ã€â€œè¿‡é‡‡æ ·æ–¹æ³•â€ï¼Œ[*ç¬¬3ç« *](B17259_03.xhtml#_idTextAnchor079)ã€â€œæ¬ é‡‡æ ·æ–¹æ³•â€ï¼ˆä¸¤è€…å‡åŸºäºé‡‡æ ·ï¼‰ï¼Œä»¥åŠ[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)ï¼Œâ€œæ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•â€ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸€äº›ï¼ˆè¿‡/æ¬ ï¼‰é‡‡æ ·æŠ€æœ¯æˆ–æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜å°‘æ•°ç±»ï¼ˆsï¼‰æ ·æœ¬çš„ç›¸å¯¹æ•°é‡ï¼Œä»¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ”¹å˜äº†è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒã€‚å°½ç®¡ä¸‹é‡‡æ ·å¢å¼ºäº†æ¨¡å‹åŒºåˆ†ç±»åˆ«çš„èƒ½åŠ›ï¼Œä½†å®ƒä¹Ÿå¯¼è‡´äº†é¢„æµ‹æ¦‚ç‡çš„é«˜ä¼°ã€‚å› æ­¤ï¼Œåœ¨æ¨ç†ï¼ˆç°å®ä¸–ç•Œé¢„æµ‹ï¼‰æ—¶é—´å†…çš„æ¨¡å‹åˆ†æ•°ä»ç„¶å¤„äºä¸‹é‡‡æ ·ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥å°†è¿™äº›åˆ†æ•°å¸¦å›çœŸå®åˆ†å¸ƒã€‚
- en: Typically, the goal of downsampling is to balance the dataset in terms of the
    number of instances of positive and negative classes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä¸‹é‡‡æ ·çš„ç›®æ ‡æ˜¯å¹³è¡¡æ•°æ®é›†ä¸­æ­£ç±»å’Œè´Ÿç±»å®ä¾‹çš„æ•°é‡ã€‚
- en: As an example, if there are 100 positive class instances and 200 negative class
    instances after downsampling, the ratio is w = 100/200 = 0.5.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœç»è¿‡ä¸‹é‡‡æ ·åï¼Œæœ‰100ä¸ªæ­£ç±»å®ä¾‹å’Œ200ä¸ªè´Ÿç±»å®ä¾‹ï¼Œé‚£ä¹ˆæ¯”ä¾‹w = 100/200 = 0.5ã€‚
- en: 'Assuming the number of negative class examples is more than the number of positive
    class examples, we define w as the ratio:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾è´Ÿç±»æ ·æœ¬çš„æ•°é‡å¤šäºæ­£ç±»æ ·æœ¬çš„æ•°é‡ï¼Œæˆ‘ä»¬å®šä¹‰wä¸ºè¯¥æ¯”ä¾‹ï¼š
- en: w = Â Number of positive class instances in downsampled datasetÂ Â Â Â ________________________________________Â Â Â Â Number
    of negative class instances in downsampled dataset
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: w = Â ä¸‹é‡‡æ ·æ•°æ®é›†ä¸­æ­£ç±»å®ä¾‹çš„æ•°é‡Â Â Â Â ________________________________________Â Â Â Â ä¸‹é‡‡æ ·æ•°æ®é›†ä¸­è´Ÿç±»å®ä¾‹çš„æ•°é‡
- en: Now, letâ€™s assume p is the probability of selecting a positive class example
    from the original dataset (without any kind of downsampling used).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå‡è®¾pæ˜¯åŸå§‹æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»æ ·æœ¬çš„æ¦‚ç‡ï¼ˆæ²¡æœ‰ä½¿ç”¨ä»»ä½•ä¸‹é‡‡æ ·ï¼‰ã€‚
- en: 'If p is the probability of selecting a positive class in the original dataset,
    then the probability pÂ d of selecting a positive class from the downsampled dataset
    can be computed using the following formula [8]:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœpæ˜¯åŸå§‹æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆä»ä¸‹é‡‡æ ·æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»çš„æ¦‚ç‡pÂ då¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼[8]è®¡ç®—ï¼š
- en: pÂ d = Â pÂ _Â p+ w (1 âˆ’ p)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: pÂ d = Â pÂ _Â p+ w (1 âˆ’ p)
- en: Note that if w=1, that is, when no downsampling is done, pÂ d = p. You can refer
    to the paper by Moscatelli et al. [9] for a proof of this relationship between
    the original and downsampled dataset probabilities.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¦‚æœw=1ï¼Œå³æ²¡æœ‰è¿›è¡Œä¸‹é‡‡æ ·æ—¶ï¼ŒpÂ d = pã€‚æ‚¨å¯ä»¥å‚è€ƒMoscatelliç­‰äºº[9]çš„è®ºæ–‡ï¼Œä»¥è¯æ˜åŸå§‹å’Œä¸‹é‡‡æ ·æ•°æ®é›†æ¦‚ç‡ä¹‹é—´çš„å…³ç³»ã€‚
- en: Explaining the denominator
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é‡Šåˆ†æ¯
- en: 'The following are the various terms in the denominator of the previous formula:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä¸ºå‰ä¸€ä¸ªå…¬å¼åˆ†æ¯ä¸­çš„å„ç§æœ¯è¯­ï¼š
- en: p is the probability of selecting a positive class instance in the original
    dataset
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pæ˜¯åŸå§‹æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»å®ä¾‹çš„æ¦‚ç‡
- en: (1 âˆ’ p) is the probability of selecting a negative class instance in the original
    dataset
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1 âˆ’ p)æ˜¯åŸå§‹æ•°æ®é›†ä¸­é€‰æ‹©è´Ÿç±»å®ä¾‹çš„æ¦‚ç‡
- en: w (1 âˆ’ p) is the probability of selecting a negative class instance when downsampling
    is used
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w(1 - p) æ˜¯ä½¿ç”¨ä¸‹é‡‡æ ·æ—¶é€‰æ‹©è´Ÿç±»å®ä¾‹çš„æ¦‚ç‡
- en: The summation p+ w (1 âˆ’ p) is the total sum of probabilities for both the positive
    and downsampled negative classes in the dataset after downsampling the negative
    class by a factor of w
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚å’Œ p + w(1 - p) æ˜¯åœ¨è´Ÿç±»ä»¥wå€æ•°ä¸‹é‡‡æ ·åï¼Œæ•°æ®é›†ä¸­æ­£ç±»å’Œä¸‹é‡‡æ ·è´Ÿç±»çš„æ¦‚ç‡æ€»å’Œ
- en: 'Now that we understand the previous formula, we can flip it to find out the
    probability p of selecting a positive class in the original dataset [10]:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç†è§£äº†ä¹‹å‰çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åè½¬æ¥æ‰¾å‡ºåŸå§‹æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»çš„æ¦‚ç‡ p [10]ï¼š
- en: p = Â pÂ dÂ _Â pÂ d + 1 âˆ’ pÂ dÂ _Â w
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: p = p_d / (p_d + 1 - p_d * w)
- en: 'where:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: w is the ratio of positive class to negative class (called the negative downsampling
    ratio)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w æ˜¯æ­£ç±»ä¸è´Ÿç±»çš„æ¯”ä¾‹ï¼ˆç§°ä¸ºè´Ÿç±»ä¸‹é‡‡æ ·ç‡ï¼‰
- en: pÂ d is the probability of selecting a positive class from the downsampled dataset
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p_d æ˜¯ä»ä¸‹é‡‡æ ·æ•°æ®é›†ä¸­é€‰æ‹©æ­£ç±»çš„æ¦‚ç‡
- en: 'Letâ€™s understand this with some numbers:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨ä¸€äº›æ•°å­—æ¥ç†è§£è¿™ä¸€ç‚¹ï¼š
- en: We have a total of 100,000 examples with 10,000 from the positive class and
    90,000 from the negative class.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»å…±æœ‰100,000ä¸ªç¤ºä¾‹ï¼Œå…¶ä¸­10,000ä¸ªæ¥è‡ªæ­£ç±»ï¼Œ90,000ä¸ªæ¥è‡ªè´Ÿç±»ã€‚
- en: Letâ€™s say we use a downsampling rate w = 0.5, which means that after downsampling,
    we have 10,000 positives and 20,000 negatives. This also implies that during downsampling,
    for every positive class example, we selected only two negative class examples.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬ä½¿ç”¨ä¸‹é‡‡æ ·ç‡ w = 0.5ï¼Œè¿™æ„å‘³ç€ä¸‹é‡‡æ ·åï¼Œæˆ‘ä»¬æœ‰10,000ä¸ªæ­£ç±»å’Œ20,000ä¸ªè´Ÿç±»ã€‚è¿™ä¹Ÿæ„å‘³ç€åœ¨ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ªæ­£ç±»ç¤ºä¾‹ï¼Œæˆ‘ä»¬åªé€‰æ‹©äº†ä¸¤ä¸ªè´Ÿç±»ç¤ºä¾‹ã€‚
- en: Letâ€™s assume our prediction score from a model when trained on the downsampled
    dataset, pÂ d, is 0.9.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬åœ¨å¯¹ä¸‹é‡‡æ ·æ•°æ®é›†è¿›è¡Œè®­ç»ƒæ—¶çš„æ¨¡å‹é¢„æµ‹åˆ†æ•°ä¸º p_dï¼Œä¸º0.9ã€‚
- en: Letâ€™s compute the prediction score in the original dataset. From the previous
    defined formula, p = Â pÂ dÂ _Â pÂ d + 1 âˆ’ pÂ dÂ _Â wÂ  = Â 0.9Â _Â 0.9 + 1 âˆ’ 0.9Â _Â 0.5Â  =
    Â 0.9Â _Â 0.9 + 0.2 = 0.82.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¡ç®—åŸå§‹æ•°æ®é›†ä¸­çš„é¢„æµ‹åˆ†æ•°ã€‚æ ¹æ®ä¹‹å‰å®šä¹‰çš„å…¬å¼ï¼Œp = p_d / (p_d + 1 - p_d * w) = 0.9 / (0.9 + 0.2)
    = 0.9 / 1.1 = 0.82ã€‚
- en: So, a model prediction score of 0.9 in downsampled example changed to 0.82 in
    the original dataset. Notice how the probability got lowered.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸‹é‡‡æ ·ç¤ºä¾‹ä¸­çš„æ¨¡å‹é¢„æµ‹åˆ†æ•°0.9åœ¨åŸå§‹æ•°æ®é›†ä¸­å˜ä¸º0.82ã€‚æ³¨æ„æ¦‚ç‡æ˜¯å¦‚ä½•é™ä½çš„ã€‚
- en: 'A more generic and simpler formula [11] is the relationship between odds before
    and after sampling:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ›´é€šç”¨ä¸”ç®€å•çš„å…¬å¼ [11] æ˜¯é‡‡æ ·å‰åæ¦‚ç‡ä¹‹æ¯”çš„å…³ç³»ï¼š
- en: odds _ after = odds _ before * Â proportion _ of _ 1 _ before _ sampling * (1
    âˆ’ proportion _ of _ 1 _ after _ sampling)Â Â Â Â Â _____________________________________________________Â Â Â Â Â (1
    âˆ’ proportion _ of _ 1 _ before _ sampling) * (proportion _ of _ 1 _ after _ sampling)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬ é‡‡æ ·åçš„æ¦‚ç‡ = æ¬ é‡‡æ ·å‰çš„æ¦‚ç‡ * æ¬ é‡‡æ ·å‰1çš„æ¯”ä¾‹ * (1 - æ¬ é‡‡æ ·å1çš„æ¯”ä¾‹) / (1 - æ¬ é‡‡æ ·å‰1çš„æ¯”ä¾‹) * æ¬ é‡‡æ ·å1çš„æ¯”ä¾‹
- en: 'where *odds* are a way to express the probability of an event as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *odds* æ˜¯ä»¥ä»¥ä¸‹æ–¹å¼è¡¨è¾¾äº‹ä»¶æ¦‚ç‡çš„ä¸€ç§æ–¹å¼ï¼š
- en: odds = Â probabilityÂ ___________Â 1 âˆ’ probability
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡ = 1 - æ¦‚ç‡
- en: '*Figure 10**.14* and *Figure 10**.15* show the model calibration plots before
    and after applying the previous calibration formula, respectively, when using
    random undersampling on the `thyroid_sick` UCI dataset available from the `imblearn.datasets`
    package. You can find the complete notebook on GitHub.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾10.14* å’Œ *å›¾10.15* åˆ†åˆ«æ˜¾ç¤ºäº†åœ¨åº”ç”¨ä¹‹å‰çš„æ ¡å‡†å…¬å¼ä¹‹å‰å’Œä¹‹åï¼Œä½¿ç”¨éšæœºæ¬ é‡‡æ ·å¯¹æ¥è‡ª `imblearn.datasets` åŒ…çš„
    `thyroid_sick` UCI æ•°æ®é›†è¿›è¡Œæ ¡å‡†æ—¶æ¨¡å‹æ ¡å‡†å›¾ã€‚æ‚¨å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°å®Œæ•´çš„ç¬”è®°æœ¬ã€‚'
- en: '![](img/B17259_10_14.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_14.jpg)'
- en: Figure 10.14 â€“ A model calibration plot before calibration when using undersampling
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.14 â€“ ä½¿ç”¨æ¬ é‡‡æ ·è¿›è¡Œæ ¡å‡†å‰çš„æ¨¡å‹æ ¡å‡†å›¾
- en: '![](img/B17259_10_15.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_15.jpg)'
- en: Figure 10.15 â€“ A model calibration plots after calibration when using undersampling
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.15 â€“ ä½¿ç”¨æ¬ é‡‡æ ·è¿›è¡Œæ ¡å‡†åçš„æ¨¡å‹æ ¡å‡†å›¾
- en: ğŸš€ Model calibration in production at Meta
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Metaåœ¨ç”Ÿäº§ä¸­çš„æ¨¡å‹æ ¡å‡†
- en: '**ğŸ¯** **Problem** **being solved:**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¯** **è§£å†³çš„é—®é¢˜ï¼š**'
- en: Meta aimed to accurately predict **Click-Through Rates** (**CTR**) for ads to
    optimize online bidding and auctions in Metaâ€™s advertising system [10]. Accurate
    click prediction is crucial for optimizing online bidding and auctions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Metaæ—¨åœ¨å‡†ç¡®é¢„æµ‹å¹¿å‘Šçš„ç‚¹å‡»é€šè¿‡ç‡ï¼ˆ**CTR**ï¼‰ä»¥ä¼˜åŒ–Metaå¹¿å‘Šç³»ç»Ÿä¸­çš„åœ¨çº¿ç«ä»·å’Œæ‹å– [10]ã€‚å‡†ç¡®çš„ç‚¹å‡»é¢„æµ‹å¯¹äºä¼˜åŒ–åœ¨çº¿ç«ä»·å’Œæ‹å–è‡³å…³é‡è¦ã€‚
- en: '**âš–ï¸** **Data** **imbalance issue:**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**âš–ï¸** **æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼š**'
- en: Meta dealt with massive volumes of data, which inherently contained imbalances.
    A full day of Facebook ad impression data contained a huge number of instances.
    Meta used negative downsampling to speed up training and improve model performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Metaå¤„ç†äº†å¤§é‡çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®æœ¬èº«å­˜åœ¨ä¸å¹³è¡¡ã€‚ä¸€å¤©å†…Facebookå¹¿å‘Šæ›å…‰æ•°æ®åŒ…å«å¤§é‡çš„å®ä¾‹ã€‚Metaä½¿ç”¨è´Ÿä¸‹é‡‡æ ·æ¥åŠ é€Ÿè®­ç»ƒå¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚
- en: '**ğŸ¨** **Model** **calibration strategy:**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¨** **æ¨¡å‹** **æ ¡å‡†ç­–ç•¥ï¼š**'
- en: Since Meta used downsampling, they used the formula from the previous section,
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºMetaä½¿ç”¨äº†ä¸‹é‡‡æ ·ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸Šä¸€èŠ‚ä¸­çš„å…¬å¼ï¼Œ
- en: p = Â pÂ dÂ _Â pÂ d + 1 âˆ’ pÂ dÂ _Â wÂ  , to re-calibrate the model prediction score.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: p = Â pÂ dÂ _Â pÂ d + 1 âˆ’ pÂ dÂ _Â wÂ  ï¼Œä»¥é‡æ–°æ ¡å‡†æ¨¡å‹é¢„æµ‹åˆ†æ•°ã€‚
- en: '**ğŸ“Š** **Additional** **important points:**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“Š** **é¢å¤–** **é‡è¦ç‚¹ï¼š**'
- en: They explored the impact of data freshness and online learning on prediction
    accuracy. The efficiency of an ads auction depended on the accuracy and calibration
    of click prediction. They also used normalized cross-entropy loss and calibration
    as their major evaluation metrics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ç ”ç©¶äº†æ•°æ®æ–°é²œåº¦å’Œåœ¨çº¿å­¦ä¹ å¯¹é¢„æµ‹å‡†ç¡®æ€§çš„å½±å“ã€‚å¹¿å‘Šæ‹å–çš„æ•ˆç‡å–å†³äºç‚¹å‡»é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ ¡å‡†ã€‚ä»–ä»¬è¿˜ä½¿ç”¨äº†å½’ä¸€åŒ–äº¤å‰ç†µæŸå¤±å’Œæ ¡å‡†ä½œä¸ºä»–ä»¬ä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡ã€‚
- en: Plattâ€™s scaling
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Plattç¼©æ”¾
- en: 'With this technique, we try to map the classifierâ€™s probabilities to the perfect
    calibration line. More precisely, we just fit a logistic regression model, with
    the input being the original modelâ€™s probability scores and the labels being the
    actual labels. The `CalibratedClassifierCV` API in `sklearn` already facilitates
    the implementation of this technique:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬è¯•å›¾å°†åˆ†ç±»å™¨çš„æ¦‚ç‡æ˜ å°„åˆ°å®Œç¾çš„æ ¡å‡†çº¿ä¸Šã€‚æ›´ç²¾ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬åªæ˜¯æ‹Ÿåˆä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼Œè¾“å…¥æ˜¯åŸå§‹æ¨¡å‹çš„æ¦‚ç‡åˆ†æ•°ï¼Œæ ‡ç­¾æ˜¯å®é™…æ ‡ç­¾ã€‚`sklearn`ä¸­çš„`CalibratedClassifierCV`
    APIå·²ç»ç®€åŒ–äº†è¿™ç§æŠ€æœ¯çš„å®ç°ï¼š
- en: '[PRE10]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the Brier score output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯Brieråˆ†æ•°çš„è¾“å‡ºç»“æœï¼š
- en: '[PRE11]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Brier score for Plattâ€™s scaled model is smaller than that of the uncalibrated
    model, which was 0.0447, meaning that the Plattâ€™s scaled model is calibrated better.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Plattç¼©æ”¾æ¨¡å‹çš„Brieråˆ†æ•°å°äºæœªæ ¡å‡†æ¨¡å‹çš„Brieråˆ†æ•°ï¼Œåè€…ä¸º0.0447ï¼Œè¿™æ„å‘³ç€Plattç¼©æ”¾æ¨¡å‹çš„æ ¡å‡†æ›´å¥½ã€‚
- en: Isotonic regression
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç­‰è°ƒå›å½’
- en: Isotonic regression is particularly useful when we expect a monotonic relationship
    between the input variables and the output. In this context, a monotonic function
    is one that is either entirely non-decreasing or entirely non-increasing. The
    monotonicity here refers to the relationship between the modelâ€™s raw output and
    the true probabilities, not the arrangement of the data points.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æœŸæœ›è¾“å…¥å˜é‡å’Œè¾“å‡ºä¹‹é—´å­˜åœ¨å•è°ƒå…³ç³»æ—¶ï¼Œç­‰è°ƒå›å½’ç‰¹åˆ«æœ‰ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå•è°ƒå‡½æ•°æ˜¯æŒ‡è¦ä¹ˆå®Œå…¨éé€’å‡ï¼Œè¦ä¹ˆå®Œå…¨éé€’å¢çš„å‡½æ•°ã€‚è¿™é‡Œçš„å•è°ƒæ€§æŒ‡çš„æ˜¯æ¨¡å‹çš„åŸå§‹è¾“å‡ºå’ŒçœŸå®æ¦‚ç‡ä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸æ˜¯æ•°æ®ç‚¹çš„æ’åˆ—ã€‚
- en: If the modelâ€™s output does not follow this expected monotonic behavior, isotonic
    regression can be applied to enforce it. Isotonic regression can be used in cases
    such as credit scoring or medical diagnosis, where a higher score should consistently
    indicate a higher likelihood of a particular outcome.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„è¾“å‡ºä¸éµå¾ªè¿™ç§é¢„æœŸçš„å•è°ƒè¡Œä¸ºï¼Œå¯ä»¥ä½¿ç”¨ç­‰è°ƒå›å½’æ¥å¼ºåˆ¶æ‰§è¡Œã€‚ç­‰è°ƒå›å½’å¯ç”¨äºä¿¡ç”¨è¯„åˆ†æˆ–åŒ»ç–—è¯Šæ–­ç­‰åœºæ™¯ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ›´é«˜çš„åˆ†æ•°åº”å§‹ç»ˆè¡¨ç¤ºæ›´é«˜çš„ç‰¹å®šç»“æœçš„å¯èƒ½æ€§ã€‚
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºç»“æœï¼š
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This Brier score value is a further improvement over Plattâ€™s scaling method.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªBrieråˆ†æ•°å€¼æ˜¯åœ¨Plattç¼©æ”¾æ–¹æ³•ä¹‹ä¸Šçš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚
- en: 'Letâ€™s plot the calibration curves for both the techniques:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶è¿™ä¸¤ç§æŠ€æœ¯çš„æ ¡å‡†æ›²çº¿ï¼š
- en: '[PRE14]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/B17259_10_16.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_16.jpg)'
- en: Figure 10.16 â€“ Calibration curves for a random forest model
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.16 â€“ éšæœºæ£®æ—æ¨¡å‹çš„æ ¡å‡†æ›²çº¿
- en: As *Figure 10**.16* shows, the isotonic regression is the closest to the perfectly
    calibrated curve; hence, it performed the best for our model and data. Plattâ€™s
    scaling did pretty well at calibrating the model, too.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚*å›¾10.16*æ‰€ç¤ºï¼Œç­‰è°ƒå›å½’æœ€æ¥è¿‘å®Œç¾çš„æ ¡å‡†æ›²çº¿ï¼›å› æ­¤ï¼Œå®ƒåœ¨æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®ä¸Šè¡¨ç°æœ€ä½³ã€‚Plattç¼©æ”¾åœ¨æ ¡å‡†æ¨¡å‹æ–¹é¢ä¹Ÿåšå¾—ç›¸å½“ä¸é”™ã€‚
- en: Choosing between Plattâ€™s scaling and Isotonic regression
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Plattç¼©æ”¾å’Œç­‰è°ƒå›å½’ä¹‹é—´è¿›è¡Œé€‰æ‹©
- en: Plattâ€™s scaling is considered more apt for problems where the model predictions
    follow the sigmoid curve. This makes sense because logistic regression (which
    is used by Plattâ€™s scaling) uses a sigmoid to fit the data points. Isotonic regression
    has a much broader coverage of distortions that it can cover for the predicted
    probabilities. However, some research studies [2] show that isotonic regression
    is more prone to overfitting the predicted probabilities. Hence, its performance
    can be worse than Plattâ€™s scaling when we only have a limited dataset since it
    doesnâ€™t generalize well with the limited dataset.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Plattç¼©æ”¾è¢«è®¤ä¸ºæ›´é€‚åˆæ¨¡å‹é¢„æµ‹éµå¾ªsigmoidæ›²çº¿çš„é—®é¢˜ã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºé€»è¾‘å›å½’ï¼ˆPlattç¼©æ”¾ä½¿ç”¨çš„ï¼‰ä½¿ç”¨sigmoidæ¥æ‹Ÿåˆæ•°æ®ç‚¹ã€‚ç­‰è·å›å½’å¯ä»¥è¦†ç›–æ›´å¹¿æ³›çš„é¢„æµ‹æ¦‚ç‡çš„æ‰­æ›²ã€‚ç„¶è€Œï¼Œä¸€äº›ç ”ç©¶[2]è¡¨æ˜ï¼Œç­‰è·å›å½’æ›´å®¹æ˜“è¿‡åº¦æ‹Ÿåˆé¢„æµ‹æ¦‚ç‡ã€‚å› æ­¤ï¼Œå½“åªæœ‰æœ‰é™çš„æ•°æ®é›†æ—¶ï¼Œå®ƒçš„æ€§èƒ½å¯èƒ½æ¯”Plattç¼©æ”¾æ›´å·®ï¼Œå› ä¸ºå®ƒä¸æœ‰é™æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ä¸ä½³ã€‚
- en: A general rule to follow
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: éµå¾ªçš„ä¸€èˆ¬è§„åˆ™
- en: When the dataset at hand is very small or limited, choose Plattâ€™s scaling. However,
    when data is sufficient enough not to have an overfitted model, isotonic regression
    usually does better than Plattâ€™s scaling.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‰‹å¤´çš„æ•°æ®é›†éå¸¸å°æˆ–æœ‰é™æ—¶ï¼Œé€‰æ‹©Plattç¼©æ”¾ã€‚ç„¶è€Œï¼Œå½“æ•°æ®è¶³å¤Ÿå¤šï¼Œä¸ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆæ¨¡å‹æ—¶ï¼Œç­‰è·å›å½’é€šå¸¸æ¯”Plattç¼©æ”¾è¡¨ç°æ›´å¥½ã€‚
- en: For the calibration of a multi-class classifier, we can use the **one-vs-rest**
    approach with individual calibration plots per class. We can apply techniques
    such as Plattâ€™s scaling or Isotonic regression for enhanced predictability, just
    like binary classification.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤šç±»åˆ†ç±»å™¨çš„æ ¡å‡†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**ä¸€å¯¹ä½™**æ–¹æ³•ï¼Œå¹¶ä¸ºæ¯ä¸ªç±»åˆ«ä½¿ç”¨å•ç‹¬çš„æ ¡å‡†å›¾ã€‚æˆ‘ä»¬å¯ä»¥åº”ç”¨åƒPlattç¼©æ”¾æˆ–ç­‰è·å›å½’è¿™æ ·çš„æŠ€æœ¯æ¥æé«˜å¯é¢„æµ‹æ€§ï¼Œå°±åƒäºŒå…ƒåˆ†ç±»ä¸€æ ·ã€‚
- en: Temperature scaling
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¸©åº¦ç¼©æ”¾
- en: Temperature scaling is a post-processing technique used to improve the calibration
    of neural networks. It works by scaling the logits (the output of the final layer
    of the network before applying the softmax function) using a temperature parameter.
    This has the effect of sharpening or softening the probabilities assigned to each
    class depending on the temperature value. By adjusting the temperature parameter,
    it is possible to achieve better calibration of the modelâ€™s confidence estimates,
    which can be useful in applications such as classification or ranking.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸©åº¦ç¼©æ”¾æ˜¯ä¸€ç§åå¤„ç†æŠ€æœ¯ï¼Œç”¨äºæé«˜ç¥ç»ç½‘ç»œçš„æ ¡å‡†ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ¸©åº¦å‚æ•°ç¼©æ”¾logitsï¼ˆåœ¨åº”ç”¨softmaxå‡½æ•°ä¹‹å‰ç½‘ç»œçš„æœ€ç»ˆå±‚è¾“å‡ºï¼‰æ¥å®ç°ã€‚è¿™ä¼šæ ¹æ®æ¸©åº¦å€¼è°ƒæ•´åˆ†é…ç»™æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œä½¿å…¶å˜å°–é”æˆ–å˜æŸ”å’Œã€‚é€šè¿‡è°ƒæ•´æ¸©åº¦å‚æ•°ï¼Œå¯ä»¥å®ç°å¯¹æ¨¡å‹ç½®ä¿¡åº¦ä¼°è®¡çš„æ›´å¥½æ ¡å‡†ï¼Œè¿™åœ¨åˆ†ç±»æˆ–æ’åç­‰åº”ç”¨ä¸­å¯èƒ½å¾ˆæœ‰ç”¨ã€‚
- en: Temperature scaling can be considered a multi-class extension of Plattâ€™s scaling
    with only one hyper-parameter of temperature *T* > 0 for all classes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸©åº¦ç¼©æ”¾å¯ä»¥è¢«è§†ä¸ºPlattç¼©æ”¾çš„å¤šç±»æ‰©å±•ï¼Œå®ƒåªæœ‰ä¸€ä¸ªæ¸©åº¦è¶…å‚æ•° *T* > 0ï¼Œé€‚ç”¨äºæ‰€æœ‰ç±»åˆ«ã€‚
- en: Label smoothing
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ‡ç­¾å¹³æ»‘
- en: Label smoothing [12] is a regularization technique thatâ€™s known to improve model
    calibration. It modifies the training data that is used to train the model and
    is usually handled as a part of the model training. It is not a post-processing
    technique like temperature scaling and previous techniques.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡ç­¾å¹³æ»‘[12]æ˜¯ä¸€ç§å·²çŸ¥çš„å¯ä»¥æ”¹å–„æ¨¡å‹æ ¡å‡†çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚å®ƒä¿®æ”¹ç”¨äºè®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œé€šå¸¸ä½œä¸ºæ¨¡å‹è®­ç»ƒçš„ä¸€éƒ¨åˆ†æ¥å¤„ç†ã€‚å®ƒä¸åƒæ¸©åº¦ç¼©æ”¾å’Œä¹‹å‰çš„æŠ€æœ¯é‚£æ ·æ˜¯ä¸€ç§åå¤„ç†æŠ€æœ¯ã€‚
- en: When neural networks are trained, they often develop excessive confidence in
    their predictions, which can hinder their ability to generalize and perform well
    on new, unseen data. Therefore, it is beneficial to introduce a form of **regularization**
    that reduces the networkâ€™s level of certainty and improves its performance on
    new data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒä»¬å¾€å¾€å¯¹è‡ªå·±çš„é¢„æµ‹è¿‡äºè‡ªä¿¡ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å®ƒä»¬æ³›åŒ–èƒ½åŠ›å’Œåœ¨æ–°ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚å› æ­¤ï¼Œå¼•å…¥ä¸€ç§**æ­£åˆ™åŒ–**å½¢å¼ä»¥é™ä½ç½‘ç»œçš„ä¸ç¡®å®šæ€§æ°´å¹³å¹¶æé«˜å…¶åœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½æ˜¯æœ‰ç›Šçš„ã€‚
- en: Letâ€™s say we have a binary classification problem where the true labels can
    be either 0 or 1\. Without label smoothing, the training labels would be one-hot
    encoded, meaning the true label would be 1 for positive examples and 0 for negative
    examples.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­çœŸå®æ ‡ç­¾å¯ä»¥æ˜¯0æˆ–1ã€‚åœ¨æ²¡æœ‰æ ‡ç­¾å¹³æ»‘çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ ‡ç­¾å°†è¢«è¿›è¡Œone-hotç¼–ç ï¼Œè¿™æ„å‘³ç€å¯¹äºæ­£ä¾‹ï¼ŒçœŸå®æ ‡ç­¾ä¸º1ï¼Œå¯¹äºè´Ÿä¾‹ï¼ŒçœŸå®æ ‡ç­¾ä¸º0ã€‚
- en: With label smoothing, we add a small amount of **noise** to the true labels.
    For example, we can set a smoothing factor of 0.1.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œæˆ‘ä»¬åœ¨çœŸå®æ ‡ç­¾ä¸Šæ·»åŠ ä¸€å°éƒ¨åˆ†**å™ªå£°**ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªå¹³æ»‘å› å­ä¸º0.1ã€‚
- en: 'Here is an example of the original and smoothed labels for a positive example
    in binary classification:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»ä¸­æ­£ä¾‹åŸå§‹æ ‡ç­¾å’Œå¹³æ»‘æ ‡ç­¾çš„ä¾‹å­ï¼š
- en: '[PRE15]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By adding this noise to the labels, the model is encouraged to be less confident
    in its predictions and to be more robust to small changes in the input data. This
    can lead to improved performance on unseen data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å‘æ ‡ç­¾æ·»åŠ è¿™ç§å™ªå£°ï¼Œæ¨¡å‹è¢«é¼“åŠ±å¯¹å…¶é¢„æµ‹ä¸é‚£ä¹ˆè‡ªä¿¡ï¼Œå¹¶å¯¹è¾“å…¥æ•°æ®çš„å°å˜åŒ–æ›´åŠ é²æ£’ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šæ€§èƒ½æå‡ã€‚
- en: In large datasets, mislabeled data can be a concern. Neural networks should
    be designed to approach the correct answer cautiously to mitigate the impact of
    incorrect labels. Label smoothing helps in this regard by slightly adjusting the
    target labels, making the model less confident about its predictions. This can
    prevent the model from overfitting to noisy or incorrect labels.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹æ•°æ®é›†ä¸­ï¼Œé”™è¯¯æ ‡è®°çš„æ•°æ®å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚ç¥ç»ç½‘ç»œåº”è¯¥è¢«è®¾è®¡æˆè°¨æ…åœ°æ¥è¿‘æ­£ç¡®ç­”æ¡ˆï¼Œä»¥å‡è½»é”™è¯¯æ ‡ç­¾çš„å½±å“ã€‚æ ‡ç­¾å¹³æ»‘åœ¨è¿™æ–¹é¢æœ‰æ‰€å¸®åŠ©ï¼Œé€šè¿‡ç¨å¾®è°ƒæ•´ç›®æ ‡æ ‡ç­¾ï¼Œä½¿æ¨¡å‹å¯¹å…¶é¢„æµ‹ä¸é‚£ä¹ˆè‡ªä¿¡ã€‚è¿™å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå™ªå£°æˆ–é”™è¯¯æ ‡ç­¾ã€‚
- en: According to the paper by MÃ¼ller [13], label smoothing can improve model calibration
    by automatically adjusting the networkâ€™s output probabilities. This eliminates
    the need for manual temperature scaling.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®MÃ¼llerçš„è®ºæ–‡[13]ï¼Œæ ‡ç­¾å¹³æ»‘å¯ä»¥é€šè¿‡è‡ªåŠ¨è°ƒæ•´ç½‘ç»œçš„è¾“å‡ºæ¦‚ç‡æ¥æé«˜æ¨¡å‹æ ¡å‡†ã€‚è¿™æ¶ˆé™¤äº†æ‰‹åŠ¨æ¸©åº¦ç¼©æ”¾çš„éœ€è¦ã€‚
- en: Label smoothing can help improve accuracy in various domains, such as image
    classification, text, and speech recognition problems. Most of the modern machine
    learning frameworks, including TensorFlow, PyTorch, and Transformers (from Hugging
    Face), provide built-in implementations for label smoothing in some form in their
    APIs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡ç­¾å¹³æ»‘å¯ä»¥å¸®åŠ©æé«˜å„ä¸ªé¢†åŸŸçš„å‡†ç¡®æ€§ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬å’Œè¯­éŸ³è¯†åˆ«é—®é¢˜ã€‚å¤§å¤šæ•°ç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬TensorFlowã€PyTorchå’Œæ¥è‡ªHugging
    Faceçš„Transformersï¼Œåœ¨å…¶APIä¸­ä»¥æŸç§å½¢å¼æä¾›äº†æ ‡ç­¾å¹³æ»‘çš„å†…ç½®å®ç°ã€‚
- en: 'In PyTorch, itâ€™s implemented in the cross-entropy loss function:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­ï¼Œå®ƒåœ¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­å®ç°ï¼š
- en: '[PRE16]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we can specify the amount of smoothing (as a floating-point value between
    0 and 1) when computing the loss and where a value of 0.0 (default) means no smoothing.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æŸå¤±æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šå¹³æ»‘é‡ï¼ˆä»‹äº0å’Œ1ä¹‹é—´çš„æµ®ç‚¹å€¼ï¼‰ï¼Œå…¶ä¸­é»˜è®¤å€¼ä¸º0.0ï¼ˆ0.0è¡¨ç¤ºæ— å¹³æ»‘ï¼‰ã€‚
- en: In general, it can be helpful to add label smoothing to the loss functions if
    you are looking to add some regularization to your network.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œå¦‚æœä½ æƒ³åœ¨ç½‘ç»œä¸­æ·»åŠ ä¸€äº›æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆå°†æ ‡ç­¾å¹³æ»‘æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚
- en: Arguments against label smoothing
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡ç­¾å¹³æ»‘çš„åå¯¹æ„è§
- en: There are some arguments against label smoothing. Itâ€™s just another hyperparameter
    to tune, and when there are few better regularization techniques such as weight
    decay and L1 regularization, it may be overkill to make your network more complex
    and implicitly modify the labels of your training data. Another point to consider
    is that since it adds random noise to the labels, itâ€™s possible that the network
    might underfit in certain cases.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ ‡ç­¾å¹³æ»‘æœ‰ä¸€äº›åå¯¹æ„è§ã€‚å®ƒåªæ˜¯å¦ä¸€ä¸ªéœ€è¦è°ƒæ•´çš„è¶…å‚æ•°ï¼Œå½“æœ‰æ›´å¥½çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå¦‚æƒé‡è¡°å‡å’ŒL1æ­£åˆ™åŒ–æ—¶ï¼Œå¯èƒ½è¿‡åº¦å¤æ‚åŒ–ä½ çš„ç½‘ç»œå¹¶éšå¼ä¿®æ”¹è®­ç»ƒæ•°æ®çš„æ ‡ç­¾ã€‚å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„é—®é¢˜æ˜¯ï¼Œç”±äºå®ƒå‘æ ‡ç­¾æ·»åŠ éšæœºå™ªå£°ï¼Œç½‘ç»œå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ¬ æ‹Ÿåˆã€‚
- en: There are some further improved variants of label smoothing, such as **label-aware
    smoothing**, as mentioned in Zhong et al.â€™s *Improving Calibration for Long-Tailed*
    *Recognition* [14].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡ç­¾å¹³æ»‘è¿˜æœ‰ä¸€äº›æ”¹è¿›çš„å˜ä½“ï¼Œä¾‹å¦‚æ–‡ä¸­æåˆ°çš„**æ ‡ç­¾æ„ŸçŸ¥å¹³æ»‘**ï¼Œè§Zhongç­‰äººå…³äº*æ”¹è¿›é•¿å°¾è¯†åˆ«æ ¡å‡†*çš„[14]ã€‚
- en: 'The following table shows a comparison of the four techniques we just discussed
    for model calibration:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è¡¨æ˜¾ç¤ºäº†æˆ‘ä»¬å¯¹æ¨¡å‹æ ¡å‡†æ‰€è®¨è®ºçš„å››ç§æŠ€æœ¯çš„æ¯”è¾ƒï¼š
- en: '| **Theme** | **Temperature scaling** | **Label smoothing** | **Plattâ€™s scaling**
    | **Isotonic Regression** |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸»é¢˜** | **æ¸©åº¦ç¼©æ”¾** | **æ ‡ç­¾å¹³æ»‘** | **Plattçš„ç¼©æ”¾** | **ç­‰è°ƒå›å½’** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Change in label values | No change in label values of training data | Change
    in label values of training data | No change in the label values of the training
    data | No change in the label values of the training data |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| æ ‡ç­¾å€¼çš„å˜åŒ– | è®­ç»ƒæ•°æ®çš„æ ‡ç­¾å€¼æ²¡æœ‰å˜åŒ– | è®­ç»ƒæ•°æ®çš„æ ‡ç­¾å€¼å‘ç”Ÿå˜åŒ– | è®­ç»ƒæ•°æ®çš„æ ‡ç­¾å€¼æ²¡æœ‰å˜åŒ– | è®­ç»ƒæ•°æ®çš„æ ‡ç­¾å€¼æ²¡æœ‰å˜åŒ– |'
- en: '| Timing | After training has been completed, the value of the hyperparameter
    T is computed on a validation dataset | Done during the actual training of the
    model | Applied after training | Applied after training |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| æ—¶é—´ | è®­ç»ƒå®Œæˆåï¼Œåœ¨éªŒè¯æ•°æ®é›†ä¸Šè®¡ç®—è¶…å‚æ•°Tçš„å€¼ | åœ¨æ¨¡å‹å®é™…è®­ç»ƒæœŸé—´å®Œæˆ | è®­ç»ƒååº”ç”¨ | è®­ç»ƒååº”ç”¨ |'
- en: '| Prediction value adjustment | Prediction values from the model are manually
    adjusted | Predicted values are changed by applying label smoothing | Prediction
    values from the model are manually adjusted | Prediction values from the model
    are manually adjusted |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| é¢„æµ‹å€¼è°ƒæ•´ | æ¨¡å‹çš„é¢„æµ‹å€¼è¢«æ‰‹åŠ¨è°ƒæ•´ | é€šè¿‡åº”ç”¨æ ‡ç­¾å¹³æ»‘æ”¹å˜é¢„æµ‹å€¼ | æ¨¡å‹çš„é¢„æµ‹å€¼è¢«æ‰‹åŠ¨è°ƒæ•´ | æ¨¡å‹çš„é¢„æµ‹å€¼è¢«æ‰‹åŠ¨è°ƒæ•´ |'
- en: '| Role | Acts as a regularizer | Acts as a regularizer | Acts as a model calibrator
    or prediction score transformer | Acts as a model calibrator or prediction score
    transformer |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| è§’è‰² | ä½œä¸ºæ­£åˆ™åŒ–å™¨ | ä½œä¸ºæ­£åˆ™åŒ–å™¨ | ä½œä¸ºæ¨¡å‹æ ¡å‡†å™¨æˆ–é¢„æµ‹åˆ†æ•°è½¬æ¢å™¨ | ä½œä¸ºæ¨¡å‹æ ¡å‡†å™¨æˆ–é¢„æµ‹åˆ†æ•°è½¬æ¢å™¨ |'
- en: Table 10.2 â€“ Comparing temperature scaling, label smoothing, Plattâ€™s scaling,
    and isotonic regression
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨10.2 â€“ æ¯”è¾ƒæ¸©åº¦ç¼©æ”¾ã€æ ‡ç­¾å¹³æ»‘ã€Plattç¼©æ”¾å’Œç­‰è°ƒå›å½’
- en: There are other model calibration techniques that we didnâ€™t get a chance to
    explore. For instance, **spline calibration** [15][16] is a non-parametric method
    that employs a spline function, a piece-wise polynomial function that is smooth
    and continuous. This technique is somewhat similar to isotonic regression in its
    non-parametric nature.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–æ¨¡å‹æ ¡å‡†æŠ€æœ¯æˆ‘ä»¬æ²¡æœ‰æœºä¼šæ¢ç´¢ã€‚ä¾‹å¦‚ï¼Œ**æ ·æ¡æ ¡å‡†** [15][16] æ˜¯ä¸€ç§éå‚æ•°æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ ·æ¡å‡½æ•°ï¼Œè¿™æ˜¯ä¸€ç§å¹³æ»‘ä¸”è¿ç»­çš„åˆ†æ®µå¤šé¡¹å¼å‡½æ•°ã€‚è¿™ç§æŠ€æœ¯åœ¨éå‚æ•°æ€§è´¨ä¸Šä¸ç­‰è°ƒå›å½’æœ‰äº›ç›¸ä¼¼ã€‚
- en: On the other hand, **beta calibration** [17] is a parametric method that fits
    a beta distribution to the modelâ€™s predictions. This technique is conceptually
    similar to Plattâ€™s scaling, as both are parametric methods. Beta calibration is
    particularly useful for modeling probabilities, such as click-through rates or
    customer conversion rates.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œ**betaæ ¡å‡†** [17] æ˜¯ä¸€ç§å‚æ•°æ–¹æ³•ï¼Œå®ƒå°†betaåˆ†å¸ƒæ‹Ÿåˆåˆ°æ¨¡å‹çš„é¢„æµ‹ä¸­ã€‚è¿™ç§æŠ€æœ¯ä¸Plattç¼©æ”¾åœ¨æ¦‚å¿µä¸Šç›¸ä¼¼ï¼Œå› ä¸ºä¸¤è€…éƒ½æ˜¯å‚æ•°æ–¹æ³•ã€‚Betaæ ¡å‡†ç‰¹åˆ«é€‚ç”¨äºå»ºæ¨¡æ¦‚ç‡ï¼Œä¾‹å¦‚ç‚¹å‡»ç‡æˆ–å®¢æˆ·è½¬åŒ–ç‡ã€‚
- en: '**Focal loss**, discussed in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*, is another method commonly used in
    deep learning models. As demonstrated in the paper by Mukhoti et al. [3], focal
    loss produces well-calibrated models and is often combined with temperature scaling
    for optimal results. Given that neural networks with multiple layers tend to be
    overconfident in their predictions, focal loss serves as a regularizing effect.
    It forces the model to focus on harder examples, thereby reducing overconfidence
    and improving calibration [3].'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç„¦ç‚¹æŸå¤±**ï¼Œåœ¨ç¬¬[*ç¬¬8ç« *](B17259_08.xhtml#_idTextAnchor235)ä¸­è®¨è®ºï¼Œ*ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯*ï¼Œæ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¸¸ç”¨çš„ä¸€ç§æ–¹æ³•ã€‚æ­£å¦‚Mukhotiç­‰äºº
    [3] çš„è®ºæ–‡æ‰€ç¤ºï¼Œç„¦ç‚¹æŸå¤±äº§ç”Ÿæ ¡å‡†è‰¯å¥½çš„æ¨¡å‹ï¼Œå¹¶ä¸”é€šå¸¸ä¸æ¸©åº¦ç¼©æ”¾ç»“åˆä»¥è·å¾—æœ€ä½³ç»“æœã€‚é‰´äºå¤šå±‚ç¥ç»ç½‘ç»œå¾€å¾€å¯¹å…¶é¢„æµ‹è¿‡äºè‡ªä¿¡ï¼Œç„¦ç‚¹æŸå¤±èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ã€‚å®ƒè¿«ä½¿æ¨¡å‹å…³æ³¨æ›´éš¾çš„é—®é¢˜ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘è¿‡åº¦è‡ªä¿¡å¹¶æé«˜æ ¡å‡†
    [3]ã€‚'
- en: ğŸš€ Model calibration with focal loss at Amazon
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ åœ¨äºšé©¬é€Šä½¿ç”¨ç„¦ç‚¹æŸå¤±è¿›è¡Œæ¨¡å‹æ ¡å‡†
- en: '**ğŸ¯** **Problem** **being solved:**'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¯** **é—®é¢˜è§£å†³ï¼š**'
- en: When Amazon deployed conversational bots [18] to handle customer requests, the
    calibration of the underlying ML models proved to be of importance. In one instance,
    Amazonâ€™s chatbot was tasked with automatically classifying return reason codes.
    These return reason codes exhibited class imbalance. When a customer wanted to
    return an item, determining the appropriate reason became pivotal for efficient
    return processing. For instance, if a customer expressed dissatisfaction with
    an itemâ€™s size or color, it was classified under â€œCustomer Preference.â€ In such
    cases, Amazon understood that offering a replacement wasnâ€™t the optimal solution;
    rather, a refund was more appropriate.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: å½“äºšé©¬é€Šéƒ¨ç½²ä¼šè¯æœºå™¨äºº [18] æ¥å¤„ç†å®¢æˆ·è¯·æ±‚æ—¶ï¼Œåº•å±‚æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ ¡å‡†è¯æ˜éå¸¸é‡è¦ã€‚åœ¨ä¸€æ¬¡å®ä¾‹ä¸­ï¼Œäºšé©¬é€Šçš„èŠå¤©æœºå™¨äººè¢«è¦æ±‚è‡ªåŠ¨åˆ†ç±»é€€è´§åŸå› ä»£ç ã€‚è¿™äº›é€€è´§åŸå› ä»£ç è¡¨ç°å‡ºç±»åˆ«ä¸å¹³è¡¡ã€‚å½“å®¢æˆ·æƒ³è¦é€€è´§æ—¶ï¼Œç¡®å®šé€‚å½“çš„ç†ç”±å¯¹äºé«˜æ•ˆçš„é€€è´§å¤„ç†å˜å¾—è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå®¢æˆ·å¯¹å•†å“çš„å¤§å°æˆ–é¢œè‰²è¡¨ç¤ºä¸æ»¡ï¼Œå®ƒä¼šè¢«å½’ç±»ä¸ºâ€œå®¢æˆ·åå¥½â€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œäºšé©¬é€Šç†è§£æä¾›æ›¿ä»£å“ä¸æ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆï¼›ç›¸åï¼Œé€€æ¬¾æ›´ä¸ºåˆé€‚ã€‚
- en: '**ğŸ¨** **Model** **calibration strategy:**'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¨** **æ¨¡å‹æ ¡å‡†ç­–ç•¥ï¼š**'
- en: Through rigorous testing, they uncovered the robustness of focal loss in addressing
    model miscalibration in such real-world tasks. Focal loss was used as a calibration
    method. Moreover, it wasnâ€™t merely about adopting focal loss; the value of *Î³*
    within the loss function played a crucial role in enhancing model calibration.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸¥æ ¼çš„æµ‹è¯•ï¼Œä»–ä»¬æ­ç¤ºäº†åœ¨å¤„ç†æ­¤ç±»ç°å®ä»»åŠ¡ä¸­çš„æ¨¡å‹è¯¯æ ¡å‡†é—®é¢˜æ—¶ï¼Œç„¦ç‚¹æŸå¤±çš„é²æ£’æ€§ã€‚ç„¦ç‚¹æŸå¤±è¢«ç”¨ä½œæ ¡å‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿™ä¸ä»…ä»…å…³äºé‡‡ç”¨ç„¦ç‚¹æŸå¤±ï¼›æŸå¤±å‡½æ•°ä¸­*Î³*çš„å€¼åœ¨å¢å¼ºæ¨¡å‹æ ¡å‡†æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚
- en: '**ğŸ“Š****Additional points:**'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ“Š** **å…¶ä»–è¦ç‚¹ï¼š**'
- en: Focal loss outperformed traditional cross-entropy loss in achieving better-calibrated
    models. The technique was tested in an internal A/B experiment at Amazon. The
    results showed improvements in automation rate and customer experience, meaning
    the bot could resolve more queries without human intervention and receive more
    positive responses from customers.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Focal lossåœ¨å®ç°æ›´å¥½æ ¡å‡†çš„æ¨¡å‹æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„äº¤å‰ç†µæŸå¤±ã€‚è¿™é¡¹æŠ€æœ¯å·²åœ¨äºšé©¬é€Šå†…éƒ¨è¿›è¡Œçš„A/Bæµ‹è¯•ä¸­å¾—åˆ°éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨åŒ–ç‡å’Œå®¢æˆ·ä½“éªŒå¾—åˆ°äº†æå‡ï¼Œè¿™æ„å‘³ç€æœºå™¨äººå¯ä»¥åœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è§£å†³æ›´å¤šæŸ¥è¯¢ï¼Œå¹¶ä»å®¢æˆ·é‚£é‡Œè·å¾—æ›´å¤šç§¯æçš„åé¦ˆã€‚
- en: Next, letâ€™s see in what ways calibration might impact the performance of a model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ ¡å‡†å¯èƒ½ä¼šä»¥ä½•ç§æ–¹å¼å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: The impact of calibration on a modelâ€™s performance
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¡å‡†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
- en: Accuracy, log-loss, and Brier scores usually improve because of calibration.
    However, since the model calibration still involves approximately fitting a model
    to the calibration curve plotted on the held-out calibration dataset, it may sometimes
    worsen the accuracy or other performance metrics by small amounts. Nevertheless,
    the benefits of having calibrated probabilities in terms of giving us actual interpretable
    probability values that represent likelihood far outweigh the slight performance
    impact.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®ç‡ã€å¯¹æ•°æŸå¤±å’ŒBrieråˆ†æ•°é€šå¸¸ä¼šå› ä¸ºæ ¡å‡†è€Œæé«˜ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹æ ¡å‡†ä»ç„¶æ¶‰åŠå°†æ¨¡å‹æ‹Ÿåˆåˆ°åœ¨ä¿ç•™çš„æ ¡å‡†æ•°æ®é›†ä¸Šç»˜åˆ¶çš„æ ¡å‡†æ›²çº¿ï¼Œå®ƒæœ‰æ—¶å¯èƒ½ä¼šç•¥å¾®é™ä½å‡†ç¡®ç‡æˆ–å…¶ä»–æ€§èƒ½æŒ‡æ ‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‹¥æœ‰æ ¡å‡†æ¦‚ç‡çš„å¥½å¤„â€”â€”å³ç»™æˆ‘ä»¬æä¾›å®é™…å¯è§£é‡Šçš„æ¦‚ç‡å€¼ï¼Œè¿™äº›å€¼ä»£è¡¨å¯èƒ½æ€§â€”â€”è¿œè¿œè¶…è¿‡äº†è½»å¾®çš„æ€§èƒ½å½±å“ã€‚
- en: As discussed in [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction
    to Data Imbalance in Machine Learning*, ROC-AUC is a rank-based metric, meaning
    it evaluates the modelâ€™s ability to distinguish between classes based on the ranking
    of predicted scores rather than their absolute values. ROC-AUC doesnâ€™t make any
    claim about accurate probability estimates. Strictly monotonic calibration functions,
    which continuously increase or decrease without any flat regions, preserve this
    ranking; they adjust the scale of the probabilities without altering their relative
    order. For instance, if one score is higher than another before calibration, it
    remains higher afterward. Because ROC-AUC is concerned with the ranking of predictions
    rather than the actual probability values, it remains unaffected by such monotonic
    calibration functions.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[*ç¬¬ä¸€ç« *](B17259_01.xhtml#_idTextAnchor015)ã€Šæœºå™¨å­¦ä¹ ä¸­æ•°æ®ä¸å¹³è¡¡çš„ä»‹ç»ã€‹ä¸­æ‰€è¿°ï¼ŒROC-AUCæ˜¯ä¸€ä¸ªåŸºäºæ’åçš„æŒ‡æ ‡ï¼Œè¿™æ„å‘³ç€å®ƒè¯„ä¼°æ¨¡å‹åŒºåˆ†ä¸åŒç±»åˆ«çš„èƒ½åŠ›æ˜¯åŸºäºé¢„æµ‹åˆ†æ•°çš„æ’åï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹å€¼ã€‚ROC-AUCå¯¹å‡†ç¡®çš„æ¦‚ç‡ä¼°è®¡ä¸åšä»»ä½•å£°æ˜ã€‚ä¸¥æ ¼å•è°ƒçš„æ ¡å‡†å‡½æ•°ï¼Œå³è¿ç»­å¢åŠ æˆ–å‡å°‘è€Œæ²¡æœ‰ä»»ä½•å¹³å¦åŒºåŸŸï¼Œä¿ç•™è¿™ç§æ’åï¼›å®ƒä»¬è°ƒæ•´æ¦‚ç‡çš„å°ºåº¦ï¼Œè€Œä¸æ”¹å˜å®ƒä»¬çš„ç›¸å¯¹é¡ºåºã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªåˆ†æ•°åœ¨æ ¡å‡†ä¹‹å‰æ¯”å¦ä¸€ä¸ªåˆ†æ•°é«˜ï¼Œé‚£ä¹ˆå®ƒåœ¨ä¹‹åä»ç„¶æ›´é«˜ã€‚å› ä¸ºROC-AUCå…³æ³¨çš„æ˜¯é¢„æµ‹çš„æ’åè€Œä¸æ˜¯å®é™…çš„æ¦‚ç‡å€¼ï¼Œæ‰€ä»¥å®ƒä¸å—è¿™ç§å•è°ƒæ ¡å‡†å‡½æ•°çš„å½±å“ã€‚
- en: However, in rare cases, closely ranked predictions might become tied due to
    calibration, especially if the calibration function is loosely monotonic and has
    flat stretches. This could slightly affect the ROC-AUC.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨ç½•è§çš„æƒ…å†µä¸‹ï¼Œç”±äºæ ¡å‡†ï¼Œç´§å¯†æ’åºçš„é¢„æµ‹å¯èƒ½ä¼šå› ä¸ºæ ¡å‡†è€Œå˜å¾—ç›¸åŒï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ ¡å‡†å‡½æ•°æ˜¯æ¾æ•£å•è°ƒçš„å¹¶ä¸”æœ‰å¹³å¦éƒ¨åˆ†ã€‚è¿™å¯èƒ½ä¼šç•¥å¾®å½±å“ROC-AUCã€‚
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we went through the basic concepts of model calibration, why
    we should care about it, how to measure whether a model is calibrated, how data
    imbalance affects the model calibration, and, finally, how to calibrate an uncalibrated
    model. Some of the calibration techniques we talked about include Plattâ€™s scaling,
    isotonic regression, temperature scaling, and label smoothing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ¨¡å‹æ ¡å‡†çš„åŸºæœ¬æ¦‚å¿µã€ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥å…³æ³¨å®ƒã€å¦‚ä½•è¡¡é‡æ¨¡å‹æ˜¯å¦æ ¡å‡†ã€æ•°æ®ä¸å¹³è¡¡å¦‚ä½•å½±å“æ¨¡å‹æ ¡å‡†ï¼Œä»¥åŠæœ€åå¦‚ä½•æ ¡å‡†æœªæ ¡å‡†çš„æ¨¡å‹ã€‚æˆ‘ä»¬è®¨è®ºçš„ä¸€äº›æ ¡å‡†æŠ€æœ¯åŒ…æ‹¬Plattç¼©æ”¾ã€ç­‰è°ƒå›å½’ã€æ¸©åº¦ç¼©æ”¾å’Œæ ‡ç­¾å¹³æ»‘ã€‚
- en: With this, we come to the end of this book. Thank you for dedicating your time
    to reading the book. We trust that it has broadened your knowledge of handling
    imbalanced datasets and their practical applications in machine learning. As we
    draw this book to a close, weâ€™d like to offer some concluding advice on how to
    effectively utilize the techniques discussed.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬è¿™æœ¬ä¹¦å°±ç»“æŸäº†ã€‚æ„Ÿè°¢æ‚¨æŠ½å‡ºæ—¶é—´é˜…è¯»è¿™æœ¬ä¹¦ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œå®ƒå·²ç»æ‹“å®½äº†æ‚¨å¯¹å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†åŠå…¶åœ¨æœºå™¨å­¦ä¹ ä¸­çš„å®é™…åº”ç”¨çš„çŸ¥è¯†ã€‚éšç€æˆ‘ä»¬è¿™æœ¬ä¹¦çš„ç»“æŸï¼Œæˆ‘ä»¬æƒ³æä¾›ä¸€äº›å…³äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ‰€è®¨è®ºæŠ€æœ¯çš„ç»“è®ºæ€§å»ºè®®ã€‚
- en: Like other machine learning techniques, the methods discussed in this book can
    be highly useful under the right conditions, but they also come with their own
    set of challenges. Recognizing when and where to apply these techniques is essential,
    as overly complex solutions can lead to less-than-optimal performance.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸€æ ·ï¼Œæœ¬ä¹¦ä¸­è®¨è®ºçš„æ–¹æ³•åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹éå¸¸æœ‰ç”¨ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†ä¸€ç»„è‡ªå·±çš„æŒ‘æˆ˜ã€‚è®¤è¯†åˆ°ä½•æ—¶ä½•åœ°åº”ç”¨è¿™äº›æŠ€æœ¯æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºè¿‡äºå¤æ‚çš„è§£å†³æ–¹æ¡ˆå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚
- en: Establishing a sound baseline solution is crucial. Implementing various methods,
    such as those in cost-sensitive learning and algorithm-level deep learning techniques,
    can offer insights into handling imbalanced datasets effectively. Each method
    has its pros and cons.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹ä¸€ä¸ªå¯é çš„åŸºçº¿è§£å†³æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚å®æ–½å„ç§æ–¹æ³•ï¼Œå¦‚æˆæœ¬æ•æ„Ÿå­¦ä¹ ä¸­çš„æ–¹æ³•ä»¥åŠç®—æ³•çº§åˆ«çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä»¥æä¾›æœ‰æ•ˆå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„è§è§£ã€‚æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚
- en: For specialized problems, the book provides targeted solutions. For small datasets,
    the oversampling methods can help manage computational resources. For large datasets,
    the chapter on undersampling methods offers suitable techniques.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç‰¹å®šé—®é¢˜ï¼Œæœ¬ä¹¦æä¾›äº†é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºå°æ•°æ®é›†ï¼Œè¿‡é‡‡æ ·æ–¹æ³•å¯ä»¥å¸®åŠ©ç®¡ç†è®¡ç®—èµ„æºã€‚å¯¹äºå¤§æ•°æ®é›†ï¼Œå…³äºæ¬ é‡‡æ ·æ–¹æ³•çš„ç« èŠ‚æä¾›äº†åˆé€‚çš„æŠ€å·§ã€‚
- en: Occasionally, more modern approaches such as graph machine learning algorithms
    can be applied to the problem at hand. The model calibration and threshold tuning
    techniques are useful for decision-making based on model predictions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œå¯ä»¥åº”ç”¨æ›´ç°ä»£çš„æ–¹æ³•ï¼Œå¦‚å›¾æœºå™¨å­¦ä¹ ç®—æ³•æ¥è§£å†³æ‰‹å¤´çš„é—®é¢˜ã€‚æ¨¡å‹æ ¡å‡†å’Œé˜ˆå€¼è°ƒæ•´æŠ€æœ¯å¯¹äºåŸºäºæ¨¡å‹é¢„æµ‹çš„å†³ç­–éå¸¸æœ‰ç”¨ã€‚
- en: Sometimes, data imbalance may not be a problem at all, and we highly encourage
    you to establish the baseline performance with the imbalanced data without applying
    any of the techniques discussed in this book. A lot of the real-world data also
    tends to be tabular, where tree-based models such as XGBoost can be robust to
    certain kinds of data imbalance.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œæ•°æ®ä¸å¹³è¡¡å¯èƒ½æ ¹æœ¬ä¸æ˜¯é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ åœ¨ä¸åº”ç”¨æœ¬ä¹¦ä¸­è®¨è®ºçš„ä»»ä½•æŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸å¹³è¡¡æ•°æ®å»ºç«‹åŸºçº¿æ€§èƒ½ã€‚è®¸å¤šç°å®ä¸–ç•Œçš„æ•°æ®ä¹Ÿå€¾å‘äºæ˜¯è¡¨æ ¼å½¢å¼ï¼Œå…¶ä¸­åŸºäºæ ‘çš„æ¨¡å‹å¦‚XGBoostå¯ä»¥å¯¹æŸäº›ç±»å‹çš„æ•°æ®ä¸å¹³è¡¡å…·æœ‰é²æ£’æ€§ã€‚
- en: We encourage you to apply this knowledge, experiment with new approaches, and
    continue to expand your expertise as you progress in this field. The landscape
    of machine learning is constantly changing, and your skills will only increase
    in value as you keep up with its evolution. We hope that the knowledge youâ€™ve
    gained will empower you to pick up any research paper that you feel interested
    in and be able to reproduce its results. We appreciate your commitment to reading
    this book, and we wish you success in all your future endeavors.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±ä½ åº”ç”¨è¿™äº›çŸ¥è¯†ï¼Œå°è¯•æ–°çš„æ–¹æ³•ï¼Œå¹¶åœ¨ä½ åœ¨è¿™ä¸ªé¢†åŸŸè¿›æ­¥çš„è¿‡ç¨‹ä¸­ç»§ç»­æ‰©å±•ä½ çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœºå™¨å­¦ä¹ çš„é¢†åŸŸä¸æ–­å˜åŒ–ï¼Œéšç€ä½ è·Ÿä¸Šå…¶å‘å±•ï¼Œä½ çš„æŠ€èƒ½å°†åªä¼šå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬å¸Œæœ›ä½ æ‰€è·å¾—çš„çŸ¥è¯†èƒ½å¤Ÿèµ‹äºˆä½ é€‰æ‹©ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ç ”ç©¶è®ºæ–‡å¹¶èƒ½å¤Ÿé‡ç°å…¶ç»“æœçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ„Ÿè°¢ä½ é˜…è¯»è¿™æœ¬ä¹¦çš„æ‰¿è¯ºï¼Œå¹¶ç¥æ„¿ä½ åœ¨æœªæ¥çš„æ‰€æœ‰äº‹ä¸šä¸­å–å¾—æˆåŠŸã€‚
- en: Questions
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: 'Can a well-calibrated model have low accuracy? What about the reverse: can
    a model with high accuracy be poorly calibrated?'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ ¡å‡†è‰¯å¥½çš„æ¨¡å‹æ˜¯å¦å¯èƒ½å…·æœ‰è¾ƒä½çš„å‡†ç¡®ç‡ï¼Ÿåä¹‹ï¼Œä¸€ä¸ªå…·æœ‰é«˜å‡†ç¡®ç‡çš„æ¨¡å‹æ˜¯å¦å¯èƒ½æ ¡å‡†ä¸è‰¯ï¼Ÿ
- en: Take a limited classification dataset with, say, only 100 data points. Train
    a decision tree model using this dataset and then assess its calibration.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥ä¸€ä¸ªæœ‰é™çš„åˆ†ç±»æ•°æ®é›†ä¸ºä¾‹ï¼Œæ¯”å¦‚åªæœ‰100ä¸ªæ•°æ®ç‚¹ã€‚ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒä¸€ä¸ªå†³ç­–æ ‘æ¨¡å‹ï¼Œç„¶åè¯„ä¼°å…¶æ ¡å‡†æƒ…å†µã€‚
- en: Calibrate the model using Plattâ€™s scaling. Measure the Brier score after calibration.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Plattçš„ç¼©æ”¾æ–¹æ³•æ ¡å‡†æ¨¡å‹ã€‚æ ¡å‡†åæµ‹é‡Brieråˆ†æ•°ã€‚
- en: Calibrate the model using isotonic regression. Measure the Brier score after
    calibration
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­‰è°ƒå›å½’æ ¡å‡†æ¨¡å‹ã€‚æ ¡å‡†åæµ‹é‡Brieråˆ†æ•°
- en: How do the Brier scores differ in (A) and (B)?
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨(A)å’Œ(B)ä¸­ï¼ŒBrieråˆ†æ•°æœ‰ä½•ä¸åŒï¼Ÿ
- en: Measure the AUC, accuracy, precision, recall, and F1 score of the model before
    and after calibration.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ ¡å‡†å‰åæµ‹é‡æ¨¡å‹çš„AUCã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚
- en: Take a balanced dataset, say with 10,000 points. Train a decision tree model
    using it. Then check how calibrated it is.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸€ä¸ªå¹³è¡¡çš„æ•°æ®é›†ï¼Œæ¯”å¦‚æœ‰10,000ä¸ªæ•°æ®ç‚¹ã€‚ä½¿ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªå†³ç­–æ ‘æ¨¡å‹ã€‚ç„¶åæ£€æŸ¥å…¶æ ¡å‡†æƒ…å†µã€‚
- en: Calibrate the model using Plattâ€™s scaling. Measure the Brier score after calibration.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Plattçš„ç¼©æ”¾æ–¹æ³•æ ¡å‡†æ¨¡å‹ã€‚æ ¡å‡†åæµ‹é‡Brieråˆ†æ•°ã€‚
- en: Calibrate the model using isotonic regression. Measure the Brier score after
    calibration.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­‰è°ƒå›å½’æ ¡å‡†æ¨¡å‹ã€‚æ ¡å‡†åæµ‹é‡Brieråˆ†æ•°ã€‚
- en: How do the Brier scores differ in (a) and (b)?
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨(a)å’Œ(b)ä¸­ï¼ŒBrieråˆ†æ•°æœ‰ä½•ä¸åŒï¼Ÿ
- en: Measure the AUC, accuracy, precision, recall, and F1 score of the model before
    and after calibration and compare their values.
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ ¡å‡†å‰åæµ‹é‡æ¨¡å‹çš„AUCã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„å€¼ã€‚
- en: 'Given a classification dataset, compare how calibrated the following models
    are by default without applying any calibration techniques by comparing their
    Brier scores:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªåˆ†ç±»æ•°æ®é›†ï¼Œé€šè¿‡æ¯”è¾ƒå®ƒä»¬çš„Brieråˆ†æ•°æ¥æ¯”è¾ƒä»¥ä¸‹æ¨¡å‹é»˜è®¤æƒ…å†µä¸‹æœªç»æ ¡å‡†æŠ€æœ¯æ ¡å‡†çš„æ ¡å‡†ç¨‹åº¦ï¼š
- en: Logistic regression
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’
- en: Decision tree
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘
- en: XGBoost
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBoost
- en: Random forest
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—
- en: AdaBoost
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: AdaBoost
- en: Neural network
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œ
- en: 'Take an imbalanced dataset and train three models with logistic regression,
    a random forest model, and an XGBoost model, respectively. Measure the calibration
    of these models using the calibration curve and Brier scores. Finally, apply these
    techniques to handle data imbalance and measure the calibration again:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ä¸€ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œåˆ†åˆ«ä½¿ç”¨é€»è¾‘å›å½’ã€éšæœºæ£®æ—æ¨¡å‹å’ŒXGBoostæ¨¡å‹ã€‚ä½¿ç”¨æ ¡å‡†æ›²çº¿å’ŒBrieråˆ†æ•°æ¥è¡¡é‡è¿™äº›æ¨¡å‹çš„æ ¡å‡†æƒ…å†µã€‚æœ€åï¼Œåº”ç”¨è¿™äº›æŠ€æœ¯æ¥å¤„ç†æ•°æ®ä¸å¹³è¡¡å¹¶å†æ¬¡æµ‹é‡æ ¡å‡†ï¼š
- en: Undersampling
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸‹é‡‡æ ·
- en: Oversampling
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿‡é‡‡æ ·
- en: 'Cost-sensitive learning: increase the `class_weight` by doubling the previous
    value. Did the model get less calibrated because of doubling the `class_weight`?'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ ï¼šå°†`class_weight`å¢åŠ ä¸€å€ã€‚ç”±äºå°†`class_weight`åŠ å€ï¼Œæ¨¡å‹æ˜¯å¦å˜å¾—æ ¡å‡†åº¦æ›´ä½ï¼Ÿ
- en: References
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, â€œ*On Calibration of Modern
    Neural Networks*.â€ arXiv, Aug. 03, 2017\. Accessed: Nov. 21, 2022, [http://arxiv.org/abs/1706.04599](http://arxiv.org/abs/1706.04599)'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C. Guo, G. Pleiss, Y. Sun, å’Œ K. Q. Weinbergerï¼Œ"å…³äºç°ä»£ç¥ç»ç½‘ç»œçš„æ ¡å‡†"ã€‚arXivï¼Œ2017å¹´8æœˆ3æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2022å¹´11æœˆ21æ—¥ï¼Œ[http://arxiv.org/abs/1706.04599](http://arxiv.org/abs/1706.04599)ã€‚
- en: 'A. Niculescu-Mizil and R. Caruana, â€œ*Predicting good probabilities with supervised
    learning*,â€ in Proceedings of the 22nd International Conference on Machine Learning
    - ICML â€˜05, Bonn, Germany, 2005, pp. 625â€“632\. doi: 10.1145/1102351.1102430.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'A. Niculescu-Mizil å’Œ R. Caruanaï¼Œ"ä½¿ç”¨ç›‘ç£å­¦ä¹ é¢„æµ‹è‰¯å¥½çš„æ¦‚ç‡"ï¼Œè½½äºç¬¬22å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®® - ICML â€˜05ï¼Œå¾·å›½æ³¢æ©ï¼Œ2005å¹´ï¼Œç¬¬625-632é¡µã€‚doi:
    10.1145/1102351.1102430ã€‚'
- en: J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. H. S. Torr, and P. K. Dokania,
    â€œ*Calibrating Deep Neural Networks using Focal Loss*â€. Feb 2020, [https://doi.org/10.48550/arXiv.2002.09437](https://doi.org/10.48550/arXiv.2002.09437)
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. H. S. Torr, å’Œ P. K. Dokaniaï¼Œ"ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ ¡å‡†æ·±åº¦ç¥ç»ç½‘ç»œ"ã€‚2020å¹´2æœˆï¼Œ[https://doi.org/10.48550/arXiv.2002.09437](https://doi.org/10.48550/arXiv.2002.09437)ã€‚
- en: 'B. C. Wallace and I. J. Dahabreh, â€œ*Class Probability Estimates are Unreliable
    for Imbalanced Data (and How to Fix Them)*,â€ in 2012 IEEE 12th International Conference
    on Data Mining, Brussels, Belgium, Dec. 2012, pp. 695â€“704\. doi: 10.1109/ICDM.2012.115.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'B. C. Wallace å’Œ I. J. Dahabrehï¼Œ"å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼Œç±»åˆ«æ¦‚ç‡ä¼°è®¡æ˜¯ä¸å¯é çš„ï¼ˆä»¥åŠå¦‚ä½•ä¿®å¤å®ƒä»¬ï¼‰"ï¼Œè½½äº2012å¹´IEEEç¬¬12å±Šæ•°æ®æŒ–æ˜å›½é™…ä¼šè®®ï¼Œæ¯”åˆ©æ—¶å¸ƒé²å¡å°”ï¼Œ2012å¹´12æœˆï¼Œç¬¬695-704é¡µã€‚doi:
    10.1109/ICDM.2012.115ã€‚'
- en: 'M. Pakdaman Naeini, G. Cooper, and M. Hauskrecht, â€œ*Obtaining Well Calibrated
    Probabilities Using Bayesian Binning*,â€ AAAI, vol. 29, no. 1, Feb. 2015, doi:
    10.1609/aaai.v29i1.9602.'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'M. Pakdaman Naeini, G. Cooper, å’Œ M. Hauskrechtï¼Œ"ä½¿ç”¨è´å¶æ–¯åˆ†ç®±è·å¾—è‰¯å¥½æ ¡å‡†çš„æ¦‚ç‡"ï¼ŒAAAĞ˜ï¼Œç¬¬29å·ï¼Œç¬¬1æœŸï¼Œ2015å¹´2æœˆï¼Œdoi:
    10.1609/aaai.v29i1.9602ã€‚'
- en: 'H. Steck, â€œ*Calibrated recommendations*,â€ in Proceedings of the 12th ACM Conference
    on Recommender Systems, Vancouver British Columbia Canada: ACM, Sep. 2018, pp.
    154â€“162\. doi: 10.1145/3240323.3240372.'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Steckï¼Œ"æ ¡å‡†æ¨è"ï¼Œè½½äºç¬¬12å±ŠACMæ¨èç³»ç»Ÿä¼šè®®è®ºæ–‡é›†ï¼ŒåŠ æ‹¿å¤§ä¸åˆ—é¢ å“¥ä¼¦æ¯”äºšçœæ¸©å“¥åï¼šACMï¼Œ2018å¹´9æœˆï¼Œç¬¬154-162é¡µã€‚doi:
    10.1145/3240323.3240372ã€‚'
- en: 'A. Caplin, D. Martin, and P. Marx, â€œ*Calibrating for Class Weights by Modeling
    Machine Learning*.â€ arXiv, Jul. 31, 2022\. Accessed: Dec. 09, 2022\. [Online].
    Available at [http://arxiv.org/abs/2205.04613](http://arxiv.org/abs/2205.04613)'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Caplin, D. Martin, å’Œ P. Marxï¼Œ"é€šè¿‡å»ºæ¨¡æœºå™¨å­¦ä¹ æ ¡å‡†ç±»åˆ«æƒé‡"ã€‚arXivï¼Œ2022å¹´7æœˆ31æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2022å¹´12æœˆ9æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/2205.04613](http://arxiv.org/abs/2205.04613)è·å–ã€‚
- en: 'A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, â€œ*Calibrating Probability
    with Undersampling for Unbalanced Classification*,â€ in 2015 IEEE Symposium Series
    on Computational Intelligence, Cape Town, South Africa, Dec. 2015, pp. 159â€“166\.
    doi: 10.1109/SSCI.2015.33, [https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf](https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'A. D. Pozzolo, O. Caelen, R. A. Johnson, å’Œ G. Bontempiï¼Œ"ä½¿ç”¨ä¸‹é‡‡æ ·æ ¡å‡†ä¸å¹³è¡¡åˆ†ç±»çš„æ¦‚ç‡"ï¼Œè½½äº2015å¹´IEEEè®¡ç®—æ™ºèƒ½ç³»åˆ—ç ”è®¨ä¼šï¼Œå—éå¼€æ™®æ•¦ï¼Œ2015å¹´12æœˆï¼Œç¬¬159-166é¡µã€‚doi:
    10.1109/SSCI.2015.33ï¼Œ[https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf](https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf)'
- en: 'M. Moscatelli, S. Narizzano, F. Parlapiano, and G. Viggiano, *Corporate default
    forecasting with machine learning*. IT: Banca dâ€™Italia, 2019\. Accessed: Oct.
    14, 2023\. [Online]. Available at [https://doi.org/10.32057/0.TD.2019.1256](https://doi.org/10.32057/0.TD.2019.1256)'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Moscatelli, S. Narizzano, F. Parlapiano å’Œ G. Viggianoï¼Œ*ä½¿ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œå…¬å¸è¿çº¦é¢„æµ‹*ã€‚ITï¼šæ„å¤§åˆ©é“¶è¡Œï¼Œ2019å¹´ã€‚è®¿é—®æ—¥æœŸï¼š2023å¹´10æœˆ14æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[https://doi.org/10.32057/0.TD.2019.1256](https://doi.org/10.32057/0.TD.2019.1256)è·å–ã€‚
- en: 'X. He et al., â€œ*Practical Lessons from Predicting Clicks on Ads at Facebook*,â€
    in Proceedings of the Eighth International Workshop on Data Mining for Online
    Advertising, New York NY USA, Aug. 2014, pp. 1â€“9\. doi: 10.1145/2648584.2648589.'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. He ç­‰äººï¼Œ"ä»é¢„æµ‹Facebookå¹¿å‘Šç‚¹å‡»ä¸­æ±²å–çš„å®é™…ç»éªŒ"ï¼Œè½½äºç¬¬å…«å±Šå›½é™…åœ¨çº¿å¹¿å‘Šæ•°æ®æŒ–æ˜ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œçº½çº¦ NY ç¾å›½ï¼Œ2014å¹´8æœˆï¼Œç¬¬1-9é¡µã€‚doi:
    10.1145/2648584.2648589ã€‚'
- en: G. King and L. Zeng, â€œ*Logistic Regression in Rare Events* *Data*,â€ 2001.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. King å’Œ L. Zengï¼Œ"ç¨€äº‹ä»¶æ•°æ®ä¸­çš„é€»è¾‘å›å½’"ï¼Œ2001å¹´ã€‚
- en: 'Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, â€œ*Rethinking the
    Inception Architecture for Computer Vision*.â€ arXiv, Dec. 11, 2015\. Accessed:
    Dec. 17, 2022\. [Online]. Available at [http://arxiv.org/abs/1512.00567](http://arxiv.org/abs/1512.00567)'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens å’Œ Z. Wojnaï¼Œ"é‡æ–°æ€è€ƒè®¡ç®—æœºè§†è§‰ä¸­çš„Inceptionæ¶æ„ã€‚"
    arXivï¼Œ2015å¹´12æœˆ11æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2022å¹´12æœˆ17æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/1512.00567](http://arxiv.org/abs/1512.00567)è·å–ã€‚
- en: 'R. MÃ¼ller, S. Kornblith, and G. Hinton, â€œ*When Does Label Smoothing Help?*â€
    arXiv, Jun. 10, 2020\. Accessed: Dec. 11, 2022\. [Online]. Available at [http://arxiv.org/abs/1906.02629](http://arxiv.org/abs/1906.02629)'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. MÃ¼ller, S. Kornblith å’Œ G. Hintonï¼Œ"ä½•æ—¶æ ‡ç­¾å¹³æ»‘æœ‰åŠ©äºï¼Ÿ" arXivï¼Œ2020å¹´6æœˆ10æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2022å¹´12æœˆ11æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/1906.02629](http://arxiv.org/abs/1906.02629)è·å–ã€‚
- en: Zhong et al., Improving Calibration for Long-Tailed Recognition. CVPR 2021\.
    [https://arxiv.org/abs/2104.00466](https://arxiv.org/abs/2104.00466)
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhong ç­‰äººï¼Œæ”¹è¿›é•¿å°¾è¯†åˆ«çš„æ ¡å‡†ã€‚CVPR 2021ã€‚[https://arxiv.org/abs/2104.00466](https://arxiv.org/abs/2104.00466)
- en: 'B. Lucena, â€œ*Spline-Based Probability Calibration*.â€ arXiv, Sep. 20, 2018\.
    Accessed: Jul. 22, 2023\. [Online]. Available at [http://arxiv.org/abs/1809.07751](http://arxiv.org/abs/1809.07751)'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B. Lucenaï¼Œ"åŸºäºæ ·æ¡çš„æ¦‚ç‡æ ¡å‡†ã€‚" arXivï¼Œ2018å¹´9æœˆ20æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2023å¹´7æœˆ22æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/1809.07751](http://arxiv.org/abs/1809.07751)è·å–ã€‚
- en: 'K. Gupta, A. Rahimi, T. Ajanthan, T. Mensink, C. Sminchisescu, and R. Hartley,
    â€œ*Calibration of Neural Networks using Splines*.â€ arXiv, Dec. 29, 2021\. Accessed:
    Jul. 22, 2023\. [Online]. Available at [http://arxiv.org/abs/2006.12800](http://arxiv.org/abs/2006.12800)'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Gupta, A. Rahimi, T. Ajanthan, T. Mensink, C. Sminchisescu å’Œ R. Hartleyï¼Œ"ä½¿ç”¨æ ·æ¡è¿›è¡Œç¥ç»ç½‘ç»œæ ¡å‡†ã€‚"
    arXivï¼Œ2021å¹´12æœˆ29æ—¥ã€‚è®¿é—®æ—¥æœŸï¼š2023å¹´7æœˆ22æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/2006.12800](http://arxiv.org/abs/2006.12800)è·å–ã€‚
- en: 'M. Kull and P. Flach, â€œ*Beta calibration: a well-founded and easily implemented
    improvement on logistic calibration for binary classiï¬ers*,â€ in Proceedings of
    the twentieth International Conference on Artificial Intelligence and Statistics
    (pp. 623â€“631).'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Kull å’Œ P. Flachï¼Œ"Betaæ ¡å‡†ï¼šå¯¹äºŒå…ƒåˆ†ç±»å™¨çš„é€»è¾‘æ ¡å‡†çš„åˆç†ä¸”æ˜“äºå®ç°çš„æ”¹è¿›"ï¼Œè½½äºç¬¬äºŒåå±Šå›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®è®ºæ–‡é›†ï¼ˆç¬¬623-631é¡µï¼‰ã€‚
- en: 'C. Wang, J. Balazs, G. Szarvas, P. Ernst, L. Poddar, and P. Danchenko, â€œCalibrating
    Imbalanced Classifiers with Focal Loss: An Empirical Study,â€ in Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing: Industry
    Track, Abu Dhabi, UAE: Association for Computational Linguistics, 2022, pp. 145â€“153\.
    doi: 10.18653/v1/2022.emnlp-industry.14.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'C. Wang, J. Balazs, G. Szarvas, P. Ernst, L. Poddar å’Œ P. Danchenkoï¼Œ"ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ ¡å‡†ä¸å¹³è¡¡åˆ†ç±»å™¨ï¼šä¸€é¡¹å®è¯ç ”ç©¶"ï¼Œè½½äº2022å¹´è‡ªç„¶è¯­è¨€å¤„ç†å®è¯æ–¹æ³•ä¼šè®®ï¼šå·¥ä¸šè½¨è¿¹è®ºæ–‡é›†ï¼Œé˜¿å¸ƒæ‰æ¯”ï¼Œé˜¿è”é…‹ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼Œ2022å¹´ï¼Œç¬¬145-153é¡µã€‚doi:
    10.18653/v1/2022.emnlp-industry.14ã€‚'
- en: Appendix
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é™„å½•
- en: Machine Learning Pipeline in Production
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æµæ°´çº¿åœ¨ç”Ÿäº§ä¸­çš„åº”ç”¨
- en: In this appendix, we will look at when and at which step we incorporate the
    data imbalance-handling techniques within a production machine learning pipeline.
    This mainly applies to supervised classification problems.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬é™„å½•ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨åœ¨ç”Ÿäº§çš„æœºå™¨å­¦ä¹ æµæ°´çº¿ä¸­ä½•æ—¶ä»¥åŠåœ¨å“ªä¸€æ­¥å¼•å…¥æ•°æ®ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯ã€‚è¿™ä¸»è¦é€‚ç”¨äºç›‘ç£åˆ†ç±»é—®é¢˜ã€‚
- en: Machine learning training pipeline
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ è®­ç»ƒæµæ°´çº¿
- en: A machine learning pipeline is the end-to-end process of training one or more
    machine learning models and then deploying them to a live environment. It may
    involve stages such as data collection, model training, validation, deployment,
    monitoring, and iterative improvement, with a focus on scalability, efficiency,
    and robustness.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æµæ°´çº¿æ˜¯æŒ‡è®­ç»ƒä¸€ä¸ªæˆ–å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶å°†å…¶éƒ¨ç½²åˆ°å®é™…ç¯å¢ƒä¸­çš„ç«¯åˆ°ç«¯è¿‡ç¨‹ã€‚å®ƒå¯èƒ½åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€éƒ¨ç½²ã€ç›‘æ§å’Œè¿­ä»£æ”¹è¿›ç­‰é˜¶æ®µï¼Œé‡ç‚¹å…³æ³¨å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œé²æ£’æ€§ã€‚
- en: Various steps during the offline training are shown in *Figure A.1*. Please
    note that some of the steps may not be necessary depending on the problem at hand.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»çº¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æ­¥éª¤åœ¨*å›¾A.1*ä¸­å±•ç¤ºã€‚è¯·æ³¨æ„ï¼Œæ ¹æ®å…·ä½“é—®é¢˜ï¼ŒæŸäº›æ­¥éª¤å¯èƒ½ä¸æ˜¯å¿…éœ€çš„ã€‚
- en: '![](img/B17259_App_01.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_App_01.jpg)'
- en: Figure A.1 â€“ High-level steps in a machine learning training pipeline
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾A.1 â€“ æœºå™¨å­¦ä¹ è®­ç»ƒæµç¨‹çš„é«˜çº§æ­¥éª¤
- en: 'The following is the sequence of steps involved in building a model that can
    handle data imbalance:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºèƒ½å¤Ÿå¤„ç†æ•°æ®ä¸å¹³è¡¡çš„æ¨¡å‹æ‰€æ¶‰åŠçš„æ­¥éª¤å¦‚ä¸‹ï¼š
- en: '**Gather data**: The first step involves gathering the necessary data for training
    the machine learning model. This data can be sourced from various places such
    as databases, files, APIs, or through web scraping. Immediately after gathering,
    itâ€™s often beneficial to perform **data validation**. During this phase, the data
    schema and data range can be verified, along with any custom data validation checks.
    Subsequently, the data is partitioned into a training set, a validation set, and
    a test set. Many production systems often do not prioritize producing validation
    sets. The primary function of producing these validation sets is to aid in model-tuning
    activities, such as hyperparameter adjustment, early stopping, model calibration,
    threshold tuning, and so on. Such tuning often takes place during the model development
    phase, outside of the main production pipeline. **Itâ€™s crucial to split the data
    prior to executing any data transformations or imbalance handling techniques**.
    This precaution ensures data leakage is avoided, which could otherwise lead to
    a biased model performance.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ”¶é›†æ•°æ®**ï¼šç¬¬ä¸€æ­¥æ¶‰åŠæ”¶é›†è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ‰€éœ€çš„æ•°æ®ã€‚è¿™äº›æ•°æ®å¯ä»¥æ¥è‡ªæ•°æ®åº“ã€æ–‡ä»¶ã€APIæˆ–é€šè¿‡ç½‘ç»œçˆ¬è™«ã€‚æ”¶é›†æ•°æ®åï¼Œç«‹å³è¿›è¡Œ**æ•°æ®éªŒè¯**é€šå¸¸æ˜¯æœ‰ç›Šçš„ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œå¯ä»¥éªŒè¯æ•°æ®æ¨¡å¼å’Œæ•°æ®èŒƒå›´ï¼Œä»¥åŠä»»ä½•è‡ªå®šä¹‰çš„æ•°æ®éªŒè¯æ£€æŸ¥ã€‚éšåï¼Œæ•°æ®è¢«åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚è®¸å¤šç”Ÿäº§ç³»ç»Ÿé€šå¸¸ä¸ä¼˜å…ˆè€ƒè™‘ç”ŸæˆéªŒè¯é›†ã€‚ç”Ÿæˆè¿™äº›éªŒè¯é›†çš„ä¸»è¦åŠŸèƒ½æ˜¯å¸®åŠ©è¿›è¡Œæ¨¡å‹è°ƒä¼˜æ´»åŠ¨ï¼Œå¦‚è¶…å‚æ•°è°ƒæ•´ã€æ—©æœŸåœæ­¢ã€æ¨¡å‹æ ¡å‡†ã€é˜ˆå€¼è°ƒæ•´ç­‰ã€‚è¿™ç§è°ƒæ•´é€šå¸¸åœ¨æ¨¡å‹å¼€å‘é˜¶æ®µè¿›è¡Œï¼Œè€Œä¸æ˜¯åœ¨ä¸»ç”Ÿäº§æµç¨‹ä¹‹å¤–ã€‚**åœ¨æ‰§è¡Œä»»ä½•æ•°æ®è½¬æ¢æˆ–ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯ä¹‹å‰ï¼Œå¯¹æ•°æ®è¿›è¡Œæ‹†åˆ†è‡³å…³é‡è¦**ã€‚è¿™ç§é¢„é˜²æªæ–½ç¡®ä¿é¿å…äº†æ•°æ®æ³„éœ²ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½åå·®ã€‚'
- en: '**Data transformation**: The next step is to transform the data into a format
    that can be easily fed into the machine learning model. This may involve tasks
    such as data cleaning, feature selection, normalization, and scaling. These transformation
    steps may need to be stored in order to apply them during model prediction time.
    It can be helpful to store these transformations somewhere (for example, a file
    or database) so that they can be retrieved later during inferencing.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è½¬æ¢**ï¼šä¸‹ä¸€æ­¥æ˜¯å°†æ•°æ®è½¬æ¢æˆå¯ä»¥è½»æ¾è¾“å…¥æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ ¼å¼ã€‚è¿™å¯èƒ½æ¶‰åŠæ•°æ®æ¸…æ´—ã€ç‰¹å¾é€‰æ‹©ã€å½’ä¸€åŒ–å’Œç¼©æ”¾ç­‰ä»»åŠ¡ã€‚è¿™äº›è½¬æ¢æ­¥éª¤å¯èƒ½éœ€è¦å­˜å‚¨èµ·æ¥ï¼Œä»¥ä¾¿åœ¨æ¨¡å‹é¢„æµ‹æ—¶åº”ç”¨ã€‚å°†è½¬æ¢å­˜å‚¨åœ¨æŸä¸ªåœ°æ–¹ï¼ˆä¾‹å¦‚ï¼Œæ–‡ä»¶æˆ–æ•°æ®åº“ï¼‰å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œè¿™æ ·å¯ä»¥åœ¨åç»­çš„æ¨ç†è¿‡ç¨‹ä¸­æ£€ç´¢å®ƒä»¬ã€‚'
- en: '**Handle data imbalance (if needed)**: The machine learning model might underperform
    on a minority class(es) due to a bias toward the majority class. Throughout this
    book, weâ€™ve delved into both data-level and algorithm-level techniques. To summarize,
    data-level techniques focus on resampling the dataset to achieve balanced samples
    across each class, whereas algorithm-level techniques modify the learning algorithm
    to address imbalanced data. For a deeper understanding, please refer to the relevant
    chapters in the book.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤„ç†æ•°æ®ä¸å¹³è¡¡ï¼ˆå¦‚æœéœ€è¦ï¼‰**ï¼šç”±äºå¯¹å¤šæ•°ç±»çš„åå·®ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å¯èƒ½åœ¨å°‘æ•°ç±»ï¼ˆæˆ–å¤šä¸ªå°‘æ•°ç±»ï¼‰ä¸Šè¡¨ç°ä¸ä½³ã€‚åœ¨è¿™æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æ•°æ®çº§å’Œç®—æ³•çº§çš„æŠ€æœ¯ã€‚æ€»ç»“æ¥è¯´ï¼Œæ•°æ®çº§æŠ€æœ¯ä¾§é‡äºé‡é‡‡æ ·æ•°æ®é›†ï¼Œä»¥å®ç°æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬å¹³è¡¡ï¼Œè€Œç®—æ³•çº§æŠ€æœ¯åˆ™ä¿®æ”¹å­¦ä¹ ç®—æ³•ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®ã€‚ä¸ºäº†æ›´æ·±å…¥çš„ç†è§£ï¼Œè¯·å‚è€ƒä¹¦ä¸­çš„ç›¸å…³ç« èŠ‚ã€‚'
- en: '**Train model**: After the data has been preprocessed, itâ€™s time to train the
    machine learning model. This step involves the following:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ¨¡å‹**ï¼šæ•°æ®é¢„å¤„ç†å®Œæˆåï¼Œå°±æ˜¯è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ—¶å€™äº†ã€‚è¿™ä¸€æ­¥éª¤åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š'
- en: Selecting an appropriate algorithm
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆé€‚çš„ç®—æ³•
- en: Setting its hyperparameters
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®å…¶è¶…å‚æ•°
- en: Feeding the preprocessed data into the algorithm
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é¢„å¤„ç†æ•°æ®è¾“å…¥åˆ°ç®—æ³•ä¸­
- en: The training process may require several iterations to fine-tune the model until
    it produces satisfactory results. The trained model binary should be versioned
    and stored for future use, including deploying to a production environment for
    online inferencing.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£ä»¥å¾®è°ƒæ¨¡å‹ï¼Œç›´åˆ°äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚è®­ç»ƒå¥½çš„æ¨¡å‹äºŒè¿›åˆ¶æ–‡ä»¶åº”è¯¥è¿›è¡Œç‰ˆæœ¬æ§åˆ¶å¹¶å­˜å‚¨èµ·æ¥ï¼Œä»¥ä¾›å°†æ¥ä½¿ç”¨ï¼ŒåŒ…æ‹¬éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒè¿›è¡Œåœ¨çº¿æ¨ç†ã€‚
- en: If any data imbalance handling technique was applied, it could miscalibrate
    the model. If calibrated prediction scores are expected from the model, itâ€™s crucial
    to recalibrate the prediction scores. For more information on various model calibration
    techniques, please refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœåº”ç”¨äº†ä»»ä½•æ•°æ®ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯ï¼Œå¯èƒ½ä¼šä½¿æ¨¡å‹å¤±å‡†ã€‚å¦‚æœæœŸæœ›æ¨¡å‹æä¾›æ ¡å‡†åçš„é¢„æµ‹åˆ†æ•°ï¼Œé‚£ä¹ˆé‡æ–°æ ¡å‡†é¢„æµ‹åˆ†æ•°è‡³å…³é‡è¦ã€‚æœ‰å…³å„ç§æ¨¡å‹æ ¡å‡†æŠ€æœ¯çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[*ç¬¬10ç« *](B17259_10.xhtml#_idTextAnchor279)ï¼Œ*æ¨¡å‹æ ¡å‡†*ã€‚
- en: '**Evaluate model**: This step involves assessing the performance of the trained
    model on the test set produced in *step 1*. For classification problems, metrics
    such as accuracy, precision, and recall are typically used, while for other types
    of problems, appropriate metrics should be chosen. If the modelâ€™s performance
    doesnâ€™t meet the desired benchmarks, you may need to not only revisit the data
    transformations (as outlined in *step 2*) but also consider adjusting the modelâ€™s
    architecture, hyperparameters, or even the problem formulation. For binary classification
    models specifically, youâ€™ll want to determine an appropriate threshold for classifying
    predictions. For more in-depth information on threshold tuning techniques, please
    refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning.*'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æ¨¡å‹**ï¼šè¿™ä¸€æ­¥éª¤æ¶‰åŠè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨*æ­¥éª¤1*ä¸­äº§ç”Ÿçš„æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡å’Œå¬å›ç‡ç­‰æŒ‡æ ‡ï¼Œè€Œå¯¹äºå…¶ä»–ç±»å‹çš„é—®é¢˜ï¼Œåº”é€‰æ‹©é€‚å½“çš„æŒ‡æ ‡ã€‚å¦‚æœæ¨¡å‹çš„æ€§èƒ½æœªè¾¾åˆ°é¢„æœŸçš„åŸºå‡†ï¼Œæ‚¨å¯èƒ½ä¸ä»…éœ€è¦å›é¡¾æ•°æ®è½¬æ¢ï¼ˆå¦‚*æ­¥éª¤2*ä¸­æ¦‚è¿°çš„ï¼‰ï¼Œè¿˜éœ€è¦è€ƒè™‘è°ƒæ•´æ¨¡å‹çš„æ¶æ„ã€è¶…å‚æ•°ï¼Œç”šè‡³é—®é¢˜è¡¨è¿°ã€‚å¯¹äºäºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼Œæ‚¨éœ€è¦ç¡®å®šä¸€ä¸ªåˆé€‚çš„é˜ˆå€¼æ¥å¯¹é¢„æµ‹è¿›è¡Œåˆ†ç±»ã€‚æœ‰å…³é˜ˆå€¼è°ƒæ•´æŠ€æœ¯çš„æ›´æ·±å…¥ä¿¡æ¯ï¼Œè¯·å‚é˜…[*ç¬¬5ç« *](B17259_05.xhtml#_idTextAnchor151)ï¼Œ*æˆæœ¬æ•æ„Ÿå­¦ä¹ *ã€‚'
- en: After successfully evaluating the model, its fitness for deployment as a service
    is assessed, enabling it to handle live traffic or make batch predictions. We
    will delve deeper into inferencing in the following section.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆåŠŸè¯„ä¼°æ¨¡å‹åï¼Œè¯„ä¼°å…¶ä½œä¸ºæœåŠ¡çš„é€‚ç”¨æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å®æ—¶æµé‡æˆ–è¿›è¡Œæ‰¹é‡é¢„æµ‹ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚æ›´æ·±å…¥åœ°æ¢è®¨æ¨ç†ã€‚
- en: Inferencing (online or batch)
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨ç†ï¼ˆåœ¨çº¿æˆ–æ‰¹é‡ï¼‰
- en: Inferencing is a process of using a trained machine learning model to make predictions
    on new unseen data. **Online inferencing** refers to making predictions in real
    time on live data as it arrives. Latency is of utmost importance during online
    inferencing in order to prevent any lags to the end user.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†æ˜¯ä¸€ä¸ªä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ–°æœªè§æ•°æ®è¿›è¡Œé¢„æµ‹çš„è¿‡ç¨‹ã€‚**åœ¨çº¿æ¨ç†**æŒ‡çš„æ˜¯åœ¨å®æ—¶æ•°æ®åˆ°æ¥æ—¶è¿›è¡Œé¢„æµ‹ã€‚åœ¨çº¿æ¨ç†æœŸé—´ï¼Œå»¶è¿Ÿæ˜¯æœ€é‡è¦çš„ï¼Œä»¥é˜²æ­¢å¯¹æœ€ç»ˆç”¨æˆ·é€ æˆä»»ä½•å»¶è¿Ÿã€‚
- en: There is another type called **batch inferencing**, where predictions are made
    on a large set of already collected data in an offline fashion.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: å¦æœ‰ä¸€ç§ç±»å‹ç§°ä¸º**æ‰¹é‡æ¨ç†**ï¼Œåœ¨è¿™ç§æ¨ç†ä¸­ï¼Œå¯¹å·²æ”¶é›†çš„å¤§é‡æ•°æ®è¿›è¡Œç¦»çº¿é¢„æµ‹ã€‚
- en: '![](img/B17259_App_02.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_App_02.jpg)'
- en: Figure A.2 â€“ Process flow when live data comes to the model for scoring (inferencing)
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾A.2 â€“ å½“å®æ—¶æ•°æ®åˆ°æ¥æ¨¡å‹è¿›è¡Œè¯„åˆ†ï¼ˆæ¨ç†ï¼‰æ—¶çš„æµç¨‹å›¾
- en: 'Inferencing is a process of using a trained machine learning model to make
    predictions on new input (unseen) data in real time. The following are the steps
    involved in the inferencing process:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†æ˜¯ä¸€ä¸ªä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ–°è¾“å…¥ï¼ˆæœªè§ï¼‰æ•°æ®è¿›è¡Œå®æ—¶é¢„æµ‹çš„è¿‡ç¨‹ã€‚æ¨ç†è¿‡ç¨‹ä¸­æ¶‰åŠä»¥ä¸‹æ­¥éª¤ï¼š
- en: '**Input data**: The first step is to receive new input data that needs to be
    classified or predicted. This data could be in the form of text, images, audio,
    or any other data format.'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥æ•°æ®**ï¼šç¬¬ä¸€æ­¥æ˜¯æ¥æ”¶éœ€è¦åˆ†ç±»æˆ–é¢„æµ‹çš„æ–°è¾“å…¥æ•°æ®ã€‚è¿™äº›æ•°æ®å¯ä»¥æ˜¯æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘æˆ–å…¶ä»–ä»»ä½•æ•°æ®æ ¼å¼ã€‚'
- en: '**Transform data**: Before predicting, the input data needs to undergo transformations
    (such as normalization and scaling) to make it compatible with the trained model.
    Itâ€™s crucial to apply the same transformations that were used during training.'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è½¬æ¢æ•°æ®**ï¼šåœ¨é¢„æµ‹ä¹‹å‰ï¼Œè¾“å…¥æ•°æ®éœ€è¦ç»è¿‡è½¬æ¢ï¼ˆå¦‚å½’ä¸€åŒ–å’Œç¼©æ”¾ï¼‰ï¼Œä»¥ä¾¿ä¸è®­ç»ƒå¥½çš„æ¨¡å‹å…¼å®¹ã€‚åº”ç”¨åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç›¸åŒè½¬æ¢è‡³å…³é‡è¦ã€‚'
- en: '**Model prediction**: Once the input data has been transformed, it is fed into
    the trained model to generate a predicted score. The predicted score represents
    the likelihood of the input belonging to a particular class or category.'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹é¢„æµ‹**ï¼šä¸€æ—¦è¾“å…¥æ•°æ®è¢«è½¬æ¢ï¼Œå®ƒå°±ä¼šè¢«è¾“å…¥åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆä¸€ä¸ªé¢„æµ‹åˆ†æ•°ã€‚é¢„æµ‹åˆ†æ•°è¡¨ç¤ºè¾“å…¥å±äºç‰¹å®šç±»åˆ«æˆ–ç±»åˆ«çš„å¯èƒ½æ€§ã€‚'
- en: '**Calibrate score (if needed)**: Model calibration can be essential when model
    predictions are not reliable. Notably, when any data imbalance handling techniques
    are used, the risk of model miscalibration increases. For a comprehensive understanding
    of this topic, please refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*.'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ ¡å‡†åˆ†æ•°ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰**ï¼šå½“æ¨¡å‹é¢„æµ‹ä¸å¯é æ—¶ï¼Œæ¨¡å‹æ ¡å‡†å¯èƒ½è‡³å…³é‡è¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨ä»»ä½•æ•°æ®ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯æ—¶ï¼Œæ¨¡å‹è¯¯æ ¡å‡†çš„é£é™©å¢åŠ ã€‚ä¸ºäº†å…¨é¢äº†è§£è¿™ä¸€ä¸»é¢˜ï¼Œè¯·å‚é˜…[*ç¬¬10ç« *](B17259_10.xhtml#_idTextAnchor279)ï¼Œ*æ¨¡å‹æ ¡å‡†*ã€‚'
- en: '**Final prediction using threshold**: The calibrated score is then used to
    make the final prediction using an appropriate threshold and take any actionâ€”for
    example, notification to the customer, human reviews, and so on.'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨é˜ˆå€¼è¿›è¡Œæœ€ç»ˆé¢„æµ‹**ï¼šç„¶åä½¿ç”¨æ ¡å‡†åçš„åˆ†æ•°ï¼Œé€šè¿‡é€‚å½“çš„é˜ˆå€¼è¿›è¡Œæœ€ç»ˆé¢„æµ‹ï¼Œå¹¶é‡‡å–ä»»ä½•è¡ŒåŠ¨â€”â€”ä¾‹å¦‚ï¼Œé€šçŸ¥å®¢æˆ·ã€äººå·¥å®¡æŸ¥ç­‰ã€‚'
- en: ğŸŒŸ Monitoring data as well as model in production
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒŸ ç›‘æ§ç”Ÿäº§ä¸­çš„æ•°æ®å’Œæ¨¡å‹
- en: Monitoring data and its distribution is crucial as it can change over time,
    potentially impacting the effectiveness of any initially applied imbalance-handling
    techniques. Such shifts can affect model performance and evaluation metrics, necessitating
    a re-evaluation and potential recalibration of strategies. Beyond just data imbalance,
    phenomena such as model drift and data driftâ€”where there is a change in the modelâ€™s
    performance or the nature of incoming dataâ€”pose significant concerns. Implementing
    automated mechanisms to track these variations is essential for ensuring optimal
    model performance and consistent predictions.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘æ§æ•°æ®å’Œå…¶åˆ†å¸ƒè‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½ä¼šéšæ—¶é—´å˜åŒ–ï¼Œå¯èƒ½å½±å“æœ€åˆåº”ç”¨çš„ä»»ä½•ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§å˜åŒ–å¯èƒ½ä¼šå½±å“æ¨¡å‹æ€§èƒ½å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œéœ€è¦é‡æ–°è¯„ä¼°å’Œå¯èƒ½é‡æ–°æ ¡å‡†ç­–ç•¥ã€‚é™¤äº†æ•°æ®ä¸å¹³è¡¡ä¹‹å¤–ï¼Œæ¨¡å‹æ¼‚ç§»å’Œæ•°æ®æ¼‚ç§»ç­‰ç°è±¡â€”â€”å³æ¨¡å‹æ€§èƒ½æˆ–ä¼ å…¥æ•°æ®çš„æ€§è´¨å‘ç”Ÿå˜åŒ–â€”â€”ä¹Ÿå¼•èµ·é‡å¤§å…³æ³¨ã€‚å®æ–½è‡ªåŠ¨æœºåˆ¶ä»¥è·Ÿè¸ªè¿™äº›å˜åŒ–å¯¹äºç¡®ä¿æœ€ä½³æ¨¡å‹æ€§èƒ½å’Œä¸€è‡´çš„é¢„æµ‹è‡³å…³é‡è¦ã€‚
- en: In conclusion, inferencing entails transforming new input data, producing a
    predicted score using a trained machine learning model, calibrating that score,
    and determining a final prediction using a threshold. This procedure is reiterated
    for every incoming data point requiring a prediction.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæ¨ç†æ¶‰åŠå°†æ–°çš„è¾“å…¥æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹åˆ†æ•°ï¼Œæ ¡å‡†è¯¥åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨é˜ˆå€¼ç¡®å®šæœ€ç»ˆé¢„æµ‹ã€‚æ­¤è¿‡ç¨‹å¯¹æ¯ä¸ªéœ€è¦é¢„æµ‹çš„è¾“å…¥æ•°æ®ç‚¹éƒ½ä¼šé‡å¤è¿›è¡Œã€‚
- en: Assessments
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: "Chapter 1 â€“ Introduction to Data Imbalance in \LMachine Learning"
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬1ç«  - æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ä¸å¹³è¡¡ç®€ä»‹
- en: The choice of loss function when training a model can greatly affect the performance
    of the model on imbalanced datasets. Some loss functions may be more sensitive
    to class imbalance than others. For instance, a model trained with a loss function
    such as cross-entropy might be heavily influenced by the majority class and perform
    poorly on the minority class.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹æ—¶é€‰æ‹©æŸå¤±å‡½æ•°å¯ä»¥æå¤§åœ°å½±å“æ¨¡å‹åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ä¸€äº›æŸå¤±å‡½æ•°å¯èƒ½å¯¹ç±»åˆ«ä¸å¹³è¡¡æ›´æ•æ„Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è®­ç»ƒçš„æ¨¡å‹å¯èƒ½ä¼šå—åˆ°å¤šæ•°ç±»çš„å½±å“å¾ˆå¤§ï¼Œåœ¨å°‘æ•°ç±»ä¸Šçš„è¡¨ç°å¯èƒ½ä¸ä½³ã€‚
- en: The PR curve is more informative than the ROC curve when dealing with highly
    skewed datasets because it focuses on the performance of the classifier on the
    positive (minority) class, which is often the class of interest in imbalanced
    datasets. The ROC curve, on the other hand, considers both the TPR and the FPR
    and thus might give an overly optimistic view of the modelâ€™s performance when
    the negative class dominates the dataset.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å¤„ç†é«˜åº¦å€¾æ–œçš„æ•°æ®é›†æ—¶ï¼ŒPRæ›²çº¿æ¯”ROCæ›²çº¿æ›´æœ‰ä¿¡æ¯é‡ï¼Œå› ä¸ºå®ƒå…³æ³¨åˆ†ç±»å™¨åœ¨æ­£ç±»ï¼ˆå°‘æ•°ç±»ï¼‰ä¸Šçš„æ€§èƒ½ï¼Œè¿™åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸­é€šå¸¸æ˜¯æ„Ÿå…´è¶£çš„ç±»åˆ«ã€‚å¦ä¸€æ–¹é¢ï¼ŒROCæ›²çº¿åŒæ—¶è€ƒè™‘äº†TPRå’ŒFPRï¼Œå› æ­¤åœ¨è´Ÿç±»ä¸»å¯¼æ•°æ®é›†æ—¶ï¼Œå¯èƒ½ä¼šå¯¹æ¨¡å‹æ€§èƒ½ç»™å‡ºè¿‡äºä¹è§‚çš„çœ‹æ³•ã€‚
- en: Accuracy can be a misleading metric for model performance on imbalanced datasets
    because it does not take into account the distribution of classes. A model that
    always predicts the majority class will have high accuracy, but it is not useful
    if our goal is to correctly classify instances of the minority class.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡†ç¡®ç‡å¯èƒ½æ˜¯ä¸€ä¸ªè¯¯å¯¼æ€§çš„æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒæ²¡æœ‰è€ƒè™‘åˆ°ç±»åˆ«çš„åˆ†å¸ƒã€‚å§‹ç»ˆé¢„æµ‹å¤šæ•°ç±»çš„æ¨¡å‹å°†å…·æœ‰é«˜å‡†ç¡®ç‡ï¼Œä½†å¦‚æœæˆ‘ä»¬ç›®æ ‡æ˜¯æ­£ç¡®åˆ†ç±»å°‘æ•°ç±»å®ä¾‹ï¼Œåˆ™è¿™å¹¶ä¸æœ‰ç”¨ã€‚
- en: 'In the context of imbalanced datasets, feature engineering poses a unique challenge
    due to the limited number of instances in the minority class. With so few examples,
    it becomes difficult even for human experts to identify features that are truly
    indicative of the minority class. Poorly chosen features can worsen the problem:
    if the features capture noise rather than the underlying pattern, the model is
    likely to overfit. Conversely, if the features are too generic and fail to capture
    the nuances of the minority class, the model may underfit, leading to poor performance
    on new, unseen data.'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¸å¹³è¡¡æ•°æ®é›†çš„èƒŒæ™¯ä¸‹ï¼Œç‰¹å¾å·¥ç¨‹ç”±äºå°‘æ•°ç±»åˆ«çš„å®ä¾‹æ•°é‡æœ‰é™è€Œæå‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç”±äºç¤ºä¾‹å¦‚æ­¤ä¹‹å°‘ï¼Œå³ä½¿æ˜¯äººç±»ä¸“å®¶ä¹Ÿå¾ˆéš¾è¯†åˆ«å‡ºçœŸæ­£æŒ‡ç¤ºå°‘æ•°ç±»åˆ«çš„ç‰¹å¾ã€‚é€‰æ‹©ä¸å½“çš„ç‰¹å¾å¯èƒ½ä¼šåŠ å‰§é—®é¢˜ï¼šå¦‚æœç‰¹å¾æ•æ‰åˆ°çš„æ˜¯å™ªå£°è€Œä¸æ˜¯æ½œåœ¨æ¨¡å¼ï¼Œæ¨¡å‹å¾ˆå¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚ç›¸åï¼Œå¦‚æœç‰¹å¾è¿‡äºé€šç”¨ä¸”æœªèƒ½æ•æ‰åˆ°å°‘æ•°ç±»åˆ«çš„ç»†å¾®å·®åˆ«ï¼Œæ¨¡å‹å¯èƒ½ä¼šæ¬ æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨æ–°æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚
- en: The choice of â€œkâ€ in k-fold cross-validation can impact the modelâ€™s performance
    on imbalanced datasets. With imbalanced datasets, some folds may contain very
    few or even no examples from the minority classes, potentially leading to misleading
    evaluations of the model. A solution to this issue is to use stratified k-fold
    cross-validation, available through the `sklearn.model_selection.StratifiedKFold`
    API, which ensures that each fold maintains a similar distribution of the various
    classes.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨kæŠ˜äº¤å‰éªŒè¯ä¸­â€œkâ€çš„é€‰æ‹©å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸­ï¼ŒæŸäº›æŠ˜å¯èƒ½åŒ…å«éå¸¸å°‘çš„ç”šè‡³æ²¡æœ‰æ¥è‡ªå°‘æ•°ç±»åˆ«çš„ç¤ºä¾‹ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹è¯„ä¼°çš„è¯¯å¯¼ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæ–¹æ³•æ˜¯ä½¿ç”¨åˆ†å±‚kæŠ˜äº¤å‰éªŒè¯ï¼Œé€šè¿‡`sklearn.model_selection.StratifiedKFold`
    APIæä¾›ï¼Œè¿™ç¡®ä¿äº†æ¯ä¸ªæŠ˜ä¿æŒå„ç§ç±»åˆ«çš„ç›¸ä¼¼åˆ†å¸ƒã€‚
- en: Usually, the greater the imbalance in the test set, the more negatively the
    PR curve is affected. In contrast, the ROC curve is not affected by the class
    distribution in the test set.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæµ‹è¯•é›†ä¸­çš„ä¸å¹³è¡¡ç¨‹åº¦è¶Šå¤§ï¼ŒPRæ›²çº¿å—åˆ°çš„è´Ÿé¢å½±å“å°±è¶Šå¤§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒROCæ›²çº¿ä¸å—æµ‹è¯•é›†ä¸­ç±»åˆ«åˆ†å¸ƒçš„å½±å“ã€‚
- en: 'In *Figures 1.13* and *1.14*, we presented three test sets with imbalance ratios
    of 1:9, 1:3, and 1:1\. The ROC-AUC for all these cases is 0.96, as shown in *Figure
    1**.13*. On the other hand, the average precision value is inversely proportional
    to the level of imbalance in the test set, as illustrated in *Figure 1**.14* (that
    is, greater imbalance results in lower average precision):'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨*å›¾1.13*å’Œ*å›¾1.14*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸‰ä¸ªä¸å¹³è¡¡æ¯”ç‡ä¸º1:9ã€1:3å’Œ1:1çš„æµ‹è¯•é›†ã€‚æ‰€æœ‰è¿™äº›æƒ…å†µçš„ROC-AUCéƒ½æ˜¯0.96ï¼Œå¦‚å›¾*å›¾1.13*æ‰€ç¤ºã€‚å¦ä¸€æ–¹é¢ï¼Œå¹³å‡ç²¾åº¦å€¼ä¸æµ‹è¯•é›†ä¸­ä¸å¹³è¡¡çš„ç¨‹åº¦æˆåæ¯”ï¼Œå¦‚å›¾*å›¾1.14*æ‰€ç¤ºï¼ˆå³ï¼Œæ›´å¤§çš„ä¸å¹³è¡¡å¯¼è‡´å¹³å‡ç²¾åº¦æ›´ä½ï¼‰ï¼š
- en: '![](img/B17259_01_13.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_13.jpg)'
- en: Figure B.1 â€“ ROC curves remain the same when the imbalance ratio changes in
    the test set
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾B.1 â€“ å½“æµ‹è¯•é›†ä¸­çš„ä¸å¹³è¡¡æ¯”ç‡å˜åŒ–æ—¶ï¼ŒROCæ›²çº¿ä¿æŒä¸å˜
- en: '![](img/B17259_01_14.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_14.jpg)'
- en: Figure B.2 â€“ The PR curve changes considerably when the imbalance ratio changes
    in the test set
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾B.2 â€“ å½“æµ‹è¯•é›†ä¸­çš„ä¸å¹³è¡¡æ¯”ç‡å˜åŒ–æ—¶ï¼ŒPRæ›²çº¿ä¼šæ˜¾è‘—å˜åŒ–
- en: Having a high AUC-ROC but a low AUC-PR in the context of an imbalanced dataset
    could indicate that the model is performing well in distinguishing between the
    classes overall (as indicated by the high AUC-ROC), but it is not doing a good
    job at identifying the positive (minority) class (as indicated by the low AUC-PR).
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¸å¹³è¡¡æ•°æ®é›†çš„èƒŒæ™¯ä¸‹ï¼Œå…·æœ‰é«˜AUC-ROCä½†ä½AUC-PRå¯èƒ½è¡¨æ˜æ¨¡å‹åœ¨æ€»ä½“ä¸ŠåŒºåˆ†ç±»åˆ«è¡¨ç°è‰¯å¥½ï¼ˆå¦‚é«˜AUC-ROCæ‰€ç¤ºï¼‰ï¼Œä½†åœ¨è¯†åˆ«æ­£ç±»ï¼ˆå°‘æ•°ç±»ï¼‰æ–¹é¢åšå¾—ä¸å¥½ï¼ˆå¦‚ä½AUC-PRæ‰€ç¤ºï¼‰ã€‚
- en: Sampling bias can contribute to the challenge of imbalanced datasets in machine
    learning because it can lead to an overrepresentation of one class and an underrepresentation
    of another. This can skew the modelâ€™s learning and result in poor performance
    in the underrepresented class.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ·æœ¬åå·®å¯èƒ½å¯¼è‡´æœºå™¨å­¦ä¹ ä¸­ä¸å¹³è¡¡æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´æŸä¸€ç±»åˆ«çš„è¿‡åº¦ä»£è¡¨å’Œå¦ä¸€ç±»åˆ«çš„ä¸è¶³ä»£è¡¨ã€‚è¿™å¯èƒ½ä¼šæ‰­æ›²æ¨¡å‹çš„è®­ç»ƒå¹¶å¯¼è‡´åœ¨ä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ä¸Šè¡¨ç°ä¸ä½³ã€‚
- en: Labeling errors can contribute to the challenge of imbalanced datasets in machine
    learning because they can lead to an incorrect representation of the classes in
    the data. If instances of the minority class are mistakenly labeled as the majority
    class, the model might learn incorrect patterns and perform poorly on the minority
    class.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ‡è®°é”™è¯¯å¯èƒ½å¯¼è‡´æœºå™¨å­¦ä¹ ä¸­ä¸å¹³è¡¡æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å¯¼è‡´æ•°æ®é›†ä¸­ç±»åˆ«çš„é”™è¯¯è¡¨ç¤ºã€‚å¦‚æœå°‘æ•°ç±»åˆ«çš„å®ä¾‹è¢«é”™è¯¯åœ°æ ‡è®°ä¸ºå¤šæ•°ç±»ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ åˆ°é”™è¯¯çš„æ¨¡å¼å¹¶åœ¨å°‘æ•°ç±»ä¸Šè¡¨ç°ä¸ä½³ã€‚
- en: There are many real-world scenarios where dealing with imbalanced datasets is
    inherently part of the problem. Some examples include fraud detection (where fraudulent
    transactions are rare compared to legitimate ones), medical diagnosis (where diseases
    are often rare compared to healthy cases), and spam detection (where spam emails
    are typically fewer than non-spam emails). Can you think of any others?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†æœ¬è´¨ä¸Šå°±æ˜¯é—®é¢˜çš„ä¸€éƒ¨åˆ†ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬æ¬ºè¯ˆæ£€æµ‹ï¼ˆæ¬ºè¯ˆäº¤æ˜“ä¸åˆæ³•äº¤æ˜“ç›¸æ¯”å¾ˆå°‘è§ï¼‰ã€åŒ»ç–—è¯Šæ–­ï¼ˆç–¾ç—…ä¸å¥åº·ç—…ä¾‹ç›¸æ¯”é€šå¸¸å¾ˆå°‘è§ï¼‰å’Œåƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆåƒåœ¾é‚®ä»¶é€šå¸¸æ¯”éåƒåœ¾é‚®ä»¶å°‘ï¼‰ã€‚ä½ èƒ½æƒ³åˆ°å…¶ä»–çš„ä¾‹å­å—ï¼Ÿ
- en: 'Here are the answers:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç­”æ¡ˆï¼š
- en: Its value ranges from -1 (worst value) to +1 (best value).
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…¶å€¼èŒƒå›´ä» -1ï¼ˆæœ€å·®å€¼ï¼‰åˆ° +1ï¼ˆæœ€ä½³å€¼ï¼‰ã€‚
- en: 'The dummy model always predicts class 1, so here are our various confusion
    matrix values:'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç©ºç™½æ¨¡å‹æ€»æ˜¯é¢„æµ‹ç±»åˆ« 1ï¼Œå› æ­¤è¿™é‡Œåˆ—å‡ºäº†æˆ‘ä»¬å„ç§æ··æ·†çŸ©é˜µçš„å€¼ï¼š
- en: TP = 90, TN = 0, FP=10, FN = 0
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TP = 90ï¼ŒTN = 0ï¼ŒFP=10ï¼ŒFN = 0
- en: MCC = Â TP Â· TN âˆ’ FP Â· FN Â Â Â __________________________________Â Â Â Â âˆšÂ ______________________________________Â Â Â Â (TP
    + FP) Â· (TP + FN) Â· (TN + FP) Â· (TN + FN)
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MCC = Â TP Â· TN âˆ’ FP Â· FN Â Â Â __________________________________Â Â Â Â âˆšÂ ______________________________________Â Â Â Â (TP
    + FP) Â· (TP + FN) Â· (TN + FP) Â· (TN + FN)
- en: 'The confusion matrix values are as follows:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µçš„å€¼å¦‚ä¸‹ï¼š
- en: TP = 90
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TP = 90
- en: TN = 0
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TN = 0
- en: FP = 10
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP = 10
- en: FN = 0
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FN = 0
- en: 'By plugging these values into the formula, we get the following:'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°†è¿™äº›å€¼ä»£å…¥å…¬å¼ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: MCC = Â (90 Ã— 0) âˆ’ (10 Ã— 0)Â Â _____________________________Â Â Â âˆšÂ _________________________________Â Â Â (90
    + 10) Ã— (90 + 0) Ã— (0 + 10) Ã— (0 + 0)
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MCC = Â (90 Ã— 0) âˆ’ (10 Ã— 0)Â Â _____________________________Â Â Â âˆšÂ _________________________________Â Â Â (90
    + 10) Ã— (90 + 0) Ã— (0 + 10) Ã— (0 + 0)
- en: = Â 0 âˆ’ 0Â ______________Â Â âˆšÂ _______________Â Â 100 Ã— 90 Ã— 10 Ã— 0
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: = Â 0 âˆ’ 0Â ______________Â Â âˆšÂ _______________Â Â 100 Ã— 90 Ã— 10 Ã— 0
- en: Since the denominator becomes zero (because of the term (TN + FN = 0 + 0)),
    the MCC is undefined.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç”±äºåˆ†æ¯å˜ä¸ºé›¶ï¼ˆå› ä¸º (TN + FN = 0 + 0) è¿™ä¸€é¡¹ï¼‰ï¼ŒMCC æ˜¯æœªå®šä¹‰çš„ã€‚
- en: 'We can compute the other metrics as follows:'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¡ç®—å…¶ä»–æŒ‡æ ‡ï¼š
- en: Accuracy = TP+TN/ (TP+TN+FP+FN) = 0.90
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Accuracy = TP+TN/ (TP+TN+FP+FN) = 0.90
- en: Precision = TP/(TP+FP) = 90/(90+10) = 0.90
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Precision = TP/(TP+FP) = 90/(90+10) = 0.90
- en: Recall = TP/(TP+FN) = 90/(90+0) = 1
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Recall = TP/(TP+FN) = 90/(90+0) = 1
- en: F1 score = 2*Precision*Recall/(Precision+Recall) = 2*0.90*1/(0.90+1) = 0.95
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1 åˆ†æ•° = 2*Precision*Recall/(Precision+Recall) = 2*0.90*1/(0.90+1) = 0.95
- en: Letâ€™s compute MCC for the previous values. Itâ€™s undefined (0/0), which would
    mean something is wrong with the model, and we should go back and fix any issues
    with the data or the model.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¡ç®—å‰è¿°å€¼çš„ MCCã€‚å®ƒæ˜¯æœªå®šä¹‰çš„ï¼ˆ0/0ï¼‰ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œæˆ‘ä»¬åº”è¯¥å›å»æ£€æŸ¥æ•°æ®æˆ–æ¨¡å‹ä¸­çš„ä»»ä½•é—®é¢˜ã€‚
- en: In summary, MCC is a metric that generates a high score only if the model does
    well on both positive and negative class examples from the test set. Also, MCC
    can help inform the user about ongoing prediction issues.
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒMCC æ˜¯ä¸€ä¸ªæŒ‡æ ‡ï¼Œåªæœ‰å½“æ¨¡å‹åœ¨æµ‹è¯•é›†çš„æ­£è´Ÿç±»ç¤ºä¾‹ä¸Šéƒ½è¡¨ç°è‰¯å¥½æ—¶ï¼Œæ‰ä¼šäº§ç”Ÿé«˜åˆ†ã€‚æ­¤å¤–ï¼ŒMCC è¿˜å¯ä»¥å¸®åŠ©ç”¨æˆ·äº†è§£æŒç»­çš„é¢„æµ‹é—®é¢˜ã€‚
- en: This is left as an exercise for you.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™ç•™ç»™ä½ ä½œä¸ºç»ƒä¹ ã€‚
- en: Chapter 2 â€“ Oversampling Methods
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç«  â€“ è¿‡é‡‡æ ·æ–¹æ³•
- en: This is left as an exercise for you.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™ç•™ç»™ä½ ä½œä¸ºç»ƒä¹ ã€‚
- en: One approach is to oversample the minority class by 20x to balance both classes.
    Itâ€™s important to note that achieving the perfect balance between the classes
    is not always necessary; a slight imbalance may be acceptable, depending on the
    specific requirements and constraints. This technique is not applied at test time
    as the test data should remain representative of what we would encounter in the
    real world.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å°†å°‘æ•°ç±»è¿‡é‡‡æ · 20 å€æ¥å¹³è¡¡ä¸¤ç±»ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç±»ä¹‹é—´å®ç°å®Œç¾çš„å¹³è¡¡å¹¶ä¸æ€»æ˜¯å¿…è¦çš„ï¼›è½»å¾®çš„ä¸å¹³è¡¡å¯èƒ½æ˜¯å¯æ¥å—çš„ï¼Œå…·ä½“å–å†³äºç‰¹å®šçš„éœ€æ±‚å’Œé™åˆ¶ã€‚è¿™ç§æŠ€æœ¯ä¸åœ¨æµ‹è¯•æ—¶é—´åº”ç”¨ï¼Œå› ä¸ºæµ‹è¯•æ•°æ®åº”è¯¥ä»£è¡¨æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­ä¼šé‡åˆ°çš„æƒ…å†µã€‚
- en: The primary concern with oversampling before splitting the data into training,
    test, and validation sets is data leakage. This occurs when duplicate samples
    end up in both the training and test/validation sets, leading to overly optimistic
    performance metrics. The model may perform well during evaluation because it has
    already seen the same examples during training, but this can result in poor generalization
    to new, unseen data. To mitigate this risk, itâ€™s crucial to first split the data
    into training, test, and validation sets and then apply balancing techniques such
    as oversampling exclusively to the training set.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å°†æ•°æ®åˆ†å‰²æˆè®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯é›†ä¹‹å‰ï¼Œå¯¹æ•°æ®è¿›è¡Œè¿‡é‡‡æ ·çš„ä¸»è¦é—®é¢˜æ˜¯æ•°æ®æ³„éœ²ã€‚è¿™å‘ç”Ÿåœ¨é‡å¤æ ·æœ¬åŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’Œæµ‹è¯•/éªŒè¯é›†ä¸­ï¼Œå¯¼è‡´è¿‡åº¦ä¹è§‚çš„æ€§èƒ½æŒ‡æ ‡ã€‚æ¨¡å‹åœ¨è¯„ä¼°æœŸé—´å¯èƒ½è¡¨ç°è‰¯å¥½ï¼Œå› ä¸ºå®ƒå·²ç»åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°äº†ç›¸åŒçš„ç¤ºä¾‹ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´å¯¹æ–°ã€æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚ä¸ºäº†å‡è½»è¿™ç§é£é™©ï¼Œé¦–å…ˆå°†æ•°æ®åˆ†å‰²æˆè®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯é›†ï¼Œç„¶åä»…å°†å¹³è¡¡æŠ€æœ¯ï¼ˆå¦‚è¿‡é‡‡æ ·ï¼‰åº”ç”¨äºè®­ç»ƒé›†ã€‚
- en: Data normalization can help indirectly in dealing with data imbalance by ensuring
    that all features have the same scale, which can lead to better model performance.
    However, normalization may not directly address the imbalance between the classes
    in the dataset. To tackle data imbalance, other techniques can be employed, such
    as various sampling techniques, cost-sensitive approaches, or threshold adjustment
    techniques.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®å½’ä¸€åŒ–å¯ä»¥é€šè¿‡ç¡®ä¿æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„å°ºåº¦æ¥é—´æ¥å¸®åŠ©å¤„ç†æ•°æ®ä¸å¹³è¡¡ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½’ä¸€åŒ–å¯èƒ½ä¸ä¼šç›´æ¥è§£å†³æ•°æ®é›†ä¸­ç±»ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³æ•°æ®ä¸å¹³è¡¡ï¼Œå¯ä»¥é‡‡ç”¨å…¶ä»–æŠ€æœ¯ï¼Œä¾‹å¦‚å„ç§é‡‡æ ·æŠ€æœ¯ã€æˆæœ¬æ•æ„Ÿæ–¹æ³•æˆ–é˜ˆå€¼è°ƒæ•´æŠ€æœ¯ã€‚
- en: This has been left as an exercise for you.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: Chapter 3 â€“ Undersampling Methods
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç«  â€“ æ¬ é‡‡æ ·æ–¹æ³•
- en: This has been left as an exercise for you.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: This has been left as an exercise for you.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: This has been left as an exercise for you.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: '`TomekLinksNCR` is a custom undersampling method that combines Tomek links
    and NCR. It removes Tomek links and then applies NCR to remove more noisy and
    borderline samples from the majority class. This aims to create a more balanced
    dataset while retaining the underlying structure of the data.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TomekLinksNCR` æ˜¯ä¸€ç§è‡ªå®šä¹‰çš„æ¬ é‡‡æ ·æ–¹æ³•ï¼Œå®ƒç»“åˆäº† Tomek links å’Œ NCRã€‚å®ƒé¦–å…ˆç§»é™¤ Tomek linksï¼Œç„¶ååº”ç”¨
    NCR ä»å¤šæ•°ç±»ä¸­ç§»é™¤æ›´å¤šå™ªå£°å’Œè¾¹ç¼˜æ ·æœ¬ã€‚è¿™æ—¨åœ¨åˆ›å»ºä¸€ä¸ªæ›´å¹³è¡¡çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®çš„åº•å±‚ç»“æ„ã€‚'
- en: Chapter 4 â€“ Ensemble Methods
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬å››ç«  â€“ é›†åˆæ–¹æ³•
- en: This has been left as an exercise for you.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: The main difference between `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
    is the base classifier and the ensemble learning method they employ. `BalancedRandomForestClassifier`
    uses decision trees as base classifiers and follows a random forest as the estimator,
    while `BalancedBaggingClassifier` can use any base classifier that supports sample
    weighting and follows a bagging approach.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`BalancedRandomForestClassifier` å’Œ `BalancedBaggingClassifier` ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬ä½¿ç”¨çš„åŸºåˆ†ç±»å™¨å’Œé›†æˆå­¦ä¹ æ–¹æ³•ã€‚`BalancedRandomForestClassifier`
    ä½¿ç”¨å†³ç­–æ ‘ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œå¹¶éµå¾ªéšæœºæ£®æ—ä½œä¸ºä¼°è®¡å™¨ï¼Œè€Œ `BalancedBaggingClassifier` å¯ä»¥ä½¿ç”¨ä»»ä½•æ”¯æŒæ ·æœ¬åŠ æƒçš„åŸºåˆ†ç±»å™¨ï¼Œå¹¶éµå¾ª bagging
    æ–¹æ³•ã€‚'
- en: Random forest can be considered an extension of bagging that incorporates an
    additional layer of randomness by also randomly selecting a subset of features
    at each split in the decision tree. This helps create more diverse trees and generally
    results in better performance of random forest models.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—å¯ä»¥è¢«è§†ä¸º bagging çš„æ‰©å±•ï¼Œå®ƒé€šè¿‡åœ¨å†³ç­–æ ‘çš„æ¯ä¸ªåˆ†è£‚ç‚¹éšæœºé€‰æ‹©ç‰¹å¾å­é›†æ¥å¼•å…¥é¢å¤–çš„éšæœºæ€§ã€‚è¿™æœ‰åŠ©äºåˆ›å»ºæ›´å¤šæ ·åŒ–çš„æ ‘ï¼Œå¹¶ä¸”é€šå¸¸ä¼šå¯¼è‡´éšæœºæ£®æ—æ¨¡å‹æ€§èƒ½æ›´å¥½ã€‚
- en: Chapter 5 â€“ Cost-Sensitive Learning
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äº”ç«  â€“ æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: This chapterâ€™s questions have been left as exercises for you.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„é—®é¢˜å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: Chapter 6 â€“ Data Imbalance in Deep Learning
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬å…­ç«  â€“ æ·±åº¦å­¦ä¹ ä¸­çš„æ•°æ®ä¸å¹³è¡¡
- en: The main challenge stems from the different types of data these models handle.
    Classical machine learning models typically work with structured, tabular data,
    while deep learning models handle unstructured data such as images, text, audio,
    and video.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸»è¦æŒ‘æˆ˜æºäºè¿™äº›æ¨¡å‹å¤„ç†çš„æ•°æ®ç±»å‹ä¸åŒã€‚ç»å…¸çš„æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸å¤„ç†ç»“æ„åŒ–ã€è¡¨æ ¼æ•°æ®ï¼Œè€Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚
- en: An imbalanced version of the MNIST dataset can be created by randomly selecting
    a certain percentage of examples for each class. This process involves choosing
    indices of the samples to remove and then actually removing these samples from
    the training set.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡éšæœºé€‰æ‹©æ¯ä¸ªç±»çš„ä¸€å®šç™¾åˆ†æ¯”çš„ç¤ºä¾‹æ¥åˆ›å»º MNIST æ•°æ®é›†çš„ä¸å¹³è¡¡ç‰ˆæœ¬ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠé€‰æ‹©è¦ç§»é™¤çš„æ ·æœ¬çš„ç´¢å¼•ï¼Œç„¶åå®é™…ä¸Šä»è®­ç»ƒé›†ä¸­ç§»é™¤è¿™äº›æ ·æœ¬ã€‚
- en: This has been left as an exercise for you.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ ã€‚
- en: Random oversampling is used to address imbalance in the dataset. It works by
    duplicating samples from the minority classes until each class has an equal number
    of samples. This technique is usually considered to perform better than no sampling.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·ç”¨äºè§£å†³æ•°æ®é›†ä¸­çš„ä¸å¹³è¡¡ã€‚å®ƒé€šè¿‡å¤åˆ¶å°‘æ•°ç±»çš„æ ·æœ¬ï¼Œç›´åˆ°æ¯ä¸ªç±»éƒ½æœ‰ç›¸åŒæ•°é‡çš„æ ·æœ¬æ¥å®ç°ã€‚è¿™ç§æŠ€æœ¯é€šå¸¸è¢«è®¤ä¸ºæ¯”ä¸é‡‡æ ·è¡¨ç°å¾—æ›´å¥½ã€‚
- en: Data augmentation techniques can include rotating, scaling, cropping, blurring,
    adding noise to the image, and much more. However, ensuring these augmentations
    preserve the original labels and avoiding inadvertently removing important details
    from the data is crucial. Please refer to [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, for a detailed discussion of the various data
    augmentation techniques.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥åŒ…æ‹¬æ—‹è½¬ã€ç¼©æ”¾ã€è£å‰ªã€æ¨¡ç³Šã€å‘å›¾åƒæ·»åŠ å™ªå£°ç­‰ã€‚ç„¶è€Œï¼Œç¡®ä¿è¿™äº›å¢å¼ºä¿ç•™åŸå§‹æ ‡ç­¾ï¼Œå¹¶é¿å…æ— æ„ä¸­ä»æ•°æ®ä¸­åˆ é™¤é‡è¦ç»†èŠ‚æ˜¯è‡³å…³é‡è¦çš„ã€‚è¯·å‚é˜…[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)ï¼Œ*æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•*ï¼Œä»¥è¯¦ç»†äº†è§£å„ç§æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚
- en: 'Undersampling reduces the instances of the majority class to balance the dataset.
    However, this method has a significant limitation: important information might
    be lost if instances from the majority class are randomly removed, especially
    when the majority class has a lot of variation.'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸‹é‡‡æ ·å‡å°‘äº†å¤šæ•°ç±»çš„å®ä¾‹ä»¥å¹³è¡¡æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸€ä¸ªæ˜¾è‘—çš„å±€é™æ€§ï¼šå¦‚æœéšæœºåˆ é™¤å¤šæ•°ç±»çš„å®ä¾‹ï¼Œå¯èƒ½ä¼šä¸¢å¤±é‡è¦ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å½“å¤šæ•°ç±»æœ‰å¾ˆå¤šå˜åŒ–æ—¶ã€‚
- en: The data augmentation techniques must preserve the original labels because the
    model learns to associate the features of the data with these labels. If the labels
    change due to augmentation, the model might learn incorrect associations, which
    would degrade its performance when making predictions.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®å¢å¼ºæŠ€æœ¯å¿…é¡»ä¿ç•™åŸå§‹æ ‡ç­¾ï¼Œå› ä¸ºæ¨¡å‹å­¦ä¹ å°†æ•°æ®çš„ç‰¹å¾ä¸è¿™äº›æ ‡ç­¾å…³è”èµ·æ¥ã€‚å¦‚æœæ ‡ç­¾ç”±äºå¢å¼ºè€Œæ”¹å˜ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ åˆ°é”™è¯¯çš„å…³è”ï¼Œè¿™ä¼šé™ä½å…¶åœ¨è¿›è¡Œé¢„æµ‹æ—¶çš„æ€§èƒ½ã€‚
- en: Chapter 7 â€“ Data-Level Deep Learning Methods
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬7ç«  â€“ æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•
- en: This chapterâ€™s questions have been left as exercises for you.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„é—®é¢˜å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: Chapter 8 â€“ Algorithm-Level Deep Learning Techniques
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬8ç«  â€“ ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯
- en: This has been left as an exercise for you.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: This has been left as an exercise for you.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: 'Tversky loss is based on the Tversky index, which is defined by the following
    formula:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TverskyæŸå¤±åŸºäºTverskyæŒ‡æ•°ï¼Œå…¶å®šä¹‰å¦‚ä¸‹å…¬å¼ï¼š
- en: TverskyIndex = Â TruePositiveÂ Â Â _______________________________________Â Â Â Â TruePositive+
    Î± * FalsePositive + (1 âˆ’ Î±) * FalseNegative
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TverskyIndex = Â TruePositiveÂ Â Â _______________________________________Â Â Â Â TruePositive+
    Î± * FalsePositive + (1 âˆ’ Î±) * FalseNegative
- en: 'A smoothing factor is added to both the numerator and denominator to avoid
    division by zero. `alpha` is a hyperparameter that can be tuned:'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å­å’Œåˆ†æ¯ä¸­æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘å› å­ä»¥é¿å…é™¤ä»¥é›¶ã€‚`alpha`æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼š
- en: '[PRE17]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This has been left as an exercise for you.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: Chapter 9 â€“ Hybrid Deep Learning Methods
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬9ç«  â€“ æ··åˆæ·±åº¦å­¦ä¹ æ–¹æ³•
- en: We donâ€™t provide a full answer here, but only some functions that will help
    you with the main task.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œä¸æä¾›å®Œæ•´çš„ç­”æ¡ˆï¼Œè€Œåªæä¾›ä¸€äº›å°†å¸®åŠ©ä½ å®Œæˆä¸»è¦ä»»åŠ¡çš„å‡½æ•°ã€‚
- en: 'We could use `torch.nn.functional.triplet_margin_loss()`, or we could implement
    it from scratch:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torch.nn.functional.triplet_margin_loss()`ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä»å¤´å®ç°å®ƒï¼š
- en: '[PRE18]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You would want to generate triplets for the imbalanced MNIST dataset. The following
    function generates a list of triplets (anchor, positive, and negative) for a batch
    of images. It generates one triplet per class present in the batch. We assume
    that there are at least two examples for each class in the batch:'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ ä¼šæƒ³è¦ä¸ºä¸å¹³è¡¡çš„MNISTæ•°æ®é›†ç”Ÿæˆä¸‰å…ƒç»„ã€‚ä»¥ä¸‹å‡½æ•°ä¸ºä¸€æ‰¹å›¾åƒç”Ÿæˆä¸‰å…ƒç»„åˆ—è¡¨ï¼ˆé”šç‚¹ã€æ­£ä¾‹å’Œè´Ÿä¾‹ï¼‰ã€‚å®ƒä¸ºæ‰¹æ¬¡ä¸­å­˜åœ¨çš„æ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸€ä¸ªä¸‰å…ƒç»„ã€‚æˆ‘ä»¬å‡è®¾æ‰¹æ¬¡ä¸­æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰ä¸¤ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE19]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This has been left as an exercise for you.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: Chapter 10 â€“ Model Calibration
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬10ç«  â€“ æ¨¡å‹æ ¡å‡†
- en: 'Yes, a well-calibrated model can have low accuracy and vice-versa. Letâ€™s take
    a dumb model that always outputs 0.1 probability for any input example. This model
    is perfectly calibrated, but its accuracy is only 90%, which is quite low for
    an imbalanced dataset with a 1:9 imbalance ratio. Here is the implementation of
    such a model:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œä¸€ä¸ªæ ¡å‡†è‰¯å¥½çš„æ¨¡å‹å¯ä»¥å…·æœ‰è¾ƒä½çš„å‡†ç¡®ç‡ï¼Œåä¹‹äº¦ç„¶ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ€»æ˜¯ä¸ºä»»ä½•è¾“å…¥ç¤ºä¾‹è¾“å‡º0.1æ¦‚ç‡çš„æ„šè ¢æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹æ ¡å‡†å¾—éå¸¸å®Œç¾ï¼Œä½†å®ƒçš„å‡†ç¡®ç‡åªæœ‰90%ï¼Œå¯¹äºä¸€ä¸ª1:9ä¸å¹³è¡¡æ¯”ç‡çš„å¤±è¡¡æ•°æ®é›†æ¥è¯´ç›¸å½“ä½ã€‚ä»¥ä¸‹æ˜¯æ­¤ç±»æ¨¡å‹çš„å®ç°ï¼š
- en: '[PRE20]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces the accuracy value and calibration plot:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¼šäº§ç”Ÿå‡†ç¡®ç‡å€¼å’Œæ ¡å‡†å›¾ï¼š
- en: '[PRE21]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/B17259_10_17.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_10_17.jpg)'
- en: Figure B.3 â€“ A dummy model with perfect calibration but a low accuracy score
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾B.3 â€“ ä¸€ä¸ªæ ¡å‡†å®Œç¾ä½†å‡†ç¡®ç‡å¾—åˆ†ä½çš„è™šæ‹Ÿæ¨¡å‹
- en: This has been left as an exercise for you.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: This has been left as an exercise for you.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: This has been left as an exercise for you.
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
- en: This has been left as an exercise for you.
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å·²è¢«ç•™ä½œä½ çš„ç»ƒä¹ é¢˜ã€‚
