- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored various ways to handle the data imbalance. In this
    chapter, we will see the need to do some post-processing of the prediction scores
    that we get from the trained models. This can be helpful either during the real-time
    prediction from the model or during the offline training time evaluation of the
    model. We will also understand some ways of measuring how calibrated the model
    is and how imbalanced datasets make the model calibration inevitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to model calibration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The influence of data balancing techniques on model calibration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting calibration curves for a model trained on a real-world dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model calibration techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of calibration on a model’s performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a clear understanding of what model
    calibration means, how to measure it, and when and how to apply it.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `matplotlib`, `numpy`, `scikit-learn`, `xgboost`, and `imbalanced-learn`. The
    code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter10).
    You can open the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon on the top of the chapter’s notebook or by launching it from [https://colab.research.google.com](https://colab.research.google.com)
    using the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to model calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between stating “*The model predicted the transaction
    as fraudulent*” and “*The* *model estimated a 60% probability of the transaction
    being fraudulent*”? When would one statement be more useful than the other?
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the two is that the second statement represents likelihood.
    This likelihood can be useful in understanding the model’s confidence, which is
    needed in many applications, such as in medical diagnosis. For example, the prediction
    that a patient is 80% likely or 80% probable to have cancer is more useful to
    the doctor than just predicting whether the patient has cancer or not.
  prefs: []
  type: TYPE_NORMAL
- en: A model is considered calibrated if there is a match between the number of positive
    classes and predicted probability. Let’s try to understand this further. Let’s
    say we have 10 observations, and for each of them, the model predicts a probability
    of 0.7 to be of the positive class. If the model is calibrated, then we expect
    7 out of those 10 observations to belong to the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: However, surprisingly, most machine learning models are not calibrated, and
    their prediction values tend to be overconfident or underconfident. What does
    that mean? An overconfident model would predict the probability to be 0.9 (for
    example), while the actual probability might have been only 0.6\. Similarly, an
    underconfident model would predict the probability to be 0.6 (for example) while
    the actual probability might have been 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: '*Do we always need to calibrate* *model probabilities?*'
  prefs: []
  type: TYPE_NORMAL
- en: Actually, it depends upon the problem at hand. If the problem inherently involves
    the ordering of certain items, say in search ranking, then all we need is relative
    scores and real probabilities don’t matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of an overconfident model where we can see that most of
    the time, the predicted probabilities from the model are much higher than the
    fraction of actual positive examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The calibration curve of an overconfident model for which predicted
    probabilities are overestimated
  prefs: []
  type: TYPE_NORMAL
- en: Why bother with model calibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve discussed, model calibration may not be necessary if the primary goal
    is to obtain a relative ranking of items. However, there are several other scenarios
    where model calibration becomes crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpreting model predictions as confidence**: Calibrated models allow the
    scores to be interpreted as the model’s confidence in its predictions. For example,
    in a spam detection system, a calibrated score of 0.9 could mean the model is
    90% confident that an email is spam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpreting model predictions as probabilities**: These scores can also
    be viewed as probabilities, making them directly interpretable. In a weather prediction
    model, a calibrated score of 0.8 could be interpreted as an 80% chance of rain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-stake applications**: Such calibrated probabilities are particularly
    useful in high-stake applications such as healthcare for disease prediction or
    in fraud detection. For instance, in predicting the likelihood of a patient having
    a certain disease, a calibrated score of 0.7 could mean there’s a 70% chance the
    patient has the disease, guiding further medical tests or treatments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing human interpretability and trust**: Human interpretability and
    trust in model predictions are enhanced when the model is calibrated. For example,
    in a loan approval system, a calibrated score could help loan officers understand
    the risk associated with a loan application, thereby aiding in the decision-making
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is particularly important to be aware of model calibration when working
    with deep learning models, as several common neural network hyperparameters can
    affect model calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model capacity**: More layers (depth) and more neurons (width) usually reduce
    the classification error but have been found to lower the calibration of the model
    [1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch norm**: Although batch norm typically improves training time, has a
    mild regularizing effect, and might even improve the accuracy of the model, it
    can also make the model more miscalibrated [1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight decay**: Weight decay is a regularization technique, and more weight
    decay typically helps calibrate the model. So, instead, if we have less weight
    decay, then we would expect the model to be more miscalibrated [1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see what kind of model scores typically need to be calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: Models with and without well-calibrated probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logistic regression model is often assumed to output calibrated probabilities,
    particularly when it is an appropriate fit for the data [2]. This assumption is
    based on the model’s optimization of the cross-entropy loss or log loss function.
    However, it’s worth noting that logistic regression can produce overconfident
    predictions, and regularization techniques such as L1/L2 can help the model be
    more conservative and thus improve calibration.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes models often push probabilities close to zero or one due to their
    assumption about feature independence, which can result in poor calibration [2].
    On the other hand, bagging models (such as random forests) and boosting models
    generally produce probabilities that are away from the extremes of zero and one.
    This is due to the score-averaging nature of the individual decision trees or
    stumps they use, which often leads to better calibration.
  prefs: []
  type: TYPE_NORMAL
- en: For neural networks, some research studies show that simple networks tend to
    give calibrated scores [2]. Still, since neural network models are getting more
    complex day by day, modern neural networks tend to be fairly uncalibrated [1]
    [3]. As *Figure 10**.2* shows, a five-layer LeNet is well calibrated since its
    confidence levels closely mirror the expected accuracy, evident by the bars roughly
    aligning along the diagonal. In contrast, while a 110-layer ResNet boasts higher
    accuracy (lower error), its confidence scores don’t align as closely with this
    accuracy [1].
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Reliability diagrams for a five-layer LeNet (left) and a 110-layer
    ResNet (right) on CIFAR-100 (adapted from Guo et al. [1])
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn how to measure whether a model is calibrated or not.
  prefs: []
  type: TYPE_NORMAL
- en: Calibration curves or reliability plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see how we can understand if the model’s scores are calibrated or not.
    Let’s assume we have a model that predicts if an image is of a cat or not.
  prefs: []
  type: TYPE_NORMAL
- en: A **calibration curve** is basically obtained by plotting the fraction of actual
    positive values (*y* axis) against predicted probability scores (*x* axis).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to plot the calibration curve, also known as a **reliability
    plot**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dataset with two columns: one with actual labels and another with
    predicted probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the data into ascending order using predicted probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the predicted probability dataset into fixed-size bins ranging from 0
    to 1\. For example, if we create 10 bins, we get 0.1, 0.2, 0.3, …, 0.9, 1.0\.
    If there are too many examples in the dataset, we can use smaller-size bins and
    vice versa.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now compute the fraction of actual positives in each bin. These fraction values
    will be our *y* axis values. On the *x* axis, we plot the fixed bin value, i.e.,
    0.1, 0.2, 0.3, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We get a plot like the one in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Plotting the probability predictions of an XGBoost classifier
    against the fraction of positives
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be careful with the number of bins chosen because of the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If we choose too few bins, the plot may look linear and well-fitted, giving
    the impression that the model is calibrated. More importantly, the real danger
    of using too few bins is that the curve won’t have enough detail; it will essentially
    be just a few points connected together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, if we choose too many bins, then the plot may look noisy, and we
    may wrongly conclude the model to be uncalibrated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might become particularly difficult to identify whether a model is calibrated
    or not if we are dealing with imbalanced datasets. If our dataset is not balanced
    and has a much smaller number of examples of positive classes to plot, then the
    calibration plot may look noisy or show that the model is underconfident or overconfident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, it’s worth noting that many models are not perfectly calibrated and
    their calibration curves may deviate from the perfect calibration line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – A calibration curve plot when fitted via an XGBoost model; on
    the left is an overconfident model and on the right is an underconfident model
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` provides a function called `calibration_curve` to easily plot
    this curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: However, visually judging and comparing various calibration plots can be error-prone,
    and we might want to use a metric that can make some kind of numerical comparison
    of the calibrations of two different models.
  prefs: []
  type: TYPE_NORMAL
- en: Brier score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a commonly used measure called the **Brier score**, which is basically
    the mean squared error of the predicted probability obtained from the model as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Brier score =  1 _ N  ∑ (predicted _ probability − actual _ label) 2
  prefs: []
  type: TYPE_NORMAL
- en: where N is the number of examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This score varies between 0 (best possible score) and 1 (worst possible score).
    This metric is very similar to the `scikit-learn` makes our job a bit easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following Brier score loss value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The paper *Class Probability Estimates are Unreliable for Imbalanced Data (and
    How to Fix Them)* by Wallace and Dahabreh [4] argues that a lower Brier score
    for an imbalanced dataset might just mean that the calibration is good overall
    but not necessarily for minority or rare classes. In order to track the calibration
    of individual classes, they proposed a stratified Brier score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Stratified Brier scores for positive and negative classes [4]
  prefs: []
  type: TYPE_NORMAL
- en: where N pos denotes the number of positive class examples, N neg denotes the
    number of negative class examples, y i is the label, and  ˆ P  is the model prediction
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an alternative metric to measure calibration that is more popular
    among deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Expected Calibration Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Expected Calibration Error** (**ECE**) [5] is another metric for measuring
    how calibrated a model is. Predicted probabilities from the model are grouped
    into M bins of equal size. Let’s assume B m is the set of examples whose prediction
    scores fall into the *m*th bin.'
  prefs: []
  type: TYPE_NORMAL
- en: Then for each bin (B m), we calculate the difference between the average predicted
    probability (that is, conf(B m)) and accuracy (that is, the proportion of examples
    correctly classified). This difference is | acc(B m) − conf(B m)|.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also weigh these differences by the number of examples in each bin and finally
    sum them up to get the overall ECE value. This is equivalent to multiplying the
    difference by B m/n, where n is the total number of examples. Finally, we sum
    this over all the bins to get the final formula:'
  prefs: []
  type: TYPE_NORMAL
- en: ECE = ∑ m=1 M |B m| /n *|acc(B m) − conf(B m)|
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy and confidence can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy acc(B m) is the proportion of examples in bin B m correctly classified
    by the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acc(B m) = (1 / |B m|)* ∑ i=1 |B m| I(y i = ŷ i)
  prefs: []
  type: TYPE_NORMAL
- en: 'Confidence is the average predicted probability of the examples in bin B m:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: conf(B m) = (1 / |B m|)* ∑ i=1 |B m| p i
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an extension of the previous metric called **Maximum Calibration Error**
    (**MCE**) that measures the largest difference between the accuracy and confidence
    *across all* *the bins*:'
  prefs: []
  type: TYPE_NORMAL
- en: MCE = ma x m=1 M |acc(B m) − conf(B m)|
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful in applications where it is important that the model be well
    calibrated in all bins, and MCE can then be minimized.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.6* shows a reliability diagram with the ECE and MCE values on
    the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Reliability diagram with ECE and MCE values for the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Model calibration in production at Netflix
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved:**'
  prefs: []
  type: TYPE_NORMAL
- en: Netflix aimed to provide recommendations [6] that were closely aligned with
    a user’s varied interests rather than focusing solely on their main preferences.
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️****Data imbalance:**'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional recommendation systems can amplify a user’s primary interests, thereby
    overshadowing their secondary or tertiary preferences. This can be considered
    a form of interest imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Model** **calibration strategy:**'
  prefs: []
  type: TYPE_NORMAL
- en: Netflix employed a greedy re-ranking approach to calibration. The initial model
    ranked movies based on the predicted likelihood of a user watching them. This
    ranking is then adjusted using a greedy algorithm to ensure that the top 10 recommended
    movies match the genre distribution in the user’s watch history.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: If a user’s watch history comprises 50% action, 30% comedy, and
    20% drama, the re-ranking algorithm reshuffles the top recommendations to reflect
    this distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**📊****Additional points:**'
  prefs: []
  type: TYPE_NORMAL
- en: The greedy re-ranking algorithm was straightforward to implement. It was empirically
    shown to improve recommendation performance across various datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This approach ensured that Netflix’s recommendations cater to the full spectrum
    of a user’s interests, preventing any single interest from dominating the suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s try to understand how data balancing techniques can
    affect the calibration of models.
  prefs: []
  type: TYPE_NORMAL
- en: The influence of data balancing techniques on model calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The usual impact of applying data-level techniques, such as oversampling and
    undersampling, is that they change the distribution of the training data for the
    model. This means that the model sees an almost equal number of all the classes,
    which doesn’t reflect the actual data distribution. Because of this, the model
    becomes less calibrated against the true imbalanced distribution of data. Similarly,
    algorithm-level cost-sensitive techniques that use `class_weight` to account for
    the data imbalance have a similar degraded impact on degrading the calibration
    of the model against the true data distribution. *Figure 10**.7* (log scale) from
    a recent study [7] shows the degrading calibration of a CNN-based model for pneumonia
    detection task, as `class_weight` increases from 0.5 to 0.9 to 0.99\. The model
    becomes over-confident and hence less calibrated with the increase in `class_weight`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Degrading calibration of a CNN model as class_weight changes from
    0.5 to 0.9 to 0.99 (log scale) (image adapted from Caplin, et al. [7])
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in *Figure 10**.8*, we show the calibration curve for the logistic
    regression model on the `thyroid_sick` UCI dataset. The corresponding notebook
    can be found in the GitHub repo of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – A calibration curve using logistic regression with no sampling
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.9* and *Figure 10**.10* demonstrate how oversampling techniques
    can worsen a model’s calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – A calibration curve using logistic regression with random oversampling
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – A calibration curve using logistic regression with SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, *Figure 10**.11* shows a similar effect for undersampling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – A calibration curve using logistic regression with random undersampling
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.12* shows how class weighting can negatively affect the model
    calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – A calibration curve with no sampling technique (left) and class
    weighting (right) using logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: In the plots that we just saw, both undersampling and oversampling made the
    model over-confident. Undersampling can make the model optimistic about its ability
    to classify the minority class while oversampling can lead the model to overestimate
    the likelihood of encountering minority instances. This overconfidence arises
    because the model assumes the altered training data represents the real-world
    distribution. To elaborate, when we undersample or oversample, we’re essentially
    telling the model that the minority class is more common (or less rare) than it
    actually is. The model can then generalize this skewed view to new, unseen data.
    As a result, it can become overconfident in its predictions for the minority class,
    thinking these outcomes are more likely than they actually are. This overconfidence
    doesn’t extend to the majority class because the model still sees plenty of those
    examples during training. Therefore, the model ends up being miscalibrated and
    tends to be overly sure of its predictions for the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will utilize a real-world dataset, train a model
    using this dataset, and then determine the calibration of the model by plotting
    calibration curves.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting calibration curves for a model trained on a real-world dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model calibration should ideally be done on a dataset that is separate from
    the training and test set. Why? It’s to avoid overfitting because the model can
    become too tailored to the training/test set’s unique characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: We can have a hold-out dataset that has been specifically set aside for model
    calibration. In some cases, we may have too little data to justify splitting it
    further into a separate hold-out dataset for calibration. In such cases, a practical
    compromise might be to use the test set for calibration, assuming that the test
    set has the same distribution as the dataset on which the model will be used to
    make final predictions. However, we should keep in mind that after calibrating
    on the test set, we no longer have an unbiased estimate of the final performance
    of the model, and we need to be cautious about interpreting the model’s performance
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `HR Data for Analytics` dataset from Kaggle ([https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics](https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics)).
    This dataset contains employee profiles of a large company, where each record
    is an employee.
  prefs: []
  type: TYPE_NORMAL
- en: 'The downloaded dataset `HR_comma_sep.csv` has been added to the GitHub repo
    of the book. Let’s load the dataset into a `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows some sample rows from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `last_evaluation` | `left` | ... | `sales` | `salary` |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | 0.53 | 1 | ... | sales | low |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | 0.86 | 1 | ... | sales | medium |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | 0.88 | 1 | ... | sales | medium |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | 0.87 | 1 | ... | sales | low |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | 0.52 | 1 | ... | sales | low |'
  prefs: []
  type: TYPE_TB
- en: '| `...` | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| `14994` | 0.57 | 1 | ... | support | low |'
  prefs: []
  type: TYPE_TB
- en: '| `14995` | 0.48 | 1 | ... | support | low |'
  prefs: []
  type: TYPE_TB
- en: '| `14996` | 0.53 | 1 | ... | support | low |'
  prefs: []
  type: TYPE_TB
- en: '| `14997` | 0.96 | 1 | ... | support | low |'
  prefs: []
  type: TYPE_TB
- en: '| `14998` | 0.52 | 1 | ... | support | low |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Sample rows from the HR Data for Analytics dataset from Kaggle
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s clear that some of the columns, such as `sales` and `salary`, are categorical.
    The `left` column is our labels column. Let’s get the imbalance in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the count of the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to convert the categorical columns into ID labels using `LabelEncoder`
    and then standardize these columns using `StandardScaler` from `sklearn`. After
    preprocessing, we split the dataset into three subsets: 80% for the training set
    and 10% each for the validation and test sets. We’ll skip the code for these steps
    and jump straight into training the model. For the complete code, please refer
    to the accompanying GitHub notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we will train a random forest model on the training set and use the
    test set for evaluating the model. We will use a validation set for calibrating
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s figure out how calibrated the model is. We print the Brier’s score and
    plot the calibration curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the calibration curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_10_13_New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – A calibration curve of an uncalibrated random forest model
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.13* shows that the model is overconfident in the initial range
    of predictions (0 to ~0.4) and is then underconfident afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at some techniques to improve the calibration
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Model calibration techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several ways to calibrate a model. There are two broad categorizations
    of the calibration techniques based on the nature of the method used to adjust
    the predicted probabilities to better align with the true probabilities: parametric
    and non-parametric:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric methods**: These methods assume a specific functional form for
    the relationship between the predicted probabilities and the true probabilities.
    They have a set number of parameters that need to be estimated from the data.
    Once these parameters are estimated, the calibration function is fully specified.
    Examples include Platt scaling, which assumes a logistic function, and beta calibration,
    which assumes a beta distribution. We will also discuss temperature scaling and
    label smoothing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-parametric methods**: These methods do not assume a specific functional
    form for the calibration function. They are more flexible and can adapt to more
    complex relationships between the predicted and true probabilities. However, they
    often require more data to produce a reliable calibration. Examples include isotonic
    regression, which fits a piece-wise constant function, and spline calibration,
    which uses spline (piecewise-defined polynomial) functions to fit the predicted
    probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we will explore a theoretical, formula-based method for calibrating scores
    for models trained on sampled data, specifically in the context of imbalanced
    data. Next, we’ll examine popular methods such as Platt’s scaling and isotonic
    regression, which are commonly used with classical machine learning models. Finally,
    we’ll introduce supporting techniques such as temperature scaling and label smoothing,
    which are more prevalent among deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: The calibration of model scores to account for sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we used oversampling or undersampling to balance a dataset, we can derive
    a theoretical calibration formula. As we saw in [*Chapter 2*](B17259_02.xhtml#_idTextAnchor042),
    *Oversampling Methods*, [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods* (both based on sampling), and [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, we can apply some (over/under) sampling techniques
    or data augmentation techniques to bump up the relative number of samples of the
    minority classes(s) in order to account for a data imbalance. As a result, we
    change the distribution of training data. Although downsampling enhances the model’s
    ability to distinguish between classes, it also leads to an overestimation of
    the predicted probabilities. Because of this, the model scores during inference
    (real-world prediction) time are still in the downsampled space, and we should
    bring the scores back to the real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the goal of downsampling is to balance the dataset in terms of the
    number of instances of positive and negative classes.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if there are 100 positive class instances and 200 negative class
    instances after downsampling, the ratio is w = 100/200 = 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming the number of negative class examples is more than the number of positive
    class examples, we define w as the ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: w =  Number of positive class instances in downsampled dataset    ________________________________________    Number
    of negative class instances in downsampled dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s assume p is the probability of selecting a positive class example
    from the original dataset (without any kind of downsampling used).
  prefs: []
  type: TYPE_NORMAL
- en: 'If p is the probability of selecting a positive class in the original dataset,
    then the probability p d of selecting a positive class from the downsampled dataset
    can be computed using the following formula [8]:'
  prefs: []
  type: TYPE_NORMAL
- en: p d =  p _ p+ w (1 − p)
  prefs: []
  type: TYPE_NORMAL
- en: Note that if w=1, that is, when no downsampling is done, p d = p. You can refer
    to the paper by Moscatelli et al. [9] for a proof of this relationship between
    the original and downsampled dataset probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the denominator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the various terms in the denominator of the previous formula:'
  prefs: []
  type: TYPE_NORMAL
- en: p is the probability of selecting a positive class instance in the original
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (1 − p) is the probability of selecting a negative class instance in the original
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: w (1 − p) is the probability of selecting a negative class instance when downsampling
    is used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summation p+ w (1 − p) is the total sum of probabilities for both the positive
    and downsampled negative classes in the dataset after downsampling the negative
    class by a factor of w
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we understand the previous formula, we can flip it to find out the
    probability p of selecting a positive class in the original dataset [10]:'
  prefs: []
  type: TYPE_NORMAL
- en: p =  p d _ p d + 1 − p d _ w
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: w is the ratio of positive class to negative class (called the negative downsampling
    ratio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p d is the probability of selecting a positive class from the downsampled dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s understand this with some numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a total of 100,000 examples with 10,000 from the positive class and
    90,000 from the negative class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s say we use a downsampling rate w = 0.5, which means that after downsampling,
    we have 10,000 positives and 20,000 negatives. This also implies that during downsampling,
    for every positive class example, we selected only two negative class examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s assume our prediction score from a model when trained on the downsampled
    dataset, p d, is 0.9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compute the prediction score in the original dataset. From the previous
    defined formula, p =  p d _ p d + 1 − p d _ w  =  0.9 _ 0.9 + 1 − 0.9 _ 0.5  =
     0.9 _ 0.9 + 0.2 = 0.82.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, a model prediction score of 0.9 in downsampled example changed to 0.82 in
    the original dataset. Notice how the probability got lowered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more generic and simpler formula [11] is the relationship between odds before
    and after sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: odds _ after = odds _ before *  proportion _ of _ 1 _ before _ sampling * (1
    − proportion _ of _ 1 _ after _ sampling)     _____________________________________________________     (1
    − proportion _ of _ 1 _ before _ sampling) * (proportion _ of _ 1 _ after _ sampling)
  prefs: []
  type: TYPE_NORMAL
- en: 'where *odds* are a way to express the probability of an event as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: odds =  probability ___________ 1 − probability
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.14* and *Figure 10**.15* show the model calibration plots before
    and after applying the previous calibration formula, respectively, when using
    random undersampling on the `thyroid_sick` UCI dataset available from the `imblearn.datasets`
    package. You can find the complete notebook on GitHub.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – A model calibration plot before calibration when using undersampling
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – A model calibration plots after calibration when using undersampling
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Model calibration in production at Meta
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved:**'
  prefs: []
  type: TYPE_NORMAL
- en: Meta aimed to accurately predict **Click-Through Rates** (**CTR**) for ads to
    optimize online bidding and auctions in Meta’s advertising system [10]. Accurate
    click prediction is crucial for optimizing online bidding and auctions.
  prefs: []
  type: TYPE_NORMAL
- en: '**⚖️** **Data** **imbalance issue:**'
  prefs: []
  type: TYPE_NORMAL
- en: Meta dealt with massive volumes of data, which inherently contained imbalances.
    A full day of Facebook ad impression data contained a huge number of instances.
    Meta used negative downsampling to speed up training and improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Model** **calibration strategy:**'
  prefs: []
  type: TYPE_NORMAL
- en: Since Meta used downsampling, they used the formula from the previous section,
  prefs: []
  type: TYPE_NORMAL
- en: p =  p d _ p d + 1 − p d _ w  , to re-calibrate the model prediction score.
  prefs: []
  type: TYPE_NORMAL
- en: '**📊** **Additional** **important points:**'
  prefs: []
  type: TYPE_NORMAL
- en: They explored the impact of data freshness and online learning on prediction
    accuracy. The efficiency of an ads auction depended on the accuracy and calibration
    of click prediction. They also used normalized cross-entropy loss and calibration
    as their major evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Platt’s scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With this technique, we try to map the classifier’s probabilities to the perfect
    calibration line. More precisely, we just fit a logistic regression model, with
    the input being the original model’s probability scores and the labels being the
    actual labels. The `CalibratedClassifierCV` API in `sklearn` already facilitates
    the implementation of this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the Brier score output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Brier score for Platt’s scaled model is smaller than that of the uncalibrated
    model, which was 0.0447, meaning that the Platt’s scaled model is calibrated better.
  prefs: []
  type: TYPE_NORMAL
- en: Isotonic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Isotonic regression is particularly useful when we expect a monotonic relationship
    between the input variables and the output. In this context, a monotonic function
    is one that is either entirely non-decreasing or entirely non-increasing. The
    monotonicity here refers to the relationship between the model’s raw output and
    the true probabilities, not the arrangement of the data points.
  prefs: []
  type: TYPE_NORMAL
- en: If the model’s output does not follow this expected monotonic behavior, isotonic
    regression can be applied to enforce it. Isotonic regression can be used in cases
    such as credit scoring or medical diagnosis, where a higher score should consistently
    indicate a higher likelihood of a particular outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This Brier score value is a further improvement over Platt’s scaling method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the calibration curves for both the techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Calibration curves for a random forest model
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 10**.16* shows, the isotonic regression is the closest to the perfectly
    calibrated curve; hence, it performed the best for our model and data. Platt’s
    scaling did pretty well at calibrating the model, too.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between Platt’s scaling and Isotonic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Platt’s scaling is considered more apt for problems where the model predictions
    follow the sigmoid curve. This makes sense because logistic regression (which
    is used by Platt’s scaling) uses a sigmoid to fit the data points. Isotonic regression
    has a much broader coverage of distortions that it can cover for the predicted
    probabilities. However, some research studies [2] show that isotonic regression
    is more prone to overfitting the predicted probabilities. Hence, its performance
    can be worse than Platt’s scaling when we only have a limited dataset since it
    doesn’t generalize well with the limited dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A general rule to follow
  prefs: []
  type: TYPE_NORMAL
- en: When the dataset at hand is very small or limited, choose Platt’s scaling. However,
    when data is sufficient enough not to have an overfitted model, isotonic regression
    usually does better than Platt’s scaling.
  prefs: []
  type: TYPE_NORMAL
- en: For the calibration of a multi-class classifier, we can use the **one-vs-rest**
    approach with individual calibration plots per class. We can apply techniques
    such as Platt’s scaling or Isotonic regression for enhanced predictability, just
    like binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Temperature scaling is a post-processing technique used to improve the calibration
    of neural networks. It works by scaling the logits (the output of the final layer
    of the network before applying the softmax function) using a temperature parameter.
    This has the effect of sharpening or softening the probabilities assigned to each
    class depending on the temperature value. By adjusting the temperature parameter,
    it is possible to achieve better calibration of the model’s confidence estimates,
    which can be useful in applications such as classification or ranking.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature scaling can be considered a multi-class extension of Platt’s scaling
    with only one hyper-parameter of temperature *T* > 0 for all classes.
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Label smoothing [12] is a regularization technique that’s known to improve model
    calibration. It modifies the training data that is used to train the model and
    is usually handled as a part of the model training. It is not a post-processing
    technique like temperature scaling and previous techniques.
  prefs: []
  type: TYPE_NORMAL
- en: When neural networks are trained, they often develop excessive confidence in
    their predictions, which can hinder their ability to generalize and perform well
    on new, unseen data. Therefore, it is beneficial to introduce a form of **regularization**
    that reduces the network’s level of certainty and improves its performance on
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a binary classification problem where the true labels can
    be either 0 or 1\. Without label smoothing, the training labels would be one-hot
    encoded, meaning the true label would be 1 for positive examples and 0 for negative
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: With label smoothing, we add a small amount of **noise** to the true labels.
    For example, we can set a smoothing factor of 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the original and smoothed labels for a positive example
    in binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: By adding this noise to the labels, the model is encouraged to be less confident
    in its predictions and to be more robust to small changes in the input data. This
    can lead to improved performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In large datasets, mislabeled data can be a concern. Neural networks should
    be designed to approach the correct answer cautiously to mitigate the impact of
    incorrect labels. Label smoothing helps in this regard by slightly adjusting the
    target labels, making the model less confident about its predictions. This can
    prevent the model from overfitting to noisy or incorrect labels.
  prefs: []
  type: TYPE_NORMAL
- en: According to the paper by Müller [13], label smoothing can improve model calibration
    by automatically adjusting the network’s output probabilities. This eliminates
    the need for manual temperature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing can help improve accuracy in various domains, such as image
    classification, text, and speech recognition problems. Most of the modern machine
    learning frameworks, including TensorFlow, PyTorch, and Transformers (from Hugging
    Face), provide built-in implementations for label smoothing in some form in their
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, it’s implemented in the cross-entropy loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here we can specify the amount of smoothing (as a floating-point value between
    0 and 1) when computing the loss and where a value of 0.0 (default) means no smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it can be helpful to add label smoothing to the loss functions if
    you are looking to add some regularization to your network.
  prefs: []
  type: TYPE_NORMAL
- en: Arguments against label smoothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some arguments against label smoothing. It’s just another hyperparameter
    to tune, and when there are few better regularization techniques such as weight
    decay and L1 regularization, it may be overkill to make your network more complex
    and implicitly modify the labels of your training data. Another point to consider
    is that since it adds random noise to the labels, it’s possible that the network
    might underfit in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: There are some further improved variants of label smoothing, such as **label-aware
    smoothing**, as mentioned in Zhong et al.’s *Improving Calibration for Long-Tailed*
    *Recognition* [14].
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a comparison of the four techniques we just discussed
    for model calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Theme** | **Temperature scaling** | **Label smoothing** | **Platt’s scaling**
    | **Isotonic Regression** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Change in label values | No change in label values of training data | Change
    in label values of training data | No change in the label values of the training
    data | No change in the label values of the training data |'
  prefs: []
  type: TYPE_TB
- en: '| Timing | After training has been completed, the value of the hyperparameter
    T is computed on a validation dataset | Done during the actual training of the
    model | Applied after training | Applied after training |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction value adjustment | Prediction values from the model are manually
    adjusted | Predicted values are changed by applying label smoothing | Prediction
    values from the model are manually adjusted | Prediction values from the model
    are manually adjusted |'
  prefs: []
  type: TYPE_TB
- en: '| Role | Acts as a regularizer | Acts as a regularizer | Acts as a model calibrator
    or prediction score transformer | Acts as a model calibrator or prediction score
    transformer |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – Comparing temperature scaling, label smoothing, Platt’s scaling,
    and isotonic regression
  prefs: []
  type: TYPE_NORMAL
- en: There are other model calibration techniques that we didn’t get a chance to
    explore. For instance, **spline calibration** [15][16] is a non-parametric method
    that employs a spline function, a piece-wise polynomial function that is smooth
    and continuous. This technique is somewhat similar to isotonic regression in its
    non-parametric nature.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **beta calibration** [17] is a parametric method that fits
    a beta distribution to the model’s predictions. This technique is conceptually
    similar to Platt’s scaling, as both are parametric methods. Beta calibration is
    particularly useful for modeling probabilities, such as click-through rates or
    customer conversion rates.
  prefs: []
  type: TYPE_NORMAL
- en: '**Focal loss**, discussed in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*, is another method commonly used in
    deep learning models. As demonstrated in the paper by Mukhoti et al. [3], focal
    loss produces well-calibrated models and is often combined with temperature scaling
    for optimal results. Given that neural networks with multiple layers tend to be
    overconfident in their predictions, focal loss serves as a regularizing effect.
    It forces the model to focus on harder examples, thereby reducing overconfidence
    and improving calibration [3].'
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Model calibration with focal loss at Amazon
  prefs: []
  type: TYPE_NORMAL
- en: '**🎯** **Problem** **being solved:**'
  prefs: []
  type: TYPE_NORMAL
- en: When Amazon deployed conversational bots [18] to handle customer requests, the
    calibration of the underlying ML models proved to be of importance. In one instance,
    Amazon’s chatbot was tasked with automatically classifying return reason codes.
    These return reason codes exhibited class imbalance. When a customer wanted to
    return an item, determining the appropriate reason became pivotal for efficient
    return processing. For instance, if a customer expressed dissatisfaction with
    an item’s size or color, it was classified under “Customer Preference.” In such
    cases, Amazon understood that offering a replacement wasn’t the optimal solution;
    rather, a refund was more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '**🎨** **Model** **calibration strategy:**'
  prefs: []
  type: TYPE_NORMAL
- en: Through rigorous testing, they uncovered the robustness of focal loss in addressing
    model miscalibration in such real-world tasks. Focal loss was used as a calibration
    method. Moreover, it wasn’t merely about adopting focal loss; the value of *γ*
    within the loss function played a crucial role in enhancing model calibration.
  prefs: []
  type: TYPE_NORMAL
- en: '**📊****Additional points:**'
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss outperformed traditional cross-entropy loss in achieving better-calibrated
    models. The technique was tested in an internal A/B experiment at Amazon. The
    results showed improvements in automation rate and customer experience, meaning
    the bot could resolve more queries without human intervention and receive more
    positive responses from customers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see in what ways calibration might impact the performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of calibration on a model’s performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy, log-loss, and Brier scores usually improve because of calibration.
    However, since the model calibration still involves approximately fitting a model
    to the calibration curve plotted on the held-out calibration dataset, it may sometimes
    worsen the accuracy or other performance metrics by small amounts. Nevertheless,
    the benefits of having calibrated probabilities in terms of giving us actual interpretable
    probability values that represent likelihood far outweigh the slight performance
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction
    to Data Imbalance in Machine Learning*, ROC-AUC is a rank-based metric, meaning
    it evaluates the model’s ability to distinguish between classes based on the ranking
    of predicted scores rather than their absolute values. ROC-AUC doesn’t make any
    claim about accurate probability estimates. Strictly monotonic calibration functions,
    which continuously increase or decrease without any flat regions, preserve this
    ranking; they adjust the scale of the probabilities without altering their relative
    order. For instance, if one score is higher than another before calibration, it
    remains higher afterward. Because ROC-AUC is concerned with the ranking of predictions
    rather than the actual probability values, it remains unaffected by such monotonic
    calibration functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, in rare cases, closely ranked predictions might become tied due to
    calibration, especially if the calibration function is loosely monotonic and has
    flat stretches. This could slightly affect the ROC-AUC.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the basic concepts of model calibration, why
    we should care about it, how to measure whether a model is calibrated, how data
    imbalance affects the model calibration, and, finally, how to calibrate an uncalibrated
    model. Some of the calibration techniques we talked about include Platt’s scaling,
    isotonic regression, temperature scaling, and label smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we come to the end of this book. Thank you for dedicating your time
    to reading the book. We trust that it has broadened your knowledge of handling
    imbalanced datasets and their practical applications in machine learning. As we
    draw this book to a close, we’d like to offer some concluding advice on how to
    effectively utilize the techniques discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Like other machine learning techniques, the methods discussed in this book can
    be highly useful under the right conditions, but they also come with their own
    set of challenges. Recognizing when and where to apply these techniques is essential,
    as overly complex solutions can lead to less-than-optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a sound baseline solution is crucial. Implementing various methods,
    such as those in cost-sensitive learning and algorithm-level deep learning techniques,
    can offer insights into handling imbalanced datasets effectively. Each method
    has its pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: For specialized problems, the book provides targeted solutions. For small datasets,
    the oversampling methods can help manage computational resources. For large datasets,
    the chapter on undersampling methods offers suitable techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Occasionally, more modern approaches such as graph machine learning algorithms
    can be applied to the problem at hand. The model calibration and threshold tuning
    techniques are useful for decision-making based on model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, data imbalance may not be a problem at all, and we highly encourage
    you to establish the baseline performance with the imbalanced data without applying
    any of the techniques discussed in this book. A lot of the real-world data also
    tends to be tabular, where tree-based models such as XGBoost can be robust to
    certain kinds of data imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to apply this knowledge, experiment with new approaches, and
    continue to expand your expertise as you progress in this field. The landscape
    of machine learning is constantly changing, and your skills will only increase
    in value as you keep up with its evolution. We hope that the knowledge you’ve
    gained will empower you to pick up any research paper that you feel interested
    in and be able to reproduce its results. We appreciate your commitment to reading
    this book, and we wish you success in all your future endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Can a well-calibrated model have low accuracy? What about the reverse: can
    a model with high accuracy be poorly calibrated?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a limited classification dataset with, say, only 100 data points. Train
    a decision tree model using this dataset and then assess its calibration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calibrate the model using Platt’s scaling. Measure the Brier score after calibration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calibrate the model using isotonic regression. Measure the Brier score after
    calibration
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How do the Brier scores differ in (A) and (B)?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the AUC, accuracy, precision, recall, and F1 score of the model before
    and after calibration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a balanced dataset, say with 10,000 points. Train a decision tree model
    using it. Then check how calibrated it is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calibrate the model using Platt’s scaling. Measure the Brier score after calibration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calibrate the model using isotonic regression. Measure the Brier score after
    calibration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How do the Brier scores differ in (a) and (b)?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the AUC, accuracy, precision, recall, and F1 score of the model before
    and after calibration and compare their values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given a classification dataset, compare how calibrated the following models
    are by default without applying any calibration techniques by comparing their
    Brier scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take an imbalanced dataset and train three models with logistic regression,
    a random forest model, and an XGBoost model, respectively. Measure the calibration
    of these models using the calibration curve and Brier scores. Finally, apply these
    techniques to handle data imbalance and measure the calibration again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Undersampling
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Oversampling
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cost-sensitive learning: increase the `class_weight` by doubling the previous
    value. Did the model get less calibrated because of doubling the `class_weight`?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “*On Calibration of Modern
    Neural Networks*.” arXiv, Aug. 03, 2017\. Accessed: Nov. 21, 2022, [http://arxiv.org/abs/1706.04599](http://arxiv.org/abs/1706.04599)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Niculescu-Mizil and R. Caruana, “*Predicting good probabilities with supervised
    learning*,” in Proceedings of the 22nd International Conference on Machine Learning
    - ICML ‘05, Bonn, Germany, 2005, pp. 625–632\. doi: 10.1145/1102351.1102430.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. H. S. Torr, and P. K. Dokania,
    “*Calibrating Deep Neural Networks using Focal Loss*”. Feb 2020, [https://doi.org/10.48550/arXiv.2002.09437](https://doi.org/10.48550/arXiv.2002.09437)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'B. C. Wallace and I. J. Dahabreh, “*Class Probability Estimates are Unreliable
    for Imbalanced Data (and How to Fix Them)*,” in 2012 IEEE 12th International Conference
    on Data Mining, Brussels, Belgium, Dec. 2012, pp. 695–704\. doi: 10.1109/ICDM.2012.115.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Pakdaman Naeini, G. Cooper, and M. Hauskrecht, “*Obtaining Well Calibrated
    Probabilities Using Bayesian Binning*,” AAAI, vol. 29, no. 1, Feb. 2015, doi:
    10.1609/aaai.v29i1.9602.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H. Steck, “*Calibrated recommendations*,” in Proceedings of the 12th ACM Conference
    on Recommender Systems, Vancouver British Columbia Canada: ACM, Sep. 2018, pp.
    154–162\. doi: 10.1145/3240323.3240372.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Caplin, D. Martin, and P. Marx, “*Calibrating for Class Weights by Modeling
    Machine Learning*.” arXiv, Jul. 31, 2022\. Accessed: Dec. 09, 2022\. [Online].
    Available at [http://arxiv.org/abs/2205.04613](http://arxiv.org/abs/2205.04613)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, “*Calibrating Probability
    with Undersampling for Unbalanced Classification*,” in 2015 IEEE Symposium Series
    on Computational Intelligence, Cape Town, South Africa, Dec. 2015, pp. 159–166\.
    doi: 10.1109/SSCI.2015.33, [https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf](https://dalpozz.github.io/static/pdf/SSCI_calib_final_noCC.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Moscatelli, S. Narizzano, F. Parlapiano, and G. Viggiano, *Corporate default
    forecasting with machine learning*. IT: Banca d’Italia, 2019\. Accessed: Oct.
    14, 2023\. [Online]. Available at [https://doi.org/10.32057/0.TD.2019.1256](https://doi.org/10.32057/0.TD.2019.1256)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. He et al., “*Practical Lessons from Predicting Clicks on Ads at Facebook*,”
    in Proceedings of the Eighth International Workshop on Data Mining for Online
    Advertising, New York NY USA, Aug. 2014, pp. 1–9\. doi: 10.1145/2648584.2648589.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. King and L. Zeng, “*Logistic Regression in Rare Events* *Data*,” 2001.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “*Rethinking the
    Inception Architecture for Computer Vision*.” arXiv, Dec. 11, 2015\. Accessed:
    Dec. 17, 2022\. [Online]. Available at [http://arxiv.org/abs/1512.00567](http://arxiv.org/abs/1512.00567)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. Müller, S. Kornblith, and G. Hinton, “*When Does Label Smoothing Help?*”
    arXiv, Jun. 10, 2020\. Accessed: Dec. 11, 2022\. [Online]. Available at [http://arxiv.org/abs/1906.02629](http://arxiv.org/abs/1906.02629)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhong et al., Improving Calibration for Long-Tailed Recognition. CVPR 2021\.
    [https://arxiv.org/abs/2104.00466](https://arxiv.org/abs/2104.00466)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'B. Lucena, “*Spline-Based Probability Calibration*.” arXiv, Sep. 20, 2018\.
    Accessed: Jul. 22, 2023\. [Online]. Available at [http://arxiv.org/abs/1809.07751](http://arxiv.org/abs/1809.07751)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Gupta, A. Rahimi, T. Ajanthan, T. Mensink, C. Sminchisescu, and R. Hartley,
    “*Calibration of Neural Networks using Splines*.” arXiv, Dec. 29, 2021\. Accessed:
    Jul. 22, 2023\. [Online]. Available at [http://arxiv.org/abs/2006.12800](http://arxiv.org/abs/2006.12800)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Kull and P. Flach, “*Beta calibration: a well-founded and easily implemented
    improvement on logistic calibration for binary classiﬁers*,” in Proceedings of
    the twentieth International Conference on Artificial Intelligence and Statistics
    (pp. 623–631).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'C. Wang, J. Balazs, G. Szarvas, P. Ernst, L. Poddar, and P. Danchenko, “Calibrating
    Imbalanced Classifiers with Focal Loss: An Empirical Study,” in Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing: Industry
    Track, Abu Dhabi, UAE: Association for Computational Linguistics, 2022, pp. 145–153\.
    doi: 10.18653/v1/2022.emnlp-industry.14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Pipeline in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this appendix, we will look at when and at which step we incorporate the
    data imbalance-handling techniques within a production machine learning pipeline.
    This mainly applies to supervised classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning training pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A machine learning pipeline is the end-to-end process of training one or more
    machine learning models and then deploying them to a live environment. It may
    involve stages such as data collection, model training, validation, deployment,
    monitoring, and iterative improvement, with a focus on scalability, efficiency,
    and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Various steps during the offline training are shown in *Figure A.1*. Please
    note that some of the steps may not be necessary depending on the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_App_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.1 – High-level steps in a machine learning training pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sequence of steps involved in building a model that can
    handle data imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gather data**: The first step involves gathering the necessary data for training
    the machine learning model. This data can be sourced from various places such
    as databases, files, APIs, or through web scraping. Immediately after gathering,
    it’s often beneficial to perform **data validation**. During this phase, the data
    schema and data range can be verified, along with any custom data validation checks.
    Subsequently, the data is partitioned into a training set, a validation set, and
    a test set. Many production systems often do not prioritize producing validation
    sets. The primary function of producing these validation sets is to aid in model-tuning
    activities, such as hyperparameter adjustment, early stopping, model calibration,
    threshold tuning, and so on. Such tuning often takes place during the model development
    phase, outside of the main production pipeline. **It’s crucial to split the data
    prior to executing any data transformations or imbalance handling techniques**.
    This precaution ensures data leakage is avoided, which could otherwise lead to
    a biased model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data transformation**: The next step is to transform the data into a format
    that can be easily fed into the machine learning model. This may involve tasks
    such as data cleaning, feature selection, normalization, and scaling. These transformation
    steps may need to be stored in order to apply them during model prediction time.
    It can be helpful to store these transformations somewhere (for example, a file
    or database) so that they can be retrieved later during inferencing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Handle data imbalance (if needed)**: The machine learning model might underperform
    on a minority class(es) due to a bias toward the majority class. Throughout this
    book, we’ve delved into both data-level and algorithm-level techniques. To summarize,
    data-level techniques focus on resampling the dataset to achieve balanced samples
    across each class, whereas algorithm-level techniques modify the learning algorithm
    to address imbalanced data. For a deeper understanding, please refer to the relevant
    chapters in the book.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train model**: After the data has been preprocessed, it’s time to train the
    machine learning model. This step involves the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting an appropriate algorithm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting its hyperparameters
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Feeding the preprocessed data into the algorithm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The training process may require several iterations to fine-tune the model until
    it produces satisfactory results. The trained model binary should be versioned
    and stored for future use, including deploying to a production environment for
    online inferencing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If any data imbalance handling technique was applied, it could miscalibrate
    the model. If calibrated prediction scores are expected from the model, it’s crucial
    to recalibrate the prediction scores. For more information on various model calibration
    techniques, please refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Evaluate model**: This step involves assessing the performance of the trained
    model on the test set produced in *step 1*. For classification problems, metrics
    such as accuracy, precision, and recall are typically used, while for other types
    of problems, appropriate metrics should be chosen. If the model’s performance
    doesn’t meet the desired benchmarks, you may need to not only revisit the data
    transformations (as outlined in *step 2*) but also consider adjusting the model’s
    architecture, hyperparameters, or even the problem formulation. For binary classification
    models specifically, you’ll want to determine an appropriate threshold for classifying
    predictions. For more in-depth information on threshold tuning techniques, please
    refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After successfully evaluating the model, its fitness for deployment as a service
    is assessed, enabling it to handle live traffic or make batch predictions. We
    will delve deeper into inferencing in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing (online or batch)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inferencing is a process of using a trained machine learning model to make predictions
    on new unseen data. **Online inferencing** refers to making predictions in real
    time on live data as it arrives. Latency is of utmost importance during online
    inferencing in order to prevent any lags to the end user.
  prefs: []
  type: TYPE_NORMAL
- en: There is another type called **batch inferencing**, where predictions are made
    on a large set of already collected data in an offline fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_App_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.2 – Process flow when live data comes to the model for scoring (inferencing)
  prefs: []
  type: TYPE_NORMAL
- en: 'Inferencing is a process of using a trained machine learning model to make
    predictions on new input (unseen) data in real time. The following are the steps
    involved in the inferencing process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input data**: The first step is to receive new input data that needs to be
    classified or predicted. This data could be in the form of text, images, audio,
    or any other data format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transform data**: Before predicting, the input data needs to undergo transformations
    (such as normalization and scaling) to make it compatible with the trained model.
    It’s crucial to apply the same transformations that were used during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model prediction**: Once the input data has been transformed, it is fed into
    the trained model to generate a predicted score. The predicted score represents
    the likelihood of the input belonging to a particular class or category.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calibrate score (if needed)**: Model calibration can be essential when model
    predictions are not reliable. Notably, when any data imbalance handling techniques
    are used, the risk of model miscalibration increases. For a comprehensive understanding
    of this topic, please refer to [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final prediction using threshold**: The calibrated score is then used to
    make the final prediction using an appropriate threshold and take any action—for
    example, notification to the customer, human reviews, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 🌟 Monitoring data as well as model in production
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data and its distribution is crucial as it can change over time,
    potentially impacting the effectiveness of any initially applied imbalance-handling
    techniques. Such shifts can affect model performance and evaluation metrics, necessitating
    a re-evaluation and potential recalibration of strategies. Beyond just data imbalance,
    phenomena such as model drift and data drift—where there is a change in the model’s
    performance or the nature of incoming data—pose significant concerns. Implementing
    automated mechanisms to track these variations is essential for ensuring optimal
    model performance and consistent predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, inferencing entails transforming new input data, producing a
    predicted score using a trained machine learning model, calibrating that score,
    and determining a final prediction using a threshold. This procedure is reiterated
    for every incoming data point requiring a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "Chapter 1 – Introduction to Data Imbalance in \LMachine Learning"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The choice of loss function when training a model can greatly affect the performance
    of the model on imbalanced datasets. Some loss functions may be more sensitive
    to class imbalance than others. For instance, a model trained with a loss function
    such as cross-entropy might be heavily influenced by the majority class and perform
    poorly on the minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PR curve is more informative than the ROC curve when dealing with highly
    skewed datasets because it focuses on the performance of the classifier on the
    positive (minority) class, which is often the class of interest in imbalanced
    datasets. The ROC curve, on the other hand, considers both the TPR and the FPR
    and thus might give an overly optimistic view of the model’s performance when
    the negative class dominates the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accuracy can be a misleading metric for model performance on imbalanced datasets
    because it does not take into account the distribution of classes. A model that
    always predicts the majority class will have high accuracy, but it is not useful
    if our goal is to correctly classify instances of the minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the context of imbalanced datasets, feature engineering poses a unique challenge
    due to the limited number of instances in the minority class. With so few examples,
    it becomes difficult even for human experts to identify features that are truly
    indicative of the minority class. Poorly chosen features can worsen the problem:
    if the features capture noise rather than the underlying pattern, the model is
    likely to overfit. Conversely, if the features are too generic and fail to capture
    the nuances of the minority class, the model may underfit, leading to poor performance
    on new, unseen data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The choice of “k” in k-fold cross-validation can impact the model’s performance
    on imbalanced datasets. With imbalanced datasets, some folds may contain very
    few or even no examples from the minority classes, potentially leading to misleading
    evaluations of the model. A solution to this issue is to use stratified k-fold
    cross-validation, available through the `sklearn.model_selection.StratifiedKFold`
    API, which ensures that each fold maintains a similar distribution of the various
    classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usually, the greater the imbalance in the test set, the more negatively the
    PR curve is affected. In contrast, the ROC curve is not affected by the class
    distribution in the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In *Figures 1.13* and *1.14*, we presented three test sets with imbalance ratios
    of 1:9, 1:3, and 1:1\. The ROC-AUC for all these cases is 0.96, as shown in *Figure
    1**.13*. On the other hand, the average precision value is inversely proportional
    to the level of imbalance in the test set, as illustrated in *Figure 1**.14* (that
    is, greater imbalance results in lower average precision):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17259_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.1 – ROC curves remain the same when the imbalance ratio changes in
    the test set
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.2 – The PR curve changes considerably when the imbalance ratio changes
    in the test set
  prefs: []
  type: TYPE_NORMAL
- en: Having a high AUC-ROC but a low AUC-PR in the context of an imbalanced dataset
    could indicate that the model is performing well in distinguishing between the
    classes overall (as indicated by the high AUC-ROC), but it is not doing a good
    job at identifying the positive (minority) class (as indicated by the low AUC-PR).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sampling bias can contribute to the challenge of imbalanced datasets in machine
    learning because it can lead to an overrepresentation of one class and an underrepresentation
    of another. This can skew the model’s learning and result in poor performance
    in the underrepresented class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labeling errors can contribute to the challenge of imbalanced datasets in machine
    learning because they can lead to an incorrect representation of the classes in
    the data. If instances of the minority class are mistakenly labeled as the majority
    class, the model might learn incorrect patterns and perform poorly on the minority
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many real-world scenarios where dealing with imbalanced datasets is
    inherently part of the problem. Some examples include fraud detection (where fraudulent
    transactions are rare compared to legitimate ones), medical diagnosis (where diseases
    are often rare compared to healthy cases), and spam detection (where spam emails
    are typically fewer than non-spam emails). Can you think of any others?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are the answers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Its value ranges from -1 (worst value) to +1 (best value).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dummy model always predicts class 1, so here are our various confusion
    matrix values:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: TP = 90, TN = 0, FP=10, FN = 0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MCC =  TP · TN − FP · FN    __________________________________    √ ______________________________________    (TP
    + FP) · (TP + FN) · (TN + FP) · (TN + FN)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The confusion matrix values are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: TP = 90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TN = 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: FP = 10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: FN = 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By plugging these values into the formula, we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MCC =  (90 × 0) − (10 × 0)  _____________________________   √ _________________________________   (90
    + 10) × (90 + 0) × (0 + 10) × (0 + 0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: =  0 − 0 ______________  √ _______________  100 × 90 × 10 × 0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since the denominator becomes zero (because of the term (TN + FN = 0 + 0)),
    the MCC is undefined.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can compute the other metrics as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Accuracy = TP+TN/ (TP+TN+FP+FN) = 0.90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision = TP/(TP+FP) = 90/(90+10) = 0.90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall = TP/(TP+FN) = 90/(90+0) = 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: F1 score = 2*Precision*Recall/(Precision+Recall) = 2*0.90*1/(0.90+1) = 0.95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compute MCC for the previous values. It’s undefined (0/0), which would
    mean something is wrong with the model, and we should go back and fix any issues
    with the data or the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In summary, MCC is a metric that generates a high score only if the model does
    well on both positive and negative class examples from the test set. Also, MCC
    can help inform the user about ongoing prediction issues.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This is left as an exercise for you.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 2 – Oversampling Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One approach is to oversample the minority class by 20x to balance both classes.
    It’s important to note that achieving the perfect balance between the classes
    is not always necessary; a slight imbalance may be acceptable, depending on the
    specific requirements and constraints. This technique is not applied at test time
    as the test data should remain representative of what we would encounter in the
    real world.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The primary concern with oversampling before splitting the data into training,
    test, and validation sets is data leakage. This occurs when duplicate samples
    end up in both the training and test/validation sets, leading to overly optimistic
    performance metrics. The model may perform well during evaluation because it has
    already seen the same examples during training, but this can result in poor generalization
    to new, unseen data. To mitigate this risk, it’s crucial to first split the data
    into training, test, and validation sets and then apply balancing techniques such
    as oversampling exclusively to the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data normalization can help indirectly in dealing with data imbalance by ensuring
    that all features have the same scale, which can lead to better model performance.
    However, normalization may not directly address the imbalance between the classes
    in the dataset. To tackle data imbalance, other techniques can be employed, such
    as various sampling techniques, cost-sensitive approaches, or threshold adjustment
    techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3 – Undersampling Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TomekLinksNCR` is a custom undersampling method that combines Tomek links
    and NCR. It removes Tomek links and then applies NCR to remove more noisy and
    borderline samples from the majority class. This aims to create a more balanced
    dataset while retaining the underlying structure of the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 4 – Ensemble Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main difference between `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
    is the base classifier and the ensemble learning method they employ. `BalancedRandomForestClassifier`
    uses decision trees as base classifiers and follows a random forest as the estimator,
    while `BalancedBaggingClassifier` can use any base classifier that supports sample
    weighting and follows a bagging approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest can be considered an extension of bagging that incorporates an
    additional layer of randomness by also randomly selecting a subset of features
    at each split in the decision tree. This helps create more diverse trees and generally
    results in better performance of random forest models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 5 – Cost-Sensitive Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s questions have been left as exercises for you.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 – Data Imbalance in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main challenge stems from the different types of data these models handle.
    Classical machine learning models typically work with structured, tabular data,
    while deep learning models handle unstructured data such as images, text, audio,
    and video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An imbalanced version of the MNIST dataset can be created by randomly selecting
    a certain percentage of examples for each class. This process involves choosing
    indices of the samples to remove and then actually removing these samples from
    the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random oversampling is used to address imbalance in the dataset. It works by
    duplicating samples from the minority classes until each class has an equal number
    of samples. This technique is usually considered to perform better than no sampling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data augmentation techniques can include rotating, scaling, cropping, blurring,
    adding noise to the image, and much more. However, ensuring these augmentations
    preserve the original labels and avoiding inadvertently removing important details
    from the data is crucial. Please refer to [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, for a detailed discussion of the various data
    augmentation techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Undersampling reduces the instances of the majority class to balance the dataset.
    However, this method has a significant limitation: important information might
    be lost if instances from the majority class are randomly removed, especially
    when the majority class has a lot of variation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data augmentation techniques must preserve the original labels because the
    model learns to associate the features of the data with these labels. If the labels
    change due to augmentation, the model might learn incorrect associations, which
    would degrade its performance when making predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 7 – Data-Level Deep Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s questions have been left as exercises for you.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8 – Algorithm-Level Deep Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tversky loss is based on the Tversky index, which is defined by the following
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TverskyIndex =  TruePositive   _______________________________________    TruePositive+
    α * FalsePositive + (1 − α) * FalseNegative
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A smoothing factor is added to both the numerator and denominator to avoid
    division by zero. `alpha` is a hyperparameter that can be tuned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 9 – Hybrid Deep Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We don’t provide a full answer here, but only some functions that will help
    you with the main task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We could use `torch.nn.functional.triplet_margin_loss()`, or we could implement
    it from scratch:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You would want to generate triplets for the imbalanced MNIST dataset. The following
    function generates a list of triplets (anchor, positive, and negative) for a batch
    of images. It generates one triplet per class present in the batch. We assume
    that there are at least two examples for each class in the batch:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 10 – Model Calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yes, a well-calibrated model can have low accuracy and vice-versa. Let’s take
    a dumb model that always outputs 0.1 probability for any input example. This model
    is perfectly calibrated, but its accuracy is only 90%, which is quite low for
    an imbalanced dataset with a 1:9 imbalance ratio. Here is the implementation of
    such a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the accuracy value and calibration plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B17259_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.3 – A dummy model with perfect calibration but a low accuracy score
  prefs: []
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This has been left as an exercise for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
