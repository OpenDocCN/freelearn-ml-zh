- en: Seeing a Heartbeat with a Motion-Amplifying Camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Remove everything that has no relevance to the story. If you say in the first
    chapter that there is a rifle hanging on the wall, in the second or third chapter
    it absolutely must go off. If it''s not going to be fired, it shouldn''t be hanging
    there."'
  prefs: []
  type: TYPE_NORMAL
- en: – Anton Chekhov
  prefs: []
  type: TYPE_NORMAL
- en: '"King Julian: I don''t know why the sacrifice didn''t work. The science seemed
    so solid."'
  prefs: []
  type: TYPE_NORMAL
- en: '– Madagascar: Escape 2 Africa (2008)'
  prefs: []
  type: TYPE_NORMAL
- en: Despite their strange design and mysterious engineering, Q's gadgets always
    prove useful and reliable. Bond has such faith in the technology that he never
    even asks how to charge the batteries.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the more inventive ideas in the Bond franchise is that even a lightly
    equipped spy should be able to see and photograph concealed objects, anyplace,
    anytime. Let''s consider a timeline of a few relevant gadgets in the movies, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1967 (*You Only Live Twice*)**: An X-ray desk scans guests for hidden firearms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1979 (*Moonraker*)**: A cigarette case contains an X-ray imaging system that
    is used to reveal the tumblers of a safe''s combination lock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1989 (*License to Kill*)**: A Polaroid camera takes X-ray photos. Oddly enough,
    its flash is a visible, red laser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1995 (*GoldenEye*)**: A tea tray contains an X-ray scanner that can photograph
    documents beneath the tray.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1999 (*The World is Not Enough*)**: Bond wears a stylish pair of blue-lensed
    glasses that can see through one layer of clothing to reveal concealed weapons.
    According to the *James Bond Encyclopedia* (2007), which is an official guide
    to the movies, the glasses display infrared video after applying special processing
    to them. Despite using infrared, they are commonly called **X-ray specs**, a misnomer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These gadgets deal with unseen wavelengths of light (or radiation) and are broadly
    comparable to real-world devices such as airport security scanners and night-vision
    goggles. However, it remains difficult to explain how Bond's equipment is so compact
    and how it takes such clear pictures under diverse lighting conditions and through
    diverse materials. Moreover, if Bond's devices are active scanners (meaning they
    emit X-ray radiation or infrared light), they will be clearly visible to other
    spies using similar hardware.
  prefs: []
  type: TYPE_NORMAL
- en: To take another approach, what if we avoid unseen wavelengths of light but instead
    focus on unseen frequencies of motion? Many things move in a pattern that is too
    fast or too slow for us to easily notice. Suppose a man is standing in one place.
    If he shifts one leg more than the other, perhaps he is concealing a heavy object,
    such as a gun, on the side that he shifts more. Equally, we might also fail to
    notice deviations from a pattern; suppose the same man has been looking straight
    ahead but suddenly, when he believes no one is looking, his eyes dart to one side.
    Is he watching someone?
  prefs: []
  type: TYPE_NORMAL
- en: We can make motions of a certain frequency more visible by repeating them, like
    a delayed afterimage or a ghost, with each repetition appearing fainter (or less
    opaque) than the last. The effect is analogous to an echo or a ripple and is achieved
    using an algorithm called **Eulerian video magnification**.
  prefs: []
  type: TYPE_NORMAL
- en: By applying this technique, we will build a desktop app that allows us to simultaneously
    see the present and selected slices of the past. The idea of experiencing multiple
    images simultaneously is, to me, quite natural because, for the first 26 years
    of my life, I had **strabismus**—commonly called a **lazy eye**—which caused double
    vision. A surgeon corrected my eyesight and gave me depth perception but in memory
    of strabismus, I would like to name this application `Lazy Eyes`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look, or two or more closer looks, at the fast-paced, moving
    world that we share with other secret agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what Eulerian video magnification can do
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting repeating signals from video using the fast Fourier transform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compositing two images using image pyramids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Lazy Eyes app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and testing the app for various motions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter''s project has the following software dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Python environment with the following modules**: OpenCV, NumPy, SciPy,
    PyFFTW, wxPython'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where not otherwise noted, setup instructions are covered in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*. Setup instructions for PyFFTW are covered in the
    current chapter, in the section *Choosing and setting up an FFT library*. Always
    refer to the setup instructions for any version requirements. Basic instructions
    for running Python code are covered in [Appendix C](c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml),
    *Running with Snakes (or, First Steps with Python)*.
  prefs: []
  type: TYPE_NORMAL
- en: The complete project for this chapter can be found in this book's GitHub repository,
    [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition),
    in the `Chapter007` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the Lazy Eyes app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of all our apps, `Lazy Eyes` has the simplest user interface. It just shows
    a live video feed with a special effect that highlights motion. The parameters
    of the effect are quite complex and, moreover, modifying them at runtime would
    have a big effect on performance. Thus, we do not provide a user interface to
    reconfigure the effect, but we do provide many parameters in code to allow a programmer
    to create many variants of the effect and the app. Below the video panel, the
    app displays the current frame rate, measured in **frames per second** (**FPS**).
    The following screenshot illustrates one configuration of the app. This screenshot
    shows me eating cake. Because my hands and face are moving, we see an effect that
    looks like light and dark waves rippling near moving edges (the effect is more
    graceful in a live video than in a screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14bb7071-cc2e-424e-8fc3-42809889ee68.png)'
  prefs: []
  type: TYPE_IMG
- en: For more screenshots and an in-depth discussion of the parameters, see the *Configuring
    and testing the app for various motions *section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of how it is configured, the app loops through the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Capture an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy and downsample the image while applying a blur filter and, optionally,
    an edge-finding filter. We will downsample using so-called **image pyramids**,
    which will be discussed in the *Compositing two images using image pyramids* section
    later in this chapter. The purpose of downsampling is to achieve a higher frame
    rate by reducing the amount of image data that's used in subsequent operations.
    The purpose of applying a blur filter and, optionally, an edge-finding filter
    is to create halos that are useful in amplifying motion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the downsampled copy in a history of frames, with a timestamp. The history
    has a fixed capacity. Once it is full, the oldest frame is overwritten to make
    room for the new one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the history is not yet full, continue to the next iteration of the loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and display the average frame rate based on the timestamps of the
    frames in the history.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decompose the history into a list of frequencies describing fluctuations (motion)
    at each pixel. The decomposition function is called a **fast Fourier transform**
    (**FFT**). We will discuss it in the*Extracting repeating signals from video using
    the fast Fourier transform *section later in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set all frequencies to zero except a chosen range of interest. In other words,
    filter out the data on motions that are faster or slower than certain thresholds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompose the filtered frequencies into a series of images that are motion maps.
    Areas that are still (with respect to our chosen range of frequencies) become
    dark, and areas that are moving remain bright. The `recomposition` function is
    called an **inverse fast Fourier transform** (**IFFT**), which we will discuss
    later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upsample the latest motion map (again, using image pyramids), intensify it,
    and overlay it additively atop the original camera image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show the resulting composite image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it—a simple plan that requires rather nuanced implementation and configuration.
    So, with that in mind, let's prepare ourselves by doing a little background research
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what Eulerian video magnification can do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eulerian video magnification is inspired by a model in fluid mechanics called
    **Eulerian specification of the flow field**. Let's consider a moving, fluid body,
    such as a river. The Eulerian specification describes the river's velocity at
    a given position and time. The velocity would be fast in the mountains in springtime
    and slow at the river's mouth in winter. The velocity would also be slower at
    a silt-saturated point at the river's bottom, compared to a point where the river's
    surface hits a rock and sprays. An alternative to the Eulerian specification is
    the **Lagrangian specification**, which describes the position of a given particle
    at a given time. For example, a given bit of silt might make its way down from
    the mountains to the river's mouth over a period of many years and then spend
    eons drifting around a tidal basin.
  prefs: []
  type: TYPE_NORMAL
- en: For a more formal description of the Eulerian specification, the Lagrangian
    specification, and their relationship, refer to the following Wikipedia article
    at [http://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field](http://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field).
  prefs: []
  type: TYPE_NORMAL
- en: The Lagrangian specification is analogous to many computer vision tasks in which
    we model the motion of a particular object or feature over time. However, the
    Eulerian specification is analogous to our current task, in which we model any
    motion occurring in a particular position and a particular window of time. Having
    modeled a motion from an Eulerian perspective, we can visually exaggerate the
    motion by overlaying the model's results for a blend of positions and times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set a baseline for our expectations of Eulerian video magnification
    by studying other people''s projects, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Michael Rubenstein''s webpage at MIT ([http://people.csail.mit.edu/mrub/vidmag/](http://people.csail.mit.edu/mrub/vidmag/)):
    Gives an abstract of his team''s pioneering work on Eulerian video magnification,
    along with demo videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bryce Drennan''s eulerian-magnification library ([https://github.com/brycedrennan/eulerian-magnification](https://github.com/brycedrennan/eulerian-magnification)):
    Implements the algorithm using NumPy, SciPy, and OpenCV. This implementation is
    good inspiration for us, but is designed for processing prerecorded videos and
    is not sufficiently optimized for real-time input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's move on and understand the functions that are the building blocks
    of these projects and ours.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting repeating signals from video using the fast Fourier transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An audio signal is typically visualized as a bar chart or wave. The bar or wave
    is high when the sound is loud and low when it is soft. We recognize that a repetitive
    sound, such as a metronome's beat, makes repetitive peaks and valleys in the visualization.
    When audio has multiple channels (such as a stereo or surround-sound recording),
    each channel can be considered a separate signal and can be visualized as a separate
    bar chart or wave.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in a video, every channel of every pixel can be considered a separate
    signal, rising and falling (becoming brighter and dimmer) over time. Imagine that
    we use a stationary camera to capture a video of a metronome. In this case, certain
    pixel values will rise and fall at a regular interval as they capture the passage
    of the metronome's needle. If the camera has an attached microphone, its signal
    values will rise and fall at the same interval. Based on either the audio or the
    video, we can then measure the metronome's frequency—its **beats per minute**
    (**bpm**) or its beats per second (Hertz or Hz). Conversely, if we change the
    metronome's bpm setting, the effect on both the audio and the video will be predictable.
    From this thought experiment, we can learn that a signal—be it audio, video, or
    any other kind—can be expressed as a function of time and, *equivalently*, a function
    of frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following pair of graphs. They express the same signal, first
    as a function of time and then as a function of frequency. Within the time domain,
    we see one wide peak and valley (in other words, a tapering effect) spanning many
    narrow peaks and valleys. Within the frequency domain, we can see a low-frequency
    peak and a high-frequency peak, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320f1e75-511e-4178-84d5-d1367685ee8d.png)'
  prefs: []
  type: TYPE_IMG
- en: The transformation from the time domain to the frequency domain is called the
    **Fourier transform** (**FT**). Conversely, the transformation from the frequency
    domain to the time domain is called the **inverse Fourier transform**. Within
    the digital world, signals are discrete, not continuous, so we use the terms **discrete
    Fourier transform** (**DFT**) and **inverse discrete Fourier transform** (**IDFT**).
    There is a variety of efficient algorithms for computing the DFT or IDFT, and
    such an algorithm might be described as a FFT or IFFT.
  prefs: []
  type: TYPE_NORMAL
- en: For algorithmic descriptions, refer to the following Wikipedia article at [http://en.wikipedia.org/wiki/Fast_Fourier_transform](http://en.wikipedia.org/wiki/Fast_Fourier_transform).
  prefs: []
  type: TYPE_NORMAL
- en: The result of the FT (including its discrete variants) is a function that maps
    a frequency to an amplitude and phase. The **amplitude** represents the magnitude
    of the frequency's contribution to the signal. The **phase** represents a temporal
    shift; it determines whether the frequency's contribution starts on a high or
    a low. Typically, the amplitude and phase are encoded in a complex number, `a+bi`,
    where `amplitude=sqrt(a^2+b^2)` and `phase=atan2(a, b)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an explanation of complex numbers, see the following Wikipedia article:
    [http://en.wikipedia.org/wiki/Complex_number](http://en.wikipedia.org/wiki/Complex_number).'
  prefs: []
  type: TYPE_NORMAL
- en: The FFT and IFFT are fundamental to a field of computer science called **digital
    signal processing**. Many signal processing applications, including `Lazy Eyes`,
    involve taking the signal's FFT, modifying or removing certain frequencies in
    the FFT result, and then reconstructing the filtered signal in the time domain
    using the IFFT. For example, this approach allows us to amplify certain frequencies
    while leaving others unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Now, where do we find this functionality?
  prefs: []
  type: TYPE_NORMAL
- en: Choosing and setting up an FFT library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Several Python libraries provide FFT and IFFT implementations that can process
    NumPy arrays (and thus OpenCV images). The five major contenders are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*NumPy*, which provides FFT and IFFT implementations in a module called `numpy.fft`
    ([http://docs.scipy.org/doc/numpy/reference/routines.fft.html](http://docs.scipy.org/doc/numpy/reference/routines.fft.html)).
    The module also offers other signal processing functions for working with the
    output of the FFT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SciPy*, which provides FFT and IFFT implementations in a module called `scipy.fftpack`
    ([http://docs.scipy.org/doc/scipy/reference/fftpack.html](http://docs.scipy.org/doc/scipy/reference/fftpack.html)).
    This SciPy module is closely based on the `numpy.fft` module, but adds some optional
    arguments and dynamic optimizations based on the input format. The SciPy module
    also adds more signal processing functions for working with the output of the
    FFT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenCV* itself has implementations of FFT (`cv2.dft`) and IFT (`cv2.idft`).
    The following official tutorial provides examples and a comparison to NumPy''s
    FFT implementation: [https://docs.opencv.org/master/d8/d01/tutorial_discrete_fourier_transform.html](https://docs.opencv.org/master/d8/d01/tutorial_discrete_fourier_transform.html).
    Note that OpenCV''s FFT and IFT interfaces are not directly interoperable with
    the `numpy.fft` and `scipy.fftpack` modules, which offer a broader range of signal
    processing functionality. (They format the data very differently.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyFFTW* ([https://hgomersall.github.io/pyFFTW/](https://hgomersall.github.io/pyFFTW/)),
    which is a Python wrapper around a C library called the **Fastest Fourier Transform
    in the West** (**FFTW**) ([http://www.fftw.org/](http://www.fftw.org/)). FFTW
    provides multiple implementations of FFT and IFFT. At runtime, it dynamically
    selects implementations that are well-optimized for given input formats, output
    formats, and system capabilities. Optionally, it takes advantage of multithreading
    (and its threads may run on multiple CPU cores, as the implementation releases
    Python''s **Global Interpreter Lock** (**GIL**)). PyFFTW provides optional interfaces
    matching NumPy''s and SciPy''s FFT and IFFT functions. These interfaces have a
    low overhead cost (thanks to good caching options that are provided by PyFFTW)
    and they help to ensure that PyFFTW is interoperable with a broad range of signal
    processing functionality, as implemented in `numpy.fft` and `scipy.fftpack`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reinka* ([http://reikna.publicfields.net/en/latest/](http://reikna.publicfields.net/en/latest/)),
    which is a Python library for GPU-accelerated computations, uses either PyCUDA
    ([http://mathema.tician.de/software/pycuda/](http://mathema.tician.de/software/pycuda/))
    or PyOpenCL ([http://mathema.tician.de/software/pyopencl/](http://mathema.tician.de/software/pyopencl/))
    as a backend. Reinka provides FFT and IFFT implementations in a module called
    `reikna.fft`. Reinka internally uses PyCUDA or PyOpenCL arrays (not NumPy arrays)
    and provides interfaces for conversion from NumPy arrays to these GPU arrays and
    back. The converted NumPy output is compatible with other signal processing functionality,
    as implemented in `numpy.fft` and `scipy.fftpack`. However, this compatibility
    comes at a high overhead cost due to the need to lock, read, and convert the contents
    of GPU memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy, SciPy, OpenCV, and PyFFTW are open source libraries under the BSD license.
    Reinka is an open-source library under the MIT license.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend PyFFTW because of its optimizations and its interoperability (at
    a low overhead cost), and all the other functionality that interests us in NumPy,
    SciPy, and OpenCV. For a tour of PyFFTW's features, including its NumPy- and SciPy-compatible
    interfaces, see the official tutorial at [https://hgomersall.github.io/pyFFTW/sphinx/tutorial.html](https://hgomersall.github.io/pyFFTW/sphinx/tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on our platform, we can set up PyFFTW in one of the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Mac, the third-party MacPorts package manager offers PyFFTW packages for
    some versions of Python, currently including Python 3.6 but not Python 3.7\. To
    install PyFFTW with MacPorts, open a Terminal and run something like the following
    command (but substitute your Python version if it differs from `py36`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, on any system, use Python''s package manager, pip, to install
    PyFFTW. Open a command prompt and run something like the following command (depending
    on your system, you might need to replace `pip` with `pip3` in order to install
    PyFFTW for Python 3):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Some versions of pip''s `pyFFTW` package have installation bugs that affect
    some systems. If `pip` fails to install the `pyFFTW` package, try again, but manually
    specify version 10.4 of the package by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that some old versions of the library are called `PyFFTW3`. We do not want
    `PyFFTW3`. On Ubuntu 18.04 and its derivatives, the `python-fftw` package in the
    system's standard apt repository is an old `PyFFTW3` version.
  prefs: []
  type: TYPE_NORMAL
- en: We have our FFT and IFFT needs covered by the Fastest Fourier Transform in the
    West (and if we were cowboys instead of secret agents, we could say, *Cover me!*).
    For additional signal processing functionality, we will use SciPy, which can be
    set up in the way we described in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*, in the *Setting up a development machine* section.
  prefs: []
  type: TYPE_NORMAL
- en: Signal processing is not the only new material that we must learn about for
    Lazy Eyes, so let's look at other functionality that is provided by OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Compositing two images using image pyramids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running an FFT on a full-resolution video feed would be slow. The resulting
    frequencies may also reflect localized phenomena at each captured pixel, so that
    the motion map (the result of filtering the frequencies and then applying the
    IFFT) might appear noisy and overly sharp. To address these problems, we need
    a cheap, blurry downsampling technique. However, we also want the option to enhance
    edges, which are important to our perception of motion.
  prefs: []
  type: TYPE_NORMAL
- en: Our need for a blurry downsampling technique is fulfilled by a **Gaussian image
    pyramid**. A **Gaussian filter** blurs an image by making each output pixel a
    weighted average of multiple input pixels in the neighborhood. An image pyramid
    is a series in which each image is a fraction of the width and height of the previous
    image. Often, the fraction is one half. The halving of image dimensions is achieved
    by *decimation*, meaning that every other pixel is simply omitted. A Gaussian
    image pyramid is constructed by applying a Gaussian filter before each decimation
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Our need to enhance edges in downsampled images is fulfilled by a **Laplacian
    image pyramid**, which is constructed in the following manner. Suppose we have
    already constructed a Gaussian image pyramid. We take the image at level `i+1`
    in the Gaussian pyramid, upsample it by duplicating pixels, and apply a Gaussian
    filter to it again. We then subtract the result from the image at level `i` in
    the Gaussian pyramid to produce the corresponding image at level `i` of the Laplacian
    pyramid. Thus, the Laplacian image is the difference between a blurry, downsampled
    image and an even blurrier image that was downsampled, downsampled again, and
    upsampled.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder how such an algorithm is a form of edge-finding. Consider that
    edges are areas of local contrast, while non-edges are areas of local uniformity.
    If we blur a uniform area, it is still uniform—there is zero difference. If we
    blur a contrasting area, however, it becomes more uniform—there is a non-zero
    difference. Thus, the difference can be used to find edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gaussian and Laplacian image pyramids are described in detail in the following
    journal article: E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and
    J. M. Ogden. "Pyramid methods in image processing". RCA Engineer, Vol. 29, No.
    6, November/Dececember 1984. It can be downloaded from [http://persci.mit.edu/pub_pdfs/RCA84.pdf](http://persci.mit.edu/pub_pdfs/RCA84.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides using image pyramids to downsample the FFT's input, we can also use
    them to upsample the most recent frame of the IFFT's output. This upsampling step
    is necessary for creating an overlay that matches the size of the original camera
    image so that we can composite the two. Like in the construction of the Laplacian
    pyramid, upsampling consists of duplicating pixels and applying a Gaussian filter.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV implements the relevant downsizing and upsizing functions as `cv2.pyrDown`
    and `cv2.pyrUp`. These functions are useful in compositing two images in general
    (whether or not signal processing is involved), because they allows us to soften
    differences while preserving edges. The OpenCV documentation includes a good tutorial
    on this topic at [https://docs.opencv.org/master/dc/dff/tutorial_py_pyramids.html](https://docs.opencv.org/master/dc/dff/tutorial_py_pyramids.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are armed with the necessary knowledge, it's time to implement `Lazy
    Eyes`!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Lazy Eyes app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a new folder for Lazy Eyes and, in this folder, create copies
    of or links to the `ResizeUtils.py` and `WxUtils.py` files from any of our previous
    Python projects, such as `The Living Headlights` from [Chapter 5](b4619968-1f90-45f0-8a77-3505624bc0c0.xhtml),
    *Equipping Your Car with a Rearview Camera and Hazard Detection*. Alongside the
    copies or links, let''s create a new file, `LazyEyes.py`. Edit it and enter the
    following `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Besides the modules that we have used in the previous projects, we are now using
    the standard library's `collections` module for efficient collections, as well
    as the `timeit` module for precise timing. For the first time, we are also using
    the signal processing functionality from PyFFTW and SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like our other Python applications, `Lazy Eyes` is implemented as a class that
    extends `wx.Frame`. The following code block contains the declarations of the
    class and its initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The initializer''s arguments affect the app''s frame rate and the manner in
    which motion is amplified. These effects are discussed in detail in the section
    *Configuring and testing the app for various motions *later in this chapter. The
    following is just a brief description of the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxHistoryLength` is the number of frames (including the current frame and
    preceding frames) that are analyzed for motion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minHz` and `maxHz`, respectively, define the slowest and fastest motions that
    are amplified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amplification` is the scale of the visual effect. A higher value means motion
    is highlighted more brightly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numPyramidLevels` is the number of pyramid levels by which frames are downsampled
    before signal processing is done. Each level corresponds to downsampling by a
    factor of `2`. Our implementation assumes `numPyramidLevels>0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `useLaplacianPyramid` is `True`, frames are downsampled using a Laplacian
    pyramid before signal processing is done. The implication is that only edge motion
    is highlighted. Alternatively, if `useLaplacianPyramid` is `False`, a Gaussian
    pyramid is used, and motion in all areas is highlighted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `useGrayOverlay` is `True`, frames are converted to grayscale before signal
    processing is done. The implication is that motion is only highlighted in areas
    of grayscale contrast. Alternatively, if `useGrayOverlay` is `False`, motion is
    highlighted in areas that have contrast in any color channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numFFTThreads` and `numIFFTThreads`, are the numbers of threads that are used
    in FFT and IFFT computations, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cameraDeviceID` and `imageSize` are our usual capture parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The initializer''s implementation begins in the same way as our other Python
    apps. It sets flags to indicate that the app is running and should be mirrored
    by default. It creates the capture object and configures its resolution to match
    the requested width and height, if possible. Failing that, the device''s fallback
    capture resolution is used. The initializer also declares variables to store images,
    and creates a lock to manage thread-safe access to the images. The relevant code
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to determine the shape of the history of frames. We already know
    that it has at least three dimensions—a number of frames, and a width and height
    for each frame. The width and height are downsampled from the capture width and
    height based on the number of pyramid levels. If we are concerned with color motion,
    and not just grayscale motion, the history also has a fourth dimension that consists
    of three color channels. The following code calculates the history''s shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of `>>`, the right bitshift operator, in the preceding code; it's
    used to divide the dimensions by a power of two. The power is equal to the number
    of pyramid levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to store the specified maximum history length. For the frames in
    the history, we will create a NumPy array of the shape we just determined. For
    timestamps of the frames, we will create a **double-ended queue** (**deque**),
    a type of collection that allows us to cheaply add or remove elements from either
    end, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will store the remaining arguments because we will need to pass them to
    the pyramid functions and signal processing functions for each frame later, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure meaningful error messages and early termination in the case of invalid
    arguments, we could add code such as the following for each argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assert numPyramidLevels > 0, \`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        ''numPyramidLevels must be positive.''`'
  prefs: []
  type: TYPE_NORMAL
- en: For brevity, such assertions are omitted from our code samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to call the following two functions to tell PyFFTW to cache its
    data structures (notably, its NumPy arrays) for a period of at least 1.0 seconds
    from their last use. (The default is 0.1 seconds.) Caching is a critical optimization
    for the PyFFTW interfaces that we are using, so we will choose a period that is
    more than long enough to keep the cache alive from frame to frame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following code, the initializer''s implementation ends with
    code to set up a window, event bindings, a video panel, a layout, and a background
    thread, which are all familiar tasks from our previous Python projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We must now modify our usual `_onCloseWindow` callback to disable PyFFTW''s
    cache. Disabling the cache ensures that resources are freed and that PyFFTW''s
    threads terminate normally. The callback''s implementation is shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The escape key is bound to our usual `_onQuitCommand` callback, which just
    closes the app, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The video panel''s erase and paint events are bound to our usual callbacks,
    `_onVideoPanelEraseBackground` and `_onVideoPanelPaint`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The loop running on our background thread is similar to the one used in other
    Python apps. For each frame, it calls a helper function, `_applyEulerianVideoMagnification`.
    The loop''s implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_applyEulerianVideoMagnification` helper function is quite long, so we
    will consider its implementation in several chunks. First, we need to create a
    timestamp for the frame and copy the frame to a format that is more suitable for
    processing. Specifically, we will use a floating point with either one gray channel
    or three color channels, depending on the configuration, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this copy, we will calculate the appropriate level in the Gaussian or
    Laplacian pyramid, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For the purposes of the history and signal processing functions, we will refer
    to this pyramid level as *the image *or *the frame*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to check the number of history frames that have been filled so
    far. If the history has more than one unfilled frame (meaning the history still
    won''t be full after adding the frame), we will append and timestamp the new image,
    before returning it early, so that no signal processing is done until a later
    frame. This can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the history is just one frame short of being full (meaning the history will
    be full after adding this frame), we will append the new image and timestamp,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If the history is already full, we will drop the oldest image and timestamp
    and append the new image and timestamp, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The history of image data is a NumPy array and, as such, we are using the terms
    *append *and *drop *loosely. NumPy arrays are immutable, meaning that they cannot
    grow or shrink. Moreover, we are not recreating this array because it is large,
    and reallocating each frame would be expensive. Instead, we are just overwriting
    data within the array by moving old data leftward and copying new data in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the timestamps, we will calculate the average time per frame in the
    history, and we will display the frame rate, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then proceed with a combination of signal processing functions, collectively
    called a **temporal bandpass filter**. This filter blocks (zeros out) some frequencies
    and allows others to pass and remain unchanged. Our first step in implementing
    this filter is to run the `pyfftw.interfaces.scipy_fftpack.fft` function using
    the history and number of threads as arguments. Also, with the `axis=0` argument,
    we will specify that the history''s first axis is the time axis, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pass the FFT result and the time per frame to the `scipy.fftpack.fftfreq`
    function. This function will then return an array of midpoint frequencies (in
    Hz, in our case) corresponding to the indices in the FFT result. (This array answers
    the question, *Which frequency is the midpoint of the bin of frequencies represented
    by index* `i` *in the FFT?*) We will find the indices whose midpoint frequencies
    lie closest to our initializer''s `minHz` and `maxHz` parameters (a minimum of
    absolute value difference). Then, we will modify the FFT result by setting the
    data to zero in all ranges that do not represent frequencies of interest, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The FFT result is symmetrical—`fftResult[i]` and `fftResult[-i-1]` pertain to
    the same bin of frequencies. Thus, we modify the FFT result symmetrically.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the Fourier transform maps a frequency to a complex number that encodes
    an amplitude and phase. Thus, while the indices of the FFT result correspond to
    frequencies, the values contained at those indices are complex numbers. Zero as
    a complex number is written in Python as `0+0j` or `0j`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having filtered out the frequencies that do not interest us, we will now finish
    applying the temporal bandpass filter by passing the data to the `pyfftw.interfaces.scipy_fftpack.ifft`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'From the IFFT result, we will take the most recent frame. It should somewhat
    resemble the current camera frame, but should be black in areas that do not exhibit
    recent motion that matches our parameters. We will multiply this filtered frame
    so that the non-black areas become bright. Then, we will upsample it (using a
    pyramid technique) and add the result to the current camera frame so that areas
    of motion are lit up. The relevant code, which concludes the `_applyEulerianVideoMagnification`
    method, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This concludes the implementation of the `LazyEyes` class. Our module''s `main`
    function just instantiates and runs the app, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: That's all! Now, it's time to run the app and stay still while it builds up
    its history of frames. Until the history is full, the video feed will not show
    any special effects. At the history's default length of 360 frames, it fills in
    about 50 seconds on a machine. Once it is full, you should start to see ripples
    moving through the video feed in areas of recent motion—or perhaps even everywhere
    if the camera moves or the lighting or exposure changes. The ripples should then
    gradually settle and disappear in areas of the scene that become still, with new
    ripples appearing in new areas of motion. Feel free to experiment on your own.
    Now, let's discuss a few recipes for configuring and testing the parameters of
    the `LazyEyes` class.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and testing the app for various motions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Currently, our `main` function initializes the `LazyEyes` object with the default
    parameters. If we were to fill in the same parameter values explicitly, we would
    have the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This recipe calls for a capture resolution of *640 x 480* and a signal processing
    resolution of *160 x 120* (as we are downsampling by `2` pyramid levels, or a
    factor of `4`). We are amplifying the motion only at frequencies of 0.833 Hz to
    1.0 Hz, only at edges (as we are using the Laplacian pyramid), only in grayscale,
    and only over a history of 360 frames (about 20 to 40 seconds, depending on the
    frame rate). Motion is exaggerated by a factor of `32`. These settings are suitable
    for many subtle upper-body movements such as a person's head swaying side to side,
    shoulders heaving with breathing, nostrils flaring, eyebrows rising and falling,
    and eye scanning to and fro. For performance, FFT and IFFT are each using `4`
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'How the app looks when it runs with its default parameters is shown in the
    following screenshot. Moments before taking the screenshot, I smiled before returning
    to my normal expression. Note that my eyebrows and mustache are visible in multiple
    positions, including their current low positions and their previous high positions.
    For the sake of capturing the motion amplification effect in a still image, this
    gesture is quite exaggerated. However, in a moving video, we can see the amplification
    of more subtle movements, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8fdda93-ce9d-45d0-aac9-526c0806f5ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot illustrates an example where my eyebrows appear taller
    after being raised and then lowered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c2fde30-5d3f-43cc-af6c-1547965bb234.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameters interact with each other in complex ways. Consider the following
    relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: Frame rate is greatly affected by the size of the input data for the FFT and
    IFFT functions. The size of the input data is determined by `maxHistoryLength`
    (a shorter length provides less input and thus a faster frame rate), `numPyramidLevels`
    (more levels implies less input), `useGrayOverlay` (`True` implies less input),
    and `imageSize` (a smaller size is less input).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame rate is also greatly affected by the level of multithreading of the FFT
    and IFFT functions, as determined by `numFFTThreads` and `numIFFTThreads` (a greater
    number of threads is faster up to some point).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame rate is slightly affected by `useLaplacianPyramid` (`False` implies a
    faster frame rate), as the Laplacian algorithm requires extra steps beyond the
    Gaussian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame rate determines the amount of time that `maxHistoryLength` represents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame rate and `maxHistoryLength` determine how many repetitions of motion (if
    any) can be captured in the `minHz` to `maxHz` range. The number of captured repetitions,
    together with `amplification`, determines how greatly a motion or a deviation
    from the motion will be amplified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inclusion or exclusion of noise is affected by `minHz` and `maxHz` (depending
    on which frequencies of noise are characteristic of the camera), `numPyramidLevels`
    (more levels implies a less noisy image), `useLaplacianPyramid` (`True` is less
    noisy), `useGrayOverlay` (`True` is less noisy), and `imageSize` (a smaller size
    implies a less noisy image).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inclusion or exclusion of motion is affected by `numPyramidLevels` (fewer
    means the amplification is more inclusive of small motions), `useLaplacianPyramid`
    (`False` is more inclusive of motion in non-edge areas), `useGrayOverlay` (`False`
    is more inclusive of motion in areas of color contrast), `minHz` (a lower value
    is more inclusive of slow motion), `maxHz` (a higher value is more inclusive of
    fast motion), and `imageSize` (a bigger size is more inclusive of small motions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subjectively, the visual effect is always more impressive when the frame rate
    is high, noise is excluded, and small motions are included. Again subjectively,
    other conditions for including or excluding motion (edge versus non-edge, grayscale
    contrast versus color contrast, or fast versus slow) are application-dependent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's try our hand at reconfiguring `Lazy Eyes`, starting with the `numFFTThreads`
    and `numIFFTThreads` parameters. We want to determine the numbers of threads that
    maximize Lazy Eyes' frame rate on a given machine. The more CPU cores there are,
    the more threads one can gainfully use. However, experimentation is the best guide
    for picking a number.
  prefs: []
  type: TYPE_NORMAL
- en: Run `LazyEyes.py`. Once the history fills up, the history's average FPS will
    be displayed in the lower left corner of the window. Wait until this average FPS
    value stabilizes. It might take a minute for the average to adjust to the effect
    of the FFT and IFFT functions. Take note of the FPS value, close the app, adjust
    the thread count parameters, and test again. Repeat these steps until you feel
    that you have enough data to pick a good number of threads to use on the relevant
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: By activating additional CPU cores, multithreading can cause your system's temperature
    to rise. As you experiment, monitor your machine's temperature, fans, and CPU
    usage statistics. If you become concerned, reduce the number of FFT and IFFT threads.
    Having a sub-optimal frame rate is better than overheating your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Now, experiment with other parameters to see how they affect FPS; the `numPyramidLevels`,
    `useGrayOverlay`, and `imageSize` parameters should all have a considerable effect.
    At a threshold of approximately 12 FPS, a series of frames starts to look like
    continuous motion instead of *a slide show*. The higher the frame rate, the smoother
    the motion will appear. Traditionally, hand-drawn animated movies run at 12 drawings
    per second for most scenes, and 24 drawings per second for fast action.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the software parameters, external factors can also greatly affect the
    frame rate. Examples include the camera parameters, the lens parameters, and the
    scene's brightness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another recipe. Whereas our default recipe accentuates motion at
    edges that have high grayscale contrast, this next recipe accentuates motion in
    all areas (edge or non-edge) that have either high color or grayscale contrast.
    By considering three color channels instead of one grayscale channel, we are tripling
    the amount of data that is processed by the FFT and IFFT. To offset this change,
    we need to cut each dimension of the capture resolution to half of its default
    value, thus reducing the amount of data to *1/2 * 1/2 = 1/4* times the default
    amount. As a net change, the FFT and IFFT process *3 * 1/4 = 3/4* times the default
    amount of data, a small decrease. The following initialization statement shows
    our new recipe''s parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are still using the default values for most parameters. If you
    found non-default values that work well for `numFFTThreads` and `numIFFTThreads`
    on your machine, enter them as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshots show the effects of our new recipe. Let''s look at
    a non-extreme example first. I was typing on my laptop when this was taken. Note
    the halos around my arms, which move a lot when I type, and a slight distortion
    and discoloration of my left cheek (your left in this mirrored image). My left
    cheek twitches a little when I think. Apparently, it is a tic already known to
    my friends and family, but rediscovered by me with the help of computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a624d19-c9ea-4e30-bb24-4641e5378363.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are viewing the color version of this image in the e-book, you should
    see that the halos around my arms take a green hue from the shirt and a red hue
    from the sofa. Similarly, the halos on my cheek take a magenta hue from my skin
    and a brown hue from my hair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider a more fanciful example. If we were Jedi instead of secret
    agents, we might wave a steel ruler in the air and pretend it was a lightsaber.
    While testing the theory that Lazy Eyes could make the ruler *look like a real
    lightsaber*, I took the following screenshot. This screenshot shows two pairs
    of light and dark lines in two places where I was waving the lightsaber ruler.
    One of the pairs of lines passes through each of my shoulders. The Light Side
    (the light line) and the Dark Side (the dark line) show opposite ends of the ruler''s
    path as it moved. The lines are especially clear in the color version in the e-book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd02af24-f872-40ae-b53f-79cd91a93532.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the moment for which we have all been waiting—a recipe for amplifying
    a heartbeat! If you have a heart rate monitor, start by measuring your heart rate.
    Mine is approximately 87 **beats per minute** (**bpm**) as I type these words
    and listen to inspiring ballads by Canadian folk singer Stan Rogers. To convert
    bpm to Hz, divide the bpm value by 60 (the number of seconds per minute), which
    gives (87 / 60) Hz = 1.45 Hz in my case. The most visible effect of a heartbeat
    is that a person''s skin changes color, becoming more red or purple when blood
    is pumped through an area. Thus, let''s modify our second recipe, which is able
    to amplify color motions in non-edge areas. Choosing a frequency range centered
    on 1.45 Hz, we have the following initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Customize `minHz` and `maxHz` based on your own heart rate. Remember to also
    specify `numFFTThreads` and `numIFFTThreads` if non-default values work best for
    you on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even when amplified, a heartbeat is difficult to show in still images; it is
    much clearer in the live video when running the app. However, take a look at the
    following pair of screenshots. My skin in the left-hand screenshot is more yellow
    (and lighter), whereas in the right-hand screenshot it is more purple (and darker).
    For comparison, note that there is no change in the cream-colored curtains in
    the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/471e21b7-e4d8-41a7-bbee-6a29e4e94f42.png)'
  prefs: []
  type: TYPE_IMG
- en: Three recipes are a good start, and they're certainly enough to fill a cooking
    TV show. So, why not go and observe some other motions in your environment, try
    to estimate their frequencies, and then configure `Lazy Eyes` to amplify them.
    How do they look with grayscale amplification versus color amplification? Edge
    (Laplacian) versus area (Gaussian)? What about when different history lengths,
    pyramid levels, and amplification multipliers are used?
  prefs: []
  type: TYPE_NORMAL
- en: Check this book's GitHub repository, [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition),
    for additional recipes, and feel free to share your own by mailing me at [josephhowse@nummist.com](mailto:josephhowse@nummist.com).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has introduced the relationship between computer vision and digital
    signal processing. We have considered a video feed as a collection of many signals—one
    for each channel value of each pixel—and we have learned that repetitive motions
    create wave patterns in some of these signals. We have used the fast Fourier transform
    and its inverse to create an alternative video stream that only sees certain frequencies
    of motion. Finally, we have superimposed this filtered video atop the original
    to amplify the selected frequencies of motion. There, we summarized Eulerian video
    magnification in 100 words!
  prefs: []
  type: TYPE_NORMAL
- en: Our implementation adapts Eulerian video magnification to real-time by running
    the FFT repeatedly on a sliding window of recently captured frames, rather than
    running it once on an entire prerecorded video. We have considered optimizations
    such as limiting our signal processing to grayscale, recycling large data structures
    rather than recreating them, and using several threads.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing things in different light
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we began this chapter by presenting Eulerian video magnification as
    a useful technique for visible light, it is also applicable to other kinds of
    light or radiation. For example, a person's blood beneath the skin (in veins and
    bruises) is more visible when imaged in **ultraviolet** (**UV**) or in **near
    infrared** (**NIR**) than in visible light. This is because blood is darker in
    UV light than in visible light, and skin is more transparent in NIR light than
    in visible light. Thus, a UV or NIR video might be an even better input when trying
    to magnify a person's pulse.
  prefs: []
  type: TYPE_NORMAL
- en: We will experiment with invisible light in the next chapter, [Chapter 8](5d2f960a-10ed-4efe-a195-47843cdf608b.xhtml),
    *Stopping Time and Seeing like a Bee*. Q's gadgets will inspire us once again!
  prefs: []
  type: TYPE_NORMAL
