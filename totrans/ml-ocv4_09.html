<html><head></head><body><div><h1 class="header-title">Implementing a Spam Filter with Bayesian Learning</h1>
                
            
            
                
<p>Before we get to grips with advanced topics, such as cluster analysis, deep learning, and ensemble models, let's turn our attention to a much simpler model that we have overlooked so far: the Naive Bayes classifier.</p>
<p>Naive Bayes classifiers have their roots in Bayesian inference, named after the famed statistician and philosopher Thomas Bayes (1701-1761). Bayes' theorem famously describes the probability of an event based on prior knowledge of conditions that might lead to the event. We can use Bayes' theorem to build a statistical model that not only can classify data but can also provide us with an estimate of how likely it is that our classification is correct. In our case, we can use Bayesian inference to dismiss an email as spam with high confidence and to determine the probability of a woman having breast cancer, given a positive screening test.</p>
<p>We have now gained enough experience with the mechanics of implementing machine learning methods, and so we should no longer be afraid to try and understand the theory behind them. Don't worry, we won't write a book on it, but we need some understanding of the theory to appreciate a model's inner workings. After that, I am sure you will find that Bayesian classifiers are easy to implement, are computationally efficient, and tend to perform quite well on relatively small datasets. In this chapter, we will understand the Naive Bayes classifier and then implement our first Bayesian classifier. We will then classify emails using the Naive Bayes classifier.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding the Naive Bayes classifier</li>
<li>Implementing your first Bayesian classifier</li>
<li>Classifying emails using the Naive Bayes classifier</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter07">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter07</a>.</p>
<p>Here is a summary of the software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python version 3.x will be fine).</li>
<li>You will need Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, and Linux-based OSes—with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided along with this book.</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding Bayesian inference</h1>
                
            
            
                
<p>Although Bayesian classifiers are relatively simple to implement, the theory behind them can be quite counter-intuitive at first, especially if you are not too familiar with probability theory yet. However, the beauty of Bayesian classifiers is that they understand the underlying data better than all of the classifiers we have encountered so far. For example, standard classifiers, such as the <em>k</em>-nearest neighbor algorithm or decision trees, might be able to tell us the target label of a never-before-seen data point. However, these algorithms have no concept of how likely it is for their predictions to be right or wrong. We call them discriminative models. Bayesian models, on the other hand, have an understanding of the underlying probability distribution that caused the data. We call them generative models because they don't just put labels on existing data points—they can also generate new data points with the same statistics.</p>
<p>If this last paragraph was a bit over your head, you might enjoy the following brief on probability theory. It will be important for the upcoming sections.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Taking a short detour through probability theory</h1>
                
            
            
                
<p>In order to appreciate Bayes' theorem, we need to get a hold of the following technical terms:</p>
<ul>
<li><strong>Random variable</strong>: This is a variable whose value depends on chance. A good example is the act of flipping a coin, which might turn up heads or tails. If a random variable can take on only a limited number of values, we call it discrete (such as a coin flip or a dice roll); otherwise, we call it a continuous random variable (such as the temperature on a given day). Random variables are often typeset as capital letters.</li>
<li><strong>Probability</strong>: This is a measure of how likely it is for an event to occur. We denote the probability of an event, <em>e</em>, happening as <em>p(e)</em>, which must be a number between 0 and 1 (or between ...</li></ul></div>



  
<div><h1 class="header-title">Understanding Bayes' theorem</h1>
                
            
            
                
<p>There are quite a few scenarios where it would be really good to know how likely it is for our classifier to make a mistake. For example, in <a href="5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml" target="_blank">Chapter 5</a>, <em>Using Decision Trees to Make a Medical Diagnosis</em>, we trained a decision tree to diagnose women with breast cancer based on some medical tests. You can imagine that, in this case, we would want to avoid a misdiagnosis at all costs; diagnosing a healthy woman with breast cancer (a false positive) would be both soul-crushing and lead to unnecessary, expensive medical procedures, whereas missing a woman's breast cancer (a false negative) might eventually cost the woman her life.</p>
<p>It's good to know we have Bayesian models to count on. Let's walk through a specific (and quite famous) example from <a href="http://yudkowsky.net/rational/bayes" target="_blank">http://yudkowsky.net/rational/bayes</a>:</p>
<div><p>"1% of women at age forty who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammographies. 9.6% of women without breast cancer will also get positive mammographies. A woman in this age group had a positive mammography in a routine screening. What is the probability that she actually has breast cancer?"</p>
</div>
<p>What do you think the answer is?</p>
<p>Well, given that her mammography was positive, you might reason that the probability of her having cancer is quite high (somewhere near 80%). It seems much less likely that the woman would belong to 9.6% with false positives, so the real probability is probably somewhere between 70% and 80%.</p>
<p>I'm afraid that's not correct.</p>
<p>Here's one way to think about this problem. For the sake of simplicity, let's assume we're looking at some concrete number of patients, say 10,000. Before the mammography screening, the 10,000 women can be divided into two groups:</p>
<ul>
<li><strong>Group X</strong>: 100 women <em>with</em> breast cancer</li>
<li><strong>Group Y</strong>: 9,900 women <em>without</em> breast cancer</li>
</ul>
<p>So far, so good. If we sum up the numbers in the two groups, we get a total of 10,000 patients, confirming that nobody has been lost in the math. After the mammography screening, we can divide the 10,000 women into four groups:</p>
<ul>
<li><strong>Group 1</strong>: 80 women with breast cancer and a positive mammography</li>
<li><strong>Group 2</strong>: 20 women with breast cancer and a negative mammography</li>
<li><strong>Group 3</strong>: Around 950 women without breast cancer and with a positive mammography</li>
<li><strong>Group 4</strong>: Approx. 8,950 women without breast cancer and with a negative mammography</li>
</ul>
<p>From the preceding analysis, you can see that the sum of all the four groups is 10,000. The sum of <strong>Group 1</strong> and <strong>Group 2</strong> (with breast cancer) corresponds to <strong>Group</strong> <strong>X</strong>, and the sum of <strong>Group 3</strong> and <strong>Group 4</strong> (without breast cancer) corresponds to <strong>Group Y</strong>.</p>
<p>This might become clearer when we draw it out:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-845 image-border" src="img/bbf90aac-09df-48e2-8fd2-5be6460eccbb.png" style="width:29.17em;height:29.17em;" width="1853" height="1853"/></p>
<p>In this diagram, the top half corresponds to <strong>Group X</strong>, and the bottom half corresponds to <strong>Group Y</strong>. Analogously, the left half corresponds to all women with positive mammographies, and the right half corresponds to all women with negative mammographies.</p>
<p>Now, it is easier to see that what we are looking for concerns only the left half of the diagram. The proportion of cancer patients with positive results within the group of all patients with positive results is the proportion of Group 1 within Groups 1 and 3:</p>
<div><em>80 / (80 + 950) = 80 / 1,030 = 7.8%</em></div>
<p>In other words, if you offer a mammography to 10,000 patients, then out of the 1,030 with a positive mammographies, there would be 80 patients with positive mammography having cancer. The answer a doctor should give a positive mammography patient if she asks about the chance she has breast cancer: roughly 1 out of 13 will have cancer, given that 13 patients ask this question.</p>
<p>What we just calculated is called a <strong>conditional probability</strong>: what is our <strong>degree of belief</strong> that a woman has breast cancer <strong>under the condition</strong> of (we also say <strong>given</strong>) a positive mammography? As in the last subsection, we denote this with <em>p(cancer|mammography)</em>, or <em>p(C|M)</em> for short. Using capital letters once again enforces the idea that both health and the mammography can have several outcomes, depending on several underlying (and possibly unknown) causes. Therefore, they are random variables.</p>
<p>Then, we can express <em>P(C|M)</em> with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b561a41b-448a-49cc-8be0-3772a3e0f27f.png" style="width:14.50em;height:2.58em;" width="2690" height="480"/></p>
<p>Here, <em>p(C, M)</em> denotes the probability that both <em>C</em> and <em>M</em> are true (meaning the probability that a woman both has cancer and a positive mammography). This is equivalent to the probability of a woman belonging to Group 1, as shown earlier.</p>
<p>The comma (<em>,</em>) means logical <em>and</em>, and the tilde (<em>~</em>) stands for logical <em>not</em>. Hence, <em>p(~C, M)</em> denotes the probability that <em>C</em> is not true and <em>M</em> is true (meaning the probability that a woman does not have cancer but has a positive mammography). This is equivalent to the probability of a woman belonging to Group 3. So, the denominator basically adds up women in Group 1 (<em>p(C, M)</em>) and Group 3 (<em>p(~C, M)</em>).</p>
<p>But wait! Those two groups together simply denote the probability of a woman having a positive mammography, <em>p(M)</em>. Therefore, we can simplify the preceding equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/55a3d8a8-bb02-4a6f-888c-8f322fab1880.png" style="width:8.67em;height:2.58em;" width="1610" height="480"/></p>
<p>The Bayesian version is to re-interpret what <em>p(C, M)</em> means. We can express <em>p(C, M)</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/98a72194-6ede-4115-af51-7446f71a6a0e.png" style="width:10.25em;height:1.17em;" width="1920" height="220"/></p>
<p>Now it gets a bit confusing. Here, <em>p(C)</em> is simply the probability that a woman has cancer (corresponding to the aforementioned Group X). Given that a woman has cancer, what is the probability that her mammography will be positive? From the problem question, we know it is 80%. This is <em>p(M|C)</em>, the probability of <em>M</em> given <em>C</em>.</p>
<p>Replacing <em>p(C, M)</em> in the first equation with this new formula, we get the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/fc3c16dd-2798-44e8-976d-6a474464d8ac.png" style="width:10.50em;height:2.58em;" width="1950" height="480"/></p>
<p>In the Bayesian world, these terms all have their specific names:</p>
<ul>
<li><em>p(C|M)</em> is called the <strong>posterior</strong>, which is always the thing we want to compute. In our example, this corresponds to the degree of belief that a woman has breast cancer, given a positive mammography.</li>
<li><em>p(C)</em> is called the <strong>prior</strong> as it corresponds to our initial knowledge about how common breast cancer is. We also call this our initial degree of belief in <em>C</em>.</li>
<li><em>p(M|C)</em> is called the <strong>likelihood</strong>.</li>
<li><em>p(M)</em> is called <strong>evidence</strong>.</li>
</ul>
<p>So, you can rewrite the equation one more time, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/bf3d09c1-5036-4405-b41b-2fab3f42f49e.png" style="width:14.83em;height:2.42em;" width="2520" height="410"/></p>
<p>Mostly, there is interest only in the numerator of that fraction because the denominator does not depend on <em>C,</em> so the denominator is constant and can be neglected.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding the Naive Bayes classifier</h1>
                
            
            
                
<p>So far, we have only talked about one piece of evidence. However, in most real-world scenarios, we have to predict an outcome (such as a random variable, <em>Y</em>) given multiple pieces of evidence (such as random variables <em>X<sub>1</sub></em> and <em>X<sub>2</sub></em>). So, instead of calculating <em>p(Y|X),</em> we would often have to calculate <em>p(Y|X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>)</em>. Unfortunately, this makes the math very complicated. For two random variables, <em>X<sub>1</sub></em> and <em>X<sub>2</sub></em>, the joint probability would be computed like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5e0699f8-d90a-44d6-930a-678af88be987.png" style="width:22.67em;height:1.50em;" width="3320" height="220"/></p>
<p>The ugly part is the term <em>p(X<sub>1</sub>|X<sub>2</sub>, C)</em>, which says that the conditional probability of <em>X<sub>1</sub></em> depends on all other variables, including <em>C</em>. This gets even ...</p></div>



  
<div><h1 class="header-title">Implementing your first Bayesian classifier</h1>
                
            
            
                
<p>But enough with the math, let's do some coding!</p>
<p>In the previous chapter, we learned how to generate a number of Gaussian blobs using scikit-learn. Do you remember how that is done?</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Creating a toy dataset</h1>
                
            
            
                
<p>The function I'm referring to resides within scikit-learn's <kbd>datasets</kbd> module. Let's create 100 data points, each belonging to one of two possible classes, and group them into two Gaussian blobs. To make the experiment reproducible, we specify an integer to pick a seed for <kbd>random_state</kbd>. You can again pick whatever number you prefer. Here, I went with Thomas Bayes' year of birth (just for kicks):</p>
<pre>In [1]: from sklearn import datasets...     X, y = datasets.make_blobs(100, 2, centers=2,        random_state=1701, cluster_std=2)</pre>
<p>Let's have a look at the dataset we just created using our trusty friend, Matplotlib:</p>
<pre>In [2]: import matplotlib.pyplot as plt...     plt.style.use('ggplot')...     %matplotlib inlineIn [3]: plt.scatter(X[:, 0], X[:, ...</pre></div>



  
<div><h1 class="header-title">Classifying the data with a normal Bayes classifier</h1>
                
            
            
                
<p>We will then use the same procedure as in earlier chapters to train a <strong>normal Bayes classifier</strong>. Wait, why not a Naive Bayes classifier? Well, it turns out OpenCV doesn't really provide a true Naive Bayes classifier. Instead, it comes with a Bayesian classifier that doesn't necessarily expect features to be independent, but rather expects the data to be clustered into Gaussian blobs. This is exactly the kind of dataset we created earlier!</p>
<p>By following these steps, you will learn how to build a classifier with a normal Bayes classifier:</p>
<ol>
<li>We can create a new classifier using the following function:</li>
</ol>
<pre style="padding-left: 60px">In [5]: import cv2<br/>...     model_norm = cv2.ml.NormalBayesClassifier_create()</pre>
<ol start="2">
<li>Then, training is done via the <kbd>train</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [6]: model_norm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)<br/>Out[6]: True</pre>
<ol start="3">
<li>Once the classifier has been trained successfully, it will return <kbd>True</kbd>. We go through the motions of predicting and scoring the classifier, just like we have done a million times before:</li>
</ol>
<pre style="padding-left: 60px">In [7]: _, y_pred = model_norm.predict(X_test)<br/>In [8]: from sklearn import metrics<br/>...     metrics.accuracy_score(y_test, y_pred)<br/>Out[8]: 1.0</pre>
<ol start="4">
<li>Even better—we can reuse the plotting function from the last chapter to inspect the decision boundary! If you recall, the idea was to create a mesh grid that would encompass all data points and then classify every point on the grid. The mesh grid is created via the NumPy function of the same name:</li>
</ol>
<pre style="padding-left: 60px">In [9]: def plot_decision_boundary(model, X_test, y_test):<br/>...         # create a mesh to plot in<br/>...         h = 0.02 # step size in mesh<br/>...         x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() +<br/>            1<br/>...         y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() +<br/>            1<br/>...         xx, yy = np.meshgrid(np.arange(x_min, x_max, h),<br/>...                              np.arange(y_min, y_max, h))</pre>
<ol start="5">
<li>The <kbd>meshgrid</kbd> function will return two floating-point matrices, <kbd>xx</kbd> and <kbd>yy</kbd>, that contain the <em>x</em> and <em>y</em> coordinates of every coordinate point on the grid. We can flatten these matrices into column vectors using the <kbd>ravel</kbd> function and stack them to form a new matrix, <kbd>X_hypo</kbd>:</li>
</ol>
<pre style="padding-left: 60px">...         X_hypo = np.column_stack((xx.ravel().astype(np.float32),<br/>...                                   yy.ravel().astype(np.float32)))</pre>
<ol start="6">
<li><kbd>X_hypo</kbd> now contains all <em>x</em> values in <kbd>X_hypo[:, 0]</kbd> and all <em>y</em> values in <kbd>X_hypo[:, 1]</kbd>. This is a format that the <kbd>predict</kbd> function can understand:</li>
</ol>
<pre style="padding-left: 60px">...         ret = model.predict(X_hypo)</pre>
<ol start="7">
<li>However, we want to be able to use models from both OpenCV and scikit-learn. The difference between the two is that OpenCV returns multiple variables (a Boolean flag indicating success/failure and the predicted target labels), whereas scikit-learn returns only the predicted target labels. Hence, we can check whether the <kbd>ret</kbd> output is a tuple, in which case, we know we're dealing with OpenCV. In this case, we store the second element of the tuple (<kbd>ret[1]</kbd>). Otherwise, we are dealing with scikit-learn and don't need to index into <kbd>ret</kbd>:</li>
</ol>
<pre style="padding-left: 60px">...         if isinstance(ret, tuple):<br/>...             zz = ret[1]<br/>...         else:<br/>...             zz = ret<br/>...         zz = zz.reshape(xx.shape)</pre>
<ol start="8">
<li>All that's left to do is to create a contour plot where <kbd>zz</kbd> indicates the color of every point on the grid. On top of that, we plot the data points using our trusty scatter plot:</li>
</ol>
<pre style="padding-left: 60px">...         plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8)<br/>...         plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=200)</pre>
<ol start="9">
<li>We call the function by passing a model (<kbd>model_norm</kbd>), a feature matrix (<kbd>X</kbd>), and a target label vector (<kbd>y</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [10]: plot_decision_boundary(model_norm, X, y)</pre>
<p>The output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-847 image-border" src="img/67d6aaae-c7d9-4b3f-ae04-df35a7fbd1cf.png" style="width:42.67em;height:26.58em;" width="858" height="534"/></p>
<p>So far, so good. The interesting part is that a Bayesian classifier also returns the probability with which each data point has been classified:</p>
<pre>In [11]: ret, y_pred, y_proba = model_norm.predictProb(X_test)</pre>
<p>The function returns a Boolean flag (<kbd>True</kbd> for success and <kbd>False</kbd> for failure), the predicted target labels (<kbd>y_pred</kbd>), and the conditional probabilities (<kbd>y_proba</kbd>). Here, <kbd>y_proba</kbd> is an <em>N x</em> 2 matrix that indicates, for every one of the <em>N</em> data points, the probability with which it was classified as either class 0 or class 1:</p>
<pre>In [12]: y_proba.round(2)<br/>Out[12]: array([[ 0.15000001,  0.05      ],<br/>                [ 0.08      ,  0.        ],<br/>                [ 0.        ,  0.27000001],<br/>                [ 0.        ,  0.13      ],<br/>                [ 0.        ,  0.        ],<br/>                [ 0.18000001,  1.88      ],<br/>                [ 0.        ,  0.        ],<br/>                [ 0.        ,  1.88      ],<br/>                [ 0.        ,  0.        ],<br/>                [ 0.        ,  0.        ]], dtype=float32)</pre>
<p>This means that, for the first data point (top row), the probability of it belonging to class 0 (that is, <em>p(C<sub>0</sub>|X)</em>) is 0.15 (or 15%)). Similarly, the probability of belonging to class 1 is <em>p(C<sub>1</sub>|X)</em> = <em>0.05.</em></p>
<p>The reason why some of the rows show values greater than 1 is that OpenCV does not really return probability values. Probability values are always between 0 and 1, and each row in the preceding matrix should add up to 1. Instead, what is being reported is a <strong>likelihood</strong>, which is basically the numerator of the conditional probability equation, <em>p(C)</em> <em>p(M|C)</em>. The denominator, <em>p</em>(<em>M</em>), does not need to be computed. All we need to know is that <em>0.15 &gt; 0.05</em> (top row). Hence, the data point most likely belongs to class 0.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Classifying the data with a Naive Bayes classifier</h1>
                
            
            
                
<p>The following steps will help you build a Naive Bayes classifier:</p>
<ol>
<li>We can compare the result to a true Naive Bayes classifier by asking scikit-learn for help:</li>
</ol>
<pre style="padding-left: 60px">In [13]: from sklearn import naive_bayes...      model_naive = naive_bayes.GaussianNB()</pre>
<ol start="2">
<li>As usual, training the classifier is done via the <kbd>fit</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [14]: model_naive.fit(X_train, y_train)Out[14]: GaussianNB(priors=None)</pre>
<ol start="3">
<li>Scoring the classifier is built in:</li>
</ol>
<pre style="padding-left: 60px">In [15]: model_naive.score(X_test, y_test)Out[15]: 1.0</pre>
<ol start="4">
<li>Again a perfect score! However, in contrast to OpenCV, this classifier's <kbd>predict_proba</kbd> method returns true probability values, because all values are between 0 and 1 and because all rows add up to 1:</li>
</ol>
<pre style="padding-left: 60px">In [16]: yprob = model_naive.predict_proba(X_test) ...</pre></div>



  
<div><h1 class="header-title">Visualizing conditional probabilities</h1>
                
            
            
                
<p>By referring to the following steps, you will be able to visualize conditional probabilities:</p>
<ol>
<li>For this, we will slightly modify the plot function from the previous example. We start out by creating a mesh grid between (<kbd>x_min</kbd>, <kbd>x_max</kbd>) and (<kbd>y_min</kbd>, <kbd>y_max</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [18]: def plot_proba(model, X_test, y_test):<br/>...          # create a mesh to plot in<br/>...          h = 0.02 # step size in mesh<br/>...          x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1<br/>...          y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1<br/>...          xx, yy = np.meshgrid(np.arange(x_min, x_max, h),<br/>...                               np.arange(y_min, y_max, h))</pre>
<ol start="2">
<li>Then, we flatten <kbd>xx</kbd> and <kbd>yy</kbd> and add them column-wise to the feature matrix, <kbd>X_hypo</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><br/><strong>...          X_hypo = np.column_stack((xx.ravel().astype(np.float32),</strong><br/><strong>...                                    yy.ravel().astype(np.float32)))</strong></pre>
<ol start="3">
<li>If we want to make this function work with both OpenCV and scikit-learn, we need to implement a switch for <kbd>predictProb</kbd> (in the case of OpenCV) and <kbd>predict_proba</kbd> (in the case of scikit-learn). For this, we check whether <kbd>model</kbd> has a method called <kbd>predictProb</kbd>. If the method exists, we can call it; otherwise, we assume we're dealing with a model from scikit-learn:</li>
</ol>
<pre style="padding-left: 60px">...          if hasattr(model, 'predictProb'):<br/>...             _, _, y_proba = model.predictProb(X_hypo)<br/>...          else:<br/>...             y_proba = model.predict_proba(X_hypo)</pre>
<ol start="4">
<li>Like in <kbd>In [16]</kbd>, which we saw earlier, <kbd>y_proba</kbd> will be a 2D matrix containing, for each data point, the probability of the data belonging to class 0 (in <kbd>y_proba[:, 0]</kbd>) and to class 1 (in <kbd>y_proba[:, 1]</kbd>). An easy way to convert these two values into a color that the contour function can understand is to simply take the difference of the two probability values:</li>
</ol>
<pre style="padding-left: 60px">...          zz = y_proba[:, 1] - y_proba[:, 0]<br/>...          zz = zz.reshape(xx.shape)</pre>
<ol start="5">
<li>The last step is to plot <kbd>X_test</kbd> as a scatter plot on top of the colored mesh grid:</li>
</ol>
<pre style="padding-left: 60px">... plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8)<br/>... plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=200)</pre>
<ol start="6">
<li>Now, we are ready to call the function:</li>
</ol>
<pre style="padding-left: 60px">In [19]: plot_proba(model_naive, X, y)</pre>
<p>The result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-849 image-border" src="img/9cb91905-247c-46b9-a208-fadaf2df1d9d.png" style="width:41.67em;height:25.58em;" width="853" height="524"/></p>
<p>The preceding screenshot shows conditional probabilities of a Naive Bayes classifier.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Classifying emails using the Naive Bayes classifier</h1>
                
            
            
                
<p>The final task of this chapter will be to apply our newly gained skills to a real spam filter! This task deals with solving a binary-class (spam/ham) classification problem using the Naive Bayes algorithm.</p>
<p>Naive Bayes classifiers are actually a very popular model for email filtering. Their naivety lends itself nicely to the analysis of text data, where each feature is a word (or a <strong>bag of words</strong>), and it would not be feasible to model the dependence of every word on every other word.</p>
<p>There are a bunch of good email datasets out there, such as the following:</p>
<ul>
<li>The Hewlett-Packard spam database: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/">https://archive.ics.uci.edu/ml/machine-learning-databases/spambase</a></li>
<li>The Enrom-Spam dataset: <a href="http://www.aueb.gr/users/ion/data/enron-spam">http://www.aueb.gr/users/ion/data/enron-spam ...</a></li></ul></div>



  
<div><h1 class="header-title">Loading the dataset</h1>
                
            
            
                
<p>You can refer to these steps to load the dataset:</p>
<ol>
<li>If you downloaded the latest code from GitHub, you will find several <kbd>.zip</kbd> files in the <kbd>notebooks/data/chapter7</kbd> directory. These files contain raw email data (with fields for To:, Cc:, and text body) that are either classified as spam (with the <kbd>SPAM = 1</kbd> class label) or not (also known as ham, the <kbd>HAM = 0</kbd> class label).</li>
<li>We build a variable called <kbd>sources</kbd>, which contains all of the raw data files:</li>
</ol>
<pre style="padding-left: 60px">In [1]: HAM = 0<br/>...     SPAM = 1<br/>...     datadir = 'data/chapter7'<br/>...     sources = [<br/>...        ('beck-s.tar.gz', HAM),<br/>...        ('farmer-d.tar.gz', HAM),<br/>...        ('kaminski-v.tar.gz', HAM),<br/>...        ('kitchen-l.tar.gz', HAM),<br/>...        ('lokay-m.tar.gz', HAM),<br/>...        ('williams-w3.tar.gz', HAM),<br/>...        ('BG.tar.gz', SPAM),<br/>...        ('GP.tar.gz', SPAM),<br/>...        ('SH.tar.gz', SPAM)<br/>...     ]</pre>
<ol start="3">
<li>The first step is to extract these files into subdirectories. For this, we can use the <kbd>extract_tar</kbd> function we wrote in the previous chapter:</li>
</ol>
<pre style="padding-left: 60px">In [2]: def extract_tar(datafile, extractdir):<br/>...         try:<br/>...             import tarfile<br/>...         except ImportError:<br/>...             raise ImportError("You do not have tarfile installed. "<br/>...                               "Try unzipping the file outside of "<br/>...                               "Python.")<br/>...         tar = tarfile.open(datafile)<br/>...         tar.extractall(path=extractdir)<br/>...         tar.close()<br/>...         print("%s successfully extracted to %s" % (datafile,<br/>...                                                    extractdir))</pre>
<ol start="4">
<li>To apply the function to all data files in the sources, we need to run a loop. The <kbd>extract_tar</kbd> function expects a path to the <kbd>.tar.gz</kbd> file—which we build from <kbd>datadir</kbd> and an entry in <kbd>sources</kbd>—and a directory to extract the files to (<kbd>datadir</kbd>). This will extract all emails in, for example, <kbd>data/chapter7/beck-s.tar.gz</kbd> to the <kbd>data/chapter7/beck-s/</kbd> subdirectory:</li>
</ol>
<pre style="padding-left: 60px">In [3]: for source, _ in sources:<br/>...         datafile = '%s/%s' % (datadir, source)<br/>...         extract_tar(datafile, datadir)<br/>Out[3]: data/chapter7/beck-s.tar.gz successfully extracted to data/chapter7<br/>        data/chapter7/farmer-d.tar.gz successfully extracted to<br/>            data/chapter7<br/>        data/chapter7/kaminski-v.tar.gz successfully extracted to<br/>            data/chapter7<br/>        data/chapter7/kitchen-l.tar.gz successfully extracted to<br/>            data/chapter7<br/>        data/chapter7/lokay-m.tar.gz successfully extracted to<br/>            data/chapter7<br/>        data/chapter7/williams-w3.tar.gz successfully extracted to<br/>            data/chapter7<br/>        data/chapter7/BG.tar.gz successfully extracted to data/chapter7<br/>        data/chapter7/GP.tar.gz successfully extracted to data/chapter7<br/>        data/chapter7/SH.tar.gz successfully extracted to data/chapter7</pre>
<p>Now here's the tricky bit. Every one of these subdirectories contains many other directories, wherein the text files reside. So, we need to write two functions:</p>
<ul>
<li><kbd>read_single_file(filename)</kbd>: This is a function that extracts the relevant content from a single file called <kbd>filename</kbd>.</li>
<li><kbd>read_files(path)</kbd>: This is a function that extracts the relevant content from all files in a particular directory called <kbd>path</kbd>.</li>
</ul>
<p>To extract the relevant content from a single file, we need to be aware of how each file is structured. The only thing we know is that the header section of the email (From:, To:, and Cc:) and the main body of text are separated by a newline character, <kbd>'\n'</kbd>. So, what we can do is iterate over every line in the text file and keep only those lines that belong to the main text body, which will be stored in the variable lines. We also want to keep a Boolean flag, <kbd>past_header</kbd>, around, which is initially set to <kbd>False</kbd> but will be flipped to <kbd>True</kbd> once we are past the header section:</p>
<ol>
<li>We start by initializing those two variables:</li>
</ol>
<pre style="padding-left: 60px">In [4]: import os<br/>...     def read_single_file(filename):<br/>...         past_header, lines = False, []</pre>
<ol start="2">
<li>Then, we check whether a file with the name <kbd>filename</kbd> exists. If it does, we start looping over it line by line:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         if os.path.isfile(filename):</strong><br/><strong>...             f = open(filename, encoding="latin-1")</strong><br/><strong>...             for line in f:</strong></pre>
<p style="padding-left: 90px">You may have noticed the <kbd>encoding="latin-1"</kbd> part. Since some of the emails are not in Unicode, this is an attempt to decode the files correctly.</p>
<p style="padding-left: 90px">We do not want to keep the header information, so we keep looping until we encounter the <kbd>'\n'</kbd> character, at which point we flip <kbd>past_header</kbd> from <kbd>False</kbd> to <kbd>True</kbd>.</p>
<ol start="3">
<li>At this point, the first condition of the following <kbd>if-else</kbd> clause is met, and we append all remaining lines in the text file to the <kbd>lines</kbd> variable:</li>
</ol>
<pre style="padding-left: 60px">...                 if past_header:<br/>...                     lines.append(line)<br/>...                 elif line == '\n':<br/>...                     past_header = True<br/>...             f.close()</pre>
<ol start="4">
<li>In the end, we concatenate all lines into a single string, separated by the newline character, and return both the full path to the file and the actual content of the file:</li>
</ol>
<pre style="padding-left: 60px">...         content = '\n'.join(lines)<br/>...         return filename, content</pre>
<ol start="5">
<li>The job of the second function will be to loop over all files in a folder and call <kbd>read_single_file</kbd> on them:</li>
</ol>
<pre style="padding-left: 60px">In [5]: def read_files(path):<br/>...         for root, dirnames, filenames in os.walk(path):<br/>...             for filename in filenames:<br/>...                 filepath = os.path.join(root, filename)<br/>...                 yield read_single_file(filepath)</pre>
<p>Here, <kbd>yield</kbd> is a keyword that is similar to <kbd>return</kbd>. The difference is that <kbd>yield</kbd> returns a generator instead of the actual values, which is desirable if you expect to have a large number of items to iterate over.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Building a data matrix using pandas</h1>
                
            
            
                
<p>Now, it's time to introduce another essential data science tool that comes preinstalled with Python Anaconda: <strong>pandas</strong>. pandas is built on NumPy and provides several useful tools and methods to deal with data structures in Python. Just as we generally import NumPy under the alias, <kbd>np</kbd>, it is common to import pandas under the <kbd>pd</kbd> alias:</p>
<pre>In [6]: import pandas as pd</pre>
<p>pandas provide a useful data structure called a DataFrame, which can be understood as a generalization of a 2D NumPy array, as shown here:</p>
<pre>In [7]: pd.DataFrame({...         'model': [...             'Normal Bayes',...             'Multinomial Bayes',...             'Bernoulli Bayes'...         ],...         'class': [...             'cv2.ml.NormalBayesClassifier_create()',...             'sklearn.naive_bayes.MultinomialNB()',... 'sklearn.naive_bayes.BernoulliNB()' ...</pre></div>



  
<div><h1 class="header-title">Preprocessing the data</h1>
                
            
            
                
<p>Scikit-learn offers several options when it comes to encoding text features, which we discussed in <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>. One of the simplest methods of encoding text data, as you may recall, is by <strong>word count</strong>; for each phrase, you count the number of occurrences of each word within it. In scikit-learn, this is easily done using <kbd>CountVectorizer</kbd>:</p>
<pre>In [10]: from sklearn import feature_extraction<br/>...      counts = feature_extraction.text.CountVectorizer()<br/>...      X = counts.fit_transform(data['text'].values)<br/>...      X.shape<br/>Out[10]: (52076, 643270)</pre>
<p>The result is a giant matrix, which tells us that we harvested a total of 52,076 emails that collectively contain 643,270 different words. However, scikit-learn is smart and saved the data in a sparse matrix:</p>
<pre>In [11]: X<br/>Out[11]: &lt;52076x643270 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>                 with 8607632 stored elements in Compressed Sparse Row <br/>                 format&gt;</pre>
<p>To build the vector of target labels (<kbd>y</kbd>), we need to access data in the pandas DataFrame. This can be done by treating the DataFrame like a dictionary, where the <kbd>values</kbd> attribute will give us access to the underlying NumPy array:</p>
<pre>In [12]: y = data['class'].values</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Training a normal Bayes classifier</h1>
                
            
            
                
<p>From here on out, things are (almost) like they always were. We can use scikit-learn to split the data into training and test sets (let's reserve 20% of all data points for testing):</p>
<pre>In [13]: from sklearn import model_selection as ms...      X_train, X_test, y_train, y_test = ms.train_test_split(...          X, y, test_size=0.2, random_state=42...      )</pre>
<p>We can instantiate a new normal Bayes classifier with OpenCV:</p>
<pre>In [14]: import cv2...      model_norm = cv2.ml.NormalBayesClassifier_create()</pre>
<p>However, OpenCV does not know about sparse matrices (at least its Python interface does not). If we were to pass <kbd>X_train</kbd> and <kbd>y_train</kbd> to the <kbd>train</kbd> function as we did earlier, OpenCV would complain that the data matrix is not a NumPy array. ...</p></div>



  
<div><h1 class="header-title">Training on the full dataset</h1>
                
            
            
                
<p>However, if you want to classify the full dataset, we need a more sophisticated approach. We turn to scikit-learn's Naive Bayes classifier, as it understands how to handle sparse matrices. In fact, if you didn't pay attention and treated <kbd>X_train</kbd> like every NumPy array before, you might not even notice that anything is different:</p>
<pre>In [17]: from sklearn import naive_bayes<br/>...      model_naive = naive_bayes.MultinomialNB()<br/>...      model_naive.fit(X_train, y_train)<br/>Out[17]: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</pre>
<p>Here, we used <kbd>MultinomialNB</kbd> from the <kbd>naive_bayes</kbd> module, which is the version of Naive Bayes classifier that is best suited to handling categorical data, such as word counts.</p>
<p>The classifier is trained almost instantly and returns the scores for both the training and test sets:</p>
<pre>In [18]: model_naive.score(X_train, y_train)<br/>Out[18]: 0.95086413826212191<br/>In [19]: model_naive.score(X_test, y_test)<br/>Out[19]: 0.94422043010752688</pre>
<p>And there we have it: 94.4% accuracy on the test set! Pretty good for not doing much other than using the default values, isn't it?</p>
<p>However, what if we were super critical of our own work and wanted to improve the result even further? There are a couple of things we could do.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using n-grams to improve the result</h1>
                
            
            
                
<p>One thing to do is to use <em>n</em>-gram counts instead of plain word counts. So far, we have relied on what is known as a bag of words: we simply threw every word of an email into a bag and counted the number of its occurrences. However, in real emails, the order in which words appear can carry a great deal of information!</p>
<p>This is exactly what <em>n</em>-gram counts are trying to convey. You can think of an <em>n</em>-gram as a phrase that is <em>n</em> words long. For example, the phrase <em>Statistics has its moments</em> contains the following 1-grams: <em>Statistics</em>, <em>has</em>, <em>its</em>, and <em>moments</em>. It also has the following 2-grams: <em>Statistics has</em>, <em>has its</em>, and <em>its moments</em>. It also has two 3-grams (<em>Statistics has its</em> and <em>has its moments</em>) and only a single ...</p></div>



  </body></html>