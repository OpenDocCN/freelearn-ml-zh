<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Case Studies</h1>
                </header>
            
            <article>
                
<p class="mce-root">This book has gone through several different feature engineering algorithms and we have worked with many different datasets. In this chapter, we will go through a few case studies to help you deepen your understanding of the topics we have covered in the book. We will work through two full-length case studies from beginning to end to further understand how feature engineering tasks can help us create machine learning pipelines for real-life applications. For each case study, we will go through:</p>
<ul>
<li>The application that we are working towards</li>
<li>The data in question that we are using</li>
<li>A brief exploratory data analysis</li>
<li>Setting up our machine learning pipelines and gathering metrics</li>
</ul>
<p>Moreover, we will be going through the following cases:</p>
<ul>
<li>Facial recognition</li>
<li>Predicting hotel reviews data</li>
</ul>
<p>Let's get started!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study 1 - facial recognition</h1>
                </header>
            
            <article>
                
<p>Our first case study will be to predict the labels for image data with a popular dataset called the <strong>Labeled Faces</strong> in the <kbd>Wild</kbd> dataset from the scikit-learn library. The dataset is called the <kbd>Olivetti Face</kbd> dataset and it comprises pictures of famous people's faces, with appropriate labels. Our task is that of <strong>facial recognition</strong>, a supervised machine learning model that is able to predict the name of the person given an image of their face.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of facial recognition</h1>
                </header>
            
            <article>
                
<p>Image processing and facial recognition are far-reaching. The ability to quickly discern people's faces from a crowd of people in video/images is vital for physical security as well as for giant social media companies. Search engines such as Google, with their image search capabilities, are using image recognition algorithms to match images and quantify similarities to a point where we can upload a photo of someone to get all other images of that same person.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The data</h1>
                </header>
            
            <article>
                
<p>Let's start with loading in our dataset and several other import statements we will be using to plot our data. It is good practice to begin a Jupyter notebook (iPython) with all the import statements you will be using. Obviously, you may get partway through your work and realize that you need to import a new package; also, to stay organized, it is a good idea to keep them in the beginning of your work.</p>
<p>The following code block includes the <kbd>import</kbd> statements we will be using for this case study. We will utilize each import in the example and it will become clear to you what each of them is used for as we work out our example:</p>
<pre><span class="kn"># the olivetti face dataset<br/>from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people<br/><br/></span># feature extraction modules
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis<br/><br/># feature scaling module<br/></span>from sklearn.preprocessing import StandardScaler <span class="n"><br/><br/></span># standard python modules<br/><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span> <br/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <br/><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt<br/></span><span class="o"><br/>%</span><span class="n">matplotlib</span> <span class="n">inline # this ensures that your plotting will show directly in your jupyter notebook<br/></span><span class="nn"><br/></span># scikit-learn model selection modules <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split, </span><span class="n">GridSearchCV, </span><span class="n">cross_val_score<br/><br/></span><span class="n"># metrics<br/></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span> <br/><br/># machine learning modules<br/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression<br/></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span> <span class="n"><br/></span></pre>
<p>Now, we can get started! We proceed as follows:</p>
<ol>
<li>First, let's load in our dataset and see what we are working with. We will use the<span> </span><kbd>fetch_flw_people</kbd><span> function built in with scikit-learn:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="n">lfw_people</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span></pre>
<p>As you can see, we have a couple of optional parameters that we've invoked, specifically, <kbd>min_faces_per_person</kbd><span> </span>and <kbd>resize</kbd>. The first parameter will only retain the pictures of people who are in the minimum number of different pictures that we specify. We have set this to be a minimum of <kbd>70</kbd> different pictures per person. The <kbd>resize</kbd> parameter <span class="c1">is the ratio used to resize each face picture.</span></p>
<ol start="2">
<li>Let's inspect the image arrays to find shapes for plotting the images. We can do this with the following code:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">n_samples</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w<br/><br/>(1288, 50, 37)</span><span class="n"> </span></pre>
<p>We see that we have <kbd>1288</kbd> samples (images) and each image has a height of <kbd>50</kbd> pixels and a width of <kbd>37</kbd> pixels.</p>
<ol start="3">
<li>Now, let's set up the <kbd>X</kbd> and <kbd>y</kbd> for our machine learning pipeline. We will grab the<span> </span><kbd>data</kbd> attribute of the<span> </span><kbd>lfw_people</kbd><span> </span>object:</li>
</ol>
<pre style="padding-left: 60px"><span class="c1"># for machine learning we use the data directly (as relative pixel </span><span class="c1">positions info is ignored by this model)<br/></span>
<span class="n">X</span> <span class="o">=</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">target</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]<br/><br/></span><span class="n">n_features<br/></span><span class="p">1850 </span></pre>
<p>The fact that<span> </span><kbd>n_features</kbd><span> ends up having</span> <kbd>1,850</kbd> <span>columns comes from the fact that: </span></p>
<p style="padding-left: 210px"><img height="19" src="assets/15bd7220-096c-4238-a4ec-57a8f079f069.png" width="132"/></p>
<p>We can now see the full shape of our data, as follows:</p>
<pre><span class="n">X</span><span class="o">.</span><span class="n">shape<br/><br/></span>(1288, 1850)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Some data exploration</h1>
                </header>
            
            <article>
                
<p>We have 1,288 rows by 1,850 columns. To do some brief exploratory analysis, we can plot one of the images by using this code:</p>
<pre><span class="c1"># plot one of the faces</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">lfw_people</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></pre>
<p>This will give us the following label:</p>
<pre class="mce-root">'Hugo Chavez'</pre>
<p>The image is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="255" src="assets/5c5a295c-e25a-4aba-a87b-eb10055ee563.png" width="200"/></div>
<p>Now, let's plot the same image after applying a scaling module, as follows:</p>
<pre><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">lfw_people</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></pre>
<p>Which gives us this output:</p>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>'Hugo Chavez'</pre>
<p>We get the following image for the preceding code:</p>
</div>
</div>
<div class="output_area CDPAlignCenter CDPAlign"><img height="225" src="assets/cab13f64-db92-4d96-85e1-c5a09eac1391.png" width="177"/></div>
<p>Here, you can see that the image is slightly different, with darker pixels around the face. Now, let's set up the label to predict:</p>
<pre><span class="c1"># the label to predict is the id of the person</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">lfw_people</span><span class="o">.</span><span class="n">target_names</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="n">target_names</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span> <span class="s2">"Total dataset size:"</span>
<span class="k">print</span> <span class="s2">"n_samples: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">n_samples</span>
<span class="k">print</span> <span class="s2">"n_features: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">n_features</span>
<span class="k">print</span> <span class="s2">"n_classes: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">n_classes</span></pre>
<p>This gives us the following output:</p>
<pre>Total dataset size:
n_samples: 1288
n_features: 1850
n_classes: 7</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applied facial recognition</h1>
                </header>
            
            <article>
                
<p>Now, we can move on to the machine learning pipelines that will be used to create our facial recognition models:</p>
<ol>
<li>We can start by creating <kbd>train</kbd>, <kbd>test</kbd>, and <kbd>split</kbd> in our dataset, as shown in the following code block:</li>
</ol>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre style="padding-left: 60px"><span class="c1"># let's split our dataset into training and testing</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
</div>
<ol start="2">
<li>We are ready to perform a <strong>Principal Component Analysis</strong> (<strong>PCA</strong>) on our dataset. We will want to instantiate a <kbd>PCA</kbd> first and ensure that we <kbd>scale</kbd> our data before applying PCA in our pipeline. This can be done as follows:</li>
</ol>
<pre style="padding-left: 60px"><span class="c1"># instantiate the PCA module</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># create a pipeline called preprocessing that will scale data and then apply PCA</span>
<span class="n">preprocessing</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">'scale'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span> <span class="p">(</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">pca</span><span class="p">)])</span></pre>
<ol start="3">
<li>Now, we can <kbd>fit</kbd> our pipeline:</li>
</ol>
<pre style="padding-left: 60px"><span class="k">print</span> <span class="s2">"Extracting the top </span><span class="si">%d</span><span class="s2"> eigenfaces from </span><span class="si">%d</span><span class="s2"> faces"</span> <span class="o">%</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># fit the pipeline to the training set</span>
<span class="n">preprocessing</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># grab the PCA from the pipeline</span>
<span class="n">extracted_pca</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span></pre>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<ol start="4">
<li>The output will be our print statement:</li>
</ol>
<pre style="padding-left: 60px">Extracting the top 200 eigenfaces from 966 faces</pre></div>
</div>
</div>
</div>
</div>
<ol start="5">
<li>Let's look at the scree plot:</li>
</ol>
<pre style="padding-left: 60px"><span class="c1"># Scree Plot</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">extracted_pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span></pre>
<p>We can see that <span class="c1">starting at 100 components captures over 90% of the variance, compared to the 1,850 original features.</span></p>
<ol start="6">
<li>We can create a function to plot our PCA components, like so:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">comp</span> <span class="o">=</span> <span class="n">extracted_pca</span><span class="o">.</span><span class="n">components_</span>
<span class="n">image_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">plot_gallery</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">n_col</span><span class="p">,</span> <span class="n">n_row</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">,</span> <span class="mf">2.26</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">vmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="o">-</span><span class="n">comp</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">,</span>            
                   <span class="n">vmin</span><span class="o">=-</span><span class="n">vmax</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    </pre>
<ol start="7">
<li>We can now call our <kbd>plot_gallery</kbd> function, like so:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">plot_gallery</span><span class="p">(</span><span class="s1">'PCA components'</span><span class="p">,</span> <span class="n">comp</span><span class="p">[:</span><span class="mi">16</span><span class="p">],</span> <span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span></pre>
<p>The output gives us these images:</p>
<div class="CDPAlignCenter CDPAlign"><img height="414" src="assets/cfc5f3aa-df26-487f-8d23-c0c237f96fe6.png" width="368"/></div>
<p>This lets us see our PCA components for a specific row and column! These <strong>eigen-faces</strong> are extracted features of humans that the PCA module is finding. Compare this to our result in <a href="e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml" target="_blank">Chapter 7</a>, <em>Feature Learning</em>, where we used PCA to extract <strong>eigen-digits</strong>. Each of these components is meant to house vital information about faces that can be used to distinguish between different people. For example:</p>
<ul>
<li>The eigen-face in the third row, fourth column seems to be highlighting the moustache and beard areas in order to quantify how much facial hair would help in separating out our classes</li>
<li><span>The eigen-face in the first row, fourth column seems to be showing a contrast between the background and the face, putting a number to the lighting situation of the image</span></li>
</ul>
<p>Of course, these are interpretations by us, and different eigen-faces for different face datasets will output different images/components. We will move on to create a function that will allow us to clearly <span class="sd">print a more readable confusion matrix with heat labels and options for normalization:</span></p>
<pre><span class="kn">import</span> <span class="nn">itertools</span>
<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                          <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s1">'Confusion matrix'</span><span class="p">,</span>
                          <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span>
                 <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s2">"white"</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s2">"black"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'True label'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predicted label'</span><span class="p">)</span></pre>
<p>Now, we can fit without using PCA to see the difference. We will invoke our <kbd>plot_confusion_matrix</kbd> function so that we can visualize the accuracy of our model:</p>
<pre><span class="c1"># fit without using PCA to see what the difference will be</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'C'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span><span class="mf">1e0</span><span class="p">,</span><span class="mf">1e1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">]}</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logreg</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">best_clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="c1"># Predicting people's names on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="s2">"Accuracy score for best estimator"</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>
<span class="k">print</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)),</span> <span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span> <span class="nb">round</span><span class="p">((</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">"seconds to grid search and predict the test set"</span></pre>
<p class="mce-root">The output is as follows:</p>
<div class="output_area">
<pre>0.813664596273 Accuracy score for best estimator
                   precision    recall  f1-score   support

     Ariel Sharon       0.72      0.68      0.70        19
     Colin Powell       0.85      0.71      0.77        55
  Donald Rumsfeld       0.62      0.72      0.67        25
    George W Bush       0.88      0.91      0.89       142
Gerhard Schroeder       0.79      0.84      0.81        31
      Hugo Chavez       0.87      0.81      0.84        16
       Tony Blair       0.71      0.71      0.71        34

      avg / total       0.82      0.81      0.81       322

None
39.9 seconds to grid search and predict the test set</pre>
<p class="output_subarea output_stream output_stdout output_text CDPAlignLeft CDPAlign">We get the plot as follows:</p>
</div>
<div class="output_area CDPAlignCenter CDPAlign"><img height="326" src="assets/f34a3ca8-41f8-436c-abc1-1316a76b1da3.png" width="382"/></div>
<p>Using <span>only</span><span> </span><span>raw pixels, our linear model was able to achieve </span><strong>81.3%</strong> accuracy<span>.</span><strong> </strong><span>This time, let's apply PCA to see what the difference will be. We will hardcode the number of components to extract to 200 for now:</span></p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="n">face_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">'PCA'</span><span class="p">,</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">200</span><span class="p">)),</span> <span class="p">(</span><span class="s1">'logistic'</span><span class="p">,</span> <span class="n">logreg</span><span class="p">)])</span>

<span class="n">pipe_param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'logistic__C'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span><span class="mf">1e0</span><span class="p">,</span><span class="mf">1e1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">]}</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">face_pipeline</span><span class="p">,</span> <span class="n">pipe_param_grid</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">best_clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="c1"># Predicting people's names on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="s2">"Accuracy score for best estimator"</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>
<span class="k">print</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)),</span> <span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span> <span class="nb">round</span><span class="p">((</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">"seconds to grid search and predict the test set"</span></pre></div>
</div>
</div>
</div>
<p>The output with PCA looks like this:</p>
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.739130434783 Accuracy score for best estimator
                   precision    recall  f1-score   support

     Ariel Sharon       0.67      0.63      0.65        19
     Colin Powell       0.69      0.60      0.64        55
  Donald Rumsfeld       0.74      0.68      0.71        25
    George W Bush       0.76      0.88      0.82       142
Gerhard Schroeder       0.77      0.77      0.77        31
      Hugo Chavez       0.62      0.62      0.62        16
       Tony Blair       0.77      0.50      0.61        34

      avg / total       0.74      0.74      0.73       322

None
74.5 seconds to grid search and predict the test set</pre></div>
</div>
<div class="output_area CDPAlignCenter CDPAlign">
<p class="prompt">We get the plot as follows:</p>
<img src="assets/1b072798-7cb0-4dcd-bfe7-c8da8d0daee0.png"/></div>
<p>Interesting! We can see that our accuracy went down to <strong>73.9%</strong> and our time to predict went up by applying PCA. We should not get discouraged, however; this likely means that we have not found the optimal number of components to use yet.</p>
<p>Let's plot a few of the predicted names versus the true names within our test set to see some of the errors/correct labels that our models are producing:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="399" src="assets/2c5e4b2b-e868-4921-8619-7696a6be742a.png" width="413"/></div>
<p>This is a great way to visualize our results when working with images.</p>
<p>Let's now implement a grid search to find the best model and accuracy for our data. First, we will create a function that will perform the grid search for us and print the accuracy, parameters, average time to fit, and average time to score neatly for us. This function is created like so:</p>
<pre><span class="k">def</span> <span class="nf">get_best_model_and_accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>           <span class="c1"># the model to grid search</span>
                        <span class="n">params</span><span class="p">,</span>          <span class="c1"># the parameter set to try </span>
                        <span class="n">error_score</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>  <span class="c1"># if a parameter set raises an error, continue and set the performance as a big, fat 0</span>
    <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>           <span class="c1"># fit the model and parameters</span>
    <span class="c1"># our classical metric for performance</span>
    <span class="k">print</span> <span class="s2">"Best Accuracy: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
    <span class="c1"># the best parameters that caused the best accuracy</span>
    <span class="k">print</span> <span class="s2">"Best Parameters: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="c1"># the average time it took a model to fit to the data (in seconds)</span>
    <span class="k">print</span> <span class="s2">"Average Time to Fit (s): {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_fit_time'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="c1"># the average time it took a model to predict out of sample data (in seconds)</span>
    <span class="c1"># this metric gives us insight into how this model will perform in real-time analysis</span>
    <span class="k">print</span> <span class="s2">"Average Time to Score (s): {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_score_time'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="mi">3</span><span class="p">))</span></pre>
<p>Now, we can create a larger grid search pipeline that includes many more components, namely:</p>
<ul>
<li>A scaling module</li>
<li>A PCA module to extract the best features that capture the variance in the data</li>
<li>A <strong>Linear Discriminat Analysis</strong> (<strong>LDA</strong>) module to create features that best separate the faces from one another</li>
<li>Our linear classifier, which will reap the benefits of our three feature engineering modules and attempt to distinguish between our faces</li>
</ul>
<p>The code for creating large grid search pipeline is as follows:</p>
<pre><span class="c1"># Create a larger pipeline to gridsearch</span>
<span class="n">face_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'logistic__C'</span><span class="p">:[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e0</span><span class="p">,</span> <span class="mf">1e1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">],</span> 
               <span class="s1">'preprocessing__pca__n_components'</span><span class="p">:[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
               <span class="s1">'preprocessing__pca__whiten'</span><span class="p">:[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>
               <span class="s1">'preprocessing__lda__n_components'</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>  
               <span class="c1"># [1, 2, 3, 4, 5, 6] recall the max allowed is n_classes-1</span>
              <span class="p">}</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>

<span class="n">preprocessing</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">'scale'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span> <span class="p">(</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">pca</span><span class="p">),</span> <span class="p">(</span><span class="s1">'lda'</span><span class="p">,</span> <span class="n">lda</span><span class="p">)])</span>

<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">face_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">'preprocessing'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">),</span> <span class="p">(</span><span class="s1">'logistic'</span><span class="p">,</span> <span class="n">logreg</span><span class="p">)])</span>

<span class="n">get_best_model_and_accuracy</span><span class="p">(</span><span class="n">face_pipeline</span><span class="p">,</span> <span class="n">face_params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></pre>
<p>Here are the results:</p>
<pre>Best Accuracy: 0.840062111801
Best Parameters: {'logistic__C': 0.1, 'preprocessing__pca__n_components': 150, 'preprocessing__lda__n_components': 5, 'preprocessing__pca__whiten': False}
Average Time to Fit (s): 0.214
Average Time to Score (s): 0.009</pre>
<p>We can see that our model accuracy has improved by a good amount, and also our time to predict and train is very fast!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study 2 - predicting topics of hotel reviews data</h1>
                </header>
            
            <article>
                
<p>Our second case study will take a look at hotel reviews data and attempt to cluster the reviews into topics. We will be employing a <span class="c1"><strong>latent semantic analysis</strong></span> (<span class="c1"><strong>LSA</strong></span>), <span class="c1">which is a name given to the process of applying a PCA on sparse text document—term matrices. </span><span class="c1">It is done to find latent structures in text for the purpose of classification and clustering. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of text clustering</h1>
                </header>
            
            <article>
                
<p>Text <strong>clustering</strong> is the act of assigning different topics to pieces of text for the purpose of understanding what documents are about. Imagine a large hotel chain that gets thousands of reviews a week from around the world. Employees of the hotel would like to know what people are saying in order to have a better idea of what they are doing well and what can be improved.</p>
<p>Of course, the limiting factor here is the ability for humans to read all of these texts quickly and correctly. We can train machines to identify the types of things that people are talking about and then predict the topics of new and incoming reviews in order to automate this process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hotel review data</h1>
                </header>
            
            <article>
                
<p>The dataset that we will use to achieve this result comes from Kaggle and can be found here at: <a href="https://www.kaggle.com/datafiniti/hotel-reviews" target="_blank">https://www.kaggle.com/datafiniti/hotel-reviews</a>. It contains over 35,000 distinct reviews of 1,000 different hotels around the world. Our job will be to isolate the text of the reviews and identify <em>topics</em> (what people are talking about). Then, we'll create a machine learning model that can predict/identify the topics of incoming reviews:</p>
<p>First, let's organize our import statements, as follows:</p>
<pre><span class="kn"># used for row normalization<br/>from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer<br/><br/></span># scikit-learn's KMeans clustering module
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans<br/></span>
# data manipulation tool<br/><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd<br/></span>
<span class="c1"># import a sentence tokenizer from nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize<br/><br/></span># feature extraction module (<span class="n">TruncatedSVD will be explained soon)</span><br/><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span> <span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span></pre>
<p>Now, let's load in our data, as shown in the following code snippet:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">hotel_reviews</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'../data/7282_1.csv'</span><span class="p">)</span></pre>
<p>Once we have imported our data, let's work to take a peek into what our raw text data looks like.</p>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploration of the data</h1>
                </header>
            
            <article>
                
<p>Let's look at the <kbd>shape</kbd> of our dataset:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">hotel_reviews</span><span class="o">.</span><span class="n">shape<br/><br/>(35912, 19)</span></pre></div>
</div>
</div>
</div>
<p>This is showing us that we are working with 35,912 rows and 19 columns. Eventually, we will be concerned <span>only</span><span> </span><span>with </span><span>the column that contains the text data, but for now, let's see what the first few rows look like to get a better sense of what is included in our data:</span></p>
<pre><span class="n">hotel_reviews</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p class="mce-root">This gives us the following table:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>address</strong></p>
</td>
<td>
<p><strong>categories</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>country</strong></p>
</td>
<td>
<p><strong>latitude</strong></p>
</td>
<td>
<p><strong>longitude</strong></p>
</td>
<td>
<p><strong>name</strong></p>
</td>
<td>
<p><strong>postalCode</strong></p>
</td>
<td>
<p><strong>province</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
date</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
dateAdded</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
doRecommend</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
id</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
rating</strong></p>
</td>
<td>
<p><strong>reviews.<br/>
text</strong></p>
</td>
<td>
<p><strong>reviews.</strong></p>
<p><strong>title</strong></p>
</td>
<td>
<p><strong>reviews.</strong></p>
<p><strong>userCity</strong></p>
</td>
<td>
<p><strong>reviews.</strong></p>
<p><strong>username</strong></p>
</td>
<td>
<p><strong>reviews.</strong></p>
<p><strong>userProvince</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>Riviera San Nicol 11/a</span></p>
</td>
<td>
<p><span>Hotels</span></p>
</td>
<td>
<p><span>Mableton</span></p>
</td>
<td>
<p><span>US</span></p>
</td>
<td>
<p><span>45.421611</span></p>
</td>
<td>
<p><span>12.376187</span></p>
</td>
<td>
<p><span>Hotel Russo Palace</span></p>
</td>
<td>
<p><span>30126</span></p>
</td>
<td>
<p><span>GA</span></p>
</td>
<td>
<p><span>2013-09-22T00:00:00Z</span></p>
</td>
<td>
<p><span>2016-10-24T00:00:25Z</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>4.0</span></p>
</td>
<td>
<p><span>Pleasant 10 min walk along the sea front to th...</span></p>
</td>
<td>
<p><span>Good location away from the crouds</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Russ (kent)</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>Riviera San Nicol 11/a</span></p>
</td>
<td>
<p><span>Hotels</span></p>
</td>
<td>
<p><span>Mableton</span></p>
</td>
<td>
<p><span>US</span></p>
</td>
<td>
<p><span>45.421611</span></p>
</td>
<td>
<p><span>12.376187</span></p>
</td>
<td>
<p><span>Hotel Russo Palace</span></p>
</td>
<td>
<p><span>30126</span></p>
</td>
<td>
<p><span>GA</span></p>
</td>
<td>
<p><span>2015-04-03T00:00:00Z</span></p>
</td>
<td>
<p><span>2016-10-24T00:00:25Z</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>5.0</span></p>
</td>
<td>
<p><span>Really lovely hotel. Stayed on the very top fl...</span></p>
</td>
<td>
<p><span>Great hotel with Jacuzzi bath!</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>A Traveler</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>Riviera San Nicol 11/a</span></p>
</td>
<td>
<p><span>Hotels</span></p>
</td>
<td>
<p><span>Mableton</span></p>
</td>
<td>
<p><span>US</span></p>
</td>
<td>
<p><span>45.421611</span></p>
</td>
<td>
<p><span>12.376187</span></p>
</td>
<td>
<p><span>Hotel Russo Palace</span></p>
</td>
<td>
<p><span>30126</span></p>
</td>
<td>
<p><span>GA</span></p>
</td>
<td>
<p><span>2014-05-13T00:00:00Z</span></p>
</td>
<td>
<p><span>2016-10-24T00:00:25Z</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>5.0</span></p>
</td>
<td>
<p><span>Ett mycket bra hotell. Det som drog ner betyge...</span></p>
</td>
<td>
<p><span>Lugnt l��ge</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Maud</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>Riviera San Nicol 11/a</span></p>
</td>
<td>
<p><span>Hotels</span></p>
</td>
<td>
<p><span>Mableton</span></p>
</td>
<td>
<p><span>US</span></p>
</td>
<td>
<p><span>45.421611</span></p>
</td>
<td>
<p><span>12.376187</span></p>
</td>
<td>
<p><span>Hotel Russo Palace</span></p>
</td>
<td>
<p><span>30126</span></p>
</td>
<td>
<p><span>GA</span></p>
</td>
<td>
<p><span>2013-10-27T00:00:00Z</span></p>
</td>
<td>
<p><span>2016-10-24T00:00:25Z</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>5.0</span></p>
</td>
<td>
<p><span>We stayed here for four nights in October. The...</span></p>
</td>
<td>
<p><span>Good location on the Lido.</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Julie</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>Riviera San Nicol 11/a</span></p>
</td>
<td>
<p><span>Hotels</span></p>
</td>
<td>
<p><span>Mableton</span></p>
</td>
<td>
<p><span>US</span></p>
</td>
<td>
<p><span>45.421611</span></p>
</td>
<td>
<p><span>12.376187</span></p>
</td>
<td>
<p><span>Hotel Russo Palace</span></p>
</td>
<td>
<p><span>30126</span></p>
</td>
<td>
<p><span>GA</span></p>
</td>
<td>
<p><span>2015-03-05T00:00:00Z</span></p>
</td>
<td>
<p><span>2016-10-24T00:00:25Z</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>5.0</span></p>
</td>
<td>
<p><span>We stayed here for four nights in October. The...</span></p>
</td>
<td>
<p><span>������ ���������������</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>sungchul</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's only include reviews from the United States in order to try and include only English reviews. First, let's plot our data, like so:</p>
<pre><span class="c1"># plot the lats and longs of reviews</span>
<span class="n">hotel_reviews</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'longitude'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'latitude'</span><span class="p">)</span></pre>
<p>The output looks something like this:</p>
<div class="output_area CDPAlignCenter CDPAlign"><img height="228" src="assets/2fadf710-709b-4507-b163-984b4fa244b5.png" width="339"/></div>
<p><span>For the purpose of making our dataset a bit easier to work with, let's use pandas to subset the reviews and only include those that came from the United States:</span></p>
<pre><span class="c1"># Filter to only include reviews within the US</span>
<span class="n">hotel_reviews</span> <span class="o">=</span> <span class="n">hotel_reviews</span><span class="p">[((</span><span class="n">hotel_reviews</span><span class="p">[</span><span class="s1">'latitude'</span><span class="p">]</span><span class="o">&lt;=</span><span class="mf">50.0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">hotel_reviews</span><span class="p">[</span><span class="s1">'latitude'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mf">24.0</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">hotel_reviews</span><span class="p">[</span><span class="s1">'longitude'</span><span class="p">]</span><span class="o">&lt;=-</span><span class="mf">65.0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">hotel_reviews</span><span class="p">[</span><span class="s1">'longitude'</span><span class="p">]</span><span class="o">&gt;=-</span><span class="mf">122.0</span><span class="p">))]</span>

<span class="c1"># Plot the lats and longs again</span>
<span class="n">hotel_reviews</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'longitude'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'latitude'</span><span class="p">)</span>
<span class="c1"># Only looking at reviews that are coming from the US</span></pre>
<p>The output is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="223" src="assets/d4a20b9e-82a0-47d8-803f-9b2dc532bd15.png" width="321"/></div>
<p>It looks like a map of the U.S.! Let's <kbd>shape</kbd> our filtered dataset now:</p>
<pre><span class="n">hotel_reviews</span><span class="o">.</span><span class="n">shape</span></pre>
<p>We have 30,692 rows and 19 columns. When we write reviews for hotels, we usually write about different things in the same review. For this reason, we will attempt to assign topics to single sentences rather than to the entire review.</p>
<p>To do so, let's grab the text column from our data, like so:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">texts</span> <span class="o">=</span> <span class="n">hotel_reviews</span><span class="p">[</span><span class="s1">'reviews.text'</span><span class="p">]</span></pre></div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The clustering model</h1>
                </header>
            
            <article>
                
<p>We can tokenize the text into sentences so that we expand our dataset. We imported a function called <kbd>sent_tokenize</kbd> from the <kbd>nltk</kbd> <span>package</span><span> </span><span>(natural language toolkit). This function will take in a single string and output the sentence as an ordered list of sentences separated by punctuation.</span> <span>For example:</span></p>
<pre><span>sent_tokenize("hello! I am Sinan. How are you??? I am fine")</span><br/><br/>['hello!', 'I am Sinan.', 'How are you???', 'I am fine']</pre>
<p>We will apply this function to our entire corpus using some reduce logic in Python. Essentially, we are applying the <kbd>sent_tokenize</kbd> function to each review and creating a single list called <kbd>sentences</kbd> that will hold all of our sentences:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">sentences</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">,</span> <span class="n">texts</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">'utf-8'</span><span class="p">))))</span></pre></div>
</div>
</div>
</div>
</div>
<p>We can now see how many sentences we have:</p>
<pre><span class="c1"># the number of sentences</span>
<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)<br/><br/>118151</span></pre>
<p>This gives us 118,151—the number of sentences we have to work with. To create a document-term matrix, let's use <kbd>TfidfVectorizer</kbd> on our sentences: </p>
<pre><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">'english'</span><span class="p">)</span>

<span class="n">tfidf_transformed</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

<span class="n">tfidf_transformed</span></pre>
<p>We get the following:</p>
<pre>&lt;118151x280901 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
        with 1180273 stored elements in Compressed Sparse Row format&gt;</pre>
<p>Now, let's try to fit a PCA to this data, like so:</p>
<pre><span class="c1"># try to fit PCA</span>

<span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf_transformed</span><span class="p">)</span></pre>
<p>Upon running this code, we get the following error:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_error">
<pre><span class="ansi-red-fg">TypeError</span>: PCA does not support sparse input. See TruncatedSVD for a possible alternative.</pre></div>
</div>
</div>
</div>
</div>
<p>The is error tells us that for PCA, we cannot have a sparse input, and it suggests that we use <strong>TruncatedSVD</strong>. <span><strong>singular value decomposition </strong></span>(<strong>SVD</strong>) is a matrix <em>trick</em> for computing the same PCA components (when the data is centered) that allow us to work with sparse matrices. Let's take this suggestion and use the <kbd>TruncatedSVD</kbd> module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SVD versus PCA components</h1>
                </header>
            
            <article>
                
<p>Before we move on with our hotel data, let's do a quick experiment with our <kbd>iris</kbd> data to see whether our SVD and PCA really give us the same components:</p>
<ol>
<li>Let's start by grabbing our iris data and creating both a centered and a scaled version:</li>
</ol>
<pre style="padding-left: 60px"># import the Iris dataset from scikit-learn<br/>from sklearn.datasets import load_iris<br/><br/># load the Iris dataset<br/>iris = load_iris()<br/><br/># seperate the features and response variable<br/>iris_X, iris_y = iris.data, iris.target<br/><br/>X_centered = StandardScaler(with_std=False).fit_transform(iris_X)<br/>X_scaled = StandardScaler().fit_transform(iris_X)</pre>
<ol start="2">
<li>Let's continue by instantiating an <kbd>SVD</kbd> and a <kbd>PCA</kbd> object:</li>
</ol>
<pre style="padding-left: 60px"># test if we get the same components by using PCA and SVD<br/>svd = TruncatedSVD(n_components=2)<br/>pca = PCA(n_components=2)</pre>
<ol start="3">
<li>Now, let's apply both <kbd>SVD</kbd> and <kbd>PCA</kbd> to our raw <kbd>iris</kbd> data, centered version, and scaled version to compare:</li>
</ol>
<pre style="padding-left: 60px"># check if components of PCA and TruncatedSVD are same for a dataset<br/># by substracting the two matricies and seeing if, on average, the elements are very close to 0<br/>print (pca.fit(iris_X).components_ - svd.fit(iris_X).components_).mean() <br/><br/><strong>0.130183123094  # not close to 0</strong><br/># matrices are NOT the same<br/><br/><br/># check if components of PCA and TruncatedSVD are same for a centered dataset<br/>print (pca.fit(X_centered).components_ - svd.fit(X_centered).components_).mean() <br/><br/><strong>1.73472347598e-18  # close to 0</strong><br/># matrices ARE the same<br/><br/><br/># check if components of PCA and TruncatedSVD are same for a scaled dataset<br/>print (pca.fit(X_scaled).components_ - svd.fit(X_scaled).components_).mean() <br/><br/><strong>-1.59160878921e-16  # close to 0</strong><br/># matrices ARE the same</pre>
<ol start="4">
<li>This shows us that the SVD module will return the same components as PCA if our data is scaled, but different components when using the raw unscaled data. Let's continue with our hotel data:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">svd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf_transformed</span><span class="p">)</span></pre>
<p style="padding-left: 60px">The output is as follows:</p>
<pre style="padding-left: 60px">TruncatedSVD(algorithm='randomized', n_components=1000, n_iter=5,
       random_state=None, tol=0.0)</pre>
<ol start="5">
<li>Let's make a scree plot as we would with our PCA module to see the explained variance of our SVD components:</li>
</ol>
<pre style="padding-left: 60px"><span class="c1"># Scree Plot</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span></pre>
<p>This gives us the following plot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="208" src="assets/bc448fa3-a329-490b-b08d-645646befc7b.png" width="314"/></div>
<p>We can see that <span class="c1">1,000 components capture about 30% of the variance. </span>Now, let's set up our LSA pipeline. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Latent semantic analysis </h1>
                </header>
            
            <article>
                
<p><strong>Latent semantic analysis</strong> (<strong>LSA</strong>) is a feature extraction tool. It is helpful for text that is a series of these three steps, which we have already learned in this book:</p>
<ul>
<li>A tfidf vectorization</li>
<li>A PCA (SVD in this case to account for the sparsity of text)</li>
<li>Row normalization </li>
</ul>
<p>We can create a scikit-learn pipeline to perform LSA:</p>
<pre><span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">'english'</span><span class="p">)</span>
<span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># will extract 10 "topics"</span>
<span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span> <span class="c1"># will give each document a unit norm</span>

<span class="n">lsa</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">'tfidf'</span><span class="p">,</span> <span class="n">tfidf</span><span class="p">),</span> <span class="p">(</span><span class="s1">'svd'</span><span class="p">,</span> <span class="n">svd</span><span class="p">),</span> <span class="p">(</span><span class="s1">'normalizer'</span><span class="p">,</span> <span class="n">normalizer</span><span class="p">)])</span></pre>
<p>Now, we can fit and transform our sentences data, like so:</p>
<pre><span class="n">lsa_sentences</span> <span class="o">=</span> <span class="n">lsa</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

<span class="n">lsa_sentences</span><span class="o">.</span><span class="n">shape<br/><br/>(118151, 10)</span></pre>
<p>We have <kbd>118151</kbd> rows and <kbd>10</kbd> columns. These 10 columns come from the 10 extracted PCA/SVD components. We can now apply a <kbd>KMeans</kbd> clustering to our <kbd>lsa_sentences</kbd>, as follows:</p>
<pre><span class="n">cluster</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">cluster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lsa_sentences</span><span class="p">)</span></pre>
<div class="packt_infobox">We are assuming that the reader has basic familiarity with clustering. For more information on clustering and how clustering works, please refer to<em> Principles of Data Science</em> by Packt: <a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a></div>
<p>It should be noted that we have chosen both 10 for the <kbd>KMeans</kbd> and our PCA. This is not necessary. Generally, you may extract more columns in the SVD module. With the <kbd>10</kbd> clusters, we are basically saying here, <em>I think there are 10 topics that people are talking about. Please assign each sentence to be one of those topics</em>.</p>
<p>The output is as follows:</p>
<pre>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',
    random_state=None, tol=0.0001, verbose=0)</pre>
<p>Let's time our fit and predict for our <span class="c1">original document-term matrix of shape (<kbd>118151, 280901</kbd>) and then for our </span><span class="c1">latent semantic analysis of shape (<kbd>118151, 10</kbd>) to see the differences:</span></p>
<ol>
<li>First, the original dataset:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre style="padding-left: 60px"><span class="o">%%</span><span class="n">timeit</span>
<span class="c1"># time it takes to cluster on the original document-term matrix of shape (118151, 280901)</span>
<span class="n">cluster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf_transformed</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p style="padding-left: 60px">This gives us:</p>
<div class="output_subarea output_stream output_stdout output_text">
<pre style="padding-left: 60px">1 loop, best of 3: 4min 15s per loop</pre></div>
</div>
</div>
</div>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<ol start="2">
<li>We will also time the prediction of <kbd>Kmeans</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span class="o">%%</span><span class="n">timeit</span>
<span class="c1"># also time the prediction phase of the Kmeans clustering</span>
<span class="n">cluster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">tfidf_transformed</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p class="prompt" style="padding-left: 60px">This gives us:</p>
<div class="output_subarea output_stream output_stdout output_text">
<pre style="padding-left: 60px">10 loops, best of 3: 120 ms per loop</pre></div>
</div>
</div>
</div>
<ol start="3">
<li>Now, the LSA:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre style="padding-left: 60px"><span class="o">%%</span><span class="n">timeit</span>
<span class="c1"># time the time to cluster after latent semantic analysis of shape (118151, 10)</span>
<span class="n">cluster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lsa_sentences</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p class="prompt" style="padding-left: 60px">This gives us:</p>
<div class="output_subarea output_stream output_stdout output_text">
<pre style="padding-left: 60px">1 loop, best of 3: 3.6 s per loop</pre></div>
</div>
</div>
</div>
<ol start="4">
<li>We can see that the LSA is <span class="c1">over 80 times faster than fitting on the original tfidf dataset. Suppose we time the prediction of the clustering with LSA, like so:</span></li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre style="padding-left: 60px"><span class="o">%%</span><span class="n">timeit</span>
<span class="c1"># also time the prediction phase of the Kmeans clustering after LSA was performed</span>
<span class="n">cluster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">lsa_sentences</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<p style="padding-left: 60px">This gives us:</p>
<pre style="padding-left: 60px">10 loops, best of 3: 34 ms per loop</pre></div>
</div>
</div>
</div>
<p>We can see that the LSA dataset is <span>over four times faster than predicting on the original <kbd>tfidf</kbd> dataset.</span></p>
<ol start="5">
<li>Now, let's <span class="c1">transform the texts to a cluster distance space</span> where <span class="c1">each row represents an observation, like so:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="n">cluster</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">lsa_sentences</span><span class="p">)</span><span class="o">.</span><span class="n">shape<br/>(118151, 10)<br/></span>predicted_cluster<span> </span><span class="o">=</span><span> </span><span class="n">cluster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">lsa_sentences</span><span class="p">)<br/></span>predicted_cluster</pre>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p class="prompt output_prompt" style="padding-left: 60px">The output gives us:</p>
<div class="output_text output_subarea output_execute_result">
<pre style="padding-left: 60px">array([2, 2, 2, ..., 2, 2, 6], dtype=int32)</pre></div>
</div>
</div>
</div>
<ol start="6">
<li>We can now get the distribution of topics, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span class="c1"># Distribution of "topics"</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predicted_cluster</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="c1"># create DataFrame of texts and predicted topics</span>
<span class="n">texts_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">'text'</span><span class="p">:</span><span class="n">sentences</span><span class="p">,</span> <span class="s1">'topic'</span><span class="p">:</span><span class="n">predicted_cluster</span><span class="p">})</span>

<span class="n">texts_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="k">print</span> <span class="s2">"Top terms per cluster:"</span>
<span class="n">original_space_centroids</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">cluster</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
<span class="n">order_centroids</span> <span class="o">=</span> <span class="n">original_space_centroids</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">terms</span> <span class="o">=</span> <span class="n">lsa</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span> <span class="s2">"Cluster </span><span class="si">%d</span><span class="s2">:"</span> <span class="o">%</span> <span class="n">i</span>
    <span class="k">print</span> <span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">terms</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">order_centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]])</span>
    <span class="k">print</span> 

<span class="n">lsa</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span></pre>
<ol start="7">
<li>This gives us each topic with a list of the most <em>interesting</em> phrases (according to our <kbd>TfidfVectorizer</kbd>):</li>
</ol>
<pre style="padding-left: 60px">Top terms per cluster:
Cluster 0:
good, breakfast, breakfast good, room, great

Cluster 1:
hotel, recommend, good, recommend hotel, nice hotel

Cluster 2:
clean, room clean, rooms, clean comfortable, comfortable

Cluster 3:
room, room clean, hotel, nice, good

Cluster 4:
great, location, breakfast, hotel, stay

Cluster 5:
stay, hotel, good, enjoyed stay, enjoyed

Cluster 6:
comfortable, bed, clean comfortable, bed comfortable, room

Cluster 7:
nice, room, hotel, staff, nice hotel

Cluster 8:
hotel, room, good, great, stay

Cluster 9:
staff, friendly, staff friendly, helpful, friendly helpful</pre>
<p>We can see the top terms by cluster, and some of them make a lot of sense. For example, cluster 1 seems to be about how people would recommend this hotel to their family and friends, while cluster 9 is about the staff and how they are friendly and helpful. In order to complete this application, we want to be able to predict new reviews with topics.</p>
<p>Now, we can try to predict the cluster of a new review, like so:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="c1"># topic prediction </span>
<span class="k">print</span> <span class="n">cluster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">lsa</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">'I definitely recommend this hotel'</span><span class="p">]))</span>

<span class="k">print</span> <span class="n">cluster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">lsa</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">'super friendly staff. Love it!'</span><span class="p">]))</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p class="prompt">The output gives us cluster 1 for the first prediction and cluster 9 for the second prediction, as follows:</p>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[1]
[9]</pre></div>
</div>
</div>
</div>
<p>Cool! <kbd>Cluster 1</kbd> corresponds to the following:</p>
<pre>Cluster 1:
hotel, recommend, good, recommend hotel, nice hotel</pre>
<p><span><kbd>Cluster 9</kbd> corresponds to the following:</span></p>
<pre class="mce-root">Cluster 9:
staff, friendly, staff friendly, helpful, friendly helpful</pre>
<p>Looks like <kbd>Cluster 1</kbd> is recommending a hotel and <kbd>Cluster 9</kbd> is more staff-centered. Our predictions appear to be fairly accurate!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span class="s1">In this chapter, we saw two different case studies from two vastly different domains using many of the feature engineering methods learned in this book.</span></p>
<p><span class="s1">We</span><span class="s1"> do hope that you have found the contents of this book interesting and that you'll continue your learning! We leave it to you to keep exploring the world of feature engineering, machine learning, and data science. It is hoped that this book has been a catalyst for you to go out and learn even more about the subject.</span><span class="s2"><br/></span></p>
<p><span class="s1">For further reading past this book, I highly recommend looking into well-known data science books and blogs, such as:</span> <span class="s2"><br/></span></p>
<ul class="ul2">
<li class="li2"><span class="s1"><em>Principles of Data Science</em> by Sinan Ozdemir, available through Packt at: <a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a><br/></span></li>
<li><em>Machine Learning</em> and <em>AI</em> blog, KD-nuggets (<a href="https://www.kdnuggets.com/" target="_blank">https://www.kdnuggets.com/</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>