<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer144">
<h1 id="_idParaDest-107"><em class="italic"><a id="_idTextAnchor106"/>Chapter 8</em>: Support Vector Regression</h1>
<p><strong class="bold">Support vector regression</strong> (<strong class="bold">SVR</strong>) can <a id="_idIndexMarker688"/>be an excellent option when the assumptions of linear regression models do not hold, such as when the relationship between our features and our target is too complicated to be described by a linear combination of weights. Even better, SVR allows us to model that complexity without having to expand the feature space.</p>
<p>Support vector machines identify the hyperplane that maximizes the margin between two classes. The support vectors are the data points closest to the margin that <em class="italic">support</em> it, if you will. This turns out to be as useful for regression modeling as it is for classification. SVR finds the hyperplane containing the greatest number of data points. We will discuss how that works in the first section of this chapter.</p>
<p>Rather than minimizing the sum of the squared residuals, as ordinary least squares regression does, SVR minimizes the coefficients within an acceptable error range. Like ridge and lasso regression, this can reduce model variance and the risk of overfitting. SVR works best when we are working with a small- to medium-sized dataset.</p>
<p>The algorithm is also quite flexible, allowing us to specify the acceptable error range, use kernels to model nonlinear relationships, and adjust hyperparameters to get the best bias-variance tradeoff possible. We will demonstrate that in this chapter. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Key concepts of SVR</li>
<li>SVR with a linear model</li>
<li>Using kernels for nonlinear SVR</li>
</ul>
<h1 id="_idParaDest-108"><a id="_idTextAnchor107"/>Technical requirements</h1>
<p>In this chapter, we will be working with the scikit-learn and <strong class="source-inline">matplotlib</strong> libraries. You can use <strong class="source-inline">pip</strong> to install these packages. </p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor108"/>Key concepts of SVR</h1>
<p>We will <a id="_idIndexMarker689"/>start this section by discussing how support vector machines are used for classification. We will not go into much detail here, leaving a detailed discussion of support vector classification to <a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a>, <em class="italic">Support Vector Machine Classification</em>. But starting with support vector machines for classification will lead nicely to an explanation of SVR.</p>
<p>As I discussed at the beginning of this chapter, support vector machines find the hyperplane that maximizes the margin between classes. When there are only two features present, that hyperplane is just a line. Consider the following example plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 8.1 – Support vector machine classification based on two features " height="580" src="image/B17978_08_001.jpg" width="845"/>
</div>
</div>
<p class="figure-caption">Figure 8.1 – Support vector machine classification based on two features</p>
<p>The two classes in this diagram, represented by red circles and blue squares, are <strong class="bold">linearly separable</strong> using<a id="_idIndexMarker690"/> the two features, x1 and x2. The bold line is the decision boundary. It is the line that is furthest away from border data points for each class, or the maximum margin. These points are known as the support vectors.</p>
<p>Since the data in the preceding plot is linearly separable, we can use what is known as <strong class="bold">hard margin classification</strong> without<a id="_idIndexMarker691"/> problems; that is, we can be strict <a id="_idIndexMarker692"/>about all the observations for each class being on the correct side of the decision boundary. But what if our data points look like what’s shown in the following plot?</p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 8.2 – Support vector machine classification with soft margins " height="584" src="image/B17978_08_002.jpg" width="845"/>
</div>
</div>
<p class="figure-caption">Figure 8.2 – Support vector machine classification with soft margins</p>
<p>These data points are <a id="_idIndexMarker693"/>not linearly separable. In this case, we can choose <strong class="bold">soft margin classification</strong> and<a id="_idIndexMarker694"/> ignore the outlier red circles.</p>
<p>We will discuss support vector classification in much greater detail in <a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a>, <em class="italic">Support Vector Machine Classification</em>, but this illustrates some of the key support vector machine concepts. These concepts can be applied well to models involving a continuous target. This is<a id="_idIndexMarker695"/> called <strong class="bold">support vector regression</strong> or <strong class="bold">SVR</strong>.</p>
<p>When building an SVR model, we decide on the acceptable amount of prediction error, ɛ. Errors within ɛ of our prediction, <img alt="" height="46" src="image/B17978_08_001.png" width="173"/>, in a one-feature model are not penalized. This is sometimes referred to as the epsilon-insensitive tube. SVR minimizes the coefficients consistent with all data points falling within that range. This is illustrated in the following plot:</p>
<p> </p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 8.3 – SVR with an acceptable error range " height="584" src="image/B17978_08_003.jpg" width="768"/>
</div>
</div>
<p class="figure-caption">Figure 8.3 – SVR with an acceptable error range</p>
<p>Stated <a id="_idIndexMarker696"/>more precisely, SVR minimizes the square of the coefficients, subject to the constraint that the error, ε, does not exceed a given amount.</p>
<p>It minimizes <img alt="" height="96" src="image/B17978_08_002.png" width="130"/> with the constraint that <img alt="" height="49" src="image/B17978_08_003.png" width="255"/>, where <img alt="" height="27" src="image/B17978_08_004.png" width="35"/> is a vector of weights (or coefficients), <img alt="" height="45" src="image/B17978_08_005.png" width="132"/> is the actual target value minus the predicted value, and <img alt="" height="26" src="image/B17978_08_006.png" width="23"/> is the acceptable amount of error.</p>
<p>Of course, it is <a id="_idIndexMarker697"/>not reasonable to expect all the data points to fall within the desired range. But we can still seek to minimize that deviation. Let’s denote the distance of the wayward points from the margin as ξ. This gives us a new objective function.</p>
<p>We minimize <img alt="" height="100" src="image/B17978_08_007.png" width="375"/> with the constraint that <img alt="" height="48" src="image/B17978_08_008.png" width="383"/>, where <em class="italic">C</em> is a hyperparameter indicating how tolerant the model should be of errors outside the margin. A value of 0 for <em class="italic">C</em> means that it is not at all tolerant of those large errors. This is equivalent to the original objective function:</p>
<p class="figure-caption"> </p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 8.4 – SVR with data points outside the acceptable range " height="583" src="image/B17978_08_004.jpg" width="766"/>
</div>
</div>
<p class="figure-caption">Figure 8.4 – SVR with data points outside the acceptable range</p>
<p>Here, we can <a id="_idIndexMarker698"/>see several advantages of SVR. It is sometimes more important that our errors will not exceed a certain amount, than picking a model with the lowest absolute error. It may matter more if we are often off by a little but rarely by a lot than if we are often spot on but occasionally way off. Since this approach also minimizes our weights, it has the same advantages as regularization, and we reduce the likelihood of overfitting.</p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/>Nonlinear SVR and the kernel trick</h2>
<p>We have<a id="_idIndexMarker699"/> not yet fully addressed the issue of linear separability with SVR. For simplicity, we will return to a classification problem involving two features. Let’s look at a plot of two features against a categorical target. The target has two possible values, represented by the dots and squares. x1 and x2 are numeric and have negative values:</p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 8.5 – Class labels not linearly separable with two features " height="540" src="image/B17978_08_005.jpg" width="545"/>
</div>
</div>
<p class="figure-caption">Figure 8.5 – Class labels not linearly separable with two features</p>
<p>What can we <a id="_idIndexMarker700"/>do in a case like this to identify a margin between the classes? It is often the case that a margin can be identified at a higher dimension. In this example, we can use a polynomial transformation, as illustrated in the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 8.6 – Using polynomial transformation to establish the margin " height="715" src="image/B17978_08_006.jpg" width="911"/>
</div>
</div>
<p class="figure-caption">Figure 8.6 – Using polynomial transformation to establish the margin</p>
<p>There is now a third dimension, which is the sum of the squares of x1 and x2. The dots are all higher than the squares. This is similar to how we used polynomial transformation in the previous chapter to specify a nonlinear regression model.</p>
<p>One drawback of this<a id="_idIndexMarker701"/> approach is that we can quickly end up with too many features for our model to perform well. This is <a id="_idIndexMarker702"/>where the <strong class="bold">kernel trick</strong> comes in very handy. SVR can use a kernel function to expand the feature space implicitly without actually creating more features. This is done by creating a vector of values that can be used to fit a nonlinear margin.</p>
<p>While this allows us to fit a polynomial transformation such as the hypothetical one illustrated in the preceding plot, the most frequently used kernel function with SVR is the <strong class="bold">radial basis function</strong> (<strong class="bold">RBF</strong>). RBF is<a id="_idIndexMarker703"/> popular because it is faster than the other common kernel functions and because its gamma parameter makes it very flexible. We will explore how to use it in the last section of this chapter.</p>
<p>But for now, let’s start with a relatively straightforward linear model to see SVR in action.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor110"/>SVR with a linear model</h1>
<p>We often<a id="_idIndexMarker704"/> have enough domain knowledge to take an approach that is more nuanced than simply minimizing prediction errors in our training data. Using this knowledge may allow us to accept more bias in our model, when small amounts of bias do not matter much substantively, to reduce variance. With SVR, we can adjust hyperparameters such as epsilon (the acceptable error range) and <em class="italic">C</em> (which adjusts the tolerance for errors outside of that range) to improve our model’s performance.</p>
<p>If a linear model can perform well on your data, linear SVR might be a good choice. We can build a linear SVR model with scikit-learn’s <strong class="source-inline">LinearSVR</strong> class. Let’s try creating a linear SVR model with the gasoline tax data that we used in the previous chapter:</p>
<ol>
<li>We need many of the same libraries that we used in the previous chapter to create the training and testing DataFrames and to preprocess the data. We also need to import the <strong class="source-inline">LinearSVR</strong> and <strong class="source-inline">uniform</strong> modules from scikit-learn and scipy, respectively:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.svm import LinearSVR</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.model_selection import cross_validate, \</p><p class="source-code">  KFold, GridSearchCV, RandomizedSearchCV</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>We also <a id="_idIndexMarker705"/>need to import the <strong class="source-inline">OutlierTrans</strong> class, which we first discussed in <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>, to handle outliers:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Next, we load the gasoline tax data and create training and testing DataFrames. We create lists for numerical and binary features, as well as a separate list for <strong class="source-inline">motorization_rate</strong>. As we saw when we looked at the data in the previous chapter, we need to do a little more preprocessing with <strong class="source-inline">motorization_rate</strong>.</li>
</ol>
<p>This <a id="_idIndexMarker706"/>dataset contains gasoline tax data for each country in 2014, as well as fuel income dependence and measures of the strength of the democratic institutions: <strong class="source-inline">polity</strong>, <strong class="source-inline">democracy_polity</strong>, and <strong class="source-inline">autocracy_polity</strong>. <strong class="source-inline">democracy_polity</strong> is a binarized <strong class="source-inline">polity</strong> variable, taking on a value of 1 for countries with high <strong class="source-inline">polity</strong> scores. <strong class="source-inline">autocracy_polity</strong> has a value of 1 for countries with low <strong class="source-inline">polity</strong> scores. The <strong class="source-inline">polity</strong> feature is a measure of how democratic a country is:</p>
<p class="source-code">fftaxrate14 = pd.read_csv("data/fossilfueltaxrate14.csv")</p>
<p class="source-code">fftaxrate14.set_index('countrycode', inplace=True)</p>
<p class="source-code">num_cols = ['fuel_income_dependence',</p>
<p class="source-code">  'national_income_per_cap', 'VAT_Rate',  </p>
<p class="source-code">  'gov_debt_per_gdp', 'polity','goveffect',</p>
<p class="source-code">  'democracy_index']</p>
<p class="source-code">dummy_cols = 'democracy_polity','autocracy_polity',</p>
<p class="source-code">  'democracy', 'nat_oil_comp','nat_oil_comp_state']</p>
<p class="source-code">spec_cols = ['motorization_rate']</p>
<p class="source-code">target = fftaxrate14[['gas_tax_imp']]</p>
<p class="source-code">features = fftaxrate14[num_cols + dummy_cols + spec_cols]</p>
<p class="source-code">X_train, X_test, y_train, y_test =  \</p>
<p class="source-code">  train_test_split(features,\</p>
<p class="source-code">    target, test_size=0.2, random_state=0)</p>
<ol>
<li value="4">Let’s look <a id="_idIndexMarker707"/>at summary statistics for the training data. We will need to standardize the data since there are dramatically different ranges and SVR performs much better on standardized data. Also, notice that <strong class="source-inline">motorization_rate</strong> has a lot of missing values. We may want to do better than simple imputation with that feature. We have decent non-missing counts for the dummy columns:<p class="source-code">X_train.shape</p><p class="source-code">(123, 13)</p><p class="source-code">X_train[num_cols + spec_cols].\</p><p class="source-code">  agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                      count min    median   max</strong></p><p class="source-code"><strong class="bold">fuel_income_dependence  121 0.00   0.10     34.23</strong></p><p class="source-code"><strong class="bold">national_income_per_cap 121 260.00 6,110.00 104,540.00</strong></p><p class="source-code"><strong class="bold">VAT_Rate                121 0.00   16.00    27.00</strong></p><p class="source-code"><strong class="bold">gov_debt_per_gdp        112 1.56   38.45    194.76</strong></p><p class="source-code"><strong class="bold">polity                  121 -10.00 6.00     10.00</strong></p><p class="source-code"><strong class="bold">goveffect               123 -2.04  -0.10    2.18</strong></p><p class="source-code"><strong class="bold">democracy_index         121 0.03   0.54     0.93</strong></p><p class="source-code"><strong class="bold">motorization_rate       100 0.00   0.20     0.81</strong></p><p class="source-code">X_train[dummy_cols].apply(pd.value_counts, normalize=True).T</p><p class="source-code">                                    <strong class="bold">0.00         1.00</strong></p><p class="source-code"><strong class="bold">democracy_polity                    0.42         0.58</strong></p><p class="source-code"><strong class="bold">autocracy_polity                    0.88         0.12</strong></p><p class="source-code"><strong class="bold">democracy                           0.41         0.59</strong></p><p class="source-code"><strong class="bold">nat_oil_comp                        0.54         0.46</strong></p><p class="source-code"><strong class="bold">nat_oil_comp_state                  0.76         0.24</strong></p><p class="source-code">X_train[dummy_cols].count()</p><p class="source-code"><strong class="bold">democracy_polity           121</strong></p><p class="source-code"><strong class="bold">autocracy_polity           121</strong></p><p class="source-code"><strong class="bold">democracy                  123</strong></p><p class="source-code"><strong class="bold">nat_oil_comp               121</strong></p><p class="source-code"><strong class="bold">nat_oil_comp_state         121</strong></p></li>
<li>We <a id="_idIndexMarker708"/>need to build a column transformer to handle different data types. We can use <strong class="source-inline">SimpleImputer</strong> for the categorical features and numerical features, except for <strong class="source-inline">motorization_rate</strong>. We will use KNN imputation for the <strong class="source-inline">motorization_rate</strong> feature later:<p class="source-code">standtrans = make_pipeline(OutlierTrans(2), </p><p class="source-code"> SimpleImputer(strategy="median"), StandardScaler())</p><p class="source-code">cattrans = make_pipeline(SimpleImputer(strategy="most_frequent"))</p><p class="source-code">spectrans = make_pipeline(OutlierTrans(2), StandardScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">    ("cat", cattrans, dummy_cols),</p><p class="source-code">    ("spec", spectrans, spec_cols)</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Now, we are ready to fit our linear SVR model. We will choose a value of <strong class="source-inline">0.2</strong> for <strong class="source-inline">epsilon</strong>. This means that we are fine with any error within 0.2 standard deviations of the actual value (we use <strong class="source-inline">TransformedTargetRegressor</strong> to standardize the target). We will leave <em class="italic">C</em> – the hyperparameter determining our model’s tolerance for values outside of epsilon – at its default value of 1.0.</li>
</ol>
<p>Before we fit our model, we still need to handle missing values for <strong class="source-inline">motorization_rate</strong>. We will add the KNN imputer to a pipeline after the column transformations. Since <strong class="source-inline">motorization_rate</strong> will be the only feature with missing values after the column transformations, the KNN imputer only changes values for that feature.</p>
<p>We need to<a id="_idIndexMarker709"/> use the target transformer because the column transformer will only change the features, not the target. We will pass the pipeline we just created to the target transformer’s <strong class="source-inline">regressor</strong> parameter to do the feature transformations, and indicate that we just want to do standard scaling for the target.</p>
<p>Note that the default loss function for linear SVR is L1, but we could have chosen L2 instead:</p>
<p class="source-code">svr = LinearSVR(epsilon=0.2, max_iter=10000, </p>
<p class="source-code">  random_state=0)</p>
<p class="source-code">pipe1 = make_pipeline(coltrans, </p>
<p class="source-code">  KNNImputer(n_neighbors=5), svr)</p>
<p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p>
<p class="source-code">  transformer=StandardScaler())</p>
<p class="source-code">ttr.fit(X_train, y_train)</p>
<ol>
<li value="7">We <a id="_idIndexMarker710"/>can use <strong class="source-inline">ttr.regressor_</strong> to access all the elements of the pipeline, including the <strong class="source-inline">linearsvr</strong> object. This is how we get to the <strong class="source-inline">coef_</strong> attribute. The coefficients that are substantially different from 0 are <strong class="source-inline">VAT_Rate</strong> and the autocracy and national oil company dummies. Our model estimates a positive relationship between value-added tax rates and gasoline taxes, all else being equal. It estimates a negative relationship between having an autocracy or having a national oil company, and gasoline taxes:<p class="source-code">coefs = ttr.regressor_['linearsvr'].coef_</p><p class="source-code">np.column_stack((coefs.ravel(), num_cols + dummy_cols + spec_cols))</p><p class="source-code"><strong class="bold">array([['-0.03040694175014407', 'fuel_income_dependence'],</strong></p><p class="source-code"><strong class="bold">       ['0.10549935644031803', 'national_income_per_cap'],</strong></p><p class="source-code"><strong class="bold">       ['0.49519936241642026', 'VAT_Rate'],</strong></p><p class="source-code"><strong class="bold">       ['0.0857845735264331', 'gov_debt_per_gdp'],</strong></p><p class="source-code"><strong class="bold">       ['0.018198547504343885', 'polity'],</strong></p><p class="source-code"><strong class="bold">       ['0.12656984468734492', 'goveffect'],</strong></p><p class="source-code"><strong class="bold">       ['-0.09889163752261303', 'democracy_index'],</strong></p><p class="source-code"><strong class="bold">       ['-0.036584519840546594', 'democracy_polity'],</strong></p><p class="source-code"><strong class="bold">       ['-0.5446613604546718', 'autocracy_polity'],</strong></p><p class="source-code"><strong class="bold">       ['0.033234557366924815', 'democracy'],</strong></p><p class="source-code"><strong class="bold">       ['-0.2048732386478349', 'nat_oil_comp'],</strong></p><p class="source-code"><strong class="bold">       ['-0.6142887840649164', 'nat_oil_comp_state'],</strong></p><p class="source-code"><strong class="bold">       ['0.14488410358761755', 'motorization_rate']], dtype='&lt;U32')</strong></p></li>
</ol>
<p>Notice that <a id="_idIndexMarker711"/>we have not done any feature selection here. Instead, we are relying on the L1 regularization to push feature coefficients to near 0. If we had many more features, or we were more concerned about computation time, it would be important to think about our feature selection strategy more carefully.</p>
<ol>
<li value="8">Let’s do some cross-validation on this model. The mean absolute error and r-squared are not great, though that is certainly impacted by the small sample size:<p class="source-code">kf = KFold(n_splits=3, shuffle=True, random_state=0)</p><p class="source-code">ttr.fit(X_train, y_train)</p><p class="source-code">scores = cross_validate(ttr, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=('r2', 'neg_mean_absolute_error'),</p><p class="source-code">    n_jobs=1)</p><p class="source-code">print("Mean Absolute Error: %.2f, R-squared: %.2f" %</p><p class="source-code">  (scores['test_neg_mean_absolute_error'].mean(),</p><p class="source-code">  scores['test_r2'].mean()))</p><p class="source-code">Mean Absolute Error: -0.26, R-squared: 0.57</p></li>
</ol>
<p>We have <a id="_idIndexMarker712"/>not done any hyperparameter tuning yet. We do not know if our values for <strong class="source-inline">epsilon</strong> and <em class="italic">C</em> are the best ones for our model. Therefore, we need to do a grid search to experiment with different hyperparameter values. We will start with an exhaustive grid search, which often is not practical ( I recommend not running the next few steps on your machine unless you have a fairly high-performing one). After the exhaustive grid search, we will do a randomized grid search, which is usually substantially easier on system resources.</p>
<ol>
<li value="9">We will start by creating a <strong class="source-inline">LinearSVR</strong> object without the <strong class="source-inline">epsilon</strong> hyperparameter specified, and we will recreate the pipeline. Then, we will create a dictionary, <strong class="source-inline">svr_params</strong>, with values to check for <strong class="source-inline">epsilon</strong> and <em class="italic">C</em>, called <strong class="source-inline">regressor_linearsvr_epsilon</strong> and <strong class="source-inline">regressor_linearsvr_C</strong>, respectively. </li>
</ol>
<p>Remember from our grid search from the previous chapter that the names of the keys must correspond with our pipeline steps. Our pipeline, which in this case can be accessed as the transformed target’s <strong class="source-inline">regressor</strong> object, has a <strong class="source-inline">linearsvr</strong> object with attributes for <strong class="source-inline">epsilon</strong> and <em class="italic">C</em>. </p>
<p>We will pass the <strong class="source-inline">svr_params</strong> dictionary to a <strong class="source-inline">GridSearchCV</strong> object and indicate that we want the scoring to be based on r-squared (if we wanted to base scoring on the mean absolute error, we could have also done that).</p>
<p>Then, we will run the <strong class="source-inline">fit</strong> method of the grid search object. I should repeat the warning I mentioned previously that you may not want to run an exhaustive grid search unless you are using a high-performing machine, or you do not mind letting it run while you go get a cup of coffee. Note that it takes about 26 seconds to run each time on my machine:</p>
<p class="source-code">svr = LinearSVR(max_iter=100000, random_state=0)</p>
<p class="source-code">pipe1 = make_pipeline(coltrans, </p>
<p class="source-code">  KNNImputer(n_neighbors=5), svr)</p>
<p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p>
<p class="source-code">  transformer=StandardScaler())</p>
<p class="source-code">svr_params = {</p>
<p class="source-code">  'regressor__linearsvr__epsilon': np.arange(0.1, 1.6, 0.1),</p>
<p class="source-code">  'regressor__linearsvr__C': np.arange(0.1, 1.6, 0.1)</p>
<p class="source-code">}</p>
<p class="source-code">gs = GridSearchCV(ttr,param_grid=svr_params, cv=3, </p>
<p class="source-code">  scoring='r2')</p>
<p class="source-code">%timeit gs.fit(X_train, y_train)</p>
<p class="source-code"><strong class="bold">26.2 s ± 50.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</strong></p>
<ol>
<li value="10">Now, we can use the <strong class="source-inline">best_params_</strong> attribute of the grid search to get the hyperparameters <a id="_idIndexMarker713"/>associated with the highest score. We can see the score with those parameters with the <strong class="source-inline">best_scores_</strong> attribute. This tells us that we get the highest r-squared, which is 0.6, with a <em class="italic">C</em> of 0.1 and an <strong class="source-inline">epsilon</strong> value of 0.2:<p class="source-code">gs.best_params_</p><p class="source-code"><strong class="bold">{'regressor__linearsvr__C': 0.1, 'regressor__linearsvr__epsilon': 0.2}</strong></p><p class="source-code">gs.best_score_</p><p class="source-code"><strong class="bold">0.599751107082899</strong></p></li>
</ol>
<p>It is good to know which values to choose for our hyperparameters. However, the exhaustive grid search was quite expensive computationally. Let’s try a randomized search instead.</p>
<ol>
<li value="11">We will <a id="_idIndexMarker714"/>indicate that the random values for both <strong class="source-inline">epsilon</strong> and <em class="italic">C</em> should come from a uniform distribution with values between 0 and 1.5. Then, we will pass that dictionary to a <strong class="source-inline">RandomizedSearchCV</strong> object. This runs substantially faster than the exhaustive grid search – a little over 1 second per iteration. This gives us higher <strong class="source-inline">epsilon</strong> and <em class="italic">C</em> values than the exhaustive grid search – that is, 0.23 and 0.7, respectively. The r-squared value is a little lower, however:<p class="source-code">svr_params = {</p><p class="source-code"> 'regressor__linearsvr__epsilon': uniform(loc=0, scale=1.5),</p><p class="source-code"> 'regressor__linearsvr__C': uniform(loc=0, scale=1.5)</p><p class="source-code">}</p><p class="source-code">rs = RandomizedSearchCV(ttr, svr_params, cv=3, scoring='r2')</p><p class="source-code"><strong class="bold">%timeit rs.fit(X_train, y_train)</strong></p><p class="source-code"><strong class="bold">1.21 s ± 24.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</strong></p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'regressor__linearsvr__C': 0.23062453444814285,</strong></p><p class="source-code"><strong class="bold"> 'regressor__linearsvr__epsilon': 0.6976844872643301}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.5785452537781279</strong></p></li>
<li>Let’s<a id="_idIndexMarker715"/> look at the predictions based on the best model from the randomized grid search. The randomized grid search object’s <strong class="source-inline">predict</strong> method can generate those predictions for us:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.gas_tax_imp-preddf.prediction</p></li>
<li>Now, let’s look at the distribution of our residuals:<p class="source-code">plt.hist(preddf.resid, color="blue", bins=np.arange(-0.5,1.0,0.25))</p><p class="source-code">plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Histogram of Residuals for Gas Tax Model")</p><p class="source-code">plt.xlabel("Residuals")</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.xlim()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 8.7 – Residual distribution for the gasoline tax linear SVR model " height="446" src="image/B17978_08_007.jpg" width="563"/>
</div>
</div>
<p class="figure-caption">Figure 8.7 – Residual distribution for the gasoline tax linear SVR model</p>
<p>Here, there<a id="_idIndexMarker716"/> is a little bit of bias (some overpredicting overall) and some positive skew.</p>
<ol>
<li value="14">Let’s also view a scatterplot of the predicted values against the residuals:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Gas Tax")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax linear SVR model " height="449" src="image/B17978_08_008.jpg" width="577"/>
</div>
</div>
<p class="figure-caption">Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax linear SVR model</p>
<p>These residuals are problematic. We are always overpredicting (predicted values are higher than actual values) at the lower and upper range of the predicted values. This<a id="_idIndexMarker717"/> is not what we want and is perhaps warning us of an unaccounted-for nonlinear relationship.</p>
<p>When our data is linearly separable, linear SVR can be an efficient choice. It can be used in many of the same situations where we would have used linear regression or linear regression with regularization. Its relative efficiency means we are not as concerned about using it with datasets that contain more than 10,000 observations as we are with nonlinear SVR. However, when linear separability is not possible, we should explore nonlinear models.</p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/>Using kernels for nonlinear SVR</h1>
<p>Recall from<a id="_idIndexMarker718"/> our discussion at the beginning of this chapter that we can use a kernel function to fit a nonlinear epsilon-insensitive tube. In this section, we <a id="_idIndexMarker719"/>will run a nonlinear SVR with the land temperatures data that we worked with in the previous chapter. But first, we will construct a linear SVR with the same data for comparison.</p>
<p>We will model the average temperature for weather stations as a function of latitude and elevation. Follow these steps:</p>
<ol>
<li value="1">We will begin by loading the familiar libraries. The only new class is <strong class="source-inline">SVR</strong> from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.svm import LinearSVR, SVR</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Next, we<a id="_idIndexMarker720"/> will load the land temperatures data and create<a id="_idIndexMarker721"/> training and testing DataFrames. We will also take a look at some descriptive statistics. There are several missing values for elevation and the ranges of the two features are very different. There are also some exceedingly low average temperatures:<p class="source-code">landtemps = pd.read_csv("data/landtempsb2019avgs.csv")</p><p class="source-code">landtemps.set_index('locationid', inplace=True)</p><p class="source-code">feature_cols = ['latabs','elevation']</p><p class="source-code">landtemps[['avgtemp'] + feature_cols].\</p><p class="source-code"><strong class="bold">  agg(['count','min','median','max']).T</strong></p><p class="source-code"><strong class="bold">               count       min     median     max</strong></p><p class="source-code"><strong class="bold">avgtemp        12,095     -61      10         34</strong></p><p class="source-code"><strong class="bold">latabs         12,095      0       41         90</strong></p><p class="source-code"><strong class="bold">elevation      12,088     -350     271        4,701</strong></p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(landtemps[feature_cols],\</p><p class="source-code">  landtemps[['avgtemp']], test_size=0.1, random_state=0)</p></li>
<li>Let’s start with a linear SVR model of average temperatures. We can be fairly conservative with how we handle the outliers, only setting them to missing when the interquartile range is more than three times above or below the interquartile range. (We created the <strong class="source-inline">OutlierTrans</strong> class in <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>.) We will use KNN imputation for the missing elevation values and scale the data. Remember that we need to use the target transformer to scale the target variable.</li>
</ol>
<p>Just as <a id="_idIndexMarker722"/>we<a id="_idIndexMarker723"/> did in the previous section, we will use a dictionary, <strong class="source-inline">svr_params</strong>, to indicate that we want to sample values from a uniform distribution for our hyperparameters – that is, <strong class="source-inline">epsilon</strong> and <em class="italic">C</em>. We will pass this dictionary to the <strong class="source-inline">RandomizedSearchCV</strong> object.</p>
<p>After running <strong class="source-inline">fit</strong>, we can get the best parameters for <strong class="source-inline">epsilon</strong> and <em class="italic">C</em>, and the mean absolute error for the best model. The mean absolute error is fairly decent at about 2.8 degrees:</p>
<p class="source-code">svr = LinearSVR(epsilon=1.0, max_iter=100000)</p>
<p class="source-code">knnimp = KNNImputer(n_neighbors=45)</p>
<p class="source-code">pipe1 = make_pipeline(OutlierTrans(3), knnimp, StandardScaler(), svr)</p>
<p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p>
<p class="source-code">  transformer=StandardScaler())</p>
<p class="source-code">svr_params = {</p>
<p class="source-code"> 'regressor__linearsvr__epsilon': uniform(loc=0, scale=1.5),</p>
<p class="source-code"> 'regressor__linearsvr__C': uniform(loc=0, scale=20)</p>
<p class="source-code">}</p>
<p class="source-code">rs = RandomizedSearchCV(ttr, svr_params, cv=10, scoring='neg_mean_absolute_error')</p>
<p class="source-code">rs.fit(X_train, y_train)</p>
<p class="source-code">rs.best_params_</p>
<p class="source-code"><strong class="bold">{'regressor__linearsvr__C': 15.07662849482442,</strong></p>
<p class="source-code"><strong class="bold"> 'regressor__linearsvr__epsilon': 0.06750238486004034}</strong></p>
<p class="source-code">rs.best_score_</p>
<p class="source-code"><strong class="bold">-2.769283402595076</strong></p>
<ol>
<li value="4">Let’s<a id="_idIndexMarker724"/> look <a id="_idIndexMarker725"/>at the predictions:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.avgtemp-preddf.prediction</p><p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Gas Tax")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker726"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures linear SVR model " height="449" src="image/B17978_08_009.jpg" width="573"/>
</div>
</div>
<p class="figure-caption">Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures linear SVR model</p>
<p>There is a <a id="_idIndexMarker727"/>good amount of overpredicting at the upper range of the predicted values. We typically underpredict values just below that, between predicted gas tax values from 15 to 25 degrees. Perhaps we can improve the fit with a nonlinear model.</p>
<ol>
<li value="5">We do not have to change much to run a nonlinear SVR. We just need to create an <strong class="source-inline">SVR</strong> object and choose a kernel function. <strong class="source-inline">rbf</strong> is typically selected. (You may not want to fit this model on your machine unless you are using good hardware, or do not mind doing <a id="_idIndexMarker728"/>something else for a while and coming back for your results.) Take a<a id="_idIndexMarker729"/> look at the following code:<p class="source-code">svr = SVR(kernel='rbf')</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(3), knnimp, StandardScaler(), svr)</p><p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p><p class="source-code">  transformer=StandardScaler())</p><p class="source-code">svr_params = {</p><p class="source-code"> 'regressor__svr__epsilon': uniform(loc=0, scale=5),</p><p class="source-code"> 'regressor__svr__C': uniform(loc=0, scale=20),</p><p class="source-code"> 'regressor__svr__gamma': uniform(loc=0, scale=100)</p><p class="source-code"> }</p><p class="source-code">rs = RandomizedSearchCV(ttr, svr_params, cv=10, scoring='neg_mean_absolute_error')</p><p class="source-code">rs.fit(X_train, y_train)</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'regressor__svr__C': 5.3715128489311255,</strong></p><p class="source-code"><strong class="bold"> 'regressor__svr__epsilon': 0.03997496426101643,</strong></p><p class="source-code"><strong class="bold"> 'regressor__svr__gamma': 53.867632383007994}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">-2.1319240416548775</strong></p></li>
</ol>
<p>There is a noticeable improvement in terms of the mean absolute error. Here, we can see that the <strong class="source-inline">gamma</strong> and C hyperparameters are doing a fair bit of work for us. If we are okay being about 2 degrees off on average, this model gets us there.</p>
<p>We go into much more detail regarding the gamma and C hyperparameters in <a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a><em class="italic">, Support Vector Machine Classification</em>. We also explore other kernels besides the rbf kernel.</p>
<ol>
<li value="6">Let’s look<a id="_idIndexMarker730"/> again at the residuals to see if there is something<a id="_idIndexMarker731"/> problematic in how our errors are distributed, as was the case with our linear model:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.avgtemp-preddf.prediction</p><p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Gas Tax")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<img alt="Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures nonlinear SVR model " height="445" src="image/B17978_08_010.jpg" width="572"/>
</div>
</div>
<p class="figure-caption">Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures nonlinear SVR model</p>
<p>These residuals look substantially better than those for the linear model.</p>
<p>This<a id="_idIndexMarker732"/> illustrates <a id="_idIndexMarker733"/>how using a kernel function can increase the complexity of our model without us having to increase the feature space. By using the <strong class="source-inline">rbf</strong> kernel and adjusting the C and <strong class="source-inline">gamma</strong> hyperparameters, we address some of the underfitting we saw with the linear model. This is one of the great advantages of nonlinear SVR. The disadvantage, as we also saw, was that it was quite taxing on system resources. A dataset that contains 12,000 observations is at the upper limit of what can be handled easily with nonlinear SVR, particularly with a grid search for the best hyperparameters.</p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Summary</h1>
<p>The examples in this chapter illustrated some of the advantages of SVR. The algorithm allows us to adjust hyperparameters to address underfitting or overfitting. This can be done without increasing the number of features. SVR is also less sensitive to outliers than methods such as linear regression.</p>
<p>When we can build a good model with linear SVR, it is a perfectly reasonable choice. It can be trained much faster than a nonlinear model. However, we can often improve performance with a nonlinear SVR, as we saw in the last section of this chapter.</p>
<p>This discussion leads us to what we will explore in the next chapter, where we will look at two popular non-parametric regression algorithms: k-nearest neighbors and decision tree regression. These two algorithms make almost no assumptions about the distribution of our features and targets. Similar to SVR, they can capture complicated relationships in the data without increasing the feature space.</p>
</div>
</div>
</body></html>