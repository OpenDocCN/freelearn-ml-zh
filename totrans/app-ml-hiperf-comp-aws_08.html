<html><head></head><body>
		<div id="_idContainer125">
			<h1 id="_idParaDest-162" class="chapter-number"><a id="_idTextAnchor161"/>8</h1>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor162"/>Optimizing and Managing Machine Learning Models for Edge Deployment</h1>
			<p>Every <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) practitioner knows that the ML development life cycle is an extremely iterative process, from gathering, exploring, and engineering the right features for our algorithm, to training, tuning, and optimizing the ML model for deployment. As ML practitioners, we spend up to 80% of our time getting the right data for training the ML model, with the last 20% actually training and tuning the ML model. By the end of the process, we are all probably so relieved that we finally have an optimized ML model that we often don’t pay enough attention to exactly how the resultant model is deployed. It is, therefore, important to realize that where and how the trained model gets deployed has a significant impact on the overall ML use case. For example, let’s say that our ML use case was specific to <strong class="bold">Autonomous Vehicles</strong> (<strong class="bold">AVs</strong>), specifically a <strong class="bold">Computer Vision</strong> (<strong class="bold">CV</strong>) model that was trained to detect other vehicles. Once our CV model has been trained and optimized, we can <span class="No-Break">deploy it.</span></p>
			<p><em class="italic">But where do we deploy it? Do we deploy it on the vehicle itself, or do we deploy it on the same infrastructure used to train </em><span class="No-Break"><em class="italic">the model?</em></span></p>
			<p>Well, if we deploy the model onto the same infrastructure we used to train the model, we will also need to ensure that the vehicle can connect to this infrastructure. We will also need to ensure that the connectivity from the vehicle to the model is sufficiently robust and performant to ensure that model inference results are timely. It would be disastrous if the vehicle was unable to detect an oncoming vehicle in time. So, in this use case, it might be better to execute model inferences on the vehicle itself; that way, we won’t need to worry about network connectivity, resilience, bandwidth, and latency between the vehicle and the ML model. Essentially, deploying and executing ML model inferences on the <span class="No-Break">edge devices.</span></p>
			<p>However, deploying and managing ML models at the edge imposes additional complexities on the ML development life cycle. So, in this chapter, we will be reviewing some of these complexities, and by means of a practical example, we will see exactly how to optimize, manage, and deploy the ML model for the edge. Thus, we will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">edge computing</span></li>
				<li>Reviewing the key considerations for optimal <span class="No-Break">edge deployments</span></li>
				<li>Designing an architecture for optimal <span class="No-Break">edge deployments</span></li>
			</ul>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor163"/>Technical requirements</h1>
			<p>To work through the hands-on examples within this chapter, you should have the <span class="No-Break">following prerequisites:</span></p>
			<ul>
				<li>A web browser (for the best experience, it is recommended that you use a Chrome or <span class="No-Break">Firefox browser)</span></li>
				<li>Access to the AWS account that you’ve used in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Access to the Amazon SageMaker Studio development environment that we created in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Example code for this chapter is provided in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08</span></a><span class="No-Break">)</span></li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor164"/>Understanding edge computing</h1>
			<p>To understand how we <a id="_idIndexMarker633"/>can optimize, manage, and deploy ML models for the edge, we need to first understand what edge computing is. Edge computing is a pattern or type of architecture that brings data storage mechanisms, and computing resources closer to the actual source of the data. So, by bringing these resources closer to the data itself, we are fundamentally improving the responsiveness of the overall application and removing the requirement to provide optimal and resilient <span class="No-Break">network bandwidth.</span></p>
			<p>Therefore, if we refer to the AV example highlighted at the outset of this chapter, by moving the CV model closer to the source of the data, basically the live camera feed, we are able to detect other vehicles in real time. Consequently, instead of having our application make a connection to the infrastructure that hosts the trained model, we send the camera feed to the ML model, retrieve the inferences, and finally, have the application take some action based on <span class="No-Break">the results.</span></p>
			<p>Now, using an edge <a id="_idIndexMarker634"/>computing architecture, we can send the camera feed directly to the trained CV model, running on the compute resources inside the vehicle itself, and have the application take some action based on the retrieved inference results in real time. Hence, by using an edge computing architecture, we have alleviated any unnecessary application latency introduced by having to connect to the infrastructure hosting the CV model. Subsequently, we have allowed the vehicle to react to other detected vehicles in real time. Additionally, we have removed the dependency on a resilient and optical <span class="No-Break">network connection.</span></p>
			<p>However, while the edge computing architecture provides improved application response times, the architecture itself also introduces additional complexities, especially related to its design and implementation. So, in the next section, we will review some of the key considerations that need to be accounted for when optimally deploying an ML model to <span class="No-Break">the edge.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>Reviewing the key considerations for optimal edge deployments</h1>
			<p>As we saw in the <a id="_idIndexMarker635"/>previous two chapters, there are several key factors that need to be taken into account when designing an appropriate architecture for training as well as deploying ML models at scale. In both these chapters, we also saw how Amazon SageMaker can be used to implement an effective ephemeral infrastructure for executing these tasks. Hence, in a later part of this chapter, we will also review how SageMaker can be used to deploy ML models to the edge at scale. Nonetheless, before we can dive into edge deployments with SageMaker, it is important to review some of the key factors that influence the successful deployment of an ML model at <span class="No-Break">the edge:</span></p>
			<ul>
				<li><span class="No-Break">Efficiency</span></li>
				<li><span class="No-Break">Performance</span></li>
				<li><span class="No-Break">Reliability</span></li>
				<li><span class="No-Break">Security</span></li>
			</ul>
			<p>While not all the mentioned factors may influence how an edge architecture is designed and may not be vital to the <a id="_idIndexMarker636"/>ML use case, it is important to at least consider them. So, let’s start by examining the significance of efficiency within the edge <span class="No-Break">architecture design.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>Efficiency</h2>
			<p>Efficiency, by definition, is the ratio, or percentage, of output correlated with the input. When the ML model is deployed at the edge, it makes the execution closer to the application, as well <a id="_idIndexMarker637"/>as to the input data being used to generate the inferences. Therefore, we can say that deploying an ML model to the edge makes it efficient by default. However, this assumption is based on the fact that the ML model only provides inference results based on the input data and doesn’t need to perform any preprocessing of the input <span class="No-Break">data beforehand.</span></p>
			<p>For example, if we refer to the CV model example, if the image data provided to the ML model had to be preprocessed, for instance, the images needed to be resized, or the image tensors needed to be normalized, then this preprocessing step introduces more work for the ML model. Therefore, this ML model isn’t as efficient as one that just provides the inference result without <span class="No-Break">any preprocessing.</span></p>
			<p>So, when designing an architecture for edge deployments, we need to reduce the amount of unnecessary work being performed on the input data, to further streamline that inference result and therefore make the inference as efficient <span class="No-Break">as possible.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>Performance</h2>
			<p>Measuring performance is similar to measuring efficiency, except that it’s not a ratio but rather a measurement of <a id="_idIndexMarker638"/>quality. Thus, when it comes to measuring the quality of an ML model deployed to the edge, we measure how quickly the model provides an inference result and how true the inference result is. So, just as with efficiency, having the ML model inference results closer to the data source does improve the overall performance, but there are trade-offs that are specific to the ML model use case that also need to <span class="No-Break">be considered.</span></p>
			<p>To illustrate using the CV use case example, we may have to compromise on the quality of the model’s inference results by compressing, or pruning the model architecture, essentially making it smaller to fit into the limited memory and run on the limited processing capacity of an edge computing device. Additionally, while most CV algorithms require <a id="_idIndexMarker639"/>GPU resources for training, as well as inference, we may not be able to provide GPU resources to <span class="No-Break">edge devices.</span></p>
			<p>So, when designing an architecture for edge deployments, we need to anticipate what computing resources are available at the edge and explore how to refactor the trained ML model to ensure that it will fit on the edge device and provide the best inference results in the shortest amount <span class="No-Break">of time.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>Reliability</h2>
			<p>Depending on the ML use case and how the model’s inference results are used, reliability may not be a<a id="_idIndexMarker640"/> crucial factor influencing the design of the edge architecture. For instance, if we consider the AV use case, having the ability to detect other vehicles in proximity is a matter of life and death for the passenger. Alternatively, not being able to use an ML model to predict future temperature fluctuations on a smart thermostat may be an inconvenience but not necessarily a critical factor influencing the design of the <span class="No-Break">edge architecture.</span></p>
			<p>Being able to detect as well as alert when a deployed ML model fails are crucial aspects of the overall reliability of the edge architecture. Thus, the ability to manage ML models at the edge becomes a crucial element of the overall reliability of the edge architecture. Other factors that may influence the reliability and manageability of the architecture are the communication technologies in use. These technologies may provide different levels of reliability and may require multiple different types. For example, in the AV use case, the vehicle may use cellular connectivity as the primary communication technology, and should this fail, a satellite link may be used as <span class="No-Break">a backup.</span></p>
			<p>So, when designing an architecture for edge deployments, reliability might not be a critical factor, but having the ability to manage models deployed to the edge is also essential to the overall scalability of <span class="No-Break">the architecture.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Security</h2>
			<p>As is the case with reliability, security may not be a crucial factor influencing the edge architecture’s design, and more specific to the use case itself. For example, it may be necessary to encrypt all data stored on the edge architecture. With respect to the ML model deployed<a id="_idIndexMarker641"/> on the edge architecture, this means that all inference data (both requests and response data to and from the ML model) must be encrypted if persisted on the edge architecture. Additionally, any data transmitted between components internally and externally to the architecture must be encrypted <span class="No-Break">as well.</span></p>
			<p>It is important to bear in mind that there is a shift from a security management perspective from a centralized to a decentralized trust model and that compute resources within the edge architecture are constrained by size and performance capabilities. Consequently, the choice in the types of encryption is limited, as advanced encryption requires additional <span class="No-Break">compute resources.</span></p>
			<p>Now that we have reviewed some of the key factors that influence the design of an optimal edge architecture for ML model deployment, in the next section, we will dive into building an optimal edge architecture using Amazon SageMaker, as well as other AWS services that specialize in edge <span class="No-Break">device management.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Designing an architecture for optimal edge deployments</h1>
			<p>While there are a <a id="_idIndexMarker642"/>number of key factors that influence the edge<a id="_idIndexMarker643"/> architecture design, as was highlighted in the previous section, there is also a critical capability necessary to enable these factors, namely the ability to build, deploy, and manage the device software at the edge. Additionally, we also need the ability to manage the application, in essence, the ML model deployed to run on the edge devices. Consequently, AWS provides both of these management capabilities using a dedicated device management <a id="_idIndexMarker644"/>service called <strong class="bold">AWS IoT Greengrass</strong> (<a href="https://aws.amazon.com/greengrass/">https://aws.amazon.com/greengrass/</a>), as well as the ML model management capability built <a id="_idIndexMarker645"/>into Amazon SageMaker called <strong class="bold">Amazon SageMaker Edge</strong> (<a href="https://aws.amazon.com/sagemaker/edge">https://aws.amazon.com/sagemaker/edge</a>). AWS IoT Greengrass is a service provided by AWS to deploy software to remote devices at scale without firmware updates. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows an example of an architecture that leverages both Greengrass and SageMaker to support <span class="No-Break">edge deployments.</span></p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B18493_08_001.jpg" alt="Figure 8.1 – Architecture for edge deployments using Amazon SageMaker and IoT Greengrass"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Architecture for edge deployments using Amazon SageMaker and IoT Greengrass</p>
			<p>As you can see from <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em>, a typical architecture is divided into two separate and individual architectures. One for the <a id="_idIndexMarker646"/>cloud-based components and one for the edge components. At a high level, the cloud environment is used to build, deploy, and manage the use case application being deployed to the edge. The corresponding edge environment, or the corporate data center, in this case, is where the edge devices reside, and in turn, execute upon the ML use case by running the supported ML models. From an ML use case perspective, <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> also shows the cameras attached to the edge server device, allowing any video captured to be streamed to the ML model in order for the model to classify the objects detected in the video frames. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows a simplistic flow for the CV use case, from training an ML model on SageMaker to deploying it to the edge server using Greengrass, and then managing and monitoring the solution. But building out the actual solution is <span class="No-Break">very complicated.</span></p>
			<p>So, to illustrate this complexity, in the next section, we are going to build out this architecture by breaking out each component, starting with the corporate data center or <span class="No-Break">edge architecture.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/>Building the edge components</h2>
			<p>As highlighted in the previous section, the corporate data center serves as the edge location for our CV use<a id="_idIndexMarker647"/> case. Inside this edge location, we have a number of cameras, connected to a compute device, or edge server, that runs the CV model. Since we don’t have access to a corporate data center, within the context of the book, we will simulate building out the edge<a id="_idIndexMarker648"/> environment using an <strong class="bold">Elastic Compute Cloud</strong> (<span class="No-Break"><strong class="bold">EC2</strong></span><span class="No-Break">) instance.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are not accustomed to working with EC2 instances, you can familiarize yourself with them by referencing the following <span class="No-Break">documentation: </span><a href="https://aws.amazon.com/ec2/getting-started/"><span class="No-Break">https://aws.amazon.com/ec2/getting-started/</span></a><span class="No-Break">.</span></p>
			<p>The following steps will demonstrate how to set up the edge server using an EC2 instance and configure the necessary Greengrass software as well as the required security permissions. Let’s get started <a id="_idIndexMarker649"/>with setting up the appropriate <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) roles <span class="No-Break">and permissions:</span></p>
			<ol>
				<li>Log into your AWS account and open the IAM <span class="No-Break">console (</span><a href="https://console.aws.amazon.com/iam/home"><span class="No-Break">https://console.aws.amazon.com/iam/home</span></a><span class="No-Break">).</span></li>
				<li>Once the IAM console is open, use the left-hand navigation panel and click on <strong class="bold">Roles</strong> to open the <strong class="bold">Roles</strong> dashboard. Then click on the <strong class="bold">Create role</strong> button in the <span class="No-Break">top-right corner.</span></li>
				<li>Once the <strong class="bold">Create role</strong> wizard starts, select <strong class="bold">AWS Service</strong> as the <strong class="bold">Trusted </strong><span class="No-Break"><strong class="bold">entity </strong></span><span class="No-Break">type.</span></li>
				<li>For the <strong class="bold">Use case</strong>, select <strong class="bold">EC2</strong> and click on the <span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break"> button.</span></li>
				<li>On the <strong class="bold">Add permissions</strong> page, click on the <strong class="bold">Create policy</strong> button in the top right to open the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> page.</span></li>
				<li>On the <strong class="bold">Create policy</strong> page, select the <strong class="bold">JSON</strong> tab, paste the policy from the <strong class="source-inline">SageMakerGreenGrassV2MinimalResourcePolicy.json</strong> file in GitHub (<a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/SageMakerGreenGrassV2MinimalResourcePolicy.json</a>), and make sure to update the <strong class="source-inline">&lt;account_id&gt;</strong> tag in the policy with your AWS <span class="No-Break">account ID.</span></li>
				<li>Click on the <strong class="bold">Next: </strong><span class="No-Break"><strong class="bold">Tags</strong></span><span class="No-Break"> button.</span></li>
				<li>Click on the <strong class="bold">Next: </strong><span class="No-Break"><strong class="bold">Review</strong></span><span class="No-Break"> button.</span></li>
				<li>On the <strong class="bold">Review policy</strong> page, name the policy <strong class="source-inline">SageMakerGreenGrassV2MinimalResourcePolicy</strong>, and click the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> button.</span></li>
				<li>Go back to the <strong class="bold">Add permission</strong> page from <em class="italic">step 5</em>, and refresh the page to capture the newly created <span class="No-Break">IAM policy.</span></li>
				<li>Search for the <strong class="source-inline">SageMakerGreenGrassV2MinimalResourcePolicy</strong> policy in the <a id="_idIndexMarker650"/>search bar, and once found, select the checkbox for the policy to add the permission, then click the <span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break"> button.</span></li>
				<li>On the <strong class="bold">Role details</strong> page, enter <strong class="source-inline">SageMakerGreenGrassV2MinimalResourceRole</strong>, as the <span class="No-Break"><strong class="bold">Role name</strong></span><span class="No-Break">.</span></li>
				<li>Once again, using the <strong class="bold">Role</strong> dashboard, click the newly created <strong class="source-inline">SageMakerGreenGrassV2MinimalResourceRole</strong> role to open the <strong class="bold">Role </strong><span class="No-Break"><strong class="bold">summary</strong></span><span class="No-Break"> page.</span></li>
				<li>Now, click the <strong class="bold">Add permissions</strong> dropdown and select the <strong class="bold">Attach </strong><span class="No-Break"><strong class="bold">policies</strong></span><span class="No-Break"> option.</span></li>
				<li>Search for the <strong class="source-inline">AmazonSageMakerEdgeDeviceFleetPolicy</strong> policy, and click on the checkbox to select <span class="No-Break">this policy.</span></li>
				<li>Repeat the process shown in <em class="italic">step 15</em>, except this time, select the checkbox for the <strong class="bold">AmazonSageMakerFullAccess</strong>, <strong class="bold">AWSIoTLogging</strong>, <strong class="bold">AmazonS3FullAccess</strong>, <strong class="bold">AWSIoTRuleActions</strong>, <strong class="bold">AWSIoTThingsRegistration</strong>, and <span class="No-Break"><strong class="bold">AmazonSSMManagedInstanceCore</strong></span><span class="No-Break"> policies.</span></li>
				<li>With these policies selected, click the <strong class="bold">Attach </strong><span class="No-Break"><strong class="bold">policies</strong></span><span class="No-Break"> button.</span></li>
				<li>In the <strong class="bold">Role summary</strong> page from <em class="italic">step 13</em>, click the <strong class="bold">Add permissions</strong> dropdown and select<a id="_idIndexMarker651"/> the <strong class="bold">Create inline </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> option.</span></li>
				<li>On the <strong class="bold">Create policy</strong> page, click the <strong class="bold">JSON</strong> tab and add the following <span class="No-Break">policy statement:</span><pre class="source-code">
{</pre><pre class="source-code">
    "Version":"2012-10-17",</pre><pre class="source-code">
    "Statement":[</pre><pre class="source-code">
      {</pre><pre class="source-code">
        "Sid":"GreengrassComponentAccess",</pre><pre class="source-code">
        "Effect":"Allow",</pre><pre class="source-code">
        "Action":[</pre><pre class="source-code">
            "greengrass:CreateComponentVersion",</pre><pre class="source-code">
            "greengrass:DescribeComponent"</pre><pre class="source-code">
        ],</pre><pre class="source-code">
        "Resource":"*"</pre><pre class="source-code">
       }</pre><pre class="source-code">
    ]</pre><pre class="source-code">
}</pre></li>
				<li>Click the <strong class="bold">Review </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> button.</span></li>
				<li>Name the policy <strong class="source-inline">GreengrassComponentAccessPolicy</strong> and click the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> button.</span></li>
				<li>In the <strong class="bold">Role summary</strong> page, click the <strong class="bold">Trust relationships</strong> tab and click the <strong class="bold">Edit trust </strong><span class="No-Break"><strong class="bold">policy</strong></span><span class="No-Break"> button.</span></li>
				<li>Replace the existing <a id="_idIndexMarker652"/>policy with the following trust <span class="No-Break">policy statement:</span><pre class="source-code">
{</pre><pre class="source-code">
    "Version": "2012-10-17",</pre><pre class="source-code">
    "Statement": [</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Principal": {</pre><pre class="source-code">
                "Service": "ec2.amazonaws.com"</pre><pre class="source-code">
            },</pre><pre class="source-code">
            "Action": "sts:AssumeRole"</pre><pre class="source-code">
        },</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Principal": {</pre><pre class="source-code">
                "Service": "credentials.iot.amazonaws.com"</pre><pre class="source-code">
            },</pre><pre class="source-code">
            "Action": "sts:AssumeRole"</pre><pre class="source-code">
        },</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Principal": {</pre><pre class="source-code">
                "Service": "sagemaker.amazonaws.com"</pre><pre class="source-code">
            },</pre><pre class="source-code">
            "Action": "sts:AssumeRole"</pre><pre class="source-code">
        }</pre><pre class="source-code">
    ]</pre><pre class="source-code">
}</pre></li>
			</ol>
			<p>Now that we have set up the necessary permissions, next we can configure the edge server with the following <a id="_idIndexMarker653"/>steps showing us how to configure the <span class="No-Break">EC2 instance:</span></p>
			<ol>
				<li value="1">Before creating the EC2 instance, we need to configure the necessary scripts that customize the EC2 instance as an edge server. To provide easy command line access to AWS resources, open the <strong class="bold">AWS CloudShell</strong> <span class="No-Break">console (</span><a href="https://console.aws.amazon.com/cloudshell/home"><span class="No-Break">https://console.aws.amazon.com/cloudshell/home</span></a><span class="No-Break">).</span></li>
				<li>Once the CloudShell console has been initialized in the browser, clone the companion GitHub repository by running the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ git clone https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS src &amp;&amp; cd src/Chapter08</strong></pre></li>
				<li>Next, we create an S3 bucket and store the configuration scripts for the EC2 instance by running the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ export AWS_ACCOUNT=$(aws sts get-caller-identity --query "Account" --output text)</strong></pre><pre class="source-code">
<strong class="bold">$ aws s3 mb s3://ec2-scripts-$AWS_REGION-$AWS_ACCOUNT</strong></pre><pre class="source-code">
<strong class="bold">$ aws s3 sync scripts s3://ec2-scripts-$AWS_REGION-$AWS_ACCOUNT/scripts</strong></pre></li>
				<li>Run the following command to capture the name of the S3 bucket containing the EC2 <span class="No-Break">configuration scripts:</span><pre class="source-code">
<strong class="bold">$ echo ec2-scripts-$AWS_REGION-$AWS_ACCOUNT</strong></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure to remember the name of the S3 bucket containing the EC2 instance configuration scripts, as it will be used in a <span class="No-Break">later step.</span></p>
			<ol>
				<li value="5">Open the EC2 management console (<a href="https://console.aws.amazon.com/ec2/v2/home">https://console.aws.amazon.com/ec2/v2/home</a>) in a <span class="No-Break">browser tab.</span></li>
				<li>Once the EC2 <a id="_idIndexMarker654"/>console is open, click the <strong class="bold">Launch </strong><span class="No-Break"><strong class="bold">instance</strong></span><span class="No-Break"> button.</span></li>
				<li>In the <strong class="bold">Launch an instance</strong> wizard, in the <strong class="bold">Name and tags</strong> section, enter <strong class="source-inline">edge-server</strong> as the name for <span class="No-Break">this instance.</span></li>
				<li>Select AMI as <strong class="bold">amzn2-ami-kernel-5.10-hvm-2.0.20220426.0-x86_64-gp2</strong> and instance type <span class="No-Break">as </span><span class="No-Break"><strong class="bold">c5.large</strong></span><span class="No-Break">.</span></li>
				<li>Scroll down to the <strong class="bold">Configure storage</strong> section and specify <strong class="source-inline">20</strong> GiB for <span class="No-Break"><strong class="bold">Root volume</strong></span><span class="No-Break">.</span></li>
				<li>Using the <strong class="bold">IAM instance profile</strong> drop-down box in the <strong class="bold">Advance details</strong> section, select the <span class="No-Break"><strong class="bold">SageMakerGreenGrassV2MinimalResourceRole</strong></span><span class="No-Break"> role.</span></li>
				<li>In the <strong class="bold">User data</strong> text box of the <strong class="bold">Advance details</strong> section, paste the following <span class="No-Break">Bash code:</span><pre class="source-code">
<strong class="bold">#!/bin/bash</strong></pre><pre class="source-code">
<strong class="bold">aws s3 cp s3://&lt;REPLACE WITH THE NAME OF YOUR S3 BUCKET&gt;/scripts/ /home/ec2-user --recursive</strong></pre><pre class="source-code">
<strong class="bold">sleep 30</strong></pre><pre class="source-code">
<strong class="bold">process_id=$!</strong></pre><pre class="source-code">
<strong class="bold">wait $process_id</strong></pre><pre class="source-code">
<strong class="bold">sudo yum update -y</strong></pre><pre class="source-code">
<strong class="bold">sudo yum install docker -y</strong></pre><pre class="source-code">
<strong class="bold">sudo yum install python-pip -y</strong></pre><pre class="source-code">
<strong class="bold">sudo pip3 install boto3</strong></pre><pre class="source-code">
<strong class="bold">sudo pip3 install requests</strong></pre><pre class="source-code">
<strong class="bold">cd /home/ec2-user</strong></pre><pre class="source-code">
<strong class="bold">python3 getResourceTempCredentials.py</strong></pre><pre class="source-code">
<strong class="bold">sudo service docker start</strong></pre><pre class="source-code">
<strong class="bold">sudo usermod -a -G docker ec2-user</strong></pre><pre class="source-code">
<strong class="bold">docker build -t "aws-iot-greensgrass:2.5" ./</strong></pre><pre class="source-code">
<strong class="bold">chmod +x dockerRun.sh</strong></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure to replace the S3 bucket name with the name of your S3 bucket from the output of <span class="No-Break"><em class="italic">step 4</em></span><span class="No-Break">.</span></p>
			<ol>
				<li value="12">Click the <strong class="bold">Launch instance</strong> button to create the <span class="No-Break">EC2 instance.</span></li>
				<li>Wait 10 minutes<a id="_idIndexMarker655"/> after the EC2 instance is in the active state before logging in to the EC2 instance, as the Bash script in the user data will need some time to install the necessary packages and build the Docker image for AWS IoT <span class="No-Break">Greengrass software.</span></li>
				<li>Log into the EC2 instance, and ensure you are in the <strong class="source-inline">/home/ec2-user</strong> directory. Using <strong class="bold">vi</strong>, open the <strong class="source-inline">env</strong> file, and ensure the <strong class="source-inline">AWS_REGION</strong> variable is set to the current AWS Region being used. For example, the following output shows the <strong class="source-inline">env</strong> file configured for the <span class="No-Break"><strong class="source-inline">us-east-1</strong></span><span class="No-Break"> Region:</span><pre class="source-code">
<strong class="bold">GGC_ROOT_PATH=/greengrass/v2</strong></pre><pre class="source-code">
<strong class="bold">AWS_REGION=us-east-1</strong></pre><pre class="source-code">
<strong class="bold">PROVISION=true</strong></pre><pre class="source-code">
<strong class="bold">THING_NAME=mything</strong></pre><pre class="source-code">
<strong class="bold">THING_GROUP_NAME=mythinggroupname</strong></pre><pre class="source-code">
<strong class="bold">TES_ROLE_NAME=GreengrassV2TokenExchangeRole</strong></pre><pre class="source-code">
<strong class="bold">TES_ROLE_ALIAS_NAME=GreengrassCoreTokenExchangeRoleAlias</strong></pre><pre class="source-code">
<strong class="bold">COMPONENT_DEFAULT_USER=ggc_user:ggc_group</strong></pre><pre class="source-code">
<strong class="bold">DEPLOY_DEV_TOOLS=true</strong></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">You can also customize the <strong class="source-inline">THING_NAME</strong> and <strong class="source-inline">THING_GROUP_NAME</strong> parameters. However, make sure these variables are <span class="No-Break">in lowercase.</span></p>
			<ol>
				<li value="15">Save and <a id="_idIndexMarker656"/>exit the <span class="No-Break"><strong class="source-inline">env</strong></span><span class="No-Break"> file.</span></li>
				<li>Run the Docker container by executing the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ ./dockerRun.sh</strong></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">In case you need to restart the Greengrass V2 Docker container, make sure to get the new credentials by running the <span class="No-Break">following command:</span></p>
			<p class="callout"><strong class="source-inline">$ rm credentials #remove the old </strong><span class="No-Break"><strong class="source-inline">credentials file</strong></span></p>
			<p class="callout"><strong class="source-inline">$ sudo python3 getResourceTempCredentials.py # create credentials file with </strong><span class="No-Break"><strong class="source-inline">new credentials</strong></span></p>
			<p class="callout"><strong class="source-inline">$ ./</strong><span class="No-Break"><strong class="source-inline">dockerRun.sh</strong></span></p>
			<ol>
				<li value="17">Open another terminal window, log in to the EC2 instance, and run the following command to verify that Greengrass V2 is running and retrieve the <span class="No-Break">container ID:</span><pre class="source-code">
<strong class="bold">$ docker ps</strong></pre></li>
				<li>You can then run the following command to access the container and explore the AWS IoT Greengrass Core software running inside <span class="No-Break">the container:</span><pre class="source-code">
<strong class="bold">$ docker exec -it container-id /bin/bash</strong></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">When you use <strong class="source-inline">docker exec</strong> to run commands inside the Docker container, these are not captured in the Docker logs. As a best practice, we recommend you log your commands in the Docker logs so that you can look into the state of the Greengrass Docker container in case you need to troubleshoot <span class="No-Break">any issues.</span></p>
			<ol>
				<li value="19">Run the <a id="_idIndexMarker657"/>following command in a different terminal. It will attach your terminal’s input, output, and error to the container running currently. This will help you to view and control the container from <span class="No-Break">your terminal.</span><pre class="source-code">
<strong class="bold">$ docker attach container-id</strong></pre></li>
			</ol>
			<p>By executing these steps, we have effectively configured an EC2 instance to run our ML model as an edge server. So, with the edge components of our architecture successfully built, we can move on to building the <span class="No-Break">ML model.</span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>Building the ML model</h2>
			<p>Building the ML model involves <a id="_idIndexMarker658"/>training an optimal ML model to suit our business use case. As we’ve seen in previous chapters, Amazon SageMaker provides us with distinct capabilities that allow us to ingest and process the necessary training data, as well as train and optimize the best ML model. Additionally, we saw that SageMaker also allows us to deploy and host these models in the cloud, and as we will see, SageMaker also allows us to deploy and manage ML models at <span class="No-Break">the edge.</span></p>
			<p>The following steps will walk you through how to build an ML model that suits our use case, compile the model <a id="_idIndexMarker659"/>for an edge environment, and then deploy the model to the edge, all <span class="No-Break">using SageMaker:</span></p>
			<ol>
				<li value="1">Within your AWS account, open the Amazon SageMaker management <span class="No-Break">console (</span><a href="https://console.aws.amazon.com/sagemaker/home"><span class="No-Break">https://console.aws.amazon.com/sagemaker/home</span></a><span class="No-Break">).</span></li>
				<li>Launch the SageMaker <span class="No-Break">Studio IDE.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on how to create and launch the SageMaker Studio IDE, please refer to the <em class="italic">Setting up EMR and SageMaker Studio</em> section of <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Once the SageMaker Studio IDE has launched, use the <strong class="bold">File Browser</strong> navigation panel and double-click on the cloned <strong class="source-inline">Applied-Machine-Learning-and-High-Performance-Computing-on-AWS</strong> folder to <span class="No-Break">expand it.</span></li>
				<li>Then, double-click on the <strong class="source-inline">Chapter_8</strong> folder to open it <span class="No-Break">for browsing.</span></li>
				<li>Double-click on the <strong class="source-inline">sagemaker_notebook</strong> folder, and then launch the <span class="No-Break"><strong class="source-inline">1_compile_resnet_model_egde_manager.ipynb</strong></span><span class="No-Break"> notebook.</span></li>
				<li>Once the notebook has started, use the menu bar to select the <strong class="bold">Kernel</strong> menu and then the <strong class="bold">Restart Kernel and Run All </strong><span class="No-Break"><strong class="bold">Cells…</strong></span><span class="No-Break"> option.</span></li>
			</ol>
			<p>Once the notebook has run, we will have our image classification model running on the edge server and managed as part of a fleet. Nonetheless, let’s verify this by reviewing some of the important code cells in the notebook. The first part of the notebook downloads an already optimized or pre-trained <strong class="bold">ResNet-18</strong> (<a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>) model. This is essentially our optimized ML for the image classification use case that we have deployed to the edge. After setting up the SageMaker environment variables and establishing the necessary permissions, we upload the pre-trained model to S3. Once the model has been stored in the cloud, and as you can see from the<a id="_idIndexMarker660"/> following code snippet, we instantiate the pre-trained ResNet-18 model as a PyTorch-based SageMaker model object <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">sagemaker_model</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from sagemaker.pytorch.model import PyTorchModel
from sagemaker.predictor import Predictor
sagemaker_model = PyTorchModel(
    model_data=model_uri,
    predictor_cls=Predictor,
    framework_version=framework_version,
    role=role,
    sagemaker_session=sagemaker_session,
    entry_point="inference.py",
    source_dir="code",
    py_version="py3",
    env={"MMS_DEFAULT_RESPONSE_TIMEOUT": "500"},
)</pre>
			<p>With the model object defined, we then use SageMaker Neo (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a>) to compile a model that suites the specific compute architecture of the edge device, in this case, our X86_64 Linux <span class="No-Break">edge server:</span></p>
			<pre class="source-code">
sagemaker_client = boto3.client("sagemaker", region_name=region)
target_arch = "X86_64"
target_os = 'LINUX'
response = sagemaker_client.create_compilation_job(
    CompilationJobName=compilation_job_name,
    RoleArn=role,
    InputConfig={
        "S3Uri": sagemaker_model.model_data,
        "DataInputConfig": data_shape,
        "Framework": framework,
    },
    OutputConfig={
        "S3OutputLocation": compiled_model_path,
#         "TargetDevice": 'jetson_nano',
        "TargetPlatform": {
            "Arch": target_arch,
            "Os": target_os
        },
    },
    StoppingCondition={"MaxRuntimeInSeconds": 900},
)</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure to take note of the S3 path for the compiled model, as we will be using this path to deploy the <span class="No-Break">model package.</span></p>
			<p>After compiling the model for the<a id="_idIndexMarker661"/> edge server, we verify the model’s functionality by deploying it as a SageMaker-hosted endpoint, and then using the following code to generate a sample inference for a <span class="No-Break">test image:</span></p>
			<pre class="source-code">
import numpy as np
import json
with open("horse_cart.jpg", "rb") as f:
    payload = f.read()
    payload = bytearray(payload)
response = runtime.invoke_endpoint(
    EndpointName=ENDPOINT_NAME,
    ContentType='application/octet-stream',
    Body=payload,
    Accept = 'application/json')
result = response['Body'].read()
result = json.loads(result)
print(result)</pre>
			<p>Once we’ve verified that the model functions correctly, essentially being able to classify the image correctly, we <a id="_idIndexMarker662"/>can then package the model as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
packaging_job_name = compilation_job_name + "-packaging-ggv2"
component_name = "SagemakerEdgeManager" + packaging_job_name
response = sagemaker_client.create_edge_packaging_job(
    RoleArn=role,
    OutputConfig={
        "S3OutputLocation": s3_edge_output_location,
    },
    ModelName=packaged_model_name,
    ModelVersion=model_version,
    EdgePackagingJobName=packaging_job_name,
    CompilationJobName=compilation_job_name,
)</pre>
			<p>Finally, once the model has been packaged, we can use the <strong class="source-inline">create_device_fleet()</strong> method to create a manageable fleet of edge devices to host the newly compiled ML model and then use the <strong class="source-inline">register_device()</strong> method to initialize our EC2 edge <a id="_idIndexMarker663"/>server as a registered or managed edge device that runs our <span class="No-Break">ML model:</span></p>
			<pre class="source-code">
s3_device_fleet_output = os.path.join(s3_edge_output_location, 'fleet')
iot_role_arn = f'arn:aws:iam::{account_id}:role/SageMakerGreenGrassV2MinimalResourceRole'
device_fleet_name = "mydevicefleet"
device_name = 'mything'
sagemaker_client.create_device_fleet(
    DeviceFleetName=device_fleet_name,
    RoleArn=iot_role_arn, # IoT Role ARN specified in previous step
    OutputConfig={
        'S3OutputLocation': s3_device_fleet_output
    }
)
sagemaker_client.register_devices(
    DeviceFleetName=device_fleet_name,
    Devices=[
        {
            "DeviceName": device_name,
            "IotThingName": device_name
        }
    ]
)</pre>
			<p>Once the model<a id="_idIndexMarker664"/> has been trained and compiled and the edge server registered as a SageMaker-managed edge device, we can go ahead and deploy the model package to the <span class="No-Break">edge server.</span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>Deploying the model package</h2>
			<p>To deploy the<a id="_idIndexMarker665"/> model package to the edge server, we will register it as a Greengrass component and then deploy the component to the edge server using the Greengrass console. The following steps will walk us through how to <span class="No-Break">do this:</span></p>
			<ol>
				<li value="1">Using a web browser, open the AWS IoT management <span class="No-Break">console (</span><a href="https://console.aws.amazon.com/iot/home"><span class="No-Break">https://console.aws.amazon.com/iot/home</span></a><span class="No-Break">).</span></li>
				<li>In the <strong class="bold">Manage</strong> section of the left-hand navigation panel, expand the <strong class="bold">Greengrass devices</strong> option and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Components</strong></span><span class="No-Break">.</span></li>
				<li>Click on the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">component</strong></span><span class="No-Break"> button.</span></li>
				<li>After the <strong class="bold">Create component</strong> page loads, ensure that the <strong class="bold">Enter recipe as JSON</strong> option is selected, and paste the <strong class="source-inline">com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json</strong> file contents into the <span class="No-Break"><strong class="bold">Recipe</strong></span><span class="No-Break"> box.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json</strong> file can be found in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.Model.json</span></a><span class="No-Break">).</span></p>
			<ol>
				<li value="5">Update the S3 location of the packaged model under the <strong class="source-inline">Artifacts</strong> tag of the JSON file to <a id="_idIndexMarker666"/>match the S3 path for the <span class="No-Break">compiled model.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">The S3 location of the packaged model is the output from the <strong class="source-inline">create_compilation_job()</strong> method used in the <span class="No-Break">previous section.</span></p>
			<ol>
				<li value="6">Click on the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">component</strong></span><span class="No-Break"> button.</span></li>
				<li>Go to your S3 bucket and create the <strong class="source-inline">artifacts</strong> folder, and inside it, create another folder with the name <strong class="source-inline">com.greengrass.SageMakerEdgeManager.ImageClassification</strong>. Your S3 path should look like <span class="No-Break">this: </span><span class="No-Break"><strong class="source-inline">s3://&lt;bucket_name&gt;/artifacts/com.greengrass.SageMakerEdgeManager.ImageClassification/</strong></span><span class="No-Break">.</span></li>
				<li>Upload the <strong class="source-inline">image_classification.zip</strong> and <strong class="source-inline">installer.sh</strong> files from the GitHub repo (<a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter08</a>) to the S3 location defined in <span class="No-Break"><em class="italic">step 7</em></span><span class="No-Break">.</span></li>
				<li>Update the S3 location of the <strong class="source-inline">image_classification.zip</strong> and <strong class="source-inline">installer.sh</strong> files under the <strong class="source-inline">Artifacts</strong> tag of the JSON file to match the S3 path defined in <span class="No-Break"><em class="italic">step 8</em></span><span class="No-Break">.</span></li>
				<li>Repeat <em class="italic">step 3</em> and <em class="italic">step 4</em> for the <span class="No-Break"><strong class="bold">com.greengrass.SageMakerEdgeManager.ImageClassification</strong></span><span class="No-Break"> component.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">com.greengrass.SageMakerEdgeManager.ImageClassification.json</strong> file can be found in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/custom_component_recipes/com.greengrass.SageMakerEdgeManager.ImageClassification.json</span></a><span class="No-Break">).</span></p>
			<p>With the Greengrass <a id="_idIndexMarker667"/>components registered, we can now deploy them to the edge server and run image classification inference using the components that we just created. Deploying these components downloads a SageMaker Neo-compiled pre-trained ML model and installs the SageMaker Edge Manager agent on the edge server. However, before we can actually perform the deployment of these components, we have to subscribe to the notification topic in order to view and manage any inference tasks on the edge server. The following steps will walk us through how to subscribe to <span class="No-Break">a topic:</span></p>
			<ol>
				<li value="1">Go to the AWS IoT console (<a href="https://console.aws.amazon.com/iot/">https://console.aws.amazon.com/iot/</a>) and click on <strong class="bold">MQTT test client</strong> from the <strong class="bold">Test</strong> option of the left-hand <span class="No-Break">navigation panel.</span></li>
				<li>In the <strong class="bold">Topic name</strong> box of the <strong class="bold">Subscribe to a topic</strong> tab, <span class="No-Break">enter </span><span class="No-Break"><strong class="source-inline">gg/sageMakerEdgeManager/image-classification</strong></span><span class="No-Break">.</span></li>
				<li>Click the <span class="No-Break"><strong class="bold">Subscribe</strong></span><span class="No-Break"> button.</span></li>
			</ol>
			<p>With the ability to monitor and manage any inference requests to the ML model running on the edge server in place, we can deploy the Greengrass components. The following steps will show us <a id="_idIndexMarker668"/>how to do this (they are also highlighted in the AWS Greengrass Developer guide <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf):"><span class="No-Break">https://docs.aws.amazon.com/greengrass/v2/developerguide/greengrass-v2-developer-guide.pdf</span><span class="No-Break">):</span></a></p>
			<ol>
				<li value="1">In the AWS IoT Greengrass console (<a href="https://console.aws.amazon.com/greengrass">https://console.aws.amazon.com/greengrass</a>) navigation menu, choose <strong class="bold">Deployments</strong>, and then choose the deployment for the <span class="No-Break">target device.</span></li>
				<li>On the deployment page, choose <strong class="bold">Revise</strong> and then choose <span class="No-Break"><strong class="bold">Revise deployment</strong></span><span class="No-Break">.</span></li>
				<li>On the <strong class="bold">Specify target</strong> page, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Under the <strong class="bold">My components</strong> option of the <strong class="bold">Select components</strong> page, select both the <strong class="bold">com.greengrass.SageMakerEdgeManager.ImageClassification</strong>, and <span class="No-Break"><strong class="bold">com.greengrass.SageMakerEdgeManager.ImageClassification.Model</strong></span><span class="No-Break"> components.</span></li>
				<li>Under <strong class="bold">Public components</strong>, turn off the <strong class="bold">Show only selected components</strong> toggle, and then select the <span class="No-Break"><strong class="bold">aws.greengrass.SageMakerEdgeManager</strong></span><span class="No-Break"> component.</span></li>
				<li><span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>On the <strong class="bold">Configure components</strong> page, select the <strong class="bold">aws.greengrass.SageMakerEdgeManager</strong> component, and choose <span class="No-Break"><strong class="bold">Configure component</strong></span><span class="No-Break">.</span></li>
				<li>Under <strong class="bold">Configuration update</strong>, in <strong class="bold">Configuration to merge</strong>, enter the <span class="No-Break">following configuration:</span><pre class="source-code">
{</pre><pre class="source-code">
    "DeviceFleetName": "device-fleet-name",</pre><pre class="source-code">
    "BucketName": "S3-BUCKET"</pre><pre class="source-code">
}</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Replace <strong class="source-inline">device-fleet-name</strong> and the S3 bucket name with the corresponding values you created when creating the device fleet in <span class="No-Break">the notebook.</span></p>
			<ol>
				<li value="9">Choose <strong class="bold">Confirm</strong>, and then <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>On the <strong class="bold">Configure advanced settings</strong> page, keep the default configuration settings and <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>On the <strong class="bold">Review</strong> page, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Deploy</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>The deployment can take several minutes to complete. After the components have been deployed, we can<a id="_idIndexMarker669"/> view, manage, and monitor the ML model inference results in the component log of the Greengrass Core device, as well as in the AWS IoT MQTT client of the AWS IoT console. To view the inference results in the component log of the Greengrass Core device, log into the edge server EC2 instance and run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ sudo tail –f /greengrass/v2/logs/com.greengrass.SageMakerEdgeManager.ImageClassification.log</pre>
			<p>You should see <strong class="source-inline">Top 5 predictions with score 0.3 or above</strong> in the logs similar to the one <span class="No-Break">shown here:</span></p>
			<pre class="console">
2022-08-15T03:38:45.437Z [INFO] (Copier) com.greengrass.SageMakerEdgeManager.ImageClassification: stdout. {"timestamp": "2022-08-15 03:38:45.282879+00:00", "inference-type": "image-classification", "inference-description": "Top 5 predictions with score 0.3 or above ", "inference-results": [{"Label": "slot, one-armed bandit", "Score": "83.41295623779297"}, {"Label": "mousetrap", "Score": "75.826416015625"}, {"Label": "comic book", "Score": "73.64051055908203"}, {"Label": "microphone, mike", "Score": "71.14073181152344"}, {"Label": "honeycomb", "Score": "68.3149185180664"}]}. {scriptName=services.com.greengrass.SageMakerEdgeManager.ImageClassification.lifecycle.run.script, serviceName=com.greengrass.SageMakerEdgeManager.ImageClassification, currentState=RUNNING}</pre>
			<p>Alternatively, you can also view the results in <strong class="bold">MQTT test client</strong> on the AWS IoT console (<a href="https://console.aws.amazon.com/iot/">https://console.aws.amazon.com/iot/</a>) by clicking <strong class="bold">MQTT test client</strong> from the <strong class="bold">Test</strong> option on<a id="_idIndexMarker670"/> the left-hand navigation panel. In the <strong class="bold">Subscriptions</strong> section, you will see the prediction results, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B18493_08_002.jpg" alt="Figure 8.2 – Inference results on MQTT test client on AWS IoT console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Inference results on MQTT test client on AWS IoT console</p>
			<p>If you can’t see inference results in the MQTT client, the deployment might have failed or it did not reach the core device. This can occur primarily due to two reasons: your core device is not connected to the network, or it doesn’t have the right permissions to execute the component. To troubleshoot it, you can run the following command on your core device. This command will open the AWS IoT Greengrass Core software log file, which includes logs from the Greengrass Core device’s <span class="No-Break">deployment service.</span></p>
			<pre class="console">
$ sudo tail -f /greengrass/v2/logs/greengrass.log</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information, see the troubleshooting <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html"><span class="No-Break">https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-troubleshooting.html</span></a><span class="No-Break">.</span></p>
			<p>With the Greengrass <a id="_idIndexMarker671"/>components now deployed to the edge server, we have successfully deployed our ML model to the edge. Furthermore, by leveraging the capabilities of AWS IoT Greengrass, as well as Amazon SageMaker, we have not only compiled the ML to function <em class="italic">efficiently</em> and ensure <em class="italic">performance</em> on the edge device but also established a mechanism to <em class="italic">manage</em> and <em class="italic">monitor</em> the environment. As you will recall from the <em class="italic">Reviewing the key considerations for optimal edge deployments</em> section, these are the key factors that make up an optimal <span class="No-Break">edge architecture.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor174"/>Summary</h1>
			<p>In this chapter, we introduced you to the concept of deploying ML models outside of the cloud, primarily on an edge architecture. To lay the foundation for how to accomplish an edge deployment, we also examined what an edge architecture is, as well as the most important factors that need to be considered when designing an edge architecture, namely efficiency, performance, <span class="No-Break">and reliability.</span></p>
			<p>With these factors in mind, we explored how the AWS IoT Greengrass, as well as Amazon SageMaker services, can be used to build an optimal ML model package in the cloud, compiled to run efficiently on an edge device, and then deployed to the edge environment, in a reliable manner. In doing so, we also highlighted just how crucial the ability to manage and monitor both the edge devices, as well as the deployed ML models is to create an optimal <span class="No-Break">edge architecture.</span></p>
			<p>In the next chapter, we will continue along the lines of performance monitoring and optimization of deployed <span class="No-Break">ML models.</span></p>
		</div>
		<div>
			<div id="_idContainer126" class="IMG---Figure">
			</div>
		</div>
	</body></html>