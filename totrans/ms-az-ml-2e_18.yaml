- en: '*Chapter 14*: Model Deployment, Endpoints, and Operations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to build efficient and scalable recommender
    engines through feature engineering, natural language processing, and distributed
    algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will tackle the next step after training a **recommender
    engine** or any machine learning model; we are going to deploy and operate the
    ML model. This will require us to package and register the model, build an execution
    runtime, build a web service, and deploy all components to an execution target.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a look at all the required preparations to deploy ML models
    to production. You will learn the steps that are required in a typical deployment
    process, how to package and register trained models, how to define and build inferencing
    environments, and how to choose a deployment target to run the model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to build a web service for a real-time
    scoring service, similar to Azure Cognitive Services, but using custom models
    and custom code. We will look into model endpoints, controlled rollouts, and endpoint
    schemas so that the models can be deployed without downtime and can be integrated
    into other services. Finally, we will also build a batch-scoring solution that
    can be scheduled or triggered through a web service or pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we will focus on how to monitor and operate your ML scoring
    services. In order to optimize performance and cost, you need to keep track not
    only of system-level metrics but also of telemetry data and scoring results to
    detect model or data drift. By the end of this section, you will be able to confidently
    deploy, tune, and optimize your scoring infrastructure in Azure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Preparations for model deployments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying ML models in Azure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML operations in Azure
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    create model deployments and endpoints:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-core 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn 0.24.2`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`joblib 1.0.1`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy 1.19.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow 2.6.0`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas 1.3.3`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests 2.25.1`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nvidia-smi 0.1.3`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning. However,
    all scripts need to be scheduled to execute in Azure.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Preparations for model deployments
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have learned how to experiment with, train, and optimize
    various ML models to perform classification, regression, anomaly detection, image
    recognition, text understanding, and recommendations. Having successfully trained
    our ML model, we now want to package and deploy this model to production with
    tools in Azure.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about the most important preparation steps that
    are required to deploy a trained model to production using Azure Machine Learning.
    We will discuss the different components involved in a standardized deployment,
    customizing a deployment, auto-deployments, and how to choose the right deployment
    target. Let's delve into it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the components of an ML model
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Independent of the use case, there are similar preparation steps required for
    putting an ML model to production. First, the trained model needs to be registered
    in the model registry. This will allow us to track the model version and binaries
    and fetch a specific version of the model in a deployment. Second, we need to
    specify the deployment assets (for example, the environment, libraries, assets,
    and scoring file). These assets define exactly how the model is loaded and initialized,
    how user input is parsed, how the model is executed, and how the output is passed
    back to the user. Finally, we need to choose a compute target to run the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Azure Machine Learning for deployments, there is a well-defined
    list of things you need to specify in order to deploy and run an ML model as a
    web service. This list includes the following components:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**A trained model**: The model definition and parameters'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An inferencing environment**: A configuration describing the environment,
    for example, as a Docker file'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A scoring file**: The web service code to parse user inputs and outputs and
    invoke the model'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A runtime**: The runtime for the scoring file, for example, Python or PySpark'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A compute target**: The compute environment to run the web service, for example,
    **Azure Kubernetes Service** (**AKS**) or **Azure Container Instances** (**ACI**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look into these five components in more detail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: First, we need a trained model. A model (depending on the framework, libraries,
    and algorithm used) consists of one or multiple files storing the model parameters
    and structure. In scikit-learn, this could be a pickled estimator; in **LightGBM**,
    this could be a serialized list of decision trees; and in Keras, this could be
    a model definition and a binary blob storing the model weights. We call this the
    *model*, and we store and version it in Blob storage. At the startup time of your
    scoring service, the model will be loaded into the scoring runtime.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Besides the model, we also need an execution environment, which can be defined
    via `InferenceConfig`. In Azure Machine Learning deployments, the environment
    will be built into a *Docker* image and stored in your private Docker registry.
    During the deployment process, Azure Machine Learning will automatically build
    the Docker image from the provided environment configuration and load it into
    the private registry in your workspace.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了模型之外，我们还需要一个执行环境，这可以通过`InferenceConfig`来定义。在Azure机器学习部署中，环境将被构建成一个*Docker*镜像，并存储在您的私有Docker注册库中。在部署过程中，Azure机器学习将自动从提供的环境配置中构建Docker镜像，并将其加载到您工作区中的私有注册库中。
- en: In Azure Machine Learning deployments, you can select predefined ML environments
    or configure your own environments and Docker base images. On top of the base
    image, you can define a list of Python *Pip* or *Conda* dependencies, enable GPU
    support, or configure custom Docker steps. The environment, including all required
    packages, will automatically be provided during runtime and set up on the Docker
    image. On top of this, the environment can be registered and versioned by the
    Azure Machine Learning service. This makes it easy to track, reuse, and organize
    your deployment environments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习部署中，您可以选择预定义的ML环境或配置自己的环境和Docker基础镜像。在基础镜像之上，您可以定义一个Python *Pip* 或
    *Conda* 依赖项列表，启用GPU支持，或配置自定义Docker步骤。环境，包括所有必需的包，将在运行时自动提供，并在Docker镜像上设置。在此基础上，环境还可以由Azure机器学习服务注册和版本化。这使得跟踪、重用和组织您的部署环境变得容易。
- en: Next, we need a so-called scoring file. This file typically loads the model
    and provides a function to score the model when given some data as input. Depending
    on the type of deployment, you need to provide a scoring file for either a (real-time)
    synchronous scoring service or an asynchronous batch-scoring service. The scoring
    files should be tracked in your version control system and will be mounted in
    the Docker image.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个所谓的评分文件。这个文件通常加载模型，并提供一个函数，当给定一些数据作为输入时，对模型进行评分。根据部署的类型，您需要为（实时）同步评分服务或异步批量评分服务提供评分文件。评分文件应在您的版本控制系统中进行跟踪，并将被挂载到Docker镜像中。
- en: 'To complete `InferenceConfig`, we are missing one last but important step:
    the Python runtime, used to run your scoring file. Currently, Python and PySpark
    are the only supported runtimes.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要完成`InferenceConfig`，我们缺少最后但非常重要的一步：用于运行您的评分文件的Python运行时。目前，Python和PySpark是唯一支持的运行时。
- en: Finally, we need an execution target that defines the compute infrastructure
    that the Docker image should be executed on. In Azure, this is called the compute
    target and is defined through the deployment configuration. The compute target
    can be a managed Kubernetes cluster (such as AKS), a container instance (such
    as ACI), **Azure Machine Learning Compute** (**AmlCompute**), or one of the many
    other Azure compute services.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要一个执行目标，它定义了Docker镜像应该在其上执行的计算基础设施。在Azure中，这被称为计算目标，并通过部署配置来定义。计算目标可以是管理的Kubernetes集群（如AKS）、容器实例（如ACI）、**Azure机器学习计算**（**AmlCompute**）或许多其他Azure计算服务之一。
- en: Important Note
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The preceding components are only required for managed deployments within Azure
    Machine Learning. Nothing prevents you from fetching the model binaries in another
    environment or running an inferencing environment (the Docker image) on your on-premises
    compute target.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的组件仅适用于Azure机器学习内的托管部署。没有任何东西阻止您在其他环境中检索模型二进制文件或在您的本地计算目标上运行推理环境（Docker镜像）。
- en: If you simply want to deploy a standard model file, such as scikit-learn, **ONNX**,
    or TensorFlow models, you can also use the built-in *auto-deployment* capabilities
    in Azure Machine Learning. Instead of providing all the preceding components,
    auto-deployment requires only the name and version of the used framework and a
    resource configuration, for example, the number of CPUs and the amount of RAM
    to execute. Azure Machine Learning will do the rest; it will provide all the required
    configurations and deploy the model to an ACI. This makes it easy to deploy standard
    models with no more than one line of code – great for development, debugging,
    and testing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the basic deployment components in Azure Machine Learning,
    we can move on and look at an example of registering a model to prepare it for
    deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Registering your models in a model registry
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of the deployment process should happen during or after the training
    and optimization process, namely *registering* the best model from each run in
    the Azure Machine Learning model registry. Independent of whether your training
    script produces a single model, a model ensemble, or a model combined with multiple
    files, you should always store the training artifacts and register the best model
    from each run in your Azure Machine Learning workspace.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: It takes one additional line of code in your training script to store a model
    and register it in Azure Machine Learning and, therefore, never lose your training
    artifacts and models. The Blob storage and model registry are directly integrated
    with your workspace and so the process is tightly integrated into the training
    process. Once a model is registered, Azure Machine Learning provides a convenient
    interface to load the model from the registry.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at what this means for your training script:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the run context and train the `sklearn` classifier:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we write a small helper function that returns the best test accuracy
    metric from all previous runs. We will use this metric to check whether the new
    model performs better than all previous runs:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we check whether the model has better performance than all previous runs
    and register it in the model factory as a new version:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code block, we first use the `joblib.dump``()` function to
    serialize and store a trained classifier to disk. We then call the `run.model_register()`
    function to upload the trained model to the default datastore and register the
    model to the disk. This will automatically track and version the model by name
    and link it to the current training run.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your model is stored in the model registry of your Azure Machine Learning
    workspace, you can use it for deployments and retrieve it by name in any debugging,
    testing, or experimentation step. You can simply request the latest model by name,
    for example, by running the following snippet on your local machine:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All we did in the preceding code is run `Model.get_model_path()` to retrieve
    the latest version of a model by name. We can also specify a version number to
    load a specific model from the registry.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: A built-in model registry is one of the functionalities of the Azure Machine
    Learning workspace that gets you hooked and makes you never want to miss a model
    registry, experiment run, and metrics tracking in the future. It gives you great
    flexibility and transparency when working with model artifacts in different environments
    and during different experiments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we didn't provide any metadata about the trained model
    and, therefore, Azure Machine Learning couldn't infer anything from the model
    artifact. However, if we provide additional information about the model, Azure
    Machine Learning can autogenerate some of the required deployment configurations
    for you to enable auto-deployments. Let's take a look at this in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Auto-deployments of registered models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you stick to the standard functionality provided in scikit-learn, TensorFlow,
    or ONNX, you can also take advantage of auto-deployments in Azure Machine Learning.
    This will allow you to deploy registered models to testing, experimentation, or
    production environments without defining any of the required deployment configurations,
    assets, and service endpoints.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning model auto-deployment will automatically make your model
    available as a web service. If you provide model metadata during training, you
    can invoke auto-deployment using a single command, `Model.deploy()`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at how we need to change the previous example to take advantage
    of auto-deployments:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the resource configuration of the model as shown in the following
    code block:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we need to define the framework and framework version when registering
    the model. To do so, we need to add this additional information to the model by
    extending the `Model.register()` arguments, as shown in the following snippet:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we added the framework and framework version to the model
    registry, as well as the resource configuration for this specific model. The model
    itself is stored in a standard format in one of the supported frameworks (scikit-learn,
    ONNX, or TensorFlow). This metadata is added to the model in the model registry.
    This is all the configuration required to auto-deploy this model as a real-time
    web service in a single line of code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the `Model.deploy()` function to start the deployment process.
    This will build the deployment runtime as a Docker image, register it in your
    container registry, and start the image as a managed container instance, including
    the scoring file, REST service abstraction, and telemetry collection:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To retrieve the URL of the scoring service once the deployment is finished,
    we run the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you want more granular control over the execution environment, endpoint configuration,
    and compute target, you can use the advanced inference, deployment, and service
    configurations in order to customize your deployment. Let's now take a look at
    customized deployments.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Customizing your deployment environment
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you have seen in the previous chapters, the number of libraries, frameworks,
    and customization steps to transform data with an ML model is huge. Azure Machine
    Learning gives us enough flexibility to configure ML scoring services that can
    reflect these customizations. In this section, we will learn how to customize
    the deployment to include libraries and frameworks. Let's dive a bit deeper into
    these individual deployment steps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In the Azure Machine Learning service, you use an execution environment to specify
    a base Docker image, Python runtime, and all the dependent packages required to
    score your model. Like models, environments can also be registered and versioned
    in Azure, so both the Docker artifacts and the metadata are stored, versioned,
    and tracked in your workspace. This makes it simple to keep track of your environment
    changes, figure out which environment was used for a specific run, jump back and
    forth between multiple versions of an environment, and share an environment for
    multiple projects.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build and package your deployment in Docker:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by writing a helper function to create environments on the fly.
    This snippet is very useful when creating environments programmatically based
    on a list of packages. We will also automatically add the `azureml-defaults` package
    to each environment:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see in the preceding code block, we first initialize an `Environment`
    instance and then add multiple `conda` packages. We assign the `conda` dependencies
    by overriding the `env.python.conda_dependencies` property with the `conda_deps`
    dependencies. Using the same approach, we can also override Docker, Spark, and
    any additional Python settings using `env.docker` and `env.spark`, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can define a custom environment to use for experimentation, training,
    or deployment:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the next step, you can now register the environment using a descriptive
    name. This will add a new version of the current environment configuration to
    your environment with the same name:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also retrieve the environment from the registry using the following
    code. This is also useful when you have registered a base environment that can
    be reused and extended for multiple experiments:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As with the `model` registry, you can also load environments using a specified
    version as an additional argument. Once you have configured an execution environment,
    you can combine it with a scoring file to an `InferenceConfig` object. The scoring
    file implements all functionalities to load the model from the registry and evaluate
    it given some input data. The configuration can be defined as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see, in the preceding example, that we simply specify a relative path
    to the scoring script in the local authoring environment. Therefore, you first
    have to create this scoring file; we will go through two examples of batch and
    real-time scoring in the following sections.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'To build an environment, we can simply trigger a build of the Docker image:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The environment will be packaged and registered as a Docker image in your private
    container registry, containing the Docker base image and all specified libraries.
    If you want to package the model and the scoring file, you can package the model
    instead. This is done automatically when deploying the model or can be forced
    by using the `Model.package` function. Let''s load the model from the previous
    section and package and register the image:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important Note
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Azure ML SDK documentation contains a detailed list of possible configuration
    options, which you can find at [https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)).
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding code will build and package your deployment as a Docker image.
    In the next section, we will find out how to choose the best compute target to
    execute your ML deployment.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a deployment target in Azure
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the great advantages of Azure Machine Learning services is that they
    are tightly integrated with many other Azure services. This is extremely helpful
    with deployments where we want to run Docker images of the ML service on a managed
    service within Azure. These compute targets can be configured and leveraged for
    automatic deployment through Azure Machine Learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: If your job is to productionize ML training and deployment pipelines, you might
    not necessarily be an expert in Kubernetes. If that's the case, you might come
    to enjoy the tight integration of the management of Azure compute services in
    the Azure Machine Learning SDK. Similar to creating training environments, you
    can create GPU clusters, managed Kubernetes clusters, or simple container instances
    from within the authoring environment (for example, the Jupyter notebook orchestrating
    your ML workflow).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: We can follow a general recommendation for choosing a specific service, similar
    to choosing a compute service for regular application deployments; so, we trade
    off simplicity, cost, scalability, flexibility, and operational expense between
    the compute services that can easily start a web service from a Docker image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are recommendations of when to use each Azure compute service:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For quick experiments and local testing, use Docker and local deployment targets
    in Azure Machine Learning.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For testing and experimentation, use ACI. It is easy to set up and configure,
    and it is made to run container images.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For deployments of scalable real-time web services with GPU support, use AKS.
    This managed Kubernetes cluster is a lot more flexible and scalable, but also
    a lot harder to operate.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For batch deployments, use Azure Machine Learning clusters, the same compute
    cluster environment we already used for training.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For quick experiments, you can deploy your service locally using `LocalWebservice`
    as a deployment target. To do so, you can run the following snippet on your local
    machine, providing the scoring file and environment in the inferencing configuration:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, once your model is registered, you can deploy it to multiple
    compute targets depending on your use case. While we have covered a few different
    configuration options, we haven't yet discussed multiple deployment options and
    scoring files. We will do this in the next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ML models in Azure
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Broadly speaking, there are two common approaches to deploying ML models, namely
    deploying them as synchronous real-time web services and as asynchronous batch-scoring
    services. Please note that the same model could be deployed as two different services,
    serving different use cases. The deployment type depends heavily on the batch
    size and response time of the scoring pattern of the model. Small batch sizes
    with fast responses require a horizontally scalable real-time web service, whereas
    large batch sizes and slow response times require horizontally and vertically
    scalable batch services.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The deployment of a text-understanding model (for example, an entity recognition
    model or sentiment analysis) could include a real-time web service that evaluates
    the model whenever a new comment is posted to an app, as well as a batch scorer
    in another ML pipeline to extract relevant features from training data. With the
    former, we want to serve each request as quickly as possible, and so we will evaluate
    a small batch size synchronously. With the latter, we are evaluating large amounts
    of data, and so we will evaluate a large batch size asynchronously. Our aim is
    that, once the model is packaged and registered, we can reuse it for either a
    task or use case.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at these deployment approaches and build
    one service for real-time scoring and one for batch-scoring. We will also evaluate
    different options to manage and perform deployments for scoring services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Building a real-time scoring service
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will build a real-time scoring service in Azure Machine
    Learning. We will look into the required scoring file that will power the web
    service, as well as the configuration to start the service on an AKS cluster.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will train an NLP Hugging Face transformer model to perform
    sentiment analysis on user input. Our aim is to build our own Cognitive Services
    Text Analytics API that uses a custom model that is trained or fine-tuned on a
    custom dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we will train a sentiment analysis pipeline, save it, and register
    it as a model in Azure Machine Learning, as shown in the following snippet:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have the model, we start building the web service by taking a look
    at the scoring file. The scoring file will be loaded when the web service starts
    and gets invoked for every request to the ML service. Therefore, we use the scoring
    file to load the ML model, parse the user data from a request, invoke the ML model,
    and return the results of the ML model. To do so, you need to provide the `init()`
    and `run()` functions in the scoring file, where the `run()` function is run once
    when the service starts, and the `run` method is invoked with user inputs for
    every request. The following example shows a simple scoring file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: scoring_file_example.py
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have the trained model and we know the structure of the scoring
    file, we can go ahead and build our custom web service:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the initialization of the service. We first define a global
    model variable, and then fetch the model path from the `AZUREML_MODEL_DIR` environment
    variable. This variable contains the location of the model on the local disk.
    Next, we load the model using the Hugging Face `AutoModel` transformer:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scoring_file.py
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we tackle the actual inferencing part of the web service. To do so, we
    need to parse incoming requests, invoke the NLP model, and return the prediction
    to the caller:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scoring_file.py
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the `run()` function, we are provided with a `request` object. This object
    contains the body of the request sent to the service. As we expect JSON input,
    we parse the request body as a JSON object and access the input string via the
    `query` property. We expect a client to send a valid request that contains exactly
    this schema. Finally, we return a prediction that will be automatically serialized
    into JSON and returned to the caller.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy the service to an ACI compute target for testing purposes. To
    do so, we need to update the deployment configuration to contain the ACI resource
    configuration:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Important Note
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can find more information about Azure Container Instance in the official
    documentation at [https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview).
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we pass the environment and scoring file to the inferencing configuration:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Having all the required components, we can finally pass the model, the inferencing
    configuration, and the deployment configuration to the `Model.deploy` method and
    start the deployment:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once the service is up and running, we can try a test request to the service
    to make sure everything is working properly. By default, Azure Machine Learning
    services use key-based (primary and secondary) authentication. Let''s retrieve
    the key from the service and send some test data to the deployed service:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding snippet fetches the service URL and access key and sends the JSON
    encoded data to the ML model deployment as a `POST` request.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: That's it! You have deployed your sentiment analysis model successfully and
    tested it from Python. However, using the service endpoint and token, you can
    also send requests from any other programming language or HTTP client to your
    service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Azure Kubernetes Services
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have successfully deployed our sentiment analysis model to ACI. As a next
    step, however, we want to deploy it to AKS. While ACI is fantastic for quickly
    getting Docker containers deployed, AKS is a service for complex container-based
    production workloads. Among other features, AKS supports authentication, autoscaling,
    GPU support, replicas, and advanced metrics and logging.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information about Azure Kubernetes Services in the official
    documentation at [https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now deploy this service to an AKS cluster so we can take advantage of
    the GPU acceleration and autoscaling:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define our required infrastructure:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, we created an AKS configuration and a new AKS cluster
    as an Azure Machine Learning compute target from this configuration. All this
    happens completely within your authoring environment.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'If you already have an AKS cluster up and running, you can simply use this
    cluster for Azure Machine Learning. To do so, you have to pass the resource group
    and cluster name to the `AksCompute.attach_configuration()` method. Then, set
    the resource group that contains the AKS cluster and the cluster name:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once we have a reference to the cluster, we can deploy the ML model to the
    cluster. This step is similar to the previous one:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see in the preceding example, apart from attaching the AKS clusters
    as a target to Azure Machine Learning, the model deployment is identical to the
    example using ACI.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Defining a schema for scoring endpoints
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we parse the user input from JSON and expect it to
    contain a `query` parameter. To help users and services consuming your service
    endpoint, it would be useful to tell users which parameters the service is expecting.
    This is a common problem when building web service APIs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: To solve this, Azure Machine Learning provides an innovative way to autogenerate
    an **OpenAPI Specification** (**OAS**), previously called the **Swagger Specification**.
    This specification can be accessed by consumers of the API through the schema
    endpoint. This provides an automated standardized way to specify and consume the
    service's data format and can be used to autogenerate clients. One example is
    **Swagger Codegen**, which can be used to generate Java and C# clients for your
    new ML service.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'You can enable automatic schema generation for pandas, NumPy, PySpark, and
    standard Python objects in your service through annotations in Python. First,
    you need to include `azureml-defaults` and `inference-schema` as PIP packages
    in your environment. Then, you can autogenerate the schema by providing sample
    input and output data for your endpoint, as shown in the following example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: scoring_file.py
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding example, we defined the schema for a NumPy-based model through
    sample data and annotations in the `run()` method.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pick up the sentiment analysis model and allow it to receive multiple
    input queries. To do this, we can deserialize the user input into a pandas DataFrame
    object and return an array of predictions as a result, as shown in the following
    example. Note that this basically adds batch prediction capabilities to our real-time
    web service:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: scoring_file.py
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Defining example inputs and outputs is everything that is required to autogenerate
    an API specification that your clients can use to validate endpoints and arguments
    or to autogenerate clients. This is also the same format that can be used to create
    ML services that can be automatically integrated into Power BI, as shown in [*Chapter
    15*](B17928_15_ePub.xhtml#_idTextAnchor238), *Model Interoperability, Hardware
    Optimization, and Integrations*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Managing model endpoints
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each model deployment contains a URL to send requests to the model; online scoring
    services provide a URL to process online predictions, and batch-scoring services
    provide a URL to trigger batch predictions. While this makes it easy to spin up
    and query a service, one big problem remains during a deployment, namely, that
    the service URL changes with each deployment. This leads to the issue that we
    can't control which service a user request will hit.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we need to hide model deployment URLs behind a fixed
    service URL and provide a mechanism to resolve a user request to a specific service.
    In Azure Machine Learning, the component that fulfills this is called an **endpoint**,
    which can expose multiple deployments under a fixed endpoint URL.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the concept of endpoints and deployments. Customers
    send requests to the endpoint, and we configure the endpoint to route the request
    to one of the services. During a deployment, we would add the new model version
    behind the same scoring endpoint, and incrementally start service requests from
    the new **(green)** version instead of the previous **(blue)** version:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Azure Machine Learning endpoints and deployments ](img/B17928_14_001.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Azure Machine Learning endpoints and deployments
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: This type of deployment is also called blue-green deployment. First, you serve
    all traffic from the old service and start the new service. Once the new service
    is up and running, and the health checks have finished successfully, the service
    is registered under the endpoint, and it will start serving requests. Finally,
    if there are no active requests left on the old service, you can shut it down.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This process is a very safe way to update stateless application services with
    zero or minimal downtime. It also helps you to fall back on the old service if
    the new one doesn't deploy successfully.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning provides multiple types of endpoints, depending on the
    model deployment mechanism:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Online endpoints**: For real-time online deployments:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed online endpoints**: For managed Azure Machine Learning deployments'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes online endpoints**: For managed AKS deployments'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch endpoints**: For batch-scoring deployments'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the top level, we distinguish between online and batch endpoints. While online
    endpoints are used for synchronous scoring based on web service deployments, batch
    endpoints are used for asynchronous scoring based on pipeline deployments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: For online endpoints, we distinguish based on the deployment target between
    managed and Kubernetes-based online endpoints. This is an analog to the different
    compute targets and features for online scoring.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to configure endpoints for AKS:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we configure the endpoint details as shown in the following snippet:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The endpoint configuration serves as a deployment configuration for the AKS
    compute target.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we provide both the endpoint configuration and the compute target to
    the `Model.deploy` method:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The deployment will return an endpoint that can now be used to connect to the
    service and add additional configuration. In the next section, we will look at
    more use cases of endpoints and will see how to add additional deployments to
    the AKS endpoint.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Controlled rollouts and A/B testing
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another benefit of endpoints is to perform controlled rollouts and incremental
    testing of new model versions. ML model deployments are similar to deployments
    of new features in application development. We might not want to roll out this
    new feature to all users at once, but first, test whether the new feature improves
    our business metrics for a small group of users.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: New ML model deployments should never be uncontrolled or based on personal feelings
    or preferences; a deployment should always be based on hard metrics and real evidence.
    The best and most systematic way to test and roll out changes to your users is
    to define a key metric, roll out your new model to one section of the users (group
    B), and serve the old model to the remaining section of the users (group A). Once
    the metrics for the users in group B exceed the metrics from group A over a defined
    period, you can confidently roll out the feature to all your users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept is called **A/B testing** and is used in many tech companies to
    roll out new services and features. As you can see in the following diagram, you
    split your traffic into a control group and a challenger group, where only the
    latter is served the new model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – A/B testing using endpoints ](img/B17928_14_002.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – A/B testing using endpoints
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing and blue-green deployments work very well together, as they are
    really similar approaches. Both require the deployment of a fully functional service
    that is accessible to a subset of your users through routing policies. If you
    use Azure Machine Learning for your deployment and rollout strategy, you are very
    well covered. First, all deployments through Azure Machine Learning to ACI or
    AKS are blue-green deployments, which makes it easy for you to fall back on a
    previous version of your model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning deployments on AKS support up to six model versions
    behind the same endpoint to implement either blue-green deployments or A/B testing
    strategies. You can then define policies to split the traffic between these endpoints;
    for example, you can split traffic by percentage. Here is a small code example
    of how to create another version on an AKS endpoint that should serve another
    version of your model to 50% of the users:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first update the original deployment to serve as the control version
    and serve 50% of the traffic:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we add the challenger version, which is a deployment of `test_model`.
    As you can see in the following snippet, you can also supply a different inference
    configuration to the new deployment:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we start the deployment of the updated endpoints:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we show the preview feature of controlled rollouts for
    Azure Machine Learning and AKS. We use a different combination of model and inference
    configuration to deploy a separate service under the same endpoint. The traffic
    splitting now happens automatically through routing in Kubernetes. However, in
    order to align with a previous section of this chapter, we can expect this functionality
    to improve in the future as it gets used by many customers when rolling out ML
    models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a batch-scoring pipeline
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Operating batch-scoring services is very similar to the previously discussed
    online-scoring approach; you provide an environment, compute target, and scoring
    script. However, in your scoring file, you would rather pass a path to a Blob
    storage location with a new batch of data instead of the data itself. You can
    then use your scoring function to process the data asynchronously and output the
    predictions to a different storage location, back to the Blob storage, or push
    the data asynchronously to the calling service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: It is up to you how you implement your scoring file, as it is simply a Python
    script that you control. The only difference in the deployment process is that
    the batch-scoring script will be deployed as a computation on an Azure Machine
    Learning cluster, scheduled periodically through a pipeline, or triggered through
    a REST service. Therefore, it is important that your scoring script can be configured
    through command-line parameters. Remember that what makes batch scoring different
    is that we don't send the data to the scoring script, but instead, we send a path
    to the data and a path to write the output asynchronously.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'A batch-scoring script is typically wrapped in a pipeline step, deployed as
    a pipeline, and triggered from a REST service or batch-scoring endpoint. The pipeline
    can be configured to use an Azure Machine Learning cluster for execution. In this
    section, we will reuse all of the concepts we have previously seen in [*Chapter
    8*](B17928_08_ePub.xhtml#_idTextAnchor135), *Azure Machine Learning Pipelines*,
    and apply them to a batch-scoring pipeline step. Let''s build a batch-scoring
    pipeline that scores images using the Inception v3 **DNN** model:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a configurable batch size. In both the pipeline configuration
    and the scoring file, you can take advantage of parallelizing your work in the
    Azure Machine Learning cluster:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we define a pipeline step that will call the batch-scoring script:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we wrap the pipeline step in a pipeline. To test the batch-processing
    step, we submit the pipeline as an experiment to the Azure Machine Learning workspace:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Using this pipeline configuration, we call our scoring script with the relevant
    parameters. The pipeline is submitted as an experiment in Azure Machine Learning,
    which gives us access to all the features in runs and experiments in Azure. One
    feature would be that we can simply download the output from the experiment when
    it has finished running:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If the batch-scoring file produces a nice CSV output containing names and predictions,
    we can now display the results using the following pandas functionality:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s go ahead and publish the pipeline as a REST service:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To run the published pipeline as a service through HTTP, we now need to use
    token-based authentication:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Having retrieved the authentication token, we can now run the published pipeline:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: That's it! You can now trigger your batch-scoring pipeline using the REST endpoint.
    The data will be processed, and the results will be provided in a file that can
    be consumed programmatically or piped into the next pipeline step for further
    processing.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Running a batch-scoring pipeline on an Azure Machine Learning service is a bit
    different from running a synchronous scoring service. While the real-time scoring
    service uses Azure Machine Learning deployments and AKS or ACI as popular compute
    targets, batch-scoring models are usually deployed as published pipelines on top
    of AmlCompute. The benefit of a published pipeline is that it can be used as a
    REST service, which can trigger and parameterize the pipeline.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: ML operations in Azure
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You successfully registered a trained model, an environment, a scoring file,
    and an inference configuration in the previous section. You optimized your model
    for scoring and deployed it to a managed Kubernetes cluster. You autogenerated
    client SDKs for your ML services. So, can you finally lean back and enjoy the
    success of your hard work? Well, not yet! First, we need to make sure that we
    have all our monitoring in place so that we can observe and react to anything
    happening to our deployment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the good points: with Azure Machine Learning deployments and managed
    compute targets, you will get many things included out of the box with either
    Azure, Azure Machine Learning, or your service used as a compute target. Tools
    such as the **Azure Dashboard** on the Azure Portal, **Azure Monitor**, and **Azure
    Log Analytics** make it easy to centralize log and debug information. Once your
    data is available through Log Analytics, it can be queried, analyzed, visualized,
    alerted, and/or used for automation using Azure Automation. A great deployment
    and operations process should utilize these tools integrated with Azure and the
    Azure services.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that should come to mind when operating any application is measuring
    software and hardware metrics. It's essential to know the memory consumption,
    CPU usage, I/O latency, and network bandwidth of your application. Particularly
    for an ML service, you should always have an eye on performance bottlenecks and
    resource utilization for cost optimization. For large GPU-accelerated DNNs, it
    is essential to know your system in order to scale efficiently. These metrics
    allow you to scale your infrastructure vertically, and so move to bigger or smaller
    nodes when needed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Another monitoring target for general application deployments should be your
    users' telemetry data (how they are using your service, how often they use it,
    and which parts of the service they use). This will help you to scale horizontally
    and add more nodes or remove nodes when needed.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The final important portion to measure from your scoring service, if possible,
    is the user input over time and the scoring results. For optimal prediction performance,
    it is essential to understand what type of data users are sending to your service,
    and how similar this data is to the training data. It's relatively certain that
    your model will require retraining at some point, and monitoring the input data
    will help you to define a time that this is required (for example, through a data
    drift metric).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how we can monitor the Azure Machine Learning deployments
    and keep track of all these metrics in Azure.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Profiling models for optimal resource configuration
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure Machine Learning provides a handy tool to help you evaluate the required
    resources for your ML model deployment through model profiling. This will help
    you estimate the number of CPUs and the amount of memory required to operate your
    scoring service at a specific throughput.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the model profile of the model that we trained during
    the real-time scoring example:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to define `test_data` in the same format as the JSON request
    for your ML service; so, have `test_data` embedded in a JSON object under the
    `data` root property. Please note that if you defined a different format in your
    scoring file, then you need to use your own custom format:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, you can use the `Model.profile()` method to profile a model and evaluate
    the CPU and memory consumption of the service. This will start up your model,
    fire requests with `test_data` provided to it, and measure the resource utilization
    at the same time:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output contains a list of resources, plus a recommended value for the profiled
    model, as shown in the following snippet:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: It is good to run the model profiling tool before doing a production deployment,
    and this will help you set meaningful default values for your resource configuration.
    To further optimize and decide whether you need to scale up or down, vertically
    or horizontally, you need to measure, track, and observe various other metrics.
    We will discuss monitoring and scaling more in the last section of this chapter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs and infrastructure metrics
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are new to cloud services, or Azure specifically, log and metric collection
    can be a bit overwhelming at first. Logs and metrics are generated in different
    layers in your application and can be either infrastructure- or application-based
    and collected automatically or manually. Then, there are diagnostic metrics that
    are emitted automatically but need to be enabled manually. In this section, we
    will briefly discuss how to collect this metric for the three main managed compute
    targets in the Azure Machine Learning service: ACI, AKS, and AmlCompute.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: By default, you will get access to infrastructure metrics and logs through Azure
    Monitor. It will automatically collect Azure resources and guest OS metrics and
    logs, and provide metrics and query interfaces for logs based on Log Analytics.
    Azure Monitor should be used to track resource utilization (for example, CPU,
    RAM, disk space, disk I/O, and network bandwidth), which then can be pinned to
    dashboards or alerted on. You can even set up automatic autoscaling based on these
    metrics.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics are mostly collected as distributions over time and reported back at
    certain time intervals. So, instead of seeing thousands of values per second,
    you are asked to choose an aggregate for each metric, for example, the average
    of each interval. For most monitoring cases, I would recommend you either look
    at the 95th percentile (or maximum aggregation, for metrics where lower is better)
    to avoid smoothing any spikes during the aggregation process. In AKS, you are
    provided with four different views of your metrics through Azure Monitor: clusters,
    nodes, controllers, and containers.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: More detailed resource, guest, and virtualization host logs of your Azure Machine
    Learning deployment can be accessed by enabling diagnostic settings and providing
    a separate Log Analytics instance. This will automatically load the log data into
    your Log Analytics workspace, where you can efficiently query all your logs, analyze
    them, and create visualization and/or alerts.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: It is strongly recommended to take advantage of the diagnostic settings, as
    they give you insights into your Azure infrastructure. This is especially helpful
    when you need to debug problems in your ML service (for example, failing containers,
    non-starting services, crashes, application freezes, and slow response times).
    Another great use case for Log Analytics is to collect, store, and analyze your
    application log. In AKS, you can send the Kubernetes master node logs, *kubelet*
    logs, and API server logs to Log Analytics.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: One metric that is very important to track for ML training clusters and deployments,
    but is unfortunately not tracked automatically, is the GPU resource utilization.
    Due to this problem, GPU resource utilization has to be monitored and collected
    at the application level.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'The most effective way to solve this for AKS deployments is to run a GPU logger
    service as a sidecar with your application, which collects resource statistics
    and sends them to **Application Insights** (**App Insights**), a service that
    collects application metrics. Both App Insights and Log Analytics use the same
    data storage technology under the hood: Azure Data Explorer. However, default
    integrations for App Insights provide mainly application metrics such as access
    logs, while Log Analytics provides system logs.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'In AmlCompute, we need to start a separate monitoring thread from your application
    code to monitor GPU utilization. Then, for Nvidia GPUs, we use a wrapper around
    the `nvidia-smi` monitoring utility, for example, the `nvidia-ml-py3` Python package.
    To send data to App Insights, we simply use the Azure SDK for App Insights. Here
    is a tiny code example showing you how to achieve this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the preceding code, we first used the `nvidia-ml-py3` wrapper on top of `nvidia-smi`
    to return a handle to the current GPU. Please note that when you have multiple
    GPUs, you can also iterate over them and report multiple metrics. Then, we used
    the `TelemetryClient` API from App Insights to report these metrics back to a
    central place, where we can then visualize, analyze, and alert on these values.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Tracking telemetry and application metrics
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We briefly touched on Azure App Insights in the previous section. It is a great
    service for automatically collecting application metrics from your services, for
    example, Azure Machine Learning deployments. It also provides an SDK to collect
    any user-defined application metric that you want to track.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'To automatically track user metrics, we need to deploy the model using Azure
    Machine Learning deployments to AKS or ACI. This will not only collect the web
    service metadata but also the model''s predictions. To do so, you need to enable
    App Insights'' diagnostics, as well as data model collection, or enable App Insights
    via the Python API:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the preceding snippet, we can activate App Insights' metrics directly from
    the Python authoring environment. While this is a simple argument in the service
    class, it gives you an incredible insight into the deployment.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Two important metrics to measure are data drift coefficients for both training
    data and model predictions. We will learn more about this in the next section.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Detecting data drift
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One important problem in ML is when to retrain your models. Should you always
    retrain when new training data is available, for example, daily, weekly, monthly
    or yearly? Do we need to retrain at all, or is the training data still relevant?
    Measuring **data drift** will help to answer these questions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: By automatically tracking the user input and the model predictions, you can
    compare a statistical variation between the training data and the user input per
    feature dimension, as well as the training labels with the model prediction. The
    variation of the training data and actual data is what is referred to as data
    drift and should be tracked and monitored regularly. Data drift leads to model
    performance degradation over time, and so needs to be monitored. The best case
    is to set up monitoring and alerts to understand when your deployed model differs
    too much from the training data and so needs to be retrained.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning provides useful abstractions to implement data drift
    monitors and alerts based on registered **datasets**, and can automatically expose
    data drift metrics in Application Insights. Computing the data drift requires
    two datasets: a baseline, which is usually the training dataset, and a target
    dataset, which is usually a dataset constructed from the inputs of the scoring
    service:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the target and baseline datasets. These datasets must contain
    a column that represents the date and time of each observation:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we can set up email alerting for the monitor. This can be done in many
    different ways, but for the purpose of this example, we set up an email alert
    directly on the data drift monitor:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can set up the data drift monitor providing all the previous details.
    We configure the monitor for three specific features `[''a'', ''b'', ''c'']`,
    to measure drift on a monthly cadence with a delay of 24 hours. An alert is created
    when the target dataset drifts more than 25% from the baseline data:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, we can enable the monitor schedule to run periodically:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Data drift is an essential operational metric to look at when operating ML deployments.
    Setting up monitors and alarms will help you get alerted early when the distribution
    of your data deviates too much from the training data and, therefore, requires
    you to retrain the model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to take a trained model and deploy it as a managed
    service in Azure through a few simple lines of code. To do so, we learned how
    to prepare a model for deployment and looked into Azure Machine Learning auto-deployments
    and customized deployments.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: We then took an NLP sentiment analysis model and deployed it as a real-time
    scoring service to ACI and AKS. We also learned how to define the service schema
    and how to roll out new versions effectively using endpoints and blue-green deployments.
    Finally, we learned how to integrate a model in a pipeline for asynchronous batch
    scoring.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we learned about monitoring and operating your models using
    Azure Machine Learning services. We proposed to monitor CPU, memory, and GPU metrics
    as well as telemetry data. We also learned how to measure the data drift of your
    service by collecting user input and model output over time. Detecting data drift
    is an important metric that allows you to know when a model needs to be retrained.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will apply the learned knowledge and take a look at
    model interoperability, hardware optimization, and integration into other Azure
    services.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
