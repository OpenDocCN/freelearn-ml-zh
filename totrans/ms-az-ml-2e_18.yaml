- en: '*Chapter 14*: Model Deployment, Endpoints, and Operations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：模型部署、端点和操作'
- en: In the previous chapter, we learned how to build efficient and scalable recommender
    engines through feature engineering, natural language processing, and distributed
    algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何通过特征工程、自然语言处理和分布式算法构建高效且可扩展的推荐引擎。
- en: In this chapter, we will tackle the next step after training a **recommender
    engine** or any machine learning model; we are going to deploy and operate the
    ML model. This will require us to package and register the model, build an execution
    runtime, build a web service, and deploy all components to an execution target.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解决在训练推荐引擎或任何机器学习模型之后的下一步；我们将部署和操作机器学习模型。这需要我们打包和注册模型，构建执行运行时，构建网络服务，并将所有组件部署到执行目标。
- en: First, we will take a look at all the required preparations to deploy ML models
    to production. You will learn the steps that are required in a typical deployment
    process, how to package and register trained models, how to define and build inferencing
    environments, and how to choose a deployment target to run the model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看部署机器学习模型到生产环境所需的所有准备工作。你将学习典型部署过程中所需的步骤，如何打包和注册训练好的模型，如何定义和构建推理环境，以及如何选择部署目标来运行模型。
- en: In the next section, we will learn how to build a web service for a real-time
    scoring service, similar to Azure Cognitive Services, but using custom models
    and custom code. We will look into model endpoints, controlled rollouts, and endpoint
    schemas so that the models can be deployed without downtime and can be integrated
    into other services. Finally, we will also build a batch-scoring solution that
    can be scheduled or triggered through a web service or pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何构建一个用于实时评分服务的网络服务，类似于Azure认知服务，但使用自定义模型和自定义代码。我们将探讨模型端点、受控发布和端点模式，以便模型可以在不停机的情况下部署，并集成到其他服务中。最后，我们还将构建一个批处理评分解决方案，可以通过网络服务或管道进行调度或触发。
- en: In the last section, we will focus on how to monitor and operate your ML scoring
    services. In order to optimize performance and cost, you need to keep track not
    only of system-level metrics but also of telemetry data and scoring results to
    detect model or data drift. By the end of this section, you will be able to confidently
    deploy, tune, and optimize your scoring infrastructure in Azure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将关注如何监控和操作你的机器学习评分服务。为了优化性能和成本，你需要跟踪系统级指标，以及遥测数据和评分结果，以检测模型或数据漂移。到本节结束时，你将能够自信地在Azure中部署、调整和优化你的评分基础设施。
- en: 'In this chapter, you will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将涵盖以下主题：
- en: Preparations for model deployments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署准备
- en: Deploying ML models in Azure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure中部署机器学习模型
- en: ML operations in Azure
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure中的机器学习操作
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create model deployments and endpoints:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建模型部署和端点：
- en: '`azureml-core 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: '`joblib 1.0.1`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`joblib 1.0.1`'
- en: '`numpy 1.19.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`tensorflow 2.6.0`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow 2.6.0`'
- en: '`pandas 1.3.3`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.3`'
- en: '`requests 2.25.1`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests 2.25.1`'
- en: '`nvidia-smi 0.1.3`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-smi 0.1.3`'
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning. However,
    all scripts need to be scheduled to execute in Azure.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，你可以使用本地Python解释器或Azure机器学习托管的工作簿环境运行此代码。然而，所有脚本都需要在Azure中安排执行。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14)。
- en: Preparations for model deployments
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署准备
- en: Throughout this book, we have learned how to experiment with, train, and optimize
    various ML models to perform classification, regression, anomaly detection, image
    recognition, text understanding, and recommendations. Having successfully trained
    our ML model, we now want to package and deploy this model to production with
    tools in Azure.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经学习了如何通过 Azure 的工具对各种机器学习模型进行实验、训练和优化，以执行分类、回归、异常检测、图像识别、文本理解和推荐。在成功训练我们的机器学习模型后，我们现在希望使用
    Azure 的工具将此模型打包并部署到生产环境中。
- en: In this section, we will learn about the most important preparation steps that
    are required to deploy a trained model to production using Azure Machine Learning.
    We will discuss the different components involved in a standardized deployment,
    customizing a deployment, auto-deployments, and how to choose the right deployment
    target. Let's delve into it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习使用 Azure Machine Learning 将训练好的模型部署到生产环境所需的最重要准备步骤。我们将讨论标准化部署中的不同组件、定制部署、自动部署以及如何选择合适的部署目标。让我们深入探讨。
- en: Understanding the components of an ML model
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解机器学习模型的组件
- en: Independent of the use case, there are similar preparation steps required for
    putting an ML model to production. First, the trained model needs to be registered
    in the model registry. This will allow us to track the model version and binaries
    and fetch a specific version of the model in a deployment. Second, we need to
    specify the deployment assets (for example, the environment, libraries, assets,
    and scoring file). These assets define exactly how the model is loaded and initialized,
    how user input is parsed, how the model is executed, and how the output is passed
    back to the user. Finally, we need to choose a compute target to run the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪种用例，将机器学习模型投入生产都需要类似的准备步骤。首先，需要将训练好的模型注册到模型注册表中。这将使我们能够跟踪模型版本和二进制文件，并在部署中获取特定版本的模型。其次，我们需要指定部署资产（例如，环境、库、资产和评分文件）。这些资产定义了模型如何加载和初始化，用户输入如何解析，模型如何执行，以及输出如何返回给用户。最后，我们需要选择一个计算目标来运行模型。
- en: 'When using Azure Machine Learning for deployments, there is a well-defined
    list of things you need to specify in order to deploy and run an ML model as a
    web service. This list includes the following components:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure Machine Learning 进行部署时，你需要明确指定一系列事项，以便将机器学习模型作为 Web 服务部署和运行。这个列表包括以下组件：
- en: '**A trained model**: The model definition and parameters'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练好的模型**：模型定义和参数'
- en: '**An inferencing environment**: A configuration describing the environment,
    for example, as a Docker file'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理环境**：描述环境的配置，例如 Docker 文件'
- en: '**A scoring file**: The web service code to parse user inputs and outputs and
    invoke the model'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评分文件**：用于解析用户输入和输出并调用模型的 Web 服务代码'
- en: '**A runtime**: The runtime for the scoring file, for example, Python or PySpark'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时**：评分文件的运行时，例如 Python 或 PySpark'
- en: '**A compute target**: The compute environment to run the web service, for example,
    **Azure Kubernetes Service** (**AKS**) or **Azure Container Instances** (**ACI**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算目标**：运行 Web 服务的计算环境，例如 **Azure Kubernetes 服务**（**AKS**）或 **Azure 容器实例**（**ACI**）'
- en: 'Let''s look into these five components in more detail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨这五个组件：
- en: First, we need a trained model. A model (depending on the framework, libraries,
    and algorithm used) consists of one or multiple files storing the model parameters
    and structure. In scikit-learn, this could be a pickled estimator; in **LightGBM**,
    this could be a serialized list of decision trees; and in Keras, this could be
    a model definition and a binary blob storing the model weights. We call this the
    *model*, and we store and version it in Blob storage. At the startup time of your
    scoring service, the model will be loaded into the scoring runtime.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要一个训练好的模型。一个模型（根据所使用的框架、库和算法）可能由一个或多个存储模型参数和结构的文件组成。在 scikit-learn 中，这可能是一个序列化的估计器；在
    **LightGBM** 中，这可能是一系列决策树的序列化列表；而在 Keras 中，这可能是一个模型定义和一个存储模型权重的二进制 blob。我们称之为“模型”，并将其存储和版本化在
    Blob 存储中。在评分服务的启动时间，模型将被加载到评分运行时中。
- en: Besides the model, we also need an execution environment, which can be defined
    via `InferenceConfig`. In Azure Machine Learning deployments, the environment
    will be built into a *Docker* image and stored in your private Docker registry.
    During the deployment process, Azure Machine Learning will automatically build
    the Docker image from the provided environment configuration and load it into
    the private registry in your workspace.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了模型之外，我们还需要一个执行环境，这可以通过`InferenceConfig`来定义。在Azure机器学习部署中，环境将被构建成一个*Docker*镜像，并存储在您的私有Docker注册库中。在部署过程中，Azure机器学习将自动从提供的环境配置中构建Docker镜像，并将其加载到您工作区中的私有注册库中。
- en: In Azure Machine Learning deployments, you can select predefined ML environments
    or configure your own environments and Docker base images. On top of the base
    image, you can define a list of Python *Pip* or *Conda* dependencies, enable GPU
    support, or configure custom Docker steps. The environment, including all required
    packages, will automatically be provided during runtime and set up on the Docker
    image. On top of this, the environment can be registered and versioned by the
    Azure Machine Learning service. This makes it easy to track, reuse, and organize
    your deployment environments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习部署中，您可以选择预定义的ML环境或配置自己的环境和Docker基础镜像。在基础镜像之上，您可以定义一个Python *Pip* 或
    *Conda* 依赖项列表，启用GPU支持，或配置自定义Docker步骤。环境，包括所有必需的包，将在运行时自动提供，并在Docker镜像上设置。在此基础上，环境还可以由Azure机器学习服务注册和版本化。这使得跟踪、重用和组织您的部署环境变得容易。
- en: Next, we need a so-called scoring file. This file typically loads the model
    and provides a function to score the model when given some data as input. Depending
    on the type of deployment, you need to provide a scoring file for either a (real-time)
    synchronous scoring service or an asynchronous batch-scoring service. The scoring
    files should be tracked in your version control system and will be mounted in
    the Docker image.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个所谓的评分文件。这个文件通常加载模型，并提供一个函数，当给定一些数据作为输入时，对模型进行评分。根据部署的类型，您需要为（实时）同步评分服务或异步批量评分服务提供评分文件。评分文件应在您的版本控制系统中进行跟踪，并将被挂载到Docker镜像中。
- en: 'To complete `InferenceConfig`, we are missing one last but important step:
    the Python runtime, used to run your scoring file. Currently, Python and PySpark
    are the only supported runtimes.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要完成`InferenceConfig`，我们缺少最后但非常重要的一步：用于运行您的评分文件的Python运行时。目前，Python和PySpark是唯一支持的运行时。
- en: Finally, we need an execution target that defines the compute infrastructure
    that the Docker image should be executed on. In Azure, this is called the compute
    target and is defined through the deployment configuration. The compute target
    can be a managed Kubernetes cluster (such as AKS), a container instance (such
    as ACI), **Azure Machine Learning Compute** (**AmlCompute**), or one of the many
    other Azure compute services.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要一个执行目标，它定义了Docker镜像应该在其上执行的计算基础设施。在Azure中，这被称为计算目标，并通过部署配置来定义。计算目标可以是管理的Kubernetes集群（如AKS）、容器实例（如ACI）、**Azure机器学习计算**（**AmlCompute**）或许多其他Azure计算服务之一。
- en: Important Note
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The preceding components are only required for managed deployments within Azure
    Machine Learning. Nothing prevents you from fetching the model binaries in another
    environment or running an inferencing environment (the Docker image) on your on-premises
    compute target.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的组件仅适用于Azure机器学习内的托管部署。没有任何东西阻止您在其他环境中检索模型二进制文件或在您的本地计算目标上运行推理环境（Docker镜像）。
- en: If you simply want to deploy a standard model file, such as scikit-learn, **ONNX**,
    or TensorFlow models, you can also use the built-in *auto-deployment* capabilities
    in Azure Machine Learning. Instead of providing all the preceding components,
    auto-deployment requires only the name and version of the used framework and a
    resource configuration, for example, the number of CPUs and the amount of RAM
    to execute. Azure Machine Learning will do the rest; it will provide all the required
    configurations and deploy the model to an ACI. This makes it easy to deploy standard
    models with no more than one line of code – great for development, debugging,
    and testing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想部署标准模型文件，例如scikit-learn、**ONNX**或TensorFlow模型，您也可以使用Azure Machine Learning中内置的*自动部署*功能。自动部署不需要提供所有前面的组件，只需提供所使用框架的名称和版本以及资源配置，例如执行所需的CPU数量和RAM量。Azure
    Machine Learning将完成其余工作；它将提供所有必需的配置并将模型部署到ACI。这使得使用不超过一行代码即可轻松部署标准模型——非常适合开发、调试和测试。
- en: Now that we know the basic deployment components in Azure Machine Learning,
    we can move on and look at an example of registering a model to prepare it for
    deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Azure Machine Learning中的基本部署组件，我们可以继续并查看一个注册模型以准备部署的示例。
- en: Registering your models in a model registry
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在模型注册表中注册您的模型
- en: The first step of the deployment process should happen during or after the training
    and optimization process, namely *registering* the best model from each run in
    the Azure Machine Learning model registry. Independent of whether your training
    script produces a single model, a model ensemble, or a model combined with multiple
    files, you should always store the training artifacts and register the best model
    from each run in your Azure Machine Learning workspace.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程的第一步应该在训练和优化过程期间或之后发生，即在每个运行中注册Azure Machine Learning模型注册表中的最佳模型。无论您的训练脚本生成单个模型、模型集成还是与多个文件结合的模型，您都应该始终存储训练工件并在Azure
    Machine Learning工作区中注册每个运行的最佳模型。
- en: It takes one additional line of code in your training script to store a model
    and register it in Azure Machine Learning and, therefore, never lose your training
    artifacts and models. The Blob storage and model registry are directly integrated
    with your workspace and so the process is tightly integrated into the training
    process. Once a model is registered, Azure Machine Learning provides a convenient
    interface to load the model from the registry.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的训练脚本中，只需额外一行代码即可将模型存储在Azure Machine Learning中并注册，因此您永远不会丢失训练的工件和模型。Blob存储和模型注册直接集成到您的工作区中，因此该过程紧密集成到训练过程中。一旦模型注册，Azure
    Machine Learning提供了一个方便的界面，可以从注册表中加载模型。
- en: 'Let''s take a quick look at what this means for your training script:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下这对您的训练脚本意味着什么：
- en: 'Let''s define the run context and train the `sklearn` classifier:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义运行上下文并训练`sklearn`分类器：
- en: '[PRE0]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we write a small helper function that returns the best test accuracy
    metric from all previous runs. We will use this metric to check whether the new
    model performs better than all previous runs:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个小的辅助函数，该函数从所有之前的运行中返回最佳测试准确度指标。我们将使用此指标来检查新模型是否优于所有之前的运行：
- en: '[PRE1]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we check whether the model has better performance than all previous runs
    and register it in the model factory as a new version:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检查模型是否比所有之前的运行表现更好，并将其作为新版本注册到模型工厂中：
- en: '[PRE2]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code block, we first use the `joblib.dump``()` function to
    serialize and store a trained classifier to disk. We then call the `run.model_register()`
    function to upload the trained model to the default datastore and register the
    model to the disk. This will automatically track and version the model by name
    and link it to the current training run.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们首先使用`joblib.dump()`函数将训练好的分类器序列化并存储到磁盘上。然后我们调用`run.model_register()`函数将训练好的模型上传到默认数据存储并将模型注册到磁盘上。这将自动通过名称跟踪和版本控制模型，并将其链接到当前的训练运行。
- en: 'Once your model is stored in the model registry of your Azure Machine Learning
    workspace, you can use it for deployments and retrieve it by name in any debugging,
    testing, or experimentation step. You can simply request the latest model by name,
    for example, by running the following snippet on your local machine:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您的模型存储在Azure Machine Learning工作区的模型注册表中，您就可以用于部署，并在任何调试、测试或实验步骤中通过名称检索它。您只需通过在本地机器上运行以下代码片段即可简单地请求最新的模型：
- en: '[PRE3]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All we did in the preceding code is run `Model.get_model_path()` to retrieve
    the latest version of a model by name. We can also specify a version number to
    load a specific model from the registry.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只是运行`Model.get_model_path()`来通过名称检索模型的最新版本。我们也可以指定版本号来从注册库中加载特定模型。
- en: A built-in model registry is one of the functionalities of the Azure Machine
    Learning workspace that gets you hooked and makes you never want to miss a model
    registry, experiment run, and metrics tracking in the future. It gives you great
    flexibility and transparency when working with model artifacts in different environments
    and during different experiments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 内置模型注册库是Azure Machine Learning工作区的一项功能，它让您上瘾，并使您在未来的模型注册、实验运行和指标跟踪中永远不会错过。它在使用不同环境中的模型工件以及在不同实验期间工作时提供了极大的灵活性和透明度。
- en: In the preceding example, we didn't provide any metadata about the trained model
    and, therefore, Azure Machine Learning couldn't infer anything from the model
    artifact. However, if we provide additional information about the model, Azure
    Machine Learning can autogenerate some of the required deployment configurations
    for you to enable auto-deployments. Let's take a look at this in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们没有提供关于训练模型的任何元数据，因此Azure Machine Learning无法从模型工件中推断出任何信息。然而，如果我们提供有关模型的额外信息，Azure
    Machine Learning可以自动为您生成一些所需的部署配置，以便您启用自动部署。让我们在下一节中看看这一点。
- en: Auto-deployments of registered models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 已注册模型的自动部署
- en: If you stick to the standard functionality provided in scikit-learn, TensorFlow,
    or ONNX, you can also take advantage of auto-deployments in Azure Machine Learning.
    This will allow you to deploy registered models to testing, experimentation, or
    production environments without defining any of the required deployment configurations,
    assets, and service endpoints.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您坚持使用scikit-learn、TensorFlow或ONNX提供的标准功能，您也可以在Azure Machine Learning中利用自动部署。这将允许您将注册的模型部署到测试、实验或生产环境，而无需定义任何所需的部署配置、资产和服务端点。
- en: Important Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Azure Machine Learning model auto-deployment will automatically make your model
    available as a web service. If you provide model metadata during training, you
    can invoke auto-deployment using a single command, `Model.deploy()`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning模型的自动部署将自动将您的模型作为Web服务提供。如果您在训练期间提供了模型元数据，您可以使用单个命令`Model.deploy()`来调用自动部署。
- en: 'Let''s take a look at how we need to change the previous example to take advantage
    of auto-deployments:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何修改前面的示例以利用自动部署：
- en: 'First, we define the resource configuration of the model as shown in the following
    code block:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义模型的资源配置，如下面的代码块所示：
- en: '[PRE4]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we need to define the framework and framework version when registering
    the model. To do so, we need to add this additional information to the model by
    extending the `Model.register()` arguments, as shown in the following snippet:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要在注册模型时定义框架和框架版本。为此，我们需要通过扩展`Model.register()`参数来向模型添加此附加信息，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we added the framework and framework version to the model
    registry, as well as the resource configuration for this specific model. The model
    itself is stored in a standard format in one of the supported frameworks (scikit-learn,
    ONNX, or TensorFlow). This metadata is added to the model in the model registry.
    This is all the configuration required to auto-deploy this model as a real-time
    web service in a single line of code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将框架和框架版本添加到模型注册库中，以及为此特定模型配置的资源。模型本身以标准格式存储在支持的框架之一（scikit-learn、ONNX或TensorFlow）中。这些元数据被添加到模型注册库中的模型中。这是自动部署此模型作为实时Web服务所需的全部配置，只需一行代码即可完成。
- en: 'Finally, we call the `Model.deploy()` function to start the deployment process.
    This will build the deployment runtime as a Docker image, register it in your
    container registry, and start the image as a managed container instance, including
    the scoring file, REST service abstraction, and telemetry collection:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们调用`Model.deploy()`函数来启动部署过程。这将构建部署运行时作为Docker镜像，将其注册到您的容器注册库中，并以托管容器实例的形式启动镜像，包括评分文件、REST服务抽象和遥测收集：
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To retrieve the URL of the scoring service once the deployment is finished,
    we run the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，要检索评分服务的URL，我们运行以下代码：
- en: '[PRE7]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you want more granular control over the execution environment, endpoint configuration,
    and compute target, you can use the advanced inference, deployment, and service
    configurations in order to customize your deployment. Let's now take a look at
    customized deployments.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要对执行环境、端点配置和计算目标有更细粒度的控制，您可以使用高级推理、部署和服务配置来定制您的部署。现在让我们看看定制部署。
- en: Customizing your deployment environment
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义部署环境
- en: As you have seen in the previous chapters, the number of libraries, frameworks,
    and customization steps to transform data with an ML model is huge. Azure Machine
    Learning gives us enough flexibility to configure ML scoring services that can
    reflect these customizations. In this section, we will learn how to customize
    the deployment to include libraries and frameworks. Let's dive a bit deeper into
    these individual deployment steps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前几章中看到的，使用ML模型转换数据所需的库、框架和自定义步骤数量巨大。Azure机器学习为我们提供了足够的灵活性来配置ML评分服务，以反映这些自定义设置。在本节中，我们将学习如何自定义部署以包括库和框架。让我们更深入地探讨这些单独的部署步骤。
- en: In the Azure Machine Learning service, you use an execution environment to specify
    a base Docker image, Python runtime, and all the dependent packages required to
    score your model. Like models, environments can also be registered and versioned
    in Azure, so both the Docker artifacts and the metadata are stored, versioned,
    and tracked in your workspace. This makes it simple to keep track of your environment
    changes, figure out which environment was used for a specific run, jump back and
    forth between multiple versions of an environment, and share an environment for
    multiple projects.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习服务中，您使用执行环境来指定基础Docker镜像、Python运行时以及评分模型所需的所有依赖包。与模型一样，环境也可以在Azure中注册和版本控制，因此Docker工件和元数据都存储、版本控制和跟踪在您的工作区中。这使得跟踪环境更改变得简单，可以找出特定运行所使用的环境，在多个环境版本之间来回切换，以及为多个项目共享环境。
- en: 'Perform the following steps to build and package your deployment in Docker:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在Docker中构建和打包您的部署：
- en: 'Let''s start by writing a helper function to create environments on the fly.
    This snippet is very useful when creating environments programmatically based
    on a list of packages. We will also automatically add the `azureml-defaults` package
    to each environment:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从编写一个辅助函数开始，该函数可以动态创建环境。当根据包列表程序化创建环境时，此代码片段非常有用。我们还将自动将`azureml-defaults`包添加到每个环境中：
- en: '[PRE8]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see in the preceding code block, we first initialize an `Environment`
    instance and then add multiple `conda` packages. We assign the `conda` dependencies
    by overriding the `env.python.conda_dependencies` property with the `conda_deps`
    dependencies. Using the same approach, we can also override Docker, Spark, and
    any additional Python settings using `env.docker` and `env.spark`, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的代码块中所看到的，我们首先初始化一个`Environment`实例，然后添加多个`conda`包。我们通过覆盖`env.python.conda_dependencies`属性来分配`conda`依赖项，该属性使用`conda_deps`依赖项。使用相同的方法，我们还可以分别使用`env.docker`和`env.spark`覆盖Docker、Spark以及任何额外的Python设置。
- en: 'Next, we can define a custom environment to use for experimentation, training,
    or deployment:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义一个用于实验、训练或部署的自定义环境：
- en: '[PRE9]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the next step, you can now register the environment using a descriptive
    name. This will add a new version of the current environment configuration to
    your environment with the same name:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，您现在可以使用描述性名称注册环境。这将添加当前环境配置的新版本到具有相同名称的环境：
- en: '[PRE10]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also retrieve the environment from the registry using the following
    code. This is also useful when you have registered a base environment that can
    be reused and extended for multiple experiments:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用以下代码从注册表中检索环境。当您已注册一个可以重复使用和扩展到多个实验的基础环境时，这也很有用：
- en: '[PRE11]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As with the `model` registry, you can also load environments using a specified
    version as an additional argument. Once you have configured an execution environment,
    you can combine it with a scoring file to an `InferenceConfig` object. The scoring
    file implements all functionalities to load the model from the registry and evaluate
    it given some input data. The configuration can be defined as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与`model`注册表一样，您还可以使用指定的版本作为附加参数来加载环境。一旦您配置了执行环境，您就可以将其与评分文件组合成一个`InferenceConfig`对象。评分文件实现了从注册表中加载模型并针对一些输入数据进行评估的所有功能。配置可以定义如下：
- en: '[PRE12]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see, in the preceding example, that we simply specify a relative path
    to the scoring script in the local authoring environment. Therefore, you first
    have to create this scoring file; we will go through two examples of batch and
    real-time scoring in the following sections.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们可以看到我们只是指定了在本地创作环境中评分脚本的相对路径。因此，您首先必须创建此评分文件；在接下来的几节中，我们将通过批处理和实时评分的示例进行说明。
- en: 'To build an environment, we can simply trigger a build of the Docker image:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要构建环境，我们可以简单地触发Docker镜像的构建：
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The environment will be packaged and registered as a Docker image in your private
    container registry, containing the Docker base image and all specified libraries.
    If you want to package the model and the scoring file, you can package the model
    instead. This is done automatically when deploying the model or can be forced
    by using the `Model.package` function. Let''s load the model from the previous
    section and package and register the image:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境将被打包并注册为您的私有容器注册库中的Docker镜像，其中包含Docker基础镜像和所有指定的库。如果您想打包模型和评分文件，您可以只打包模型。这可以在部署模型时自动完成，也可以通过使用`Model.package`函数强制执行。让我们从上一节加载模型并打包和注册镜像：
- en: '[PRE14]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important Note
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Azure ML SDK documentation contains a detailed list of possible configuration
    options, which you can find at [https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)).
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Azure ML SDK文档包含一个可能的配置选项的详细列表，您可以在[https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class))找到。
- en: The preceding code will build and package your deployment as a Docker image.
    In the next section, we will find out how to choose the best compute target to
    execute your ML deployment.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将构建并打包您的部署为Docker镜像。在下一节中，我们将了解如何选择最佳的计算目标来执行您的ML部署。
- en: Choosing a deployment target in Azure
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Azure中选择部署目标
- en: One of the great advantages of Azure Machine Learning services is that they
    are tightly integrated with many other Azure services. This is extremely helpful
    with deployments where we want to run Docker images of the ML service on a managed
    service within Azure. These compute targets can be configured and leveraged for
    automatic deployment through Azure Machine Learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习服务的一个巨大优势是它们与许多其他Azure服务紧密集成。这对于我们希望在Azure托管服务中运行ML服务的Docker镜像的部署非常有帮助。这些计算目标可以配置并利用Azure机器学习进行自动部署。
- en: If your job is to productionize ML training and deployment pipelines, you might
    not necessarily be an expert in Kubernetes. If that's the case, you might come
    to enjoy the tight integration of the management of Azure compute services in
    the Azure Machine Learning SDK. Similar to creating training environments, you
    can create GPU clusters, managed Kubernetes clusters, or simple container instances
    from within the authoring environment (for example, the Jupyter notebook orchestrating
    your ML workflow).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的任务是生产化ML训练和部署管道，您可能不一定是一位Kubernetes专家。如果是这样，您可能会喜欢Azure机器学习SDK中Azure计算服务管理的紧密集成。类似于创建训练环境，您可以在创作环境中（例如，Jupyter笔记本编排您的ML工作流程）创建GPU集群、托管Kubernetes集群或简单的容器实例。
- en: We can follow a general recommendation for choosing a specific service, similar
    to choosing a compute service for regular application deployments; so, we trade
    off simplicity, cost, scalability, flexibility, and operational expense between
    the compute services that can easily start a web service from a Docker image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以遵循选择特定服务的一般建议，类似于选择计算服务进行常规应用程序部署；因此，我们在可以轻松从Docker镜像启动Web服务的计算服务之间权衡简单性、成本、可扩展性、灵活性和运营成本。
- en: 'Here are recommendations of when to use each Azure compute service:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关于何时使用每个Azure计算服务的建议：
- en: For quick experiments and local testing, use Docker and local deployment targets
    in Azure Machine Learning.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于快速实验和本地测试，请使用Docker和Azure机器学习中的本地部署目标。
- en: For testing and experimentation, use ACI. It is easy to set up and configure,
    and it is made to run container images.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试和实验，使用ACI。它易于设置和配置，并且是为了运行容器镜像而设计的。
- en: For deployments of scalable real-time web services with GPU support, use AKS.
    This managed Kubernetes cluster is a lot more flexible and scalable, but also
    a lot harder to operate.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于需要 GPU 支持的可扩展实时 Web 服务的部署，请使用 AKS。这个托管的 Kubernetes 集群更加灵活和可扩展，但操作起来也更困难。
- en: For batch deployments, use Azure Machine Learning clusters, the same compute
    cluster environment we already used for training.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于批量部署，使用 Azure Machine Learning 集群，这是我们之前已经用于训练的计算集群环境。
- en: 'For quick experiments, you can deploy your service locally using `LocalWebservice`
    as a deployment target. To do so, you can run the following snippet on your local
    machine, providing the scoring file and environment in the inferencing configuration:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于快速实验，您可以使用 `LocalWebservice` 作为部署目标在本地部署您的服务。为此，您可以在本地机器上运行以下代码片段，提供推理配置中的评分文件和环境：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, once your model is registered, you can deploy it to multiple
    compute targets depending on your use case. While we have covered a few different
    configuration options, we haven't yet discussed multiple deployment options and
    scoring files. We will do this in the next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一旦您的模型注册成功，您可以根据您的用例将其部署到多个计算目标。虽然我们已经涵盖了几个不同的配置选项，但我们还没有讨论多个部署选项和评分文件。我们将在下一节中这样做。
- en: Deploying ML models in Azure
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Azure 中部署 ML 模型
- en: Broadly speaking, there are two common approaches to deploying ML models, namely
    deploying them as synchronous real-time web services and as asynchronous batch-scoring
    services. Please note that the same model could be deployed as two different services,
    serving different use cases. The deployment type depends heavily on the batch
    size and response time of the scoring pattern of the model. Small batch sizes
    with fast responses require a horizontally scalable real-time web service, whereas
    large batch sizes and slow response times require horizontally and vertically
    scalable batch services.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，部署 ML 模型有两种常见的方法，即作为同步实时 Web 服务和异步批量评分服务部署。请注意，同一个模型可以作为两个不同的服务部署，服务于不同的用例。部署类型在很大程度上取决于模型评分模式的批量大小和响应时间。小批量大小和快速响应需要水平可扩展的实时
    Web 服务，而大批量大小和慢速响应时间则需要水平和垂直可扩展的批量服务。
- en: The deployment of a text-understanding model (for example, an entity recognition
    model or sentiment analysis) could include a real-time web service that evaluates
    the model whenever a new comment is posted to an app, as well as a batch scorer
    in another ML pipeline to extract relevant features from training data. With the
    former, we want to serve each request as quickly as possible, and so we will evaluate
    a small batch size synchronously. With the latter, we are evaluating large amounts
    of data, and so we will evaluate a large batch size asynchronously. Our aim is
    that, once the model is packaged and registered, we can reuse it for either a
    task or use case.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 文本理解模型（例如，实体识别模型或情感分析模型）的部署可能包括一个实时 Web 服务，该服务在应用中发布新评论时评估模型，以及另一个 ML 管道中的批量评分器，用于从训练数据中提取相关特征。对于前者，我们希望尽可能快地处理每个请求，因此我们将同步评估小批量。对于后者，我们正在评估大量数据，因此我们将异步评估大批量。我们的目标是，一旦模型打包并注册，我们就可以将其用于任何任务或用例。
- en: In this section, we will take a look at these deployment approaches and build
    one service for real-time scoring and one for batch-scoring. We will also evaluate
    different options to manage and perform deployments for scoring services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看这些部署方法，并构建一个用于实时评分的服务和一个用于批量评分的服务。我们还将评估不同的选项来管理和执行评分服务的部署。
- en: Building a real-time scoring service
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建实时评分服务
- en: In this section, we will build a real-time scoring service in Azure Machine
    Learning. We will look into the required scoring file that will power the web
    service, as well as the configuration to start the service on an AKS cluster.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个 Azure Machine Learning 中的实时评分服务。我们将探讨支持该 Web 服务的所需评分文件，以及启动 AKS
    集群上服务的配置。
- en: For this example, we will train an NLP Hugging Face transformer model to perform
    sentiment analysis on user input. Our aim is to build our own Cognitive Services
    Text Analytics API that uses a custom model that is trained or fine-tuned on a
    custom dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将训练一个 NLP Hugging Face transformer 模型，用于对用户输入进行情感分析。我们的目标是构建我们自己的 Cognitive
    Services Text Analytics API，该 API 使用在自定义数据集上训练或微调的自定义模型。
- en: 'To do so, we will train a sentiment analysis pipeline, save it, and register
    it as a model in Azure Machine Learning, as shown in the following snippet:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将训练一个情感分析管道，将其保存，并在Azure Machine Learning中将其注册为模型，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have the model, we start building the web service by taking a look
    at the scoring file. The scoring file will be loaded when the web service starts
    and gets invoked for every request to the ML service. Therefore, we use the scoring
    file to load the ML model, parse the user data from a request, invoke the ML model,
    and return the results of the ML model. To do so, you need to provide the `init()`
    and `run()` functions in the scoring file, where the `run()` function is run once
    when the service starts, and the `run` method is invoked with user inputs for
    every request. The following example shows a simple scoring file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了模型，我们就通过查看评分文件开始构建Web服务。评分文件将在Web服务启动时加载，并且对于每个对ML服务的请求都会被调用。因此，我们使用评分文件来加载ML模型，解析请求中的用户数据，调用ML模型，并返回ML模型的预测结果。为此，你需要在评分文件中提供`init()`和`run()`函数，其中`run()`函数在服务启动时运行一次，并且对于每个请求，都会使用用户输入调用`run`方法。以下是一个简单的评分文件示例：
- en: scoring_file_example.py
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: scoring_file_example.py
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have the trained model and we know the structure of the scoring
    file, we can go ahead and build our custom web service:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练好的模型，并且我们知道评分文件的结构，我们可以继续构建我们的自定义Web服务：
- en: 'Let''s start with the initialization of the service. We first define a global
    model variable, and then fetch the model path from the `AZUREML_MODEL_DIR` environment
    variable. This variable contains the location of the model on the local disk.
    Next, we load the model using the Hugging Face `AutoModel` transformer:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从服务的初始化开始。我们首先定义一个全局模型变量，然后从`AZUREML_MODEL_DIR`环境变量中获取模型路径。这个变量包含模型在本地磁盘上的位置。接下来，我们使用Hugging
    Face的`AutoModel`转换器加载模型：
- en: Scoring_file.py
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Scoring_file.py
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we tackle the actual inferencing part of the web service. To do so, we
    need to parse incoming requests, invoke the NLP model, and return the prediction
    to the caller:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们处理Web服务的实际推理部分。为此，我们需要解析传入的请求，调用NLP模型，并将预测结果返回给调用者：
- en: Scoring_file.py
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Scoring_file.py
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the `run()` function, we are provided with a `request` object. This object
    contains the body of the request sent to the service. As we expect JSON input,
    we parse the request body as a JSON object and access the input string via the
    `query` property. We expect a client to send a valid request that contains exactly
    this schema. Finally, we return a prediction that will be automatically serialized
    into JSON and returned to the caller.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在`run()`函数中，我们提供了一个`request`对象。该对象包含发送到服务的请求正文。由于我们期望JSON输入，我们将请求正文解析为JSON对象，并通过`query`属性访问输入字符串。我们期望客户端发送一个包含此架构的有效请求。最后，我们返回一个预测，它将被自动序列化为JSON并返回给调用者。
- en: 'Let''s deploy the service to an ACI compute target for testing purposes. To
    do so, we need to update the deployment configuration to contain the ACI resource
    configuration:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试目的，让我们将服务部署到ACI计算目标。为此，我们需要更新部署配置以包含ACI资源配置：
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Important Note
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find more information about Azure Container Instance in the official
    documentation at [https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview).
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在官方文档中找到有关Azure Container Instance的更多信息：[https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview)。
- en: 'Next, we pass the environment and scoring file to the inferencing configuration:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将环境和评分文件传递给推理配置：
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Having all the required components, we can finally pass the model, the inferencing
    configuration, and the deployment configuration to the `Model.deploy` method and
    start the deployment:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拥有所有必需组件后，我们最终可以将模型、推理配置和部署配置传递给`Model.deploy`方法并开始部署：
- en: '[PRE22]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once the service is up and running, we can try a test request to the service
    to make sure everything is working properly. By default, Azure Machine Learning
    services use key-based (primary and secondary) authentication. Let''s retrieve
    the key from the service and send some test data to the deployed service:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务启动并运行，我们可以尝试向服务发送测试请求，以确保一切正常工作。默认情况下，Azure Machine Learning服务使用基于密钥的（主和辅助）身份验证。让我们从服务中检索密钥，并发送一些测试数据到部署的服务：
- en: '[PRE23]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding snippet fetches the service URL and access key and sends the JSON
    encoded data to the ML model deployment as a `POST` request.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段获取服务 URL 和访问密钥，并将 JSON 编码的数据作为 `POST` 请求发送到 ML 模型部署。
- en: That's it! You have deployed your sentiment analysis model successfully and
    tested it from Python. However, using the service endpoint and token, you can
    also send requests from any other programming language or HTTP client to your
    service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！您已成功部署了情感分析模型，并从 Python 中进行了测试。然而，使用服务端点和令牌，您还可以从任何其他编程语言或 HTTP 客户端向您的服务发送请求。
- en: Deploying to Azure Kubernetes Services
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署到 Azure Kubernetes 服务
- en: We have successfully deployed our sentiment analysis model to ACI. As a next
    step, however, we want to deploy it to AKS. While ACI is fantastic for quickly
    getting Docker containers deployed, AKS is a service for complex container-based
    production workloads. Among other features, AKS supports authentication, autoscaling,
    GPU support, replicas, and advanced metrics and logging.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功将情感分析模型部署到 ACI。然而，作为下一步，我们希望将其部署到 AKS。虽然 ACI 对于快速部署 Docker 容器非常出色，但 AKS
    是一个用于复杂基于容器的生产工作负载的服务。AKS 支持认证、自动扩展、GPU 支持、副本以及高级指标和日志等功能。
- en: Important Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find more information about Azure Kubernetes Services in the official
    documentation at [https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方文档中找到有关 Azure Kubernetes 服务（AKS）的更多信息：[https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes)。
- en: 'Let''s now deploy this service to an AKS cluster so we can take advantage of
    the GPU acceleration and autoscaling:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将此服务部署到 AKS 集群，以便我们可以利用 GPU 加速和自动扩展：
- en: 'First, we need to define our required infrastructure:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要定义我们所需的基础设施：
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, we created an AKS configuration and a new AKS cluster
    as an Azure Machine Learning compute target from this configuration. All this
    happens completely within your authoring environment.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了一个 AKS 配置和一个新的 AKS 集群，作为 Azure Machine Learning 计算目标。所有这些都在您的创作环境中完成。
- en: 'If you already have an AKS cluster up and running, you can simply use this
    cluster for Azure Machine Learning. To do so, you have to pass the resource group
    and cluster name to the `AksCompute.attach_configuration()` method. Then, set
    the resource group that contains the AKS cluster and the cluster name:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您已经有一个正在运行的 AKS 集群，您可以直接使用此集群进行 Azure Machine Learning。为此，您必须将资源组和集群名称传递给
    `AksCompute.attach_configuration()` 方法。然后，设置包含 AKS 集群的资源组和集群名称：
- en: '[PRE25]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once we have a reference to the cluster, we can deploy the ML model to the
    cluster. This step is similar to the previous one:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了集群的引用，我们就可以将 ML 模型部署到集群。这一步骤与上一个步骤类似：
- en: '[PRE26]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see in the preceding example, apart from attaching the AKS clusters
    as a target to Azure Machine Learning, the model deployment is identical to the
    example using ACI.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，除了将 AKS 集群作为目标附加到 Azure Machine Learning 外，模型部署与使用 ACI 的示例相同。
- en: Defining a schema for scoring endpoints
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义评分端点的模式
- en: In the previous example, we parse the user input from JSON and expect it to
    contain a `query` parameter. To help users and services consuming your service
    endpoint, it would be useful to tell users which parameters the service is expecting.
    This is a common problem when building web service APIs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们从 JSON 解析用户输入并期望它包含一个 `query` 参数。为了帮助使用您的服务端点的用户和服务，告诉用户服务期望哪些参数将是有用的。这在构建
    Web 服务 API 时是一个常见问题。
- en: To solve this, Azure Machine Learning provides an innovative way to autogenerate
    an **OpenAPI Specification** (**OAS**), previously called the **Swagger Specification**.
    This specification can be accessed by consumers of the API through the schema
    endpoint. This provides an automated standardized way to specify and consume the
    service's data format and can be used to autogenerate clients. One example is
    **Swagger Codegen**, which can be used to generate Java and C# clients for your
    new ML service.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Azure Machine Learning 提供了一种创新的方法来自动生成 **OpenAPI 规范**（**OAS**），之前称为
    **Swagger 规范**。此规范可以通过模式端点被 API 的消费者访问。这提供了一种自动化的标准化方式来指定和消费服务的数据格式，并且可以用于自动生成客户端。一个例子是
    **Swagger Codegen**，它可以用来为您的新 ML 服务生成 Java 和 C# 客户端。
- en: 'You can enable automatic schema generation for pandas, NumPy, PySpark, and
    standard Python objects in your service through annotations in Python. First,
    you need to include `azureml-defaults` and `inference-schema` as PIP packages
    in your environment. Then, you can autogenerate the schema by providing sample
    input and output data for your endpoint, as shown in the following example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在Python中添加注释来为您的服务启用pandas、NumPy、PySpark和标准Python对象的自动模式生成。首先，您需要将`azureml-defaults`和`inference-schema`作为PIP包包含到您的环境中。然后，您可以通过为端点提供样本输入和输出数据来自动生成模式，如下面的示例所示：
- en: scoring_file.py
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: scoring_file.py
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding example, we defined the schema for a NumPy-based model through
    sample data and annotations in the `run()` method.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们通过`run()`方法中的样本数据和注释定义了基于NumPy的模型的模式。
- en: 'We can also pick up the sentiment analysis model and allow it to receive multiple
    input queries. To do this, we can deserialize the user input into a pandas DataFrame
    object and return an array of predictions as a result, as shown in the following
    example. Note that this basically adds batch prediction capabilities to our real-time
    web service:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择情感分析模型，并允许它接收多个输入查询。为此，我们可以将用户输入反序列化为pandas DataFrame对象，并返回一个预测数组作为结果，如下面的示例所示。请注意，这基本上为我们实时Web服务添加了批量预测功能：
- en: scoring_file.py
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: scoring_file.py
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Defining example inputs and outputs is everything that is required to autogenerate
    an API specification that your clients can use to validate endpoints and arguments
    or to autogenerate clients. This is also the same format that can be used to create
    ML services that can be automatically integrated into Power BI, as shown in [*Chapter
    15*](B17928_15_ePub.xhtml#_idTextAnchor238), *Model Interoperability, Hardware
    Optimization, and Integrations*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 定义示例输入和输出是自动生成API规范所需的一切，您的客户端可以使用该规范来验证端点和参数，或自动生成客户端。这同样是创建可以自动集成到Power BI的ML服务的格式，如[第15章](B17928_15_ePub.xhtml#_idTextAnchor238)中所示，*模型互操作性、硬件优化和集成*。
- en: Managing model endpoints
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理模型端点
- en: Each model deployment contains a URL to send requests to the model; online scoring
    services provide a URL to process online predictions, and batch-scoring services
    provide a URL to trigger batch predictions. While this makes it easy to spin up
    and query a service, one big problem remains during a deployment, namely, that
    the service URL changes with each deployment. This leads to the issue that we
    can't control which service a user request will hit.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型部署都包含一个用于发送请求到模型的URL；在线评分服务提供用于处理在线预测的URL，批量评分服务提供用于触发批量预测的URL。虽然这使得启动和查询服务变得容易，但在部署过程中仍然存在一个重大问题，即服务URL会随着每次部署而改变。这导致了一个问题，即我们无法控制用户请求将击中哪个服务。
- en: To solve this problem, we need to hide model deployment URLs behind a fixed
    service URL and provide a mechanism to resolve a user request to a specific service.
    In Azure Machine Learning, the component that fulfills this is called an **endpoint**,
    which can expose multiple deployments under a fixed endpoint URL.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要在固定的服务URL后面隐藏模型部署URL，并提供一种机制将用户请求解析到特定的服务。在Azure Machine Learning中，实现这一功能的组件被称为**端点**，它可以在固定的端点URL下暴露多个部署。
- en: 'The following figure shows the concept of endpoints and deployments. Customers
    send requests to the endpoint, and we configure the endpoint to route the request
    to one of the services. During a deployment, we would add the new model version
    behind the same scoring endpoint, and incrementally start service requests from
    the new **(green)** version instead of the previous **(blue)** version:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了端点和部署的概念。客户向端点发送请求，我们配置端点将请求路由到服务之一。在部署期间，我们会在相同的评分端点后面添加新的模型版本，并逐步从新的**（绿色）**版本而不是之前的**（蓝色）**版本开始服务请求：
- en: '![Figure 14.1 – Azure Machine Learning endpoints and deployments ](img/B17928_14_001.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – Azure Machine Learning端点和部署](img/B17928_14_001.jpg)'
- en: Figure 14.1 – Azure Machine Learning endpoints and deployments
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – Azure Machine Learning端点和部署
- en: This type of deployment is also called blue-green deployment. First, you serve
    all traffic from the old service and start the new service. Once the new service
    is up and running, and the health checks have finished successfully, the service
    is registered under the endpoint, and it will start serving requests. Finally,
    if there are no active requests left on the old service, you can shut it down.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的部署也称为蓝绿部署。首先，您将所有流量从旧服务中取出，并启动新服务。一旦新服务启动并运行，并且健康检查成功完成，服务将在端点下注册，并开始处理请求。最后，如果旧服务上没有剩余的活跃请求，您可以将其关闭。
- en: This process is a very safe way to update stateless application services with
    zero or minimal downtime. It also helps you to fall back on the old service if
    the new one doesn't deploy successfully.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是一种非常安全的更新无状态应用程序服务的方法，零停机时间或最小停机时间。它还帮助您在新服务部署不成功时回滚到旧服务。
- en: 'Azure Machine Learning provides multiple types of endpoints, depending on the
    model deployment mechanism:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习提供多种类型的端点，具体取决于模型部署机制：
- en: '**Online endpoints**: For real-time online deployments:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线端点**：用于实时在线部署：'
- en: '**Managed online endpoints**: For managed Azure Machine Learning deployments'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理在线端点**：用于管理的Azure机器学习部署'
- en: '**Kubernetes online endpoints**: For managed AKS deployments'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes在线端点**：用于管理的AKS部署'
- en: '**Batch endpoints**: For batch-scoring deployments'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量端点**：用于批量评分部署'
- en: On the top level, we distinguish between online and batch endpoints. While online
    endpoints are used for synchronous scoring based on web service deployments, batch
    endpoints are used for asynchronous scoring based on pipeline deployments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶级上，我们区分在线和批量端点。在线端点用于基于Web服务部署的同步评分，而批量端点用于基于管道部署的异步评分。
- en: For online endpoints, we distinguish based on the deployment target between
    managed and Kubernetes-based online endpoints. This is an analog to the different
    compute targets and features for online scoring.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在线端点，我们根据部署目标区分管理型基于Kubernetes的在线端点。这与在线评分的不同计算目标和功能相对应。
- en: 'Let''s take a look at how to configure endpoints for AKS:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何配置AKS的端点：
- en: 'First, we configure the endpoint details as shown in the following snippet:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们配置端点详情，如下面的代码片段所示：
- en: '[PRE29]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The endpoint configuration serves as a deployment configuration for the AKS
    compute target.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 端点配置充当AKS计算目标的部署配置。
- en: 'Next, we provide both the endpoint configuration and the compute target to
    the `Model.deploy` method:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将端点配置和计算目标都提供给`Model.deploy`方法：
- en: '[PRE30]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The deployment will return an endpoint that can now be used to connect to the
    service and add additional configuration. In the next section, we will look at
    more use cases of endpoints and will see how to add additional deployments to
    the AKS endpoint.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 部署将返回一个端点，现在可以用来连接到服务并添加额外的配置。在下一节中，我们将探讨端点的更多用例，并了解如何向AKS端点添加额外的部署。
- en: Controlled rollouts and A/B testing
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制式发布和A/B测试
- en: Another benefit of endpoints is to perform controlled rollouts and incremental
    testing of new model versions. ML model deployments are similar to deployments
    of new features in application development. We might not want to roll out this
    new feature to all users at once, but first, test whether the new feature improves
    our business metrics for a small group of users.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 端点的另一个好处是执行受控发布和新模型版本的增量测试。机器学习模型部署类似于应用程序开发中新功能的部署。我们可能不想一次性将这个新功能推出给所有用户，而是首先测试新功能是否提高了我们一小部分用户的业务指标。
- en: New ML model deployments should never be uncontrolled or based on personal feelings
    or preferences; a deployment should always be based on hard metrics and real evidence.
    The best and most systematic way to test and roll out changes to your users is
    to define a key metric, roll out your new model to one section of the users (group
    B), and serve the old model to the remaining section of the users (group A). Once
    the metrics for the users in group B exceed the metrics from group A over a defined
    period, you can confidently roll out the feature to all your users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 新的机器学习模型部署绝不应该是不受控制的或基于个人感受或偏好；部署应始终基于硬性指标和真实证据。测试和向用户推出更改的最佳和最系统的方法是定义一个关键指标，将新模型推出给用户的一部分（组B），并将旧模型服务于剩余的用户部分（组A）。一旦组B中用户的指标在定义的期间内超过组A的指标，您就可以自信地将该功能推出给所有用户。
- en: 'This concept is called **A/B testing** and is used in many tech companies to
    roll out new services and features. As you can see in the following diagram, you
    split your traffic into a control group and a challenger group, where only the
    latter is served the new model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念被称为**A/B测试**，并被许多科技公司用于推广新服务和功能。正如你可以在下面的图中看到的那样，你将流量分成控制组和挑战者组，其中只有后者被提供新模型：
- en: '![Figure 14.2 – A/B testing using endpoints ](img/B17928_14_002.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 使用端点进行A/B测试](img/B17928_14_002.jpg)'
- en: Figure 14.2 – A/B testing using endpoints
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 使用端点进行A/B测试
- en: A/B testing and blue-green deployments work very well together, as they are
    really similar approaches. Both require the deployment of a fully functional service
    that is accessible to a subset of your users through routing policies. If you
    use Azure Machine Learning for your deployment and rollout strategy, you are very
    well covered. First, all deployments through Azure Machine Learning to ACI or
    AKS are blue-green deployments, which makes it easy for you to fall back on a
    previous version of your model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: A/B测试和蓝绿部署可以很好地结合使用，因为它们实际上是相似的方法。两者都需要部署一个完全功能的服务，通过路由策略使你的部分用户可以访问。如果你使用Azure
    Machine Learning进行部署和推广策略，你将得到很好的保障。首先，所有通过Azure Machine Learning到ACI或AKS的部署都是蓝绿部署，这使得你很容易回滚到模型的前一个版本。
- en: 'Azure Machine Learning deployments on AKS support up to six model versions
    behind the same endpoint to implement either blue-green deployments or A/B testing
    strategies. You can then define policies to split the traffic between these endpoints;
    for example, you can split traffic by percentage. Here is a small code example
    of how to create another version on an AKS endpoint that should serve another
    version of your model to 50% of the users:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning在AKS上的部署支持在同一端点后面最多六个模型版本，以实现蓝绿部署或A/B测试策略。然后你可以定义策略来在这些端点之间分割流量；例如，你可以按百分比分割流量。以下是一个小代码示例，说明如何在AKS端点上创建另一个版本，该版本应服务于50%的用户：
- en: 'Let''s first update the original deployment to serve as the control version
    and serve 50% of the traffic:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先更新原始部署，作为控制版本，服务于50%的流量：
- en: '[PRE31]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we add the challenger version, which is a deployment of `test_model`.
    As you can see in the following snippet, you can also supply a different inference
    configuration to the new deployment:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加挑战者版本，这是`test_model`的部署。正如你可以在下面的代码片段中看到的那样，你也可以为新部署提供不同的推理配置：
- en: '[PRE32]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we start the deployment of the updated endpoints:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们开始部署更新后的端点：
- en: '[PRE33]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we show the preview feature of controlled rollouts for
    Azure Machine Learning and AKS. We use a different combination of model and inference
    configuration to deploy a separate service under the same endpoint. The traffic
    splitting now happens automatically through routing in Kubernetes. However, in
    order to align with a previous section of this chapter, we can expect this functionality
    to improve in the future as it gets used by many customers when rolling out ML
    models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们展示了Azure Machine Learning和AKS受控发布的预览功能。我们使用不同的模型和推理配置组合，在同一个端点下部署一个独立的服务。现在，流量分割通过Kubernetes的路由自动进行。然而，为了与本章的前一部分保持一致，我们可以预期随着许多客户在推广ML模型时使用这项功能，其功能将在未来得到改进。
- en: Implementing a batch-scoring pipeline
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现批评分流程
- en: Operating batch-scoring services is very similar to the previously discussed
    online-scoring approach; you provide an environment, compute target, and scoring
    script. However, in your scoring file, you would rather pass a path to a Blob
    storage location with a new batch of data instead of the data itself. You can
    then use your scoring function to process the data asynchronously and output the
    predictions to a different storage location, back to the Blob storage, or push
    the data asynchronously to the calling service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 运行批评分服务与之前讨论的在线评分方法非常相似；你需要提供一个环境、计算目标和评分脚本。然而，在你的评分文件中，你更愿意传递一个指向Blob存储位置的新数据批次的路径，而不是数据本身。然后你可以使用你的评分函数异步处理数据，并将预测输出到不同的存储位置，回到Blob存储，或者异步将数据推送到调用服务。
- en: It is up to you how you implement your scoring file, as it is simply a Python
    script that you control. The only difference in the deployment process is that
    the batch-scoring script will be deployed as a computation on an Azure Machine
    Learning cluster, scheduled periodically through a pipeline, or triggered through
    a REST service. Therefore, it is important that your scoring script can be configured
    through command-line parameters. Remember that what makes batch scoring different
    is that we don't send the data to the scoring script, but instead, we send a path
    to the data and a path to write the output asynchronously.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何实现评分文件取决于你，因为它只是一个你控制的Python脚本。在部署过程中唯一的区别是批评分脚本将作为Azure机器学习集群上的计算部署，通过管道定期调度或通过REST服务触发。因此，确保你的评分脚本可以通过命令行参数进行配置非常重要。记住，批评分的不同之处在于我们不向评分脚本发送数据，而是发送数据的路径和写入输出的路径异步发送。
- en: 'A batch-scoring script is typically wrapped in a pipeline step, deployed as
    a pipeline, and triggered from a REST service or batch-scoring endpoint. The pipeline
    can be configured to use an Azure Machine Learning cluster for execution. In this
    section, we will reuse all of the concepts we have previously seen in [*Chapter
    8*](B17928_08_ePub.xhtml#_idTextAnchor135), *Azure Machine Learning Pipelines*,
    and apply them to a batch-scoring pipeline step. Let''s build a batch-scoring
    pipeline that scores images using the Inception v3 **DNN** model:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 批评分脚本通常封装在管道步骤中，作为管道部署，并通过REST服务或批评分端点触发。管道可以配置为使用Azure机器学习集群进行执行。在本节中，我们将重用我们之前在[*第8章*](B17928_08_ePub.xhtml#_idTextAnchor135)，*Azure机器学习管道*中看到的所有概念，并将它们应用到批评分管道步骤中。让我们构建一个使用Inception
    v3 **DNN**模型评分图像的批评分管道：
- en: 'First, we define a configurable batch size. In both the pipeline configuration
    and the scoring file, you can take advantage of parallelizing your work in the
    Azure Machine Learning cluster:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义一个可配置的批量大小。在管道配置和评分文件中，你可以利用在Azure机器学习集群中并行化你的工作：
- en: '[PRE34]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we define a pipeline step that will call the batch-scoring script:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个将调用批评分脚本的管道步骤：
- en: '[PRE35]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we wrap the pipeline step in a pipeline. To test the batch-processing
    step, we submit the pipeline as an experiment to the Azure Machine Learning workspace:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将管道步骤封装在管道中。为了测试批处理步骤，我们将管道作为实验提交到Azure机器学习工作区：
- en: '[PRE36]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Using this pipeline configuration, we call our scoring script with the relevant
    parameters. The pipeline is submitted as an experiment in Azure Machine Learning,
    which gives us access to all the features in runs and experiments in Azure. One
    feature would be that we can simply download the output from the experiment when
    it has finished running:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此管道配置，我们使用相关参数调用我们的评分脚本。管道作为Azure机器学习中的一个实验提交，这使我们能够访问Azure中运行和实验的所有功能。一个功能就是，当实验运行完成后，我们可以简单地下载输出：
- en: '[PRE37]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If the batch-scoring file produces a nice CSV output containing names and predictions,
    we can now display the results using the following pandas functionality:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果批评分文件生成了一个包含名称和预测的漂亮的CSV输出，我们现在可以使用以下pandas功能来显示结果：
- en: '[PRE38]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s go ahead and publish the pipeline as a REST service:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续将管道作为REST服务发布：
- en: '[PRE39]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To run the published pipeline as a service through HTTP, we now need to use
    token-based authentication:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要通过HTTP将发布的管道作为服务运行，我们现在需要使用基于令牌的认证：
- en: '[PRE40]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Having retrieved the authentication token, we can now run the published pipeline:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取了认证令牌后，我们现在可以运行发布的管道：
- en: '[PRE41]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: That's it! You can now trigger your batch-scoring pipeline using the REST endpoint.
    The data will be processed, and the results will be provided in a file that can
    be consumed programmatically or piped into the next pipeline step for further
    processing.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在可以使用REST端点来触发你的批评分管道。数据将被处理，结果将以一个可以编程方式消费或管道传输到下一个处理步骤的文件提供。
- en: Running a batch-scoring pipeline on an Azure Machine Learning service is a bit
    different from running a synchronous scoring service. While the real-time scoring
    service uses Azure Machine Learning deployments and AKS or ACI as popular compute
    targets, batch-scoring models are usually deployed as published pipelines on top
    of AmlCompute. The benefit of a published pipeline is that it can be used as a
    REST service, which can trigger and parameterize the pipeline.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure Machine Learning服务上运行批处理评分流水线与运行同步评分服务略有不同。虽然实时评分服务使用Azure Machine Learning部署和AKS或ACI作为流行的计算目标，但批处理评分模型通常作为发布流水线部署在AmlCompute之上。发布流水线的优点是它可以作为一个REST服务使用，可以触发和参数化流水线。
- en: ML operations in Azure
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure中的ML操作
- en: You successfully registered a trained model, an environment, a scoring file,
    and an inference configuration in the previous section. You optimized your model
    for scoring and deployed it to a managed Kubernetes cluster. You autogenerated
    client SDKs for your ML services. So, can you finally lean back and enjoy the
    success of your hard work? Well, not yet! First, we need to make sure that we
    have all our monitoring in place so that we can observe and react to anything
    happening to our deployment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您成功注册了一个训练好的模型、一个环境、一个评分文件和一个推理配置。您已经优化了模型以进行评分，并将其部署到了一个管理的Kubernetes集群中。您为您的ML服务自动生成了客户端SDK。所以，您终于可以放松一下，享受您辛勤工作的成果了吗？嗯，还不行！首先，我们需要确保我们已经设置了所有监控，这样我们就可以观察并应对部署中发生的任何情况。
- en: 'First, the good points: with Azure Machine Learning deployments and managed
    compute targets, you will get many things included out of the box with either
    Azure, Azure Machine Learning, or your service used as a compute target. Tools
    such as the **Azure Dashboard** on the Azure Portal, **Azure Monitor**, and **Azure
    Log Analytics** make it easy to centralize log and debug information. Once your
    data is available through Log Analytics, it can be queried, analyzed, visualized,
    alerted, and/or used for automation using Azure Automation. A great deployment
    and operations process should utilize these tools integrated with Azure and the
    Azure services.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看优点：使用Azure Machine Learning部署和管理计算目标，您将获得许多内置功能，无论是使用Azure、Azure Machine
    Learning还是您的服务作为计算目标。例如，Azure门户上的**Azure仪表板**、**Azure Monitor**和**Azure日志分析**等工具使得集中日志和调试信息变得容易。一旦您的数据通过日志分析可用，就可以使用Azure
    Automation进行查询、分析、可视化、警报和/或自动化。
- en: The first thing that should come to mind when operating any application is measuring
    software and hardware metrics. It's essential to know the memory consumption,
    CPU usage, I/O latency, and network bandwidth of your application. Particularly
    for an ML service, you should always have an eye on performance bottlenecks and
    resource utilization for cost optimization. For large GPU-accelerated DNNs, it
    is essential to know your system in order to scale efficiently. These metrics
    allow you to scale your infrastructure vertically, and so move to bigger or smaller
    nodes when needed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作任何应用程序时，首先应该想到的是测量软件和硬件指标。了解您的应用程序的内存消耗、CPU使用率、I/O延迟和网络带宽是至关重要的。特别是对于ML服务，您应该始终关注性能瓶颈和资源利用率，以实现成本优化。对于大型GPU加速的DNN，了解您的系统对于高效扩展至关重要。这些指标允许您垂直扩展您的基础设施，并在需要时移动到更大的或更小的节点。
- en: Another monitoring target for general application deployments should be your
    users' telemetry data (how they are using your service, how often they use it,
    and which parts of the service they use). This will help you to scale horizontally
    and add more nodes or remove nodes when needed.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的应用程序部署，另一个监控目标应该是您的用户遥测数据（他们如何使用您的服务，他们使用频率，以及他们使用服务的哪些部分）。这将帮助您水平扩展，并在需要时添加更多节点或删除节点。
- en: The final important portion to measure from your scoring service, if possible,
    is the user input over time and the scoring results. For optimal prediction performance,
    it is essential to understand what type of data users are sending to your service,
    and how similar this data is to the training data. It's relatively certain that
    your model will require retraining at some point, and monitoring the input data
    will help you to define a time that this is required (for example, through a data
    drift metric).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，从你的评分服务中测量的最后一个重要部分是用户随时间输入的数据和评分结果。为了实现最佳的预测性能，了解用户发送到你的服务的数据类型以及这些数据与训练数据相似性如何至关重要。相对确定的是，你的模型在某个时刻将需要重新训练，监控输入数据将帮助你定义何时需要重新训练（例如，通过数据漂移指标）。
- en: Let's take a look at how we can monitor the Azure Machine Learning deployments
    and keep track of all these metrics in Azure.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何监控Azure机器学习部署并跟踪Azure中的所有这些指标。
- en: Profiling models for optimal resource configuration
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为最佳资源配置分析模型
- en: Azure Machine Learning provides a handy tool to help you evaluate the required
    resources for your ML model deployment through model profiling. This will help
    you estimate the number of CPUs and the amount of memory required to operate your
    scoring service at a specific throughput.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习提供了一个方便的工具，帮助你通过模型分析评估ML模型部署所需资源。这将帮助你估计在特定吞吐量下操作评分服务所需的CPU数量和内存量。
- en: 'Let''s take a look at the model profile of the model that we trained during
    the real-time scoring example:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在实时评分示例中训练的模型的模型分析：
- en: 'First, you need to define `test_data` in the same format as the JSON request
    for your ML service; so, have `test_data` embedded in a JSON object under the
    `data` root property. Please note that if you defined a different format in your
    scoring file, then you need to use your own custom format:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要以与你的ML服务JSON请求相同的格式定义`test_data`；因此，请将`test_data`嵌入到`data`根属性下的JSON对象中。请注意，如果你在评分文件中定义了不同的格式，那么你需要使用你自己的自定义格式：
- en: '[PRE42]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, you can use the `Model.profile()` method to profile a model and evaluate
    the CPU and memory consumption of the service. This will start up your model,
    fire requests with `test_data` provided to it, and measure the resource utilization
    at the same time:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`Model.profile()`方法来分析模型并评估服务的CPU和内存消耗。这将启动你的模型，向其发送`test_data`请求，并同时测量资源利用率：
- en: '[PRE43]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output contains a list of resources, plus a recommended value for the profiled
    model, as shown in the following snippet:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出包含资源列表，以及针对分析模型的推荐值，如下面的代码片段所示：
- en: '[PRE44]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: It is good to run the model profiling tool before doing a production deployment,
    and this will help you set meaningful default values for your resource configuration.
    To further optimize and decide whether you need to scale up or down, vertically
    or horizontally, you need to measure, track, and observe various other metrics.
    We will discuss monitoring and scaling more in the last section of this chapter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行生产部署之前运行模型分析工具是个好主意，这有助于你为资源配置设置有意义的默认值。为了进一步优化并决定是否需要向上或向下、垂直或水平扩展，你需要测量、跟踪和观察各种其他指标。我们将在本章的最后部分更详细地讨论监控和扩展。
- en: Collecting logs and infrastructure metrics
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集日志和基础设施指标
- en: 'If you are new to cloud services, or Azure specifically, log and metric collection
    can be a bit overwhelming at first. Logs and metrics are generated in different
    layers in your application and can be either infrastructure- or application-based
    and collected automatically or manually. Then, there are diagnostic metrics that
    are emitted automatically but need to be enabled manually. In this section, we
    will briefly discuss how to collect this metric for the three main managed compute
    targets in the Azure Machine Learning service: ACI, AKS, and AmlCompute.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触云服务，或者具体来说是Azure，那么日志和指标收集一开始可能会有些令人不知所措。日志和指标在你的应用程序的不同层级生成，可以是基于基础设施或应用程序的，并且可以自动或手动收集。然后，还有自动生成但需要手动启用的诊断指标。在本节中，我们将简要讨论如何在Azure机器学习服务中的三个主要托管计算目标（ACI、AKS和AmlCompute）上收集这些指标。
- en: By default, you will get access to infrastructure metrics and logs through Azure
    Monitor. It will automatically collect Azure resources and guest OS metrics and
    logs, and provide metrics and query interfaces for logs based on Log Analytics.
    Azure Monitor should be used to track resource utilization (for example, CPU,
    RAM, disk space, disk I/O, and network bandwidth), which then can be pinned to
    dashboards or alerted on. You can even set up automatic autoscaling based on these
    metrics.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，您将通过Azure Monitor获取对基础设施指标和日志的访问权限。它将自动收集Azure资源和虚拟机操作系统指标和日志，并基于日志分析提供日志的指标和查询接口。Azure
    Monitor应用于跟踪资源利用率（例如，CPU、RAM、磁盘空间、磁盘I/O和网络带宽），然后可以将其固定到仪表板或发出警报。您甚至可以根据这些指标设置自动扩展。
- en: 'Metrics are mostly collected as distributions over time and reported back at
    certain time intervals. So, instead of seeing thousands of values per second,
    you are asked to choose an aggregate for each metric, for example, the average
    of each interval. For most monitoring cases, I would recommend you either look
    at the 95th percentile (or maximum aggregation, for metrics where lower is better)
    to avoid smoothing any spikes during the aggregation process. In AKS, you are
    provided with four different views of your metrics through Azure Monitor: clusters,
    nodes, controllers, and containers.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 指标通常以时间分布的形式收集，并在特定的时间间隔内进行报告。因此，您不需要每秒看到数千个值，而是需要为每个指标选择一个汇总值，例如每个区间的平均值。在大多数监控场景中，我建议您查看95%分位数（或最大汇总值，对于越低越好的指标）以避免在汇总过程中平滑任何峰值。在AKS中，您可以通过Azure
    Monitor获得您指标的四个不同视图：集群、节点、控制器和容器。
- en: More detailed resource, guest, and virtualization host logs of your Azure Machine
    Learning deployment can be accessed by enabling diagnostic settings and providing
    a separate Log Analytics instance. This will automatically load the log data into
    your Log Analytics workspace, where you can efficiently query all your logs, analyze
    them, and create visualization and/or alerts.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启用诊断设置并提供单独的日志分析实例，您可以访问Azure机器学习部署的更详细的资源、虚拟机和虚拟化主机日志。这将自动将日志数据加载到您的日志分析工作区中，在那里您可以高效地查询所有日志、分析它们并创建可视化或警报。
- en: It is strongly recommended to take advantage of the diagnostic settings, as
    they give you insights into your Azure infrastructure. This is especially helpful
    when you need to debug problems in your ML service (for example, failing containers,
    non-starting services, crashes, application freezes, and slow response times).
    Another great use case for Log Analytics is to collect, store, and analyze your
    application log. In AKS, you can send the Kubernetes master node logs, *kubelet*
    logs, and API server logs to Log Analytics.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议利用诊断设置，因为它们为您提供了对Azure基础设施的洞察。这在您需要调试ML服务中的问题（例如，失败的容器、无法启动的服务、崩溃、应用程序冻结和缓慢的响应时间）时特别有帮助。日志分析的另一个很好的用例是收集、存储和分析您的应用程序日志。在AKS中，您可以发送Kubernetes主节点日志、*kubelet*日志和API服务器日志到日志分析。
- en: One metric that is very important to track for ML training clusters and deployments,
    but is unfortunately not tracked automatically, is the GPU resource utilization.
    Due to this problem, GPU resource utilization has to be monitored and collected
    at the application level.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习训练集群和部署来说，有一个非常重要的指标需要跟踪，但遗憾的是它并没有自动跟踪，那就是GPU资源利用率。由于这个问题，GPU资源利用率必须在应用程序级别进行监控和收集。
- en: 'The most effective way to solve this for AKS deployments is to run a GPU logger
    service as a sidecar with your application, which collects resource statistics
    and sends them to **Application Insights** (**App Insights**), a service that
    collects application metrics. Both App Insights and Log Analytics use the same
    data storage technology under the hood: Azure Data Explorer. However, default
    integrations for App Insights provide mainly application metrics such as access
    logs, while Log Analytics provides system logs.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 解决AKS部署问题的最有效方法是运行一个作为应用程序副车的GPU日志记录服务，该服务收集资源统计信息并将它们发送到**应用洞察**（**App Insights**），这是一个收集应用程序指标的服务。App
    Insights和日志分析在底层使用相同的数据存储技术：Azure数据探索器。然而，App Insights的默认集成主要提供应用程序指标，如访问日志，而日志分析提供系统日志。
- en: 'In AmlCompute, we need to start a separate monitoring thread from your application
    code to monitor GPU utilization. Then, for Nvidia GPUs, we use a wrapper around
    the `nvidia-smi` monitoring utility, for example, the `nvidia-ml-py3` Python package.
    To send data to App Insights, we simply use the Azure SDK for App Insights. Here
    is a tiny code example showing you how to achieve this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在AmlCompute中，我们需要从您的应用程序代码中启动一个单独的监控线程来监控GPU利用率。然后，对于Nvidia GPU，我们使用`nvidia-smi`监控工具的包装器，例如`nvidia-ml-py3`
    Python包。要将数据发送到App Insights，我们只需使用Azure SDK for App Insights。以下是一个简短的代码示例，展示您如何实现这一点：
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the preceding code, we first used the `nvidia-ml-py3` wrapper on top of `nvidia-smi`
    to return a handle to the current GPU. Please note that when you have multiple
    GPUs, you can also iterate over them and report multiple metrics. Then, we used
    the `TelemetryClient` API from App Insights to report these metrics back to a
    central place, where we can then visualize, analyze, and alert on these values.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先在`nvidia-smi`之上使用了`nvidia-ml-py3`包装器来返回当前GPU的句柄。请注意，当您有多个GPU时，您也可以遍历它们并报告多个指标。然后，我们使用App
    Insights的`TelemetryClient` API将这些指标报告回一个中心位置，然后我们可以可视化、分析和对这些值发出警报。
- en: Tracking telemetry and application metrics
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪遥测和应用指标
- en: We briefly touched on Azure App Insights in the previous section. It is a great
    service for automatically collecting application metrics from your services, for
    example, Azure Machine Learning deployments. It also provides an SDK to collect
    any user-defined application metric that you want to track.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要介绍了Azure App Insights。它是一个自动收集您的服务应用指标的优秀服务，例如Azure机器学习部署。它还提供了一个SDK来收集您想要跟踪的任何用户定义的应用指标。
- en: 'To automatically track user metrics, we need to deploy the model using Azure
    Machine Learning deployments to AKS or ACI. This will not only collect the web
    service metadata but also the model''s predictions. To do so, you need to enable
    App Insights'' diagnostics, as well as data model collection, or enable App Insights
    via the Python API:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要自动跟踪用户指标，我们需要使用Azure Machine Learning部署将模型部署到AKS或ACI。这不仅会收集Web服务元数据，还会收集模型的预测。为此，您需要启用App
    Insights的诊断以及数据模型收集，或者通过Python API启用App Insights：
- en: '[PRE46]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the preceding snippet, we can activate App Insights' metrics directly from
    the Python authoring environment. While this is a simple argument in the service
    class, it gives you an incredible insight into the deployment.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以直接从Python编写环境激活App Insights的指标。虽然这在服务类中只是一个简单的参数，但它为您提供了关于部署的惊人洞察。
- en: Two important metrics to measure are data drift coefficients for both training
    data and model predictions. We will learn more about this in the next section.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 需要测量的两个重要指标是训练数据和模型预测的数据漂移系数。我们将在下一节中了解更多关于这方面的内容。
- en: Detecting data drift
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测数据漂移
- en: One important problem in ML is when to retrain your models. Should you always
    retrain when new training data is available, for example, daily, weekly, monthly
    or yearly? Do we need to retrain at all, or is the training data still relevant?
    Measuring **data drift** will help to answer these questions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个重要问题是何时重新训练模型。当有新的训练数据可用时，您是否应该始终重新训练，例如每天、每周、每月或每年？我们是否需要重新训练，或者训练数据是否仍然相关？测量**数据漂移**将有助于回答这些问题。
- en: By automatically tracking the user input and the model predictions, you can
    compare a statistical variation between the training data and the user input per
    feature dimension, as well as the training labels with the model prediction. The
    variation of the training data and actual data is what is referred to as data
    drift and should be tracked and monitored regularly. Data drift leads to model
    performance degradation over time, and so needs to be monitored. The best case
    is to set up monitoring and alerts to understand when your deployed model differs
    too much from the training data and so needs to be retrained.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动跟踪用户输入和模型预测，您可以比较训练数据与用户输入在每个特征维度上的统计差异，以及训练标签与模型预测的差异。训练数据和实际数据之间的差异被称为数据漂移，应该定期跟踪和监控。数据漂移会导致模型性能随时间退化，因此需要监控。最佳情况是设置监控和警报，以便了解您的部署模型与训练数据差异过大时需要重新训练。
- en: 'Azure Machine Learning provides useful abstractions to implement data drift
    monitors and alerts based on registered **datasets**, and can automatically expose
    data drift metrics in Application Insights. Computing the data drift requires
    two datasets: a baseline, which is usually the training dataset, and a target
    dataset, which is usually a dataset constructed from the inputs of the scoring
    service:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习提供了有用的抽象，可以根据注册的**数据集**实现数据漂移监控器和警报，并且可以自动在Application Insights中公开数据漂移指标。计算数据漂移需要两个数据集：一个是基线数据集，通常是训练数据集；另一个是目标数据集，通常是来自评分服务输入构建的数据集：
- en: 'First, we define the target and baseline datasets. These datasets must contain
    a column that represents the date and time of each observation:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义目标和基线数据集。这些数据集必须包含一个表示每个观测日期和时间的列：
- en: '[PRE47]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we can set up email alerting for the monitor. This can be done in many
    different ways, but for the purpose of this example, we set up an email alert
    directly on the data drift monitor:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以为监控器设置电子邮件警报。这可以通过许多不同的方式完成，但为了本例的目的，我们直接在数据漂移监控器上设置电子邮件警报：
- en: '[PRE48]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can set up the data drift monitor providing all the previous details.
    We configure the monitor for three specific features `[''a'', ''b'', ''c'']`,
    to measure drift on a monthly cadence with a delay of 24 hours. An alert is created
    when the target dataset drifts more than 25% from the baseline data:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置数据漂移监控器，提供所有之前的详细信息。我们为三个特定的特征`['a', 'b', 'c']`配置监控器，以每月的节奏进行漂移测量，延迟24小时。当目标数据集相对于基线数据漂移超过25%时，将创建一个警报：
- en: '[PRE49]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, we can enable the monitor schedule to run periodically:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以启用监控器定期运行的计划：
- en: '[PRE50]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Data drift is an essential operational metric to look at when operating ML deployments.
    Setting up monitors and alarms will help you get alerted early when the distribution
    of your data deviates too much from the training data and, therefore, requires
    you to retrain the model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 数据漂移是操作机器学习部署时需要关注的重要运营指标。设置监控器和警报将帮助您在数据分布与训练数据偏差过大时及早得到警报，因此需要重新训练模型。
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to take a trained model and deploy it as a managed
    service in Azure through a few simple lines of code. To do so, we learned how
    to prepare a model for deployment and looked into Azure Machine Learning auto-deployments
    and customized deployments.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将训练好的模型通过几行简单的代码部署为Azure中的托管服务。为此，我们学习了如何为部署准备模型，并探讨了Azure机器学习的自动部署和自定义部署。
- en: We then took an NLP sentiment analysis model and deployed it as a real-time
    scoring service to ACI and AKS. We also learned how to define the service schema
    and how to roll out new versions effectively using endpoints and blue-green deployments.
    Finally, we learned how to integrate a model in a pipeline for asynchronous batch
    scoring.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用NLP情感分析模型并将其部署为实时评分服务到ACI和AKS。我们还学习了如何定义服务架构，以及如何使用端点和蓝绿部署有效地推出新版本。最后，我们学习了如何将模型集成到管道中进行异步批量评分。
- en: In the last section, we learned about monitoring and operating your models using
    Azure Machine Learning services. We proposed to monitor CPU, memory, and GPU metrics
    as well as telemetry data. We also learned how to measure the data drift of your
    service by collecting user input and model output over time. Detecting data drift
    is an important metric that allows you to know when a model needs to be retrained.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们学习了如何使用Azure机器学习服务监控和操作模型。我们提出了监控CPU、内存和GPU指标以及遥测数据。我们还学习了如何通过收集用户输入和模型输出随时间的变化来衡量服务的数据漂移。检测数据漂移是一个重要的指标，它允许您知道何时需要重新训练模型。
- en: In the next chapter, we will apply the learned knowledge and take a look at
    model interoperability, hardware optimization, and integration into other Azure
    services.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将应用所学知识，探讨模型互操作性、硬件优化以及集成到其他Azure服务中。
