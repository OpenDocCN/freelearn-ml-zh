- en: '*Chapter 15*: Model Interoperability, Hardware Optimization, and Integrations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discovered how to deploy our machine learning scoring
    either as a batch or real-time scorer, what endpoints are and how we can deploy
    them, and finally, we had a look at how we can monitor our deployed solutions.
    In this chapter, we will dive deeper into additional deployment scenarios for
    ML inferencing, possible other hardware infrastructure we can utilize, and how
    we can integrate our models and endpoints with other Azure services.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we will have a look at how to provide model interoperability
    by converting ML models into a standardized model format and an inference-optimized
    scoring framework. **Open Neural Network Exchange** (**ONNX**) is a standardized
    format to serialize and store ML models and acyclic computational graphs and operations
    efficiently. We will learn what the ONNX framework is, how we can convert ML models
    from popular ML frameworks to ONNX, and how we can score ONNX models on multiple
    platforms using ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, we will take a look at alternative hardware targets, such as
    **field-programmable gate arrays** (**FPGAs**). We will understand how they work
    internally and how they can lead to higher performance and better efficiency compared
    to standard hardware or even GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will have a look at how we can integrate ML models and endpoints
    into other services. We will get a deeper understanding of the process to deploy
    ML to edge devices, and we will integrate one of our previously set up endpoints
    with Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Model interoperability with ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware optimization with FPGAs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating ML models and endpoints with Azure services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will require access to a Microsoft Power BI account. You
    can get one either through your place of work or by creating a trial account here:
    [https://app.powerbi.com/signupredirect?pbi_source=web](https://app.powerbi.com/signupredirect?pbi_source=web).'
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15).'
  prefs: []
  type: TYPE_NORMAL
- en: Model interoperability with ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to deploy ML models as web services
    for online and batch scoring. However, many real-world use cases require you to
    embed a trained ML model directly into an application without the use of a separate
    scoring service. The target service is likely written in a different language
    than the language used for training the ML model. A common example is that a simple
    model trained in Python using scikit-learn needs to be embedded into a Java application.
  prefs: []
  type: TYPE_NORMAL
- en: Model interoperability gives you the flexibility to train your model with your
    language and framework of choice, export it to a common format, and then score
    it in a different language and platform using the shared format. In some cases,
    using a native runtime optimized for scoring on the target environment even achieves
    a better scoring performance than running the original model.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a look at the ONNX initiative, consisting of the specification,
    runtime, and ecosystem, and how it helps to achieve model interoperability across
    a large set of support languages, frameworks, operations, and target platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will look into converting ML models from popular frameworks to ONNX
    (called ONNX frontends) and executing ONNX models in a native inferencing runtime
    using ONNX Runtime, one of the multiple ONNX backends. Let's delve into it.
  prefs: []
  type: TYPE_NORMAL
- en: What is model interoperability and how can ONNX help?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an IT organization grows, so does the amount of tooling, development, and
    deployment platforms and choices. In ML, this problem is even more present as
    there are multiple ML frameworks as well as model serialization formats. Therefore,
    once the organization grows, it becomes a near-impossible challenge to align every
    scientist and engineer on the same tooling, frameworks, and model formats that
    also need to support all your target environments. Does your XGBoost model run
    on iOS? Does your PyTorch model work in Java? Can your scikit-learn model be loaded
    in a browser-based JavaScript application? One way to solve this problem of model
    interoperability is to ensure that trained ML models can be ported to a standardized
    format that can be executed natively across all target platforms. This is exactly
    what ONNX is about.
  prefs: []
  type: TYPE_NORMAL
- en: 'ONNX is a joint initiative from major IT companies such as Microsoft, Facebook,
    Amazon, ARM, and Intel to facilitate ML model interoperability. It allows organizations
    to choose different languages, frameworks, and environments for ML training, as
    well as different languages, environments, and devices for inferencing. As an
    example, ONNX enables an organization to train deep learning models using PyTorch
    and TensorFlow and traditional ML models using LightGBM and XGBoost, and deploy
    these models to a Java-based web service, an Objective-C-based iOS application,
    and a browser-based JavaScript application. This interoperability is enabled through
    three key ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ONNX specification**: A data format for *efficient serialization and deserialization*
    for model definitions and model weights using **Protocol Buffers** (**Protobuf**).
    To represent a wide range of ML models, the ONNX specification is comprised of
    a definition of an extensible computation graph model, as well as definitions
    of standard data types and built-in operators. With the ONNX specification, many
    ML models consisting of a variety of supported architectures, building blocks,
    operations, and data types can be efficiently represented in a single file, which
    we call the *ONNX model*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ONNX Runtime**: An efficient *native inferencing engine* with bindings to
    many higher-level languages, such as C#, Python, JavaScript, Java/Kotlin (Android),
    and Objective-C (iOS). This means that with the ONNX Runtime bindings for one
    of these languages, we can load, score, and even train ONNX models. It also provides
    built-in GPU acceleration using DirectML, TensorRT, **Deep Neural Network Library**
    (**DNNL**), nGraph, CUDA, and the **Microsoft Linear Algebra Subprograms** (**MLAS**)
    library, and weight quantization and graph optimization to run efficiently on
    various compute targets, such as Cloud Compute, Jupyter kernels, mobile phones,
    and web browsers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ONNX ecosystem**: A *collection of libraries* that facilitate conversion
    from and to ONNX. ONNX libraries can be broadly categorized into ONNX frontends
    (*to ONNX*) and ONNX backends (*from ONNX*). While *ONNX frontend* libraries help
    to convert arbitrary computations into ONNX models (models following the ONNX
    specification), *ONNX backend* libraries provide support to execute ONNX models
    or to convert ONNX models into a specific framework runtime. ONNX is widely used
    within Microsoft as well as other large companies and, therefore, supports a wide
    range of frameworks and languages. Many popular libraries are officially supported
    frontends, such as traditional ML algorithms, scikit-learn, LightGBM, XGBoost,
    and CatBoost, as well as modern DL frameworks, such as TensorFlow, Keras, PyTorch,
    Caffe 2, and CoreML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX is a great choice for providing model interoperability to allow an organization
    to decouple model training, model serialization, and model inferencing. Let's
    learn about popular ONNX frontends and backends in action in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Converting models to ONNX format with ONNX frontends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ONNX frontends are packages, tools, or libraries that can convert existing
    ML models or numeric computations into ONNX models. While popular ML frameworks
    used to implement ONNX export out of the box (similar to the PyTorch `torch.onnx`
    module), most frameworks today support ONNX through a separate conversion library.
    The most popular ONNX frontends at the time of writing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`skl2onnx`: Converts scikit-learn models to ONNX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf2onnx`: Converts TensorFlow models to ONNX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onnxmltools`: Converts XGBoost, LightGBM, CatBoost, H2O, libsvm, and CoreML
    models to ONNX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.onnx`: Converts PyTorch models to ONNX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once, the ONNX frontend libraries are installed, the conversion to ONNX specification
    is often simply done by running a single command. Let''s see this in action with
    TensorFlow as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will save a Keras model using the TensorFlow `SaveModel` format.
    We can achieve this by calling `model.save()` and providing the path to serialize
    the `SaveModel` model to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: train.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the `tf2onnx` library to convert the `SaveModel` model into
    an ONNX model, as shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: convert.sh
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we see in the preceding example, all we need is a single command to convert
    TensorFlow models into ONNX models. Once we have an ONNX model, we can use ONNX
    backends to score them, as shown in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Native scoring of ONNX models with ONNX backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once a model is exported as an ONNX model, we can load it using an ONNX-compatible
    backend. The reference implementation for the ONNX backend is called **ONNX Runtime**,
    and is a native implementation with bindings in many high-level languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can load, analyze, and check an ONNX model using the `onnx` library,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we want to score the model, we need to use the `onnxruntime` backend
    library. First, we need to load the model for an inferencing session; this means
    we can load the optimized model and don''t need to allocate any buffers for storing
    gradients. In the next step, we can score the model by executing `run(output_names,
    input_feed, run_options=None)`. The `output_names` argument refers to the named
    output layer we want to return from the model, whereas `input_feed` represents
    the data we want to pass to the model. The scoring properties, such as the log
    level, can be configured through the `run_options` argument. The following example
    shows how to score the model and return the last layer''s output from an ONNX
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we load the ONNX model optimized for inferencing, pass
    data to the model's `input` parameter, and return the last layer's output using
    the ONNX Runtime Python API. You can access the layer information, as well as
    names of inputs and outputs, using the helper method, `session.get_modelmeta()`.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about ONNX, how to create an ONNX model from trained
    ML models using ONNX frontends, and how to score an ONNX model using ONNX Runtime,
    the reference implementation for an ONNX backend. While we looked only at the
    Python API of ONNX Runtime, many other high-level bindings are available.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware optimization with FPGAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we exported a model to ONNX to take advantage of an
    inference-optimized and hardware-accelerated runtime to improve the scoring performance.
    In this section, we will take this approach one step further to deploy on even
    faster inferencing hardware: FPGAs.'
  prefs: []
  type: TYPE_NORMAL
- en: But, before we talk about how to deploy a model to an FPGA, let's first understand
    what an FPGA is and why we would choose one as a target for DL inference instead
    of a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding FPGAs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most people typically come across a specific variety of **integrated circuit**
    (**IC**), called an **application-specific integrated circuit** (**ASIC**). ASICs
    are purpose-built ICs, such as the processor in your laptop, the GPU cores on
    your graphics card, or the microcontroller in your washing machine. These chips
    share the fact that they have a fixed hardware footprint optimized to support
    a specific task. Often, like any general processor, they operate with a specific
    **instruction set**, allowing certain commands to be run. When you program something
    with a higher-level language, such as Java, C++, or Python, the compiler or interpreter
    will translate this high-level code into machine code, which is the set of commands
    the processor understands and is able to run.
  prefs: []
  type: TYPE_NORMAL
- en: The strength of an ASIC is that the underlying chip architecture can be optimized
    for the specific workload, resulting in the most optimal design for the hardware
    in terms of the area it requires. The weakness of an ASIC is that it is only good
    for performing the specific task it was designed for, and its design is fixed,
    as the underlying hardware cannot be altered.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we can run any task on a standard processor, for something very
    specific, such as the computation and backtracking for thousands of nodes in a
    neural network, they might not be optimal. Therefore, a lot of these calculations
    are now run on a GPU instead, as its chip architecture leans more toward running
    the same calculations in parallel, which leans more toward the ingrained structure
    of a neural network algorithm than a standard CPU would.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs are defined by a different concept than their ASIC counterparts. FPGAs
    trade in the most optimal design, especially when it comes to the used area on
    a chip, for the freedom of *re-programmability*. This main feature allows a user
    to purchase an FPGA and then build themselves their own processor, a hardware
    switch, a network router, or anything else, and change the underlying hardware
    design any time they feel like it.
  prefs: []
  type: TYPE_NORMAL
- en: As hardware in the end is something physical made up of some form of binary
    logic gates, registers, and wires, this capability of FPGAs might sound like magic.
    Then again, we are using flash drives daily that can store data and can erase
    data again. For example, modern **NAND flash drives** are erased through a process
    called **field electron emission**, which allows a charge to move through a thin
    layer of insulation to *reset* the setting of bits or, to be more precise, blocks
    of bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remembering this, let''s have a look at the basic building blocks of an FPGA,
    called **logic elements**. *Figure 15.1* shows the general concept of these building
    blocks. Different manufacturers tweak different aspects of these, but the base
    concept remains the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Structure of a logic element in an FPGA ](img/B17928_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Structure of a logic element in an FPGA
  prefs: []
  type: TYPE_NORMAL
- en: 'A logic element is typically made up of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input/output** (**I/O**): Denotes the interconnection with other logical
    elements or with external I/O (think of Ethernet and USB, for example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lookup table** (**LUT**): Holds the main logical function performed in this
    logic element. Any logic in a digital circuit can be broken down to a **Boolean
    function** that maps a certain number of binary inputs to a certain number of
    binary outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**D-FlipFlop (Register)**: Stores the input value of the current **clock cycle**
    for the next clock cycle, the length of which is the inverse of the **frequency**
    of the running circuit. The idea to store something for the next round is the
    basic principle of all digital hardware and a necessity to be able to do hardware
    pipelining. The maximum processing time between any adjacent registers in the
    circuit defines the maximum frequency the circuit can run at.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiplexer** (**MUX**): Chooses which of its inputs are shown as the output.
    In this case, it either shows the current result from the Boolean function, or
    the one from the previous clock cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through the LUT, any Boolean function (and through a register, any multi-layered
    hardware logic) can be realized. In addition, the LUT can be erased and reset,
    which enables the reprogrammable nature of FPGAs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full schematic structure of an FPGA is shown in *Figure 15.2*. Just understand
    that a normal-sized FPGA will have upward of 500,000 logic elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Schematic structure of an FPGA ](img/B17928_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Schematic structure of an FPGA
  prefs: []
  type: TYPE_NORMAL
- en: In addition to logic elements, *Figure 15.2* shows **switch matrices** and **I/O
    blocks**. Switch matrices are the last piece of the puzzle and allow the setting
    and resetting of the required connections among logic elements, and between them
    and the I/O blocks. With their help, it is possible to fully reprogram the circuit
    structure on an FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to facilitate the programming of an FPGA, a so-called **hardware description
    language** (**HDL**) is used. There are two major languages used for hardware
    design (be it for FPGAs or ASICs), **SystemVerilog** and **VHDL**. When you see
    code written in these languages, it might look like a high-level programming language,
    but in reality, you are not programming anything; you are instead *describing*
    the desired hardware architecture. In a sense, you give the machine a picture
    of a circuit in the form of code, and it tries to map this onto the given elements
    on the FPGA. This step is called **synthesis**. After this step, a binary is sent
    to the FPGA that populates the required logic elements with the correct Boolean
    functions and sets all the interconnections accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Besides this logical structure, you will find a lot of other integrated systems
    in modern FPGAs, combining the strength of ASICs and FPGAs. You might even find
    a processor such as an **ARM Cortex** on the IC itself. The idea is to let anything
    that would be extremely time-consuming to build from scratch on the FPGA fabric
    run on the processor instead while using the FPGA to host your custom hardware
    designs. For example, it would take a lot of time to build the lower layers of
    the Ethernet protocol on an FPGA, as TCP requires a highly sophisticated hardware
    circuit. Therefore, outsourcing this part into a processor can speed up development
    time immensely.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a general idea of what an FPGA is and how it works, let's discuss
    why they might be more useful for DL than GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing GPUs and FPGAs for deep neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the previous section, the underlying hardware structure of
    a GPU supports deep neural networks for training and inference. The reason for
    this is that they are designed with 3D image rendering in mind and, therefore,
    have a lot of logic on board to facilitate matrix multiplications, a task that
    is extremely time-consuming on CPUs and crucial for DNNs. Through GPUs, the processing
    time can typically be lowered from days to mere hours. The same can be said for
    FPGAs, as we can basically build any specialized circuit we require to optimize
    the speed and power consumption of any tasks we want to perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, both are options that are far superior for DNNs than general CPUs.
    But, which one should we choose and why? Let''s now go through a list of aspects
    to consider and how each of these two options fares in both cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity to implement**: GPUs typically offer a software-level language
    (for example, CUDA) to disconnect the programmer from the underlying hardware.
    For FPGAs, the programmer must understand the hardware domain and how to design
    for it. Therefore, building the correct circuit for an FPGA is far more complicated
    than just using another library in a high-level programming language. But, there
    is work being done to abstract this layer as much as possible with specialized
    tooling and converters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power consumption**: GPUs produce a lot of heat and require a lot of cooling
    and electricity. This is because of the additional complexity of the hardware
    design in order to facilitate software programmability, in turn supporting the
    base hardware stack of RAM, CPU, and GPU. FPGAs, on the other hand, do not require
    this stack to operate and, therefore, in most cases, have a low to medium power
    output, through which they are 4 to 10 times more power-efficient than GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware stack**: GPUs are dependent on the whole memory management of the
    standard hardware stack (CPU cache, RAM, and GPU memory), and require an external
    system to control them. This leads to an inefficient but required hardware design
    for GPUs to facilitate the connection layers to the standard hardware stack, which
    makes it less performant. FPGAs, on the other hand, have all the required elements
    (such as high-speed memory) on board the IC and, therefore, can run completely
    *autonomously* without pulling any data from system memory or any other place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency and interconnectability**: While GPUs are connected to a standard
    hardware stack and only have a few actual hardware ports at the back of it (HDMI
    and DisplayPort), which are often only outputs, an FPGA can connect to anything.
    This means it can support vastly different input and output standards at the same
    time, making it extremely flexible and adaptable to any given situation. In addition,
    it can process data with very low latency, as no data needs to pass through the
    system memory, CPU, or SW layer, making it far superior for applications such
    as real-time video processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Even though GPUs have a parallel hardware architecture, you
    might not be able to use it effectively. The specific DNN algorithm must be mapped
    to the underlying hardware, and this might be neither perfect nor even feasible.
    It falls into the same problem class as distributing processes among CPU cores.
    In addition, GPUs are designed to handle 32-bit or 64-bit standard data types.
    If you are using a very specialized data type or a custom one, you might not be
    able to run it on a GPU at all. FPGAs, on the other hand, allow you to define
    whatever data size or data type you want to work with and, on top of that, allow
    even a so-called *partial reconfiguration* during runtime, which it uses to reprogram
    parts of the logic during runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry readiness**: In a typical industrial scenario, be it defense, manufacturing,
    smart cities, or any other, the hardware deployed must be compact, must have a
    long lifespan, should have low power consumption, should survive the environment
    it is positioned in (dust, heat, humidity), and in some scenarios, needs to have
    *functional safety*, which means it must follow certain compliance standards and
    protocols. A GPU is a bad choice for any of these circumstances, as it is very
    power-hungry, has a lifespan of 2 to 5 years, requires massive amounts of cooling,
    does not survive hostile environments, and does not have functional safety. FPGAs
    were designed with industrial settings in mind and, therefore, are typically built
    for long life (10 to 30 years) and safety, while having a low footprint on power
    and required space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Costs**: If you''ve ever bought a GPU for your PC, you might have an idea
    of the cost of such an extension card. FPGAs, on the other hand, can be expensive
    but are typically cheaper to obtain for comparable setup requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taking all these points into consideration, FPGAs are technically superior
    in most ways and often cheaper, but have the major problem that they require developers
    to understand hardware design. This problem led to the creation of toolkits helping
    bridge the gap between hardware and ML development, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vitis AI for Xilinx FPGAs**: A development kit for ML inferencing utilizing
    pre-designed **Deep Learning Processor Units** (**DLUs**). More information can
    be found here: [https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).
    In addition, you can find some information on how to use this with the NP VM series
    in Azure here: [https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure](https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenVINO for Intel FPGAs**: A development kit for DL and ML inferencing.
    More information can be found here: [https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Project Brainwave**: A development platform for DL and ML inferencing
    for computer vision and NLP. More information can be found here: [https://www.microsoft.com/en-us/research/project/project-brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few options to support the deployment and acceleration of ML
    models through FPGAs.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs are a very exceptional technology, but they require an ample understanding
    of hardware design to be used efficiently and successfully in any project, or
    a very sophisticated toolkit for abstracting the hardware layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know why we might prefer to take an FPGA for DNNs, let's have a
    brief look at how FPGAs can be utilized in that regard with Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Running DNN inferencing on Intel FPGAs with Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous section, building a hardware design for an FPGA
    is not an easy task. You could certainly do this from scratch utilizing one of
    the Azure VMs sporting an FPGA ([https://docs.microsoft.com/en-us/azure/virtual-machines/np-series](https://docs.microsoft.com/en-us/azure/virtual-machines/np-series)),
    or with your own FPGA development kit. Another option is to use the hardware-accelerated
    Python package that is available in the Azure Machine Learning Python SDK. This
    package gives you an abstraction layer through a generic hardware design supporting
    a subset of models and options to use, specifically ones for DNN inferencing.
    Through this, you have access to the **Azure PBS VM family**, which has an Intel
    FPGA attached and is only available through Azure Machine Learning. This machine
    type is deployable in East US, Southeast Asia, West Europe, and West US 2.
  prefs: []
  type: TYPE_NORMAL
- en: The general approach is very similar to ONNX; you take a trained model and convert
    it to a specific format that can be executed on FPGAs. In this case, your model
    must be either ResNet, DenseNet, VGG, or SSD-VGG, and must be written in TensorFlow
    in order to fit the underlying hardware design. Furthermore, we will use quantized
    16-bit float model weights converted to ONNX models, which will be run on the
    FPGA. For these models, FPGAs give you the best inference performance in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable hardware acceleration through FPGAs, we require a few extra steps
    compared to the ONNX example. The following list shows what steps need to be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick a supported model featurizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the supported model with a custom classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantize the model featurizer's weights to 16-bit precision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the model to an ONNX format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Optional) Register the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a compute target (preferably Azure Kubernetes Service) with PBS nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the code is cluttered and hard to interpret, we will skip the code examples
    in this section. However, you can find detailed examples of FPGA model training,
    conversion, and deployments on Azure's GitHub repository at [https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's discuss these steps in some more detail.
  prefs: []
  type: TYPE_NORMAL
- en: From the DNN layers we discussed in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, only the feature extractor layers (`azureml.accel.models`
    package ([https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models](https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models)).
    You can attach any classification or regression head (or both) on top using TensorFlow
    or Keras, but they will not be hardware-accelerated, similar to running only certain
    operations on GPUs. The designers opted here to deploy only the most time-consuming
    parts onto the FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, you can train the model, consisting of a predefined feature
    extractor and a custom classification head, using your own data and weights, or
    by fine-tuning, for example, provided ImageNet weights. This should happen with
    32-bit precision, as convergence will be faster during training.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training is finished, you need to quantize the weights of the featurizer
    into half-precision floats, using the quantized models provided in the `azureml.accel.models`
    package. This step needs to be done because the designers opted here for a fixed
    data size of 16-bit in order to make the hardware design as generic and reusable
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: For the next step, you convert the whole model into an ONNX model, using the
    `AccelOnnxConverter` method from the same Azure package. In addition, the `AccelContainerImage`
    class helps you to define `InferenceConfig` for the FPGA-based compute targets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can register your model using the Azure Machine Learning model
    registry, and you can create an AKS cluster using the `Standard_PB6s` nodes. Once
    the cluster is up and running, you use your `Webservice.deploy_from_image` method
    to deploy the web service.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a detailed example of the deployment steps in the Azure Machine
    Learning documentation here: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service).'
  prefs: []
  type: TYPE_NORMAL
- en: The workflow to deploy a model through Azure Machine Learning to an FPGA-based
    compute target is a bit different from simply deploying ONNX models, as you have
    to consider the limited supported selection of models right from the beginning.
    Another difference is that, while you choose a predefined supported model for
    FPGA deployment, you can only accelerate the feature extractor part of the model.
    This means you have to attach an additional classification or regression head—a
    step that is not immediately obvious. Once you understand this, it will make more
    sense that you only quantize the feature extractor to half-precision floats after
    training.
  prefs: []
  type: TYPE_NORMAL
- en: While this process seems a bit difficult and customized, the performance and
    latency gain, especially when dealing with predictions on image data, is huge.
    But, you should take advantage of this optimization only if you are ready to adapt
    your training processes and pipelines to this specific environment, as shown throughout
    the section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of what FPGAs are and how we can utilize
    them through Azure Machine Learning, let's have a look in the next section at
    what other Azure services we can integrate with our models.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating ML models and endpoints with Azure services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Relying on the Azure Machine Learning service either for experimentation, performing
    end-to-end training, or simply registering your trained models and environments
    brings you a ton of value. In [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployment, Endpoints, and Operations*, we covered two main scenarios,
    a real-time scoring web service through automated deployments and batch scoring
    through a deployed pipeline. While these two use cases are quite different in
    requirement and deployment types, they show what is possible once you have a trained
    model and packaged environment stored in Azure Machine Learning. In this section,
    we will discuss how to use and integrate these models or their endpoints in other
    Azure services.
  prefs: []
  type: TYPE_NORMAL
- en: In many scenarios, abstracting your batch-scoring pipeline from the actual data
    processing pipeline to separate concerns and responsibilities makes a lot of sense.
    However, sometimes your scoring should happen directly during the data processing
    or querying time and in the same system. Once your ML model is registered and
    versioned with Azure Machine Learning, you can pull out a specific version of
    the model anywhere using the Azure ML SDK, either in Python, C#, the command line,
    or any other language that can make a call to a REST service.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it possible to pull trained and converted ONNX models from a desktop
    application, either during build time or at runtime. You can load models while
    running a Spark job, for example, on Azure Databricks or Azure Synapse. Through
    that, you can avoid transferring TBs of data to a separate scoring service.
  prefs: []
  type: TYPE_NORMAL
- en: Other services, such as Azure Data Explorer, allow you to call models directly
    from the service through a Python extension ([https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin)).
    Azure Data Explorer is an exciting managed service for storing and querying large
    amounts of telemetry data efficiently. It is used internally at Azure to power
    Azure Log Analytics, Azure Application Insights, and Time Series Insights. It
    has a powerful Python runtime with many popular packages available, and so provides
    the perfect service for performing anomaly detection or time-series analysis based
    on your custom models. In addition, it allows you to access its time-series data
    during ML modeling through a Python extension called **Kqlmagic** ([https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic](https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic)).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: When using Azure Machine Learning for model deployments, you can take advantage
    of all the Azure ecosystem and can expect to see model or endpoint integration
    with more and more Azure services over time.
  prefs: []
  type: TYPE_NORMAL
- en: Closing this chapter, we will dive deeper into two other integration options
    in the upcoming sections. We will have a look at deploying ML models through **Azure
    IoT Edge** to a gateway or device in the field, and we will look at how to utilize
    ML endpoints for data augmentation in **Power BI**.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Azure IoT Edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have discussed different ways to make our models run on systems in
    the cloud, be it on machines with CPUs, GPUs, or FPGAs, either as a batch-scoring
    process or as a real-time endpoint. Now, let's discuss another interesting deployment
    scenario, deploying real-time scorers to one to up to hundreds of thousands of
    devices in the field. The control of such devices and the processing of gathered
    telemetry and events fall under the topic of the so-called **Internet of Things**
    (**IoT**), which enables us to react in near real time to changes and critical
    problems in any sort of environment.
  prefs: []
  type: TYPE_NORMAL
- en: In these scenarios, the integration of ML allows us to distribute a model to
    a multitude of systems and devices simultaneously, allowing these so-called **edge
    devices** to execute the model on the local runtime in order to react to the result
    of the ML processing accordingly. This could be a local camera system that performs
    ML-powered image processing to react to intruders and send out alarms or any other
    scenario you might imagine.
  prefs: []
  type: TYPE_NORMAL
- en: To get a base understanding of how to achieve this utilizing the Azure platform,
    let's first have a look at how IoT scenarios are realized through the help of
    **Azure IoT Hub** and other services, and then discuss how this can be integrated
    with Azure Machine Learning and our trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding IoT solutions on Azure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The basis for any IoT architecture in Azure is Azure IoT Hub. This serves as
    a cloud gateway to communicate with devices and other gateways in the field and
    offers the ability to control them to a certain extent. On the one hand, it runs
    Azure Event Hubs underneath to be able to handle a huge amount of incoming telemetry
    through a distributed structure, not too different from Apache Kafka. On the other
    hand, it serves as a control instrument serving the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Device cataloging**: The ledger of all devices registered to Azure IoT Hub.
    Any device connected receives its own device name and connection configuration,
    defining how the direct connection between hub and device is secured, which happens
    using either a rotating key or a device certificate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device provisioning**: A service that allows devices to automatically register
    themselves to IoT Hub to obtain either a connection string with a key or a certificate.
    Useful if more than a handful of devices must be registered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device twin**: A configuration file that defines important properties for
    the device, which can be set or requested. In between the stream of telemetry,
    the device is asked to send this file sporadically, updating the state of the
    device in the cloud gateway. Therefore, the device twin always holds the most
    recent state of the device. This functionality is automatically implemented when
    using the **Azure IoT device SDK** on the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Command and control**: This is enabled through the **Azure IoT Service SDK**.
    Commands from a console or an external application can be used to either send
    new desired properties to single devices, define configurations for a group of
    devices, or send a predefined command that the device needs to understand and
    implement. This could be a request to restart the device or flash its firmware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and diagnostics**: A diagnostic view on any incoming and outgoing
    messaging from and to IoT Hub. It can be used to understand the throughput of
    incoming telemetry, understand any control plane information exchanged, and warn
    if a device is unreachable and malfunctioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to this cloud gateway, Azure offers a device runtime on the edge
    called Azure IoT Edge, which can be installed on a device or gateway. It is powered
    by the Moby Docker runtime ([https://mobyproject.org/](https://mobyproject.org/)),
    which allows users to deploy Docker containers to a device in the field. The setup
    of any solution operating in this runtime is defined by a **deployment manifest**
    that is set up for an edge device through a device twin configuration file in
    IoT Hub. This manifest defines the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IoT Edge agent**: Verifies and instantiates modules, checks their state during
    runtime, and reports back any configuration or runtime problem utilizing the device
    twin configuration file. It is the main module of the runtime and is *required*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IoT Edge hub**: Enables the IoT Edge runtime to mimic IoT Hub for additional
    devices connecting to this local edge device. This enables any form of complex
    hierarchy, while devices can use the same protocol communicating with an IoT Edge
    device as they would with IoT Hub. This module is *required*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container modules**: Defines the container images to be copied to the edge
    runtime. This is done by defining a link to the source files stored in Azure Container
    Registry. Besides any user-defined container that can be deployed in this manner,
    there are also a bunch of containerized versions of Azure services that can be
    sent to the runtime. This list includes Blob storage, an Azure Function app, certain
    Cognitive Services, and even a small, optimized version of a SQL server called
    **SQL Edge**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local communication via routing**: Defines the first option to connect modules
    together by setting direct connections between inputs and outputs of the various
    modules defined before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local communication via an MQTT broker**: Defines the second option to connect
    modules together. Instead of setting direct connections, a broker is used to which
    modules can subscribe. This broker also offers connections to external devices
    that understand how to talk to an MQTT broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the main components and options to consider when defining the deployment
    manifest.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The greatest strength that Azure IoT Edge brings to the table is the ability
    to define, manage, and version containers in the cloud, and deploy them to thousands
    of devices. With the help of device configurations, we can group devices and only
    target a certain group for a new test update, thus enabling best practices for
    DevOps in an IoT setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s briefly have a look at an example. *Figure 15.3* shows a simple
    setup for scoring a containerized ML model on incoming telemetry through Azure
    IoT Edge and its connection with Azure IoT Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – Azure IoT Hub connecting to the edge runtime ](img/B17928_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – Azure IoT Hub connecting to the edge runtime
  prefs: []
  type: TYPE_NORMAL
- en: The connections in *Figure 15.3* show the internal routing between containers,
    including actioning that takes place locally, while any insights from the ML scoring
    and any initial telemetry are sent additionally to the cloud for further analysis.
    This is the typical scenario for any ML model operating on the edge.
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge in mind, let's now have a look at how to integrate Azure
    Machine Learning in such an IoT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Azure Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054), *Preparing the Azure
    Machine Learning Workspace*, we learned that every Azure Machine Learning workspace
    comes with its own Azure Container Registry. We can now use this registry to achieve
    our goal. *Figure 15.4* shows an example of an end-to-end solution for ML on the
    edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – End-to-end ML on Azure IoT Edge scenario ](img/B17928_15_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – End-to-end ML on Azure IoT Edge scenario
  prefs: []
  type: TYPE_NORMAL
- en: 'It depicts the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting telemetry in a storage account, either through routing single messages
    from IoT Hub or through a batch upload from the Blob storage on the edge to the
    storage account in the cloud
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training an ML model on the captured data as we learned previously
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Registering a container including the trained model and dependencies in the
    existing Azure Container Registry of the Azure Machine Learning workspace
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating an IoT Edge deployment manifest defining an ML module sourced fromAzure
    Container Registry
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying the created configuration through Azure IoT Hub to the edge device
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through this setup, we are now able to deploy and control an ML model on the
    edge, enabling vast scenarios for running low-latency ML solutions on external
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested to try this out, feel free to follow the tutorial for
    setting up an example ML model on Azure IoT Edge, found here: [https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro](https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you are interested in further options for ML solutions on the edge,
    have a look at one of the newest additions to the Azure IoT space, called **Azure
    Percept** ([https://azure.microsoft.com/en-us/services/azure-percept/](https://azure.microsoft.com/en-us/services/azure-percept/)).
    It offers a ready-made hardware development kit for video and audio inferencing
    that works together with Azure IoT Hub and Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've had a glimpse into the world of IoT and scenarios for ML on the
    edge, let's have a look at how to utilize real-time ML endpoints with Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Power BI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most interesting integrations from an enterprise perspective is the
    Azure Machine Learning integration with Power BI. It allows us to utilize our
    ML endpoints to apply our models to data columns from the comfort of the built-in
    **Power Query editor**. Think for a second how powerful this concept of rolling
    out ML models to be used by data analysts in their BI tools is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try this out by utilizing the `sentiment-analysis-pbi` endpoint we created
    in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217), *Model Deployment, Endpoints,
    and Operations*, by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't done so already, download the Power BI Desktop application ([https://powerbi.microsoft.com/en-gb/desktop/](https://powerbi.microsoft.com/en-gb/desktop/))
    to your machine, run it, and log in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the `sentiment_examples.csv` file from the chapter repository, and
    select **Get Data** | **Text/CSV** to load the content of this local file into
    an in-memory dataset in Power BI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Power Query editor will open and will show you an icon of the file with
    the name and size. Right-click on that, and select **Text**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should be greeted by a table with one column. Rename the column `Phrases`,
    as shown in *Figure 15.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.5 – Sample phrases for sentiment analysis ](img/B17928_15_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 – Sample phrases for sentiment analysis
  prefs: []
  type: TYPE_NORMAL
- en: The editor gives you a lot of possibilities to apply transformations to this
    data. Looking at the menu, you should see a button on the far-right side called
    **Azure Machine Learning**. Click on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are logged in correctly, you should see all available endpoints in all
    the Azure Machine Learning workspaces you have access to. Select our previously
    created endpoint, `AzureML.sentiment-analysis-pbi`. In the `Phrases` column. This
    will be the input for our ML endpoint. *Figure 15.6* shows what this should look
    like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.6 – Choosing the right ML endpoint in Power BI ](img/B17928_15_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.6 – Choosing the right ML endpoint in Power BI
  prefs: []
  type: TYPE_NORMAL
- en: Click on **OK**. Power BI will now start sending the request to the endpoint.
    Please be aware that you might get a warning in one of the Power BI windows concerning
    data privacy, as we are sending potentially private data to another service. Please
    accept this by selecting the first checkbox, so the action can be performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a result, you should now see a new column called `AzureML.sentiment-analysis-pbi`,
    with a lot of fields denoted as `Record`. As our endpoints send more than one
    output, we receive a record. You can now click on each record individually, or
    you can click on the small button showing two arrows next to the column header
    name. This allows you to expand this `Record` column into multiple ones. Select
    all column names and press **OK**. *Figure 15.7* shows the result you should see:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.7 – Power BI sentiment results ](img/B17928_15_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.7 – Power BI sentiment results
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the model gives a label for each sentence (`NEGATIVE` or `POSITIVE`)
    and a confidence value score, denoting how sure the ML model is about the label
    given. The results are reasonably accurate, except perhaps for the fourth phrase.
  prefs: []
  type: TYPE_NORMAL
- en: You can now click **Close & Apply** in the upper left-hand corner, which will
    result in Power BI creating an ML-enhanced dataset, with which you could now build
    visuals in a report and eventually publish a report to the Power BI service in
    the cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see for yourself, integrating with Power BI is a quick and easy way
    to empower everyone to utilize your deployed ML endpoints with their business
    data, while not understanding much about the inner workings of the ML services.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to add some of your own phrases to play around with.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to convert ML models into a portable and executable
    format with ONNX, what an FPGA is, and how we can deploy a DNN featurizer to an
    FPGA VM through Azure Machine Learning. In addition, we learned how to integrate
    our ML models into various Azure services, such as Azure IoT Edge and Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion through the previous two chapters on the various
    options to deploy ML models for batch or real-time inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will bring everything we learned so far together to
    understand and build an end-to-end MLOps pipeline, enabling us to create an enterprise-ready
    and automated environment for any kind of process that requires the addition of
    ML.
  prefs: []
  type: TYPE_NORMAL
