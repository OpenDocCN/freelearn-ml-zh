- en: Analyzing Text Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析文本数据
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Preprocessing data using tokenization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记化进行数据预处理
- en: Stemming text data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本数据进行词干提取
- en: Converting text to its base form using lemmatization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词形还原将文本转换为基本形式
- en: Dividing text using chunking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分块划分文本
- en: Building a bag-of-words model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建词袋模型
- en: Building a text classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建文本分类器
- en: Identifying the gender of a name
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别名字的性别
- en: Analyzing the sentiment of a sentence
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析句子的情感
- en: Identifying patterns in text using topic modeling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主题建模在文本中识别模式
- en: Parts of speech tagging with spaCy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spaCy进行词性标注
- en: Word2Vec using gensim
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用gensim的Word2Vec
- en: Shallow learning for spam detection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用浅层学习进行垃圾邮件检测
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the recipes in this chapter, you will need the following files
    (available on GitHub):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章的食谱，您需要以下文件（可在GitHub上找到）：
- en: '`tokenizer.py`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer.py`'
- en: '`stemmer.py`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stemmer.py`'
- en: '`lemmatizer.py`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemmatizer.py`'
- en: '`chunking.py`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunking.py`'
- en: '`bag_of_words.py`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bag_of_words.py`'
- en: '`tfidf.py`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tfidf.py`'
- en: '`` `gender_identification.py` ``'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `gender_identification.py` ``'
- en: '`sentiment_analysis.py`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentiment_analysis.py`'
- en: '`topic_modeling.py`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topic_modeling.py`'
- en: '`data_topic_modeling.txt`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_topic_modeling.txt`'
- en: '`PosTagging.py`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PosTagging.py`'
- en: '`GensimWord2Vec.py`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GensimWord2Vec.py`'
- en: '`LogiTextClassifier.py`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogiTextClassifier.py`'
- en: '`spam.csv`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spam.csv`'
- en: Introduction
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Text analysis and **natural language processing** (**NLP**) are an integral
    part of modern artificial intelligence systems. Computers are good at understanding
    rigidly structured data with limited variety. However, when we deal with unstructured,
    free-form text, things begin to get difficult. Developing NLP applications is
    challenging because computers have a hard time understanding the underlying concepts.
    There are also many subtle variations to the way that we communicate things. These
    can be in the form of dialects, context, slang, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析和**自然语言处理**（NLP）是现代人工智能系统的一个基本组成部分。计算机擅长理解结构严格、变化有限的数据。然而，当我们处理非结构化、自由形式的文本时，事情开始变得困难。开发NLP应用具有挑战性，因为计算机很难理解其背后的概念。我们的交流方式也有很多微妙的变体，这些变体可能以方言、上下文、俚语等形式存在。
- en: In order to solve this problem, NLP applications are developed based on machine
    learning. These algorithms detect patterns in text data so that we can extract
    insights from them. Artificial intelligence companies make heavy use of NLP and
    text analysis in order to deliver relevant results. Some of the most common applications
    of NLP include search engines, sentiment analysis, topic modeling, part-of-speech
    tagging, and entity recognition. The goal of NLP is to develop a set of algorithms
    so that we can interact with computers in plain English. If we can achieve this,
    then we won't need programming languages to instruct computers on what they should
    do. In this chapter, we will look at a few recipes that focus on text analysis
    and how we can extract meaningful information from text data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，基于机器学习的NLP应用被开发出来。这些算法在文本数据中检测模式，以便我们可以从中提取见解。人工智能公司大量使用NLP和文本分析来提供相关结果。NLP最常见的一些应用包括搜索引擎、情感分析、主题建模、词性标注和实体识别。NLP的目标是开发一套算法，以便我们可以用普通的英语与计算机交互。如果我们能实现这一点，那么我们就无需编程语言来指导计算机应该做什么。在本章中，我们将探讨一些专注于文本分析和如何从文本数据中提取有意义信息的食谱。
- en: 'We will use a Python package called **Natural Language Toolkit** (**NLTK**)
    heavily in this chapter. Make sure that you install this before you proceed:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将大量使用名为**自然语言工具包**（NLTK）的Python包。在继续之前，请确保您已经安装了它：
- en: You can find the installation steps at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在[http://www.nltk.org/install.html](http://www.nltk.org/install.html)找到安装步骤。
- en: You will also need to install `NLTK Data`, which contains many corpora and trained
    models. This is an integral part of text analysis! You can find the installation
    steps at [http://www.nltk.org/data.html](http://www.nltk.org/data.html).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还需要安装`NLTK Data`，它包含许多语料库和训练模型。这是文本分析的一个基本组成部分！您可以在[http://www.nltk.org/data.html](http://www.nltk.org/data.html)找到安装步骤。
- en: Preprocessing data using tokenization
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标记化进行数据预处理
- en: '**Tokenization** is the process of dividing text into a set of meaningful pieces.
    These pieces are called **tokens**. For example, we can divide a chunk of text
    into words, or we can divide it into sentences. Depending on the task at hand,
    we can define our own conditions to divide the input text into meaningful tokens.
    Let''s take a look at how to do this.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**是将文本分割成一系列有意义的片段的过程。这些片段被称为**标记**。例如，我们可以将一大段文本分割成单词，或者将其分割成句子。根据手头的任务，我们可以定义自己的条件将输入文本分割成有意义的标记。让我们看看如何做到这一点。'
- en: Getting ready
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Tokenization is the first step in the computational analysis of the text and
    involves dividing the sequences of characters into minimal units of analysis called
    **tokens**. Tokens include various categories of text parts (words, punctuation,
    numbers, and so on), and can also be complex units (such as dates). In this recipe,
    we will illustrate how to divide a complex sentence into many tokens.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是文本计算分析的第一步，涉及将字符序列分割成称为**标记**的最小分析单位。标记包括各种文本部分类别（单词、标点、数字等），也可以是复杂单位（如日期）。在这个菜谱中，我们将展示如何将一个复杂的句子分割成许多标记。
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s look at how to preprocess data using tokenization:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用分词进行数据预处理：
- en: 'Create a new Python file and add the following lines (the full code is in the
    `tokenizer.py` file that''s already been provided to you). Let''s `import` the
    package and corpora:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并添加以下行（完整的代码在已经提供给你的`tokenizer.py`文件中）。让我们`导入`包和语料库：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s define some sample `text` for analysis:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一些用于分析的样本`text`：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s start with sentence tokenization. NLTK provides a sentence tokenizer,
    so let''s `import` that:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从句子分词开始。NLTK提供了一个句子标记器，所以让我们`导入`它：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run the sentence tokenizer on the input `text` and extract the tokens:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入`text`上运行句子标记器并提取标记：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Print the list of sentences to see whether it works correctly:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印句子列表以查看是否正确工作：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Word tokenization is very commonly used in NLP. NLTK comes with a couple of
    different word tokenizers. Let''s start with the basic word tokenizer:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词分词在NLP中非常常用。NLTK附带了几种不同的单词分词器。让我们从基本的单词分词器开始：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to split this punctuation into separate tokens, then you will need
    to use the `WordPunct` tokenizer:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想要将这个标点符号分割成单独的标记，那么你需要使用`WordPunct`标记器：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you run this code, you will see the following output on your Terminal:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将在你的终端上看到以下输出：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we illustrated how to divide a complex sentence into many tokens. To
    do this, three methods of the `nltk.tokenize` package were used—`sent_tokenize`,
    `word_tokenize`, and `WordPunctTokenizer`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何将一个复杂的句子分割成许多标记。为此，使用了`nltk.tokenize`包的三个方法——`sent_tokenize`、`word_tokenize`和`WordPunctTokenizer`：
- en: '`sent_tokenize` returns a sentence-tokenized copy of text, using NLTK''s recommended
    sentence tokenizer.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sent_tokenize`函数返回文本的句子标记副本，使用NLTK推荐的句子标记器。'
- en: '`word_tokenize` tokenizes a string to split off punctuation other than periods.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_tokenize`将字符串标记化以分割除句点以外的标点符号。'
- en: '`WordPunctTokenizer` tokenizes a text into a sequence of alphabetic and non-alphabetic
    characters, using the regexp `\w+|[^\w\s]+`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WordPunctTokenizer`使用正则表达式`\w+|[^\w\s]+`将文本标记化为一个由字母和非字母字符组成的序列。'
- en: There's more...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Tokenization is a procedure that, depending on the language that is analyzed,
    can be an extremely complex task. In English, for example, we could be content
    to consider taking sequences of characters that do not have spaces and the various
    punctuation marks. Languages such as Japanese or Chinese, in which words are not
    separated by spaces but the union of different symbols, can completely change
    the meaning, and the task is much more complex. But in general, even in languages
    with words separated by spaces, precise criteria must be defined, as punctuation
    is often ambiguous.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是一个过程，根据所分析的语言，可能是一个非常复杂的任务。例如，在英语中，我们可能满足于考虑没有空格的字符序列和各种标点符号。在像日语或中文这样的语言中，其中单词不是由空格分开，而是由不同符号的组合，这可能会完全改变意义，任务也更为复杂。但一般来说，即使在单词由空格分开的语言中，也必须定义精确的标准，因为标点符号通常是模糊的。
- en: See also
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'The official documentation of the `nltk.tokenize` package: [https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk.tokenize`包的官方文档：[https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html)'
- en: '*Tokenization* (from the Natural Language Processing Group at Stanford University):
    [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分词*（来自斯坦福大学自然语言处理组）：[https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)'
- en: Stemming text data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取文本数据
- en: When we deal with a text document, we encounter different forms of words. Consider
    the word **play.** This word can appear in various forms, such as play, plays,
    player, playing, and so on. These are basically families of words with similar
    meanings. During text analysis, it's useful to extract the base forms of these
    words. This will help us to extract some statistics to analyze the overall text.
    The goal of **stemming** is to reduce these different forms into a common base
    form. This uses a heuristic process to cut off the ends of words in order to extract
    the base form.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本文档时，我们会遇到不同形式的单词。考虑单词**play**。这个单词可以以各种形式出现，如play、plays、player、playing等。这些都是具有相似意义的单词家族。在文本分析过程中，提取这些单词的基本形式是有用的。这将帮助我们提取一些统计数据来分析整个文本。**词干提取**的目标是将这些不同形式减少到共同的基形式。这使用启发式过程来截断单词的末尾以提取基形式。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `nltk.stem` package that offers a processing
    interface for removing morphological affixes from words. Different stemmers are
    available for different languages. For the English language, we will use `PorterStemmer`,
    `LancasterStemmer`, and `SnowballStemmer`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`nltk.stem`包，它提供了一个用于从单词中移除形态词缀的处理接口。对于不同的语言，有不同的词干提取器可用。对于英语，我们将使用`PorterStemmer`、`LancasterStemmer`和`SnowballStemmer`。
- en: How to do it...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s look at how to stem text data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何词干提取文本数据：
- en: 'Create a new Python file and import the following packages (the full code is
    in the `stemmer.py` file that''s already been provided to you):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在已经提供给你的`stemmer.py`文件中）：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s define a few `words` to play with, as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一些用于操作的`words`，如下所示：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ll define a list of `stemmers` that we want to use:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个我们想要使用的`stemmers`列表：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Initialize the required objects for all three stemmers:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有三个词干提取器所需的必要对象：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In order to print the output data in a neat tabular form, we need to format
    it in the correct way:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了以整洁的表格形式打印输出数据，我们需要以正确的方式对其进行格式化：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s iterate through the list of `words` and stem them by using the three
    stemmers:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们遍历`words`列表，并使用以下三个词干提取器对其进行词干提取：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you run this code, you will see the following output in your Terminal. Observe
    how the LANCASTER stemmer behaves differently for a couple of words:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将在你的终端中看到以下输出。观察LANCASTER词干提取器对几个单词的不同行为：
- en: '![](img/02ff3262-46ca-4b94-9475-f565c52a98ad.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02ff3262-46ca-4b94-9475-f565c52a98ad.png)'
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: All three stemming algorithms basically aim at achieving the same thing. The
    difference between the three stemming algorithms is basically the level of strictness
    with which they operate. If you observe the output, you will see that the LANCASTER
    stemmer is stricter than the other two stemmers. The PORTER stemmer is the least
    in terms of strictness, and the LANCASTER is the strictest. The stemmed words
    that we get from the LANCASTER stemmer tend to get confusing and obfuscated. The
    algorithm is really fast, but it will reduce the words a lot. So, a good rule
    of thumb is to use the SNOWBALL stemmer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种词干提取算法基本上都旨在实现相同的目标。三种词干提取算法之间的区别基本上在于它们操作的严格程度。如果你观察输出，你会看到LANCASTER词干提取器比其他两个词干提取器更严格。PORTER词干提取器在严格性方面是最小的，而LANCASTER是最严格的。从LANCASTER词干提取器得到的词干往往显得混乱和模糊。该算法非常快，但它会大量减少单词。因此，一个好的经验法则是使用SNOWBALL词干提取器。
- en: There's more...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Stemming is the process of reducing the inflected form of a word to its root
    form, called the **stem**. The stem doesn''t necessarily correspond to the morphological
    root (lemma) of the word: it''s normally sufficient that the related words are
    mapped to the same stem, even if the latter isn''t a valid root for the word.
    The creation of a stemming algorithm has been a prevalent issue in computer science.
    The stemming process is applied in search engines for query expansion, and in
    other natural language processing problems.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是将单词的屈折形式还原到其词根形式的过程，称为**词干**。词干不一定对应于单词的形态学词根（词元）：通常，相关单词被映射到相同的词干就足够了，即使后者不是单词的有效词根。创建词干提取算法一直是计算机科学中的一个普遍问题。词干提取过程应用于搜索引擎的查询扩展，以及其他自然语言处理问题。
- en: See also
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'The official home page for distribution of the Porter Stemming Algorithm, written
    and maintained by its author, Martin Porter: [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波特词干提取算法的官方分发主页，由其作者马丁·波特编写和维护：[https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)
- en: 'The official documentation of the `nltk.stem` package: [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk.stem`包的官方文档：[https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
- en: '*Stemming* (from Wikipedia): [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词干提取*（来自维基百科）：[https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)'
- en: Converting text to its base form using lemmatization
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词形还原将文本转换为它的基本形式
- en: The goal of lemmatization is also to reduce words to their base forms, but this
    is a more structured approach. In the previous recipe, you saw that the base words
    that we obtained using stemmers don't really make sense. For example, the word
    wolves was reduced to wolv, which is not a real word. Lemmatization solves this
    problem by doing things with a vocabulary and morphological analysis of words.
    It removes inflectional word endings, such as -ing or -ed, and returns the base
    form of a word. This base form is known as the lemma. If you lemmatize the word
    `wolves`, you will get `wolf` as the output. The output depends on whether the
    token is a verb or a noun.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原的目标也是将单词还原到其基本形式，但这是一种更结构化的方法。在先前的菜谱中，您看到我们使用词干提取器获得的基词实际上并没有什么意义。例如，单词wolves被还原为wolv，这并不是一个真正的单词。词形还原通过使用词汇和词的形态学分析来解决这个问题。它移除了屈折词尾，如-ing或-ed，并返回单词的基本形式。这种基本形式被称为词元。如果您对单词`wolves`进行词形还原，您将得到`wolf`作为输出。输出取决于标记是动词还是名词。
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `nltk.stem` package to reducing a word's inflected
    form to its canonical form, called a **lemma**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`nltk.stem`包将单词的屈折形式还原到其规范形式，称为**词元**。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s look at how to convert text to its base form using lemmatization:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用词形还原将文本转换为它的基本形式：
- en: 'Create a new Python file and import the following package (the full code is
    in the `lemmatizer.py` file that''s already been provided to you):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并导入以下包（完整代码在已提供的`lemmatizer.py`文件中）：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s define the same set of words that we used during stemming:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义与我们在词干提取过程中使用的相同的一组单词：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will compare two lemmatizers: the `NOUN` and `VERB` lemmatizers. Let''s
    list them:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将比较两个词形还原器：`NOUN`和`VERB`词形还原器。让我们列出它们：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the object based on the `WordNet` lemmatizer:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于词形还原器`WordNet`创建对象：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In order to print the output in a tabular form, we need to format it in the
    right way:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了以表格形式打印输出，我们需要以正确的方式格式化它：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Iterate through the words and lemmatize them:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历单词并对它们进行词形还原：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you run this code, you will see the following output:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您运行此代码，您将看到以下输出：
- en: '![](img/8f65ff6c-7ca0-40a1-91fc-126bc6e796ef.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f65ff6c-7ca0-40a1-91fc-126bc6e796ef.png)'
- en: Observe how the `NOUN` and `VERB` lemmatizers differ when they lemmatize the
    word, as shown in the preceding screenshot
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 观察当词形还原器对单词进行词形还原时，`NOUN`和`VERB`词形还原器的区别，如图所示的前一个截图
- en: How it works...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Lemmatization is the process of reducing a word's inflected form to its canonical
    form, called a lemma. In the processing of natural language, lemmatization is
    the algorithmic process that automatically determines the word of a given word.
    The process may involve other language processing activities, such as morphological
    and grammatical analysis. In many languages, words appear in different inflected
    forms. The combination of the canonical form with its part of speech is called
    the **lexeme** of the word. A lexeme is, in structural lexicology, the minimum
    unit that constitutes the lexicon of a language. Hence, every lexicon of a language
    may correspond to its registration in a dictionary in the form of a lemma.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是将单词的屈折形式还原为其规范形式的过程，称为词元。在自然语言处理过程中，词形还原是自动确定给定单词的单词的算法过程。这个过程可能涉及其他语言处理活动，如形态学和语法分析。在许多语言中，单词以不同的屈折形式出现。规范形式与其词性的组合称为单词的**词素**。在结构词理学中，词素是构成语言词汇的最小单位。因此，每种语言的词汇可能对应于词典中的词元形式。
- en: There's more...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In NLTK, for lemmatization, `WordNet` is available, but this resource is limited
    to the English language. It's a large lexical database of the English language.
    In this package, names, verbs, adjectives, and adverbs are grouped into sets of
    cognitive synonyms (**synsets**), each of which expresses a distinct concept.
    The synsets are interconnected by means of semantic and lexical conceptual relationships.
    The resulting network of significantly related words and concepts can be navigated
    with the browser. `WordNet` groups words according to their meanings, connecting
    not only word forms (strings of letters) but specific words. Hence, the words
    that are in close proximity to each other in the network are semantically disambiguated.
    In addition, `WordNet` labels the semantic relationships between words.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLTK中，对于词形还原，有`WordNet`可用，但这个资源仅限于英语语言。这是一个包含大量英语词汇的大型词汇数据库。在这个包中，名词、动词、形容词和副词被分组为认知同义词集（**synsets**），每个集合都表达一个独特概念。synsets通过语义和词汇概念关系相互连接。通过浏览器可以导航这个显著相关的单词和概念的网络。`WordNet`根据单词的意义分组单词，不仅连接词形（字母字符串），还连接特定的单词。因此，在网络中彼此靠近的单词在语义上是区分的。此外，`WordNet`还标记单词之间的语义关系。
- en: See also
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: '*Keras 2.x Projects* by Giuseppe Ciaburro, from Packt Publishing'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giuseppe Ciaburro所著的《Keras 2.x Projects》，由Packt Publishing出版
- en: 'The official documentation of the `nltk.stem` package: [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk.stem`包的官方文档：[https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
- en: '*Lemmatisation* (from Wikipedia): [https://en.wikipedia.org/wiki/Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词形还原*（来自维基百科）：[https://en.wikipedia.org/wiki/Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)'
- en: Dividing text using chunking
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用块划分文本
- en: '**Chunking** refers to dividing the input text into pieces, which are based
    on any random condition. This is different from tokenization in the sense that
    there are no constraints, and the chunks do not need to be meaningful at all.
    This is used very frequently during text analysis. While dealing with large text
    documents, it''s better to do it in chunks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**块划分**指的是根据任何随机条件将输入文本分割成片段。这与分词不同，因为没有任何限制，并且块不需要有任何意义。这在文本分析中非常常用。在处理大型文本文档时，最好分块进行。'
- en: How to do it...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s look at how to divide text by using chunking:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用块划分来划分文本：
- en: 'Create a new Python file and import the following packages (the full code is
    in the `chunking.py` file that''s already been provided to you):'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并导入以下包（完整代码在已提供的`chunking.py`文件中）：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s define a function to `split` the text into chunks. The first step is
    to divide the text based on spaces:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来`split`文本成块。第一步是根据空格划分文本：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Initialize a couple of required variables:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一些必需的变量：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s iterate through the `words`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们遍历`words`：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once you have hit the required number of words, reset the variables:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦达到所需的单词数量，重置变量：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Append the chunks to the `output` variable, and return it:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将块追加到`output`变量中，并返回它：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now define the main function. Load the data from the `brown` corpus.
    We will use the first 10,000 words:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以定义主函数。从`brown`语料库加载数据。我们将使用前10,000个单词：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the number of words in each chunk:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义每个块中的单词数量：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Initialize a couple of relevant variables:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一些相关变量：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Call the `splitter` function on this text `data` and `print` the output:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此文本 `data` 上调用 `splitter` 函数并打印输出：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you run this code, you will see the number of chunks that were generated
    printed in the Terminal:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将在终端中看到生成的分块数量被打印出来：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Chunking (also called **shallow parsing**) is the analysis of a proposition,
    which is formed in a simple form by a subject and a predicate. The subject is
    typically a noun phrase, while the predicate is a verbal phrase formed by a verb
    with zero or more complements and adverbs. A chunk is made up of one or more adjacent
    tokens.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 分块（也称为**浅层解析**）是对一个命题的分析，该命题由一个主语和一个谓语以简单形式构成。主语通常是名词短语，而谓语是由一个动词和零个或多个补语及副词构成的动词短语。一个分块由一个或多个相邻的标记组成。
- en: There are numerous approaches to the problem of chunking. For example, in the
    assigned task, a chunk is represented as a group of words delimited by square
    brackets, for which a tag representing the type of chunk is indicated. The dataset
    that was used was derived from a given corpora by taking the part related to journal
    articles and extracting chunks of information from the syntactic trees of the
    corpora.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分块问题有众多方法。例如，在指定的任务中，一个分块被表示为一个由方括号分隔的词组，其中有一个表示分块类型的标签。所使用的数据集是从给定的语料库中提取与期刊文章相关的部分，并从语料库的句法树中提取信息块得到的。
- en: There's more...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Brown University Standard Corpus of Present-Day American English (or simply,
    the `brown` corpus) is a corpus that was compiled in the 1960s by Henry Kucera
    and W. Nelson Francis at Brown University, Providence, Rhode Island. It contains
    500 text extracts in English, obtained from works published in the United States
    of America in 1961, for a total of about one million words.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 布朗大学当代美国英语标准语料库（或简称 `brown` 语料库）是由亨利·库切拉（Henry Kucera）和W. Nelson Francis在1960年代在罗德岛普罗维登斯的布朗大学编制的语料库。它包含500个英文文本摘录，这些文本摘录来自1961年在美国出版的作品，总共有大约一百万个单词。
- en: See also
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'The official documentation of the `nltk.corpus` package: [https://www.nltk.org/api/nltk.corpus.html](https://www.nltk.org/api/nltk.corpus.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk.corpus` 包的官方文档：[https://www.nltk.org/api/nltk.corpus.html](https://www.nltk.org/api/nltk.corpus.html)'
- en: '*Basics of Natural Language Processing* (from the University of Zagreb): [https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf](https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言处理基础*（来自萨格勒布大学）：[https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf](https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf)'
- en: Building a bag-of-words model
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建词袋模型
- en: When it comes to dealing with text documents that consist of millions of words,
    converting them into numerical representations is necessary. The reason for this
    is to make them usable for machine learning algorithms. These algorithms need
    numerical data so that they can analyze them and output meaningful information.
    This is where the **bag-of-words** approach comes into the picture. This is basically
    a model that learns a vocabulary from all of the words in all the documents. It
    models each document by building a histogram of all of the words in the document.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到处理由数百万个单词组成的文本文档时，将它们转换为数值表示是必要的。这样做的原因是为了使它们能够用于机器学习算法。这些算法需要数值数据以便分析并输出有意义的信息。这就是**词袋**方法出现的地方。这基本上是一个从所有文档中的所有单词中学习词汇的模型。它通过构建文档中所有单词的直方图来对每个文档进行建模。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will build a bag-of-words model to extract a document term
    matrix, using the `sklearn.feature_extraction.text` package.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用 `sklearn.feature_extraction.text` 包构建一个词袋模型来提取文档词矩阵。
- en: How to do it...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let''s look at how to build a bag-of-words model, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个词袋模型，如下所示：
- en: 'Create a new Python file and import the following packages (the full code is
    in the `bag_of_words.py` file that''s already been provided to you):'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 Python 文件并导入以下包（完整的代码已在提供的 `bag_of_words.py` 文件中）：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s define the `main` function. Load the input `data` from the `brown` corpus:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义 `main` 函数。从 `brown` 语料库中加载输入 `data`：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Divide the text data into five chunks:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本数据分为五个分块：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a dictionary that is based on these text chunks:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个基于以下文本分块的字典：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The next step is to extract a document term matrix. This is basically a matrix
    that counts the number of occurrences of each word in the document. We will use
    `scikit-learn` to do this because it has better provisions, compared to NLTK,
    for this particular task. Import the following package:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the object and extract the document term matrix:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Extract the vocabulary from the `vectorizer` object and print it:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Print the `Document term matrix`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To print it in a tabular form, you will need to format this, as follows:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Iterate through the words and print the number of times each word has occurred
    in different chunks:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you run this code, you will see two main things printed in the Terminal.
    The first output is the vocabulary, as shown in the following screenshot:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The second thing is the Document term matrix, which is pretty long. The first
    few lines will look like the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e469a5e4-b7b8-4b75-bb9e-9f70e7f68682.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following sentences:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: The brown dog is running.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: The black dog is in the black room.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: Running in the room is forbidden.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you consider all three of these sentences, you will have the following nine
    unique words:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: the
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brown
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dog
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: is
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: running
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: black
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: room
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forbidden
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s convert each sentence into a histogram, using the count of words
    in each sentence. Each feature vector will be nine-dimensional, because we have
    nine unique words:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: [1, 1, 1, 1, 1, 0, 0, 0, 0]'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: [2, 0, 1, 1, 0, 2, 1, 1, 0]'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: [0, 0, 0, 1, 1, 0, 1, 1, 1]'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have extracted these feature vectors, we can use machine learning algorithms
    to analyze them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model is a method that's used in information retrieval and
    in the processing of the natural language in order to represent documents by ignoring
    the word order. In this model, each document is considered to contain words, similar
    to a stock exchange; this allows for the management of these words based on lists,
    where each stock contains certain words from a list.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.CountVectorizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Bag-of-Words Model* (from Wikipedia): [https://en.wikipedia.org/wiki/Bag-of-words_model](https://en.wikipedia.org/wiki/Bag-of-words_model)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text classifier
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main aim of text classification is to sort text documents into different
    classes. This is a vital analysis technique in NLP. We will use a technique that
    is based on a statistic called **tf-idf**, which stands for **term frequency**-**inverse
    document frequency**. This is an analysis tool that helps us to understand how
    important a word is to a document in a set of documents. This serves as a feature
    vector that's used to categorize documents.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的主要目的是将文本文档分类到不同的类别中。这是NLP中的一个重要分析技术。我们将使用一种基于**tf-idf**统计方法的技巧，它代表**词频**-**逆文档频率**。这是一个分析工具，帮助我们了解一个词在文档集中的重要性。这作为了一个用于分类文档的特征向量。
- en: Getting ready
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the term frequency-inverse document frequency method
    to evaluate the importance of a word for a document in a collection or a corpus,
    and to build a text classifier.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用词频-逆文档频率方法来评估一个词在集合或语料库中对于一个文档的重要性，并构建一个文本分类器。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s look at how to build a text classifier:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个文本分类器：
- en: 'Create a new Python file and import the following package (the full code is
    in the `tfidf.py` file that''s already been provided to you):'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并导入以下包（完整的代码在已经提供给你的`tfidf.py`文件中）：
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s select a list of categories and name them using a dictionary mapping.
    These categories are available as a part of the news groups dataset that we just
    imported:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们选择一个类别列表并使用字典映射命名它们。这些类别是我们刚刚导入的新闻组数据集的一部分：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Load the training data based on the categories that we just defined:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据我们刚刚定义的类别加载训练数据：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Import the feature extractor:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入特征提取器：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Extract the features by using the training data:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据提取特征：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We are now ready to train the classifier. We will use the multinomial Naive
    Bayes classifier:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好训练分类器了。我们将使用多项式朴素贝叶斯分类器：
- en: '[PRE47]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define a couple of random input sentences:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义几个随机的输入句子：
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the `tfidf_transformer` object and train it:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`tfidf_transformer`对象并对其进行训练：
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once we have the feature vectors, train the multinomial Naive Bayes classifier
    using this data:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了特征向量，就使用这些数据训练多项式朴素贝叶斯分类器：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Transform the input data using the word counts:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词频转换输入数据：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Transform the input data using the `tfidf_transformer` module:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tfidf_transformer`模块转换输入数据：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Predict the output categories of these input sentences by using the trained
    classifier:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的分类器预测这些输入句子的输出类别：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Print the output, as follows:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式打印输出：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If you run this code, you will see the following output printed in your Terminal:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将在你的终端中看到以下输出：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: How it works...
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The tf-idf technique is used frequently in information retrieval. The goal is
    to understand the importance of each word within a document. We want to identify
    words that occur many times in a document. At the same time, common words such
    as **is** and **be** don't really reflect the nature of the content. So, we need
    to extract the words that are true indicators. The importance of each word increases
    as the count increases. At the same time, as it appears a lot, the frequency of
    this word increases, too. These two things tend to balance each other out. We
    extract the term counts from each sentence. Once we have converted this to a feature
    vector, we can train the classifier to categorize these sentences.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf技术常用于信息检索。目标是理解文档中每个词的重要性。我们想要识别在文档中多次出现的词。同时，像**is**和**be**这样的常见词并不能真正反映内容的本质。因此，我们需要提取真正的指示词。每个词的重要性随着计数的增加而增加。同时，随着它出现的频率增加，这个词的频率也会增加。这两者往往相互平衡。我们从每个句子中提取词频。一旦我们将其转换为特征向量，我们就可以训练分类器来对这些句子进行分类。
- en: The **term frequency** (**TF**) measures how frequently a word occurs in a given
    document. As multiple documents differ in length, the numbers in the histogram
    tend to vary a lot. So, we need to normalize this so that it becomes a level playing
    field. To achieve normalization, we can divide the term-frequency by the total
    number of words in a given document. The **inverse document frequency** (**IDF**)
    measures the importance of a given word. When we compute the TF, all words are
    considered to be equally important. To counterbalance the frequencies of commonly
    occurring words, we need to weigh them down and scale up the rare ones. We need
    to calculate the ratio of the number of documents with the given word and divide
    it by the total number of documents. The IDF is calculated by taking the negative
    algorithm of this ratio.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple words, such as **is** or **the**, tend to appear a lot in various documents.
    However, this doesn't mean that we can characterize the document based on these
    words. At the same time, if a word appears a single time, that is not useful,
    either. So, we look for words that appear a number of times, but not so much that
    they become noisy. This is formulated in the tf-idf technique and is used to classify
    documents. Search engines frequently use this tool to order search results by
    relevance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.feature_extraction.text.TfidfTransformer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the following page: *What does tf-idf mean?*: [http://www.tfidf.com/](http://www.tfidf.com/)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the gender of a name
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying the gender of a name is an interesting task in NLP. We will use
    the heuristic that the last few characters in a name is its defining characteristic.
    For example, if the name ends with **la**, it's most likely a female name, such
    as Angela or Layla. On the other hand, if the name ends with **im**, it's most
    likely a male name, such as Tim or Jim. As we aren't sure of the exact number
    of characters to use, we will experiment with this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the names corpora to extract labeled names, and
    then we will classify the gender based on the final part of the name.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to identify the gender:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `gender_identification.py` file that''s already been provided to you):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We need to define a function to extract features from input words:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Let''s define the main function. We need some labeled training data:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Seed the random number generator and shuffle the training data:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define some input names to play with:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'As we don''t know how many ending characters we need to consider, we will sweep
    the parameter space from `1` to `5`. Each time, we will extract the features,
    as follows:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Divide this into train and test datasets:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We will use the Naive Bayes classifier to do this:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Evaluate the `classifier` model for each value in the parameter space:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'If you run this code, you will see the following output printed in your Terminal:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: How it works...
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the names corpus to extract labeled names, and then
    we classified the gender based on the final part of the name. A Naive Bayes classifier
    is a supervised learning classifier that uses Bayes' theorem to build the model.
    This topic was addressed in the *Building a Naive Bayes classifier* recipe in [Chapter
    2](102c5690-d978-4a56-a586-1f741cde6b3d.xhtml)*, Constructing a Classifier*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bayesian classifier is called naive because it ingenuously assumes that
    the presence or absence of a particular characteristic in a given class of interest
    is not related to the presence or absence of other characteristics, greatly simplifying
    the calculation. Let's go ahead and build a Naive Bayes classifier.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 2](102c5690-d978-4a56-a586-1f741cde6b3d.xhtml)*, Constructing a Classifier*'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Keras 2.x Projects* by Giuseppe Ciaburro, from Packt Publishing'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The official documentation of the `nltk.classify`package: [http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier](http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayes'' Theorem* (from Stanford Encyclopedia of Philosophy): [https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the sentiment of a sentence
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sentiment analysis** is one of the most popular applications of NLP. Sentiment
    analysis refers to the process of determining whether a given piece of text is
    positive or negative. In some variations, we consider neutral as a third option.
    This technique is commonly used to discover how people feel about a particular
    topic. This is used to analyze the sentiments of users in various forms, such
    as marketing campaigns, social media, e-commerce, and so on.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will analyze the sentiment of a sentence by using a Naive
    Bayes classifier, starting with the data contained in the `movie_reviews` corpus.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to analyze the sentiment of a sentence:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `sentiment_analysis.py` file that''s already been provided to you):'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Define a function to extract the features:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We need training data for this, so we will use the movie reviews in NLTK:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s separate them into positive and negative reviews:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Divide the data into train and test datasets:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Extract the features:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We will use a `NaiveBayesClassifier`. Define the object and train it:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The `classifier` object contains the most informative words that it obtained
    during analysis. These words basically have a strong say in what''s classified
    as a positive or a negative review. Let''s print them out:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Create a couple of random input sentences:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Run the classifier on those input sentences and obtain the predictions:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Print the output:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'If you run this code, you will see three main things printed in the Terminal.
    The first is the accuracy, as shown in the following code snippet:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The next item is a list of the most informative words:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The last item is the list of predictions, which are based on the input sentences:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used NLTK's Naive Bayes classifier for our task here. In the feature extractor
    function, we basically extracted all the unique words. However, the NLTK classifier
    needs the data to be arranged in the form of a dictionary. Hence, we arranged
    it in such a way that the NLTK `classifier` object can ingest it. Once we divided
    the data into training and testing datasets, we trained the classifier to categorize
    the sentences into positive and negative ones.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the top informative words, you can see that we have words such
    as outstanding to indicate positive reviews and words such as insulting to indicate
    negative reviews. This is interesting information, because it tells us what words
    are being used to indicate strong reactions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term sentiment analysis refers to the use of NLP techniques, text analysis,
    and computational linguistics to find information in written or spoken text sources.
    If this subjective information is taken from large amounts of data, and therefore
    from the opinions of large groups of people, sentiment analysis can also be called
    **opinion mining**.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.corpus` package: [https://www.nltk.org/api/nltk.corpus.html](https://www.nltk.org/api/nltk.corpus.html)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.classify` package: [http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier](http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sentiment Analysis* (from Stanford University): [https://web.stanford.edu/class/cs124/lec/sentiment.pdf](https://web.stanford.edu/class/cs124/lec/sentiment.pdf)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns in text using topic modeling
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** refers to the process of identifying hidden patterns in
    text data. The goal is to uncover a hidden thematic structure in a collection
    of documents. This will help us to organize our documents in a better way, so
    that we can use them for analysis. This is an active area of research in NLP.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a library called `gensim` to identify patterns in
    text, using topic modeling.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to identify patterns in text by using topic modeling:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `topic_modeling.py` file that''s already been provided to you):'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Define a function to load the input data. We will use the `data_topic_modeling.txt`
    text file that has already been provided to you:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let''s define a `class` to preprocess the text. This preprocessor will take
    care of creating the required objects and extracting the relevant features from
    the input text:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We need a list of stop words so that we can exclude them from analysis. These
    are common words, such as **in**, **the**, **is**, and so on:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Define the `SnowballStemmer` module:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Define a processor function that takes care of tokenization, stop word removal,
    and stemming:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Remove the stop words from the text:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Perform stemming on the tokens:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Return the processed tokens:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We are now ready to define the main function. Load the input data from the
    text file:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Define an object that is based on the class that we defined:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We need to process the text in the file and extract the processed tokens:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Create a dictionary that is based on tokenized documents so that it can be
    used for topic modeling:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We need to create a document term matrix using the processed tokens, as follows:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Let''s suppose that we know that the text can be divided into two topics. We
    will use a technique called **latent Dirichlet allocation** (**LDA**) for topic
    modeling. Define the required parameters and initialize the `LdaModel` object:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Once this has identified the two topics, we can see how it''s separating these
    two topics by looking at the most contributed words:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The full code is in the `topic_modeling.py` file. If you run this code, you
    will see the following printed in your Terminal:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: How it works...
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** works by identifying the important words or themes in a
    document. These words tend to determine what the topic is about. We use a regular
    expression tokenizer, because we just want the words, without any punctuation
    or other kinds of tokens. Hence, we use this to extract the tokens. The stop word
    removal is another important step, because this helps us to eliminate the noise
    caused by words such as **is** or **the**. After that, we need to stem the words
    to get to their base forms. This entire thing is packaged as a preprocessing block
    in text analysis tools. That is what we are doing here, as well!'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: We use a technique called LDA to model the topics. LDA basically represents
    the documents as a mixture of different topics that tend to spit out words. These
    words are spat out with certain probabilities. The goal is to find these topics!
    This is a generative model that tries to find the set of topics that are responsible
    for the generation of the given set of documents.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see from the output, we have words such as talent and train to characterize
    the sports topic, whereas we have encrypt to characterize the cryptography topic.
    We are working with a really small text file, which is why some words might seem
    less relevant. Obviously, the accuracy will improve if you work with a larger
    dataset.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `gensim` library: [https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Latent Dirichlet Allocation* (from MIT): [http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Topic Modeling* (from Columbia University): [http://www.cs.columbia.edu/~blei/topicmodeling.html](http://www.cs.columbia.edu/~blei/topicmodeling.html)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of speech tagging with spaCy
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Parts**-**of**-**speech tagging** (**PoS tagging**) is the process of labeling
    the words that correspond to particular lexical categories. The common linguistic
    categories include nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions,
    and so on.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a library called `spacy` to perform PoS tagging.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform PoS tagging using `spacy`:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `PosTagging.py` file that''s already been provided to you):'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Load the `en_core_web_sm` model:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Let''s define an input text:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: As a source, I used a passage based on the novel *The Adventures of Huckleberry
    Finn* by Mark Twain.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will perform a PoS tagging:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'The following results are returned:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: How it works...
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PoS tagging involves assigning a tag to each word of a document/corpus. The
    choice of the tagset to use depends on the language. The input is a string of
    words and the tagset to be used, and the output is the association of the best
    tag with each word. There may be multiple tags compatible with a word (**ambiguity**).
    The task of the PoS tagger is to solve these ambiguities by choosing the most
    appropriate tags, based on the context in which the word is located.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform a PoS tagging, we used the `spacy` library. This library extracts
    linguistic features, such as PoS tags, dependency labels, and named entities,
    customizing the tokenizer and working with the rule-based matcher.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `en_core_web_sm` model, use the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: See also
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `spacy` library: [https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Parts-of-Speech Tagging* (from New York University): [https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf](https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf)'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec using gensim
  id: totrans-409
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Word embedding** allows us to memorize both the semantic and syntactic information
    of words, starting with an unknown corpus and constructing a vector space in which
    the vectors of words are closer if the words occur in the same linguistic contexts,
    that is, if they are recognized as semantically similar. Word2Vec is a set of
    templates that are used to produce word embedding; the package was originally
    created in C by Tomas Mikolov, and was then implemented in Python and Java.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `gensim` library to build a Word2Vec model.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform word embedding by using `gensim`:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `GensimWord2Vec.py` file that''s already been provided to you):'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Build a model based on the Word2Vec methodology:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Let''s extract the vocabulary from the data and put it into a `list`:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Now, we will find similarities with the word `''science''`:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Finally, we will `print` the `data`:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The following results will be returned:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: How it works...
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec is a simple two-layer artificial neural network that was designed to
    process natural language; the algorithm requires a corpus in the input and returns
    a set of vectors that represent the semantic distributions of words in the text.
    For each word contained in the corpus, in a univocal way, a vector is constructed
    in order to represent it as a point in the created multidimensional space. In
    this space, the words will be closer if they are recognized as semantically more
    similar.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we used the Australian National Corpus (`abc`), a great collection
    of language data, both text-based and digital. To use this corpus, you must download
    it with the following code:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: See also
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `gensim` library: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Word2vec* (from Wikipedia): [https://en.wikipedia.org/wiki/Word2vec](https://en.wikipedia.org/wiki/Word2vec)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow learning for spam detection
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spamming** means sending large amounts of unwanted messages (usually commercial).
    It can be implemented through any medium, but the most commonly used are email
    and SMS. The main purpose of spamming is advertising, from the most common commercial
    offers to proposals for the sale of illegal material, such as pirated software
    and drugs without a prescription, and from questionable financial projects to
    genuine attempts at fraud.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a logistic regression model for spam detection.
    To do this, a collection of labeled SMS messages collected for mobile phone spam
    research will be used. This dataset comprises of 5,574 real English non-encoded
    messages, tagged according to whether they are legitimate (`ham`) or spamming
    (`spam`).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform shallow learning for spam detection:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `LogiTextClassifier.py` file that''s already been provided to you):'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Load the `spam.csv` file that was provided to you:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Let''s extract the data for training and testing:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'We need to `vectorize` the text data contained in the DataFrame:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'We can now build the logistic regression model:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Define two SMS messages as test data:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Finally, we will perform a prediction by using the model:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'The following results will be returned:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: These indicate that the first SMS was identified as `spam`, while the second
    SMS was identified as `ham`.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression analysis is a method for estimating the regression function
    that best links the probability of a dichotomous attribute with a set of explanatory
    variables. **Logistic assault** is a nonlinear regression model that's used when
    the dependent variable is dichotomous. The objective of the model is to establish
    the probability with which an observation can generate one or the other values
    of the dependent variable; it can also be used to classify the observations into
    two categories, according to their characteristics.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the measurement scale of the dependent variable, logistic regression
    analysis is distinguished from linear regression because a normal distribution
    of *y* is assumed for this, whereas if *y* is dichotomous, its distribution is
    obviously binomial. Similarly, in linear regression analysis, the *y* estimate
    obtained from the regression varies from -∞ to + ∞, while in logistic regression
    analysis, the *y* estimate varies between 0 and 1.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.linear_model.LogisticRegression()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.TfidfVectorizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression Analysis with R* by Giuseppe Ciaburro, from Packt Publishing'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logistic Regression* (from the University of Sheffield): [https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf](https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
