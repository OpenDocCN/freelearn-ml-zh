- en: Analyzing Text Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data using tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting text to its base form using lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing text using chunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a bag-of-words model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the gender of a name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the sentiment of a sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns in text using topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of speech tagging with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec using gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow learning for spam detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the recipes in this chapter, you will need the following files
    (available on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stemmer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lemmatizer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunking.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bag_of_words.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tfidf.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `gender_identification.py` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sentiment_analysis.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topic_modeling.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_topic_modeling.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PosTagging.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GensimWord2Vec.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogiTextClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spam.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text analysis and **natural language processing** (**NLP**) are an integral
    part of modern artificial intelligence systems. Computers are good at understanding
    rigidly structured data with limited variety. However, when we deal with unstructured,
    free-form text, things begin to get difficult. Developing NLP applications is
    challenging because computers have a hard time understanding the underlying concepts.
    There are also many subtle variations to the way that we communicate things. These
    can be in the form of dialects, context, slang, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this problem, NLP applications are developed based on machine
    learning. These algorithms detect patterns in text data so that we can extract
    insights from them. Artificial intelligence companies make heavy use of NLP and
    text analysis in order to deliver relevant results. Some of the most common applications
    of NLP include search engines, sentiment analysis, topic modeling, part-of-speech
    tagging, and entity recognition. The goal of NLP is to develop a set of algorithms
    so that we can interact with computers in plain English. If we can achieve this,
    then we won't need programming languages to instruct computers on what they should
    do. In this chapter, we will look at a few recipes that focus on text analysis
    and how we can extract meaningful information from text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a Python package called **Natural Language Toolkit** (**NLTK**)
    heavily in this chapter. Make sure that you install this before you proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the installation steps at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also need to install `NLTK Data`, which contains many corpora and trained
    models. This is an integral part of text analysis! You can find the installation
    steps at [http://www.nltk.org/data.html](http://www.nltk.org/data.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing data using tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tokenization** is the process of dividing text into a set of meaningful pieces.
    These pieces are called **tokens**. For example, we can divide a chunk of text
    into words, or we can divide it into sentences. Depending on the task at hand,
    we can define our own conditions to divide the input text into meaningful tokens.
    Let''s take a look at how to do this.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is the first step in the computational analysis of the text and
    involves dividing the sequences of characters into minimal units of analysis called
    **tokens**. Tokens include various categories of text parts (words, punctuation,
    numbers, and so on), and can also be complex units (such as dates). In this recipe,
    we will illustrate how to divide a complex sentence into many tokens.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to preprocess data using tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and add the following lines (the full code is in the
    `tokenizer.py` file that''s already been provided to you). Let''s `import` the
    package and corpora:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define some sample `text` for analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with sentence tokenization. NLTK provides a sentence tokenizer,
    so let''s `import` that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the sentence tokenizer on the input `text` and extract the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the list of sentences to see whether it works correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Word tokenization is very commonly used in NLP. NLTK comes with a couple of
    different word tokenizers. Let''s start with the basic word tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to split this punctuation into separate tokens, then you will need
    to use the `WordPunct` tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we illustrated how to divide a complex sentence into many tokens. To
    do this, three methods of the `nltk.tokenize` package were used—`sent_tokenize`,
    `word_tokenize`, and `WordPunctTokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sent_tokenize` returns a sentence-tokenized copy of text, using NLTK''s recommended
    sentence tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_tokenize` tokenizes a string to split off punctuation other than periods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WordPunctTokenizer` tokenizes a text into a sequence of alphabetic and non-alphabetic
    characters, using the regexp `\w+|[^\w\s]+`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is a procedure that, depending on the language that is analyzed,
    can be an extremely complex task. In English, for example, we could be content
    to consider taking sequences of characters that do not have spaces and the various
    punctuation marks. Languages such as Japanese or Chinese, in which words are not
    separated by spaces but the union of different symbols, can completely change
    the meaning, and the task is much more complex. But in general, even in languages
    with words separated by spaces, precise criteria must be defined, as punctuation
    is often ambiguous.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.tokenize` package: [https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tokenization* (from the Natural Language Processing Group at Stanford University):
    [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we deal with a text document, we encounter different forms of words. Consider
    the word **play.** This word can appear in various forms, such as play, plays,
    player, playing, and so on. These are basically families of words with similar
    meanings. During text analysis, it's useful to extract the base forms of these
    words. This will help us to extract some statistics to analyze the overall text.
    The goal of **stemming** is to reduce these different forms into a common base
    form. This uses a heuristic process to cut off the ends of words in order to extract
    the base form.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `nltk.stem` package that offers a processing
    interface for removing morphological affixes from words. Different stemmers are
    available for different languages. For the English language, we will use `PorterStemmer`,
    `LancasterStemmer`, and `SnowballStemmer`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to stem text data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `stemmer.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a few `words` to play with, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define a list of `stemmers` that we want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the required objects for all three stemmers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to print the output data in a neat tabular form, we need to format
    it in the correct way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s iterate through the list of `words` and stem them by using the three
    stemmers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output in your Terminal. Observe
    how the LANCASTER stemmer behaves differently for a couple of words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/02ff3262-46ca-4b94-9475-f565c52a98ad.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All three stemming algorithms basically aim at achieving the same thing. The
    difference between the three stemming algorithms is basically the level of strictness
    with which they operate. If you observe the output, you will see that the LANCASTER
    stemmer is stricter than the other two stemmers. The PORTER stemmer is the least
    in terms of strictness, and the LANCASTER is the strictest. The stemmed words
    that we get from the LANCASTER stemmer tend to get confusing and obfuscated. The
    algorithm is really fast, but it will reduce the words a lot. So, a good rule
    of thumb is to use the SNOWBALL stemmer.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stemming is the process of reducing the inflected form of a word to its root
    form, called the **stem**. The stem doesn''t necessarily correspond to the morphological
    root (lemma) of the word: it''s normally sufficient that the related words are
    mapped to the same stem, even if the latter isn''t a valid root for the word.
    The creation of a stemming algorithm has been a prevalent issue in computer science.
    The stemming process is applied in search engines for query expansion, and in
    other natural language processing problems.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official home page for distribution of the Porter Stemming Algorithm, written
    and maintained by its author, Martin Porter: [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.stem` package: [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stemming* (from Wikipedia): [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting text to its base form using lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of lemmatization is also to reduce words to their base forms, but this
    is a more structured approach. In the previous recipe, you saw that the base words
    that we obtained using stemmers don't really make sense. For example, the word
    wolves was reduced to wolv, which is not a real word. Lemmatization solves this
    problem by doing things with a vocabulary and morphological analysis of words.
    It removes inflectional word endings, such as -ing or -ed, and returns the base
    form of a word. This base form is known as the lemma. If you lemmatize the word
    `wolves`, you will get `wolf` as the output. The output depends on whether the
    token is a verb or a noun.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `nltk.stem` package to reducing a word's inflected
    form to its canonical form, called a **lemma**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to convert text to its base form using lemmatization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following package (the full code is
    in the `lemmatizer.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the same set of words that we used during stemming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will compare two lemmatizers: the `NOUN` and `VERB` lemmatizers. Let''s
    list them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the object based on the `WordNet` lemmatizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to print the output in a tabular form, we need to format it in the
    right way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through the words and lemmatize them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8f65ff6c-7ca0-40a1-91fc-126bc6e796ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe how the `NOUN` and `VERB` lemmatizers differ when they lemmatize the
    word, as shown in the preceding screenshot
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lemmatization is the process of reducing a word's inflected form to its canonical
    form, called a lemma. In the processing of natural language, lemmatization is
    the algorithmic process that automatically determines the word of a given word.
    The process may involve other language processing activities, such as morphological
    and grammatical analysis. In many languages, words appear in different inflected
    forms. The combination of the canonical form with its part of speech is called
    the **lexeme** of the word. A lexeme is, in structural lexicology, the minimum
    unit that constitutes the lexicon of a language. Hence, every lexicon of a language
    may correspond to its registration in a dictionary in the form of a lemma.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In NLTK, for lemmatization, `WordNet` is available, but this resource is limited
    to the English language. It's a large lexical database of the English language.
    In this package, names, verbs, adjectives, and adverbs are grouped into sets of
    cognitive synonyms (**synsets**), each of which expresses a distinct concept.
    The synsets are interconnected by means of semantic and lexical conceptual relationships.
    The resulting network of significantly related words and concepts can be navigated
    with the browser. `WordNet` groups words according to their meanings, connecting
    not only word forms (strings of letters) but specific words. Hence, the words
    that are in close proximity to each other in the network are semantically disambiguated.
    In addition, `WordNet` labels the semantic relationships between words.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Keras 2.x Projects* by Giuseppe Ciaburro, from Packt Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.stem` package: [https://www.nltk.org/api/nltk.stem.html](https://www.nltk.org/api/nltk.stem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lemmatisation* (from Wikipedia): [https://en.wikipedia.org/wiki/Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing text using chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Chunking** refers to dividing the input text into pieces, which are based
    on any random condition. This is different from tokenization in the sense that
    there are no constraints, and the chunks do not need to be meaningful at all.
    This is used very frequently during text analysis. While dealing with large text
    documents, it''s better to do it in chunks.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to divide text by using chunking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `chunking.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function to `split` the text into chunks. The first step is
    to divide the text based on spaces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a couple of required variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s iterate through the `words`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have hit the required number of words, reset the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Append the chunks to the `output` variable, and return it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define the main function. Load the data from the `brown` corpus.
    We will use the first 10,000 words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of words in each chunk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a couple of relevant variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `splitter` function on this text `data` and `print` the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the number of chunks that were generated
    printed in the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chunking (also called **shallow parsing**) is the analysis of a proposition,
    which is formed in a simple form by a subject and a predicate. The subject is
    typically a noun phrase, while the predicate is a verbal phrase formed by a verb
    with zero or more complements and adverbs. A chunk is made up of one or more adjacent
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous approaches to the problem of chunking. For example, in the
    assigned task, a chunk is represented as a group of words delimited by square
    brackets, for which a tag representing the type of chunk is indicated. The dataset
    that was used was derived from a given corpora by taking the part related to journal
    articles and extracting chunks of information from the syntactic trees of the
    corpora.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Brown University Standard Corpus of Present-Day American English (or simply,
    the `brown` corpus) is a corpus that was compiled in the 1960s by Henry Kucera
    and W. Nelson Francis at Brown University, Providence, Rhode Island. It contains
    500 text extracts in English, obtained from works published in the United States
    of America in 1961, for a total of about one million words.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.corpus` package: [https://www.nltk.org/api/nltk.corpus.html](https://www.nltk.org/api/nltk.corpus.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basics of Natural Language Processing* (from the University of Zagreb): [https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf](https://www.fer.unizg.hr/_download/repository/TAR-02-NLP.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to dealing with text documents that consist of millions of words,
    converting them into numerical representations is necessary. The reason for this
    is to make them usable for machine learning algorithms. These algorithms need
    numerical data so that they can analyze them and output meaningful information.
    This is where the **bag-of-words** approach comes into the picture. This is basically
    a model that learns a vocabulary from all of the words in all the documents. It
    models each document by building a histogram of all of the words in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will build a bag-of-words model to extract a document term
    matrix, using the `sklearn.feature_extraction.text` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to build a bag-of-words model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `bag_of_words.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the `main` function. Load the input `data` from the `brown` corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the text data into five chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dictionary that is based on these text chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to extract a document term matrix. This is basically a matrix
    that counts the number of occurrences of each word in the document. We will use
    `scikit-learn` to do this because it has better provisions, compared to NLTK,
    for this particular task. Import the following package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the object and extract the document term matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the vocabulary from the `vectorizer` object and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the `Document term matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To print it in a tabular form, you will need to format this, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through the words and print the number of times each word has occurred
    in different chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see two main things printed in the Terminal.
    The first output is the vocabulary, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The second thing is the Document term matrix, which is pretty long. The first
    few lines will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e469a5e4-b7b8-4b75-bb9e-9f70e7f68682.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: The brown dog is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: The black dog is in the black room.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: Running in the room is forbidden.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you consider all three of these sentences, you will have the following nine
    unique words:'
  prefs: []
  type: TYPE_NORMAL
- en: the
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: black
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: room
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forbidden
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s convert each sentence into a histogram, using the count of words
    in each sentence. Each feature vector will be nine-dimensional, because we have
    nine unique words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence 1**: [1, 1, 1, 1, 1, 0, 0, 0, 0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 2**: [2, 0, 1, 1, 0, 2, 1, 1, 0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence 3**: [0, 0, 0, 1, 1, 0, 1, 1, 1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have extracted these feature vectors, we can use machine learning algorithms
    to analyze them.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model is a method that's used in information retrieval and
    in the processing of the natural language in order to represent documents by ignoring
    the word order. In this model, each document is considered to contain words, similar
    to a stock exchange; this allows for the management of these words based on lists,
    where each stock contains certain words from a list.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.CountVectorizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Bag-of-Words Model* (from Wikipedia): [https://en.wikipedia.org/wiki/Bag-of-words_model](https://en.wikipedia.org/wiki/Bag-of-words_model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a text classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main aim of text classification is to sort text documents into different
    classes. This is a vital analysis technique in NLP. We will use a technique that
    is based on a statistic called **tf-idf**, which stands for **term frequency**-**inverse
    document frequency**. This is an analysis tool that helps us to understand how
    important a word is to a document in a set of documents. This serves as a feature
    vector that's used to categorize documents.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the term frequency-inverse document frequency method
    to evaluate the importance of a word for a document in a collection or a corpus,
    and to build a text classifier.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to build a text classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following package (the full code is
    in the `tfidf.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s select a list of categories and name them using a dictionary mapping.
    These categories are available as a part of the news groups dataset that we just
    imported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the training data based on the categories that we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the feature extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the features by using the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train the classifier. We will use the multinomial Naive
    Bayes classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a couple of random input sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `tfidf_transformer` object and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the feature vectors, train the multinomial Naive Bayes classifier
    using this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the input data using the word counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the input data using the `tfidf_transformer` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the output categories of these input sentences by using the trained
    classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the output, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output printed in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tf-idf technique is used frequently in information retrieval. The goal is
    to understand the importance of each word within a document. We want to identify
    words that occur many times in a document. At the same time, common words such
    as **is** and **be** don't really reflect the nature of the content. So, we need
    to extract the words that are true indicators. The importance of each word increases
    as the count increases. At the same time, as it appears a lot, the frequency of
    this word increases, too. These two things tend to balance each other out. We
    extract the term counts from each sentence. Once we have converted this to a feature
    vector, we can train the classifier to categorize these sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The **term frequency** (**TF**) measures how frequently a word occurs in a given
    document. As multiple documents differ in length, the numbers in the histogram
    tend to vary a lot. So, we need to normalize this so that it becomes a level playing
    field. To achieve normalization, we can divide the term-frequency by the total
    number of words in a given document. The **inverse document frequency** (**IDF**)
    measures the importance of a given word. When we compute the TF, all words are
    considered to be equally important. To counterbalance the frequencies of commonly
    occurring words, we need to weigh them down and scale up the rare ones. We need
    to calculate the ratio of the number of documents with the given word and divide
    it by the total number of documents. The IDF is calculated by taking the negative
    algorithm of this ratio.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple words, such as **is** or **the**, tend to appear a lot in various documents.
    However, this doesn't mean that we can characterize the document based on these
    words. At the same time, if a word appears a single time, that is not useful,
    either. So, we look for words that appear a number of times, but not so much that
    they become noisy. This is formulated in the tf-idf technique and is used to classify
    documents. Search engines frequently use this tool to order search results by
    relevance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.feature_extraction.text.TfidfTransformer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the following page: *What does tf-idf mean?*: [http://www.tfidf.com/](http://www.tfidf.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the gender of a name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying the gender of a name is an interesting task in NLP. We will use
    the heuristic that the last few characters in a name is its defining characteristic.
    For example, if the name ends with **la**, it's most likely a female name, such
    as Angela or Layla. On the other hand, if the name ends with **im**, it's most
    likely a male name, such as Tim or Jim. As we aren't sure of the exact number
    of characters to use, we will experiment with this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the names corpora to extract labeled names, and
    then we will classify the gender based on the final part of the name.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to identify the gender:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `gender_identification.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to define a function to extract features from input words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the main function. We need some labeled training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Seed the random number generator and shuffle the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Define some input names to play with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'As we don''t know how many ending characters we need to consider, we will sweep
    the parameter space from `1` to `5`. Each time, we will extract the features,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide this into train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the Naive Bayes classifier to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the `classifier` model for each value in the parameter space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output printed in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the names corpus to extract labeled names, and then
    we classified the gender based on the final part of the name. A Naive Bayes classifier
    is a supervised learning classifier that uses Bayes' theorem to build the model.
    This topic was addressed in the *Building a Naive Bayes classifier* recipe in [Chapter
    2](102c5690-d978-4a56-a586-1f741cde6b3d.xhtml)*, Constructing a Classifier*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bayesian classifier is called naive because it ingenuously assumes that
    the presence or absence of a particular characteristic in a given class of interest
    is not related to the presence or absence of other characteristics, greatly simplifying
    the calculation. Let's go ahead and build a Naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 2](102c5690-d978-4a56-a586-1f741cde6b3d.xhtml)*, Constructing a Classifier*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Keras 2.x Projects* by Giuseppe Ciaburro, from Packt Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The official documentation of the `nltk.classify`package: [http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier](http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayes'' Theorem* (from Stanford Encyclopedia of Philosophy): [https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the sentiment of a sentence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sentiment analysis** is one of the most popular applications of NLP. Sentiment
    analysis refers to the process of determining whether a given piece of text is
    positive or negative. In some variations, we consider neutral as a third option.
    This technique is commonly used to discover how people feel about a particular
    topic. This is used to analyze the sentiments of users in various forms, such
    as marketing campaigns, social media, e-commerce, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will analyze the sentiment of a sentence by using a Naive
    Bayes classifier, starting with the data contained in the `movie_reviews` corpus.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to analyze the sentiment of a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `sentiment_analysis.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to extract the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We need training data for this, so we will use the movie reviews in NLTK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s separate them into positive and negative reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the data into train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use a `NaiveBayesClassifier`. Define the object and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The `classifier` object contains the most informative words that it obtained
    during analysis. These words basically have a strong say in what''s classified
    as a positive or a negative review. Let''s print them out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a couple of random input sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the classifier on those input sentences and obtain the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see three main things printed in the Terminal.
    The first is the accuracy, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The next item is a list of the most informative words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The last item is the list of predictions, which are based on the input sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used NLTK's Naive Bayes classifier for our task here. In the feature extractor
    function, we basically extracted all the unique words. However, the NLTK classifier
    needs the data to be arranged in the form of a dictionary. Hence, we arranged
    it in such a way that the NLTK `classifier` object can ingest it. Once we divided
    the data into training and testing datasets, we trained the classifier to categorize
    the sentences into positive and negative ones.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the top informative words, you can see that we have words such
    as outstanding to indicate positive reviews and words such as insulting to indicate
    negative reviews. This is interesting information, because it tells us what words
    are being used to indicate strong reactions.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term sentiment analysis refers to the use of NLP techniques, text analysis,
    and computational linguistics to find information in written or spoken text sources.
    If this subjective information is taken from large amounts of data, and therefore
    from the opinions of large groups of people, sentiment analysis can also be called
    **opinion mining**.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.corpus` package: [https://www.nltk.org/api/nltk.corpus.html](https://www.nltk.org/api/nltk.corpus.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `nltk.classify` package: [http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier](http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sentiment Analysis* (from Stanford University): [https://web.stanford.edu/class/cs124/lec/sentiment.pdf](https://web.stanford.edu/class/cs124/lec/sentiment.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns in text using topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** refers to the process of identifying hidden patterns in
    text data. The goal is to uncover a hidden thematic structure in a collection
    of documents. This will help us to organize our documents in a better way, so
    that we can use them for analysis. This is an active area of research in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a library called `gensim` to identify patterns in
    text, using topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to identify patterns in text by using topic modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `topic_modeling.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to load the input data. We will use the `data_topic_modeling.txt`
    text file that has already been provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a `class` to preprocess the text. This preprocessor will take
    care of creating the required objects and extracting the relevant features from
    the input text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a list of stop words so that we can exclude them from analysis. These
    are common words, such as **in**, **the**, **is**, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `SnowballStemmer` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a processor function that takes care of tokenization, stop word removal,
    and stemming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the stop words from the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform stemming on the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the processed tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to define the main function. Load the input data from the
    text file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an object that is based on the class that we defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to process the text in the file and extract the processed tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dictionary that is based on tokenized documents so that it can be
    used for topic modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a document term matrix using the processed tokens, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s suppose that we know that the text can be divided into two topics. We
    will use a technique called **latent Dirichlet allocation** (**LDA**) for topic
    modeling. Define the required parameters and initialize the `LdaModel` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this has identified the two topics, we can see how it''s separating these
    two topics by looking at the most contributed words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is in the `topic_modeling.py` file. If you run this code, you
    will see the following printed in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** works by identifying the important words or themes in a
    document. These words tend to determine what the topic is about. We use a regular
    expression tokenizer, because we just want the words, without any punctuation
    or other kinds of tokens. Hence, we use this to extract the tokens. The stop word
    removal is another important step, because this helps us to eliminate the noise
    caused by words such as **is** or **the**. After that, we need to stem the words
    to get to their base forms. This entire thing is packaged as a preprocessing block
    in text analysis tools. That is what we are doing here, as well!'
  prefs: []
  type: TYPE_NORMAL
- en: We use a technique called LDA to model the topics. LDA basically represents
    the documents as a mixture of different topics that tend to spit out words. These
    words are spat out with certain probabilities. The goal is to find these topics!
    This is a generative model that tries to find the set of topics that are responsible
    for the generation of the given set of documents.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see from the output, we have words such as talent and train to characterize
    the sports topic, whereas we have encrypt to characterize the cryptography topic.
    We are working with a really small text file, which is why some words might seem
    less relevant. Obviously, the accuracy will improve if you work with a larger
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `gensim` library: [https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Latent Dirichlet Allocation* (from MIT): [http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Topic Modeling* (from Columbia University): [http://www.cs.columbia.edu/~blei/topicmodeling.html](http://www.cs.columbia.edu/~blei/topicmodeling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of speech tagging with spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Parts**-**of**-**speech tagging** (**PoS tagging**) is the process of labeling
    the words that correspond to particular lexical categories. The common linguistic
    categories include nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a library called `spacy` to perform PoS tagging.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform PoS tagging using `spacy`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `PosTagging.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `en_core_web_sm` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define an input text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: As a source, I used a passage based on the novel *The Adventures of Huckleberry
    Finn* by Mark Twain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will perform a PoS tagging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PoS tagging involves assigning a tag to each word of a document/corpus. The
    choice of the tagset to use depends on the language. The input is a string of
    words and the tagset to be used, and the output is the association of the best
    tag with each word. There may be multiple tags compatible with a word (**ambiguity**).
    The task of the PoS tagger is to solve these ambiguities by choosing the most
    appropriate tags, based on the context in which the word is located.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform a PoS tagging, we used the `spacy` library. This library extracts
    linguistic features, such as PoS tags, dependency labels, and named entities,
    customizing the tokenizer and working with the rule-based matcher.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `en_core_web_sm` model, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `spacy` library: [https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Parts-of-Speech Tagging* (from New York University): [https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf](https://cs.nyu.edu/courses/fall16/CSCI-UA.0480-006/lecture4-hmm.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec using gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Word embedding** allows us to memorize both the semantic and syntactic information
    of words, starting with an unknown corpus and constructing a vector space in which
    the vectors of words are closer if the words occur in the same linguistic contexts,
    that is, if they are recognized as semantically similar. Word2Vec is a set of
    templates that are used to produce word embedding; the package was originally
    created in C by Tomas Mikolov, and was then implemented in Python and Java.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `gensim` library to build a Word2Vec model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform word embedding by using `gensim`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `GensimWord2Vec.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a model based on the Word2Vec methodology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the vocabulary from the data and put it into a `list`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will find similarities with the word `''science''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will `print` the `data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results will be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec is a simple two-layer artificial neural network that was designed to
    process natural language; the algorithm requires a corpus in the input and returns
    a set of vectors that represent the semantic distributions of words in the text.
    For each word contained in the corpus, in a univocal way, a vector is constructed
    in order to represent it as a point in the created multidimensional space. In
    this space, the words will be closer if they are recognized as semantically more
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we used the Australian National Corpus (`abc`), a great collection
    of language data, both text-based and digital. To use this corpus, you must download
    it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `gensim` library: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Word2vec* (from Wikipedia): [https://en.wikipedia.org/wiki/Word2vec](https://en.wikipedia.org/wiki/Word2vec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow learning for spam detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spamming** means sending large amounts of unwanted messages (usually commercial).
    It can be implemented through any medium, but the most commonly used are email
    and SMS. The main purpose of spamming is advertising, from the most common commercial
    offers to proposals for the sale of illegal material, such as pirated software
    and drugs without a prescription, and from questionable financial projects to
    genuine attempts at fraud.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a logistic regression model for spam detection.
    To do this, a collection of labeled SMS messages collected for mobile phone spam
    research will be used. This dataset comprises of 5,574 real English non-encoded
    messages, tagged according to whether they are legitimate (`ham`) or spamming
    (`spam`).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to perform shallow learning for spam detection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `LogiTextClassifier.py` file that''s already been provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `spam.csv` file that was provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the data for training and testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to `vectorize` the text data contained in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now build the logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Define two SMS messages as test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will perform a prediction by using the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results will be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: These indicate that the first SMS was identified as `spam`, while the second
    SMS was identified as `ham`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression analysis is a method for estimating the regression function
    that best links the probability of a dichotomous attribute with a set of explanatory
    variables. **Logistic assault** is a nonlinear regression model that's used when
    the dependent variable is dichotomous. The objective of the model is to establish
    the probability with which an observation can generate one or the other values
    of the dependent variable; it can also be used to classify the observations into
    two categories, according to their characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the measurement scale of the dependent variable, logistic regression
    analysis is distinguished from linear regression because a normal distribution
    of *y* is assumed for this, whereas if *y* is dichotomous, its distribution is
    obviously binomial. Similarly, in linear regression analysis, the *y* estimate
    obtained from the regression varies from -∞ to + ∞, while in logistic regression
    analysis, the *y* estimate varies between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.linear_model.LogisticRegression()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.feature_extraction.text.TfidfVectorizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression Analysis with R* by Giuseppe Ciaburro, from Packt Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logistic Regression* (from the University of Sheffield): [https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf](https://www.sheffield.ac.uk/polopoly_fs/1.233565!/file/logistic_regression_using_SPSS_level1_MASH.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
