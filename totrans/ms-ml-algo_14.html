<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to introduce the fundamental concepts of <strong>Reinforcement Learning</strong> (<strong>RL</strong>), which is a set of approaches that allows an agent to learn how to behave in an unknown environment, thanks to the rewards that are provided after each possible action. RL has been studied for decades, but it has reached a very high maturity level in the last few years when it became possible to employ deep learning models together with standard (and often simple) algorithms in order to solve extremely complex problems (such as learning how to play an Atari game perfectly).</p>
<p>In particular, we will discuss:</p>
<ul>
<li>The concepts of environment, agent, policy, and reward</li>
<li>The concept of the <strong>Markov Decision Process</strong> <span>(</span><strong>MDP</strong><span>)</span></li>
<li>The policy iteration algorithm</li>
<li>The value iteration algorithm</li>
<li>The TD(0) algorithm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning fundamentals</h1>
                </header>
            
            <article>
                
<p>Imagine that you want to learn to ride a bike and ask a friend for advice. They explain how the gears work, how to release the brake and a few other technical details. In the end, you ask the secret to keeping balanced. What kind of answer do you expect? In an imaginary supervised world, you should be able to perfectly quantify your actions and correct the errors by comparing the outcomes with precise reference values. In the real world, you have no idea about the quantities underlying your actions and, above all, you will never know what the right value is. Increasing the level of abstraction, the scenario we're considering can be described as: a generic <strong>agent</strong> performs actions inside an <strong>environment</strong> and receives <strong>feedback</strong> that is somehow proportional to the <span>competence</span> of its actions.</p>
<p>According to this <strong>feedback</strong>, the <strong>agent</strong> can correct its actions in order to reach a specific goal. This basic schema is represented in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1374 image-border" src="assets/d5f766b1-6137-4232-830c-af782674037b.png" style="width:23.58em;height:14.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Basic RL schema</div>
<p class="mce-root">Returning to our initial example, when you ride a bike for the first time and try to keep your balance, you will notice that the wrong movement causes an increase in the slope, which in turn increases the horizontal component of the gravity force, pushing the bike laterally. As the vertical component is compensated, the result is a rotation that ends when the bike falls down completely. However, as you can use your legs to control the balance, when the bike starts falling, thanks to Newton's third law, the force on the leg increases and your brain understands that it's necessary to make a movement in the opposite direction. Even if this problem can be easily expressed in terms of physical laws, nobody learns to ride a bike by computing forces and momentums. This is one of the main concepts of RL: an agent must always make its choices considering a piece of information, usually defined as a <em>reward</em>, that represents the response, provided by the environment. If the action is correct, the reward will be positive, otherwise, it will be negative. After receiving a reward, an agent can fine-tune the strategy, called <em>policy</em>, in order to maximize the expected future reward. For example, after a few rides, you will be able to slightly move your body so as to keep the balance while turning, but probably, in the beginning, you needed to extend your leg to avoid falling down. Hence, your initial policy suggested a wrong action, which received repeated negative rewards and so your brain corrected it by increasing the probability of choosing another action. The implicit hypothesis that underlies this approach is that an agent is always <em>rational</em>, meaning that its goal is to maximize the expected return of its actions (nobody would like to fall down just to feel a different emotion).</p>
<p class="mce-root">Before discussing the single components of an RL system, it's necessary to add a couple of fundamental assumptions. The first one is that an agent can repeat the experiences an infinite number of times. In other words, we assume that it's possible to learn a valid policy (possibly the optimal one) only if we have enough time. Clearly, this is unacceptable in the animal world and we all know that many experiences are extremely dangerous; however, this assumption is necessary to prove the convergence of some algorithms. Indeed, sub-optimal policies sometimes can be learned very quickly, but it's necessary to iterate many times to reach the optimal one. In real artificial systems, we always stop the learning process after a finite number of iterations, but it's almost impossible to find valid solutions if some experiences prevent the agent from continuing to interact with the environment. As many tasks have final states (either positive or negative), we assume that the agent can play any number of <em>episodes</em> (somewhat analogous to the epochs of supervised learning), exploiting the experience previously learned.</p>
<p>The second assumption is a little bit more technical and it's usually known as the <em>Markov property</em>. When the agent interacts with the environment, it observes a sequence of states. Even if it can seem like an oxymoron, we assume that each state is stateful. We can explain this concept with a simple example; suppose that you're filling a tank and every five seconds you measure the level. Imagine that at <em>t = 0</em>, the level <em>L = 10</em> and the water is flowing in. What do you expect at <em>t = 1</em>? Obviously, <em>L &gt; 10</em>. In other words, without external unknown causes, we assume that a state contains the previous history, so that the sequence, even if discretized, represents a continuous evolution where no jumps are allowed. When an RL task satisfies this property, it's called a Markov Decision Process and it's very easy to employ simple algorithms to evaluate the actions. Luckily, the majority of natural events can be modeled as MDPs (when you're walking toward a door, every step in the right direction must decrease the distance), but there are some games that are implicitly stateless. For example, if you want to employ an RL algorithm to learn how to guess the outcome of a probabilistic sequence of independent events (such as tossing a coin), the result could be dramatically wrong. The reason is clear: any state is independent of the previous ones and every attempt to build up a history is a failure. Therefore, if you observe a sequence of <em>0, 0, 0, 0, ...</em> you are not justified in increasing the value of betting on 0 unless, after considering the likelihood of the events, you suppose that the coin is loaded. However, if there's no reason to do so, the process isn't an MDP and every episode (event) is completely independent. All the assumptions that we, either implicitly or explicitly, make are based on this fundamental concept, so pay attention when evaluating new, unusual scenarios because you may discover that the employment of a specific algorithm isn't theoretically justified.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Environment</h1>
                </header>
            
            <article>
                
<p>The <strong>environment</strong> is the entity where the <strong>agent</strong> has to reach its goals. For our purposes, a generic environment is a system that receives an input action, <em>a<sub>t</sub></em> (we use the index <em>t</em> because this is a natural time process), and outputs a tuple composed by a state, <em>s<sub>t+1</sub></em>, and a reward, <em>r<sub>t+1</sub></em>. These two elements are the only pieces of information provided to the agent to make its next decision. If we are working with an MDP and the sets of possible actions, A, and states, S, are discrete and finite, the problem is a defined finite MDP (in many continuous cases, it's possible to treat the problem as a finite MDP by discretizing the spaces). If there are final states, the task is called <em>episodic</em> and, in general, the goal is to reach a positive final state in the shortest amount of time or maximize a score. The schema of the cyclic interaction between agent an environment is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1381 image-border" src="assets/e7f8fb93-5d5f-4d1f-9040-e19abef9fd6a.png" style="width:23.33em;height:14.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Agent-environment interaction schema</div>
<p class="mce-root">A very important feature of an environment is its internal nature. It can be either <em>deterministic</em> or <em>stochastic</em>. A deterministic environment is characterized by a function that associates each possible action, <em>a<sub>t</sub></em>, in a specific state, <em>s<sub>t</sub></em>, to a well-defined successor, <em>s</em><sub><em>t+1</em></sub>, with a precise reward, <em>r<sub>t+1</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a85f55e3-9b7d-4101-8345-5c39db4ad788.png" style="width:28.83em;height:1.50em;"/></div>
<p>Conversely, a stochastic environment is characterized by a transition probability between the current state, <em>s<sub>t</sub></em>, and a set of possible successors, <em>s<sup>i</sup><sub>t+1</sub></em>, given an action, <em>a<sub>t</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c5824d4e-6559-437e-a8d0-724d4a324541.png" style="width:32.50em;height:2.00em;"/></div>
<p>If a state, <em>s<sub>i</sub></em>, has a transitional probability, <em>T(s<sub>i</sub>, s<sub>i</sub>, a<sub>t</sub>) = 1 ∀ a<sub>t </sub>∈ A</em>, the state is defined as <em>absorbing</em>. In general, all ending states in episodic tasks are modeled as absorbing ones, to avoid any further transition. When an episode is not limited to a fixed number of steps, the only criterion to determine its end is to check whether the agent has reached an absorbing state.</p>
<p class="mce-root">As we don't know which state will be the successor, it's necessary to consider the expected value of all possible rewards considering the initial state, <em>s<sub>t</sub></em>, and the action, <em>a<sub>t</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5fbf6f10-762b-4510-a62f-9de83b98f0a2.png" style="width:8.33em;height:1.92em;"/></div>
<p class="mce-root">In general, it's easier to manage stochastic environments because they can be immediately converted into deterministic ones by setting all probabilities to zero except the one corresponding to the actual successor (for example, T(•) = (0, 0, ..., 1, ..., 0)). In the same way, the expected return can be set equal to <em>r<sub>t+1</sub></em>. The knowledge of T(•), as well as <em>E[r<sup>i</sup><sub>t+1</sub>]</em>, is necessary to employ some specific algorithms, but it can become problematic when finding a suitable model for the environment requires an extremely complex analysis. In all those cases, model-free methods can be employed and, therefore, the environment is considered as a black-box, whose output at time, <em>t</em> (subsequent to an action performed by the agent, <em>a<sub>t-1</sub></em>), is the only available piece of information for the evaluation of a policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rewards</h1>
                </header>
            
            <article>
                
<p>We have seen that rewards (sometimes negative rewards are called <em>penalties</em>, but it's preferable to use a standardized notation) are the only feedback provided by the environment after each action. However, there are two different approaches to the use of rewards. The first one is the strategy of a very short-sighted agent and consists in taking into account only the reward just received. The main problem with this approach is clearly the inability to consider longer sequences that can lead to a very high reward. For example, an agent has to traverse a few states with negative reward (for example, -0.1), but after them, they arrive at a state with a very positive reward (for example, +5.0). A short-sighted agent couldn't  find out the best policy because it will simply try to avoid the immediate negative rewards. On the other side, it's better to suppose that a single reward contains a part of the future rewards that will be obtained following the same policy. This concept can be expressed by introducing a <em>discounted reward</em>, which is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8473c1dd-1fbc-4804-8cd5-7620a2d93b15.png" style="width:34.83em;height:4.50em;"/></div>
<p class="mce-root">In the previous expression, we are assuming an infinite horizon with a discount factor, γ, which is a real number bounded between 0 and 1 (not included). When <span>γ = 0, the agent is extremely short-sighted, because of <em>R<sub>t</sub> = r<sub>t+1</sub></em>, but when <em>γ →</em> 1, the current reward takes into account the future contributions discounted in a way that is inversely proportional to the time-step. In this way, very close rewards will have a higher weight than very distant ones. If the absolute value of all rewards is limited by a maximum immediate absolute reward, <em>|r<sub>i</sub>| ≤ |r<sub>max</sub>|</em>, the previous expression will be always bounded. In fact, considering the properties of a geometric series, we get:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2819bbb5-a656-419b-a7a4-0167fe6c9af9.png" style="width:40.83em;height:4.75em;"/></div>
<p class="mce-root">Clearly, the right choice of γ is a crucial factor in many problems and cannot be easily generalized. As in many other similar cases, I suggest testing different values, picking the one that minimizes the convergence speed while yielding a quasi-optimal policy. Of course, if the tasks are episodic with length, <em>T(e<sub>i</sub>)</em>, the discounted reward becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4d0ea980-a9a0-4c24-be1c-93ceccf4076a.png" style="width:48.75em;height:5.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checkerboard environment in Python</h1>
                </header>
            
            <article>
                
<p>We are going to consider an example based on a checkerboard environment representing a tunnel. The goal of the agent is to reach the ending state (lower-right corner), avoiding 10 wells that are negative absorbing states. The rewards are:</p>
<ul>
<li><strong>Ending state</strong>: +5.0</li>
<li><strong>Wells</strong>: -5.0</li>
<li><strong>All other states</strong>: -0.1</li>
</ul>
<p>Selecting a small negative reward for all non-terminal states is helpful to force the agent to move forward until the maximum (final) reward has been achieved. Let's start modeling an environment that has a 5 × 15 matrix:</p>
<pre>import numpy as np<br/><br/>width = 15<br/>height = 5<br/><br/>y_final = width - 1<br/>x_final = height - 1<br/><br/>y_wells = [0, 1, 3, 5, 5, 7, 9, 11, 12, 14]<br/>x_wells = [3, 1, 2, 0, 4, 1, 3, 2, 4, 1]<br/><br/>standard_reward = -0.1<br/>tunnel_rewards = np.ones(shape=(height, width)) * standard_reward<br/><br/>for x_well, y_well in zip(x_wells, y_wells):<br/>    tunnel_rewards[x_well, y_well] = -5.0<br/><br/>tunnel_rewards[x_final, y_final] = 5.0</pre>
<p>The graphical representation of the environment (in terms of rewards) is shown in the following chart:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1386 image-border" src="assets/3d68b884-208b-479e-8aaf-cbcfd3cf630a.png" style="width:70.75em;height:26.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Rewards in the tunnel environment</div>
<p>The agent is allowed to move in four directions: up, down, left, and right. Clearly, in this case, the environment is deterministic because every action moves the agent to a predefined cell. We assume that whenever an action is forbidden (such as trying to move on the left when the agent is in the first column), the successor state is the same one (with the corresponding reward).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy</h1>
                </header>
            
            <article>
                
<p>A <em>policy</em> is formally a deterministic or stochastic law that the agent follows in order to maximize its return. Conventionally, all policies are denoted with the letter <em>π</em>. A <em>deterministic policy</em> is usually a function of the current state that outputs a precise action:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/162f2b33-d17f-40f1-bbf9-508873a21c95.png" style="width:6.33em;height:1.42em;"/></div>
<p>A <em>stochastic policy</em>, analogously to environments, outputs the probability of each action (in this case, we are assuming we work with a finite MPD):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f34fa72a-a348-41f8-ab78-623ed830cc93.png" style="width:27.25em;height:1.92em;"/></div>
<p class="mce-root">However, contrary to the environment, an agent must always pick a specific action, transforming any stochastic policy into a deterministic sequence of choices. In general, a policy where <em>π(s, a) &gt; 0 ∀ a ∈ A</em>, is called <em>soft</em> and it's often very useful during the training process because it allows a more flexible modeling without the premature selection of a suboptimal action. Instead, when <span><em>π(s, a<sub>i</sub>) = 0 ∀ i ≠ j</em> and <em>π(s, a<sub>j</sub>) = 1</em>, the policy is also defined as <em>hard</em>.</span> This transformation can be performed in many ways, but the most common one is to define a policy that is greedy with respect to a value (we're going to discuss this concept in the next section). This means that, at every step, the policy will select the action that maximizes the value of the successor state. Obviously, this is a very rational approach, which could be too pragmatic. In fact, when the values of some states don't change, a greedy policy will always force the agent to perform the same actions.</p>
<p class="mce-root">Such a problem is known as the <em>exploration-exploitation dilemma</em> and arises when it would be better to allow the agent to evaluate alternative strategies that could appear initially to be suboptimal. In other words, we want the agent to explore the environment before starting to exploit the policy, to know whether the policy is really the best one or if there are hidden alternatives. To solve this problem, it's possible to employ an <em>ε-greedy policy</em>, where the value, <span><em>ε</em>, is called the <em>exploration factor</em> and represents a probability. In this case, the policy will pick a random action with probability <em>ε</em> and a greedy one with probability <em>1 - ε</em>. In general, at the beginning of the training process, <em>ε</em> is kept very close to 1.0 to incentivize the exploration and it's progressively </span>decreased when the policy becomes more stable. In many Deep RL applications, this approach is fundamental, in particular, when there are no models of the environment. The reason is that greedy policies can be initially wrong and it's necessary to allow the agent to explore many possible state and action sequences before forcing a deterministic decision.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to analyze a strategy to find an optimal policy based on a complete knowledge of the environment (in terms of transition probability and expected returns). The first step is to define a method that can be employed to build a greedy policy. Let's suppose we're working with a finite MDP and a generic policy, π; we can define the intrinsic value of a state, <em>s<sub>t</sub></em>, as the expected discounted return obtained by the agent starting from <em>s<sub>t</sub></em> and following the stochastic policy, <span><em>π</em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/38a56381-2dab-457c-a2ca-9e2241245ce8.png" style="width:26.83em;height:4.42em;"/></div>
<p>In this case, we are assuming that, as the agent will follow <em>π</em>, state <em>s<sub>a</sub></em> is more useful than <em>s<sub>b</sub></em> if the expected return starting from <em>s<sub>a</sub></em> is greater than the one obtained starting from s<sub>b</sub>. Unfortunately, trying to directly find the value of each state using the previous definition is almost impossible when <em>γ &gt; 0</em>. However, this a problem that can be solved using Dynamic Programming (for further information, please refer to <em>Dynamic Programming and Markov Process</em>, <em>Ronald A. Howard</em>, The MIT Press), which allows us to solve the problem iteratively.</p>
<p><span>In particular, we need to turn the previous formula into a <em>Bellman equation</em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2551adf2-6d67-424c-8d66-f83ed1e7ce83.png" style="width:58.17em;height:4.17em;"/></div>
<p class="mce-root">The first term on the right-hand side can be expressed as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2c30e3ba-5d6a-43ac-815b-786086908258.png" style="width:30.25em;height:3.08em;"/></div>
<p>In other words, it is the weighted average of all expected returns considering that the agent is state, <em>s<sub>t</sub></em>, and evaluates all possible actions and the consequent state transitions. For the second term, we need a small trick. Let's suppose we start from <em>s<sub>t+1</sub></em>, so that the expected value corresponds to <em>V(s<sub>t+1</sub>;π)</em>; however, as the sum starts from <em>s<sub>t</sub></em>, we need to consider all possible transitions starting from <em>s<sub>t</sub></em>. In this case, we can rewrite the term as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/41fed7cd-c7aa-4832-bf13-e2a3deccaca3.png" style="width:33.00em;height:4.25em;"/></div>
<p class="mce-root">Again, the first terms take into account all possible transitions starting from <em>s<sub>t</sub></em> (and ending in <em>s<sub>t+1</sub></em>), while the second one is the value of each ending state. Therefore the complete expression becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e9ccdae6-a592-489f-9c0c-01a5be43ee7b.png" style="width:38.67em;height:3.33em;"/></div>
<p>For a deterministic policy, instead, the formula is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3aa382e2-5c0e-4915-99d2-261a65c5a260.png" style="width:38.17em;height:3.50em;"/></div>
<p>The previous equations are particular cases of a generic discrete <em>Bellman equation</em> for a finite MDP that can be expressed as a vectorial operator, <em>L<sub>π</sub></em>, applied to the value vector:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a8acd02-9687-4551-8cfa-1288d48e0adc.png" style="width:9.58em;height:1.33em;"/></div>
<p>It's easy to prove that there exists a unique fixed point that corresponds to <em>V(s; π)</em>, so <em><span>L</span>π <span>V(s; π) = V(s; π)</span></em>. However, in order to solve the system, we need to consider all equations at the same time because, both on the left-hand and on the right-hand side of the <em>Bellman equation</em>, there is the <em>V(•; π)</em> term. Is it possible to transform the problem into an iterative procedure, so that a previous computation can be exploited for the following one? The answer is yes and it's the consequence of an important property of <em><span>L</span><sub>π</sub></em>. Let's consider the infinity norm of the difference between two value vectors computed at time <em>t</em> and <em>t+1</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b0670783-6c0d-4f2a-8553-48da529179b2.png" style="width:57.67em;height:2.00em;"/></div>
<p class="mce-root">As the discount factor γ ∈ [0, 1[, the Bellman operator, <em><span>L</span><sub>π</sub></em>, is a <span>γ-contraction that reduces the distance between the arguments by a factor of γ (they get more and more similar). The <em>Banach Fixed-Point Theorem</em> states that a contraction, <em>L: D → D</em>, on a metric space, <em>D</em>, admits a unique fixed point, <em>d<sup>*</sup> ∈ D</em>, that can be found by repeatedly applying the contraction to any <em>d<sup>(0)</sup> ∈ D</em>.</span></p>
<p class="mce-root"><span>Hence, we know about the existence of a unique fixed point, <em>V(s; π)</em>, that is the goal of our research. If we now consider a generic starting point, <em>V<sup>(t)</sup></em>, and we compute the norm of the difference with <em>V(s; π)</em>, we obtain:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bf2fbc9e-7282-4cda-b31c-40921fca6251.png" style="width:41.08em;height:2.08em;"/></div>
<p class="mce-root">Repeating this procedure iteratively until <em>t = 0</em>, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1f23a51b-189b-4fc7-9fa1-778ced322b11.png" style="width:42.83em;height:2.00em;"/></div>
<p>The term <em>γ<sup>t+1</sup> →</em> <em>0</em>, while continuing the iterations over the distance between <em>V<sup>(t)</sup></em> and <span><em>V(s; π)</em>, gets smaller and smaller, authorizing us to employ the iterative approach instead of the one-shot closed method. Hence, the <em>Bellman equation</em> becomes:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c1e54c26-05bf-4af6-b868-05d18dcc5a71.png" style="width:36.00em;height:3.25em;"/></div>
<p>This formula allows us to find the value for each state (the step is formally called <em>policy evaluation</em>), but, of course, it requires a policy. At the first step, we can randomly select the actions because we don't have any other piece of information, but after a complete evaluation cycle, we can start defining a greedy policy with respect to the values. In order to achieve this goal, we need to introduce a very important concept in RL, the <em>Q function</em><strong> </strong>(which must not be confused with the <em>Q function</em> defined in the EM algorithm), which is defined as the expected discounted return obtained by an agent starting from the state, <em>s<sub>t</sub></em>, and selecting a specific action, <em>a<sub>t</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ca507797-275d-4403-a25d-36a984ab908c.png" style="width:31.83em;height:4.33em;"/></div>
<p>The definition is very similar to <span><em>V(s; π)</em>, but, in this case, we include the action, a<sub>t</sub>, as a variable. Clearly, it's possible to define a Bellman equation for <em>Q(s, a; π)</em> by simply removing the policy/action summation:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/febf2bce-8a9a-481a-964e-5d0face4df19.png" style="width:38.08em;height:3.67em;"/></div>
<p>Sutton and Barto (in <em>Reinforcement Learning</em>, <em>Sutton R. S.,‎ Barto A. G.</em>, The MIT Press) proved a simple but very important theorem (called the <em>Policy improvement theorem</em>), which states that given the deterministic policies, <em>π<sub>1</sub></em> and <em>π<sub>2</sub></em>, if <em>Q(s, <span>π</span><sub>2</sub>(s); <span>π</span><sub>2</sub>) ≥ V(s; <span>π</span><sub>1</sub>) ∀ s ∈ S</em>, then <em><span>π</span><sub>2</sub></em> is better than or equal to <em><span>π</span><sub>1</sub></em><span>. The proof is very compact and can be found in their book, however, the result can be understood intuitively. If we consider a sequence of states, <em>s<sub>1</sub> → s<sub>2</sub> → ... → s<sub>n</sub></em> and <em>π<sub>2</sub>(s<sub>i</sub>) = π<sub>1</sub>(s<sub>i</sub>) ∀ i &lt; m &lt; n</em>, while <em>π<sub>2</sub>(s<sub>i</sub>) ≥ π<sub>1</sub>(s<sub>i</sub>) ∀ i ≥ m</em>, the policy, <em>π<sub>2</sub></em>, is at least equal to <em>π<sub>1</sub></em> and it's become better if at least an inequality is strict. Conversely, if <em>Q(s, π<sub>2</sub>(s); π<sub>2</sub>) ≥ V(s; π<sub>1</sub>)</em>, this means that <em>π<sub>2</sub>(s) ≥ π<sub>1</sub>(s)</em> and, again, <em>Q(s, π<sub>2</sub>(s); π<sub>2</sub>) &gt; V(s; π<sub>1</sub>)</em> if there's at least a state, <em>s<sub>i</sub></em>, where <em>π<sub>2</sub>(s<sub>i</sub>) &gt; π<sub>1</sub>(s<sub>i</sub>)</em>. Hence, after a complete policy evaluation cycle, we are authorized to define a new greedy policy as:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a6f0176d-da62-4f2e-9b0f-1e6677395158.png" style="width:21.08em;height:1.92em;"/></div>
<p class="mce-root">This step is called <em>policy improvement</em> and its goal is to set the action associated with each state as the one that leads to the transition to the successor state with the maximum value. It's not difficult to understand that an optimal policy will remain stable when <em><span>V</span><sup>(t)</sup><span> → </span></em><span><em>V(s; π)</em>. In fact, when <em>t → ∞</em>, the Q function will converge to a stable fixed point determined by <em>V(s; π)</em> and the argmax(•) will always select the same actions. However, if we start with a random policy, in general, a single policy evaluation cycle isn't enough to assure the convergence. Therefore, after a policy improvement step, it's often necessary to repeat the evaluation and continue alternating the two phases until the policy becomes stable (that's why the algorithm is called policy iteration). In general, the convergence is quite fast, but the actual speed depends on the nature of the problem, the number of states and actions, and the consistency of the rewards.</span></p>
<p>The complete policy iteration algorithm (as proposed by Sutton and Barto) is:</p>
<ol>
<li>Set an initial deterministic random policy <em>π(s)</em></li>
<li>Set the initial value array <em>V(s) = 0 ∀ s ∈ S</em></li>
<li>Set a tolerance threshold Thr (for example, Thr = 0.0001)</li>
<li>Set a maximum number of iterations <em>N<sub>iter</sub></em></li>
<li>Set a counter <em>e = 0</em></li>
</ol>
<ol>
<li>While <em>e &lt; N<sub>iter</sub></em>:
<ol>
<li>e += 1</li>
<li>Do:
<ol>
<li>Set <em>V<sub>old</sub>(s) = V(s) <span>∀ s </span><span>∈ S</span></em></li>
<li>Perform a Policy Evaluation step reading the current value from <em>V<sub>old</sub>(s)</em> and updating <em>V(s)</em></li>
</ol>
</li>
<li>While <em>Avg(|V(s) - V<sub>old</sub>(s)|) &gt; Thr</em></li>
<li>Set <em>π<sub>old</sub>(s) = π(s) <span>∀ s </span><span>∈ S</span></em></li>
<li>Perform a policy improvement step</li>
<li>If <em><span>π</span><sub>old</sub></em><span><em>(s) == π(s)</em>:</span>
<ol>
<li>Break</li>
</ol>
</li>
</ol>
</li>
<li>Output the final deterministic policy <em><span>π(s)</span></em></li>
</ol>
<div class="packt_infobox">In this case, as we have a full knowledge of the environment, there's no need for an exploration phase. The policy is always exploited as it's built to be greedy to the real value (obtained when t → ∞).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>We want to apply the policy iteration algorithm in order to find an optimal policy for the tunnel environment. Let's start by defining a random initial policy and a value matrix with all values (except the terminal states) equal to 0:</p>
<pre>import numpy as np<br/><br/>nb_actions = 4<br/><br/>policy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)<br/>tunnel_values = np.zeros(shape=(height, width))</pre>
<p>The initial random policy (t=0) is shown in the following chart:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1392 image-border" src="assets/5cf3aae5-0698-46d0-b37b-e578de590bcc.png" style="width:63.00em;height:24.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Initial (t=0) random policy</div>
<p>The states denoted with ⊗ represent the wells, while the final positive one is represented by the capital letter <em>E</em>. Hence, the initial value matrix (t=0) is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1409 image-border" src="assets/d43a363e-87a2-42c2-ba00-4db6a92904f9.png" style="width:62.58em;height:24.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Initial (t=0) value matrix</div>
<p>At this point, we need to define the functions to perform the policy evaluation and improvement steps. As the environment is deterministic, the processes are slightly simpler because the generic transition probability, <em>T(s<sub>i</sub>, s<sub>j</sub>; a<sub>k</sub>)</em>, is equal to 1 for the only possible successor and 0 otherwise. In the same way, the policy is deterministic and only a single action is taken into account. The policy evaluation step is performed, freezing the current values and updating the whole matrix, <em>V<sup>(t+1)</sup></em>, with <em>V<sup>(t)</sup></em>; however, it's also possible to use the new values immediately. I invite the reader to test both strategies in order to find the fastest way. In this example, we are employing a discount factor, γ = 0.9 (it goes without saying that an interesting exercise consists of testing different values and comparing the result of the evaluation process and the final behavior):</p>
<pre>import numpy as np<br/><br/>gamma = 0.9<br/><br/>def policy_evaluation():<br/>    old_tunnel_values = tunnel_values.copy() <br/>    <br/>    for i in range(height):<br/>        for j in range(width): <br/>            action = policy[i, j]<br/>            <br/>            if action == 0:<br/>                if i == 0:<br/>                    x = 0<br/>                else:<br/>                    x = i - 1<br/>                y = j<br/>                <br/>            elif action == 1:<br/>                if j == width - 1:<br/>                    y = width - 1<br/>                else:<br/>                    y = j + 1<br/>                x = i<br/>                <br/>            elif action == 2:<br/>                if i == height - 1:<br/>                    x = height - 1<br/>                else:<br/>                    x = i + 1<br/>                y = j<br/>                <br/>            else:<br/>                if j == 0:<br/>                    y = 0<br/>                else:<br/>                    y = j - 1<br/>                x = i<br/>                <br/>            reward = tunnel_rewards[x, y]<br/>            tunnel_values[i, j] = reward + (gamma * old_tunnel_values[x, y])<br/><br/>def is_final(x, y):<br/>    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):<br/>        return True<br/>    return False<br/><br/>def policy_improvement():<br/>    for i in range(height):<br/>        for j in range(width):<br/>            if is_final(i, j):<br/>                continue<br/>            <br/>            values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i &gt; 0 else -np.inf<br/>            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j &lt; width - 1 else -np.inf<br/>            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i &lt; height - 1 else -np.inf<br/>            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j &gt; 0 else -np.inf<br/>            <br/>            policy[i, j] = np.argmax(values).astype(np.uint8)</pre>
<p class="mce-root">Once the functions have been defined, we start the policy iteration cycle (with a maximum number of epochs, <em>N<sub>iter</sub></em> = 100,000, and a tolerance threshold equal to 10<sup>-5</sup>):</p>
<pre>import numpy as np<br/><br/>nb_max_epochs = 100000<br/>tolerance = 1e-5<br/><br/>e = 0<br/><br/>while e &lt; nb_max_epochs:<br/>    e += 1<br/>    old_tunnel_values = tunnel_values.copy()<br/>    policy_evaluation()<br/>    <br/>    if np.mean(np.abs(tunnel_values - old_tunnel_values)) &lt; tolerance:<br/>        old_policy = policy.copy()<br/>        policy_improvement()<br/><br/>        if np.sum(policy - old_policy) == 0:<br/>            break</pre>
<p>At the end of the process (in this case, the algorithm converged after 182 iterations, but this value can change with different initial policies), the value matrix is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1467 image-border" src="assets/2959dfcb-e800-4921-bc65-68566063961a.png" style="width:56.25em;height:22.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix</div>
<p>Analyzing the values, it's possible to see how the algorithm discovered that they are an implicit function of the distance between a cell and the ending state. Moreover, the policy always avoids the wells because the maximum value is always found in an adjacent state. It's easy to verify this behavior by plotting the final policy:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-1419 image-border" src="assets/9a1c460a-f0b9-4ed2-a650-536169c1cf7e.png" style="width:59.92em;height:23.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy</div>
<p>Picking a random initial state, the agent will always reach the ending one, avoiding the wells and confirming the optimality of the policy iteration algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value iteration</h1>
                </header>
            
            <article>
                
<p>An alternative approach to policy iteration is provided by the <em>value iteration</em> algorithm. The main assumption is based on the empirical observation that the policy evaluation step converges rather quickly and it's reasonable to stop the process after a fixed number of steps (normally 1). In fact, policy iteration can be imagined like a game where the first player tries to find the correct values considering a stable policy, while the other one creates a new policy that is greedy with respect to the new values. Clearly, the second step compromises the validity of the previous evaluation, forcing the first player to repeat the process. However, as the Bellman equation uses a single fixed point, the algorithm converges to a solution characterized by the fact that the policy doesn't change anymore and, consequently, the evaluation becomes stable. This process can be simplified by removing the policy improvement step and continuing the evaluation in a greedy fashion. Formally, each step is based on the following update rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/186e0f2f-9264-4455-8232-2095e3813a59.png" style="width:39.17em;height:3.67em;"/></div>
<p>Now the iteration doesn't consider the policy anymore (assuming implicitly that it will be greedy with respect to the values), and selects <em>V<sup>(t+1)</sup></em> as the maximum possible value among all <em>V<sup>(t)</sup>(a<sub>t</sub>)</em>. In other words, value iteration anticipates the choice that is made by the policy improvement step by selecting the value that corresponds to the action that is likely (p → 1) to be selected. It's not difficult to extend the convergence proof presented in the previous section to this case, therefore, <em>V<sup>(∞)</sup> → V<sup>(opt)</sup></em>, as well as policy iteration does. However, the average number of iterations is normally smaller because we are starting with a random policy that can contrast the value iteration process.</p>
<p>When the values become stable, the optimal greedy policy is simply obtained as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/19a2096e-6fa3-4062-aac7-ba08e39c17a2.png" style="width:18.33em;height:1.75em;"/></div>
<p class="mce-root">This step is formally equivalent to a policy improvement iteration, which, however, is done only once at the end of the process.</p>
<p>The complete value iteration algorithm (as proposed by Sutton and Barto) is:</p>
<ol>
<li>Set the initial value array, <em>V(s) = 0 ∀ s ∈ S</em></li>
<li>Set a tolerance threshold, <em>Thr</em>, (for example, <em>Thr = 0.0001</em>)</li>
<li>Set a maximum number of iteration, <em>N<sub>iter</sub></em></li>
<li>Set a counter, <em>e</em> = 0</li>
<li>While <em>e &lt; N<sub>iter</sub></em>:
<ol>
<li><em>e += 1</em></li>
<li>Do:
<ol>
<li>Set <em>V<sub>old</sub>(s) = V(s) <span>∀ s </span><span>∈ S</span></em></li>
<li>Perform a value evaluation step reading the current value from <em>V<sub>old</sub>(s)</em> and updating <em>V(s)</em></li>
</ol>
</li>
<li>While <em>Avg(|V(s) - V<sub>old</sub>(s)|) &gt; Thr</em></li>
</ol>
</li>
<li>Output the final deterministic policy <em><span>π(s) = argmax<sub>a</sub> Q(s, a)</span></em></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value iteration in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>To test this algorithm, we need to set an initial value matrix with all values equal to 0 (they can be also randomly chosen but, as we don't have any prior information on the final configuration, every initial choice is probabilistically equivalent):</p>
<pre>import numpy as np<br/><br/>tunnel_values = np.zeros(shape=(height, width))</pre>
<p>At this point, we can define the two functions to perform the value evaluation and the final policy selection (the function <kbd>is_final()</kbd> is the one defined in the previous example):</p>
<pre>import numpy as np<br/><br/>def value_evaluation():<br/>    old_tunnel_values = tunnel_values.copy() <br/>    <br/>    for i in range(height):<br/>        for j in range(width): <br/>            rewards = np.zeros(shape=(nb_actions, ))<br/>            old_values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            for k in range(nb_actions):<br/>                if k == 0:<br/>                    if i == 0:<br/>                        x = 0<br/>                    else:<br/>                        x = i - 1<br/>                    y = j<br/><br/>                elif k == 1:<br/>                    if j == width - 1:<br/>                        y = width - 1<br/>                    else:<br/>                        y = j + 1<br/>                    x = i<br/><br/>                elif k == 2:<br/>                    if i == height - 1:<br/>                        x = height - 1<br/>                    else:<br/>                        x = i + 1<br/>                    y = j<br/><br/>                else:<br/>                    if j == 0:<br/>                        y = 0<br/>                    else:<br/>                        y = j - 1<br/>                    x = i<br/>                <br/>                rewards[k] = tunnel_rewards[x, y]<br/>                old_values[k] = old_tunnel_values[x, y]<br/>                <br/>            new_values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            for k in range(nb_actions):<br/>                new_values[k] = rewards[k] + (gamma * old_values[k])<br/>                <br/>            tunnel_values[i, j] = np.max(new_values)<br/><br/>def policy_selection():<br/>    policy = np.zeros(shape=(height, width)).astype(np.uint8)<br/>    <br/>    for i in range(height):<br/>        for j in range(width):<br/>            if is_final(i, j):<br/>                continue<br/>            <br/>            values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i &gt; 0 else -np.inf<br/>            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j &lt; width - 1 else -np.inf<br/>            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i &lt; height - 1 else -np.inf<br/>            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j &gt; 0 else -np.inf<br/>            <br/>            policy[i, j] = np.argmax(values).astype(np.uint8)<br/>            <br/>    return policy</pre>
<p>The main differences are in the <kbd>value_evaluation()</kbd> function, which now has to consider all possible successor states and select the value corresponding to the action that leads to the state with the highest value. Instead, the <kbd>policy_selection()</kbd> function is equivalent to <kbd>policy_improvement()</kbd>, but, as it is invoked only once, it outputs directly to the final optimal policy.</p>
<p>At this point, we can run a training cycle (assuming the same constants as before):</p>
<pre>import numpy as np<br/><br/>e = 0<br/><br/>policy = None<br/><br/>while e &lt; nb_max_epochs:<br/>    e += 1<br/>    old_tunnel_values = tunnel_values.copy()<br/>    value_evaluation()<br/>    <br/>    if np.mean(np.abs(tunnel_values - old_tunnel_values)) &lt; tolerance:<br/>        policy = policy_selection()<br/>        break</pre>
<p>The final value configuration (after 127 iterations) is shown in the following chart:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1471 image-border" src="assets/aad39ec1-85b2-491a-af13-66db44f7841c.png" style="width:166.58em;height:65.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix</div>
<p>As in the previous example, the final value configuration is a function of the distance between each state and the ending one, but, in this case, the choice of <em>γ = 0.9</em> isn't optimal. In fact, the wells close to the final state aren't considered very dangerous anymore. Plotting the final policy can help us understand the behavior:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1473 image-border" src="assets/44dc4df9-0c90-40d7-8bec-eccfd897d1ec.png" style="width:54.42em;height:20.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy</div>
<p>As expected, the wells that are far from the target are avoided, but the two that are close to the final state are accepted as reasonable penalties. This happens because the value iteration algorithm is very greedy with respect to the value and the discount factor, <em>γ &lt; 1.0</em>; the effect of negative states can be compensated for by the final reward. In many scenarios, these states are absorbing, therefore their implicit reward is <em>+∞</em> or <em>-∞</em>, meaning that no other actions can change the final value. I invite the reader to repeat the example with different discount factors (remember that an agent with <em>γ → 1</em> is very short-sighted and will avoid any obstacle, even reducing the efficiency of the policy) and change the values of the final states. Moreover, the reader should be able to answer the question: What is the agent's behavior when the standard reward (whose default value is <em>-0.1</em>) is increased or decreased?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD(0) algorithm</h1>
                </header>
            
            <article>
                
<p>One of the problems with Dynamic Programming algorithms is the need for a full knowledge of the environment in terms of states and transition probabilities. Unfortunately, there are many cases where these pieces of information are unknown before the direct experience. In particular, the states can be discovered by letting the agent explore the environment, but the transition probabilities require us to count the number of transitions to a certain state and this is often impossible.</p>
<p>Moreover, an environment with absorbing states can prevent visiting many states if the agent has learned a good initial policy. For example, in a game, which can be described as an episodic MDP, the agent discovers the environment while learning how to move forward without ending in a negative absorbing state.</p>
<p>A general solution to these problems is provided by a different evaluation strategy, called <strong>Temporal Difference</strong> (<strong>TD</strong>) RL. In this case, we start with an empty value matrix and we let the agent follow a greedy policy with respect to the value (but the initial one, which is generally random). Once the agent observes a transition, <em>s<sub>i</sub> → s<sub>j</sub></em>, due to an action, <em>a<sub>t</sub></em>, with a reward, <em>r<sub>ij</sub></em>, it updates the estimation of <em>V(<span>s</span><sub>i</sub>)</em>. The process is structured in episodes (which is the most natural way) and ends when a maximum number of steps have been done or a terminal state is met. In particular, the TD(0) algorithm updates the value according to the rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8832ace7-a132-4c8a-96a8-7f7e0b719eb1.png" style="width:27.67em;height:2.42em;"/></div>
<p class="mce-root">The constant, <em>α</em>, is bound between 0 and 1 and acts as a learning rate. Each update considers a variation with respect to the current value, <em>V<sup>(t)</sup>(s<sub>i</sub>)</em>, which is proportional to the difference between the actual return and the previous estimation. The term <em>r<sub>ij</sub> + γV<sup>(t)</sup>(s<sub>j</sub>)</em> is analogous to the one employed in the previous methods and represents the expected value given the current return and the discounted value starting from the successor state. However, as <em><span>V</span><sup>(t)</sup>(s<sub>j</sub>)</em> is an estimation, the process is based on a bootstrap from the previous values. In other words, we start from an estimation to determine the next one, which should be closer to the stable fixed point. Indeed, TD(0) is the simplest example of a family of TD algorithms that are based on a sequence (usually called backup) that can be generalized as (considering k steps):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/94db5eb2-cbcf-4559-9c1d-d11c3e3dcc18.png" style="width:29.25em;height:1.92em;"/></div>
<p>As we're using a single reward to approximate the expected discounted return, TD(0) is usually called a one-step TD method (or one-step backup). A more complex algorithm can be built considering more subsequent rewards or alternative strategies. We're going to analyze a generic variant called TD(<em>λ</em>) in <a href="345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml">Chapter 15</a>, <span><em>Advanced Policy Estimation Algorithms</em> </span>and explain why this algorithm corresponds to a choice of <em>λ = 0</em>.</p>
<p class="mce-root">TD(0) has been proven to converge, even if the proof <span>(which can be found for a model-based approach in <em>Convergence of Model-Based Temporal Difference Learning for Control</em>, <em>Van Hasselt H.</em>, <em>Wiering M. A.</em>, <em>Proceedings of the 2007 IEEE Symposium on Approximate Dynamic </em><em>Programming and Reinforcement Learning</em></span> (ADPRL 2007)) is more complex because it's necessary to consider the evolution of the Markov Process. In fact, in this case, we are approximating the expected discounted return with both a truncated estimation and a bootstrap value, <em>V(s<sub>j</sub>)</em>, which is initially (and for a large number of iterations) unstable. However, assuming the convergence for <em>t → ∞</em>, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5a45126c-3a25-4e27-8279-352de7b6b902.png" style="width:44.25em;height:2.33em;"/></div>
<p class="mce-root">The last formula expresses the value of the state, <em>s<sub>i</sub></em>, assuming that the greedy optimal policy forces the agent to perform the action that causes the transition to <em>s<sub>j</sub></em>. Of course, at this point, it's natural to ask under which conditions the algorithm converges. In fact, we are considering episodic tasks and the estimation, <em>V<sup>(∞)</sup>(s<sub>i</sub>)</em>, can be correct only if the agent performs a transition to <em>s<sub>i</sub></em> an infinite number of times, selecting all possible actions an infinite number of times. <span>Such a condition is often expressed by saying that the policy must be</span><span> <strong>Greedy in the Limit with Infinite Explorations</strong> (<strong>GLIE</strong></span>)<span>. In other words, the real greediness is achieved only as an asymptotic state when the agent is able to explore the environment without limitations for an unlimited number of episodes.</span></p>
<p class="mce-root">This is probably the most important limitation of TD RL, because, in real-life scenarios, some states can be very unlikely and, hence, the estimation can never accumulate the experience needed to converge to the actual value. We are going to analyze some methods to solve this problem in <a href="345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml">Chapter 15</a>, <em><span>Advanced Policy Estimation Algorithms</span></em>, but, in our example, we employ a random start. In other words, as the policy is greedy and could always avoid some states, we force the agent to start each episode in a random nonterminal cell. In this way, we allow a deep exploration even with a greedy policy. Whenever this approach is not feasible (because, for example, the environment dynamics are not controllable), the exploration-exploitation dilemma can be solved only by employing an ε-greedy policy, which selects a fraction of suboptimal (or even wrong) actions. In this way, it's possible to observe a higher number of transitions paying the price of a slower convergence.</p>
<p class="mce-root">However, as pointed out by Sutton and Barto, TD(0) converges to the maximum-likelihood estimation of the value function determined by the MDP, finding the implicit transition probabilities of the model. Therefore, if the number of observations is high enough, TD(0) can quickly find an optimal policy, but, at the same time, it's also more sensitive to biased estimations if some couple's state-action are never experienced (or experienced very seldom). In our example, we don't know which the initial state is, hence selecting a fixed starting point yields a policy that is extremely rigid and almost completely unable to manage noisy situations. For example, if the starting point is changed to an adjacent (but never explored) cell, the algorithm could fail to find the optimal path to the positive terminal state. On the other hand, if we know that the dynamics are well-defined, TD(0) will force the agent to select the actions that are most likely to produce the optimal result given the current knowledge of the environment. If the dynamics are partially stochastic, the advantage of an <span>ε-greedy policy can be understood considering a sequence of episodes where the agent experiences the same transitions and the corresponding values are increased proportionally. If, for example, the environment changes one transition after many experiences, the agent has to face a brand new experience when the policy is already almost stable. The correction requires many episodes and, as this random change has a very low probability, it's possible that the agent will never learn the correct behavior. Instead, by selecting a few random actions, the probability of encountering a similar state (or even the same one) increases (think about a game where the state is represented by a screenshot) and the algorithm can become more robust with respect to very unlikely transitions.</span></p>
<p>The complete<span> </span>TD(0)<span> </span>algorithm is:</p>
<ol>
<li>Set an initial deterministic random policy, <em>π(s)</em></li>
<li>Set the initial value array, <em>V(s) = 0 ∀ s ∈ S</em></li>
<li>Set the number of episodes, <em>N<sub>episodes</sub></em></li>
<li>Set a maximum number of steps per episode, <em>N<sub>max</sub></em></li>
<li>Set a constant, <em>α</em> (for example, <span><em>α = 0.1</em>)</span></li>
<li>Set a constant, <em>γ</em> (for example, <em><span>γ</span></em><span><em> = 0.9</em>)</span></li>
<li>Set a counter, <em>e = 0</em></li>
<li>For <em>i = 1</em> to <em>N<sub>episodes</sub></em>:
<ol>
<li>Observe the initial state, <em>s<sub>i</sub></em> </li>
<li>While <em>s<sub>j</sub></em> is non-terminal and <em>e &lt; N<sub>max</sub></em>:
<ol>
<li><em>e += 1</em></li>
<li>Select the action, <em><em>a<sub>t</sub> = <span>π(s<sub>i</sub>)</span></em></em></li>
<li>Observe the transition, <em>(a<sub>t</sub>, s<sub>i</sub>) → (s<sub>j</sub>, r<sub>ij</sub>)</em></li>
<li>Update the value function for the state, <em>s<sub>i</sub></em></li>
<li>Set <em>s<sub>i</sub> = s<sub>j</sub></em></li>
</ol>
</li>
<li>Update the policy to be greedy with respect to the value function, <em><span>π(s) = argmax<sub>a</sub> Q(s, a)</span></em></li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD(0) in the checkerboard environment</h1>
                </header>
            
            <article>
                
<p>At this point, we can test the TD(0) algorithm on the checkerboard environment. The first step is to define an initial random policy and a value matrix with all elements equal to 0:</p>
<pre>import numpy as np<br/><br/>policy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)<br/>tunnel_values = np.zeros(shape=(height, width))</pre>
<p>As we want to select a random starting point at the beginning of each episode, we need to define a helper function that must exclude the terminal states (all the constants are the same as previously defined):</p>
<pre>import numpy as np<br/><br/>xy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)<br/>xy_grid = np.array(xy_grid).T.reshape(-1, 2)<br/><br/>xy_final = list(zip(x_wells, y_wells))<br/>xy_final.append([x_final, y_final])<br/><br/>xy_start = []<br/><br/>for x, y in xy_grid:<br/>    if (x, y) not in xy_final:<br/>        xy_start.append([x, y])<br/>        <br/>xy_start = np.array(xy_start)<br/><br/>def starting_point():<br/>    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])<br/>    return xy[0], xy[1]</pre>
<p>Now we can implement the function to evaluate a single episode (setting the maximum number of steps equal to 500 and the constant to <em>α</em> = 0.25):</p>
<pre>max_steps = 1000<br/>alpha = 0.25<br/><br/>def episode():<br/>    (i, j) = starting_point()<br/>    x = y = 0<br/>    <br/>    e = 0<br/>    <br/>    while e &lt; max_steps:<br/>        e += 1<br/>        <br/>        action = policy[i, j]<br/>            <br/>        if action == 0:<br/>            if i == 0:<br/>                x = 0<br/>            else:<br/>                x = i - 1<br/>            y = j<br/>                <br/>        elif action == 1:<br/>            if j == width - 1:<br/>                y = width - 1<br/>            else:<br/>                y = j + 1<br/>            x = i<br/>                <br/>        elif action == 2:<br/>            if i == height - 1:<br/>                x = height - 1<br/>            else:<br/>                x = i + 1<br/>            y = j<br/>                <br/>        else:<br/>            if j == 0:<br/>                y = 0<br/>            else:<br/>                y = j - 1<br/>            x = i<br/>                <br/>        reward = tunnel_rewards[x, y]<br/>        tunnel_values[i, j] += alpha * (reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j])<br/>        <br/>        if is_final(x, y):<br/>            break<br/>        else:<br/>            i = x<br/>            j = y</pre>
<p>The function to determine the greedy policy with respect to the values is the same as already implemented in the previous examples; however, we report it to guarantee the consistency of the example:</p>
<pre>def policy_selection():<br/>    for i in range(height):<br/>        for j in range(width):<br/>            if is_final(i, j):<br/>                continue<br/>            <br/>            values = np.zeros(shape=(nb_actions, ))<br/>            <br/>            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i &gt; 0 else -np.inf<br/>            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j &lt; width - 1 else -np.inf<br/>            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i &lt; height - 1 else -np.inf<br/>            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j &gt; 0 else -np.inf<br/>            <br/>            policy[i, j] = np.argmax(values).astype(np.uint8) </pre>
<p>At this point, we can start a training cycle with 5,000 episodes:</p>
<pre>n_episodes = 5000<br/><br/>for _ in range(n_episodes):<br/>    episode()<br/>    policy_selection()</pre>
<p>The final value matrix is shown in the following chart:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1488 image-border" src="assets/5c8f833f-bc0f-4df7-9bcf-44c337ab56e0.png" style="width:166.58em;height:64.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix with random starts</div>
<p>Like in the previous examples, the final values are inversely proportional to the distance from the final positive state. Let's analyze the resulting policy to understand whether the algorithm converged to a consistent solution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1493 image-border" src="assets/daddaf8e-2d83-4d14-8b34-9c36cee7499a.png" style="width:73.92em;height:28.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final policy with random starts</div>
<p>As can be seen, the random choice of the starting state is allowed to find the best path independently from the initial condition. To better understand the advantage of this strategy, let's plot the final value matrix when the initial state is fixed to the cell <em>(0, 0)</em>, corresponding to the upper-left corner:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1498 image-border" src="assets/6397023e-d14b-418b-92c8-cdf5f2fd971e.png" style="width:75.92em;height:29.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final value matrix with a fixed initial state (0, 0)</div>
<p>Without any further analysis, it's possible to see that many states have never been visited or visited only a few times, and the resulting policy is therefore extremely greedy with respect to the specific initial state. The blocks containing values equal to -1.0 indicate states where the agent often has to pick a random action because there's no difference in the values, hence it can be extremely difficult to solve the environment with a different initial state. The resulting policy confirms this analysis:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1502 image-border" src="assets/92b1dadf-3967-4406-ab59-56adb216913f.png" style="width:77.25em;height:29.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Final policy with a fixed initial state (0, 0)</span></div>
<p class="mce-root">As it's possible to see, the agent is able to reach the final state only when the initial point allows us to cross the trajectory starting from <em>(0, 0)</em>. In all these cases, it's possible to recover the optimal policy, even if the paths longer than the ones obtained in the previous example. Instead, states such as <em>(0, 4)</em> are clearly situations where there's a <em>loss of policy</em>. In other words, the agent acts without any knowledge or awareness and the probability of success converges to 0. As an exercise, I invite the reader to test this algorithm with different starting points (for example, a set of fixed ones) and higher <em>α</em> values. The goal is also to answer these questions: Is it possible to speed up the learning process? Is it necessary to start from all possible states in order to obtain a global optimal policy?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the most important RL concepts, focusing on the mathematical structure of an environment as a Markov Decision Process, and on the different kinds of policy and how they can be derived from the expected reward obtained by an agent. In particular, we defined the value of a state as the expected future reward considering a sequence discounted by a factor, <em>γ</em>. In the same way, we introduced the concept of the Q function, which is the value of an action when the agent is in a specific state.</p>
<p>These concepts directly employed the policy iteration algorithm, which is based on a Dynamic Programming approach assuming complete knowledge of the environment. The task is split into two stages; during the first one, the agent evaluates all the states given the current policy, while in the second one, the policy is updated in order to be greedy with respect to the new value function. In this way, the agent is forced to always pick the action that leads to a transition that maximizes the obtained value.</p>
<p>We also analyzed a variant, called value iteration, that performs a single evaluation and selects the policy in a greedy manner. The main difference from the previous approach is that now the agent immediately selects the highest value assuming that the result of this process is equivalent to a policy iteration. Indeed, it's easy to prove that, after infinite transitions, both algorithms converge on the optimal value function.</p>
<p>The last algorithm is called TD(0) and it's based on a model-free approach. In fact, in many cases, it's difficult to know all the transition probabilities and, sometimes, even all possible states are unknown. This method is based on the Temporal Difference evaluation, which is performed directly while interacting with the environment. If the agent can visit all the states an infinite number of times (clearly, this is only a theoretical condition), the algorithm has been proven to converge to the optimal value function more quickly than other methods.</p>
<p>In the next chapter, <a href="345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml" target="_blank">Chapter 15</a>, <em>Advanced Policy Estimation Algorithms</em> we'll continue the discussion of RL algorithms, introducing some more advanced methods that can be immediately implemented using Deep Convolutional Networks.</p>


            </article>

            
        </section>
    </body></html>