<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Time Series with LSTMs</h1>
                </header>
            
            <article>
                
<p>In many situations involving multiple fields of real life, the need to plan future actions<span> </span><span>arises</span><span>. <strong>Forecasting</strong> is an important tool for efficient planning. Moreover, this tool makes the decision-maker less susceptible to unexpected events because it requires a more scientific approach to the knowledge of the environment in which it operates. Often, the planning of future action arises from the analysis of data accumulated over time to extract information for the characterization of the phenomenon under observation.</span></p>
<p>A chronological recording of events gives rise to a new type of act, which is precisely called a <strong>time series</strong>. A time series constitutes a sequence of observations on a phenomenon carried out in consecutive instants or time intervals. Usually, even if not necessarily, they are evenly spaced or of the same length. Time series prediction requires the neural network to have some sort of memory on the sequence of data. Specific architectures called <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) network, are well suited for time series analysis. In this chapter we show how to create and train our own LSTMs using Keras on GCD and apply them to predicting financial time series. We'll discover the most used modeling approaches: <span><strong>autoregressive</strong> (</span><strong>AR</strong>), <strong>moving average</strong> (<strong>MA</strong>), <span><strong>autoregressive moving average</strong> (</span><strong>ARMA</strong>), and <span><strong>autoregressive integrated moving average</strong> (</span><strong>ARIMA</strong>).</p>
<p>The topics covered in this chapter are:</p>
<ul>
<li>Classical approach to time series</li>
<li>Time series decomposition</li>
<li>Time series models</li>
<li>LSTM for time series analysis</li>
</ul>
<p>At the end of the chapter, we will be able to deal with problems regarding time series. We will know how to identify the different components of a time series, trend seasonality and residual, as well as eliminate seasonality to make predictions easier to understand. Finally, we will understand how to implement a recurring LSTM network with a practical example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing time series </h1>
                </header>
            
            <article>
                
<p>A time series constitutes a sequence of observations on a phenomenon <em>y</em> carried out in consecutive instants or time intervals that are usually, even if not necessarily, evenly spaced or of the same length. The trend of commodity prices, stock market indices, the BTP/BUND spread, and the unemployment rate are just a few examples of times series.</p>
<p>Contrary to what happens in classical statistics, where it is assumed that an independent observations come from a single random variable, in a time series, it is assumed that there are n observations coming from as many dependent random variables. The inference of the time series is thus configured as a procedure that attempts to bring the time series back to its generating process.</p>
<p>The time series can be of two types:</p>
<ul>
<li><strong>Deterministic</strong>: If the values of the variable can be exactly determined on the basis of the previous values</li>
<li><strong>Stochastic</strong>: If the values of the variable can be determined on the basis of the previous values only partially</li>
</ul>
<p>The majority of time series are stochastic, and therefore it is impossible to draw up forecasts without errors. It is generally assumed that an observed time series is the result of the composition of these two components. The two sequences are not individually observable but must be determined on the basis of a sample.</p>
<p>We indicate the series as the sum of these two contributions:</p>
<div class="CDPAlignCenter CDPAlign"><em>Y<sub>t</sub> = f(t) + w(t)</em></div>
<p>According to the classical approach to time series, it is assumed that there exists a law of temporal evolution of the phenomenon, represented by <em>f(t)</em>. The random component <em>w(t)</em> is assumed to represent the set of circumstances, each of negligible entities, which we do not want or cannot consider in <em>Y<sub>t</sub></em>.</p>
<p>Therefore, the residual part of <em>Y<sub>t</sub></em>, not explained by <em>f(t)</em>, is imputed to the case and assimilated to a set of accidental errors. This is equivalent to hypothesizing that the stochastic component <em>w(t)</em> is generated by a white noise process, that is, by a sequence of independent and identically distributed random variables of zero mean and constant variance. In summary, in the classic approach, the attention is concentrated on <em>f(t)</em><em>,</em> being <em>w(t)</em> considered a process with uncorrelated components and therefore negligible.</p>
<p>Denoting<span> </span><span>the time</span><span> with</span> <em>t = 1…. T</em><span>, we will indicate this sequence</span> <em>y<sub>t</sub></em><span>; time is the parameter that determines the sequence of events that cannot be neglected, so we also need to know the position of observation along the temporal dimension. Generally, it is used to represent the pair of values ​​</span><em>(t, y<sub>t</sub>)</em> <span>on a Cartesian diagram with a continuous line graph as if the phenomenon were detected continuously. This graph is called a</span> <strong>time series plot</strong><span>. In the following graph, we see a time series plot of the flow of the river Nile at Aswan from 1871 to 1970:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d67c0c49-def5-4e0f-a85a-645ed7397742.png" style=""/></div>
<p>A time series plot immediately reveals trends or regular oscillations, and other systematic trends over time. The previous graph shows annual data in a systematically decreasing trend over the long term. In particular, it has a zigzag pattern; since the data is monthly, there is the phenomenon called <strong>seasonality</strong>. It can be noted that high peaks are always recorded in those months when rains are expected.</p>
<p>The univariate analysis of the time series proposes to interpret the dynamic mechanism that generated the series, and to foresee future realizations of the phenomenon. In these operations, the information that is exploited regards only the couple <em>(t; Y<sub>t</sub>)</em>, where <em>t = 1,…, T</em>. The fundamental point is that the past and the present contain relevant information to predict the future evolution of the phenomenon.</p>
<p>It can be considered that univariate analysis is too restrictive; we usually have information on phenomena related to the one to be forecast, which should be appropriately incorporated in order to improve the performance of the model of revision. Nonetheless, it is a useful benchmark that allows validation more sophisticated alternatives.</p>
<p>In a time series plot, four types of patterns can be identified with respect to time:</p>
<ul>
<li><strong>Horizontal pattern</strong>: In this case, the series oscillates around a constant value (series average). This series is called <strong>stationary</strong> on <strong>average</strong>. This is the typical case that occurs in quality control when the process is kept under control with respect to the average.</li>
<li><strong>Seasonal pattern</strong>: This exists when the series is influenced by seasonal factors (example, monthly, semi-annual, quarterly, and so on). Products such as ice cream, soft drinks, electricity consumption are subject to the seasonal phenomenon. The series influenced by seasonality are also called <strong>periodic series</strong> since the seasonal cycle repeats itself in a fixed period. In the annual data, seasonality is not present.</li>
<li><strong>Cyclic pattern</strong>: This type of trend is present when the series has increases and decreases that are not of fixed period. This is the main difference between cyclical and seasonal fluctuations. Moreover, the amplitude of cyclical oscillations is generally larger than that due to seasonality. In economic series, the cyclical pattern is determined by the expansions and contractions of the economy due to conjectural phenomena.</li>
<li><strong>Trend or underlying trend</strong>: It is characterized by an increasing or decreasing long-term trend. The series of the world resident population is an example of an increasing trend; the series of monthly beer sales, on the other hand, does not show any trend. It has a horizontal background pattern.</li>
</ul>
<p>Many series highlight a combination of these patterns. It is precisely this kind of complexity that makes the forecasting operation extremely interesting. The forecasting methods, in fact, must be able to recognize the various components of the series in order to reproduce them in the future, in the hypothesis that the past pattern continues to repeat itself in its evolutionary characteristics also in the future.</p>
<p>The classic approach to time series is based on the decomposition of the deterministic part of the series into a set of signal components (which express the structural information of the series) with respect to the negligible part of noise. In practice, we will try to identify some of the patterns that we have previously listed in the time series trend. The following figure shows a time series with some components identified:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-739 image-border" src="assets/69e99378-6407-4224-a3d5-4d085775b33c.png" style=""/></div>
<p>In the previous figure, the components we have identified are:</p>
<ul>
<li><strong>Trend</strong>: It is the underlying trend of the phenomenon considered, referring to a long period of time.</li>
<li><strong>Seasonal</strong>: This consists of movements of the phenomena during the year. Due to the influence of climatic and social factors, they tend to repeat themselves in a similar way in the same period (for example, month, quarter, and so on).</li>
<li><strong>Residual</strong>: In the <span>time series </span>models, there is never a perfect relation between the variable under observation and the different components. The accidental component takes into account this and the unpredictable behavior of economic agents, social, and so on.</li>
</ul>
<p>Finally, we can say that by adopting this approach a time series can be seen as the sum of the three components just analyzed (additive method).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classical approach to time series</h1>
                </header>
            
            <article>
                
<p>So far we have dealt with time series according to a classic approach to the topic. In this perspective, the classic models that try to simulate the phenomenon can be of two types:</p>
<ul>
<li><strong>Composition models</strong>: The elementary components are known, and, by assuming a certain form of aggregation, the resulting series is obtained</li>
<li><strong>Decomposition models</strong>: From an observed series is hypothesized the existence of some elementary trends of which we want to establish the characteristics</li>
</ul>
<p>The decomposition models are the most used in practice, and, for this reason, we will analyze them in detail.</p>
<p>The components of a time series can be aggregated according to different types of methods:</p>
<ul>
<li><strong>Additive method</strong>: <em>Y(t) = τ(t) + C(t) + S(t) + r(t)</em></li>
<li><strong>Multiplicative method</strong>: <em>Y(t) = τ(t) * C(t) * S(t) * r(t)</em></li>
<li><strong>Mixed method</strong>: <em>Y(t) = τ(t) * C(t) + S(t) * r(t)</em></li>
</ul>
<p>In these formulas, the factors are defined as follows:</p>
<ul>
<li><em>Y(t)</em> represents the time series</li>
<li><em>τ(t) </em><span>represents the </span>trend component</li>
<li><em>C(t) </em><span>represents the </span>cyclic component</li>
<li><em>S(t) </em><span>represents the </span>seasonality component</li>
<li><em>r(t)</em> <span>represents the </span>residual component</li>
</ul>
<p>The multiplicative model can be traced back to the additive model through a logarithmic transformation of the components of the series:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y(t) = τ(t) * C(t) * S(t) * r(t)</em></p>
<p>This formula, by applying the logarithm function to all factors, becomes:</p>
<p class="CDPAlignCenter CDPAlign"><em>lnY(t) = lnτ(t) + lnC(t) + lnS(t) + lnr(t)</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimation of the trend component</h1>
                </header>
            
            <article>
                
<p>Estimation of the trend component can occur in two different modes depending on the linear/non-linear characteristic.</p>
<p>If the series trend is linear or linearizable in the parameters through a logarithmic transformation, then these trends can be estimated through the procedures derived from linear regression. We can hypothesize a polynomial trend that can be represented by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><em>τ (t) = α<sub>0</sub> + α<sub>1</sub> t + α<sub>2</sub> t<sub>2</sub> + ... + α<sub>q</sub> t<sub>q</sub> + εt</em></p>
<p>In this formula, <em>q</em> represents the degree of the polynomial.</p>
<p>Depending on the value assumed by <em>q</em>, the following cases can be represented:</p>
<table>
<tbody>
<tr>
<td><strong>q</strong></td>
<td><strong>Cases</strong></td>
</tr>
<tr>
<td><em>0</em></td>
<td><span>A constant trend is obtained</span></td>
</tr>
<tr>
<td><em>1</em></td>
<td><span>We obtain a linear trend</span></td>
</tr>
<tr>
<td><em>2</em></td>
<td><span>We obtain a parabolic trend</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>On the contrary, the presence of a non-linear trend makes it difficult, if not impossible, to identify a known functional form <em>f(t)</em> with which to express the trend component.</p>
<p>In these cases, the MA instrument is used. MA is an arithmetic mean (simple or weighted) that moves to each new iteration (at any time <em>t</em>) from the beginning to the end of the data sequence.</p>
<p>Suppose we have <em>n</em> data terms:</p>
<p class="CDPAlignCenter CDPAlign"><em>a1, a2, a3, ..., a<sup>(n-1)</sup>, a<sup>n</sup></em></p>
<p>The following procedure is adopted:</p>
<ol>
<li>First, we calculate the average of the first three data and substitute the average value for the central data</li>
<li>Then, we repeat the procedure with the second three data</li>
<li>The procedure is exhausted when there is no more data available</li>
</ol>
<p>In the case considered, the MA is composed of only three data. The MA order can be extended to 5, 7, 9, and so on. In order for the MA to be centered with respect to the available data, the order must be odd.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating the seasonality component</h1>
                </header>
            
            <article>
                
<p>The study of the seasonality of a historical series can have the purpose of:</p>
<ul>
<li>Simply estimating the seasonal component</li>
<li>Eliminating it from the general course once it has been estimated</li>
</ul>
<p>If you have to compare several time series with different seasonality, the only way to compare them is by a seasonal adjustment of them.</p>
<p>There are several ways to estimate the seasonal component. One of these is the use of a regression model using dichotomous auxiliary variables (dummy variables).</p>
<p>Suppose the existence of an additive model without a trend component:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y(t) = S(t) + r(t)</em></p>
<p>And suppose we have measured the series on a monthly basis. The dummy variables can be defined in the following way:</p>
<ul>
<li><em>d<sub>j</sub>(t)</em>: 1 if the observation <em>t</em> is relative to the <em>j<sup>th</sup></em> month of the year</li>
<li><em>d<sub>j</sub>(t)</em>: 0 otherwise</li>
</ul>
<p>Once the periodic dummy variables have been created, the seasonal component can be estimated using the following regression model:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y(t) = β<sub>1</sub>D<sub>1</sub> + β<sub>2</sub>D<sub>2</sub> + ... + β<sub>n</sub>D<sub>n</sub> + ε(t)</em></p>
<p>The remaining <em>ε(t)</em> part of the model represents the part of the series not explained by seasonality. If a trend component is present in the series, it will coincide precisely with <em>ε(t)</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time series models</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we explored the basics behind time series. To perform correct predictions of future events based on what happened in the past, it is necessary to construct an appropriate numerical simulation model. Choosing an appropriate model is extremely important as it reflects the underlying structure of the series. In practice, two types of models are available: linear or non-linear (depending on whether the current value of the series is a linear or non-linear function of past observations).</p>
<p>The following are the most widely used models for forecasting time series data:</p>
<ul>
<li>AR</li>
<li>MA</li>
<li>ARMA</li>
<li>ARIMA</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoregressive models</h1>
                </header>
            
            <article>
                
<p>AR models are a very useful tool to tackle the prediction problem in relation to a time series. A strong correlation between consecutive values of a series is often observed. In this case, we speak of autocorrelation of the first order when we consider adjacent values, of the second order if we refer to the relation between the values of the series after two periods, and in general of the <em>p<sup>th</sup></em> order if the values considered have <em>p</em> periods between them. AR models allow exploiting these bonds to obtain useful forecasts of the future behavior of the series.</p>
<p>AR is a linear predictive modeling technique. This model tries to predict the time series based on the previous values assumed using the AR parameters as coefficients. The number of samples used for the forecast determines the order of the model (<em>p</em>). As the name indicates, it is a regression of the variable against itself; that is, a linear combination of past values of the variables is used to forecast the future value. The AR model of <em>p</em> order is defined as:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/8eda0e86-d38f-4a1b-8bef-eaef767b632f.png" style="width:17.25em;height:4.58em;"/></div>
<p>In the previous formula, the terms are defined as follows:</p>
<ul>
<li><em>Y<sub>t</sub></em> is the actual value at time period <em>t</em></li>
<li><em>c</em> is a constant</li>
<li><em>ϕ<sub>i</sub> (i = 1,2,..., p)</em> are model parameters</li>
<li><em>Y<sub>t-i</sub></em> is the past value at time period <em>t-i</em></li>
<li><em>ε<sub>t</sub></em> is the random error at time period <em>t</em> (white noise)</li>
</ul>
<p>It may happen that the constant term is omitted; this is done to make the model as simple as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Moving average models</h1>
                </header>
            
            <article>
                
<p>The MA model specifies that the output variable depends linearly on the past and current past values of a stochastic term (imperfectly predictable). The MA model should not be confused with the MA we have seen in the previous sections. This is an essentially different concept although some similarities are evident. Unlike the AR model, the finished MA model is always stationary.</p>
<p>Just as a model AR (<em>p</em>) regresses with respect to the past values of the series, an MA (<em>q</em>) model uses past errors as explanatory variables.</p>
<p>The MA model of <em>q</em> order is defined as:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/c1d41ff4-ed45-4e09-8e06-0f0627147180.png" style="width:17.08em;height:4.58em;"/></div>
<p>In the previous formula, the terms are defined as follows:</p>
<ul>
<li><em>Y<sub>t</sub></em> is the actual value at time period <em>t</em></li>
<li><em>μ</em> is the mean of the series</li>
<li><em>θ<sub>i</sub> (i = 1,2,..., q)</em> are model parameters</li>
<li><em>ε<sub>t-i</sub></em> is the past random error at time period <em>t-i</em></li>
<li><em>ε<sub>t</sub></em> is the random error at time period <em>t</em> (white noise)</li>
</ul>
<p>The MA model is essentially a finite impulsive response filter applied to white noise, with some additional interpretations placed on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoregressive moving average model </h1>
                </header>
            
            <article>
                
<p>ARMA is a type of linear mathematical model that provides instant by instant an output value based on the previous input and output values. The system is seen as an entity that, instant by instant, receives an input value (input) and generates an output (output), calculated on the basis of internal parameters that in turn vary according to linear laws. Each internal parameter, therefore, will be at each instant place equal to a linear combination of all internal parameters of the previous instant and the incoming value. The output value, <span>in turn, </span>will be a linear combination of internal parameters, and, in rare cases, also of the incoming one.</p>
<p>Much more simply, ARMA can be seen as an effective combination of the AR and MA models to form a general and useful class of time series models.</p>
<p>The model is generally defined as the ARMA model <em>(p, q)</em> where <em>p</em> is the order of the AR part and <em>q</em> is the order of the part of the MA. The ARMA model is defined by the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/7e0a27b4-8a05-46ac-9914-23ff6eb59e28.png" style="width:26.75em;height:4.58em;"/></div>
<p>The terms are defined as follows:</p>
<ul>
<li><em>Y<sub>t</sub></em> is the actual value at time period <em>t</em></li>
<li><em>c</em> is <span>again </span>a constant</li>
<li><em>ϕ<sub>i</sub> (i = 1,2,..., p)</em> are AR model parameters</li>
<li><em>Y<sub>t-i</sub></em> is the past value at time period <em>t-i</em></li>
<li><em>θ<sub>i</sub> (i = 1,2,..., q)</em> are MA model parameters</li>
<li><em>ε<sub>t-i</sub></em> is the past random error at time period <em>t-i</em></li>
<li><em>ε<sub>t</sub></em> is the random error at time period <em>t</em> (white noise)</li>
</ul>
<p>In general, once the order <em>(p, q)</em> has been chosen, the parameters of an ARMA model <em>(p, q)</em> can be estimated through the maximum likelihood estimator, for example. As for the AR model, the choice of the model order must respond to the opposing needs of a good adaptation to the data and parsimony in the number of parameters to be estimated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoregressive integrated moving average models</h1>
                </header>
            
            <article>
                
<p>An ARIMA model is a generalization of a ARMA model. ARIMA models are applied in cases where data show a clear tendency to non-stationarity. In these cases, to eliminate the non-stationarity, an initial differentiation step is added to the ARMA algorithm (corresponding to the integrated part of the model) that is applied one or more times.</p>
<p>This algorithm is therefore essentially composed of three parts:</p>
<ul>
<li>The part AR that determines a regression on its own delayed (that is, previous) values ​​to the evolving variable of interest.</li>
<li>The MA part. It indicates that the regression error is actually a linear combination of error terms whose values ​​have occurred simultaneously and at various times in the past.</li>
<li>The integrated part; it indicates that the data values ​​have been replaced with the difference between their current values ​​and the previous values ​​(and this differentiation process may have been performed more than once).</li>
</ul>
<p>The purpose of each of these features is to make the model suitable for data in the best possible way.</p>
<p>To formulate the representative equation of the ARIMA model we start from the ARMA model equation:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/fdfb1f75-0c54-4ca2-9945-90f23595341a.png" style="width:26.75em;height:4.58em;"/></div>
<p class="NormalPACKT"><span>Simply move the AR part to the right side of equation to obtain the following equation (less than the constant <em>c</em>):</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/a889d8fd-b97c-47e2-b994-81c1815c0107.png" style="width:24.42em;height:5.00em;"/></div>
<p class="NormalPACKT"><span>By introducing the lag operator (<em>L</em>), we can rewrite this equation as follows:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e458fe3c-64d7-4820-94e3-a8b40bdebfc0.png" style="width:23.33em;height:5.00em;"/></div>
<div class="packt_tip">
<p>Remember: The lag operator (<em>L</em>) operates on an element of a time series to produce the previous element, with the meaning that <em>LY<sub>t</sub> = Y<sub>t-1</sub></em>.</p>
</div>
<p>Assuming that:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/67e700d3-edea-4f37-8641-01f05a20fb3a.png" style="width:26.58em;height:5.00em;"/></div>
<p class="NormalPACKT"><span>Which expresses precisely the factoring procedure of order <em>d</em> previously carried out to eliminate the non-stationarity. Based on this assumption and setting <em>p = p'-d</em>, we can write the following equation to represent the mathematical formulation of the ARIMA <em>(p,d,q)</em> model using lag polynomials:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/d7a23d6b-bc3e-45c2-b110-a3c4ae0931c7.png" style="width:29.00em;height:4.58em;"/></div>
<p>The <em>d</em> parameter controls the level of <span>differentiating</span>. Generally <em>d=1</em> is enough in most cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing seasonality from a time series</h1>
                </header>
            
            <article>
                
<p> In economic and financial analyses, which are commonly carried out on the basis of numerous indicators, the use of data presented in a seasonally adjusted form (that is, net of seasonal fluctuations), is widely used in order to be able to grasp more clearly the<span> </span><span>short-term</span><span> evolution of the phenomena considered.</span></p>
<p>Seasonality, in the dynamics of a time series, is the component that repeats itself at regular intervals every year, with variations of intensity more or less similar in the same period (month, quarter, semester, and so on) of successive years; there is different intensity during the same year. Typical examples of this are a decrease in industrial production in August following holiday closures of many companies, and increase in retail sales in December due to the holiday season.</p>
<p>Seasonal fluctuations, disguising other movements of interest (typically cyclical fluctuations), are often considered to be a nuisance in the analysis of economic conjuncture. The presence of seasonality creates, for example, problems in analysis and interpretation of the variations observed in a historical series between two consecutive periods (months and quarters) of the year—so-called <strong>economic variation</strong>. These are often influenced to a prevalent extent by seasonal fluctuations rather than movements due to other causes (for example, the economic cycle). The latter, on the other hand, can be correctly highlighted by calculating economic variations on seasonally adjusted data. Furthermore, since each time series is characterized by a specific seasonal profile, the use of seasonally adjusted data makes it possible to compare the evolution of different time series, and it is widely applied in joint use of statistics produced by different countries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing a time series dataset</h1>
                </header>
            
            <article>
                
<p>To see how to perform a seasonality removal operation on a time series, we will use a dataset on monthly milk production (pounds per cow; January 1962 – December 1975). Here is some useful information about this dataset:</p>
<ul>
<li><strong>Units</strong>: Pounds per cow</li>
<li><strong>Dataset metrics</strong>: 168 fact values in one time series</li>
<li><strong>Time granularity</strong>: Month</li>
<li><strong>Time range</strong>: January 1962 – December 1975</li>
<li><strong>Source</strong>: Time Series Data Library</li>
</ul>
<div>
<p>The <strong>Time Series Data Library</strong> (<strong>TSDL</strong>) was created by Rob Hyndman, a professor of statistics at Monash University, Australia.</p>
</div>
<p>The data is available in a <kbd>.csv</kbd> file named <kbd>milk-production-pounds.csv</kbd>. To start, let's see how to import the data into Python and then how to display it to identify the possible presence of seasonality. The first thing to do is to import the library that we will use:</p>
<pre>import pandas as pd<br/>import matplotlib.pyplot as plt</pre>
<p>With the first line, we imported the <kbd>pandas</kbd> library, and with the second line, we imported the <kbd>pyplot</kbd> module from the <kbd>matplotlib</kbd> library.</p>
<div class="packt_tip"><kbd>pandas</kbd> is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. In particular, it offers data structures and operations for manipulating numerical tables and time series.</div>
<p>Matplotlib is a Python 2D plotting library that produces publication-quality figures in a variety of hard copy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, Jupyter Notebook, web application servers, and four graphical user interface toolkits. The <kbd>matplotlib.pyplot</kbd> module contains functions that allow you to generate many kinds of plots quickly. Now let's see how to import the data contained in the dataset in Python:</p>
<pre>data = pd.read_csv('milk-production-pounds.csv',<br/>               parse_dates=True,index_col='DateTime',<br/>               names=['DateTime', 'Milk'], header=None)</pre>
<p>To import a dataset, we used the <kbd>read_csv</kbd> module of the <kbd>pandas</kbd> library. The <kbd>read_csv</kbd> method loads the data in a Pandas DataFrame we named <kbd>data</kbd>. To display the first five rows of the DataFrame imported <span>on video, </span>we can use the <kbd>head()</kbd> function as follows:</p>
<pre>print(data.head())</pre>
<p>The following results are returned:</p>
<pre><strong>DateTime     Milk</strong><br/><strong>1962-01-01   589</strong><br/><strong>1962-02-01   561</strong><br/><strong>1962-03-01   640</strong><br/><strong>1962-04-01   656</strong><br/><strong>1962-05-01   727</strong></pre>
<p>The <kbd>head()</kbd> function, with no arguments, gets the first five rows of data from the DataFrame. Now the time series is available in our Python environment; to get a preview of the data contained in it, we can calculate a series of basic statistics. To do so, we will use the <kbd>describe()</kbd> function in the following way:</p>
<pre>print(data.describe())</pre>
<p>The following results are returned:</p>
<pre><strong>            Milk</strong><br/><strong>count  168.000000</strong><br/><strong>mean   754.708333</strong><br/><strong>std    102.204524</strong><br/><strong>min    553.000000</strong><br/><strong>25%    677.750000</strong><br/><strong>50%    761.000000</strong><br/><strong>75%    824.500000</strong><br/><strong>max    969.000000</strong></pre>
<p>The <kbd>describe()</kbd> function generates descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset's distribution, excluding NaN values. It analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. To extract further information, we can invoke the function <kbd>info()</kbd> as follows:</p>
<pre>print(data.info())</pre>
<p>The following results are returned:</p>
<pre><strong>&lt;class 'pandas.core.frame.DataFrame'&gt;</strong><br/><strong>DatetimeIndex: 168 entries, 1962-01-01 to 1975-12-01</strong><br/><strong>Data columns (total 1 columns):</strong><br/><strong>Milk    168 non-null int64</strong><br/><strong>dtypes: int64(1)</strong><br/><strong>memory usage: 2.6 KB</strong><br/><strong>None</strong></pre>
<p>After having taken a look at the content of the dataset, we are going to perform an initial visual exploratory analysis. There's a relatively extensive plotting functionality built into Pandas that can be used for exploratory charts—especially useful in data analysis. A huge amount of functionality is provided by the <kbd>.plot()</kbd> command natively by Pandas:</p>
<pre>data.plot()<br/>plt.show()</pre>
<p>The <kbd>data.plot()</kbd> command makes plots of the DataFrame using <kbd>matplotlib</kbd>/<kbd>pylab</kbd>. To display the graph just created <span>on video</span>, we have to use the <kbd>plt.show()</kbd> function, as shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-741 image-border" src="assets/92e6df22-1259-4c7e-bbd4-83510b84d329.png" style=""/></div>
<div>
<p>From the analysis of the previous figure, we can certainly recognize that milk production is growing (we note a positive trend) but denoting a certain variability (oscillations around a hypothetical trend line). This is maintained almost constantly with the passage of time.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying a trend in a time series</h1>
                </header>
            
            <article>
                
<p>If we want to try a prediction of milk production in the next January, we can think in the following way: with the acquired data, we can trace the trend line and extend it until the following January. In this way, we would have a rough estimate of milk production that we should expect in the immediate future.</p>
<p>But tracing a trend line means tracing the regression line. The linear regression method consists of precisely identifying a line that is capable of representing point distribution in a two-dimensional plane. As is easy to imagine, if the points corresponding to the observations are near the line, then the chosen model will be able to effectively describe the link between the variables. In theory, there are an infinite number of lines that may approximate the observations. In practice, there is only one mathematical model that optimizes the representation of the data.</p>
<p>To fit a linear regression model, we start importing two more libraries:</p>
<pre>import numpy<br/>from sklearn.linear_model import LinearRegression</pre>
<p>NumPy is the fundamental package for scientific computing with Python. It contains, among other things:</p>
<ul>
<li>A powerful N-dimensional array object</li>
<li>Sophisticated (broadcasting) functions</li>
<li>Tools for integrating C/C++ and FORTRAN code</li>
<li>Useful linear algebra, Fourier transform, and random number capabilities</li>
</ul>
<p>Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary datatypes can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.</p>
<p>sklearn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.</p>
<div>
<div class="packt_tip">Remember, to import a library that is not present in the initial distribution of Python, you must use the <kbd>pip</kbd> install command followed by the name of the library. This command should be used only once and not every time you run the code.</div>
</div>
<p>We begin to prepare the data:</p>
<pre>X = [i for i in range(0, len(data))]<br/>X = numpy.reshape(X, (len(X), 1))<br/>y = data.values</pre>
<p>First, we counted the data; then we used the <kbd>reshape()</kbd> function to give a new shape to an array without changing its data. Finally, we inserted the time series values into the <kbd>y</kbd> variable. Now we can build the linear regression model:</p>
<pre>LModel = LinearRegression()</pre>
<p>The <kbd>LinearRegression()</kbd> function performs a ordinary least squares linear regression. The ordinary least squares method is an optimization technique (or regression) that allows us to find a function, represented by an optimal curve (or regression curve) that is as close as possible to a set of data. In particular, the function found must be one that minimizes the sum of squares of distances between the observed data and those of the curve that represents the function itself.</p>
<p>Given n points (<em>x<sub>1</sub></em>, <em>y<sub>1</sub></em>), (<em>x<sub>2</sub></em>, <em>y<sub>2</sub></em>), ... (<em>x<sub>n</sub></em>, <em>y<sub>n</sub></em>) in the observed population, a least squares regression line is defined as the equation line:</p>
<p class="CDPAlignCenter CDPAlign"><em>y=α*x+β</em></p>
<p class="NormalPACKT"><span>For which the following quantity is minimal:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/030fe6e9-bb43-4d99-9110-c10d99248be8.png" style="width:12.58em;height:3.58em;"/></div>
<p>This quantity represents the sum of squares of distances of each experimental datum (<em>x<sub>i</sub>, y<sub>i</sub></em>) from the corresponding point on the straight line <em>(<span>x</span><sub>i</sub><span>, α</span>x<sub>i</sub><span>+</span><span>β)</span></em>, as shown in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c3f1540d-3f16-4fd4-b574-131621ec52d1.png" style=""/></div>
<p>Now, we have to apply the <kbd>fit</kbd> method to fit the linear model:</p>
<pre>LModel.fit(X, y)</pre>
<p>A linear regression model basically finds the best value for the intercept and slope, which results in a line that best fits the data. To see the value of the intercept and slope calculated by the linear regression algorithm for our dataset, execute the following code:</p>
<pre>print(LModel.intercept_,LModel.coef_)</pre>
<p>The following results are returned:</p>
<pre>[613.37496478] [[1.69261519]]</pre>
<p>The first is the intercept; the second is the coefficient of the regression line. Now that we have trained our algorithm, it's time to make some predictions. To do so, we will use the whole data and see how accurately our algorithm predicts the percentage score. Remember, our scope is to locate the time series trend. To make predictions on the whole data, execute the following code:</p>
<pre>trend = LModel.predict(X)</pre>
<p>It is time to visualize what we have achieved:</p>
<pre>plt.plot(y)<br/>plt.plot(trend)<br/>plt.show()</pre>
<p>With this code, we first traced the time series. So we added the regression line that represents the data trend, and finally we printed the whole thing, as shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-738 image-border" src="assets/56c969c3-7c64-45f7-b228-360787f01f08.png" style=""/></div>
<div>
<p>We recall that this represents a long-term monotonous trend movement, which highlights a structural evolution of the phenomenon due to causes that act in a systematic manner on the same. From the analysis of the previous figure, it is possible to note this: making a estimation of milk production in a precise period based on the line that indicates the trend of the time series can, in some cases, <span>be </span>disastrous. This is due to the fact that the seasonal highs and lows are at important distances from the line of regression. It is clear that it is not possible to use this line to make estimates of milk production.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time series decomposition</h1>
                </header>
            
            <article>
                
<p>One of the fundamental purposes of the classical analysis of time series is to break down the series into its components, isolating them in order to study them better. Moreover, to be able to apply the stochastic approach to a time series, it is almost always necessary to eliminate the trend and the seasonality to have a steady process. As we have specified in the previous sections, the components of a time series are usually the following: trend, seasonality, cycle, and residual.</p>
<p>As already mentioned, they can be decomposed by an additive way:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y(t) = τ(t) + S(t) + r(t)</em></p>
<p>They can also be decomposed by a multiplicative method:</p>
<p class="CDPAlignCenter CDPAlign"><em>Y(t) = τ(t) * S(t) * r(t)</em></p>
<p>In the following sections, we will look at how to derive these components using both these methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Additive method</h1>
                </header>
            
            <article>
                
<p>To perform a time series decomposition, we can use automated procedures. The <kbd>stats</kbd> models library provides an implementation of the naive, or classical, decomposition method in a function called <kbd>seasonal_decompose()</kbd>. Additive or multiplicative methods are available.</p>
<p>We start importing the <kbd>stats</kbd> models library:</p>
<pre>from statsmodels.tsa.seasonal import seasonal_decompose</pre>
<p>In particular, we imported the <kbd>seasonal_decompose</kbd> module to perform seasonal decomposition using MAs. We perform the decomposition by applying the additive method:</p>
<pre>DecompDataAdd = seasonal_decompose(data, model='additive', freq=1)</pre>
<p>The seasonal component is first removed by applying a convolution filter to the data. The average of this smoothed series for each period is the returned seasonal component. Let's see what happened through the visualization of the components identified:</p>
<pre>DecompDataAdd.plot()<br/>plt.show()</pre>
<p>The following graph shows the decomposition results by additive method:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8cc10757-6cf6-406d-befa-8600a842f876.png" style=""/></div>
<p>In this figure, the three components of the time series are clearly represented: trend, seasonal, and residual. These attributes are contained in the object returned by the method <kbd>seasonal_decompose()</kbd>. This means that we can use the content of that object to remove the effect of seasonality from the time series. Let's see how:</p>
<pre>SeasRemov= data-DecompDataAdd.seasonal</pre>
<p>With this line of code, we have simplified the seasonal attribute returned by the <kbd>seasonal_decompose()</kbd> method from the data. At this point, we just have to visualize the result:</p>
<pre>SeasRemov.plot()<br/>plt.show()</pre>
<p>The following graph shows the monthly milk production (pounds per cow from January 1962 – December 1975) net of seasonality:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-736 image-border" src="assets/60c6dbf7-ad1c-45e7-a3ea-1c240e5f8ac1.png" style=""/></div>
<p class="NormalPACKT"><span>In the graph obtained, the component due to seasonality has clearly been removed, while the one due to the trend is clearly visible.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiplicative method</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT"><span>As we said, the <kbd>seasonal_decompose()</kbd> performs both additive and multiplicative decomposition. To run multiplicative method, just type the following command:</span></p>
<pre class="CodeEndPACKT"><span>DecompDataMult = seasonal_decompose(data, model='multiplicative')</span></pre>
<p class="NormalPACKT"><span>At this point, we just have to visualize the result:</span></p>
<pre class="CodePACKT"><span>DecompDataMult.plot()<br/></span><span>plt.show()</span></pre>
<p class="NormalPACKT"><span>The following graph shows the decomposition results by multiplicative method:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb7bd241-4501-42ec-bfd0-50badb4e87a2.png"/></div>
<div>
<p>In the previous figure, we can note that the trend and seasonality information extracted from the time series do seem reasonable. The residuals show an interesting variation; periods of high variability are clearly identified in the early and later years of the time series.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM for time series analysis</h1>
                </header>
            
            <article>
                
<div>
<p>LSTM is a particular architecture of recurrent neural network, originally conceived by Hochreiter and Schmidhuber in 1997. This type of neural network has been recently rediscovered in the context of deep learning because it is free from the problem of vanishing gradient, and in practice it offers excellent results and performance.</p>
<p>LSTM-based networks are ideal for prediction and classification of time series, and are supplanting many classic machine learning approaches. This is due to the fact that LSTM networks are able to consider long-term dependencies between data, and in the case of speech recognition, this means managing the context within a sentence to improve recognition capacity.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the time series dataset</h1>
                </header>
            
            <article>
                
<p>Scientists from the US <strong>National Oceanic and Atmospheric Administration</strong> <span>(<strong>NOAA</strong>)</span> have measured atmospheric carbon dioxide from 1965 to 1980 near the top of the Mauna Loa volcanic cone (Hawaii). The dataset covers carbon dioxide concentrations of 317.25 to 341.19 <strong>parts per million</strong> <span>(</span><strong>ppm</strong><span>) </span>by volume and contains 192 monthly records. Here is some useful information about this dataset:</p>
<ul>
<li><strong>Units</strong>: ppm</li>
<li><strong>Dataset metrics</strong>: 192 fact values in one time series</li>
<li><strong>Time granularity</strong>: Month</li>
<li><strong>Time range</strong>: January 1965-December 1980</li>
</ul>
<div>
<p>Source: TSDL, created by Rob Hyndman, a professor of statistics at Monash University, Australia.</p>
</div>
<p>Data is available in the <kbd>.csv</kbd> file named <kbd>co2-ppm-mauna-loa-19651980.csv</kbd>. To start, let's see how to import data into Python and then how to display them to identify the possible presence of seasonality. The first thing to do is to import the library that we will use:</p>
<pre>import pandas as pd<br/>import matplotlib.pyplot as plt</pre>
<p>With the first line, we imported <kbd>pandas</kbd> and with second line we imported the <kbd>pyplot</kbd> module from the <kbd>matplotlib</kbd> library. Now let's see how to import the data contained in the dataset in Python:</p>
<pre>dataset = pd.read_csv(' co2-ppm-mauna-loa-19651980.csv',<br/>               parse_dates=True,index_col='DateTime',<br/>               names=['DateTime', 'CO2'], header=None)</pre>
<p>To import a dataset, we used the <kbd>read_csv</kbd> module of the <kbd>pandas</kbd> library. The <kbd>read_csv</kbd> method loads the data in a Pandas DataFrame we named dataset. To display on video the first five rows of DataFrame imported, we can use the <kbd>head()</kbd> function as follows:</p>
<pre>print(dataset.head())</pre>
<p>The following results are returned:           </p>
<pre><strong>DateTime    CO2</strong><br/><strong>1965-01-01  319.32</strong><br/><strong>1965-02-01  320.36</strong><br/><strong>1965-03-01  320.82</strong><br/><strong>1965-04-01  322.06</strong><br/><strong>1965-05-01  322.17</strong></pre>
<p>The <kbd>head()</kbd> function, with no arguments, gets the first five rows of data from the DataFrame. Now the time series is now available in our Python environment; to get a preview of the data contained in it, we can calculate a series of basic statistics. To do so, we will use the <kbd>describe()</kbd> function in the following way:</p>
<pre>print(dataset.describe())</pre>
<p>The following results are returned:</p>
<pre><strong>             CO2</strong><br/><strong>count  192.000000</strong><br/><strong>mean   328.463958</strong><br/><strong>std      5.962682</strong><br/><strong>min    317.250000</strong><br/><strong>25%    323.397500</strong><br/><strong>50%    328.295000</strong><br/><strong>75%    333.095000</strong><br/><strong>max    341.190000</strong></pre>
<p>The <kbd>describe()</kbd> function generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. It analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. To extract further information, we can invoke the function <kbd>info()</kbd>:</p>
<pre>print(data.info())</pre>
<p>The following results are returned:</p>
<pre><strong>&lt;class 'pandas.core.frame.DataFrame'&gt;</strong><br/><strong>DatetimeIndex: 192 entries, 1965-01-01 to 1980-12-01</strong><br/><strong>Data columns (total 1 columns):</strong><br/><strong>CO2    192 non-null float64</strong><br/><strong>dtypes: float64(1)</strong><br/><strong>memory usage: 3.0 KB</strong></pre>
<p>After having taken a look at the content of the dataset, we are going to perform an initial visual exploratory analysis. There's a relatively extensive plotting functionality built into Pandas that can be used for exploratory charts; this is especially useful in data analysis. A huge amount of functionality is provided by the <kbd>.plot()</kbd> command natively by Pandas:</p>
<pre>dataset.plot()<br/>plt.show()</pre>
<p>The <kbd>dataset.plot()</kbd> command make plots of the DataFrame using <kbd>matplotlib</kbd>/<kbd>pylab</kbd>. To display the graph just created <span>on video,</span><span> </span><span>we have to use the</span> <kbd>plt.show()</kbd> <span>function, as shown in the following graph:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-740 image-border" src="assets/c0e05687-3b9f-41fe-91da-f4d54da69b3f.png" style=""/></div>
<div>
<p>From the analysis of the previous figure, we can certainly recognize that atmospheric carbon dioxide is growing. We note a positive trend. But it is also denoting a certain variability (oscillations around a hypothetical trend line), which is maintained almost constantly with the passage of time.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data scaling</h1>
                </header>
            
            <article>
                
<p><strong>Data scaling</strong> is a preprocessing technique usually employed before feature selection and classification. Many artificial intelligence-based systems use features that are generated by many different feature extraction algorithms, with different kinds of sources. These features may have different dynamic ranges.</p>
<p>In addition, in several data mining applications with huge numbers of features with large dynamic ranges, feature scaling may improve the performance of the fitting model. However, the appropriate choice of these techniques is an important issue, since applying scaling on the input could change the structure of data and thereby affect the outcome of multivariate analysis used in data mining.</p>
<p>To scaling the data we will use the min-max normalization (usually called <strong>feature scaling</strong>); it performs a linear transformation on the original data. This technique gets all the scaled data in the range (0,1). The formula to achieve this is:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/7acb1819-3d3e-494d-844b-c4cd6ff64476.png" style="width:14.75em;height:3.50em;"/></div>
<p>Min-max normalization preserves the relationships among the original data values. The cost of having this bounded range is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p>
<p>To perform min-max normalization, we will use the <kbd>MinMaxScaler()</kbd> module of the <kbd>sklearn.preprocessing</kbd> class. This module transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, that is, between zero and one. The following codes show how to apply this module to our data:</p>
<pre>scaler = MinMaxScaler()<br/>dataset = scaler.fit_transform(dataset)</pre>
<p>First we have used the <kbd>MinMaxScaler()</kbd> function to set the normalization interval (by default (0, 1)). In the second line of the code, we applied the <kbd>fit_transform()</kbd> function; it fits the transformer to the dataset and returns a transformed version of the data. This function is particularly useful as it stores the transformation parameters used. These parameters will be useful when, after having made the forecasts, we will have to report the data in the initial form (before normalization) to compare it with actual data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data splitting</h1>
                </header>
            
            <article>
                
<p><span>Let's now split the data for the training and the test model. Training and testing the model forms the basis for further usage of the model for prediction in predictive analytics. Given a dataset of 192 rows of data, we split it into a convenient ratio (say 70:30), and allocate 134 rows for training and 58 rows for testing.</span></p>
<div>
<p>In general, in the algorithms based on artificial neural networks, the splitting is done by selecting rows randomly to reduce the bias. With the time series data, the sequence of values is important, so this procedure is not practicable. A simple method that we can use is to divide the ordered dataset into train and test. As we anticipated, the following code calculates the division point index and separates the data in the training datasets, with 70% of the observations for us to use to train our model; this leaves the remaining 30% to test the model:</p>
<pre>train_len = int(len(dataset) * 0.70)<br/>test_len  = len(dataset) - train_len<br/>train = dataset[0:train_len,:]<br/>test  = dataset[train_len:len(dataset),:]</pre>
<p>The first two lines of code set the length of the two groups of data. The next two lines split the dataset into two parts: from row 1 to <span>row </span><kbd>train_len -1</kbd> for the train set, and from the <kbd>train_len</kbd> row to the last row for the test set. To confirm the correct split of data, we can print the length of the two datasets:</p>
<pre>print(len(train), len(test))</pre>
<p>This gives the following results:</p>
<pre><strong>134 58</strong></pre>
<p>As we anticipated, the operation divided the dataset into <kbd>134</kbd> (train set) and <kbd>58</kbd> rows (test set).</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p><span>Our aim is to use data in the dataset to make predictions. In particular, we want to predict the presence of carbon dioxide in the air based on the data available in the <kbd>.csv</kbd> file. We need input and output to train and test our network. It is clear that the input is represented by the data present in the dataset. We must then construct our output; we will do so by supposing we want to predict the CO2 present in the atmosphere at time <em>t + 1</em> with respect to the value measured at time <em>t</em>. So we will have:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>Input = data(t)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>Output = data(t + 1)</em></p>
<p>We have said that a recurrent network has memory, and it is maintained by fixing the so-called <strong>time step</strong>. The time step has to do with how many steps back in time backprop uses when calculating gradients for weight updates during training. In this way, we set <em>time step = 1</em>. Then we define a function that gives a dataset and a time step returns the input and output data:</p>
<pre>def dataset_creating(dataset):<br/>   Xdata, Ydata = [], []<br/>   for i in range(len(dataset)-1):<br/>         Xdata.append(dataset[i, 0])<br/>         Ydata.append(dataset[i + 1, 0])<br/>   return numpy.array(Xdata), numpy.array(Ydata)</pre>
<p>In this function, <kbd>Xdata=Input= data(t)</kbd> is the input variable and <kbd>Ydata=output= data(t + 1)</kbd> is the predicted value at the next time period. Let's use this function to set the train and test datasets that we will use in the next phase (network modeling):</p>
<pre>trainX, trainY = create_dataset(train)<br/>testX, testY = create_dataset(test)</pre>
<p>In this way, we created all the data needed for the network training and testing. This function converts an array of values into a dataset matrix. Now we have to prepare the two input datasets (<kbd>trainX</kbd> and <kbd>testX</kbd>) in the form required by the machine learning algorithm we intend to use (LSTM). To do this, it is necessary to deepen this concept.</p>
<p>In a classic feed-forward network, like those already analyzed in the previous chapters, the input contains the values assumed by the variables for each observation made. This means that the input takes the following shape:</p>
<p class="CDPAlignCenter CDPAlign"><em>(number of observations, number of features)</em></p>
<p>In an LSTM/RNN network, the input for each LSTM layer must contain the following information:</p>
<ul>
<li><strong>Observations</strong>: Number of observations collected</li>
<li><strong>Time steps</strong>: A time step is an observation point in the sample</li>
<li><strong>Features</strong>: One feature for each step</li>
</ul>
<p>Therefore it is necessary to add a temporal dimension to those foreseen for a classical network. Thus the input shape becomes:</p>
<p class="CDPAlignCenter CDPAlign"><em>(number of observations, number of time steps, number of features per steps)</em></p>
<p>In this way, the input for each LSTM layer becomes three-dimensional. To transform the input datasets in 3D form, we will use the <kbd>numpy.reshape()</kbd> function as follows:</p>
<pre>trainX = numpy.reshape(trainX, (trainX.shape[0], 1, 1))<br/>testX = numpy.reshape(testX, (testX.shape[0], 1, 1))</pre>
<p>The <kbd>numpy.reshape()</kbd> function gives a new shape to an array without changing its data. The function parameters used are:</p>
<ul>
<li><kbd> trainX</kbd>, <kbd>testX</kbd>: Array to be reshaped</li>
<li><kbd>(trainX.shape[0], 1, 1)</kbd>, <kbd>(testX.shape[0], 1, 1)</kbd>: New shape</li>
</ul>
<p>The new shape should be compatible with the original shape. In our case, the new shape is (133,1,1) for <kbd>trainX</kbd> and (57,1,1) for <kbd>testX</kbd>. Now that the data is in the right format, it's time to create the model:</p>
<pre>timesteps = 1<br/>model = Sequential()</pre>
<p>We start defining the time steps; then we use a sequential model, that is, a linear stack of layers. To create a sequential model, we have to pass a list of layer instances to the constructor. We can also simply add layers via the <kbd>.add()</kbd> method:</p>
<pre>model.add(LSTM(4, input_shape=(1, timesteps)))<br/>model.add(Dense(1))</pre>
<p>The first layer is an LSTM layer, with a hidden layer with four LSTM blocks. The model needs to know what input shape it should expect. For this reason, we passed an <kbd>input_shape</kbd> argument to this layer. In the next line, we added a dense layer that implements the default sigmoid activation function. Now, we have to configure the model for training:</p>
<pre>model.compile(loss='mean_squared_error', optimizer='adam')</pre>
<p>To do this, we used the compile module. The arguments passed are a loss function as <kbd>mean_squared_error</kbd> and stochastic gradient descent as <kbd>optimizer</kbd>. Finally, we can fit the model:</p>
<pre>model.fit(trainX, trainY, epochs=1000, batch_size=1, verbose=2)</pre>
<p>In the training phase, the <kbd>trainX</kbd> and <kbd>trainY</kbd> data is used, with 1,000 epochs (full training cycle on the training set). A batch size of 1 (batch_size = number of samples per gradient update) is passed. Fynally <kbd>verbose=2</kbd> (verbose argument provides additional details as to what the computer is doing) prints the loss value for each epoch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions</h1>
                </header>
            
            <article>
                
<p>Our model is now ready for use. We can therefore use it to execute our predictions:</p>
<pre>trainPred = model.predict(trainX)<br/>testPred = model.predict(testX)</pre>
<p>The <kbd>predict()</kbd> module has been used, which generates output predictions for the input samples. Computation is done in batches. A Numpy array of predictions is returned. Previously, when data scaling was performed, we used the <kbd>fit_transform()</kbd> function. As we said, this function is particularly useful as it stores the transformation parameters used. These parameters will be useful when, after having made the forecasts, we will have to report the data in the initial form (before normalization), to compare it to the actual data. In fact, now the predictions must be reported in original form to compare with the actual values:</p>
<pre>trainPred = scaler.inverse_transform(trainPred)<br/>trainY = scaler.inverse_transform([trainY])<br/>testPred = scaler.inverse_transform(testPred)<br/>testY = scaler.inverse_transform([testY])</pre>
<p>This code block is used exclusively to cancel the effect of normalization and to restore the initial form to the dataset. To estimate the performance of the algorithm, we will calculate the root mean squared error:</p>
<pre>trainScore = math.sqrt(mean_squared_error(trainY[0], trainPred[:,0]))<br/>print('Train Score: %.2f RMSE' % (trainScore))<br/>testScore = math.sqrt(mean_squared_error(testY[0], testPred[:,0]))<br/>print('Test Score: %.2f RMSE' % (testScore))</pre>
<div class="packt_tip"><strong>Root mean square error</strong> (<strong>RMSE</strong>) measures how much error there is between two datasets. In other words, it compares a predicted value and an observed value.</div>
<p>The following results are returned:</p>
<pre><strong>Train Score: 1.12 RMSE</strong><br/><strong>Test Score: 1.35 RMSE</strong></pre>
<p>After evaluating the method's performance, we can now visualize the results by drawing an appropriate graph. To display the time series correctly, a prediction shift is required. This operation must be carried out both on the train set and on the test set:</p>
<pre>trainPredPlot = numpy.empty_like(dataset)<br/>trainPredPlot[:,:] = numpy.nan<br/>trainPredPlot[1:len(trainPred)+1,:] = trainPred</pre>
<p>Then perform the same operation on the test set:</p>
<pre>testPredPlot = numpy.empty_like(dataset)<br/>testPredPlot[:,:] = numpy.nan<br/>testPredPlot[len(trainPred)+2:len(dataset),:] = testPred</pre>
<p>Finally, we have to plot the actual data and the predictions:</p>
<pre>plt.plot(scaler.inverse_transform(dataset))<br/>plt.plot(trainPredPlot)<br/>plt.plot(testPredPlot)<br/>plt.show()</pre>
<p>In the following graph are shown the actual data and the predictions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-737 image-border" src="assets/74b02eb1-05a0-43a4-9d19-74b49c5a736a.png" style=""/></div>
<div>
<p>From the analysis of the previous graph, we can see that what is reported by the RMSE is confirmed by the graph. In fact, we can see that the model has done an excellent job in the fitting of both the training and test datasets.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<div>
<p>In this chapter, we explored time series data. A time series constitutes a sequence of observations on a phenomenon. In a time series, we can identify several components: trend, seasonality, cycle, and residual. We learned how to remove seasonality from a time series with a practical example.</p>
<p>Then the most used models to represent time series were addressed: AR, MA, ARMA, and ARIMA. For each one, the basic concepts were analyzed and then a mathematical formulation of the model was provided.</p>
<p>Finally, an LSTM model for time series analysis was proposed. Using a practical example, we could see how to deal with a time series regression problem with a recurrent neural network model of the LSTM type.</p>
</div>


            </article>

            
        </section>
    </body></html>