<html><head></head><body>
<div id="_idContainer078">
<h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-142"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.2.1">Natural Language Processing Using MATLAB</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Natural language processing</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">NLP</span></strong><span class="koboSpan" id="kobo.6.1">) automatically </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.7.1">processes information conveyed through spoken or written language. </span><span class="koboSpan" id="kobo.7.2">This task is fraught with difficulty and complexity, largely due to the innate ambiguity of human language. </span><span class="koboSpan" id="kobo.7.3">To enable </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">machine learning</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">ML</span></strong><span class="koboSpan" id="kobo.11.1">) and interaction</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.12.1"> with the world in ways typical of humans, it is essential not only to store data but also to teach machines how to translate this data simultaneously into meaningful concepts. </span><span class="koboSpan" id="kobo.12.2">As natural language interacts with the environment, it generates predictive knowledge. </span><span class="koboSpan" id="kobo.12.3">In this chapter, we will learn the basic concepts of NLP and how to build a model to </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">label sentences.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">main topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Explaining NLP</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Exploring corpora and word and </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">sentence tokenize</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Implementing a MATLAB model to </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">label sentences</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Understanding gradient </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">boosting techniques</span></span></li>
</ul>
<h1 id="_idParaDest-143"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">In this chapter, we will introduce basic ML concepts. </span><span class="koboSpan" id="kobo.24.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.24.3">You will also require working knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">MATLAB environment.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">To work with the MATLAB code in this chapter, you’ll need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.29.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">IMDBSentimentClassification.m</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">ImdbDataset.xlsx</span></strong></span></li>
</ul>
<h1 id="_idParaDest-144"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.32.1">Explaining NLP</span></h1>
<p><span class="koboSpan" id="kobo.33.1">NLP is a </span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.34.1">field that’s dedicated to the development of technology that enables computers to interact with, understand, and generate human language in a way that mimics natural human communication. </span><span class="koboSpan" id="kobo.34.2">This involves various techniques and approaches aimed at processing and analyzing the complexities of natural languages, such as English, Chinese, Arabic, and more. </span><span class="koboSpan" id="kobo.34.3">The goal is to bridge the gap between human language and computer language, allowing computers to comprehend and generate text as if they were engaging in a conversation with a human interlocutor (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.35.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.36.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.38.1"><img alt="Figure 7.1 – NLP tasks" src="image/B21156_07_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.39.1">Figure 7.1 – NLP tasks</span></p>
<p><span class="koboSpan" id="kobo.40.1">NLP strives to develop information technology tools for analyzing, comprehending, and creating texts in a manner that resonates with human understanding, mimicking interactions with another human rather than a machine. </span><span class="koboSpan" id="kobo.40.2">Natural language, both spoken and written, represents the most instinctive and widespread mode of communication. </span><span class="koboSpan" id="kobo.40.3">In contrast to formal languages, it holds a greater level of intricacy, often carrying connotations and</span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.41.1"> uncertainties, which renders its processing </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">quite challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">NLP encompasses a range of tasks, including but not limited to </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Text understanding</span></strong><span class="koboSpan" id="kobo.46.1">: This </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.47.1">involves</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.48.1"> extracting meaning and information from text. </span><span class="koboSpan" id="kobo.48.2">It includes tasks such as sentiment analysis (determining the emotional tone of a text), named entity recognition (identifying names of people, places, organizations, and so on), and text classification (categorizing text into </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">predefined classes).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Language generation</span></strong><span class="koboSpan" id="kobo.51.1">: This</span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.52.1"> aspect focuses </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.53.1">on generating coherent and contextually appropriate human-like text. </span><span class="koboSpan" id="kobo.53.2">It includes tasks such as machine translation (translating text from one language to another), text summarization (creating concise summaries of longer texts), and dialogue generation (constructing natural-sounding </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">conversational responses).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Speech recognition</span></strong><span class="koboSpan" id="kobo.56.1">: NLP </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.57.1">also extends to the </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.58.1">realm of spoken language. </span><span class="koboSpan" id="kobo.58.2">Speech recognition technology converts spoken language into text, enabling applications such as voice assistants and </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">transcription services.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Language models</span></strong><span class="koboSpan" id="kobo.61.1">: Recent</span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.62.1"> advances in NLP</span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.63.1"> have led to the development of large language models such as GPT-3, which can generate remarkably human-like text based on the input it receives. </span><span class="koboSpan" id="kobo.63.2">These models are trained on massive amounts of text data and can be fine-tuned for various </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">NLP tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Chatbots and virtual assistants</span></strong><span class="koboSpan" id="kobo.66.1">: NLP </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.67.1">is</span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.68.1"> the </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.69.1">foundation of chatbots and virtual assistants, which can engage in text-based or voice-based conversations with users, providing information, assistance, </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">and responses.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.71.1">The field of </span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.72.1">NLP is interdisciplinary, drawing from computer science, linguistics, cognitive psychology, and more. </span><span class="koboSpan" id="kobo.72.2">It involves working with linguistic structures, statistical models, ML algorithms, and </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">deep learning</span></strong><span class="koboSpan" id="kobo.74.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.75.1">DL</span></strong><span class="koboSpan" id="kobo.76.1">) techniques</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.77.1"> to process, understand, and generate human language. </span><span class="koboSpan" id="kobo.77.2">While significant progress has been made, NLP still faces challenges, such as handling context, understanding nuances, and truly comprehending the intricacies of </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">human communication:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.79.1"><img alt="Figure 7.2 – Text analysis and text generation in ﻿NLP" src="image/B21156_07_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.80.1">Figure 7.2 – Text analysis and text generation in NLP</span></p>
<p><span class="koboSpan" id="kobo.81.1">The pursuits in this domain encompass two main objectives: text analysis and text generation. </span><span class="koboSpan" id="kobo.81.2">These</span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.82.1"> principles give rise to the subsequent disciplines, known </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.83.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">natural language analysis</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.86.1">NLA</span></strong><span class="koboSpan" id="kobo.87.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.88.1">natural language generation</span></strong><span class="koboSpan" id="kobo.89.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.90.1">NLG</span></strong><span class="koboSpan" id="kobo.91.1">). </span><span class="koboSpan" id="kobo.91.2">We’ll delve deep </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">into both.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.93.1">NLA</span></h2>
<p><span class="koboSpan" id="kobo.94.1">This field</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.95.1"> centers on enhancing machines’ grasp of natural language. </span><span class="koboSpan" id="kobo.95.2">It involves transforming a natural language text into a structured and unequivocal representation. </span><span class="koboSpan" id="kobo.95.3">NLA involves the process of enabling machines to comprehend and interpret human language. </span><span class="koboSpan" id="kobo.95.4">Its primary goal is to bridge the gap between the unstructured nature of natural language and the structured representation that computers can work with. </span><span class="koboSpan" id="kobo.95.5">NLA encompasses </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">several tasks:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Syntax analysis</span></strong><span class="koboSpan" id="kobo.98.1">: This</span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.99.1"> involves parsing the</span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.100.1"> grammatical structure of sentences to understand the relationships between words and their roles (subject, object, verb, and so on). </span><span class="koboSpan" id="kobo.100.2">It helps in identifying how words are organized to </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">convey meaning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.102.1">Semantic understanding</span></strong><span class="koboSpan" id="kobo.103.1">: NLA </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.104.1">seeks to </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.105.1">comprehend the meaning of words and phrases in context. </span><span class="koboSpan" id="kobo.105.2">It involves extracting the underlying concepts and intentions behind the words used in </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">a text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Named entity recognition</span></strong><span class="koboSpan" id="kobo.108.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.109.1">NER</span></strong><span class="koboSpan" id="kobo.110.1">): NER </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.111.1">is the </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.112.1">process of identifying and classifying named entities, such as the names of people, organizations, locations, dates, and more, within </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">a text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.114.1">Sentiment analysis</span></strong><span class="koboSpan" id="kobo.115.1">: Sentiment </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.116.1">analysis</span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.117.1"> aims to determine the emotional tone expressed in a piece of text, classifying it as positive, negative, neutral, or even more </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">nuanced emotions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.119.1">Coreference resolution</span></strong><span class="koboSpan" id="kobo.120.1">: Coreference</span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.121.1"> refers to cases </span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.122.1">where different words or phrases in a text refer to the same entity. </span><span class="koboSpan" id="kobo.122.2">Coreference resolution identifies and connects these references to provide a coherent understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">the text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.124.1">Text classification</span></strong><span class="koboSpan" id="kobo.125.1">: Text</span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.126.1"> classification</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.127.1"> involves categorizing text into predefined classes or categories based on its content. </span><span class="koboSpan" id="kobo.127.2">This is used for tasks such as spam detection, topic classification, and </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">content tagging.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Information extraction</span></strong><span class="koboSpan" id="kobo.130.1">: This </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.131.1">involves extracting specific information or structured data from unstructured </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.132.1">text. </span><span class="koboSpan" id="kobo.132.2">An example of this is extracting dates, events, relationships, or numerical data from </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">news articles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.134.1">Dependency parsing</span></strong><span class="koboSpan" id="kobo.135.1">: Dependency </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.136.1">parsing analyzes the grammatical relationships between words in a sentence, often</span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.137.1"> represented as a tree structure that shows how words depend on </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">each other.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Language modeling</span></strong><span class="koboSpan" id="kobo.140.1">: Language </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.141.1">models, often</span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.142.1"> trained using ML techniques, are used to predict the likelihood of words or phrases occurring based on the context of the surrounding text. </span><span class="koboSpan" id="kobo.142.2">These models play a crucial role in understanding the probabilities and patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">of language.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.144.1">Parsing ambiguity</span></strong><span class="koboSpan" id="kobo.145.1">: Languages</span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.146.1"> often contain</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.147.1"> ambiguous structures that can be interpreted in multiple ways. </span><span class="koboSpan" id="kobo.147.2">NLA aims to resolve such ambiguities to arrive at the intended meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">a sentence.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.149.1">NLA draws from linguistic theories, computational linguistics, and ML. </span><span class="koboSpan" id="kobo.149.2">It’s a complex field that continues to evolve, driven by advances in technology and the increasing demand for machines to understand and process human language in diverse applications, including search engines, chatbots, sentiment analysis tools, </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">and more.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.151.1">NLG</span></h2>
<p><span class="koboSpan" id="kobo.152.1">Here, the </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.153.1">emphasis is on enabling machines to construct sentences in natural language. </span><span class="koboSpan" id="kobo.153.2">This involves developing applications that can produce accurate sentences in a specific language. </span><span class="koboSpan" id="kobo.153.3">NLG is a pivotal aspect of NLP that revolves around the creation of human-like text by computers. </span><span class="koboSpan" id="kobo.153.4">The main objective of NLG is to enable machines to generate coherent, contextually appropriate, and linguistically accurate textual content, mimicking the way humans communicate. </span><span class="koboSpan" id="kobo.153.5">NLG encompasses various techniques and processes to transform structured data or information into readable and understandable language. </span><span class="koboSpan" id="kobo.153.6">Here are some key points </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">regarding NLG:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.155.1">Machine translation</span></strong><span class="koboSpan" id="kobo.156.1">: One of </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.157.1">the </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.158.1">earliest forms of NLG, machine translation involves translating text from one language into another while retaining its meaning and context. </span><span class="koboSpan" id="kobo.158.2">Modern machine translation systems often utilize neural network-based approaches for </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">improved accuracy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Text summarization</span></strong><span class="koboSpan" id="kobo.161.1">: NLG is</span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.162.1"> employed in</span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.163.1"> summarizing longer texts, such as articles or documents, by condensing the main ideas and relevant information into a concise and </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">coherent summary.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Dialogue generation</span></strong><span class="koboSpan" id="kobo.166.1">: NLG </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.167.1">powers </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.168.1">chatbots, virtual assistants, and other conversational agents. </span><span class="koboSpan" id="kobo.168.2">It involves constructing natural-sounding responses in conversations, considering the ongoing context and </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">user inputs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.170.1">Data-to-text generation</span></strong><span class="koboSpan" id="kobo.171.1">: NLG </span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.172.1">is used to </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.173.1">convert structured data, such as statistical figures or database entries, into human-readable narratives. </span><span class="koboSpan" id="kobo.173.2">An example of this is generating weather reports from </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">weather data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.175.1">Content creation</span></strong><span class="koboSpan" id="kobo.176.1">: NLG </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.177.1">systems can </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.178.1">create articles, reports, and other content based on predefined templates or prompts. </span><span class="koboSpan" id="kobo.178.2">This can be particularly useful for generating routine reports or </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">news updates.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.180.1">Personalized messaging</span></strong><span class="koboSpan" id="kobo.181.1">: NLG</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.182.1"> can be employed to generate personalized messages or recommendations</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.183.1"> tailored to individual users – for instance, crafting product recommendations based on a user’s </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">browsing history.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.185.1">Storytelling and creative writing</span></strong><span class="koboSpan" id="kobo.186.1">: Some</span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.187.1"> NLG systems can generate fictional stories, poetry, or creative</span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.188.1"> pieces of writing. </span><span class="koboSpan" id="kobo.188.2">While these might not be as advanced as </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.189.1">human-generated content, they showcase the potential of NLG in </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">creative domains.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.191.1">Automatic code generation</span></strong><span class="koboSpan" id="kobo.192.1">: In</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.193.1"> programming, NLG can be used to generate code snippets or </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.194.1">documentation based on high-level descriptions </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">or specifications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.196.1">Medical reports</span></strong><span class="koboSpan" id="kobo.197.1">: NLG </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.198.1">can assist in generating medical reports</span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.199.1"> or patient summaries based on electronic health records and </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">medical data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">NLG techniques can range from rule-based approaches to more advanced ML methods, such as </span><strong class="bold"><span class="koboSpan" id="kobo.202.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.203.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.204.1">RNNs</span></strong><span class="koboSpan" id="kobo.205.1">) and</span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.206.1"> transformer models such as </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">generative pre-trained transformer</span></strong><span class="koboSpan" id="kobo.208.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.209.1">GPT</span></strong><span class="koboSpan" id="kobo.210.1">). </span><span class="koboSpan" id="kobo.210.2">These models learn patterns and</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.211.1"> structures from vast amounts of text data to produce coherent and contextually </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">relevant language.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">NLG is integral in applications where conveying information in a human-readable format is essential, making technology more accessible and user-friendly. </span><span class="koboSpan" id="kobo.213.2">While NLG systems have made significant progress, challenges still exist, including maintaining coherence over longer texts, ensuring accuracy, and avoiding biases in </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">generated content.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.215.1">Analyzing NLP tasks</span></h2>
<p><span class="koboSpan" id="kobo.216.1">NLP </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.217.1">encompasses a series of tasks within text analysis, resulting in a layered structure. </span><span class="koboSpan" id="kobo.217.2">The foundational strata upon which sentence analysis relies are outlined </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.219.1">Morphology analysis</span></strong><span class="koboSpan" id="kobo.220.1">: The </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.221.1">objective of this stage is to break down input language strings into sets of tokens, which correspond to discrete words, sub-words, and punctuation elements. </span><span class="koboSpan" id="kobo.221.2">Through the process of tokenization, the text is fragmented, resulting in a sequence of tokens, with each token representing a word from the text. </span><span class="koboSpan" id="kobo.221.3">Within this stage, two concepts hold significance: the stem and the lemma. </span><span class="koboSpan" id="kobo.221.4">The stem is the foundational form of a word and is obtained by removing inflections (such as verb conjugations or noun plurals) from its altered version. </span><span class="koboSpan" id="kobo.221.5">Conversely, the lemma represents the standardized form of the word, chosen conventionally to encompass all its inflected variations. </span><span class="koboSpan" id="kobo.221.6">This phase involves identifying both the stem and lemma for each word, which can be accomplished through two distinct operations: stemming and lemmatization, respectively. </span><span class="koboSpan" id="kobo.221.7">This information becomes instrumental in subsequent analysis stages. </span><span class="koboSpan" id="kobo.221.8">The rationale behind this approach lies in the efficient utilization of memory – maintaining rules based on word components and their combinations that form specific inflected forms is much more resource-effective than managing each word as an individual element within an </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">extensive inventory.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Syntax analysis</span></strong><span class="koboSpan" id="kobo.224.1">: To </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.225.1">grasp the meaning of a sentence, it’s insufficient to merely comprehend the definitions of its constituent words; it’s equally crucial to discern the relationships between these words. </span><span class="koboSpan" id="kobo.225.2">This stage addresses the syntactic examination of the provided text. </span><span class="koboSpan" id="kobo.225.3">All components of speech are recognized and encompass verbs, nouns, adjectives, adverbs, prepositions, and pronouns. </span><span class="koboSpan" id="kobo.225.4">The process that annotates each word with its respective part of speech is </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.226.1">termed </span><strong class="bold"><span class="koboSpan" id="kobo.227.1">part-of-speech tagging</span></strong><span class="koboSpan" id="kobo.228.1">. </span><span class="koboSpan" id="kobo.228.2">This process is divided into two sub-processes: firstly, shallow parsing generates a binary tree in which elementary segments – namely, the </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">nominal part</span></strong><span class="koboSpan" id="kobo.230.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.231.1">NP</span></strong><span class="koboSpan" id="kobo.232.1">) and</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.233.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">verbal part</span></strong><span class="koboSpan" id="kobo.235.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.236.1">VP</span></strong><span class="koboSpan" id="kobo.237.1">) – are </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.238.1">identified. </span><span class="koboSpan" id="kobo.238.2">Secondly, full parsing produces a syntactic tree that designates the syntactic role of each word within </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">the sentence.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.240.1">Semantic analysis</span></strong><span class="koboSpan" id="kobo.241.1">: The semantic analysis</span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.242.1"> phase leverages insights garnered from preceding analytical stages, encompassing the meanings of individual words and their interconnected relationships, to interpret the overall significance of the sentence. </span><span class="koboSpan" id="kobo.242.2">The process of named entity recognition is employed to detect and classify groups of words that collectively form an entity, which might include personal names, countries, events, and the like. </span><span class="koboSpan" id="kobo.242.3">Semantic processing endeavors to deduce the potential interpretation of a sentence, with a particular focus on the interplay between the meanings of the words within the sentence being examined. </span><span class="koboSpan" id="kobo.242.4">This level of processing may encompass semantic disambiguation for words that possess multiple meanings, akin to how disambiguation is performed for words that can adopt multiple syntactic roles. </span><span class="koboSpan" id="kobo.242.5">Diverse methodologies can be employed for disambiguation, some of which involve assessing the frequency of each meaning within a specific corpus, while others consider contextual cues. </span><span class="koboSpan" id="kobo.242.6">Additionally, certain methods tap into pragmatic knowledge within the document’s domain to aid </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">in disambiguation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.244.1">Pragmatic analysis</span></strong><span class="koboSpan" id="kobo.245.1">: This </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.246.1">phase revolves around identifying the contextual environment in which the text is situated and the subsequent utilization of that context. </span><span class="koboSpan" id="kobo.246.2">Specifically, pragmatics delves into how the context influences the interpretation of meanings. </span><span class="koboSpan" id="kobo.246.3">In this context, </span><em class="italic"><span class="koboSpan" id="kobo.247.1">context</span></em><span class="koboSpan" id="kobo.248.1"> refers to the surrounding circumstances, including a range of non-linguistic factors (such as social dynamics, environmental cues, and psychological factors) that impact linguistic expressions. </span><span class="koboSpan" id="kobo.248.2">Indeed, human language is not solely rooted in its morphological, syntactic, and semantic attributes; it is also deeply intertwined with external knowledge linked to the situation in which a sentence is embedded. </span><span class="koboSpan" id="kobo.248.3">Within pragmatic analysis, a distinction is often drawn between the literal meaning of an utterance and the intended meaning of the speaker. </span><span class="koboSpan" id="kobo.248.4">The literal meaning pertains to the direct interpretation of the expression, while the speaker’s intention refers to the underlying concept the speaker aims to convey. </span><span class="koboSpan" id="kobo.248.5">For accurate interpretation of communication, several factors may be essential. </span><span class="koboSpan" id="kobo.248.6">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.250.1">Understanding the roles and statuses of the </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">conversational participants</span></span></li><li><span class="koboSpan" id="kobo.252.1">Recognizing the spatiotemporal context of </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">the situation</span></span></li><li><span class="koboSpan" id="kobo.254.1">Possessing </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.255.1">knowledge about the subject matter </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">being discussed</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.257.1">The ability to comprehend implied meanings from another speaker is termed </span><strong class="bold"><span class="koboSpan" id="kobo.258.1">pragmatic competence</span></strong><span class="koboSpan" id="kobo.259.1">. </span><span class="koboSpan" id="kobo.259.2">Despite its</span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.260.1"> significance, this type of analysis is still not extensively detailed in the literature, primarily due to the considerable challenges </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">it poses.</span></span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.262.1">Introducing automatic processing</span></h2>
<p><span class="koboSpan" id="kobo.263.1">From an automated processing </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.264.1">perspective, the syntactic level presents fewer challenges. </span><span class="koboSpan" id="kobo.264.2">Complex sentences can be analyzed, their grammatical accuracy identified, and their syntactic structures reconstructed seamlessly. </span><span class="koboSpan" id="kobo.264.3">The semantic level, on the other hand, proves significantly more intricate. </span><span class="koboSpan" id="kobo.264.4">In simpler scenarios, the following approach </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">is feasible:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.266.1">Sentence meaning is deduced from the meanings of individual words once the sentence’s syntactic structure has </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">been discerned</span></span></li>
<li><span class="koboSpan" id="kobo.268.1">The meanings of individual words are derived from a readily accessible dictionary through </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">automated means</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.270.1">However, numerous issues arise, </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.272.1">Firstly, the same word can bear distinct meanings in </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">diverse contexts.</span></span></li>
<li><span class="koboSpan" id="kobo.274.1">Secondly, the syntactic structure of a sentence can be ambiguous, occasionally attributing different structures to the same sequence </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">of words.</span></span></li>
<li><span class="koboSpan" id="kobo.276.1">Lastly, certain idiomatic phrases possess meanings separate from the literal interpretations derived from composing the meanings of their </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">constituent words.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.278.1">Artificial intelligence endeavors to tackle these challenges by leveraging a suitable knowledge base that the language processing program </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">can access.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">The pragmatic</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.281.1"> level introduces an even more intricate terrain. </span><span class="koboSpan" id="kobo.281.2">The primary challenge stems from our capacity, during a conversation, to anticipate the mental states of our conversational partners. </span><span class="koboSpan" id="kobo.281.3">In essence, effective communication necessitates the representation of interlocutors’ intentions, which are only partially evident in their </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">spoken words.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">Now that we’ve introduced the basic concepts of NLP, let’s analyze the specific contents that allow us to approach the problem from a practical point </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">of view.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.285.1">Exploring corpora and word and sentence tokenizers</span></h1>
<p><span class="koboSpan" id="kobo.286.1">The analysis of </span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.287.1">corpora, words, and sentence tokenization forms the basis for comprehensive language understanding. </span><span class="koboSpan" id="kobo.287.2">Corpora provides real-world language data for analysis, words </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.288.1">constitute the elements of expression, and sentence tokenization structures the text into meaningful units</span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.289.1"> for further investigation. </span><span class="koboSpan" id="kobo.289.2">This trio of concepts plays a central role in advancing linguistic research and enhancing </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">NLP capabilities.</span></span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.291.1">Corpora</span></h2>
<p><span class="koboSpan" id="kobo.292.1">In linguistics and NLP, corpora </span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.293.1">refer to extensive collections of written or spoken texts that serve as valuable sources of data for linguistic analysis and language-related studies. </span><span class="koboSpan" id="kobo.293.2">Corpora provides a diverse range of language samples, enabling researchers to examine patterns, trends, and variations in language usage, syntax, and semantics across different contexts </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">and genres.</span></span></p>
<p><span class="koboSpan" id="kobo.295.1">Linguistic corpora represent sizable collections of spoken or written texts, often originating from authentic communication contexts (including speeches or newspaper articles). </span><span class="koboSpan" id="kobo.295.2">These collections are digitized and frequently accompanied by computerized tools for </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">convenient access:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.297.1"><img alt="Figure 7.3 – Word cloud of corpora basic key concepts. The words are displayed in different sizes, with the most frequent words being the largest. Word clouds are often used to identify the key concepts in a corpus" src="image/B21156_07_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.298.1">Figure 7.3 – Word cloud of corpora basic key concepts. </span><span class="koboSpan" id="kobo.298.2">The words are displayed in different sizes, with the most frequent words being the largest. </span><span class="koboSpan" id="kobo.298.3">Word clouds are often used to identify the key concepts in a corpus</span></p>
<p><span class="koboSpan" id="kobo.299.1">Corpora serve the purpose of examining how language is practically utilized and validating overarching trends through statistical analysis. </span><span class="koboSpan" id="kobo.299.2">They hold a pivotal role in contemporary lexicography, aiding in tasks such as selecting lemmas based on their usage frequency, identifying typical linguistic structures involving specific words, and grasping subtle nuances of meaning within </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">various contexts.</span></span></p>
<p><span class="koboSpan" id="kobo.301.1">Furthermore, corpora </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.302.1">wield significance in the advancement of language technologies such as automatic translation and speech recognition. </span><span class="koboSpan" id="kobo.302.2">In these applications, corpora are employed to construct statistical language models. </span><span class="koboSpan" id="kobo.302.3">They also find utility in language education, supporting the creation of instructional resources. </span><span class="koboSpan" id="kobo.302.4">Particularly for advanced learners, corpora enable the deduction of word properties and linguistic structures by observing their </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">contextual applications.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">There are various examples of corpora; the most used are </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.306.1">The Brown Corpus</span></strong><span class="koboSpan" id="kobo.307.1">: One of the</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.308.1"> first and most well-known corpora in English, it </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.309.1">contains over a million words of text from </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">various sources</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.311.1">The New York Times Corpus</span></strong><span class="koboSpan" id="kobo.312.1">: A large </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.313.1">corpus of news articles from the </span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.314.1">New York Times, spanning from 1987 </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">to 2007</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.316.1">The Project Gutenberg Corpus</span></strong><span class="koboSpan" id="kobo.317.1">: A collection</span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.318.1"> of over 60,000 free</span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.319.1"> electronic books in the </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">public domain</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.321.1">The Linguistic Data Consortium</span></strong><span class="koboSpan" id="kobo.322.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.323.1">LDC</span></strong><span class="koboSpan" id="kobo.324.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.325.1">Corpora</span></strong><span class="koboSpan" id="kobo.326.1">: A repository of a wide variety of corpora, including those for</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.327.1"> different languages </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">and</span></span><span class="No-Break"><a id="_idIndexMarker831"/></span><span class="No-Break"><span class="koboSpan" id="kobo.329.1"> domains</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.330.1">MATLAB provides a variety of tools for accessing and processing corpora. </span><span class="koboSpan" id="kobo.330.2">Some of the most commonly used tools are </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.332.1">The Text Analysis Toolbox</span></strong><span class="koboSpan" id="kobo.333.1">: A comprehensive toolbox </span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.334.1">for text processing, including functions for tokenization, stemming, lemmatization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">sentiment analysis</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.336.1">The Natural Language Processing Toolbox</span></strong><span class="koboSpan" id="kobo.337.1">: A toolbox for NLP tasks, such as named entity recognition, part-of-speech</span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.338.1"> tagging, and </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">dependency parsing</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">The Web Services Toolbox</span></strong><span class="koboSpan" id="kobo.341.1">: A </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.342.1">toolbox for accessing and interacting with web services, including those that provide access </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">to corpora</span></span></li>
</ul>
<h2 id="_idParaDest-151"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.344.1">Words</span></h2>
<p><span class="koboSpan" id="kobo.345.1">Words</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.346.1"> are the foundational units of language that carry meaning and convey ideas. </span><span class="koboSpan" id="kobo.346.2">The analysis of words involves investigating their forms, meanings, relationships, and usage patterns. </span><span class="koboSpan" id="kobo.346.3">This examination can encompass aspects such as morphological structure, part-of-speech categorization, frequency distribution, and semantic associations. </span><span class="koboSpan" id="kobo.346.4">Studying words allows linguists to understand how language is structured and how meanings are conveyed </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">through vocabulary.</span></span></p>
<p><span class="koboSpan" id="kobo.348.1">In ML and DL tasks, words are represented as vectors of numbers. </span><span class="koboSpan" id="kobo.348.2">This is done because ML algorithms can only work with numerical data. </span><span class="koboSpan" id="kobo.348.3">The process of converting words into vectors is called </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">word embedding.</span></span></p>
<p><span class="koboSpan" id="kobo.350.1">There are two main types of </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">word embedding:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.352.1">Bag of words</span></strong><span class="koboSpan" id="kobo.353.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.354.1">BoW</span></strong><span class="koboSpan" id="kobo.355.1">): In </span><strong class="bold"><span class="koboSpan" id="kobo.356.1">BoW</span></strong><span class="koboSpan" id="kobo.357.1">, each </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.358.1">unique word in a corpus is assigned a unique index. </span><span class="koboSpan" id="kobo.358.2">A document is then represented as a vector of the counts of each</span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.359.1"> word in the document. </span><span class="koboSpan" id="kobo.359.2">Let’s assume vocab size = 4; here, a sample document that states “I love cats” is represented as [ 1 1 1 0]. </span><span class="koboSpan" id="kobo.359.3">In BoW, we simply represent the document by the frequency of each word. </span><span class="koboSpan" id="kobo.359.4">For example, if we have a vocabulary of 1,000 words, then the whole document is represented by a 1,000-dimensional vector, where the </span><em class="italic"><span class="koboSpan" id="kobo.360.1">ith</span></em><span class="koboSpan" id="kobo.361.1"> entry of the vector represents the frequency of the </span><em class="italic"><span class="koboSpan" id="kobo.362.1">ith</span></em><span class="koboSpan" id="kobo.363.1"> vocabulary word in </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">the document.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.365.1">Word2vec</span></strong><span class="koboSpan" id="kobo.366.1">: Word2vec</span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.367.1"> is a more sophisticated approach to word embedding that takes into</span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.368.1"> account the context in which words appear. </span><span class="koboSpan" id="kobo.368.2">Word2vec learns two types of embeddings: word embeddings and context embeddings. </span><span class="koboSpan" id="kobo.368.3">Word embeddings are vectors that represent individual words, while context embeddings are vectors that represent the words that appear around a given word. </span><span class="koboSpan" id="kobo.368.4">For example, the word embedding for “cat” might be [0.1, 0.2, 0.3], while the context embedding for “cat” might be [0.4, </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">0.5, 0.6].</span></span></li>
</ul>
<h2 id="_idParaDest-152"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.370.1">Sentence tokenize</span></h2>
<p><span class="koboSpan" id="kobo.371.1">During the </span><strong class="bold"><span class="koboSpan" id="kobo.372.1">sentence tokenization</span></strong><span class="koboSpan" id="kobo.373.1"> process, we identify atomic elements known as tokens within each sentence. </span><span class="koboSpan" id="kobo.373.2">These</span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.374.1"> tokens serve as the basis for analyzing and evaluating the sentence itself. </span><span class="koboSpan" id="kobo.374.2">Consequently, during the tokenization phase, we not only recognize and assess these elements but also occasionally convert negative constructs. </span><span class="koboSpan" id="kobo.374.3">This results in the text being divided into tokens, which can be thought of as </span><em class="italic"><span class="koboSpan" id="kobo.375.1">indivisible</span></em><span class="koboSpan" id="kobo.376.1"> units. </span><span class="koboSpan" id="kobo.376.2">This task is relatively straightforward for languages that use spaces to separate words but becomes considerably more intricate for languages with continuous spelling systems. </span><span class="koboSpan" id="kobo.376.3">Focusing on languages such as English, which fall into the former category, a token can be defined as any sequence of characters enclosed by spaces. </span><span class="koboSpan" id="kobo.376.4">However, it’s worth noting that this definition allows for numerous exceptions. </span><span class="koboSpan" id="kobo.376.5">In languages where word boundaries aren’t explicitly indicated in </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.377.1">written text, this process is referred to as </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">word segmentation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.379.1"><img alt="Figure 7.4 – Word tokenization process" src="image/B21156_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.380.1">Figure 7.4 – Word tokenization process</span></p>
<p><span class="koboSpan" id="kobo.381.1">In the broader context of tokenization, several</span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.382.1"> challenges need to </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">be addressed:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.384.1">Lack of spaces separating words from preceding or </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">succeeding punctuation</span></span></li>
<li><span class="koboSpan" id="kobo.386.1">Instances where sequences of characters are not separated by spaces should be treated as two </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">separate tokens</span></span></li>
<li><span class="koboSpan" id="kobo.388.1">Occasions when sequences of characters are separated by spaces should be considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">single token</span></span></li>
<li><span class="koboSpan" id="kobo.390.1">Managing uppercase and </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">lowercase variations</span></span></li>
<li><span class="koboSpan" id="kobo.392.1">Navigating variations in </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">spelling conventions</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.394.1">Ultimately, at the end of this procedure, we identify a string with an assigned meaning, which we refer to as a </span><strong class="bold"><span class="koboSpan" id="kobo.395.1">token</span></strong><span class="koboSpan" id="kobo.396.1">. </span><span class="koboSpan" id="kobo.396.2">Each token</span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.397.1"> is structured as a pair comprising a token name and an optional </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">token value.</span></span></p>
<p><span class="koboSpan" id="kobo.399.1">Now that we have examined the basics of NLP in detail, we can proceed to analyze the implementation of a sentence classifier </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">in MATLAB.</span></span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.401.1">Implementing a MATLAB model to label sentences</span></h1>
<p><span class="koboSpan" id="kobo.402.1">In this section, we will discuss a very interesting topic that is very popular in today’s society. </span><span class="koboSpan" id="kobo.402.2">I am referring to the importance of reviews in influencing a customer’s interest in making the </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">right decision.</span></span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.404.1">Introducing sentiment analysis</span></h2>
<p><span class="koboSpan" id="kobo.405.1">Sentiment analysis, a </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.406.1">technique that utilizes NLP, extracts and analyzes subjective information from text. </span><span class="koboSpan" id="kobo.406.2">Analyzing vast datasets reveals collective opinions that impact various domains. </span><span class="koboSpan" id="kobo.406.3">While manual sentiment analysis is challenging, automated methods have emerged. </span><span class="koboSpan" id="kobo.406.4">However, automating language modeling is complex and costly due to the nuances of human language. </span><span class="koboSpan" id="kobo.406.5">Additionally, the methodology varies across languages, </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">increasing complexity.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">A major challenge lies in determining the polarity of opinions. </span><span class="koboSpan" id="kobo.408.2">Polarity classification is subjective, with one sentence perceived differently by individuals based on their value systems. </span><span class="koboSpan" id="kobo.408.3">The rise of social media has heightened interest in sentiment analysis. </span><span class="koboSpan" id="kobo.408.4">As online expressions proliferate, this field has become valuable for businesses to promote products, identify opportunities, and protect their reputation. </span><span class="koboSpan" id="kobo.408.5">Challenges arise from the use of simplistic terms in sentiment analysis algorithms to convey opinions. </span><span class="koboSpan" id="kobo.408.6">Cultural influences, linguistic disparities, and contextual factors make converting written text into a binary positive or negative sentiment </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">highly intricate:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.410.1"><img alt="Figure 7.5 – Sentiment analysis polarity" src="image/B21156_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.411.1">Figure 7.5 – Sentiment analysis polarity</span></p>
<p><span class="koboSpan" id="kobo.412.1">Advanced sentiment analysis endeavors to pinpoint specific moods, such as happiness, sadness, or anger. </span><span class="koboSpan" id="kobo.412.2">For instance, it can involve tasks such as categorizing a song review not just as positive or negative, but also predicting a numerical score, conducting a comprehensive analysis of hotel reviews, and providing ratings for various aspects such as comfort, noise levels, </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">and design.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">Furthermore, it’s feasible to categorize a given text into one of two classes: objective or subjective. </span><span class="koboSpan" id="kobo.414.2">A text may contain objective information, as seen in a news article, or it can be subjective, such as the expression of political opinions in an interview. </span><span class="koboSpan" id="kobo.414.3">The subjectivity of sentences often hinges on their context, and even an objective document may feature subjective phrases, such as quotations. </span><span class="koboSpan" id="kobo.414.4">Discerning subjectivity versus objectivity can be more challenging than classifying polarity, as it relies heavily on the surrounding </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">textual context.</span></span></p>
<p><span class="koboSpan" id="kobo.416.1">Sentiment analysis offers </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.417.1">several approaches, with the most commonly employed methods falling into four </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">broad categories:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.419.1">Lexicon-based methods</span></strong><span class="koboSpan" id="kobo.420.1">: These techniques identify emotional keywords within the text and assign them a </span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.421.1">predetermined affinity to represent </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">specific emotions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.423.1">Rule-based methods</span></strong><span class="koboSpan" id="kobo.424.1">: This approach classifies text based on explicit emotional categories, relying</span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.425.1"> on the presence of unambiguous emotional words such as </span><em class="italic"><span class="koboSpan" id="kobo.426.1">happy</span></em><span class="koboSpan" id="kobo.427.1">, </span><em class="italic"><span class="koboSpan" id="kobo.428.1">sad</span></em><span class="koboSpan" id="kobo.429.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">or </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.431.1">bored</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.433.1">Statistical methods</span></strong><span class="koboSpan" id="kobo.434.1">: In this </span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.435.1">category, the aim is to ascertain the source of the sentiment (the subject) and the target (the object being evaluated). </span><span class="koboSpan" id="kobo.435.2">To gauge opinions in context and identify the assessed attribute, grammatical relations among the words are examined through an extensive analysis of </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">the text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.437.1">ML methods</span></strong><span class="koboSpan" id="kobo.438.1">: These </span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.439.1">methods employ various learning algorithms to deduce sentiment, often utilizing a dataset with predefined classifications (supervised methods). </span><span class="koboSpan" id="kobo.439.2">The learning process is iterative, requiring the construction of models that associate polarity with different types of comments and, when necessary, topic analysis for a more </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">comprehensive understanding.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.441.1">The utilization of emojis and emoticons is on the rise. </span><span class="koboSpan" id="kobo.441.2">Instead of relying solely on words, people often express their emotions using tiny round faces and other symbols. </span><span class="koboSpan" id="kobo.441.3">These visual elements prove valuable for swiftly and pleasantly conveying one’s message. </span><span class="koboSpan" id="kobo.441.4">Using DL architectures, it is possible to learn the relationship between emoticons and sentiments. </span><span class="koboSpan" id="kobo.441.5">For example, a DL method might be trained on a large corpus of text and emoticons to learn the features of emoticons that are most predictive </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">of sentiment.</span></span></p>
<p><span class="koboSpan" id="kobo.443.1">Let’s learn how to apply the tools made available by DL to tackle a practical case of </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">sentiment analysis.</span></span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.445.1">Movie review sentiment analysis</span></h2>
<p><span class="koboSpan" id="kobo.446.1">Reviews have become an </span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.447.1">essential element for businesses to thrive, providing valuable feedback from customers and driving purchasing decisions. </span><span class="koboSpan" id="kobo.447.2">They serve as a reliable source of information for prospective buyers and offer sellers insights to enhance their products and services. </span><span class="koboSpan" id="kobo.447.3">Moreover, they foster user engagement and amplify the power of word-of-mouth marketing, significantly impacting a company’s online reputation. </span><span class="koboSpan" id="kobo.447.4">Securing positive reviews is crucial for businesses of all sizes to </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">maintain profitability.</span></span></p>
<p><span class="koboSpan" id="kobo.449.1">The rise of social media and the internet has dramatically transformed consumer buying behaviors. </span><span class="koboSpan" id="kobo.449.2">Potential buyers now actively seek online reviews and information before making purchasing decisions, relying less on traditional word-of-mouth recommendations. </span><span class="koboSpan" id="kobo.449.3">They have become more independent and discerning, carefully considering </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">their choices.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">In the past, film critics played a dominant role in shaping consumer preferences, influencing movie choices and fostering critical thinking. </span><span class="koboSpan" id="kobo.451.2">However, the advent of consumer-generated reviews has shifted the paradigm, with consumer opinions now holding greater weight in influencing purchasing decisions. </span><span class="koboSpan" id="kobo.451.3">This democratization of opinion has empowered consumers to share their experiences and shape the </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">cultural landscape.</span></span></p>
<p><span class="koboSpan" id="kobo.453.1">For the task of classifying movie reviews as either positive or negative sentiment, we will employ a short version of the IMDb Movie Reviews dataset. </span><span class="koboSpan" id="kobo.453.2">This dataset is specifically designed for binary sentiment classification and contains 748 reviews opportunely labeled. </span><span class="koboSpan" id="kobo.453.3">The dataset was adequately revisited in this example using only two features: review </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">and class.</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">Let’s learn how to build a model for classifying sentences using </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">an RNN.</span></span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.457.1">Using an LSTM model for label sentences</span></h2>
<p><span class="koboSpan" id="kobo.458.1">An LSTM model </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.459.1">is an advanced form </span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.460.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.461.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.462.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.463.1">RNN</span></strong><span class="koboSpan" id="kobo.464.1">) that can learn long-term dependencies within sequences, making it a powerful tool for processing sequential data, such as text, speech, and time series data. </span><span class="koboSpan" id="kobo.464.2">This makes it ideal for tasks such as label sentence classification, where the goal is to assign a label to a sentence based on its overall sentiment. </span><span class="koboSpan" id="kobo.464.3">LSTM has been successfully applied to a variety of tasks, including sentiment analysis, machine translation, speech recognition, and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">language generation.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">To learn how to</span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.467.1"> implement a model to label sentences in MATLAB, we will use movie reviews that have been made by a lot of </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">online user:.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.469.1">To begin, we need to import the dataset into the MATLAB workspace. </span><span class="koboSpan" id="kobo.469.2">You can find the dataset in </span><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">.xlsx</span></strong><span class="koboSpan" id="kobo.471.1"> format on the project’s GitHub page (</span><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">ImdbDataset.xlsx</span></strong><span class="koboSpan" id="kobo.473.1">). </span><span class="koboSpan" id="kobo.473.2">To simplify this task, the dataset has been appropriately trimmed. </span><span class="koboSpan" id="kobo.473.3">Once you’ve downloaded the file, all you need to do is specify the root folder’s path. </span><span class="koboSpan" id="kobo.473.4">Then, you can load the data into the MATLAB workspace </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">like so:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.475.1">
ImdbData = readtable("ImdbDataset.xlsx",'TextType','string');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.476.1">To load the data, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">readtable()</span></strong><span class="koboSpan" id="kobo.478.1"> function. </span><span class="koboSpan" id="kobo.478.2">When dealing with spreadsheet files, this function generates a distinct variable for each column found in the file and extracts variable names from the initial row of </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">the file.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.480.1">The objective of this example is to categorize reviews based on the labels in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">Class</span></strong><span class="koboSpan" id="kobo.482.1"> column. </span><span class="koboSpan" id="kobo.482.2">To categorize the data into classes, you should convert these labels into </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">categorical variables:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.484.1">ImdbData.Class = categorical(ImdbData.Class);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.485.1">After loading the data, we need to visualize the distribution of the classes in the dataset by creating a histogram. </span><span class="koboSpan" id="kobo.485.2">This will help us assess whether the data is </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">evenly distributed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.487.1">figure
histogram(ImdbData.Class);
xlabel("Class")
ylabel("Frequency")
title("Class Distribution")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.488.1">The </span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.489.1">diagram shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.490.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.491.1">.6</span></em><span class="koboSpan" id="kobo.492.1"> will </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">be drawn:</span></span></p></li> </ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.494.1"><img alt="Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive" src="image/B21156_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.495.1">Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.496.1">The subsequent step involves dividing the data into training and validation sets. </span><span class="koboSpan" id="kobo.496.2">Partition the data into a training set and a separate held-out set for validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">testing purposes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.498.1">
DataSplitting = cvpartition(ImdbData.Class,'Holdout',0.3);</span></pre></li> <li><span class="koboSpan" id="kobo.499.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.501.1"> function establishes a random dataset partition. </span><span class="koboSpan" id="kobo.501.2">You can employ this partition to create training and testing sets for assessing a statistical model through cross-validation. </span><span class="koboSpan" id="kobo.501.3">Set the holdout percentage to 30%; this means that 70% of the data will be allocated for training, while the remaining 30% will be reserved </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">for validation.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.503.1">Now, we will utilize the training option to obtain the training indices and the test option to obtain the testing indices </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">for cross-validation:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.505.1">
TrainDataset = ImdbData (training(DataSplitting),:);
TestDataset = ImdbData (test(DataSplitting),:);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.506.1">Following </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.507.1">this, we will retrieve the review data and corresponding classes from the </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">partitioned tables:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.509.1">TrainReview = TrainDataset.Review;
TestReview = TestDataset.Review;
TrainClass = TrainDataset.Class;
TestClass = TestDataset.Class;</span></pre></li> <li><span class="koboSpan" id="kobo.510.1">At this point, our data has been prepared for use, and we can start preprocessing the review data. </span><span class="koboSpan" id="kobo.510.2">We will employ a function designed for tokenization and text preprocessing called </span><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">preprocessText</span></strong><span class="koboSpan" id="kobo.512.1">. </span><span class="koboSpan" id="kobo.512.2">This function carries out the </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">following tasks:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.514.1"> Tokenizes the text </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">tokenizedDocument</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.518.1"> Converts the text into lowercase </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">lower</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.522.1"> Removes punctuation </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">erasePunctuation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.526.1">We will apply the </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">preprocessText</span></strong><span class="koboSpan" id="kobo.528.1"> function to both the training and validation data </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">for preprocessing:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.530.1">
TrainDoc = preprocessText(TrainReview);
TestDoc = preprocessText(TestReview);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.531.1">After that, we have to apply the word encoding technique. </span><span class="koboSpan" id="kobo.531.2">This method involves transforming words from a human language, such as English, into a numerical or symbolic format that’s suitable for a wide range of computational tasks, with a particular focus on applications in NLP </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">and ML.</span></span></p></li> <li><span class="koboSpan" id="kobo.533.1">To transform the documents into sequences of numeric indices, we will employ the </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">wordEncoding()</span></strong><span class="koboSpan" id="kobo.535.1"> function, </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.537.1">
EncText = wordEncoding(TrainDoc);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.538.1">This function associates words within a vocabulary with </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">numeric indices.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.540.1">To use </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.541.1">the reviews as input for an LSTM, all inputs must have the same length. </span><span class="koboSpan" id="kobo.541.2">However, the reviews in the dataset vary in length. </span><span class="koboSpan" id="kobo.541.3">One solution is to truncate longer reviews and pad shorter ones to ensure they are all the same length. </span><span class="koboSpan" id="kobo.541.4">To truncate and pad the reviews, start by choosing a target length. </span><span class="koboSpan" id="kobo.541.5">Then, truncate longer documents and pad shorter ones. </span><span class="koboSpan" id="kobo.541.6">To achieve optimal results, the target length should be short enough that you can avoid discarding a significant amount </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">of data.</span></span></p></li> <li><span class="koboSpan" id="kobo.543.1">To find an appropriate target length, we can visualize a histogram of the lengths of the </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">training documents:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.545.1">
NumDoc = doclength(TrainDoc);
figure
histogram(NumDoc)
xlabel("Number of tokens")
ylabel("Number of Reviews")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.546.1">To start, we counted the length of each review. </span><span class="koboSpan" id="kobo.546.2">Then, we drew a histogram of the distribution. </span><span class="koboSpan" id="kobo.546.3">The following chart </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">was printed:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.548.1"><img alt="Figure 7.7 – Tokens distribution of the movie reviews" src="image/B21156_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.549.1">Figure 7.7 – Tokens distribution of the movie reviews</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.550.1">By</span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.551.1"> analyzing the tokens distribution of the movie reviews, we’ll notice that most reviews have several tokens between 0 and 30. </span><span class="koboSpan" id="kobo.551.2">So, we can choose this number as the </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">target length:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.553.1">
TokNum = 30;</span></pre> <ol>
<li value="7"><span class="koboSpan" id="kobo.554.1">Now, we need to convert the reviews into numeric </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">indices sequences:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.556.1">
RewTrain = doc2sequence(EncText,TrainDoc,'Length',TokNum);
RewTest = doc2sequence(EncText,TestDoc,'Length',TokNum);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.557.1">To do this, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">doc2sequence()</span></strong><span class="koboSpan" id="kobo.559.1"> function, which transforms documents into sequences that are suitable for DL. </span><span class="koboSpan" id="kobo.559.2">This function provides a cell array containing the numeric indices of words in the documents, as determined by the word encoding provided. </span><span class="koboSpan" id="kobo.559.3">Each element within the sequences array is a vector consisting of indices representing the words within the </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">respective document.</span></span></p></li> <li><span class="koboSpan" id="kobo.561.1">Now, we will proceed with defining the architecture of the </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">LSTM network:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.563.1">
inputSize = 1;
embeddingDimension = 150;
numHiddenUnits = 50;
numWords = EncText.NumWords;
numClasses = numel(categories(TrainClass));
layers = [ ...
</span><span class="koboSpan" id="kobo.563.2">    sequenceInputLayer(inputSize)
    wordEmbeddingLayer(embeddingDimension,numWords)
    lstmLayer(numHiddenUnits,'OutputMode','last')
    fullyConnectedLayer(numClasses)
    softmaxLayer
    classificationLayer]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.564.1">To feed </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.565.1">sequence data into the network, incorporate a sequence input layer with an input size of 1. </span><span class="koboSpan" id="kobo.565.2">A sequence input with 1 dimension means each numerically encoded token is represented by a 1x1 scalar. </span><span class="koboSpan" id="kobo.565.3">Following that, introduce a word embedding layer with a dimension of 150, which should match the number of words specified in the word encoding. </span><span class="koboSpan" id="kobo.565.4">Word embedding layers learn to map words to vectors in a way that captures the meaning of the words. </span><span class="koboSpan" id="kobo.565.5">Subsequently, include an LSTM layer with 50 hidden units. </span><span class="koboSpan" id="kobo.565.6">For a sequence-to-label classification task using the LSTM layer, configure </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">OutputMode</span></strong><span class="koboSpan" id="kobo.567.1"> last. </span><span class="koboSpan" id="kobo.567.2">Finally, add a fully connected layer that’s the same size as the number of classes, followed by a softmax layer and a </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">classification layer.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.569.1">The architecture of the LSTM that’s been defined is </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">shown here:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.571.1">layers =
6×1 Layer array with layers:
1   ''   Sequence Input          Sequence input with 1 dimensions
2   ''   Word Embedding Layer    Word embedding layer with 150 dimensions and 2410 unique words
3   ''   LSTM                    LSTM with 50 hidden units
4   ''   Fully Connected         2 fully connected layer
5   ''   Softmax                 softmax
6   ''   Classification Output   crossentropyex</span></pre></li> <li><span class="koboSpan" id="kobo.572.1">Now, we have to specify the </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">training options:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.574.1">
options = trainingOptions('adam', ...
</span><span class="koboSpan" id="kobo.574.2">    'MiniBatchSize',16, ...
</span><span class="koboSpan" id="kobo.574.3">    'GradientThreshold',2, ...
</span><span class="koboSpan" id="kobo.574.4">    'Shuffle','every-epoch', ...
</span><span class="koboSpan" id="kobo.574.5">    'ValidationData',{RewTest,TestClass}, ...
</span><span class="koboSpan" id="kobo.574.6">    'Plots','training-progress', ...
</span><span class="koboSpan" id="kobo.574.7">    'Verbose',false);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.575.1">The following</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.576.1"> options </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">were set:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">adam solver</span></strong><span class="koboSpan" id="kobo.579.1">: Adam, derived from </span><em class="italic"><span class="koboSpan" id="kobo.580.1">Adaptive Moment Estimation</span></em><span class="koboSpan" id="kobo.581.1">, is a widely embraced optimization algorithm in the realms of ML and DL. </span><span class="koboSpan" id="kobo.581.2">Its primary role is to adjust the parameters, while encompassing weights and biases, within a model as it undergoes training. </span><span class="koboSpan" id="kobo.581.3">Adam is especially renowned for its effectiveness in training </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">neural networks.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">MiniBatchSize</span></strong><span class="koboSpan" id="kobo.584.1">: This is a hyperparameter that dictates the number of examples, which are individual data points or samples, that are employed in each training iteration during the execution of Adam or a similar </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">optimization algorithm.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">GradientThreshold</span></strong><span class="koboSpan" id="kobo.587.1">: This specifies a threshold value for gradients during the training process. </span><span class="koboSpan" id="kobo.587.2">It is used to make training models more stable and efficient, particularly when dealing with RNNs or </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">deep architectures.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">Shuffle</span></strong><span class="koboSpan" id="kobo.590.1">: This option shuffles the data in each epoch to increase the degree of randomness in </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">the algorithm.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">ValidationData</span></strong><span class="koboSpan" id="kobo.593.1">: This option specifies the data used for the </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">validation process.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">Plots</span></strong><span class="koboSpan" id="kobo.596.1">: With this option, the training process will be checked through a plot that upgrades the evolution of </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">the process.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">Verbose</span></strong><span class="koboSpan" id="kobo.599.1">: The output of the training process will </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">be hidden.</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.601.1">Finally, we </span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.602.1">can start the </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">training process:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.604.1">
LSTMNet = trainNetwork(RewTrain,TrainClass,layers,options);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.605.1">The following plot will </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">be printed:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.607.1"><img alt="Figure 7.8 – Training process of the LSTM" src="image/B21156_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.608.1">Figure 7.8 – Training process of the LSTM</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.609.1">The training process processed the data by building a network capable of returning a validation accuracy of 75.89%. </span><span class="koboSpan" id="kobo.609.2">Given the small amount of data available, this result can be </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">considered satisfactory.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.611.1">To improve the performance of the model, we can try the simulation with a slightly large number of LSTM units (100 instead of 50). </span><span class="koboSpan" id="kobo.611.2">However, changing the embedding</span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.612.1"> dimension (from 150 to 100) could increase the validation accuracy to at </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">least 85%.</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">Now that we’ve analyzed how to build an LSTM to classify movie reviews, let’s learn how to improve the accuracy of predictions using </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">ensemble methods.</span></span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.616.1">Understanding gradient boosting techniques</span></h1>
<p><span class="koboSpan" id="kobo.617.1">To improve the </span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.618.1">performance of an algorithm, we can perform a series of steps and use different techniques, depending on the type of algorithm and the specific problems being addressed. </span><span class="koboSpan" id="kobo.618.2">The first approach involves a thorough analysis of the data to identify possible inaccuracies or shortcomings. </span><span class="koboSpan" id="kobo.618.3">In addition, many algorithms have parameters that can be adjusted to achieve better performance – not to mention techniques such as feature scaling or feature selection. </span><span class="koboSpan" id="kobo.618.4">A popular technique is to combine the capabilities offered by different algorithms to achieve better </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">overall performance.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.620.1">Approaching ensemble learning</span></h2>
<p><span class="koboSpan" id="kobo.621.1">The concept of </span><strong class="bold"><span class="koboSpan" id="kobo.622.1">ensemble learning</span></strong><span class="koboSpan" id="kobo.623.1"> involves </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.624.1">the use of multiple models </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.625.1">combined in a way that maximizes performance by exploiting their strengths and mitigating their relative weaknesses. </span><span class="koboSpan" id="kobo.625.2">These ensemble learning methods are based on weak learning models that do not achieve high levels of accuracy on their own, but when combined can produce robust predictions. </span><span class="koboSpan" id="kobo.625.3">In the context of ensemble learning, a </span><strong class="bold"><span class="koboSpan" id="kobo.626.1">weak learner</span></strong><span class="koboSpan" id="kobo.627.1"> is </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.628.1">a model that can produce slightly better results than randomly, while </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.629.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">strong learner</span></strong><span class="koboSpan" id="kobo.631.1"> approaches an ideal model and can overcome problems typical of traditional models, such </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">as overfitting.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">Each model that’s used returns predictions that may or may not be correct. </span><span class="koboSpan" id="kobo.633.2">The prediction error, in a supervised learning problem, is computed </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.635.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.636.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.637.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.638.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.639.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.640.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.641.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.642.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.643.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.644.1">s</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.645.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.646.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.647.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.648.1">v</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.649.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.650.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.651.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.652.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.653.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.654.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.655.1">e</span></span></span></p>
<p><span class="koboSpan" id="kobo.656.1">The two fundamental components of error are precision and variance. </span><span class="koboSpan" id="kobo.656.2">The goal of models that use ensemble</span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.657.1"> approaches is to obtain reliable predictions by trying to reduce both </span><strong class="bold"><span class="koboSpan" id="kobo.658.1">variance</span></strong><span class="koboSpan" id="kobo.659.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.660.1">bias</span></strong><span class="koboSpan" id="kobo.661.1">. </span><span class="koboSpan" id="kobo.661.2">Variance is the variability of a model’s prediction for a given data point. </span><span class="koboSpan" id="kobo.661.3">A model </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.662.1">with high variance is said to be overfitting, which means that it is learning the noise in the training data rather than the underlying patterns. </span><span class="koboSpan" id="kobo.662.2">This can also lead to poor generalization performance. </span><span class="koboSpan" id="kobo.662.3">Bias is the difference between the average prediction of a model and the correct value. </span><span class="koboSpan" id="kobo.662.4">A model with high bias is said to be underfitting, which means that it is not learning the underlying patterns in the data. </span><span class="koboSpan" id="kobo.662.5">This can lead to poor generalization performance, where the model does not perform well on unseen data. </span><span class="koboSpan" id="kobo.662.6">There is often a trade-off between these two components: classifiers with low accuracy tend to have high variance and vice versa. </span><span class="koboSpan" id="kobo.662.7">The goal of ML is to find a classifier that has a good balance of bias and variance. </span><span class="koboSpan" id="kobo.662.8">This is known as the bias-variance trade-off. </span><span class="koboSpan" id="kobo.662.9">In general, it is difficult to find a classifier that has both low bias and low variance. </span><span class="koboSpan" id="kobo.662.10">However, several techniques can be used to reduce bias and variance, such as feature selection, regularization, </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">and cross-validation.</span></span></p>
<p><span class="koboSpan" id="kobo.664.1">To improve the</span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.665.1"> accuracy of the model, it is assumed that the different classifiers can make different errors on each sample, but agree on the correct classification. </span><span class="koboSpan" id="kobo.665.2">By averaging the outputs of the classifiers, the overall error is reduced by averaging the error components. </span><span class="koboSpan" id="kobo.665.3">Combining the outputs by the model does not necessarily guarantee significantly better classification performance than other models, but it does help to reduce </span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.666.1">the likelihood of selecting a classifier with </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">disappointing performance.</span></span></p>
<p><span class="koboSpan" id="kobo.668.1">There are several ensemble methods. </span><span class="koboSpan" id="kobo.668.2">They mainly differ in the way they process the data. </span><span class="koboSpan" id="kobo.668.3">The most common are </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.670.1">Bagging</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.671.1">Random forest</span></strong></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.672.1">Boosting</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.673.1">The first ensemble models were developed in the 1990s, but at the time of writing, there is no specific study that can help a user select the most appropriate meta-classifier. </span><span class="koboSpan" id="kobo.673.2">These methods are widely used to solve a variety of learning problems, including feature selection. </span><span class="koboSpan" id="kobo.673.3">In the subsequent sections, we will analyze the basics of each kind of </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">ensemble technique.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.675.1">Bagging definition and meaning</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.676.1">Bagging</span></strong><span class="koboSpan" id="kobo.677.1"> is </span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.678.1">an </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.679.1">ensemble learning method that collects the predictions of weak learners (basic models), compares them, and combines them into a single prediction. </span><span class="koboSpan" id="kobo.679.2">The distinguishing feature of bagging is how the training sets for each decision tree </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">are selected.</span></span></p>
<p><span class="koboSpan" id="kobo.681.1">To create each training set, bagging uses sampling with replacement, known as bootstrapping, on the original dataset. </span><strong class="bold"><span class="koboSpan" id="kobo.682.1">Bootstrapping</span></strong><span class="koboSpan" id="kobo.683.1"> is a statistical resampling technique that’s used to approximate the sample </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.684.1">distribution of an estimator. </span><span class="koboSpan" id="kobo.684.2">This means that data is randomly selected from the original training set, with the possibility of some data being selected multiple times. </span><span class="koboSpan" id="kobo.684.3">This procedure increases the diversity of the decision trees that </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">are produced.</span></span></p>
<p><span class="koboSpan" id="kobo.686.1">A special feature of this method is that it works best when the basic learners (weak learners) are not very stable and therefore very sensitive to variations in the training data. </span><span class="koboSpan" id="kobo.686.2">In the presence of stable basic learners, the overall performance may </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">even deteriorate.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">The basic concept behind bagging is to combine multiple models that are unstable but have a low bias to produce a low variance overall model. </span><span class="koboSpan" id="kobo.688.2">The logic behind bagging is based on the idea that the expected value of the mean of identically distributed random variables is equal to the expected value of a single </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">random variable.</span></span></p>
<p><span class="koboSpan" id="kobo.690.1">The bagging equation is </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.692.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.693.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.694.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.695.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.696.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.697.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.698.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.699.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.700.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.701.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.702.1">d</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.703.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.704.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.705.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.706.1">T</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.707.1">Σ</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.708.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.709.1">t</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.710.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.711.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.712.1">^</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.713.1">T</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.714.1">f</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.715.1">_</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.716.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.717.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.718.1">x</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.719.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.720.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.722.1">T</span></em><span class="koboSpan" id="kobo.723.1"> is the number of models in </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">the ensemble</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.725.1">f_t(x)</span></em><span class="koboSpan" id="kobo.726.1"> is the prediction of the </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">t-th model</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.728.1">Therefore, the goal is to reduce variance by exploiting this </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">averaging operation.</span></span></p>
<p><span class="koboSpan" id="kobo.730.1">Typically, the decision tree is an appropriate choice for this approach because, despite its simplicity, it can effectively capture non-linear relationships and interactions </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">between variables.</span></span></p>
<p><span class="koboSpan" id="kobo.732.1">The key parameter for adjusting bagging is the number of trees. </span><span class="koboSpan" id="kobo.732.2">However, simulation studies and empirical evidence have shown that the performance of this model is not overly influenced by the choice of this parameter. </span><span class="koboSpan" id="kobo.732.3">Consequently, it is sufficient to choose a sufficiently large number of trees to obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">satisfactory results.</span></span></p>
<p><span class="koboSpan" id="kobo.734.1">In summary, bagging </span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.735.1">is an ensemble learning approach that’s based on repeatedly sampling training</span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.736.1"> data and combining weak learner predictions and is particularly effective when working with basic models that are sensitive to variations in </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">the data.</span></span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.738.1">Discovering random forest</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.739.1">Random forest</span></strong><span class="koboSpan" id="kobo.740.1"> is a special </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.741.1">case of bagging, with the basic aim of reducing variance by averaging over </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.742.1">many models. </span><span class="koboSpan" id="kobo.742.2">Again, models based on decision trees are often the ideal choice as they can effectively capture the complex interaction structures in the data, which can be difficult to achieve with other approaches. </span><span class="koboSpan" id="kobo.742.3">By generating a large number of trees, bias tends to </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">be low.</span></span></p>
<p><span class="koboSpan" id="kobo.744.1">Although the conceptual basis of both techniques is similar, random forest can produce superior results with the correct adjustment of model parameters. </span><span class="koboSpan" id="kobo.744.2">This is due to its ensemble learning approach and the ability to tune its hyperparameters effectively. </span><span class="koboSpan" id="kobo.744.3">This is because, although the trees have errors, the use of bagging allows trees to be generated from an identical distribution so that the expected value of each tree is equal to the expected value of the mean of the </span><em class="italic"><span class="koboSpan" id="kobo.745.1">N</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.746.1">trees generated.</span></span></p>
<p><span class="koboSpan" id="kobo.747.1">The bias of bagged models is similar to that of individual trees, so improvement focuses on reducing variance. </span><span class="koboSpan" id="kobo.747.2">This is the distinguishing feature of random forest models, which aim to improve the variance of bagging by reducing the correlation </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">between trees.</span></span></p>
<p><span class="koboSpan" id="kobo.749.1">For </span><em class="italic"><span class="koboSpan" id="kobo.750.1">N</span></em><span class="koboSpan" id="kobo.751.1"> identically distributed random variables, each with variance, </span><em class="italic"><span class="koboSpan" id="kobo.752.1">σ2</span></em><span class="koboSpan" id="kobo.753.1">, and with variance of the mean over </span><em class="italic"><span class="koboSpan" id="kobo.754.1">N</span></em><span class="koboSpan" id="kobo.755.1"> variables equal to </span><em class="italic"><span class="koboSpan" id="kobo.756.1">(1/N)σ2</span></em><span class="koboSpan" id="kobo.757.1">, if the variables are identically distributed but not necessarily independent, and a positive pairwise correlation, ρ, is assumed, the variance of the mean is expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.759.1">V</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.760.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.761.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.762.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.763.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.764.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.765.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.766.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.767.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.768.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.769.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.770.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.771.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.772.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.773.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.774.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.775.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.776.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.777.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.778.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.779.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.780.1">N</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.781.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.782.1">σ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.783.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.784.1">2</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.785.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.786.1">[</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.787.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.788.1">N</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.789.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.790.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.791.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.792.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.793.1">N</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.794.1">]</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.795.1">ρ</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.796.1">σ</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.797.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.798.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.799.1">In this formula, the first term represents the variance of the individual random variables divided by </span><em class="italic"><span class="koboSpan" id="kobo.800.1">N</span></em><span class="koboSpan" id="kobo.801.1">, which represents the variance of the mean when the variables are independent. </span><span class="koboSpan" id="kobo.801.2">The second term considers the pairwise correlation between the random variables, increasing the variance of the mean according to how correlated </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">they are.</span></span></p>
<p><span class="koboSpan" id="kobo.803.1">The technique that’s used by random forest is to reduce the correlation between the trees, and hence the variance is to grow the trees by randomly selecting the input variables. </span><span class="koboSpan" id="kobo.803.2">The main difference with bagging is that random forest selects fewer candidate variables for partitioning than the total number of </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">variables available.</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">Random</span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.806.1"> forest is a method that combines a series of decision trees to produce</span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.807.1"> more accurate results. </span><span class="koboSpan" id="kobo.807.2">These random decision trees are constructed while taking into account two </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">key concepts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.809.1">Random sampling of the training data</span></strong><span class="koboSpan" id="kobo.810.1">: During the training phase, each tree in a random forest learns by</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.811.1"> focusing only on a random sample of the original data. </span><span class="koboSpan" id="kobo.811.2">These samples are extracted with replacement, a technique known as bootstrapping, which means that some data can be used multiple times within the same tree. </span><span class="koboSpan" id="kobo.811.3">While this theoretically produces a high variance in each tree compared to training on a specific dataset, the overall forest (the set of trees) tends to have a lower variance without a significant increase </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">in bias.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.813.1">Random feature sets for partition choice</span></strong><span class="koboSpan" id="kobo.814.1">: In random forest, only a subset of the available features is </span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.815.1">considered for partitioning the nodes in each decision tree. </span><span class="koboSpan" id="kobo.815.2">Typically, this subset is the square root of the total number of features. </span><span class="koboSpan" id="kobo.815.3">For example, if there are 16 features, only four random features are considered at each node in each decision tree to perform the partitioning. </span><span class="koboSpan" id="kobo.815.4">Of course, it is also possible to train trees by considering all the features at each node, as you would typically do </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">in regression.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.817.1">In summary, random forest combines hundreds or thousands of decision trees, each trained on a slightly different set of data and using only a limited number of features for subdivisions. </span><span class="koboSpan" id="kobo.817.2">The final predictions of random forest are obtained by averaging the predictions of each decision tree, which means that each tree contributes equally to the </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">final decision.</span></span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.819.1">Boosting algorithms explained</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.820.1">Boosting</span></strong><span class="koboSpan" id="kobo.821.1"> is </span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.822.1">another</span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.823.1"> approach that can be applied to decision trees to improve their predictive ability. </span><span class="koboSpan" id="kobo.823.2">Like bagging, boosting can be applied to both regression and classification </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">statistical models.</span></span></p>
<p><span class="koboSpan" id="kobo.825.1">Both Bagging and boosting can be applied to decision trees to predict their abilities. </span><span class="koboSpan" id="kobo.825.2">However, there are some differences. </span><span class="koboSpan" id="kobo.825.3">The crucial aspect to focus on in bagging is the independence of each tree generated by a subset from </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">the others.</span></span></p>
<p><span class="koboSpan" id="kobo.827.1">Boosting, however, takes a different approach. </span><span class="koboSpan" id="kobo.827.2">Instead of generating completely independent trees as in bagging, boosting trains a sequence of trees iteratively. </span><span class="koboSpan" id="kobo.827.3">Each successive tree tries to correct the errors of the previous ones. </span><span class="koboSpan" id="kobo.827.4">In other words, the subsets of data used to train successive trees are weighted according to the erroneous predictions of the previous trees. </span><span class="koboSpan" id="kobo.827.5">This continuous process aims to progressively improve the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">overall model.</span></span></p>
<p><span class="koboSpan" id="kobo.829.1">In summary, while bagging is based on creating independent trees from subsets of data, boosting focuses on the iterative training of trees that correct the errors of their predecessors. </span><span class="koboSpan" id="kobo.829.2">Both are used to improve the predictive capabilities of decision trees, but they take </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">different approaches.</span></span></p>
<p><span class="koboSpan" id="kobo.831.1">The special feature of boosting is the sequential construction of decision trees – that is, each tree is constructed based on the information processed and obtained from the </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">previous tree.</span></span></p>
<p><span class="koboSpan" id="kobo.833.1">Unlike other techniques that attempt to model a decision tree on the entire dataset, or at least a training set, the goal of boosting is to obtain a predictive model with incremental learning. </span><span class="koboSpan" id="kobo.833.2">We say stepwise learning because a first model is built on the data of the training set. </span><span class="koboSpan" id="kobo.833.3">A second model is then built that attempts to correct the errors of the first model. </span><span class="koboSpan" id="kobo.833.4">Rather than building a new model based on the previous </span><em class="italic"><span class="koboSpan" id="kobo.834.1">Y</span></em><span class="koboSpan" id="kobo.835.1">-prediction, the approach is to build the next tree by trying to model the errors that were obtained as best as possible. </span><span class="koboSpan" id="kobo.835.2">The idea behind this is to create sequential weak learners (weak models), each of which tries to correct the errors of the previous model. </span><span class="koboSpan" id="kobo.835.3">In boosting, each subsequent tree in the ensemble is trained on the residuals of the previous tree, which are the differences between the actual and predicted values. </span><span class="koboSpan" id="kobo.835.4">This approach allows each tree to focus on correcting the errors of the previous trees, leading to improved </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">overall accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.837.1">Each of these trees may have as few final nodes as possible, even though the initial dataset is large. </span><span class="koboSpan" id="kobo.837.2">This may be due to the parameters of the algorithm or because smaller trees may perform better in correcting the errors of the </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">previous tree.</span></span></p>
<p><span class="koboSpan" id="kobo.839.1">The goal, tree </span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.840.1">by tree, is to improve the final model in the areas where it is not </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">performing best.</span></span></p>
<p><span class="koboSpan" id="kobo.842.1">Boosting offers three </span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.843.1">parameters that can be manipulated to optimize the modeling of </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">decision trees:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.845.1">Number of trees</span></strong><span class="koboSpan" id="kobo.846.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.847.1">T</span></strong><span class="koboSpan" id="kobo.848.1">): As with bagging techniques, increasing the number of trees (</span><em class="italic"><span class="koboSpan" id="kobo.849.1">T</span></em><span class="koboSpan" id="kobo.850.1">) generally leads to greater prediction accuracy. </span><span class="koboSpan" id="kobo.850.2">However, it is important to note that too high a value of </span><em class="italic"><span class="koboSpan" id="kobo.851.1">T</span></em><span class="koboSpan" id="kobo.852.1"> can increase the risk of overfitting the model. </span><span class="koboSpan" id="kobo.852.2">For example, compared to random forest, boosting tends to be more sensitive to overfitting at high values </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.854.1">T</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.856.1">Learning rate</span></strong><span class="koboSpan" id="kobo.857.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.858.1">λ</span></strong><span class="koboSpan" id="kobo.859.1">): This parameter indicates the learning rate of the tree and usually has a low value, typically between 0.01 and 0.001. </span><span class="koboSpan" id="kobo.859.2">The value of λ has a huge impact on the results of the model. </span><span class="koboSpan" id="kobo.859.3">A very low value of λ requires a larger number of trees (</span><em class="italic"><span class="koboSpan" id="kobo.860.1">T</span></em><span class="koboSpan" id="kobo.861.1">) to obtain a well-performing model. </span><span class="koboSpan" id="kobo.861.2">The choice of the optimal value of λ can significantly influence the behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.863.1">Number of splits</span></strong><span class="koboSpan" id="kobo.864.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.865.1">s</span></strong><span class="koboSpan" id="kobo.866.1">): </span><em class="italic"><span class="koboSpan" id="kobo.867.1">s</span></em><span class="koboSpan" id="kobo.868.1"> indicates the number of splits and has a value of one by default, which means that you get “stumps” or trees with only one parent node and two leaf nodes. </span><span class="koboSpan" id="kobo.868.2">Although a value of 1 for </span><em class="italic"><span class="koboSpan" id="kobo.869.1">s</span></em><span class="koboSpan" id="kobo.870.1"> is often used to speed up the calculation, changing it can significantly affect the model results. </span><span class="koboSpan" id="kobo.870.2">By changing the value of </span><em class="italic"><span class="koboSpan" id="kobo.871.1">s</span></em><span class="koboSpan" id="kobo.872.1">, it is possible to obtain trees with different structures </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">and complexities.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.874.1">There are several boosting techniques, each with variations in how trees are constructed or weighted to improve overall predictive ability. </span><span class="koboSpan" id="kobo.874.2">Some of the main boosting techniques are </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.876.1">Adaptive Boosting</span></strong><span class="koboSpan" id="kobo.877.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.878.1">AdaBoost</span></strong><span class="koboSpan" id="kobo.879.1">): AdaBoost is one of the earliest boosting algorithms and </span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.880.1">one of the most </span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.881.1">popular. </span><span class="koboSpan" id="kobo.881.2">In this approach, each training sample is given an initial weight, and subsequent iterations focus on correcting errors made in previous iterations. </span><span class="koboSpan" id="kobo.881.3">Weak learning trees, often stumps, are built based on these weights. </span><span class="koboSpan" id="kobo.881.4">AdaBoost gives more weight to past errors, allowing subsequent trees to focus on improving predictions for previously </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">misclassified examples.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.883.1">Gradient boosting</span></strong><span class="koboSpan" id="kobo.884.1">: Gradient boosting </span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.885.1">is a generic boosting framework that </span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.886.1">can be implemented in different ways. </span><span class="koboSpan" id="kobo.886.2">In gradient boosting, trees are built sequentially, but unlike AdaBoost, each tree tries to reduce the gradient of the loss function. </span><span class="koboSpan" id="kobo.886.3">This means that each successive tree focuses on examples that were misclassified or mispredicted by the model, gradually </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">improving performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.888.1">eXtreme Gradient Boosting</span></strong><span class="koboSpan" id="kobo.889.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.890.1">XGBoost</span></strong><span class="koboSpan" id="kobo.891.1">): XGBoost</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.892.1"> is an advanced </span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.893.1">implementation of gradient boosting that has gained popularity in ML competitions. </span><span class="koboSpan" id="kobo.893.2">It introduces several optimizations and regularizations to improve model performance and stability. </span><span class="koboSpan" id="kobo.893.3">XGBoost is known for its speed and can be used for both classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.894.1">regression problems.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.895.1">Each of these boosting techniques has its characteristics and specific adaptations for different data types and ML problems. </span><span class="koboSpan" id="kobo.895.2">The choice of boosting algorithm will depend on the specific needs of your problem and the </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">desired performance.</span></span></p>
<h1 id="_idParaDest-162"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.897.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.898.1">In this chapter, we studied NLP, which automatically processes information that’s transmitted through spoken or written language. </span><span class="koboSpan" id="kobo.898.2">To begin, we analyzed the basic concepts of NLP by identifying the tasks that can be tackled and then moved on to the main approaches concerning text analysis and text generation. </span><span class="koboSpan" id="kobo.898.3">We then moved on to analyze corpora, words, and sentence tokenization. </span><span class="koboSpan" id="kobo.898.4">Corpora offers authentic language data for examination, with words serving as the fundamental components of expression, and sentence tokenization organizing the text into coherent units for </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">in-depth analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.900.1">In the second part of this chapter, we analyzed a practical case of using NLP for labeling movie reviews. </span><span class="koboSpan" id="kobo.900.2">This is a sentiment analysis problem that aims to automatically identify the polarity of a textual comment. </span><span class="koboSpan" id="kobo.900.3">In this example, we were able to practically learn which tools to use in MATLAB to perform this type of analysis. </span><span class="koboSpan" id="kobo.900.4">In the final part of this chapter, we analyzed ensemble learning techniques to improve the performance of the algorithms. </span><span class="koboSpan" id="kobo.900.5">We understood the differences between boosting and bagging and discovered the different </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">boosting techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.902.1">In the next chapter, we will understand the basic concepts of image processing and computer vision. </span><span class="koboSpan" id="kobo.902.2">We will discover the MATLAB tools for computer vision and how to implement a MATLAB model for object recognition. </span><span class="koboSpan" id="kobo.902.3">We will also understand transfer learning, domain adaptation, multi-task learning and saliency maps, feature importance scores, and gradient-based </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">attribution methods.</span></span></p>
</div>
</body></html>