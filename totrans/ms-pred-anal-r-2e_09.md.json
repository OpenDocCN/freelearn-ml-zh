["```py\n> baggedtree <- bagging(LeagueIndex ~ ., data = skillcraft_train,    \n                        nbagg = 100, coob = T)\n> baggedtree_predictions <- predict(baggedtree, skillcraft_test)\n> (baggedtree_SSE <- compute_SSE(baggedtree_predictions, \n                                 skillcraft_test$LeagueIndex))\n[1] 646.3555\n```", "```py\n> M <- 11\n> seeds <- 70000 : (70000 + M - 1)\n> n <- nrow(heart_train)\n> sample_vectors <- sapply(seeds, function(x) { set.seed(x); \n  return(sample(n, n, replace = T)) })\n```", "```py\n train_1glm <- function(sample_indices) { \n     data <- heart_train[sample_indices,]; \n     model <- glm(OUTPUT ~ ., data = data, family = binomial(\"logit\")); \n     return(model)\n }\n\n> models <- apply(sample_vectors, 2, train_1glm)\n```", "```py\n get_1bag <- function(sample_indices) {\n     unique_sample <- unique(sample_indices); \n     df <- heart_train[unique_sample, ]; \n     df$ID <- unique_sample; \n     return(df)\n }\n\n> bags <- apply(sample_vectors, 2, get_1bag)\n```", "```py\n glm_predictions <- function(model, data, model_index) {\n     colname <- paste(\"PREDICTIONS\", model_index);\n     data[colname] <- as.numeric( \n                      predict(model, data, type = \"response\") > 0.5); \n     return(data[,c(\"ID\", colname), drop = FALSE])\n }\n\n> training_predictions <- \n     mapply(glm_predictions, models, bags, 1 : M, SIMPLIFY = F)\n```", "```py\n> train_pred_df <- Reduce(function(x, y) merge(x, y, by = \"ID\",  \n                          all = T), training_predictions)\n```", "```py\n> head(training_prediction_df[, 1:5])\n  ID PREDICTIONS 1 PREDICTIONS 2 PREDICTIONS 3 PREDICTIONS 4\n1  1             1            NA             1            NA\n2  2             0            NA            NA             0\n3  3            NA             0             0            NA\n4  4            NA             1             1             1\n5  5             0             0             0            NA\n6  6             0             1             0             0\n```", "```py\n> train_pred_vote <- apply(train_pred_df[,-1], 1, \n                function(x) as.numeric(mean(x, na.rm = TRUE) > 0.5))\n> (training_accuracy <- mean(train_pred_vote == \n                heart_train$OUTPUT[as.numeric(train_pred_df$ID)]))\n[1] 0.9173913\n```", "```py\n get_1oo_bag <- function(sample_indices) {\n     unique_sample <- setdiff(1 : n, unique(sample_indices)); \n     df <- heart_train[unique_sample,]; \n     df$ID <- unique_sample; \n     if (length(unique(heart_train[sample_indices,]$ECG)) < 3) \n         df[df$ECG == 1,\"ECG\"] = NA; \n     return(df)\n }\n\n> oo_bags <- apply(sample_vectors, 2, get_1oo_bag)\n```", "```py\n> oob_predictions <- mapply(glm_predictions, models, oo_bags, \n                            1 : M, SIMPLIFY = F)\n> oob_pred_df <- Reduce(function(x, y) merge(x, y, by = \"ID\", \n                        all = T), oob_predictions)\n> oob_pred_vote <- apply(oob_pred_df[,-1], 1, \n                   function(x) as.numeric(mean(x, na.rm = TRUE) > 0.5))\n\n> (oob_accuracy <- mean(oob_pred_vote == \n             heart_train$OUTPUT[as.numeric(oob_pred_df$ID)], \n             na.rm = TRUE))\n[1] 0.8515284\n```", "```py\n get_1test_bag <- function(sample_indices) {\n     df <- heart_test; \n     df$ID <- row.names(df); \n     if (length(unique(heart_train[sample_indices,]$ECG)) < 3) \n         df[df$ECG == 1,\"ECG\"] = NA; \n     return(df)\n }\n\n> test_bags <- apply(sample_vectors, 2, get_1test_bag)\n> test_predictions <- mapply(glm_predictions, models, test_bags, \n                             1 : M, SIMPLIFY = F)\n> test_pred_df <- Reduce(function(x, y) merge(x, y, by = \"ID\",\n                         all = T), test_predictions)\n> test_pred_vote <- apply(test_pred_df[,-1], 1, \n              function(x) as.numeric(mean(x, na.rm = TRUE) > 0.5))\n> (test_accuracy <- mean(test_pred_vote == \n              heart_test[test_pred_df$ID,\"OUTPUT\"], na.rm = TRUE))\n[1] 0.8\n```", "```py\n> heart_bagger_df\n     M Training Accuracy Out-of-bag Accuracy Test Accuracy\n1   11         0.9173913           0.8515284         0.800\n2   51         0.9130435           0.8521739         0.800\n3  101         0.9173913           0.8478261         0.800\n4  501         0.9086957           0.8521739         0.775\n5 1001         0.9130435           0.8565217         0.775\n```", "```py\n> magic <- read.csv(\"magic04.data\", header = FALSE)\n> names(magic) <- c(\"FLENGTH\", \"FWIDTH\", \"FSIZE\", \"FCONC\", \"FCONC1\",\n  \"FASYM\", \"FM3LONG\", \"FM3TRANS\", \"FALPHA\", \"FDIST\", \"CLASS\")\n> magic$CLASS <- as.factor(ifelse(magic$CLASS =='g', 1, -1))\n```", "```py\n> library(caret)\n> set.seed(33711209)\n> magic_sampling_vector <- createDataPartition(magic$CLASS, \n                             p = 0.80, list = FALSE)\n> magic_train <- magic[magic_sampling_vector, 1:10]\n> magic_train_output <- magic[magic_sampling_vector, 11]\n> magic_test <- magic[-magic_sampling_vector, 1:10]\n> magic_test_output <- magic[-magic_sampling_vector, 11]\n```", "```py\n> magic_pp <- preProcess(magic_train, method = c(\"center\", \n                                                 \"scale\"))\n> magic_train_pp <- predict(magic_pp, magic_train)\n> magic_train_df_pp <- cbind(magic_train_pp, \n                             CLASS = magic_train_output)\n> magic_test_pp <- predict(magic_pp, magic_test)\n```", "```py\n> library(nnet)\n> n_model <- nnet(CLASS ~ ., data = magic_train_df_pp, size = 1)\n> n_test_predictions <- predict(n_model, magic_test_pp,\n                                type = \"class\")\n> (n_test_accuracy <- mean(n_test_predictions ==  \n                           magic_test_output))\n[1] 0.7948988\n```", "```py\nAdaBoostNN <- function(training_data, output_column, M,  \n                       hidden_units) {\n  require(\"nnet\")\n  models <- list()\n  alphas <- list()\n  n <- nrow(training_data)\n  model_formula <- as.formula(paste(output_column, '~ .', sep = ''))\n  w <- rep((1/n), n)\n  for (m in 1:M) {\n    model <- nnet(model_formula, data = training_data, \n                size = hidden_units, weights = w)\n    models[[m]] <- model\n    predictions <- as.numeric(predict(model, \n                    training_data[, -which(names(training_data) ==\n                    output_column)], type = \"class\"))\n    errors <- predictions != training_data[, output_column]\n    error_rate <- sum(w * as.numeric(errors)) / sum(w)\n    alpha <- 0.5 * log((1 - error_rate) / error_rate)\n    alphas[[m]] <- alpha\n    temp_w <- mapply(function(x, y) if (y) { x * exp(alpha) } \n                    else { x * exp(-alpha)}, w, errors)\n    w <- temp_w / sum(temp_w)\n  }\n  return(list(models = models, alphas = unlist(alphas)))\n}\n```", "```py\nAdaBoostNN.predict <- function(ada_model, test_data) {\n  models <- ada_model$models\n  alphas <- ada_model$alphas\n  prediction_matrix <- sapply(models, function (x) \n             as.numeric(predict(x, test_data, type = \"class\")))\n  weighted_predictions <- t(apply(prediction_matrix, 1, \n             function(x) mapply(function(y, z) y * z, x, alphas)))\n  final_predictions <- apply(weighted_predictions, 1, function(x) sign(sum(x)))\n  return(final_predictions)\n}\n```", "```py\n> ada_model <- AdaBoostNN(magic_train_df_pp, 'CLASS', 10, 1)\n> predictions <- AdaBoostNN.predict(ada_model, magic_test_pp, \n                                    'CLASS')\n> mean(predictions == magic_test_output)\n [1] 0.804365\n```", "```py\n> boostedtree <- gbm(LeagueIndex ~ ., data = skillcraft_train, \n  distribution = \"gaussian\", n.trees = 10000, shrinkage = 0.1)\n```", "```py\n> best.iter <- gbm.perf(boostedtree, method = \"OOB\")\n> boostedtree_predictions <- predict(boostedtree, \n                                     skillcraft_test, best.iter)\n> (boostedtree_SSE <- compute_SSE(boostedtree_predictions, \n                                  skillcraft_test$LeagueIndex))\n[1] 555.2997\n```", "```py\n> library(\"randomForest\")\n> library(\"e1071\")\n> rf_ranges <- list(ntree = c(500, 1000, 1500, 2000), mtry = 3:8)\n> rf_tune <- tune(randomForest, LeagueIndex ~ ., data = \n                  skillcraft_train, ranges = rf_ranges)\n> rf_tune$best.parameters\n   ntree mtry\n14  1000    6\n> rf_best <- rf_tune$best.model\n> rf_best_predictions <- predict(rf_best, skillcraft_test)\n> (rf_best_SSE <- compute_SSE(rf_best_predictions, \n                              skillcraft_test$LeagueIndex))\n[1] 555.7611\n```"]