- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Processing with dplyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we covered the basics of the R language itself. Grasping
    these fundamentals will help us better tackle the challenges in the most common
    task in data science projects: **data processing**. Data processing refers to
    a series of data wrangling and massaging steps that transform the data into its
    intended format for downstream analysis and modeling. We can consider it as a
    function that accepts the raw data and outputs the desired data. However, we need
    to explicitly specify how the function executes the cooking recipe and processes
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to perform common data wrangling
    steps such as filtering, selection, grouping, and aggregation using `dplyr`, one
    of the most widely used data processing libraries in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing `tidyverse` and `dplyr`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation with `dplyr`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data aggregation with `dplyr`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data merging with `dplyr`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study – working with the Stack Overflow dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the exercises in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of the `tidyverse` package, which is 1.3.1 at the time of
    writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing tidyverse and dplyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most widely used R libraries that contains a set of individual packages
    is `tidyverse`; it includes `dplyr` and `ggplot2` (to be covered in [*Chapter
    4*](B18680_04.xhtml#_idTextAnchor077)). It can support most data processing and
    visualization needs and comes with an easy and fast implementation compared to
    base R commands. Therefore, it is recommended to outsource a specific data processing
    or visualization task to `tidyverse` instead of implementing it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into the world of data processing, there is one more data structure
    that’s used in the ecosystem of `tidyverse`: `tibble`. A `tibble` is an advanced
    version of a DataFrame and offers much better format control, leading to clean
    expressions in code. It is the central data structure in `tidyverse`. A DataFrame
    can be converted into a `tibble` object and vice versa. Let’s go through an exercise
    on this.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.01 – converting between tibble and a DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will explore the `tidyverse` ecosystem by installing this package
    and then converting the `iris` DataFrame into `tibble` format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `tidyverse` package and load the `dplyr` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Installing the `tidyverse` package will automatically install `dplyr`, which
    can be loaded into our working environment via the `library()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the `iris` dataset and check its data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `data()` function loads the `iris` dataset, a default dataset provided by
    base R, and a DataFrame that’s checked using the `class()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert the dataset into `tibble` format and verify its data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are three class attributes in `iris_tbl`, which means that the object
    can be used as both a `tibble` and a DataFrame. Having multiple class attributes
    in one object supports better compatibility since we can treat it differently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A `tibble` object also supports smart printing by listing the top few rows,
    the shape of the dataset (150 rows and 5 columns), and the data type of each column.
    On the other hand, a DataFrame would just display all its contents to the console
    when printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Multiple utility functions for data transformation are provided by `tidyverse`
    and `dplyr`. Let’s look at a few commonly used functions, such as `filter()` and
    `arrange()`.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation with dplyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`dplyr` functions. In this section, we will cover five fundamental functions
    for data transformation: `filter()`, `arrange()`, `mutate()`, `select()`, and
    `top_n()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Slicing the dataset using the filter() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the biggest highlights of the `tidyverse` ecosystem is the `%>%`, which
    provides the statement before it as the contextual input for the statement after
    it. Using the pipe operator gives us better clarity in terms of code structuring,
    besides saving the need to type multiple repeated contextual statements. Let’s
    go through an exercise on how to use the pipe operator to slice the `iris` dataset
    using the `filter()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.02 – filtering using the pipe operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this exercise, we have been asked to keep only the `setosa` species in
    the `iris` dataset using the pipe operator and the `filter()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Print all unique species in the `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the `Species` column is a factor with three levels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Keep only the `"setosa"` species in `iris_tbl` using `filter()` and save the
    result in `iris_tbl_subset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pipe operator indicates that the following filtering operation is applied
    to the `iris_tbl` object. Given this context, we could directly reference the
    `Species` column (instead of using `iris_tbl$Species`) and use the `==` logical
    operator to set the equality condition to be evaluated row-wise. The result shows
    a total of 50 rows stored in `iris_tbl_subset`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To double-check the filtering result, we could print out the unique species
    in `iris_tbl_subset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the dataset only contains the `"setosa"` species. However, the `Species`
    column still encodes the previous information as a factor that has three levels.
    This is a unique feature for the factor data type, where information about the
    total levels is encoded in all individual elements of a factor-typed column. We
    can remove such information by converting it into a character, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are chaining together two functions that are evaluated from the
    innermost `as.character()` to the outermost `unique()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `filter()` function makes it easy to add multiple filtering conditions
    by separating them using a comma. For example, we can add another condition to
    set the maximum value of `Sepal.Length` as `5`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the maximum `Sepal.Length` is now `5` and there are `28`
    rows left out of the original 150.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how to sort a `tibble` object (or a DataFrame) based on
    a specific column(s).
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the dataset using the arrange() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common data transformation operation is sorting, which leads to a dataset
    with one or multiple columns arranged in increasing or decreasing order. This
    can be achieved via the `arrange()` function, which is provided by `dplyr`. Let’s
    go through an exercise to look at different ways of sorting a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.03 – sorting using the arrange() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at how to sort columns of a dataset in either
    ascending or descending order, as well as combine the sorting operation with filtering
    via the pipe operator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sort the `Sepal.Length` column of the `iris` dataset in ascending order using
    `arrange()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the `arrange()` function sorts the specific column in
    ascending order by default. Now, let’s look at how to sort in descending order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sort the same column in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Adding the `desc()` function to the column before passing in `arrange()`can
    flip the ordering and achieve sorting in descending order. We can also pass in
    multiple columns to sort them sequentially.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides this, the `arrange()` function can also be used together with other
    data processing steps, such as filtering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sort both `Sepal.Length` and `Sepal.Width` in descending order after keeping
    `Species` set to `"setosa"` only and `Sepal.Length` up to a maximum value of `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows a two-layer sorting, where for the same value of `Sepal.Length`,
    `Sepal.Width` is further sorted in descending order. These two sorting criteria
    are separated by a comma, just like separating multiple conditions in `filter()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, the pipe operator connects and evaluates multiple functions sequentially.
    In this case, we start by setting the context for working with `iris_tbl`, followed
    by filtering and then sorting, both of which are connected via the pipe operator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Adding or changing a column using the mutate() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `tibble` object or DataFrame essentially consists of multiple columns stored
    together as a list of lists. We may want to edit an existing column by changing
    its contents, type, or format; such editing could also make us end up with a new
    column appended to the original dataset. Column-level editing can be achieved
    via the `mutate()` function. Let’s go through an example of how to use this function
    in conjunction with other functions.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.04 – changing and adding columns using the mutate() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at how to change the type of an existing column
    and add a new column to support the filtering operation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the `Species` column to the `character` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used the `mutate()` function to change the type of `Species`, which
    is referenced directly within the context of the `iris_tbl` object via the pipe
    operator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a column called `ind` to indicate whether `Sepal.Width` is bigger than
    `Petal.Length`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result shows that we have added an indicator column with logical values.
    We can get the counts of `TRUE` and `FALSE` values via the `table()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Keep only rows whose `Sepal.Width` is bigger than `Petal.Length`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we are essentially performing a filtering operation, this two-step process
    of first creating an indicator column and then filtering could be combined into
    a single step by directly setting the filtering condition within the `filter()`
    function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is the same as the two-step approach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let’s cover the last commonly used utility function – `select()`.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting columns using the select() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `select()` function works by selecting the columns specified by the input
    argument, a vector of strings representing one or multiple columns. When using
    `select()` in the context of the pipe operator, it means that all the following
    statements are evaluated based on the selected columns. When the `select` statement
    comes last, it returns the selected columns as the output `tibble` object.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise on different ways of selecting columns from a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.05 – selecting columns using select()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at different ways of selecting columns from
    a `tibble` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the first three columns from the `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When you need to increase the number of columns to be selected, typing them
    one by one would become tedious. Another way to do this is to pass the first and
    last columns separated by a colon (`:`), as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This approach selects all columns contained between `Sepal.Length` and `Petal.Length`.
    Using a colon helps us select multiple consecutive columns in one shot. Besides,
    we can also combine it with other individual columns via the `c()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select columns that contain `"length"`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used the `contains()` function to perform a case-insensitive string
    match. Other utility functions that support string matching include `starts_with()`
    and `ends_with()`. Let’s look at an example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select columns that start with `"petal"`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will look at selecting the top rows using the `top_n()` function, which
    comes in handy when we want to examine a few rows after sorting the DataFrame
    based on a specific column.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the top rows using the top_n() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `top_n()` function can be helpful when we are interested in the top few
    observations for a particular column. It expects two input arguments: the number
    of top observations (implicitly sorted in descending order) returned and the specific
    column sorted. The mechanism would be the same if we were to sort a column in
    descending order using `arrange()` and return the top few rows using `head()`.
    Let’s try it out.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.06 – selecting the top rows using top_n()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will demonstrate how to use `top_n()` in combination with
    other verbs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Return the observation with the biggest `Sepal.Length`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that the result is a full row whose `Sepal.Length` is the biggest.
    This can also be achieved by explicitly sorting the dataset using this column
    and returning the first row, illustrated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also apply `top_n()` in a `group_by()` context, which aggregates the
    data into different groups. We will cover more details on data aggregation in
    the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Return the biggest `Sepal.Length` for each category of `Species`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also use the `max()` function to achieve the same purpose:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `summarize()` function compresses the dataset into one row (with the maximum
    `Sepal.Length`) for each group of `Species`. More on this later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Return the biggest `Sepal.Length` and its category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This example shows that we can use `top_n()` in multiple contexts together with
    other verbs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let’s combine the five verbs we’ve covered here.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the five verbs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The five utility functions we have covered so far can be combined, thus offering
    a flexible and concise way of processing data. Let’s go through an exercise that
    involves all five functions.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.07 – combining the five utility functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The example we’ll cover in this exercise is a somewhat contrived one so that
    all five verb functions can be used. In this exercise, we have been asked to find
    the average absolute difference between `Sepal.Length` and `Petal.Length` for
    the top 100 rows with the highest `Sepal.Length` and whose `Sepal.Width` is bigger
    than `Petal.Length`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When performing a complex query like this one, it is helpful to work through
    the requirements backward, starting from the conditions for subsetting the dataset
    and then working on the metric. In this case, we will start by sorting `Sepal.Length`
    in a descending order using the `arrange()` function and keep the top 100 rows
    using the `head()` function. Another filtering condition that uses the `filter()`
    function then comes in to retain the rows whose `Sepal.Width` is bigger than `Petal.Length`.
    Next, we must create a new column using the `mutate()` function to represent the
    absolute difference between `Sepal.Length` and `Petal.Length`. Finally, we must
    apply the `select()` function to focus on the new column and calculate its average.
    See the following code block for a detailed implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will look at additional two verbs: `rename()` and `transmute()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing other verbs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two other verbs are also commonly used: `rename()` and `transmute()`. The `rename()`
    function changes the name of a specific column. For example, when using the `count()`
    function, a column named `n` is automatically created. We can use `rename(Count
    = n)` within the pipe context to change its default name from `n` to `Count`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another way to change the column’s name. When selecting a column of
    a dataset, we can pass the same statement that we did to `rename()` to the `select()`
    function. For example, the following code shows selecting the `Sepal.Length` and
    `Sepal.Width` columns from the `iris` dataset while renaming the second column
    `Sepal_Width`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the `transmute()` function is a combination of `select()`
    and `mutate()`. It will return a subset of columns where some could be transformed.
    For example, suppose we would like to calculate the absolute between `Sepal.Length`
    and `Petal.Length` and return the result together with `Species`. We can achieve
    both tasks using `transmute()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Although these verbs could be used interchangeably, there are a few technical
    differences. As shown in *Figure 2**.1*, the `select()` function returns the specified
    columns without changing the value of these columns, which can be achieved via
    either `mutate()` or `transmutate()`. Both `mutate()` and `rename()` keep the
    original columns in the returned result when creating additional new columns,
    while `select()` and `transmute()` only return specified columns in the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Summarizing the four verbs in terms of their purposes and connections](img/B18680_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Summarizing the four verbs in terms of their purposes and connections
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to transform the data, we can go a step further to make
    it more interpretable and presentable via aggregation and summarization. We will
    cover different ways to aggregate the data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation with dplyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data aggregation** refers to a set of techniques that summarizes the dataset
    at an aggregate level and characterizes the original dataset at a higher level.
    Compared to data transformation, it operates at the row level for the input and
    the output.'
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered a few aggregation functions, such as calculating
    the mean of a column. This section will cover some of the most widely used aggregation
    functions provided by `dplyr`. We will start with the `count()` function, which
    returns the number of observations/rows for each category of the specified input
    column.
  prefs: []
  type: TYPE_NORMAL
- en: Counting observations using the count() function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `count()` function automatically groups the dataset into different categories
    according to the input argument and returns the number of observations for each
    category. The input argument could include one or more columns of the dataset.
    Let’s go through an exercise and apply it to the `iris` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.08 – counting observations by species
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise will use the `count()` function to get the number of observations
    for each unique species, followed by adding filtering conditions using the `filter()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Count the number of observations for each unique type of species in the `iris`
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is a `tibble` dataset with two columns, where the first column contains
    the unique category in `Species` and the second column (named `n` by default)
    is the corresponding count of rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s look at how to perform filtering before the counting operation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform the exact counting for those observations whose absolute difference
    between `Sepal.Length` and `Sepal.Width` is greater than that of `Petal.Length`
    and `Petal.Width`. Return the result in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we added a filtering condition to keep rows that meet the specified criterion
    before counting. We enabled the `sort` argument to arrange the results in descending
    order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `count()` function essentially combines two steps: grouping by each category
    of a specified column and then counting the number of observations. It turns out
    that we can achieve the same task using the `group_by()` and `summarize()` functions,
    as introduced in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating data via group_by() and summarize()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`count()` is a helpful way to aggregate data. However, it is a particular case
    of two more general aggregation functions, `group_by()` and `summarize()`, which
    are often used together. The `group_by()` function splits the original dataset
    into different groups according to one or more columns in the input argument,
    while the `summarize()` function summarizes and collapses all observations within
    a specific category into one metric, which could be the count of rows in the case
    of `count()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple summarization functions can be used in the `summarize()` function.
    Typical ones include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sum()`: Sums all observations of a particular group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean()`: Calculates the average of all observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median()`: Calculates the median of all observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max()`: Calculates the maximum of all observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min()`: Calculates the minimum of all observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through an exercise on calculating different summary statistics using
    `group_by()` and `summarize()`.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.09 – summarizing a dataset using group_by() and summarize()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise covers using the `group_by()` and `summarize()` functions to
    extract the count and mean statistics, combined with some of the verbs introduced
    earlier, including `filter()`, `mutate()`, and `arrange()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the count of observations for each unique type of `Species`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we used the `n()` function to get the number of observations
    and assigned the result to a column named `n`. The counting comes after grouping
    the observations based on the unique type of `Species`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the same filter as in the previous exercise and sort the result in descending
    order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code block, the filtering condition is applied first to limit the grouping
    operations to a subset of observations. In the `arrange()` function, we directly
    used the `n` column to sort in descending order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a logical column based on the same filter and perform a two-level grouping
    using `Species`. Then, create a logical column to calculate the average `Sepal.Length`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can put multiple categorical columns in the `group_by()` function to perform
    a multi-level grouping. Note that the result contains a `Groups` attribute based
    on `Species`, suggesting that the `tibble` object has a group structure. Let’s
    learn how to remove the structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove the group structure in the returned `tibble` object using `ungroup()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, the result contains a normal `tibble` object with the average sepal length
    for each unique combination of `Species` and `ind`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we know how to transform and aggregate one dataset, we will cover how
    to work with multiple datasets via merging and joining.
  prefs: []
  type: TYPE_NORMAL
- en: Data merging with dplyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practical data analysis, the information we need is not necessarily confined
    to one table but is spread across multiple tables. Storing data in separate tables
    is memory-efficient but not analysis-friendly. **Data merging** is the process
    of merging different datasets into one table to facilitate data analysis. When
    joining two tables, there need to be one or more columns, or **keys**, that exist
    in both tables and serve as the common ground for joining.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section will cover different ways to join tables and analyze them in combination,
    including inner join, left join, right join, and full join. The following list
    shows the verbs and their definitions for these four types of joining:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inner_join()`: Returns common observations in both tables according to the
    matching key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`left_join()`: Returns all observations from the left table and matched observations
    from the right table. Note that in the case of a duplicate key value in the right
    table, an additional row will be automatically created and added to the left table.
    Empty cells are filled with `NA`. More on this in the exercise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`right_join()`: Returns all observations from the right table and matched observations
    from the left table. Empty cells are filled with `NA`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`full_join()`: Returns all observations from both tables. Empty cells are filled
    with `NA`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.2* illustrates the four joins using Venn diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Four different joins commonly used in practice](img/B18680_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Four different joins commonly used in practice
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an exercise on these four joins.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.10 – joining datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This exercise will create two dummy `tibble` datasets and apply different joining
    verbs to merge them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create two dummy datasets by following the steps in this code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Both dummy datasets have three rows and two columns, with the first column being
    the key column to be used for joining.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform an inner join of the two datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code shows that the matching keys are specified in the `by` argument
    via the `c()` function. Since `"key_A"` and `"key_B"` have only two values in
    common, the resulting table after the inner join operation is a 2x3 `tibble` that
    keeps only `"key_A"` as the key column and all other non-key columns from both
    tables. It only keeps observations that have an exact match and works the same
    way with either table in either direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can also pass in additional match keys (on which the tables will be joined)
    in the `by` argument while following the same format to perform a multi-level
    merging.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s look at how to perform a left join.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a left join of the two datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the resulting tables contain the whole `tbl_A` and an additional column,
    `col_B`, that’s referenced from `tbl_B`. Since `col_B` does not have 1, the corresponding
    cell in `col_B` shows `NA`. In general, any cell that cannot be matched will assume
    a value of `NA` in the resulting table.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that when there are multiple rows with duplicate values in `col_B`, the
    resulting table after a left join will also have a duplicate row automatically
    created since it is now a one-to-two mapping from left to right. Let’s see an
    example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create another table with a duplicate key value and perform a left join with
    `tbl_A`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we used the `bind_rows()` function to append a new row with the same
    value in `"key_B"` as the first row and a different value for `col_B`. Let’s see
    what happens when we join it to `tbl_A`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having a duplicate value in the key column of the right table is a common source
    of bugs that could be difficult to trace. Seasoned data scientists should pay
    particular attention to this potential unintended result by checking the dimension
    of the dataset *before* and *after* the left join. Now, let’s look at how to perform
    a right join.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a right join of `tbl_A` to `tbl_B`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Similarly, all observations in `tbl_B` are kept and the missing value in `col_A`
    is filled with `NA`. In addition, the key column is named `"key_A"` instead of
    `"key_B"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a full join of `tbl_A` and `tbl_B`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using a full join, all matched results from both tables are maintained, with
    missing values filled with `NA`. We can use this when we do not want to leave
    out any observations from the source tables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These four joining statements can be repeatedly used to join multiple tables
    and combined with any data transformation verbs covered earlier. For example,
    we can remove the rows with `NA` values and keep only the `complete` rows, which
    should give us the same result in the inner join. This can be achieved using `drop_na()`,
    a utility function provided by the `tidyr` package that’s designed specifically
    for data cleaning within the `tidyverse` ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We may also want to replace `NA` values with 0, which can be achieved via the
    `replace_na()` function provided by `tidyr`. In the following code, we are specifying
    the replacement values for each column of interest and wrapping them in a list
    to be passed into `replace_na()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that there are other merging options, such as the semi-join and the anti-join,
    which correspond to the `semi_join()` and `anti_join()` functions, respectively.
    Semi-join returns only the rows from the table in the first argument where there
    is a match in the second table. Although similar to a full join operation, a semi-join
    only keeps *the columns in the first table*. The anti-join operation, on the other
    hand, is the opposite of a semi-join, returning only *the unmatched rows in the
    first table*. Since many merging operations, including these two, can be derived
    using the four basic operations we introduced in this section, we will not cover
    these slightly more complicated joining operations in detail. Instead, we encourage
    you to explore using these four fundamental joining functions to achieve complicated
    operations instead of relying on other shortcut joining functions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will go through a case study and observe how to transform, merge, and
    aggregate a dataset using the functions we’ve covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – working with the Stack Overflow dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will cover an exercise to help you practice different data transformation,
    aggregation, and merging techniques based on the public Stack Overflow dataset,
    which contains a set of tables related to technical questions and answers posted
    on the Stack Overflow platform. The supporting raw data has been uploaded to the
    accompanying Github repository of this book. We will directly download it from
    the source GitHub link using the `readr` package, another `tidyverse` offering
    that provides an easy, fast, and friendly way to read a wide range of data sources,
    including those from the web.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.11 – working with the Stack Overflow dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s begin this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download three data sources on questions, tags, and their mapping table from
    GitHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The questions dataset contains the question ID, date of creation, and score,
    which indicates the number of (positive) upvotes and (negative) downvotes. We
    can examine the range of scores using the `summary()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This dataset contains the ID and the content of each tag. To analyze the tags,
    we need to merge the three datasets into one using the relevant mapping keys.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reference the tag ID from `df_question_tags` into `df_questions` via a left
    join:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the total number of rows almost doubled when comparing `df_questions`
    to `df_all`. You may have noticed that this is due to the one-to-many relationship:
    a question often has multiple tags, so each tag gets appended as a separate row
    to the left table during the left join operation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s continue to reference the tags from `df_tags`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will do some analysis of the tags, starting with counting their frequency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Count the occurrence of each non-`NA` tag in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first used `filter()` to remove rows if `tag_name` was `NA`, then calculated
    the counts using the `count()` function. The result shows that `dplyr` is among
    one of the most popular R-related tags on Stack Overflow, which is a good sign
    as it shows we are learning about something useful and trendy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Count the number of tags in each year:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result shows an increasing number of tags per year, with 2019 being a special
    case as the data ends in mid-2019 (verified here). Note that we used the `year()`
    function from the `lubricate` package from `tidyverse` to convert a date-formatted
    column into the corresponding year:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calculate the average occurrence of tags per month.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to derive the monthly occurrence of tags to calculate their average.
    First, we must create two columns to indicate the month and year-month for each
    tag:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must count the occurrence of tags per month for each year-month:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must average over all years for each month:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that March has the highest occurrence of tags on average. Maybe
    school just got started and people are actively learning and asking questions
    more in March.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the count, minimum, average score, and maximum score for each tag
    and sort the result by count in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used multiple summary functions in the `group_by()` and `summarize()`
    context to calculate the metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered essential functions and techniques for data transformation,
    aggregation, and merging. For data transformation at the row level, we learned
    about common utility functions such as `filter()`, `mutate()`, `select()`, `arrange()`,
    `top_n()`, and `transmute()`. For data aggregation, which summarizes the raw dataset
    into a smaller and more concise summary view, we introduced functions such as
    `count()`, `group_by()`, and `summarize()`. For data merging, which combines multiple
    datasets into one, we learned about different joining methods, including `inner_join()`,
    `left_join()`, `right_join()`, and `full_join()`. Although there are other more
    advanced joining functions, the essential tools we covered in our toolkit are
    enough for us to achieve the same task. Finally, we went through a case study
    based on the Stack Overflow dataset. The skills we learned in this chapter will
    come in very handy in many data analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover a more advanced topic on natural language
    processing, taking us one step further to work with textual data using `tidyverse`.
  prefs: []
  type: TYPE_NORMAL
