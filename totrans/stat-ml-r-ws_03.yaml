- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Data Processing with dplyr
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 dplyr 进行数据处理
- en: 'In the previous chapter, we covered the basics of the R language itself. Grasping
    these fundamentals will help us better tackle the challenges in the most common
    task in data science projects: **data processing**. Data processing refers to
    a series of data wrangling and massaging steps that transform the data into its
    intended format for downstream analysis and modeling. We can consider it as a
    function that accepts the raw data and outputs the desired data. However, we need
    to explicitly specify how the function executes the cooking recipe and processes
    the data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 R 语言的基础知识。掌握这些基础知识将帮助我们更好地应对数据科学项目中最常见的任务：**数据处理**。数据处理是指一系列数据整理和润色步骤，将数据转换为下游分析和建模所需的目标格式。我们可以将其视为一个接受原始数据并输出所需数据的函数。然而，我们需要明确指定函数如何执行烹饪食谱并处理数据。
- en: By the end of this chapter, you will be able to perform common data wrangling
    steps such as filtering, selection, grouping, and aggregation using `dplyr`, one
    of the most widely used data processing libraries in R.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用 `dplyr`（R 中最广泛使用的数据处理库之一）执行常见的数据处理步骤，如过滤、选择、分组和聚合。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing `tidyverse` and `dplyr`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 `tidyverse` 和 `dplyr`
- en: Data transformation with `dplyr`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dplyr` 进行数据转换
- en: Data aggregation with `dplyr`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dplyr` 进行数据聚合
- en: Data merging with `dplyr`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dplyr` 进行数据合并
- en: Case study – working with the Stack Overflow dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究 – 使用 Stack Overflow 数据集
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the exercises in this chapter, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的练习，你需要以下内容：
- en: The latest version of the `tidyverse` package, which is 1.3.1 at the time of
    writing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyverse` 包的最新版本，写作时为 1.3.1'
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有代码和数据均可在[https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/tree/main/Chapter_2)找到。
- en: Introducing tidyverse and dplyr
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 tidyverse 和 dplyr
- en: One of the most widely used R libraries that contains a set of individual packages
    is `tidyverse`; it includes `dplyr` and `ggplot2` (to be covered in [*Chapter
    4*](B18680_04.xhtml#_idTextAnchor077)). It can support most data processing and
    visualization needs and comes with an easy and fast implementation compared to
    base R commands. Therefore, it is recommended to outsource a specific data processing
    or visualization task to `tidyverse` instead of implementing it ourselves.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的包含一系列独立包的 R 库之一是 `tidyverse`；它包括 `dplyr` 和 `ggplot2`（将在[*第 4 章*](B18680_04.xhtml#_idTextAnchor077)中介绍）。它可以支持大多数数据处理和可视化需求，并且与
    base R 命令相比，实现起来既简单又快速。因此，建议将特定的数据处理或可视化任务外包给 `tidyverse` 而不是自行实现。
- en: 'Before we dive into the world of data processing, there is one more data structure
    that’s used in the ecosystem of `tidyverse`: `tibble`. A `tibble` is an advanced
    version of a DataFrame and offers much better format control, leading to clean
    expressions in code. It is the central data structure in `tidyverse`. A DataFrame
    can be converted into a `tibble` object and vice versa. Let’s go through an exercise
    on this.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入数据处理的世界之前，还有另一种在 `tidyverse` 生态系统中使用的数据结构：`tibble`。`tibble` 是 DataFrame
    的高级版本，提供了更好的格式控制，从而在代码中产生整洁的表达式。它是 `tidyverse` 中的核心数据结构。DataFrame 可以转换为 `tibble`
    对象，反之亦然。让我们通过一个练习来了解这一点。
- en: Exercise 2.01 – converting between tibble and a DataFrame
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.01 – 在 tibble 和 DataFrame 之间转换
- en: 'First, we will explore the `tidyverse` ecosystem by installing this package
    and then converting the `iris` DataFrame into `tibble` format:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过安装此包并将 `iris` DataFrame 转换为 `tibble` 格式来探索 `tidyverse` 生态系统：
- en: 'Install the `tidyverse` package and load the `dplyr` package:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `tidyverse` 包并加载 `dplyr` 包：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Installing the `tidyverse` package will automatically install `dplyr`, which
    can be loaded into our working environment via the `library()` function.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装 `tidyverse` 包将自动安装 `dplyr`，可以通过 `library()` 函数将其加载到我们的工作环境中。
- en: 'Load the `iris` dataset and check its data structure:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `iris` 数据集并检查其数据结构：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `data()` function loads the `iris` dataset, a default dataset provided by
    base R, and a DataFrame that’s checked using the `class()` function.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`data()`函数加载了`iris`数据集，这是由基础R提供的默认数据集，以及使用`class()`函数检查的DataFrame。'
- en: 'Convert the dataset into `tibble` format and verify its data structure:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集转换为`tibble`格式并验证其数据结构：
- en: '[PRE2]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are three class attributes in `iris_tbl`, which means that the object
    can be used as both a `tibble` and a DataFrame. Having multiple class attributes
    in one object supports better compatibility since we can treat it differently.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`iris_tbl`中有三个类属性，这意味着该对象可以用作`tibble`和DataFrame。一个对象具有多个类属性支持更好的兼容性，因为我们可以对其进行不同的处理。'
- en: 'A `tibble` object also supports smart printing by listing the top few rows,
    the shape of the dataset (150 rows and 5 columns), and the data type of each column.
    On the other hand, a DataFrame would just display all its contents to the console
    when printed:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tibble`对象也支持智能打印，通过列出前几行、数据集的形状（150行和5列）以及每列的数据类型。另一方面，DataFrame在打印时只会显示其所有内容到控制台：'
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Multiple utility functions for data transformation are provided by `tidyverse`
    and `dplyr`. Let’s look at a few commonly used functions, such as `filter()` and
    `arrange()`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`tidyverse`和`dplyr`提供了多个数据转换的实用函数。让我们看看一些常用的函数，例如`filter()`和`arrange()`。'
- en: Data transformation with dplyr
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dplyr进行数据转换
- en: '`dplyr` functions. In this section, we will cover five fundamental functions
    for data transformation: `filter()`, `arrange()`, `mutate()`, `select()`, and
    `top_n()`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`dplyr`函数。在本节中，我们将介绍五个基本的数据转换函数：`filter()`、`arrange()`、`mutate()`、`select()`和`top_n()`。'
- en: Slicing the dataset using the filter() function
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用filter()函数切片数据集
- en: One of the biggest highlights of the `tidyverse` ecosystem is the `%>%`, which
    provides the statement before it as the contextual input for the statement after
    it. Using the pipe operator gives us better clarity in terms of code structuring,
    besides saving the need to type multiple repeated contextual statements. Let’s
    go through an exercise on how to use the pipe operator to slice the `iris` dataset
    using the `filter()` function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`tidyverse`生态系统中最显著的亮点之一是`%>%`操作符，它将前面的语句作为上下文输入提供给后面的语句。使用管道操作符可以让我们在代码结构方面有更好的清晰度，同时避免了多次输入重复的上下文语句的需要。让我们通过一个练习来了解如何使用管道操作符通过`filter()`函数来切片`iris`数据集。'
- en: Exercise 2.02 – filtering using the pipe operator
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.02 – 使用管道操作符进行过滤
- en: 'For this exercise, we have been asked to keep only the `setosa` species in
    the `iris` dataset using the pipe operator and the `filter()` function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们被要求仅使用管道操作符和`filter()`函数保留`iris`数据集中的`setosa`物种：
- en: 'Print all unique species in the `iris` dataset:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`iris`数据集中的所有独特物种：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result shows that the `Species` column is a factor with three levels.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示`Species`列是一个具有三个级别的因子。
- en: 'Keep only the `"setosa"` species in `iris_tbl` using `filter()` and save the
    result in `iris_tbl_subset`:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`filter()`函数仅保留`iris_tbl`中的`"setosa"`物种并将结果保存在`iris_tbl_subset`中：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The pipe operator indicates that the following filtering operation is applied
    to the `iris_tbl` object. Given this context, we could directly reference the
    `Species` column (instead of using `iris_tbl$Species`) and use the `==` logical
    operator to set the equality condition to be evaluated row-wise. The result shows
    a total of 50 rows stored in `iris_tbl_subset`.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道操作符表示以下过滤操作应用于`iris_tbl`对象。在这种上下文中，我们可以直接引用`Species`列（而不是使用`iris_tbl$Species`），并使用`==`逻辑运算符设置逐行评估的相等条件。结果显示`iris_tbl_subset`中存储了总共50行。
- en: 'To double-check the filtering result, we could print out the unique species
    in `iris_tbl_subset`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了双重检查过滤结果，我们可以打印出`iris_tbl_subset`中的独特物种：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, the dataset only contains the `"setosa"` species. However, the `Species`
    column still encodes the previous information as a factor that has three levels.
    This is a unique feature for the factor data type, where information about the
    total levels is encoded in all individual elements of a factor-typed column. We
    can remove such information by converting it into a character, as follows:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据集只包含`"setosa"`物种。然而，`Species`列仍然将先前信息编码为具有三个级别的因子。这是因子数据类型的一个独特特性，其中关于总级别的信息编码在因子类型列的所有单个元素中。我们可以通过将其转换为字符来删除此类信息，如下所示：
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that we are chaining together two functions that are evaluated from the
    innermost `as.character()` to the outermost `unique()`.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们正在将两个函数链接在一起，从最内层的`as.character()`到最外层的`unique()`进行评估。
- en: 'The `filter()` function makes it easy to add multiple filtering conditions
    by separating them using a comma. For example, we can add another condition to
    set the maximum value of `Sepal.Length` as `5`, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()` 函数通过使用逗号分隔条件，可以轻松地添加多个过滤条件。例如，我们可以添加另一个条件将 `Sepal.Length` 的最大值设置为
    `5`，如下所示：'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The result shows that the maximum `Sepal.Length` is now `5` and there are `28`
    rows left out of the original 150.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，现在的最大 `Sepal.Length` 是 `5`，并且从原始的 150 行中留下了 `28` 行。
- en: Next, we will look at how to sort a `tibble` object (or a DataFrame) based on
    a specific column(s).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何根据特定列（或列）对 `tibble` 对象（或 DataFrame）进行排序。
- en: Sorting the dataset using the arrange() function
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 arrange() 函数对数据集进行排序
- en: Another common data transformation operation is sorting, which leads to a dataset
    with one or multiple columns arranged in increasing or decreasing order. This
    can be achieved via the `arrange()` function, which is provided by `dplyr`. Let’s
    go through an exercise to look at different ways of sorting a dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的数据转换操作是排序，这会导致一个数据集，其中一列或多列按升序或降序排列。这可以通过 `dplyr` 提供的 `arrange()` 函数实现。让我们通过一个练习来看看不同的排序数据集的方法。
- en: Exercise 2.03 – sorting using the arrange() function
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.03 – 使用 arrange() 函数进行排序
- en: 'In this exercise, we will look at how to sort columns of a dataset in either
    ascending or descending order, as well as combine the sorting operation with filtering
    via the pipe operator:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将探讨如何按升序或降序对数据集的列进行排序，以及如何通过管道操作符将排序操作与过滤结合：
- en: 'Sort the `Sepal.Length` column of the `iris` dataset in ascending order using
    `arrange()`:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `arrange()` 对 `iris` 数据集中的 `Sepal.Length` 列进行升序排序：
- en: '[PRE9]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The result shows that the `arrange()` function sorts the specific column in
    ascending order by default. Now, let’s look at how to sort in descending order.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，`arrange()` 函数默认按升序对特定列进行排序。现在，让我们看看如何按降序排序。
- en: 'Sort the same column in descending order:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以降序对相同的列进行排序：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Adding the `desc()` function to the column before passing in `arrange()`can
    flip the ordering and achieve sorting in descending order. We can also pass in
    multiple columns to sort them sequentially.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在将列传递给 `arrange()` 之前添加 `desc()` 函数可以反转排序顺序并实现降序排序。我们也可以传递多个列以按顺序排序它们。
- en: Besides this, the `arrange()` function can also be used together with other
    data processing steps, such as filtering.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，`arrange()` 函数还可以与其他数据处理步骤一起使用，例如过滤。
- en: 'Sort both `Sepal.Length` and `Sepal.Width` in descending order after keeping
    `Species` set to `"setosa"` only and `Sepal.Length` up to a maximum value of `5`:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将 `Species` 设置为 `"setosa"` 并将 `Sepal.Length` 限制在最大值 `5` 的情况下，按降序排序 `Sepal.Length`
    和 `Sepal.Width`：
- en: '[PRE11]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The result shows a two-layer sorting, where for the same value of `Sepal.Length`,
    `Sepal.Width` is further sorted in descending order. These two sorting criteria
    are separated by a comma, just like separating multiple conditions in `filter()`.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示了两层排序，其中对于相同的 `Sepal.Length` 值，`Sepal.Width` 进一步按降序排序。这两个排序标准由逗号分隔，就像在 `filter()`
    中分隔多个条件一样。
- en: In addition, the pipe operator connects and evaluates multiple functions sequentially.
    In this case, we start by setting the context for working with `iris_tbl`, followed
    by filtering and then sorting, both of which are connected via the pipe operator.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，管道操作符可以按顺序连接和评估多个函数。在这种情况下，我们首先为使用 `iris_tbl` 设置上下文，然后进行过滤和排序，这两个操作都通过管道操作符连接。
- en: Adding or changing a column using the mutate() function
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 mutate() 函数添加或更改列
- en: A `tibble` object or DataFrame essentially consists of multiple columns stored
    together as a list of lists. We may want to edit an existing column by changing
    its contents, type, or format; such editing could also make us end up with a new
    column appended to the original dataset. Column-level editing can be achieved
    via the `mutate()` function. Let’s go through an example of how to use this function
    in conjunction with other functions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`tibble` 对象或 DataFrame 实质上是由多个列组成的列表的列表。我们可能想要通过更改其内容、类型或格式来编辑现有列；这种编辑也可能导致在原始数据集中附加新列。列级编辑可以通过
    `mutate()` 函数实现。让我们通过一个示例来看看如何与其他函数结合使用此函数。'
- en: Exercise 2.04 – changing and adding columns using the mutate() function
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.04 – 使用 mutate() 函数更改和添加列
- en: 'In this exercise, we will look at how to change the type of an existing column
    and add a new column to support the filtering operation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将探讨如何更改现有列的类型并添加新列以支持过滤操作：
- en: 'Change the `Species` column to the `character` type:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Species`列改为`character`类型：
- en: '[PRE12]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we used the `mutate()` function to change the type of `Species`, which
    is referenced directly within the context of the `iris_tbl` object via the pipe
    operator.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`mutate()`函数来改变`Species`的类型，该类型通过管道操作符直接在`iris_tbl`对象上下文中引用。
- en: 'Create a column called `ind` to indicate whether `Sepal.Width` is bigger than
    `Petal.Length`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`ind`的列，以指示`Sepal.Width`是否大于`Petal.Length`：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The result shows that we have added an indicator column with logical values.
    We can get the counts of `TRUE` and `FALSE` values via the `table()` function:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，我们添加了一个包含逻辑值的指示列。我们可以通过`table()`函数获取`TRUE`和`FALSE`值的计数：
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Keep only rows whose `Sepal.Width` is bigger than `Petal.Length`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅保留`Sepal.Width`大于`Petal.Length`的行：
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since we are essentially performing a filtering operation, this two-step process
    of first creating an indicator column and then filtering could be combined into
    a single step by directly setting the filtering condition within the `filter()`
    function:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们本质上是在执行过滤操作，因此首先创建指示列然后过滤的两步过程可以通过在`filter()`函数中直接设置过滤条件合并为一步：
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result is the same as the two-step approach.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果与两步方法相同。
- en: Now, let’s cover the last commonly used utility function – `select()`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来介绍最后一个常用的实用函数——`select()`。
- en: Selecting columns using the select() function
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用select()函数选择列
- en: The `select()` function works by selecting the columns specified by the input
    argument, a vector of strings representing one or multiple columns. When using
    `select()` in the context of the pipe operator, it means that all the following
    statements are evaluated based on the selected columns. When the `select` statement
    comes last, it returns the selected columns as the output `tibble` object.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`select()`函数通过选择由输入参数指定的列来工作，该参数是一个表示一个或多个列的字符串向量。当在管道操作符的上下文中使用`select()`时，意味着所有后续语句都是基于所选列进行评估的。当`select`语句是最后一个时，它返回所选列作为输出`tibble`对象。'
- en: Let’s go through an exercise on different ways of selecting columns from a dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来了解从数据集中选择列的不同方法。
- en: Exercise 2.05 – selecting columns using select()
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.05 – 使用select()选择列
- en: 'In this exercise, we will look at different ways of selecting columns from
    a `tibble` dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将探讨从`tibble`数据集中选择列的不同方法：
- en: 'Select the first three columns from the `iris` dataset:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`iris`数据集中选择前三个列：
- en: '[PRE17]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When you need to increase the number of columns to be selected, typing them
    one by one would become tedious. Another way to do this is to pass the first and
    last columns separated by a colon (`:`), as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你需要增加要选择的列数时，逐个输入它们会变得繁琐。另一种方法是使用冒号（`:`）分隔首尾列，如下所示：
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This approach selects all columns contained between `Sepal.Length` and `Petal.Length`.
    Using a colon helps us select multiple consecutive columns in one shot. Besides,
    we can also combine it with other individual columns via the `c()` function.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法选择所有位于`Sepal.Length`和`Petal.Length`之间的列。使用冒号可以帮助我们一次性选择多个连续列。此外，我们还可以通过`c()`函数将其与其他单个列结合使用。
- en: 'Select columns that contain `"length"`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择包含`"length"`的列：
- en: '[PRE19]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we used the `contains()` function to perform a case-insensitive string
    match. Other utility functions that support string matching include `starts_with()`
    and `ends_with()`. Let’s look at an example.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`contains()`函数来执行不区分大小写的字符串匹配。支持字符串匹配的其他实用函数包括`starts_with()`和`ends_with()`。让我们看一个例子。
- en: 'Select columns that start with `"petal"`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择以`"petal"`开头的列：
- en: '[PRE20]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we will look at selecting the top rows using the `top_n()` function, which
    comes in handy when we want to examine a few rows after sorting the DataFrame
    based on a specific column.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用`top_n()`函数选择顶部行，这在根据特定列对DataFrame进行排序后想要检查几行时非常有用。
- en: Selecting the top rows using the top_n() function
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用top_n()函数选择顶部行
- en: 'The `top_n()` function can be helpful when we are interested in the top few
    observations for a particular column. It expects two input arguments: the number
    of top observations (implicitly sorted in descending order) returned and the specific
    column sorted. The mechanism would be the same if we were to sort a column in
    descending order using `arrange()` and return the top few rows using `head()`.
    Let’s try it out.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`top_n()` 函数在我们要关注特定列的前几个观测值时非常有用。它期望两个输入参数：返回的顶部观测值的数量（隐式按降序排序）和要排序的特定列。如果我们使用
    `arrange()` 对列进行降序排序并使用 `head()` 返回顶部几行，机制将是相同的。让我们试试看。'
- en: Exercise 2.06 – selecting the top rows using top_n()
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.06 – 使用 top_n() 选择顶部行
- en: 'In this exercise, we will demonstrate how to use `top_n()` in combination with
    other verbs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将演示如何将 `top_n()` 与其他动词结合使用：
- en: 'Return the observation with the biggest `Sepal.Length`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回具有最大 `Sepal.Length` 的观测值：
- en: '[PRE21]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that the result is a full row whose `Sepal.Length` is the biggest.
    This can also be achieved by explicitly sorting the dataset using this column
    and returning the first row, illustrated as follows:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，结果是包含最大 `Sepal.Length` 的完整行。这也可以通过显式使用此列对数据集进行排序并返回第一行来实现，如下所示：
- en: '[PRE22]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can also apply `top_n()` in a `group_by()` context, which aggregates the
    data into different groups. We will cover more details on data aggregation in
    the next section.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以在 `group_by()` 上下文中应用 `top_n()`，这会将数据聚合到不同的组中。我们将在下一节中详细介绍数据聚合的更多细节。
- en: 'Return the biggest `Sepal.Length` for each category of `Species`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回每个 `Species` 类别的最大 `Sepal.Length`：
- en: '[PRE23]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also use the `max()` function to achieve the same purpose:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以使用 `max()` 函数达到相同的目的：
- en: '[PRE24]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `summarize()` function compresses the dataset into one row (with the maximum
    `Sepal.Length`) for each group of `Species`. More on this later.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`summarize()` 函数将数据集压缩为每个 `Species` 组的一行（具有最大的 `Sepal.Length`）。关于这一点，我们稍后再谈。'
- en: 'Return the biggest `Sepal.Length` and its category:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回最大的 `Sepal.Length` 及其类别：
- en: '[PRE25]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This example shows that we can use `top_n()` in multiple contexts together with
    other verbs.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此示例表明，我们可以将 `top_n()` 与其他动词一起在多个上下文中使用。
- en: Now, let’s combine the five verbs we’ve covered here.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们结合我们在这里所涵盖的五个动词。
- en: Combining the five verbs
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合五个动词
- en: The five utility functions we have covered so far can be combined, thus offering
    a flexible and concise way of processing data. Let’s go through an exercise that
    involves all five functions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所涵盖的五个实用函数可以组合使用，从而提供一种灵活且简洁的数据处理方式。让我们通过一个涉及所有五个函数的练习来了解一下。
- en: Exercise 2.07 – combining the five utility functions
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.07 – 结合五个实用函数
- en: The example we’ll cover in this exercise is a somewhat contrived one so that
    all five verb functions can be used. In this exercise, we have been asked to find
    the average absolute difference between `Sepal.Length` and `Petal.Length` for
    the top 100 rows with the highest `Sepal.Length` and whose `Sepal.Width` is bigger
    than `Petal.Length`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本练习中涵盖的示例是有点人为的，以便所有五个动词函数都可以使用。在本练习中，我们被要求找到具有最高 `Sepal.Length` 且 `Sepal.Width`
    大于 `Petal.Length` 的前 100 行的平均绝对差值。
- en: 'When performing a complex query like this one, it is helpful to work through
    the requirements backward, starting from the conditions for subsetting the dataset
    and then working on the metric. In this case, we will start by sorting `Sepal.Length`
    in a descending order using the `arrange()` function and keep the top 100 rows
    using the `head()` function. Another filtering condition that uses the `filter()`
    function then comes in to retain the rows whose `Sepal.Width` is bigger than `Petal.Length`.
    Next, we must create a new column using the `mutate()` function to represent the
    absolute difference between `Sepal.Length` and `Petal.Length`. Finally, we must
    apply the `select()` function to focus on the new column and calculate its average.
    See the following code block for a detailed implementation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行此类复杂查询时，从数据集子集的条件开始，然后处理指标，逆向工作是有帮助的。在这种情况下，我们将首先使用 `arrange()` 函数按降序排序 `Sepal.Length`，并使用
    `head()` 函数保留顶部 100 行。然后，使用 `filter()` 函数的另一个过滤条件来保留 `Sepal.Width` 大于 `Petal.Length`
    的行。接下来，我们必须使用 `mutate()` 函数创建一个新列，以表示 `Sepal.Length` 和 `Petal.Length` 之间的绝对差值。最后，我们必须应用
    `select()` 函数来关注新列并计算其平均值。以下代码块展示了详细的实现：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will look at additional two verbs: `rename()` and `transmute()`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨另外两个动词：`rename()` 和 `transmute()`。
- en: Introducing other verbs
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍其他动词
- en: 'Two other verbs are also commonly used: `rename()` and `transmute()`. The `rename()`
    function changes the name of a specific column. For example, when using the `count()`
    function, a column named `n` is automatically created. We can use `rename(Count
    = n)` within the pipe context to change its default name from `n` to `Count`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个常用的动词是`rename()`和`transmute()`。`rename()`函数更改特定列的名称。例如，当使用`count()`函数时，会自动创建一个名为`n`的列。我们可以在管道上下文中使用`rename(Count
    = n)`来将其默认名称从`n`更改为`Count`。
- en: 'There is another way to change the column’s name. When selecting a column of
    a dataset, we can pass the same statement that we did to `rename()` to the `select()`
    function. For example, the following code shows selecting the `Sepal.Length` and
    `Sepal.Width` columns from the `iris` dataset while renaming the second column
    `Sepal_Width`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更改列名的方法是，在从数据集中选择列时，我们可以将传递给`rename()`的相同语句传递给`select()`函数。例如，以下代码展示了从`iris`数据集中选择`Sepal.Length`和`Sepal.Width`列，并将第二个列重命名为`Sepal_Width`：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'On the other hand, the `transmute()` function is a combination of `select()`
    and `mutate()`. It will return a subset of columns where some could be transformed.
    For example, suppose we would like to calculate the absolute between `Sepal.Length`
    and `Petal.Length` and return the result together with `Species`. We can achieve
    both tasks using `transmute()`, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`transmute()`函数是`select()`和`mutate()`的组合。它将返回一些可能被转换的列的子集。例如，假设我们想要计算`Sepal.Length`和`Petal.Length`之间的绝对值，并返回与`Species`一起的结果。我们可以使用`transmute()`实现这两个任务，如下所示：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Although these verbs could be used interchangeably, there are a few technical
    differences. As shown in *Figure 2**.1*, the `select()` function returns the specified
    columns without changing the value of these columns, which can be achieved via
    either `mutate()` or `transmutate()`. Both `mutate()` and `rename()` keep the
    original columns in the returned result when creating additional new columns,
    while `select()` and `transmute()` only return specified columns in the result:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些动词可以互换使用，但它们之间有一些技术上的差异。如图*图2**.1*所示，`select()`函数返回指定的列而不改变这些列的值，这可以通过`mutate()`或`transmutate()`实现。`mutate()`和`rename()`在创建新的附加列时都会保留原始列在返回结果中，而`select()`和`transmute()`只返回结果中的指定列：
- en: '![Figure 2.1 – Summarizing the four verbs in terms of their purposes and connections](img/B18680_02_001.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 按目的和关系总结四个动词](img/B18680_02_001.jpg)'
- en: Figure 2.1 – Summarizing the four verbs in terms of their purposes and connections
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 按目的和关系总结四个动词
- en: Now that we know how to transform the data, we can go a step further to make
    it more interpretable and presentable via aggregation and summarization. We will
    cover different ways to aggregate the data in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何转换数据，我们可以进一步通过聚合和总结来使数据更具可解释性和可展示性。我们将在下一节中介绍不同的数据聚合方法。
- en: Data aggregation with dplyr
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dplyr进行数据聚合
- en: '**Data aggregation** refers to a set of techniques that summarizes the dataset
    at an aggregate level and characterizes the original dataset at a higher level.
    Compared to data transformation, it operates at the row level for the input and
    the output.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据聚合**指的是一组技术，它以聚合级别总结数据集，并在更高级别上描述原始数据集。与数据转换相比，它对输入和输出都操作在行级别。'
- en: We have already encountered a few aggregation functions, such as calculating
    the mean of a column. This section will cover some of the most widely used aggregation
    functions provided by `dplyr`. We will start with the `count()` function, which
    returns the number of observations/rows for each category of the specified input
    column.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到了一些聚合函数，例如计算列的平均值。本节将介绍`dplyr`提供的最广泛使用的聚合函数中的一些。我们将从`count()`函数开始，它返回指定输入列每个类别的观测数/行数。
- en: Counting observations using the count() function
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`count()`函数计数观测值
- en: The `count()` function automatically groups the dataset into different categories
    according to the input argument and returns the number of observations for each
    category. The input argument could include one or more columns of the dataset.
    Let’s go through an exercise and apply it to the `iris` dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`函数会根据输入参数自动将数据集分组到不同的类别，并返回每个类别的观测数。输入参数可以包括数据集的一个或多个列。让我们通过一个练习来应用它到`iris`数据集。'
- en: Exercise 2.08 – counting observations by species
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.08 – 按物种计数观测值
- en: 'This exercise will use the `count()` function to get the number of observations
    for each unique species, followed by adding filtering conditions using the `filter()`
    function:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将使用`count()`函数获取每个独特物种的观测值数量，然后使用`filter()`函数添加过滤条件：
- en: 'Count the number of observations for each unique type of species in the `iris`
    dataset:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在`iris`数据集中每种独特物种类型的观测值数量：
- en: '[PRE29]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is a `tibble` dataset with two columns, where the first column contains
    the unique category in `Species` and the second column (named `n` by default)
    is the corresponding count of rows.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出是一个包含两列的`tibble`数据集，其中第一列包含`Species`中的唯一类别，第二列（默认命名为`n`）是对应的行计数。
- en: Let’s look at how to perform filtering before the counting operation.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看如何在计数操作之前进行过滤。
- en: 'Perform the exact counting for those observations whose absolute difference
    between `Sepal.Length` and `Sepal.Width` is greater than that of `Petal.Length`
    and `Petal.Width`. Return the result in descending order:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对那些`Sepal.Length`和`Sepal.Width`之间的绝对差值大于`Petal.Length`和`Petal.Width`的观测值进行精确计数。按降序返回结果：
- en: '[PRE30]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we added a filtering condition to keep rows that meet the specified criterion
    before counting. We enabled the `sort` argument to arrange the results in descending
    order.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一个过滤条件，在计数之前保留满足指定标准的行。我们启用了`sort`参数来按降序排列结果。
- en: 'The `count()` function essentially combines two steps: grouping by each category
    of a specified column and then counting the number of observations. It turns out
    that we can achieve the same task using the `group_by()` and `summarize()` functions,
    as introduced in the next section.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`函数本质上结合了两个步骤：按指定列的每个类别进行分组，然后计数观测值的数量。结果证明，我们可以使用下一节中介绍的`group_by()`和`summarize()`函数完成相同的任务。'
- en: Aggregating data via group_by() and summarize()
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过`group_by()`和`summarize()`进行数据聚合
- en: '`count()` is a helpful way to aggregate data. However, it is a particular case
    of two more general aggregation functions, `group_by()` and `summarize()`, which
    are often used together. The `group_by()` function splits the original dataset
    into different groups according to one or more columns in the input argument,
    while the `summarize()` function summarizes and collapses all observations within
    a specific category into one metric, which could be the count of rows in the case
    of `count()`.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`是一种有用的数据聚合方法。然而，它是两个更通用聚合函数`group_by()`和`summarize()`的特例，这两个函数通常一起使用。`group_by()`函数根据输入参数中的一个或多个列将原始数据集分割成不同的组，而`summarize()`函数则将特定类别内的所有观测值汇总并折叠成一个指标，在`count()`的情况下，这个指标可能是行数。'
- en: 'Multiple summarization functions can be used in the `summarize()` function.
    Typical ones include the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在`summarize()`函数中使用多个汇总函数。典型的一些包括以下：
- en: '`sum()`: Sums all observations of a particular group'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sum()`: 对特定组的所有观测值求和'
- en: '`mean()`: Calculates the average of all observations'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean()`: 计算所有观测值的平均值'
- en: '`median()`: Calculates the median of all observations'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`median()`: 计算所有观测值的中位数'
- en: '`max()`: Calculates the maximum of all observations'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max()`: 计算所有观测值的最大值'
- en: '`min()`: Calculates the minimum of all observations'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min()`: 计算所有观测值的最小值'
- en: Let’s go through an exercise on calculating different summary statistics using
    `group_by()` and `summarize()`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用`group_by()`和`summarize()`计算不同汇总统计的练习来了解。
- en: Exercise 2.09 – summarizing a dataset using group_by() and summarize()
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.09 – 使用group_by()和summarize()汇总数据集
- en: 'This exercise covers using the `group_by()` and `summarize()` functions to
    extract the count and mean statistics, combined with some of the verbs introduced
    earlier, including `filter()`, `mutate()`, and `arrange()`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习涵盖使用`group_by()`和`summarize()`函数提取计数和均值统计，结合之前介绍的一些动词，包括`filter()`、`mutate()`和`arrange()`：
- en: 'Get the count of observations for each unique type of `Species`:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每种独特类型的`Species`的观测值计数：
- en: '[PRE31]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, we used the `n()` function to get the number of observations
    and assigned the result to a column named `n`. The counting comes after grouping
    the observations based on the unique type of `Species`.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`n()`函数来获取观测值的数量，并将结果分配给名为`n`的列。计数是在根据唯一的`Species`类型对观测值进行分组之后进行的。
- en: 'Add the same filter as in the previous exercise and sort the result in descending
    order:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加与之前练习相同的过滤器，并按降序排序结果：
- en: '[PRE32]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this code block, the filtering condition is applied first to limit the grouping
    operations to a subset of observations. In the `arrange()` function, we directly
    used the `n` column to sort in descending order.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此代码块中，首先应用过滤条件以限制分组操作到观察值的子集。在 `arrange()` 函数中，我们直接使用 `n` 列按降序排序。
- en: 'Create a logical column based on the same filter and perform a two-level grouping
    using `Species`. Then, create a logical column to calculate the average `Sepal.Length`:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于相同的过滤条件创建一个逻辑列，并使用 `Species` 进行两级分组。然后，创建一个逻辑列来计算平均 `Sepal.Length`：
- en: '[PRE33]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can put multiple categorical columns in the `group_by()` function to perform
    a multi-level grouping. Note that the result contains a `Groups` attribute based
    on `Species`, suggesting that the `tibble` object has a group structure. Let’s
    learn how to remove the structure.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在 `group_by()` 函数中放入多个分类列以执行多级分组。注意，结果包含基于 `Species` 的 `Groups` 属性，表明 `tibble`
    对象具有分组结构。让我们学习如何移除该结构。
- en: 'Remove the group structure in the returned `tibble` object using `ungroup()`:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ungroup()` 移除返回的 `tibble` 对象中的分组结构：
- en: '[PRE34]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, the result contains a normal `tibble` object with the average sepal length
    for each unique combination of `Species` and `ind`.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，结果包含一个正常的 `tibble` 对象，其中包含 `Species` 和 `ind` 每个唯一组合的平均花萼长度。
- en: Now that we know how to transform and aggregate one dataset, we will cover how
    to work with multiple datasets via merging and joining.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何转换和聚合一个数据集，我们将介绍如何通过合并和连接来处理多个数据集。
- en: Data merging with dplyr
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 dplyr 进行数据合并
- en: In practical data analysis, the information we need is not necessarily confined
    to one table but is spread across multiple tables. Storing data in separate tables
    is memory-efficient but not analysis-friendly. **Data merging** is the process
    of merging different datasets into one table to facilitate data analysis. When
    joining two tables, there need to be one or more columns, or **keys**, that exist
    in both tables and serve as the common ground for joining.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际数据分析中，我们需要的信息不一定局限于一个表，而是分散在多个表中。将数据存储在单独的表中是内存高效的，但不是分析友好的。**数据合并**是将不同的数据集合并到一个表中以方便数据分析的过程。在连接两个表时，需要有一个或多个存在于两个表中的列，或**键**，作为连接的共同基础。
- en: 'This section will cover different ways to join tables and analyze them in combination,
    including inner join, left join, right join, and full join. The following list
    shows the verbs and their definitions for these four types of joining:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍不同的连接表和组合分析的方法，包括内部连接、左连接、右连接和全连接。以下列表显示了这些连接类型的动词及其定义：
- en: '`inner_join()`: Returns common observations in both tables according to the
    matching key.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inner_join()`: 根据匹配的关键值返回两个表中的共同观察值。'
- en: '`left_join()`: Returns all observations from the left table and matched observations
    from the right table. Note that in the case of a duplicate key value in the right
    table, an additional row will be automatically created and added to the left table.
    Empty cells are filled with `NA`. More on this in the exercise.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`left_join()`: 返回左表中的所有观察值和右表中匹配的观察值。注意，如果右表中存在重复的关键值，将自动在左表中创建并添加额外的行。空单元格将填充为
    `NA`。更多内容请参考练习。'
- en: '`right_join()`: Returns all observations from the right table and matched observations
    from the left table. Empty cells are filled with `NA`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`right_join()`: 返回右表中的所有观察值和左表中匹配的观察值。空单元格将填充为 `NA`。'
- en: '`full_join()`: Returns all observations from both tables. Empty cells are filled
    with `NA`.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`full_join()`: 返回两个表中的所有观察值。空单元格将填充为 `NA`。'
- en: '*Figure 2**.2* illustrates the four joins using Venn diagrams:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2**.2* 使用维恩图说明了这四种连接方式：'
- en: '![Figure 2.2 – Four different joins commonly used in practice](img/B18680_02_002.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 实践中常用的四种连接方式](img/B18680_02_002.jpg)'
- en: Figure 2.2 – Four different joins commonly used in practice
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 实践中常用的四种连接方式
- en: Let’s go through an exercise on these four joins.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过练习来了解这四种连接方式。
- en: Exercise 2.10 – joining datasets
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.10 – 数据集连接
- en: 'This exercise will create two dummy `tibble` datasets and apply different joining
    verbs to merge them:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习将创建两个虚拟 `tibble` 数据集，并应用不同的连接动词将它们合并：
- en: 'Create two dummy datasets by following the steps in this code block:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照此代码块中的步骤创建两个虚拟数据集：
- en: '[PRE35]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Both dummy datasets have three rows and two columns, with the first column being
    the key column to be used for joining.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个虚拟数据集都有三行两列，第一列是用于连接的关键列。
- en: 'Perform an inner join of the two datasets:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对两个数据集执行内部连接：
- en: '[PRE36]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code shows that the matching keys are specified in the `by` argument
    via the `c()` function. Since `"key_A"` and `"key_B"` have only two values in
    common, the resulting table after the inner join operation is a 2x3 `tibble` that
    keeps only `"key_A"` as the key column and all other non-key columns from both
    tables. It only keeps observations that have an exact match and works the same
    way with either table in either direction.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码显示，匹配的键是通过`c()`函数在`by`参数中指定的。由于`"key_A"`和`"key_B"`只有两个共同的值，内连接操作后的结果表是一个2x3的`tibble`，只保留`"key_A"`作为键列，以及来自两个表的所有其他非键列。它只保留具有精确匹配的观测值，并且无论在哪个方向上与哪个表连接，工作方式都是相同的。
- en: We can also pass in additional match keys (on which the tables will be joined)
    in the `by` argument while following the same format to perform a multi-level
    merging.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以在`by`参数中传递额外的匹配键（表将根据这些键进行合并），同时遵循相同的格式来执行多级合并。
- en: Let’s look at how to perform a left join.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看如何执行左连接。
- en: 'Perform a left join of the two datasets:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行两个数据集的左连接：
- en: '[PRE37]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that the resulting tables contain the whole `tbl_A` and an additional column,
    `col_B`, that’s referenced from `tbl_B`. Since `col_B` does not have 1, the corresponding
    cell in `col_B` shows `NA`. In general, any cell that cannot be matched will assume
    a value of `NA` in the resulting table.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，结果表包含整个`tbl_A`以及一个额外的列`col_B`，该列是从`tbl_B`引用的。由于`col_B`中没有1，因此`col_B`中相应的单元格显示为`NA`。一般来说，任何无法匹配的单元格在结果表中都将假设为`NA`的值。
- en: Note that when there are multiple rows with duplicate values in `col_B`, the
    resulting table after a left join will also have a duplicate row automatically
    created since it is now a one-to-two mapping from left to right. Let’s see an
    example.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，当`col_B`中有多个具有重复值的行时，左连接后的结果表也会自动创建一个重复行，因为现在它是一个从左到右的一对二映射。让我们看看一个例子。
- en: 'Create another table with a duplicate key value and perform a left join with
    `tbl_A`:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个具有重复键值的表，并与`tbl_A`执行左连接：
- en: '[PRE38]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here, we used the `bind_rows()` function to append a new row with the same
    value in `"key_B"` as the first row and a different value for `col_B`. Let’s see
    what happens when we join it to `tbl_A`:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`bind_rows()`函数来追加一个新行，其中`"key_B"`的值与第一行相同，而`col_B`的值不同。让我们看看当我们将其与`tbl_A`连接时会发生什么：
- en: '[PRE39]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Having a duplicate value in the key column of the right table is a common source
    of bugs that could be difficult to trace. Seasoned data scientists should pay
    particular attention to this potential unintended result by checking the dimension
    of the dataset *before* and *after* the left join. Now, let’s look at how to perform
    a right join.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在右侧表的键列中存在重复值是常见的问题来源，这些问题可能难以追踪。经验丰富的数据科学家应该特别注意在左连接之前和之后检查数据集的维度，以避免这种潜在的不期望的结果。现在，让我们看看如何执行右连接。
- en: 'Perform a right join of `tbl_A` to `tbl_B`:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`tbl_A`与`tbl_B`执行右连接：
- en: '[PRE40]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Similarly, all observations in `tbl_B` are kept and the missing value in `col_A`
    is filled with `NA`. In addition, the key column is named `"key_A"` instead of
    `"key_B"`.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，`tbl_B`中的所有观测值都保留，`col_A`中的缺失值用`NA`填充。此外，键列被命名为`"key_A"`而不是`"key_B"`。
- en: 'Perform a full join of `tbl_A` and `tbl_B`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`tbl_A`和`tbl_B`的全连接：
- en: '[PRE41]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Using a full join, all matched results from both tables are maintained, with
    missing values filled with `NA`. We can use this when we do not want to leave
    out any observations from the source tables.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用全连接，保留两个表的所有匹配结果，缺失值用`NA`填充。当我们不希望从源表中遗漏任何观测值时，可以使用此方法。
- en: 'These four joining statements can be repeatedly used to join multiple tables
    and combined with any data transformation verbs covered earlier. For example,
    we can remove the rows with `NA` values and keep only the `complete` rows, which
    should give us the same result in the inner join. This can be achieved using `drop_na()`,
    a utility function provided by the `tidyr` package that’s designed specifically
    for data cleaning within the `tidyverse` ecosystem:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个连接语句可以重复使用以连接多个表，并可与之前覆盖的任何数据转换动词结合使用。例如，我们可以删除具有`NA`值的行，并仅保留`complete`行，这应该在内连接中给出相同的结果。这可以通过使用`tidyr`包提供的实用函数`drop_na()`来实现，该函数专门设计用于`tidyverse`生态系统中的数据清理：
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We may also want to replace `NA` values with 0, which can be achieved via the
    `replace_na()` function provided by `tidyr`. In the following code, we are specifying
    the replacement values for each column of interest and wrapping them in a list
    to be passed into `replace_na()`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能想用0替换`NA`值，这可以通过`tidyr`提供的`replace_na()`函数实现。在下面的代码中，我们指定了每个感兴趣列的替换值，并将它们包装在一个列表中传递给`replace_na()`：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that there are other merging options, such as the semi-join and the anti-join,
    which correspond to the `semi_join()` and `anti_join()` functions, respectively.
    Semi-join returns only the rows from the table in the first argument where there
    is a match in the second table. Although similar to a full join operation, a semi-join
    only keeps *the columns in the first table*. The anti-join operation, on the other
    hand, is the opposite of a semi-join, returning only *the unmatched rows in the
    first table*. Since many merging operations, including these two, can be derived
    using the four basic operations we introduced in this section, we will not cover
    these slightly more complicated joining operations in detail. Instead, we encourage
    you to explore using these four fundamental joining functions to achieve complicated
    operations instead of relying on other shortcut joining functions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，还有其他合并选项，例如半连接和反连接，分别对应于`semi_join()`和`anti_join()`函数。半连接只返回第一个参数表中在第二个表中存在匹配的行。尽管与全连接操作类似，但半连接只保留*第一个表中的列*。反连接操作，另一方面，是半连接的相反操作，只返回*第一个表中不匹配的行*。由于许多合并操作，包括这两个，都可以使用我们在本节中介绍的基本操作推导出来，因此我们不会详细介绍这些稍微复杂一些的连接操作。相反，我们鼓励您探索使用这四个基本的连接函数来实现复杂的操作，而不是依赖于其他快捷连接函数。
- en: Next, we will go through a case study and observe how to transform, merge, and
    aggregate a dataset using the functions we’ve covered in this chapter.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过一个案例研究来观察如何使用本章中介绍的功能来转换、合并和聚合数据集。
- en: Case study – working with the Stack Overflow dataset
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 使用Stack Overflow数据集
- en: This section will cover an exercise to help you practice different data transformation,
    aggregation, and merging techniques based on the public Stack Overflow dataset,
    which contains a set of tables related to technical questions and answers posted
    on the Stack Overflow platform. The supporting raw data has been uploaded to the
    accompanying Github repository of this book. We will directly download it from
    the source GitHub link using the `readr` package, another `tidyverse` offering
    that provides an easy, fast, and friendly way to read a wide range of data sources,
    including those from the web.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一个练习，帮助您练习基于公共Stack Overflow数据集的不同数据转换、聚合和合并技术，该数据集包含一系列与Stack Overflow平台上发布的技术问题和答案相关的表。支持的原数据已上传到本书的配套GitHub仓库。我们将直接从源GitHub链接使用`readr`包下载，`readr`是`tidyverse`提供的一个易于使用、快速且友好的包，可以轻松读取各种数据源，包括来自网络的源。
- en: Exercise 2.11 – working with the Stack Overflow dataset
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.11 – 使用Stack Overflow数据集
- en: 'Let’s begin this exercise:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个练习：
- en: 'Download three data sources on questions, tags, and their mapping table from
    GitHub:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从GitHub下载关于问题、标签及其映射表的三个数据源：
- en: '[PRE44]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The questions dataset contains the question ID, date of creation, and score,
    which indicates the number of (positive) upvotes and (negative) downvotes. We
    can examine the range of scores using the `summary()` function:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题数据集包含问题ID、创建日期和分数，分数表示（正）赞同票数和（负）反对票数。我们可以使用`summary()`函数检查分数的范围：
- en: '[PRE45]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This dataset contains the ID and the content of each tag. To analyze the tags,
    we need to merge the three datasets into one using the relevant mapping keys.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该数据集包含每个标签的ID和内容。为了分析标签，我们需要使用相关的映射键将这三个数据集合并成一个。
- en: '[PRE46]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Reference the tag ID from `df_question_tags` into `df_questions` via a left
    join:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过左连接将`df_question_tags`中的标签ID引用到`df_questions`中：
- en: '[PRE47]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Note that the total number of rows almost doubled when comparing `df_questions`
    to `df_all`. You may have noticed that this is due to the one-to-many relationship:
    a question often has multiple tags, so each tag gets appended as a separate row
    to the left table during the left join operation.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，当比较`df_questions`和`df_all`时，行数几乎翻了一番。您可能已经注意到这是由于一对一关系：一个问题通常有多个标签，所以在左连接操作期间，每个标签都会作为单独的行附加到左表中。
- en: 'Let’s continue to reference the tags from `df_tags`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续参考`df_tags`中的标签：
- en: '[PRE48]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next, we will do some analysis of the tags, starting with counting their frequency.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将对标签进行一些分析，从计算它们的频率开始。
- en: 'Count the occurrence of each non-`NA` tag in descending order:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按降序统计每个非`NA`标签的出现次数：
- en: '[PRE49]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, we first used `filter()` to remove rows if `tag_name` was `NA`, then calculated
    the counts using the `count()` function. The result shows that `dplyr` is among
    one of the most popular R-related tags on Stack Overflow, which is a good sign
    as it shows we are learning about something useful and trendy.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先使用`filter()`删除`tag_name`为`NA`的行，然后使用`count()`函数计算计数。结果显示，`dplyr`是Stack
    Overflow上最受欢迎的R相关标签之一，这是一个好兆头，因为它表明我们正在学习有用的和流行的东西。
- en: 'Count the number of tags in each year:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每年标签的数量：
- en: '[PRE50]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The result shows an increasing number of tags per year, with 2019 being a special
    case as the data ends in mid-2019 (verified here). Note that we used the `year()`
    function from the `lubricate` package from `tidyverse` to convert a date-formatted
    column into the corresponding year:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，每年标签的数量都在增加，2019年是一个特殊情况，因为数据在2019年中途结束（在此处验证）。请注意，我们使用了`tidyverse`中的`lubricate`包的`year()`函数将日期格式的列转换为相应的年份：
- en: '[PRE51]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Calculate the average occurrence of tags per month.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每月标签的平均出现次数。
- en: 'We need to derive the monthly occurrence of tags to calculate their average.
    First, we must create two columns to indicate the month and year-month for each
    tag:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要推导出标签的月度出现次数来计算它们的平均值。首先，我们必须为每个标签创建两个列来表示月份和年月：
- en: '[PRE52]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, we must count the occurrence of tags per month for each year-month:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们必须计算每年每月标签的出现次数：
- en: '[PRE53]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Finally, we must average over all years for each month:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们必须对每个月份的所有年份进行平均：
- en: '[PRE54]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The result shows that March has the highest occurrence of tags on average. Maybe
    school just got started and people are actively learning and asking questions
    more in March.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，3月份标签的平均出现次数最高。也许学校刚刚开学，人们在3月份更加积极地学习和提问。
- en: 'Calculate the count, minimum, average score, and maximum score for each tag
    and sort the result by count in descending order:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个标签的计数、最小值、平均分数和最大分数，并按计数降序排序：
- en: '[PRE55]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Here, we used multiple summary functions in the `group_by()` and `summarize()`
    context to calculate the metrics.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们在`group_by()`和`summarize()`的上下文中使用了多个汇总函数来计算指标。
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered essential functions and techniques for data transformation,
    aggregation, and merging. For data transformation at the row level, we learned
    about common utility functions such as `filter()`, `mutate()`, `select()`, `arrange()`,
    `top_n()`, and `transmute()`. For data aggregation, which summarizes the raw dataset
    into a smaller and more concise summary view, we introduced functions such as
    `count()`, `group_by()`, and `summarize()`. For data merging, which combines multiple
    datasets into one, we learned about different joining methods, including `inner_join()`,
    `left_join()`, `right_join()`, and `full_join()`. Although there are other more
    advanced joining functions, the essential tools we covered in our toolkit are
    enough for us to achieve the same task. Finally, we went through a case study
    based on the Stack Overflow dataset. The skills we learned in this chapter will
    come in very handy in many data analysis tasks.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了数据转换、聚合和合并的基本函数和技术。对于行级别的数据转换，我们学习了常见的实用函数，如`filter()`、`mutate()`、`select()`、`arrange()`、`top_n()`和`transmute()`。对于数据聚合，它将原始数据集总结成一个更小、更简洁的概览视图，我们介绍了`count()`、`group_by()`和`summarize()`等函数。对于数据合并，它将多个数据集合并成一个，我们学习了不同的连接方法，包括`inner_join()`、`left_join()`、`right_join()`和`full_join()`。尽管还有其他更高级的连接函数，但我们工具箱中涵盖的基本工具已经足够我们完成相同任务。最后，我们通过Stack
    Overflow数据集的案例研究进行了说明。本章学到的技能在许多数据分析任务中都将非常有用。
- en: In the next chapter, we will cover a more advanced topic on natural language
    processing, taking us one step further to work with textual data using `tidyverse`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一个更高级的自然语言处理主题，这将使我们进一步使用`tidyverse`处理文本数据。
