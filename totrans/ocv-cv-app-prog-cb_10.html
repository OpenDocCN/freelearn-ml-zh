<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;10.&#xA0;Estimating Projective Relations in Images"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>Chapter 10. Estimating Projective Relations in Images</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Calibrating a camera</li><li class="listitem">Computing the fundamental matrix of an image pair</li><li class="listitem">Matching images using a random sample consensus</li><li class="listitem">Computing a homography between two images</li></ul></div></div>

<div class="book" title="Chapter&#xA0;10.&#xA0;Estimating Projective Relations in Images">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec61" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">Images are generally produced using a digital camera, which captures a scene by projecting light going through its lens onto an image sensor. The fact that an image is formed by the projection of a 3D scene onto a 2D plane implies the existence of important relationships between a scene and its image and between different images of the same scene. Projective geometry is the tool that is used to describe and characterize, in mathematical terms, the process of image formation. In this chapter, we will introduce you to some of the fundamental projective relations that exist in multiview imagery and explain how these can be used in computer vision programming. You will learn how matching can be made more accurate through the use of projective constraints and how a mosaic from multiple images can be composited using two-view relations. Before we start the recipes, let's explore the basic concepts related to scene projection and image formation.</p></div></div>

<div class="book" title="Chapter&#xA0;10.&#xA0;Estimating Projective Relations in Images">
<div class="book" title="Introduction">
<div class="book" title="Image formation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec179" class="calibre1"/>Image formation</h2></div></div></div><p class="calibre8">Fundamentally, the process used to produce images has not changed since the beginning of photography. The light coming from an observed scene is captured by a camera through a frontal <a id="id772" class="calibre1"/>
<span class="strong"><strong class="calibre2">aperture</strong></span>; the captured light rays hit an <a id="id773" class="calibre1"/>
<span class="strong"><strong class="calibre2">image plane</strong></span> (or an <span class="strong"><strong class="calibre2">image sensor</strong></span>) located<a id="id774" class="calibre1"/> at the back of the camera. Additionally, a lens is used to concentrate the rays coming from the different scene elements. This process is illustrated by the following figure:</p><div class="mediaobject"><img src="../images/00161.jpeg" alt="Image formation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <span class="strong"><strong class="calibre2">do</strong></span> is the distance from the lens to the observed object, <span class="strong"><strong class="calibre2">di</strong></span> is the distance from the lens to the image plane, and <span class="strong"><strong class="calibre2">f</strong></span> is the focal length of the lens. These quantities are related by the so-called thin lens equation:</p><div class="mediaobject"><img src="../images/00162.jpeg" alt="Image formation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In computer vision, this camera model can be simplified in a number of ways. First, we can neglect the effect of the lens by considering that we have a camera with an infinitesimal aperture since, in theory, this does not change the image appearance. (However, by doing so, we ignore the focusing effect by creating an image with an infinite <a id="id775" class="calibre1"/>
<span class="strong"><strong class="calibre2">depth of field</strong></span>.) In this case, therefore, only the central ray is considered. Second, since most of the time we have <code class="email">do&gt;&gt;di</code>, we can assume that the image plane is located at the focal distance. Finally, we can note from the geometry of the system that the image on the plane is inverted. We can obtain an identical but upright image by simply positioning the image plane in front of the lens. Obviously, this is not physically feasible, but from a mathematical point of view, this is completely equivalent. This simplified model is often referred to as the <a id="id776" class="calibre1"/>
<span class="strong"><strong class="calibre2">pin-hole camera</strong></span> model, and it is represented as follows:</p><div class="mediaobject"><img src="../images/00163.jpeg" alt="Image formation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From this model, and using the law of similar triangles, we can easily derive the basic projective equation that relates a pictured object with its image:</p><div class="mediaobject"><img src="../images/00164.jpeg" alt="Image formation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The size (<span class="strong"><strong class="calibre2">hi</strong></span>) of the image of an object (of height <span class="strong"><strong class="calibre2">ho</strong></span>) is therefore inversely proportional to its distance (<span class="strong"><strong class="calibre2">do</strong></span>) from the camera, which is naturally true. In general, this relation describes where a 3D scene point will be projected on the image plane given the geometry of the camera.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Calibrating a camera"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec62" class="calibre1"/>Calibrating a camera</h1></div></div></div><p class="calibre8">From the <a id="id777" class="calibre1"/>introduction of this chapter, we learned that the essential parameters of a camera under the pin-hole model are its focal length and the size of the image plane (which defines the<a id="id778" class="calibre1"/> <span class="strong"><strong class="calibre2">field of view</strong></span> of the camera). Also, since we are dealing with digital images, the number of pixels on the image plane (its <span class="strong"><strong class="calibre2">resolution</strong></span>) is <a id="id779" class="calibre1"/>another important characteristic of a camera. Finally, in order to be able to compute the position of an image's scene point in pixel coordinates, we need one additional piece of information. Considering the line coming from the focal point that is orthogonal to the image plane, we need to know at which pixel position this line pierces the image plane. This point is called the <a id="id780" class="calibre1"/>
<span class="strong"><strong class="calibre2">principal point</strong></span>. It might be logical to assume that this principal point is at the center of the image plane, but in practice, this point might be off by a few pixels depending on the precision at which the camera has been manufactured.</p><p class="calibre8">Camera calibration is the process by which the different camera parameters are obtained. One can obviously use the specifications provided by the camera manufacturer, but for some tasks, such as 3D reconstruction, these specifications are not accurate enough. Camera calibration will proceed by showing known patterns to the camera and analyzing the obtained images. An optimization process will then determine the optimal parameter values that explain the observations. This is a complex process that has been made easy by the availability of OpenCV calibration functions.</p></div>

<div class="book" title="Calibrating a camera">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch10lvl2sec180" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">To calibrate a camera, the idea is to show it a set of scene points for which their 3D positions are known. Then, you need to observe where these points project on the image. With the knowledge of a sufficient number of 3D points and associated 2D image points, the exact camera parameters can be inferred from the projective equation. Obviously, for accurate results, we need to observe as many points as possible. One way to achieve this would be to take one picture of a scene with many known 3D points, but in practice, this is rarely feasible. A more convenient way is to take several images of a set of some 3D points from different viewpoints. This approach is simpler but requires you to compute the position of each camera view in addition to the computation of the internal camera parameters, which fortunately is feasible.</p><p class="calibre8">OpenCV proposes that you use a chessboard pattern to generate the set of 3D scene points required for calibration. This pattern creates points at the corners of each square, and since this pattern is flat, we can freely assume that the board is located at <code class="email">Z=0</code>, with the <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">Y</em></span> axes well-aligned with the grid. In this case, the calibration process simply consists of showing the chessboard pattern to the camera from different viewpoints. Here is one example of a <code class="email">6x4</code> calibration pattern image:</p><div class="mediaobject"><img src="../images/00165.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The good thing<a id="id781" class="calibre1"/> is that OpenCV has a function that automatically detects the corners of this chessboard pattern. You simply provide an image and the size of the chessboard used (the number of horizontal and vertical inner corner points). The function will return the position of these chessboard corners on the image. If the function fails to find the pattern, then it simply returns <code class="email">false</code>:</p><div class="informalexample"><pre class="programlisting">    // output vectors of image points
    std::vector&lt;cv::Point2f&gt; imageCorners;
    // number of inner corners on the chessboard
    cv::Size boardSize(6,4);
    // Get the chessboard corners
    bool found = cv::findChessboardCorners(image, 
                                 boardSize, imageCorners);</pre></div><p class="calibre8">The output parameter, <code class="email">imageCorners</code>, will simply contain the pixel coordinates of the detected inner corners of the shown pattern. Note that this function accepts additional parameters if you need to tune the algorithm, which are not discussed here. There is also a special function that draws the detected corners on the chessboard image, with lines connecting them in a sequence:</p><div class="informalexample"><pre class="programlisting">    //Draw the corners
    cv::drawChessboardCorners(image, 
                    boardSize, imageCorners, 
                    found); // corners have been found</pre></div><p class="calibre8">The following image is obtained:</p><div class="mediaobject"><img src="../images/00166.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The lines that <a id="id782" class="calibre1"/>connect the points show the order in which the points are listed in the vector of detected image points. To perform a calibration, we now need to specify the corresponding 3D points. You can specify these points in the units of your choice (for example, in centimeters or in inches); however, the simplest is to assume that each square represents one unit. In that case, the coordinates of the first point would be <code class="email">(0,0,0)</code> (assuming that the board is located at a depth of <code class="email">Z=0</code>), the coordinates of the second point would be <code class="email">(1,0,0)</code>, and so on, the last point being located at <code class="email">(5,3,0)</code>. There are a total of <code class="email">24</code> points in this pattern, which is too small to obtain an accurate calibration. To get more points, you need to show more images of the same calibration pattern from various points of view. To do so, you can either move the pattern in front of the camera or move the camera around the board; from a mathematical point of view, this is completely equivalent. The OpenCV calibration function assumes that the reference frame is fixed on the calibration pattern and will calculate the rotation and translation of the camera with respect to the reference frame.</p><p class="calibre8">Let's now encapsulate the calibration process in a <code class="email">CameraCalibrator</code> class. The attributes of this class are as follows:</p><div class="informalexample"><pre class="programlisting">class CameraCalibrator {

    // input points:
    // the points in world coordinates
    std::vector&lt;std::vector&lt;cv::Point3f&gt;&gt; objectPoints;
    // the point positions in pixels
    std::vector&lt;std::vector&lt;cv::Point2f&gt;&gt; imagePoints;
    // output Matrices
    cv::Mat cameraMatrix;
    cv::Mat distCoeffs;
    // flag to specify how calibration is done
    int flag;</pre></div><p class="calibre8">Note that<a id="id783" class="calibre1"/> the input vectors of the scene and image points are in fact made of <code class="email">std::vector</code> of point instances; each vector element is a vector of the points from one view. Here, we decided to add the calibration points by specifying a vector of the chessboard image filename as input:</p><div class="informalexample"><pre class="programlisting">// Open chessboard images and extract corner points
int CameraCalibrator::addChessboardPoints(
         const std::vector&lt;std::string&gt;&amp; filelist, 
         cv::Size &amp; boardSize) {

   // the points on the chessboard
   std::vector&lt;cv::Point2f&gt; imageCorners;
   std::vector&lt;cv::Point3f&gt; objectCorners;

   // 3D Scene Points:
   // Initialize the chessboard corners 
   // in the chessboard reference frame
   // The corners are at 3D location (X,Y,Z)= (i,j,0)
   for (int i=0; i&lt;boardSize.height; i++) {
      for (int j=0; j&lt;boardSize.width; j++) {
         objectCorners.push_back(cv::Point3f(i, j, 0.0f));
      }
    }

    // 2D Image points:
    cv::Mat image; // to contain chessboard image
    int successes = 0;
    // for all viewpoints
    for (int i=0; i&lt;filelist.size(); i++) {
        // Open the image
        image = cv::imread(filelist[i],0);
        // Get the chessboard corners
        bool found = cv::findChessboardCorners(
                        image, boardSize, imageCorners);
        // Get subpixel accuracy on the corners
        cv::cornerSubPix(image, imageCorners, 
                  cv::Size(5,5), 
                  cv::Size(-1,-1), 
         cv::TermCriteria(cv::TermCriteria::MAX_ITER +
                          cv::TermCriteria::EPS, 
                  30,     // max number of iterations 
                  0.1));  // min accuracy

        //If we have a good board, add it to our data
        if (imageCorners.size() == boardSize.area()) {
            // Add image and scene points from one view
            addPoints(imageCorners, objectCorners);
            successes++;
        }
    }
   return successes;
}</pre></div><p class="calibre8">The first loop<a id="id784" class="calibre1"/> inputs the 3D coordinates of the chessboard, and the corresponding image points are the ones provided by the <code class="email">cv::findChessboardCorners</code> function. This is done for all the available viewpoints. Moreover, in order to obtain a more accurate image point location, the <code class="email">cv::cornerSubPix</code> function can be used, and as the name suggests, the image points will then be localized at a subpixel accuracy. The termination criterion that is specified by the <code class="email">cv::TermCriteria</code> object defines the maximum number of iterations and the minimum accuracy in subpixel coordinates. The first of these two conditions that is reached will stop the corner refinement process.</p><p class="calibre8">When a set of chessboard corners have been successfully detected, these points are added to our vectors of the image and scene points using our <code class="email">addPoints</code> method. Once a sufficient number of chessboard images have been processed (and consequently, a large number of 3D scene point / 2D image point correspondences are available), we can initiate the computation of the calibration parameters as follows:</p><div class="informalexample"><pre class="programlisting">// Calibrate the camera
// returns the re-projection error
double CameraCalibrator::calibrate(cv::Size &amp;imageSize)
{
   //Output rotations and translations
    std::vector&lt;cv::Mat&gt; rvecs, tvecs;

   // start calibration
   return 
     calibrateCamera(objectPoints, // the 3D points
               imagePoints,  // the image points
               imageSize,    // image size
               cameraMatrix, // output camera matrix
               distCoeffs,   // output distortion matrix
               rvecs, tvecs, // Rs, Ts 
               flag);        // set options
}</pre></div><p class="calibre8">In practice, 10 to 20 chessboard images are sufficient, but these must be taken from different viewpoints<a id="id785" class="calibre1"/> at different depths. The two important outputs of this function are the camera matrix and the distortion parameters. These will be described in the next section.</p></div></div>

<div class="book" title="Calibrating a camera">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec181" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">In order to explain the result of the calibration, we need to go back to the figure in the introduction, which describes the pin-hole camera model. More specifically, we want to demonstrate the relationship between a point in 3D at the position (X,Y,Z) and its image (x,y) on a camera specified in pixel coordinates. Let's redraw this figure by adding a reference frame that we position at the center of the projection as seen here:</p><div class="mediaobject"><img src="../images/00167.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that the <span class="strong"><em class="calibre9">y</em></span> axis is pointing downward to get a coordinate system compatible with the usual convention that places the image origin at the upper-left corner. We learned previously that the point <span class="strong"><strong class="calibre2">(X,Y,Z)</strong></span> will be projected onto the image plane at <code class="email">(fX/Z,fY/Z)</code>. Now, if we want to translate this coordinate into pixels, we need to divide the 2D image position by the pixel's width (<code class="email">px</code>) and height (<code class="email">py</code>), respectively. Note that by dividing the focal length given in world units (generally given in millimeters) by <code class="email">px</code>, we obtain the focal length expressed in (horizontal) pixels. Let's then define this term as <code class="email">fx</code>. Similarly, <code class="email">fy =f/py</code> is defined as the focal length expressed in vertical pixel units. Therefore, the complete projective equation is as follows:</p><div class="mediaobject"><img src="../images/00168.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00169.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Recall that (u<sub class="calibre20">0</sub>,v<sub class="calibre20">0</sub>) is the principal point that is added to the result in order to move the origin to the upper-left corner of the image. These equations can be rewritten in the matrix form through the<a id="id786" class="calibre1"/> introduction of <a id="id787" class="calibre1"/>
<span class="strong"><strong class="calibre2">homogeneous coordinates</strong></span>, in which 2D points are represented by 3-vectors and 3D points are represented by 4-vectors (the extra coordinate is simply an arbitrary scale factor, <code class="email">S</code>, that needs to be removed when a 2D coordinate needs to be extracted from a homogeneous 3-vector).</p><p class="calibre8">Here is the rewritten projective equation:</p><div class="mediaobject"><img src="../images/00170.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The second matrix is a simple projection matrix. The first matrix includes all of the camera parameters, which are called the intrinsic parameters of the camera. This <code class="email">3x3</code> matrix is one of the output matrices returned by the <code class="email">cv::calibrateCamera</code> function. There is also a function called <a id="id788" class="calibre1"/>
<code class="email">cv::calibrationMatrixValues</code> that returns the value of the intrinsic parameters given by a calibration matrix.</p><p class="calibre8">More generally, when the reference frame is not at the projection center of the camera, we will need to add a rotation vector (a <code class="email">3x3</code> matrix) and a translation vector (a <code class="email">3x1</code> matrix). These two matrices describe the rigid transformation that must be applied to the 3D points in order to bring them back to the camera reference frame. Therefore, we can rewrite the projection equation in its most general form:</p><div class="mediaobject"><img src="../images/00171.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Remember that in our calibration example, the reference frame was placed on the chessboard. Therefore, there <a id="id789" class="calibre1"/>is a rigid transformation (made of a rotation component represented by the matrix entries <code class="email">r1</code> to <code class="email">r9</code> and a translation represented by <code class="email">t1</code>, <code class="email">t2</code>, and <code class="email">t3</code>) that must be computed for each view. These are in the output parameter list of the <code class="email">cv::calibrateCamera</code> function. The rotation and translation components are often called the <span class="strong"><strong class="calibre2">extrinsic parameters</strong></span><a id="id790" class="calibre1"/> of the calibration, and they are different for each view. The intrinsic parameters remain constant for a given camera/lens system. The intrinsic parameters of our test camera obtained from a calibration based on 20 chessboard images are <code class="email">fx=167</code>, <code class="email">fy=178</code>, <code class="email">u0=156</code>, and <code class="email">v0=119</code>. These results are obtained by <code class="email">cv::calibrateCamera</code> through an optimization process aimed at finding the intrinsic and extrinsic parameters that will minimize the difference between the predicted image point position, as computed from the projection of the 3D scene points, and the actual image point position, as observed on the image. The sum of this difference for all the points specified during the calibration is called the <a id="id791" class="calibre1"/>
<span class="strong"><strong class="calibre2">re-projection error</strong></span>.</p><p class="calibre8">Let's now turn our attention to the distortion parameters. So far, we have mentioned that under the pin-hole camera model, we can neglect the effect of the lens. However, this is only possible if the lens that is used to capture an image does not introduce important optical distortions. Unfortunately, this is not the case with lower quality lenses or with lenses that have a very short focal length. You may have already noted that the chessboard pattern shown in the image that we used for our example is clearly distorted—the edges of the rectangular board are curved in the image. Also, note that this distortion becomes more important as we move away from the center of the image. This is a typical distortion observed with a fish-eye lens, and it is called <a id="id792" class="calibre1"/>
<span class="strong"><strong class="calibre2">radial distortion</strong></span>. The lenses used in common digital cameras usually do not exhibit such a high degree of distortion, but in the case of the lens used here, these distortions certainly cannot be ignored.</p><p class="calibre8">It is possible to compensate for these deformations by introducing an appropriate distortion model. The idea is to represent the distortions induced by a lens by a set of mathematical equations. Once established, these equations can then be reverted in order to undo the distortions visible on the image. Fortunately, the exact parameters of the transformation that will correct the distortions can be obtained together with the other camera parameters during the calibration phase. Once this is done, any image from the newly calibrated camera will be undistorted. Therefore, we have added an additional method to our<a id="id793" class="calibre1"/> calibration class:</p><div class="informalexample"><pre class="programlisting">// remove distortion in an image (after calibration)
cv::Mat CameraCalibrator::remap(const cv::Mat &amp;image) {

   cv::Mat undistorted;

   if (mustInitUndistort) { // called once per calibration
    
    cv::initUndistortRectifyMap(
      cameraMatrix,  // computed camera matrix
      distCoeffs,    // computed distortion matrix
      cv::Mat(),     // optional rectification (none) 
      cv::Mat(),     // camera matrix to generate undistorted
      image.size(),  // size of undistorted
      CV_32FC1,      // type of output map
      map1, map2);   // the x and y mapping functions

    mustInitUndistort= false;
   }

   // Apply mapping functions
   cv::remap(image, undistorted, map1, map2, 
      cv::INTER_LINEAR); // interpolation type

   return undistorted;
}</pre></div><p class="calibre8">Running this code results in the following image:</p><div class="mediaobject"><img src="../images/00172.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As you can see, once the <a id="id794" class="calibre1"/>image is undistorted, we obtain a regular perspective image.</p><p class="calibre8">To correct the distortion, OpenCV uses a polynomial function that is applied to the image points in order to move them at their undistorted position. By default, five coefficients are used; a model made of eight coefficients is also available. Once these coefficients are obtained, it is possible to compute two <code class="email">cv::Mat</code> mapping functions (one for the <code class="email">x</code> coordinate and one for the <code class="email">y</code> coordinate) that will give the new undistorted position of an image point on a distorted image. This is computed by the <code class="email">cv::initUndistortRectifyMap</code> function, and the <code class="email">cv::remap</code> function remaps all the points of an input image to a new image. Note that because of the nonlinear transformation, some pixels of the input image now fall outside the boundary of the output image. You can expand the size of the output image to compensate for this loss of pixels, but you will now obtain output pixels that have no values in the input image (they will then be displayed as black pixels).</p></div></div>

<div class="book" title="Calibrating a camera">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch10lvl2sec182" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">More options are available when it comes to camera calibration.</p><div class="book" title="Calibration with known intrinsic parameters"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch10lvl3sec47" class="calibre1"/>Calibration with known intrinsic parameters</h3></div></div></div><p class="calibre8">When a good <a id="id795" class="calibre1"/>estimate of the camera's intrinsic parameters is known, it could be advantageous to input them in the <code class="email">cv::calibrateCamera</code> function. They will then be used as initial values in the optimization process. To do so, you just need to add the <code class="email">CV_CALIB_USE_INTRINSIC_GUESS</code> flag and input these values in the calibration matrix parameter. It is also possible to impose a fixed value for the principal point (<code class="email">CV_CALIB_FIX_PRINCIPAL_POINT</code>), which can often be assumed to be the central pixel. You can also impose a fixed ratio for the focal lengths <code class="email">fx</code> and <code class="email">fy</code> (<code class="email">CV_CALIB_FIX_RATIO</code>); in which case, you assume the pixels of the square shape.</p></div><div class="book" title="Using a grid of circles for calibration"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch10lvl3sec48" class="calibre1"/>Using a grid of circles for calibration </h3></div></div></div><p class="calibre8">Instead of the <a id="id796" class="calibre1"/>usual chessboard pattern, OpenCV also offers the possibility to calibrate a camera by using a grid of circles. In this case, the centers of the circles are used as calibration points. The corresponding function is very similar to the function we used to locate the chessboard corners:</p><div class="informalexample"><pre class="programlisting">      cv::Size boardSize(7,7);
      std::vector&lt;cv::Point2f&gt; centers;
      bool found = cv:: findCirclesGrid(
                          image, boardSize, centers);</pre></div></div></div></div>

<div class="book" title="Calibrating a camera">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch10lvl2sec183" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <span class="strong"><em class="calibre9">Computing a homography between two images</em></span> recipe in this chapter will examine the projective equation in special situations</li><li class="listitem">The <span class="strong"><em class="calibre9">A flexible new technique for camera calibration</em></span> article by Z. Zhang  in <span class="strong"><em class="calibre9">IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no 11, 2000</em></span>, is a classic paper on the problem of camera calibration</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Computing the fundamental matrix of an image pair"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec63" class="calibre1"/>Computing the fundamental matrix of an image pair</h1></div></div></div><p class="calibre8">The previous <a id="id797" class="calibre1"/>recipe showed you how to recover the projective equation of a single camera. In this recipe, we will explore the projective<a id="id798" class="calibre1"/> relationship that exists between two images that display the same scene. These two images could have been obtained by moving a camera at two different locations to take pictures from two viewpoints or by using two cameras, each of them taking a different picture of the scene. When these two cameras are separated by a rigid baseline, we use the term<a id="id799" class="calibre1"/> <span class="strong"><strong class="calibre2">stereovision</strong></span>.</p></div>

<div class="book" title="Computing the fundamental matrix of an image pair">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch10lvl2sec184" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">Let's now consider two cameras observing a given scene point, as shown in the following figure:</p><div class="mediaobject"><img src="../images/00173.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We learned that <a id="id800" class="calibre1"/>we can find the image <span class="strong"><strong class="calibre2">x</strong></span> of a 3D point <span class="strong"><strong class="calibre2">X</strong></span> by tracing a line joining this 3D point with the camera's center. Conversely, the<a id="id801" class="calibre1"/> scene point that has its image at the position <span class="strong"><strong class="calibre2">x</strong></span> on the image plane can be located anywhere on this line in the 3D space. This implies that if we want to find the corresponding point of a given image point in another image, we need to search along the projection of this line onto the second image plane. This imaginary line is called the <a id="id802" class="calibre1"/>
<span class="strong"><strong class="calibre2">epipolar line</strong></span> of point <span class="strong"><strong class="calibre2">x</strong></span>. It defines a fundamental constraint that must satisfy two corresponding points; that is, the match of a given point must lie on the epipolar line of this point in the other view, and the exact orientation of this epipolar line depends on the respective position of the two cameras. In fact, the configuration of the epipolar line characterizes the geometry of a two-view system.</p><p class="calibre8">Another observation that can be made from the geometry of this two-view system is that all the epipolar lines pass through the same point. This point corresponds to the projection of one camera's center onto the other camera. This special point is<a id="id803" class="calibre1"/> called an <span class="strong"><strong class="calibre2">epipole</strong></span>.</p><p class="calibre8">Mathematically, the<a id="id804" class="calibre1"/> relationship between an image point and its corresponding epipolar line can be expressed using a <code class="email">3x3</code> matrix as follows:</p><div class="mediaobject"><img src="../images/00174.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In projective geometry, a 2D line is also represented by a 3-vector. It corresponds to the set of 2D points, <code class="email">(x',y')</code>, that <a id="id805" class="calibre1"/>satisfy the equation <span class="strong"><em class="calibre9">l</em></span>
<span class="strong"><em class="calibre9"><sub class="calibre20">1</sub></em></span>
<span class="strong"><em class="calibre9">'x'+ l</em></span>
<span class="strong"><em class="calibre9"><sub class="calibre20">2</sub></em></span>
<span class="strong"><em class="calibre9">'y'+ l</em></span>
<span class="strong"><em class="calibre9"><sub class="calibre20">3</sub></em></span>
<span class="strong"><em class="calibre9">'=0</em></span> (the prime superscript denotes that this line belongs to the second image). Consequently, the matrix <span class="strong"><strong class="calibre2">F</strong></span>, called the fundamental matrix, maps a 2D image point in one view to an epipolar line in the other view.</p></div></div>

<div class="book" title="Computing the fundamental matrix of an image pair">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec185" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The fundamental matrix of an image pair can be estimated by solving a set of equations that involve a certain number of known matched points between the two images. The minimum number of such matches is seven. In order to illustrate the fundamental matrix estimation process and using the image pair from the previous chapter, we can manually select seven good matches. These will be used to compute the fundamental matrix using the <code class="email">cv::findFundamentalMat</code> OpenCV function, as shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00175.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If we have<a id="id806" class="calibre1"/> the image <a id="id807" class="calibre1"/>points in each image as the <code class="email">cv::keypoint</code> instances (for example, if they were detected using a keypoint detector as in <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <span class="strong"><em class="calibre9">Detecting Interest Points</em></span>), they first need to be converted into <code class="email">cv::Point2f</code> in order to be used with <code class="email">cv::findFundamentalMat</code>. An OpenCV function can be used to this end:</p><div class="informalexample"><pre class="programlisting">   // Convert keypoints into Point2f
   std::vector&lt;cv::Point2f&gt; selPoints1, selPoints2;
   std::vector&lt;int&gt; pointIndexes1, pointIndexes2;
   cv::KeyPoint::convert(keypoints1,selPoints1,pointIndexes1);
   cv::KeyPoint::convert(keypoints2,selPoints2,pointIndexes2);</pre></div><p class="calibre8">The two vectors <code class="email">selPoints1</code> and <code class="email">selPoints2</code> contain the corresponding points in the two images. The keypoint instances are <code class="email">keypoints1</code> and <code class="email">keypoints2</code>. The <code class="email">pointIndexes1</code> and <code class="email">pointIndexes2</code> vectors contain the indexes of the keypoints to be converted. The call to the <code class="email">cv::findFundamentalMat</code> function is then as follows:</p><div class="informalexample"><pre class="programlisting">   // Compute F matrix from 7 matches
   cv::Mat fundamental= cv::findFundamentalMat(
      selPoints1,    // 7 points in first image
      selPoints2,    // 7 points in second image
      CV_FM_7POINT); // 7-point method</pre></div><p class="calibre8">One way to<a id="id808" class="calibre1"/> visually<a id="id809" class="calibre1"/> verify the validity of the fundamental matrix is to draw the epipolar lines of some selected points. Another OpenCV function allows the epipolar lines of a given set of points to be computed. Once these are computed, they can be drawn using the <code class="email">cv::line</code> function. The following lines of code accomplish these two steps (that is, computing and drawing epipolar lines in the image on the right from the points in the image on the left):</p><div class="informalexample"><pre class="programlisting">   // draw the left points corresponding epipolar 
   // lines in right image 
   std::vector&lt;cv::Vec3f&gt; lines1; 
   cv::computeCorrespondEpilines(
      selPoints1,  // image points 
      1,           // in image 1 (can also be 2)
      fundamental, // F matrix
      lines1);     // vector of epipolar lines

   // for all epipolar lines
   for (vector&lt;cv::Vec3f&gt;::const_iterator it= lines1.begin();
       it!=lines1.end(); ++it) {
          // draw the line between first and last column
          cv::line(image2,
            cv::Point(0,-(*it)[2]/(*it)[1]),
            cv::Point(image2.cols,-((*it)[2]+
                      (*it)[0]*image2.cols)/(*it)[1]),
                      cv::Scalar(255,255,255));
   }</pre></div><p class="calibre8">The result can be seen in the following screenshot:</p><div class="mediaobject"><img src="../images/00176.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Remember that the <a id="id810" class="calibre1"/>epipole is at the intersection of all the epipolar lines, and it is the projection of the other camera's center. This epipole is<a id="id811" class="calibre1"/> visible in the preceding image. Often, the epipolar lines intersect outside the image boundaries. In the case of our example, it is at the location where the first camera would be visible if the two images were taken at the same instant. Note that the results can be quite instable when the fundamental matrix is computed from seven matches. Indeed, substituting one match for another could lead to a significantly different set of epipolar lines.</p></div></div>

<div class="book" title="Computing the fundamental matrix of an image pair">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch10lvl2sec186" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">We previously explained that for a point in one image, the fundamental matrix gives the equation of the line on which its corresponding point in the other view should be found. If the corresponding point of a point <code class="email">p</code> (expressed in homogenous coordinates) is <code class="email">p'</code> and if <code class="email">F</code> is the fundamental matrix between the two views, then since <code class="email">p'</code> lies on the epipolar line <code class="email">Fp</code>, we have the following equation:</p><div class="mediaobject"><img src="../images/00177.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This equation expresses <a id="id812" class="calibre1"/>the relationship between two corresponding points and is known as the <a id="id813" class="calibre1"/>
<span class="strong"><strong class="calibre2">epipolar constraint</strong></span>. Using this equation, it <a id="id814" class="calibre1"/>becomes possible to estimate the entries of the matrix using known matches. Since the entries of the <code class="email">F</code> matrix are given up to a scale factor, there are only eight entries to be estimated (the ninth can be arbitrarily set to <code class="email">1</code>). Each match contributes to one equation. Therefore, with eight known matches, the matrix can be fully estimated by solving the resulting set of linear equations. This is what is done when you use the <code class="email">CV_FM_8POINT</code> flag with the <code class="email">cv::findFundamentalMat</code> function. Note that in this case, it is possible (and preferable) to input more than eight matches. The obtained over-determined system of linear equations can then be solved in a mean-square sense.</p><p class="calibre8">To estimate the fundamental matrix, an additional constraint can also be exploited. Mathematically, the F matrix maps a 2D point to a 1D pencil of lines (that is, lines that intersect at a common point). The fact that all these epipolar lines pass through this unique point (that is, the epipole) imposes a constraint on the matrix. This constraint reduces the number of matches required to estimate the fundamental matrix to seven. Unfortunately, in this case, the set of equations become nonlinear with up to three possible solutions (in this case, <code class="email">cv::findFundamentalMat</code> will return a fundamental matrix of the size <code class="email">9x3</code>, that is, three <code class="email">3x3</code> matrices stacked up). The seven-match solution of the <code class="email">F</code> matrix estimation can be invoked in OpenCV by using the <code class="email">CV_FM_7POINT</code> flag. This is what we did in the example of the preceding section.</p><p class="calibre8">Lastly, we would like to mention that the choice of an appropriate set of matches in the image is important to obtain an accurate estimation of the fundamental matrix. In general, the matches should be well distributed across the image and include points at different depths in the scene. Otherwise, the solution will become unstable or degenerate configurations. In particular, the selected scene points should not be coplanar as the fundamental matrix (in this case) becomes degenerated.</p></div></div>

<div class="book" title="Computing the fundamental matrix of an image pair">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch10lvl2sec187" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre9">Multiple View Geometry in Computer Vision, Cambridge University Press, 2004, R. Hartley and A. Zisserman</em></span>, is the most complete reference on projective geometry in computer vision</li><li class="listitem">The next recipe explains how a fundamental matrix can be robustly estimated from a larger match set</li><li class="listitem">The <span class="strong"><em class="calibre9">Computing a homography between two images</em></span> recipe explains why a fundamental matrix cannot be computed when the matched points are coplanar or are the result of a pure rotation</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Matching images using a random sample consensus"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec64" class="calibre1"/>Matching images using a random sample consensus</h1></div></div></div><p class="calibre8">When two <a id="id815" class="calibre1"/>cameras observe the same scene, they see the same elements but under different viewpoints. We have already studied <a id="id816" class="calibre1"/>the feature point matching problem in the previous chapter. In this recipe, we come back to this problem, and we will learn how to exploit the epipolar constraint between two views to match image features more reliably.</p><p class="calibre8">The principle that we will follow is simple: when we match feature points between two images, we only accept those matches that fall on the corresponding epipolar lines. However, to be able to check this condition, the fundamental matrix must be known, but we need good matches to estimate this matrix. This seems to be a chicken-and-egg problem. However, in this recipe, we propose a solution in which the fundamental matrix and a set of good matches will be jointly computed.</p></div>

<div class="book" title="Matching images using a random sample consensus">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch10lvl2sec188" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The objective is to be able to compute a fundamental matrix and a set of good matches between two views. To do so, all the found feature point correspondences will be validated using the epipolar constraint introduced in the previous recipe. To this end, we have created a class that encapsulates the different steps of the proposed robust matching process:</p><div class="informalexample"><pre class="programlisting">class RobustMatcher {
  private:
    // pointer to the feature point detector object
    cv::Ptr&lt;cv::FeatureDetector&gt; detector;
    // pointer to the feature descriptor extractor object
    cv::Ptr&lt;cv::DescriptorExtractor&gt; extractor;
    int normType;
    float ratio; // max ratio between 1st and 2nd NN
    bool refineF; // if true will refine the F matrix
    double distance; // min distance to epipolar
    double confidence; // confidence level (probability)

  public:
    RobustMatcher(std::string detectorName, // specify by name
                   std::string descriptorName) 
      : normType(cv::NORM_L2), ratio(0.8f), 
          refineF(true), confidence(0.98), distance(3.0) {    

      // construct by name
      if (detectorName.length()&gt;0) {
      detector= cv::FeatureDetector::create(detectorName); 
      extractor= cv::DescriptorExtractor::
                           create(descriptorName);
      }
    }</pre></div><p class="calibre8">Note how we <a id="id817" class="calibre1"/>used the <code class="email">create</code> methods of the <code class="email">cv::FeatureDetector</code> and <code class="email">cv::DescriptorExtractor</code> interfaces so that a user can<a id="id818" class="calibre1"/> select the <code class="email">create</code> methods by their names. Note that the <code class="email">create</code> methods can also be specified using the defined <code class="email">setFeatureDetector</code> and <code class="email">setDescriptorExtractor</code> setter methods.</p><p class="calibre8">The main method is our <code class="email">match</code> method that returns matches, detected keypoints, and the estimated fundamental matrix. The method proceeds in four distinct steps (explicitly identified in the comments of the following code) that we will now explore:</p><div class="informalexample"><pre class="programlisting">// Match feature points using RANSAC
// returns fundamental matrix and output match set
cv::Mat match(cv::Mat&amp; image1, cv::Mat&amp; image2, // input images 
    std::vector&lt;cv::DMatch&gt;&amp; matches,           // output matches
    std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1,      // output keypoints
    std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2) { 
        
    // 1. Detection of the feature points
    detector-&gt;detect(image1,keypoints1);
    detector-&gt;detect(image2,keypoints2);

    // 2. Extraction of the feature descriptors
    cv::Mat descriptors1, descriptors2;
    extractor-&gt;compute(image1,keypoints1,descriptors1);
    extractor-&gt;compute(image2,keypoints2,descriptors2);

    // 3. Match the two image descriptors
    //    (optionnally apply some checking method)
   
    // Construction of the matcher with crosscheck 
    cv::BFMatcher matcher(normType,   //distance measure
                            true);    // crosscheck flag
    // match descriptors
    std::vector&lt;cv::DMatch&gt; outputMatches;
    matcher.match(descriptors1,descriptors2,outputMatches);

    // 4. Validate matches using RANSAC
    cv::Mat fundaental= ransacTest(outputMatches, 
                              keypoints1, keypoints2, matches);
    // return the found fundamental matrix
    return fundamental;
  }</pre></div><p class="calibre8">The first two <a id="id819" class="calibre1"/>steps simply detect the feature points and compute their descriptors. Next, we proceed to feature matching using<a id="id820" class="calibre1"/> the <code class="email">cv::BFMatcher</code> class, as we did in the previous chapter. We use the crosscheck flag to obtain matches of better quality.</p><p class="calibre8">The fourth step is the new concept introduced in this recipe. It consists of an additional filtering test that will this time use the fundamental matrix in order to reject matches that do not obey the epipolar constraint. This test is based on the <code class="email">RANSAC</code> method that can compute the fundamental matrix even when outliers are still present in the match set (this method will be explained in the next section):</p><div class="informalexample"><pre class="programlisting">// Identify good matches using RANSAC
// Return fundamental matrix and output matches
cv::Mat ransacTest(const std::vector&lt;cv::DMatch&gt;&amp; matches,
                   const std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1, 
                   const std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2,
                   std::vector&lt;cv::DMatch&gt;&amp; outMatches) {

// Convert keypoints into Point2f  
  std::vector&lt;cv::Point2f&gt; points1, points2;    
  for (std::vector&lt;cv::DMatch&gt;::const_iterator it= 
  matches.begin(); it!= matches.end(); ++it) {

       // Get the position of left keypoints
       points1.push_back(keypoints1[it-&gt;queryIdx].pt);
       // Get the position of right keypoints
       points2.push_back(keypoints2[it-&gt;trainIdx].pt);
    }

  // Compute F matrix using RANSAC
  std::vector&lt;uchar&gt; inliers(points1.size(),0);
  cv::Mat fundamental= cv::findFundamentalMat(
      points1,points2, // matching points
      inliers,      // match status (inlier or outlier)  
      CV_FM_RANSAC, // RANSAC method
      distance,     // distance to epipolar line
      confidence);  // confidence probability
  
  // extract the surviving (inliers) matches
  std::vector&lt;uchar&gt;::const_iterator itIn= inliers.begin();
  std::vector&lt;cv::DMatch&gt;::const_iterator itM= matches.begin();
  // for all matches
  for ( ;itIn!= inliers.end(); ++itIn, ++itM) {
    if (*itIn) { // it is a valid match
      outMatches.push_back(*itM);
    }
  }
  return fundamental;
}</pre></div><p class="calibre8">This code is a<a id="id821" class="calibre1"/> bit long because the keypoints need<a id="id822" class="calibre1"/> to be converted into <code class="email">cv::Point2f</code> before the F matrix computation. Using this class, the robust matching of an image pair is then easily accomplished by the following calls:</p><div class="informalexample"><pre class="programlisting">  // Prepare the matcher (with default parameters)
  RobustMatcher rmatcher("SURF"); // we use SURF features here
  // Match the two images
  std::vector&lt;cv::DMatch&gt; matches;
  std::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2;
  cv::Mat fundamental= rmatcher.match(image1,image2,
                           matches, keypoints1, keypoints2);</pre></div><p class="calibre8">This results in <code class="email">62</code> matches that are shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00178.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Interestingly, almost <a id="id823" class="calibre1"/>all these matches are correct, even<a id="id824" class="calibre1"/> if a few false matches remain; these accidently fell on the corresponding epipolar lines of the computed fundamental matrix.</p></div></div>

<div class="book" title="Matching images using a random sample consensus">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec189" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">In the preceding recipe, we learned that it is possible to estimate the fundamental matrix associated with an image pair from a number of feature point matches. Obviously, to be exact, this match set must be made up of only good matches. However, in a real context, it is not possible to guarantee that a match set obtained by comparing the descriptors of the detected feature points will be completely exact. This is why a fundamental matrix estimation method based on the <a id="id825" class="calibre1"/>
<span class="strong"><strong class="calibre2">RANSAC</strong></span> (<span class="strong"><strong class="calibre2">RANdom SAmpling Consensus</strong></span>) strategy has been introduced.</p><p class="calibre8">The RANSAC algorithm aims at estimating a given mathematical entity from a data set that may contain a number of outliers. The idea is to randomly select some data points from the set and perform the estimation only with these. The number of selected points should be the minimum number of points required to estimate the mathematical entity. In the case of the fundamental matrix, eight matched pairs is the minimum number (in fact, it could be seven matches, but the 8-point linear algorithm is faster to compute). Once the fundamental matrix is estimated from these eight random matches, all the other matches in the match set are tested against the epipolar constraint that derives from this matrix. All the matches that fulfill this constraint (that is, matches for which the corresponding feature is at a short distance from its epipolar line) are identified. These matches form the <span class="strong"><strong class="calibre2">support set</strong></span><a id="id826" class="calibre1"/> of the computed fundamental matrix.</p><p class="calibre8">The central<a id="id827" class="calibre1"/> idea behind the RANSAC <a id="id828" class="calibre1"/>algorithm is that the larger the support set, the higher the probability that the computed matrix is the right one. Conversely, if one (or more) of the randomly selected matches is a wrong match, then the computed fundamental matrix will also be incorrect, and its support set is expected to be small. This process is repeated a number of times, and in the end, the matrix with the largest support will be retained as the most probable one.</p><p class="calibre8">Therefore, our objective is to pick eight random matches several times so that eventually we select eight good ones, which should give us a large support set. Depending on the number of wrong matches in the entire data set, the probability of selecting a set of eight correct matches will differ. We, however, know that the more selections we make, the higher our confidence will be that we have at least one good match set among those selections. More precisely, if we assume that the match set is made of <code class="email">w%</code> inliers (good matches), then the probability that we select eight good matches is <code class="email">w%</code>. Consequently, the probability that a selection contains at least one wrong match is <code class="email">(1-w)</code>. If we make <code class="email">k</code> selections, the probability of having one random set that contains good matches only is <code class="email">1-(1-w)k</code>. This is the confidence probability, <code class="email">c</code>, and we want this probability to be as high as possible since we need at least one good set of matches in order to obtain the correct fundamental matrix. Therefore, when running the RANSAC algorithm, one needs to determine the number of <code class="email">k</code> selections that need to be made in order to obtain a given confidence level.</p><p class="calibre8">When using the <code class="email">cv::findFundamentalMat</code> function with the <code class="email">CV_FM_RANSAC</code> method, two extra parameters are provided. The first parameter is the confidence level, which determines the number of iterations to be made (by default, it is <code class="email">0.99</code>). The second parameter is the maximum distance to the epipolar line for a point to be considered as an inlier. All the matched pairs in which a point is at a greater distance from its epipolar line than the distance specified will be reported as an outlier. The function also returns <code class="email">std::vector</code> of the character value, indicating that the corresponding match in the input set has been identified as an outlier (<code class="email">0</code>) or as an inlier (<code class="email">1</code>).</p><p class="calibre8">The more good matches you have in your initial match set, the higher the probability that RANSAC will give you the correct fundamental matrix. This is why we applied the crosscheck filter <a id="id829" class="calibre1"/>when matching the feature <a id="id830" class="calibre1"/>points. You could have also used the ratio test presented in the previous recipe in order to further improve the quality of the final match set. It is just a question of balancing the computational complexity, the final number of matches, and the required level of confidence that the obtained match set will contain only exact matches.</p></div></div>

<div class="book" title="Matching images using a random sample consensus">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch10lvl2sec190" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The result of the robust matching process presented in this recipe is an estimate of the fundamental matrix computed using the eight selected matches that have the largest support and the set matches included in this support set. Using this information, it is possible to refine these results in two ways.</p><div class="book" title="Refining the fundamental matrix"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch10lvl3sec49" class="calibre1"/>Refining the fundamental matrix</h3></div></div></div><p class="calibre8">Since we now <a id="id831" class="calibre1"/>have a match set of good quality, as a last step, it might be a good idea to use all of them to re-estimate the fundamental matrix. We already mentioned that there exists a linear 8-point algorithm to estimate this matrix. We can, therefore, obtain an over-determined system of equations that will solve the fundamental matrix in a least-squares sense. This step can be added at the end of our <code class="email">ransacTest</code> function:</p><div class="informalexample"><pre class="programlisting">    if (refineF) {
      // The F matrix will 
      // be recomputed with all accepted matches

      // Convert keypoints into Point2f 
      points1.clear();
      points2.clear();
      for (std::vector&lt;cv::DMatch&gt;::
                  const_iterator it= outMatches.begin();
         it!= outMatches.end(); ++it) {

         // Get the position of left keypoints
         points1.push_back(keypoints1[it-&gt;queryIdx].pt);

         // Get the position of right keypoints
         points2.push_back(keypoints2[it-&gt;trainIdx].pt);
      }

      // Compute 8-point F from all accepted matches
      fundamental= cv::findFundamentalMat(
        points1,points2, // matching points
        CV_FM_8POINT); // 8-point method solved using SVD
    }</pre></div><p class="calibre8">The <code class="email">cv::findFundamentalMat</code> function <a id="id832" class="calibre1"/>indeed accepts more than <code class="email">8</code> matches by <a id="id833" class="calibre1"/>solving the linear system of equations using singular value decomposition.</p></div><div class="book" title="Refining the matches"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch10lvl3sec50" class="calibre1"/>Refining the matches</h3></div></div></div><p class="calibre8">We learned <a id="id834" class="calibre1"/>that in a two-view system, every point must lie on the epipolar line of its corresponding point. This is the epipolar constraint expressed by the fundamental matrix. Consequently, if you have a good estimate of a fundamental matrix, you can use this epipolar constraint to correct the obtained matches by forcing them to lie on their epipolar lines. This can be easily done by using the <code class="email">cv::correctMatches</code> OpenCV function:</p><div class="informalexample"><pre class="programlisting">  std::vector&lt;cv::Point2f&gt; newPoints1, newPoints2;  
  // refine the matches
  correctMatches(fundamental,            // F matrix
                points1, points2,        // original position
                newPoints1, newPoints2); // new position</pre></div><p class="calibre8">This function proceeds by modifying the position of each corresponding point such that it satisfies the epipolar constraint while minimizing the cumulative (squared) displacement.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Computing a homography between two images"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec65" class="calibre1"/>Computing a homography between two images</h1></div></div></div><p class="calibre8">The second <a id="id835" class="calibre1"/>recipe of this chapter showed you how to compute the fundamental matrix of an image pair from a set of matches. In projective geometry, another very useful mathematical entity also exists. This one can be computed from multiview imagery and, as we will see, is a matrix with special properties.</p></div>

<div class="book" title="Computing a homography between two images">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch10lvl2sec191" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">Again, let's consider the projective relation between a 3D point and its image on a camera, which we introduced in the first recipe of this chapter. Basically, we learned that this equation relates a 3D point with its image using the intrinsic properties of the camera and the position of this camera (specified with a rotation and a translation component). If we now carefully examine this equation, we realize that there are two special situations of particular interest. The first situation is when two views of a scene are separated by a pure rotation. It can then be observed that the fourth column of the extrinsic matrix will be made up of 0s (that is, the translation is null):</p><div class="mediaobject"><img src="../images/00179.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As a result, the projective relation in this special case becomes a <code class="email">3x3</code> matrix. A similarly interesting situation <a id="id836" class="calibre1"/>also occurs when the object we observe is a plane. In this specific case, we can assume that the points on this plane will be located at <code class="email">Z=0</code>, without the loss of generality. As a result, we obtain the following equation:</p><div class="mediaobject"><img src="../images/00180.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This zero coordinate of the scene points will then cancel the third column of the projective matrix, which will then again become a <code class="email">3x3</code> matrix. This special matrix is called a <a id="id837" class="calibre1"/>
<span class="strong"><strong class="calibre2">homography</strong></span>, and it implies that, under special circumstances (here, a pure rotation or a planar object), a point is related to its image by a linear relation of the following form:</p><div class="mediaobject"><img src="../images/00181.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <code class="email">H</code> is a <code class="email">3x3</code> matrix. This relation holds up to a scale factor represented here by the <code class="email">s</code> scalar value. Once this matrix is estimated, all the points in one view can be transferred to a second view using this relation. Note that as a side effect of the homography relation, the fundamental matrix becomes undefined in these cases.</p></div></div>

<div class="book" title="Computing a homography between two images">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch10lvl2sec192" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Suppose that we have<a id="id838" class="calibre1"/> two images separated by a pure rotation. This happens, for example, when you take pictures of a building or a landscape by rotating yourself; as you are sufficiently far away from your subject, the translational component is negligible. These two images can be matched using the features of your choice and the <code class="email">cv::BFMatcher</code> function. Then, as we did in the previous recipe, we will apply a RANSAC step that will this time involve the estimation of a homography based on a match set (which obviously contains a good number of outliers). This is done by using the <code class="email">cv::findHomography</code> function, which is very similar to the <code class="email">cv::findFundamentalMat</code> function:</p><div class="informalexample"><pre class="programlisting">// Find the homography between image 1 and image 2
std::vector&lt;uchar&gt; inliers(points1.size(),0);
cv::Mat homography= cv::findHomography(
  points1, points2, // corresponding points
  inliers,   // outputed inliers matches 
  CV_RANSAC, // RANSAC method
  1.);       // max distance to reprojection point</pre></div><p class="calibre8">Recall that a homography exists (instead of a fundamental matrix) because our two images are separated by a pure rotation. The images are shown here. We also displayed the inlier keypoints as identified by the <code class="email">inliers</code> argument of the function. Refer to the following screenshot:</p><div class="mediaobject"><img src="../images/00182.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The<a id="id839" class="calibre1"/> second image is shown as follows:</p><div class="mediaobject"><img src="../images/00183.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The resulting<a id="id840" class="calibre1"/> inliers that comply with the found homography have been drawn on these images using the following loop:</p><div class="informalexample"><pre class="programlisting">   // Draw the inlier points
   std::vector&lt;cv::Point2f&gt;::const_iterator itPts=  
                                            points1.begin();
   std::vector&lt;uchar&gt;::const_iterator itIn= inliers.begin();
   while (itPts!=points1.end()) {

      // draw a circle at each inlier location
      if (*itIn) 
          cv::circle(image1,*itPts,3,
                    cv::Scalar(255,255,255));
      ++itPts;
      ++itIn;
   }</pre></div><p class="calibre8">The homography is a <code class="email">3x3</code> invertible matrix; therefore, once it has been computed, you can transfer image points from one image to the other. In fact, you can do this for every pixel of an image. Consequently, you can transfer a complete image to the point of view of a second image. This process is called image<a id="id841" class="calibre1"/> <span class="strong"><strong class="calibre2">mosaicking</strong></span>, and it is often used to build a large panorama from multiple images. An OpenCV function that does exactly this is given as follows:</p><div class="informalexample"><pre class="programlisting">   // Warp image 1 to image 2
   cv::Mat result;
   cv::warpPerspective(image1,  // input image
      result,                   // output image
      homography,               // homography
      cv::Size(2*image1.cols,
                 image1.rows)); // size of output image</pre></div><p class="calibre8">Once this <a id="id842" class="calibre1"/>new image is obtained, it can be appended to the other image in order to expand the view (since the two images are now from the same point of view):</p><div class="informalexample"><pre class="programlisting">   // Copy image 1 on the first half of full image
   cv::Mat half(result,cv::Rect(0,0,image2.cols,image2.rows));
   image2.copyTo(half); // copy image2 to image1 roi</pre></div><p class="calibre8">The following image is the result:</p><div class="mediaobject"><img src="../images/00184.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Computing a homography between two images">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch10lvl2sec193" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">When two views are related by a homography, it becomes possible to determine where a given scene point on one image is found on the other image. This property becomes particularly interesting for the points in one image that fall outside the image boundaries of the other. Indeed, since the second view shows a portion of the scene that is not visible in the first image, you can use the homography in order to expand the image by reading the color value of the additional pixels in the other image. That's how we were able to create a new image that is an expansion of our second image in which extra columns were added to the right-hand side.</p><p class="calibre8">The homography <a id="id843" class="calibre1"/>computed by <code class="email">cv::findHomography</code> is the one that maps the points in the first image to the points in the second image. This homography can be computed from a minimum of four matches, and the RANSAC algorithm is again used here. Once the homography with the best support is found, the <code class="email">cv::findHomography</code> method<a id="id844" class="calibre1"/> refines it using all the identified inliers.</p><p class="calibre8">Now, in order to transfer the points of image <code class="email">1</code> to image <code class="email">2</code>, what we need is, in fact, inverse homography. This is exactly what the <code class="email">cv::warpPerspective</code> function is doing by default; that is, it uses the inverse of the homography provided as the input to get the color value of each point of the output image (this is what we called backward mapping in <a class="calibre1" title="Chapter 2. Manipulating Pixels" href="part0019_split_000.html#page">Chapter 2</a>, <span class="strong"><em class="calibre9">Manipulating Pixels</em></span>). When an output pixel is transferred to a point outside the input image, a black value (<code class="email">0</code>) is simply assigned to this pixel. Note that a <code class="email">cv::WARP_INVERSE_MAP</code> flag can be specified as the optional fifth argument in <code class="email">cv::warpPerspective</code> if you want to use direct homography instead of the inverted one during the pixel transfer process.</p></div></div>

<div class="book" title="Computing a homography between two images">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch10lvl2sec194" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">A homography also exists between two images of a plane. We can then make use of this to recognize a planar object in an image.</p><div class="book" title="Detecting planar targets in an image"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch10lvl3sec51" class="calibre1"/>Detecting planar targets in an image</h3></div></div></div><p class="calibre8">Suppose you want <a id="id845" class="calibre1"/>to detect the occurrence of a planar object in an image. This object could be a poster, painting, signage, book cover (as in the following example), and so on. Based on what we learned in this chapter, the strategy would consist of detecting feature points on this object and to try and match them with the feature points in the image. These matches would then be validated using a robust matching scheme similar to the one we used in the previous recipe, but this time based on a homography.</p><p class="calibre8">Let's define a <code class="email">TargetMatcher</code> class very similar to our <code class="email">RobustMatcher</code> class:</p><div class="informalexample"><pre class="programlisting">class TargetMatcher {

  private:

    // pointer to the feature point detector object
    cv::Ptr&lt;cv::FeatureDetector&gt; detector;
    // pointer to the feature descriptor extractor object
    cv::Ptr&lt;cv::DescriptorExtractor&gt; extractor;
    cv::Mat target; // target image
    int normType;
    double distance; // min reprojection error</pre></div><p class="calibre8">Here, we <a id="id846" class="calibre1"/>simply add a <code class="email">target</code> attribute that represents the reference image of the planar object to be matched. The matching methods are identical to the ones of the <code class="email">RobustMatcher</code> class, except that they include <code class="email">cv::findHomography</code> instead of <code class="email">cv::findFundamentalMat</code> in the <code class="email">ransacTest</code> method. We also added a method to initiate target matching and find the position of the target:</p><div class="informalexample"><pre class="programlisting">  // detect the defined planar target in an image
  // returns the homography 
  // the 4 corners of the detected target
  // plus matches and keypoints
  cv::Mat detectTarget(const cv::Mat&amp; image, 
    // position of the target corners (clock-wise)
    std::vector&lt;cv::Point2f&gt;&amp; detectedCorners,       
    std::vector&lt;cv::DMatch&gt;&amp; matches,
    std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1,
    std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2) {
    // find a RANSAC homography between target and image
    cv::Mat homography= match(target,image,matches, 
                                keypoints1, keypoints2);
    // target corners
    std::vector&lt;cv::Point2f&gt; corners;  
    corners.push_back(cv::Point2f(0,0));
    corners.push_back(cv::Point2f(target.cols-1,0));
    corners.push_back(cv::Point2f(target.cols-1,target.rows-1));
    corners.push_back(cv::Point2f(0,target.rows-1));

    // reproject the target corners
    cv::perspectiveTransform(corners,detectedCorners,
                                      homography);
    return homography;
  }</pre></div><p class="calibre8">Once the homography has been found by the match method, we define the four corners of the target (that is, the four corners of its reference image). These are then transferred to the image using the <code class="email">cv::perspectiveTransform</code> function. This function simply multiplies each point in the input vector by the homography matrix. This gives us the coordinates of these points in the other image. Target matching is then performed as follows:</p><div class="informalexample"><pre class="programlisting">// Prepare the matcher 
TargetMatcher tmatcher("FAST","FREAK");
tmatcher.setNormType(cv::NORM_HAMMING);

// definition of the output data
std::vector&lt;cv::DMatch&gt; matches;
std::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2;
std::vector&lt;cv::Point2f&gt; corners;
// the reference image
tmatcher.setTarget(target); 
// match image with target
tmatcher.detectTarget(image,corners,matches,
                            keypoints1,keypoints2);
// draw the target corners on the image
cv::Point pt= cv::Point(corners[0]);
cv::line(image,cv::Point(corners[0]),cv::Point(corners[1]),
               cv::Scalar(255,255,255),3);
cv::line(image,cv::Point(corners[1]),cv::Point(corners[2]),
               cv::Scalar(255,255,255),3);
cv::line(image,cv::Point(corners[2]),cv::Point(corners[3]),
               cv::Scalar(255,255,255),3);
cv::line(image,cv::Point(corners[3]),cv::Point(corners[0]),
               cv::Scalar(255,255,255),3);</pre></div><p class="calibre8">Using <a id="id847" class="calibre1"/>the <code class="email">cv::drawMatches</code> function, we display the results as follows:</p><div class="mediaobject"><img src="../images/00185.jpeg" alt="Detecting planar targets in an image" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">You can also use homographies to modify the perspectives of planar objects. For example, if you <a id="id848" class="calibre1"/>have several pictures from different points of view of the flat facade of a building, you can compute the homography between these images and build a large mosaic of the facade by wrapping the images and assembling them together, as we did in this recipe. A minimum of four matched points between two views are required to compute a homography. The <code class="email">cv::getPerspectiveTransform</code> function<a id="id849" class="calibre1"/> allows such a transformation from four corresponding points to be computed.</p></div></div></div>

<div class="book" title="Computing a homography between two images">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch10lvl2sec195" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <span class="strong"><em class="calibre9">Remapping an image</em></span> recipe in <a class="calibre1" title="Chapter 2. Manipulating Pixels" href="part0019_split_000.html#page">Chapter 2</a>, <span class="strong"><em class="calibre9">Manipulating Pixels</em></span>, discusses the concept of backward mapping</li><li class="listitem">The <span class="strong"><em class="calibre9">Automatic panoramic image stitching using invariant features</em></span> article by M.Brown and D.Lowe in <span class="strong"><em class="calibre9">International Journal of Computer Vision,74, 1, 2007</em></span>, describes the complete method to build panoramas from multiple images</li></ul></div></div></div></body></html>