- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If science-fiction stories are to be believed, the invention of artificial
    intelligence inevitably leads to apocalyptic wars between machines and their makers.
    The stories begin with today’s reality: computers being taught to play simple
    games like tic-tac-toe and to automate routine tasks. As the stories go, machines
    are later given control of traffic lights and communications, followed by military
    drones and missiles. The machines’ evolution takes an ominous turn once the computers
    become sentient and learn how to teach themselves. Having no more need for human
    programmers, humankind is then “deleted.”'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, at the time of writing, machines still require user input.
  prefs: []
  type: TYPE_NORMAL
- en: Though your impressions of machine learning may be colored by these mass-media
    depictions, today’s algorithms have little danger of becoming self-aware. The
    goal of today’s machine learning is not to create an artificial brain, but rather
    to assist us with making sense of and acting on the world’s rapidly accumulating
    data stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting popular misconceptions aside, by the end of this chapter, you will
    gain a more nuanced understanding of machine learning. You will also be introduced
    to the fundamental concepts that define and differentiate the most common machine
    learning approaches. You will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: The origins, applications, ethics, and pitfalls of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How computers transform data into knowledge and action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps needed to match a machine learning algorithm with your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of machine learning provides a set of algorithms that transform data
    into actionable knowledge. Keep reading to see how easy it is to use R to start
    applying machine learning to real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: The origins of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beginning at birth, we are inundated with data. Our body’s sensors—the eyes,
    ears, nose, tongue, and nerves—are continually assailed with raw data that our
    brain translates into sights, sounds, smells, tastes, and textures. Using language,
    we can share these experiences with others.
  prefs: []
  type: TYPE_NORMAL
- en: Since the advent of written language, humans have recorded their observations.
    Hunters monitored the movement of animal herds; early astronomers recorded the
    alignment of planets and stars; and cities recorded tax payments, births, and
    deaths. Today, such observations, and many more, are increasingly automated and
    recorded systematically in ever-growing computerized databases.
  prefs: []
  type: TYPE_NORMAL
- en: The invention of electronic sensors has additionally contributed to an explosion
    in the volume and richness of recorded data. Specialized sensors, such as cameras,
    microphones, chemical noses, electronic tongues, and pressure sensors mimic the
    human ability to see, hear, smell, taste, and feel. These sensors process the
    data far differently than a human being would. Unlike a human’s limited and subjective
    attention, an electronic sensor never takes a break and has no emotions to skew
    its perception.
  prefs: []
  type: TYPE_NORMAL
- en: Although sensors are not clouded by subjectivity, they do not necessarily report
    a single, definitive depiction of reality. Some have an inherent measurement error
    due to hardware limitations. Others are limited by their scope. A black-and-white
    photograph provides a different depiction of its subject than one shot in color.
    Similarly, a microscope provides a far different depiction of reality than a telescope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Between databases and sensors, many aspects of our lives are recorded. Governments,
    businesses, and individuals are recording and reporting information, from the
    monumental to the mundane. Weather sensors obtain temperature and pressure data;
    surveillance cameras watch sidewalks and subway tunnels; and all manner of electronic
    behaviors are monitored: transactions, communications, social media relationships,
    and many others.'
  prefs: []
  type: TYPE_NORMAL
- en: This deluge of data has led some to state that we have entered an era of **big
    data**, but this may be a bit of a misnomer. Human beings have always been surrounded
    by large amounts of data—one would need only to look to the sky and attempt to
    count its stars to discover a virtually endless supply. What makes the current
    era unique is that we have vast amounts of *recorded* data, much of which can
    be directly accessed by computers. Larger and more interesting datasets are increasingly
    accessible at the tips of our fingers, only a web search away. This wealth of
    information has the potential to inform action, given a systematic way of making
    sense of it all.
  prefs: []
  type: TYPE_NORMAL
- en: The field of study dedicated to the development of computer algorithms for transforming
    data into intelligent action is known as **machine learning**. This field originated
    in an environment where the available data, statistical methods, and computing
    power rapidly and simultaneously evolved. Growth in the volume of data necessitated
    additional computing power, which in turn spurred the development of statistical
    methods for analyzing large datasets. This created a cycle of advancement, allowing
    even larger and more interesting data to be collected, and enabled today’s environment
    in which endless streams of data are available on virtually any topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The cycle of advancement that enabled machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: A closely related sibling of machine learning, **data mining**, is concerned
    with the generation of novel insight from large databases. As the term implies,
    data mining involves a systematic hunt for nuggets of actionable intelligence.
    Although there is some disagreement over how widely machine learning and data
    mining overlap, one point of distinction is that machine learning focuses on teaching
    computers how to use data to solve a problem, while data mining focuses on teaching
    computers to identify patterns that humans then use to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Virtually all data mining involves the use of machine learning, but not all
    machine learning requires data mining. For example, you might apply machine learning
    to data mine automobile traffic data for patterns related to accident rates. On
    the other hand, if the computer is learning how to identify traffic signs, this
    is purely machine learning without data mining.
  prefs: []
  type: TYPE_NORMAL
- en: The phrase “data mining” is also sometimes used as a pejorative to describe
    the deceptive practice of cherry-picking data to support a theory.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is also intertwined with the field of **artificial intelligence**
    (**AI**), which is a nebulous discipline and, depending on whom you might ask,
    is simply machine learning with a strong marketing spin or a distinct field of
    study altogether. A cynic might suggest that the field of AI tends to exaggerate
    its importance such as by calling a simple predictive model an “AI bot,” while
    an AI proponent may point out that the field tends to tackle the most challenging
    learning tasks while aiming for human-level performance. The truth is somewhere
    in between.
  prefs: []
  type: TYPE_NORMAL
- en: Just as machine learning itself depends on statistical methods, artificial intelligence
    depends a great deal on machine learning, but the business contexts and applications
    tend to differ. The table that follows highlights some differentiators among traditional
    statistics, machine learning, and artificial intelligence; however, keep in mind
    that the lines between the three disciplines are often less rigid than they may
    appear.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Traditional statistics** | **Machine learning** | **Artificial intelligence**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Application** | Hypothesis testing and insight | Prediction and knowledge
    generation | Automation |'
  prefs: []
  type: TYPE_TB
- en: '| **Success criterion** | Greater understanding | Ability to intervene before
    things happen | Efficiency and cost savings |'
  prefs: []
  type: TYPE_TB
- en: '| **Success metric** | Statistical significance | Trustworthiness of predictions
    | Return on investment (ROI) |'
  prefs: []
  type: TYPE_TB
- en: '| **Input data size** | Smaller data | Medium data | Bigger data |'
  prefs: []
  type: TYPE_TB
- en: '| **Implementation** | Reports and presentations for knowledge sharing | Scoring
    databases or interventions in business practices | Custom applications and automated
    processes |'
  prefs: []
  type: TYPE_TB
- en: In this formulation, machine learning sits firmly at the intersection of human
    and computer partnership, whereas traditional statistics relies primarily on the
    human to drive insights and AI seeks to minimize human involvement as much as
    possible. Learning how to maximize the human-machine partnership and apply learning
    algorithms to real-world problems is the focus of this book. Understanding the
    use cases and limitations of machine learning is an important starting point in
    this journey.
  prefs: []
  type: TYPE_NORMAL
- en: Uses and abuses of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most people have heard of Deep Blue, the chess-playing computer that in 1997
    was the first to win a game against a world champion. Another famous computer,
    Watson, defeated two human opponents on the television trivia game show *Jeopardy*
    in 2011\. Based on these stunning accomplishments, some have speculated that computer
    intelligence will replace workers in information technology occupations, just
    as automobiles replaced horses and machines replaced workers in fields and assembly
    lines. Recently, these fears have become more pronounced as artificial intelligence-based
    algorithms, such as GPT-3 and DALL·E 2 from the OpenAI research group ([https://openai.com/](https://openai.com/)),
    have reached impressive milestones and are proving that computers are capable
    of writing text and creating artwork that is virtually indistinguishable from
    that produced by humans. Ultimately, this may lead to massive shifts in occupations
    like marketing, customer support, illustration, and so on, as creativity is outsourced
    to machines that can produce endless streams of material more cheaply than the
    former employees.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, humans may still be necessary because the truth is that even as
    machines reach such impressive milestones, they are still relatively limited in
    their ability to thoroughly understand a problem, or understand how the work is
    going to be applied toward a real-world goal. Learning algorithms are pure intellectual
    horsepower without direction. A computer may be more capable than a human of finding
    subtle patterns in large databases, but it still needs a human to motivate the
    analysis and turn the result into meaningful action. In most cases, the human
    will determine whether the machine’s output is valuable and will help the machine
    avoid creating a limitless supply of nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without completely discounting the achievements of Deep Blue and Watson, it
    is important to note that neither is even as intelligent as a typical five-year-old.
    For more on why “comparing smarts is a slippery business,” see the *Popular Science*
    article FYI, *Which Computer Is Smarter, Watson Or Deep Blue?*, by Will Grunewald,
    2012: [https://www.popsci.com/science/article/2012-12/fyi-which-computer-smarter-watson-or-deep-blue](https://www.popsci.com/science/article/2012-12/fyi-which-computer-smarter-watson-or-deep-blue).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machines are not good at asking questions or even knowing what questions to
    ask. They are much better at answering them, provided the question is stated in
    a way that the computer can comprehend. Present-day machine learning algorithms
    partner with people much like a bloodhound works with its trainer: the dog’s sense
    of smell may be many times stronger than its master’s, but without being carefully
    directed, the hound may end up chasing its tail.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Machine learning algorithms are powerful tools that require careful
    direction'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the worst-case scenario, if machine learning were implemented carelessly,
    it might lead to what controversial tech billionaire Elon Musk provocatively called
    “summoning the demon.” This perspective suggests that we may be unleashing forces
    outside our control, despite the hubristic sense that we will be able to reign
    them in when needed. Given the power of artificial intelligence to automate processes
    and react to changing conditions much faster and more objectively than humans,
    there may come a point at which Pandora’s box has been opened and it is difficult
    or impossible to return to the old ways of life where humans are in control. As
    Musk describes:'
  prefs: []
  type: TYPE_NORMAL
- en: ”If AI has a goal and humanity just happens to be in the way, it will destroy
    humanity as a matter of course without even thinking about it. No hard feelings…
    It’s just like, if we’re building a road and an anthill just happens to be in
    the way, we don’t hate ants, we’re just building a road, and so, goodbye anthill.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While this may seem to be a bleak portrayal, it is still the realm of far-future
    science fiction, as you will soon learn when reading about the present day’s state-of-the-art
    machine learning successes.
  prefs: []
  type: TYPE_NORMAL
- en: However, Musk’s warning does help emphasize the importance of understanding
    the likelihood of machine learning and AI being a double-edged sword. For all
    of its benefits, there are some places where it still has room for improvement,
    and some situations where it may do more harm than good. If machine learning practitioners
    cannot be trusted to act ethically, it may be necessary for governments to intervene
    to prevent the greatest harm to society.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more on Musk’s fears of “summoning the demon” see the following 2018 article
    from CNBC: [https://www.cnbc.com/2018/04/06/elon-musk-warns-ai-could-create-immortal-dictator-in-documentary.html](https://www.cnbc.com/2018/04/06/elon-musk-warns-ai-could-create-immortal-dictator-in-documentary.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning successes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning is most successful when it augments the specialized knowledge
    of a subject-matter expert rather than replacing the expert altogether. It works
    with medical doctors at the forefront of the fight to eradicate cancer; assists
    engineers with efforts to create smarter homes and automobiles; helps social scientists
    and economists build better societies; and provides business and marketing professionals
    with valuable insights. Toward these ends, it is employed in countless scientific
    laboratories, hospitals, companies, and governmental organizations. Any effort
    that generates or aggregates data likely employs at least one machine learning
    algorithm to help make sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though it is impossible to list every successful application of machine learning,
    a selection of prominent examples is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of unwanted spam messages in email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation of customer behavior for targeted advertising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasts of weather behavior and long-term climate changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preemptive interventions for customers likely to churn (stop purchasing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction of fraudulent credit card transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actuarial estimates of financial damage from storms and natural disasters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction of and influence over election outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development of algorithms for auto-piloting drones and self-driving cars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization of energy use in homes and office buildings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projection of areas where criminal activity is most likely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovery of genetic sequences useful for precision medicine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this book, you will understand the basic machine learning algorithms
    that are employed to teach computers to perform these tasks. For now, it suffices
    to say that no matter the context, the fundamental machine learning process is
    the same. In every task, an algorithm takes data and identifies patterns that
    form the basis for further action.
  prefs: []
  type: TYPE_NORMAL
- en: The limits of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although machine learning is used widely and has tremendous potential, it is
    important to understand its limits. The algorithms used today—even those on the
    cutting edge of artificial intelligence—emulate a relatively limited subset of
    the capabilities of the human brain. They offer little flexibility to extrapolate
    outside of strict parameters and know no common sense. Considering this, one should
    be extremely careful to recognize exactly what an algorithm has learned before
    setting it loose in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Without a lifetime of past experiences to build upon, computers are limited
    in their ability to make simple inferences about logical next steps. Consider
    the banner advertisements on websites, which are served according to patterns
    learned by data mining the browsing history of millions of users. Based on this
    data, someone who views websites selling mattresses is interested in buying a
    mattress and should therefore see advertisements for mattresses. The problem is
    that this becomes a never-ending cycle in which, even after a mattress has been
    purchased, additional mattress advertisements are shown, rather than advertisements
    for pillows and bed sheets.
  prefs: []
  type: TYPE_NORMAL
- en: Many people are familiar with the deficiencies of machine learning’s ability
    to understand or translate language, or to recognize speech and handwriting. Perhaps
    the earliest example of this type of failure is in a 1994 episode of the television
    show *The Simpsons*, which showed a parody of the Apple Newton tablet. In its
    time, the Newton was known for its state-of-the-art handwriting recognition. Unfortunately
    for Apple, it would occasionally fail to great effect. The television episode
    illustrated this through a sequence in which a bully’s note to “Beat up Martin”
    was misinterpreted by the Newton as “Eat up Martha.”
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B17290_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Screen captures from *Lisa on Ice, The Simpsons, 20th Century Fox*
    (1994)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine language processing has improved enough in the time since the Apple
    Newton that Google, Apple, and Microsoft are all confident in their ability to
    offer voice-activated virtual concierge services, such as Google Assistant, Siri,
    and Cortana. Still, these services routinely struggle to answer relatively simple
    questions. Furthermore, online translation services sometimes misinterpret sentences
    that a toddler would readily understand, and the predictive text feature on many
    devices has led to humorous “autocorrect fail” websites that illustrate computers’
    ability to understand basic language but completely misunderstand context.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these mistakes are to be expected. Language is complicated, with multiple
    layers of text and subtext, and even human beings sometimes misunderstand context.
    Although machine learning is rapidly improving at language processing, and current
    state-of-the-art algorithms like GPT-3 are quite good in comparison to prior generations,
    machines still make mistakes that are obvious to humans that know where to look.
    These predictable shortcomings illustrate the important fact that machine learning
    is only as good as the data it has learned from. If context is not explicit in
    the input data, then just like a human, the computer will have to make its best
    guess from its set of past experiences. However, the computer’s past experiences
    are usually much more limited than the human’s.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning ethics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, machine learning is simply a tool that assists us with making sense
    of the world’s complex data. Like any tool, it can be used for good or evil. Machine
    learning goes wrong mostly when it is applied so broadly, or so callously, that
    humans are treated as lab rats, automata, or mindless consumers. A process that
    may seem harmless can lead to unintended consequences when automated by an emotionless
    computer. For this reason, those using machine learning or data mining would be
    remiss not to at least briefly consider the ethical implications of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the relative youth of machine learning as a discipline and the speed
    at which it is progressing, the associated legal issues and social norms are often
    quite uncertain, and constantly in flux. Caution should be exercised when obtaining
    or analyzing data in order to avoid breaking laws, violating terms of service
    or data use agreements, or abusing the trust or violating the privacy of customers
    or the public. The informal corporate motto of Google, an organization that collects
    perhaps more data on individuals than any other, was at one time, “don’t be evil.”
    While this seems clear enough, it may not be sufficient. A better approach may
    be to follow the *Hippocratic Oath*, a medical principle that states, “above all,
    do no harm.” Following the principle of “do no harm” may have helped avoid recent
    scandals at Facebook and other companies, such as the Cambridge Analytica controversy,
    which alleged that social media data was being used to manipulate elections.
  prefs: []
  type: TYPE_NORMAL
- en: Retailers routinely use machine learning for advertising, targeted promotions,
    inventory management, or the layout of items in a store. Many have equipped checkout
    lanes with devices that print coupons for promotions based on a customer’s buying
    history. In exchange for a bit of personal data, the customer receives discounts
    on the specific products they want to buy. At first, this may appear relatively
    harmless, but consider what happens when this practice is taken a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: One possibly apocryphal tale concerns a large retailer in the United States
    that employed machine learning to identify expectant mothers for coupon mailings.
    The retailer hoped that if these mothers-to-be received substantial discounts,
    they would become loyal customers who would later purchase profitable items such
    as diapers, baby formula, and toys. Equipped with machine learning methods, the
    retailer identified items in the customer purchase history that could be used
    to predict with a high degree of certainty not only whether a woman was pregnant,
    but also the approximate timing for when the baby was due.
  prefs: []
  type: TYPE_NORMAL
- en: After the retailer used this data for a promotional mailing, an angry man contacted
    the chain and demanded to know why his young daughter received coupons for maternity
    items. He was furious that the retailer seemed to be encouraging teenage pregnancy!
    As the story goes, when the retail chain called to offer an apology, it was the
    father who ultimately apologized after confronting his daughter and discovering
    that she was indeed pregnant!
  prefs: []
  type: TYPE_NORMAL
- en: 'For more detail on how retailers use machine learning to identify pregnancies,
    see the *New York Times Magazine* article titled *How Companies Learn Your Secrets*,
    by Charles Duhigg, 2012: [https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html](https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the story was completely true or not, the lesson learned from the preceding
    tale is that common sense should be applied before blindly applying the results
    of a machine learning analysis. This is particularly true in cases where sensitive
    information, such as health data, is concerned. With a bit more care, the retailer
    could have foreseen this scenario and used greater discretion when choosing how
    to reveal the pregnancy status its machine learning analysis had discovered. Unfortunately,
    as history tends to repeat itself, social media companies have been under fire
    recently for targeting expectant mothers with advertisements for baby products
    even after these mothers experience the tragedy of a miscarriage.
  prefs: []
  type: TYPE_NORMAL
- en: Because machine learning algorithms are developed with historical data, computers
    may learn some unfortunate behaviors of human societies. Sadly, this sometimes
    includes perpetuating race or gender discrimination and reinforcing negative stereotypes.
    For example, researchers found that Google’s online advertising service was more
    likely to show ads for high-paying jobs to men than women and was more likely
    to display ads for criminal background checks to black people than white people.
    Although the machine may have correctly learned that men once held jobs that were
    not offered to most women, it is not desirable to have the algorithm perpetuate
    such injustices. Instead, it may be necessary to teach the machine to reflect
    society not as it currently is, but how it ought to be.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, algorithms that are specifically designed with the intention of being
    “content-neutral” eventually come to reflect undesirable beliefs or ideologies.
    In one egregious case, a Twitter chatbot service developed by Microsoft was quickly
    taken offline after it began spreading Nazi and anti-feminist propaganda, which
    it may have learned from so-called “trolls” posting inflammatory content on internet
    forums and chat rooms. In another case, an algorithm created to reflect an objective
    conception of human beauty sparked controversy when it favored almost exclusively
    white people. Imagine the consequences if this had been applied to facial recognition
    software for criminal activity!
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about the real-world consequences of machine learning
    and discrimination see the *Harvard Business Review* article *Addressing the Biases
    Plaguing Algorithms*, by Michael Li, 2019: [https://hbr.org/2019/05/addressing-the-biases-plaguing-algorithms](https://hbr.org/2019/05/addressing-the-biases-plaguing-algorithms).'
  prefs: []
  type: TYPE_NORMAL
- en: To limit the ability of algorithms to discriminate illegally, certain jurisdictions
    have well-intentioned laws that prevent the use of racial, ethnic, religious,
    or other protected class data for business reasons. However, excluding this data
    from a project may not be enough because machine learning algorithms can still
    inadvertently learn to discriminate. If a certain segment of people tends to live
    in a certain region, buys a certain product, or otherwise behaves in a way that
    uniquely identifies them as a group, machine learning algorithms can infer the
    protected information from other factors. In such cases, you may need to *completely*
    de-identify these people by excluding any *potentially* identifying data in addition
    to the already-protected statuses.
  prefs: []
  type: TYPE_NORMAL
- en: In a recent example of this type of alleged algorithmic bias, the Apple credit
    card, which debuted in 2019, was almost immediately accused of providing substantially
    higher credit limits to men than to women—sometimes by 10 to 20 times the amount—even
    for spouses with joint assets and similar credit histories. Although Apple and
    the issuing bank, Goldman Sachs, denied that gender bias was at play and confirmed
    that no legally protected applicant characteristics were used in the algorithm,
    this did not slow speculation that perhaps some bias crept in unintentionally.
    It did not help matters that for competitive reasons, Apple and Goldman Sachs
    chose to keep the details of the algorithm secret, which led people to assume
    the worst. If the systematic bias allegations were untrue, being able to explain
    what was truly happening and exactly how the decisions were made might have alleviated
    much of the outrage. A potential worst-case scenario would have occurred if Apple
    and Goldman Sachs were investigated yet couldn’t explain the result to regulators,
    due to the algorithm’s complexity!
  prefs: []
  type: TYPE_NORMAL
- en: 'The Apple credit card fiasco is described in a 2019 BBC article, *Apple’s ‘sexist’
    credit card investigated by US regulator*: [https://www.bbc.com/news/business-50365609](https://www.bbc.com/news/business-50365609).'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the legal consequences, customers may feel uncomfortable or become
    upset if aspects of their lives they consider private are made public. The challenge
    is that privacy expectations differ across people and contexts. To illustrate
    this fact, imagine driving by someone’s house and incidentally glancing through
    the window. This is unlikely to offend most people. In contrast, using a camera
    to take a picture from across the street is likely to make most feel uncomfortable;
    walking up to the house and pressing a face against the glass to peer inside is
    likely to anger virtually everybody. Although all three of these scenarios are
    arguably using “public” information, two of the three cross a line that will upset
    most people. In much the same way, it is possible to go too far with the use of
    data and cross a threshold that many will see as inconsiderate at best and creepy
    at worst.
  prefs: []
  type: TYPE_NORMAL
- en: Just as computing hardware and statistical methods kicked off the big data era,
    these methods also unlocked a **post-privacy era** in which many aspects of our
    lives that were once private are now public, or available to the public at a price.
    Even prior to the big data era, it would have been possible to learn a great deal
    about someone by observing public information. Watching their comings and goings
    may reveal information about their occupation or leisure activity, and a quick
    glance at their trash and recycling bins may reveal what they eat, drink, and
    read. A private investigator could learn even more with a bit of focused digging
    and observation. Companies applying machine learning methods to large datasets
    are essentially acting as large-scale private investigators, and while they claim
    to be working on anonymized datasets, many still argue that the companies have
    gone too far with their digital surveillance.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, some high-profile web applications have experienced a mass
    exodus of users who felt exploited when the applications’ terms of service agreements
    changed, or their data was used for purposes beyond what the users had originally
    intended. The fact that privacy expectations differ by context, age cohort, and
    locale adds complexity to deciding the appropriate use of personal data. It would
    be wise to consider the cultural implications of your work before you begin on
    your project, in addition to being aware of ever-more-restrictive regulations
    such as the European Union’s **General Data Protection Regulation** (**GDPR**)
    and the inevitable policies that will follow in its footsteps.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that you *can* use data for a particular end does not always mean that
    you *should*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is important to note that as machine learning algorithms become
    progressively more important to our everyday lives, there are greater incentives
    for nefarious actors to work to exploit them. Sometimes, attackers simply want
    to disrupt algorithms for laughs or notoriety—such as “Google bombing,” the crowdsourced
    method of tricking Google’s algorithms to highly rank a desired page. Other times,
    the effects are more dramatic. A timely example of this is the recent wave of
    so-called fake news and election meddling, propagated via the manipulation of
    advertising and recommendation algorithms that target people according to their
    personality. To avoid giving such control to outsiders, when building machine
    learning systems, it is crucial to consider how they may be influenced by a determined
    individual or crowd.
  prefs: []
  type: TYPE_NORMAL
- en: Social media scholar danah boyd (styled lowercase) presented a keynote at the
    Strata Data Conference 2017 in New York City that discussed the importance of
    hardening machine learning algorithms against attackers. For a recap, refer to
    [https://points.datasociety.net/your-data-is-being-manipulated-a7e31a83577b](https://points.datasociety.net/your-data-is-being-manipulated-a7e31a83577b).
  prefs: []
  type: TYPE_NORMAL
- en: The consequences of malicious attacks on machine learning algorithms can also
    be deadly. Researchers have shown that by creating an “adversarial attack” that
    subtly distorts a road sign with carefully chosen graffiti, an attacker might
    cause an autonomous vehicle to misinterpret a stop sign, potentially resulting
    in a fatal crash. Even in the absence of ill intent, software bugs and human errors
    have already led to fatal accidents in autonomous vehicle technology from Uber
    and Tesla. With such examples in mind, it is of the utmost importance and ethical
    concern that machine learning practitioners should worry about how their algorithms
    will be used and abused in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: How machines learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A formal definition of machine learning, attributed to computer scientist Tom
    M. Mitchell, states that a machine learns whenever it utilizes its experience
    such that its performance improves on similar experiences in the future. Although
    this definition makes sense intuitively, it completely ignores the process of
    exactly how experience is translated into future action—and, of course, learning
    is always easier said than done!
  prefs: []
  type: TYPE_NORMAL
- en: Where human brains are naturally capable of learning from birth, the conditions
    necessary for computers to learn must be made explicit by the programmer hoping
    to utilize machine learning methods. For this reason, although it is not strictly
    necessary to understand the theoretical basis for learning, having a strong theoretical
    foundation helps the practitioner to understand, distinguish, and implement machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As you relate machine learning to human learning, you may find yourself examining
    your own mind in a different light.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of whether the learner is a human or a machine, the basic learning
    process is the same. It can be divided into four interrelated components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data storage** utilizes observation, memory, and recall to provide a factual
    basis for further reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abstraction** involves the translation of stored data into broader representations
    and concepts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalization** uses abstracted data to create knowledge and inferences
    that drive action in new contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation** provides a feedback mechanism to measure the utility of learned
    knowledge and inform potential improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: The four steps in the learning process'
  prefs: []
  type: TYPE_NORMAL
- en: Although the learning process has been conceptualized here as four distinct
    components, they are merely organized this way for illustrative purposes. In reality,
    the entire learning process is inextricably linked. In human beings, the process
    occurs subconsciously. We recollect, deduce, induct, and intuit within the confines
    of our mind’s eye, and because this process is hidden, any differences from person
    to person are attributed to a vague notion of subjectivity. In contrast, computers
    make these processes explicit, and because the entire process is transparent,
    the learned knowledge can be examined, transferred, utilized for future action,
    and treated as a data “science.”
  prefs: []
  type: TYPE_NORMAL
- en: The **data science** buzzword suggests a relationship between the data, the
    machine, and the people who guide the learning process. The term’s growing use
    in job descriptions and academic degree programs reflects its operationalization
    as a field of study concerned with both statistical and computational theory,
    as well as the technological infrastructure enabling machine learning and its
    applications. The field often asks its practitioners to be compelling storytellers,
    balancing an audacity in the use of data with the limitations of what one may
    infer and forecast from it. To be a strong data scientist, therefore, requires
    a strong understanding of how the learning algorithms work in the context of a
    business application, as we will discuss in greater detail in *Chapter 11*, *Being
    Successful with Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All learning begins with data. Humans and computers alike utilize **data storage**
    as a foundation for more advanced reasoning. In a human being, this consists of
    a brain that uses electrochemical signals in a network of biological cells to
    store and process observations for short- and long-term future recall. Computers
    have similar capabilities of short- and long-term recall using hard disk drives,
    flash memory, and **random-access memory** (**RAM**) in combination with a **central
    processing unit** (**CPU**).
  prefs: []
  type: TYPE_NORMAL
- en: It may seem obvious, but the ability to store and retrieve data alone is insufficient
    for learning. Stored data is merely ones and zeros on a disk. It is a collection
    of memories, meaningless without a broader context. Without a higher level of
    understanding, knowledge is purely recall, limited to what has been seen before
    and nothing else.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the nuances of this idea, it may help to think about the
    last time you studied for a difficult test, perhaps for a university final exam
    or a career certification. Did you wish for an eidetic (photographic) memory?
    If so, you may be disappointed to know that perfect recall would unlikely be of
    much assistance. Even if you could memorize material perfectly, this rote learning
    would provide no benefit without knowing the exact questions and answers that
    would appear on the exam. Otherwise, you would need to memorize answers to every
    question that could *conceivably* be asked, on a subject in which there is likely
    to be an infinite number of questions. Obviously, this is an unsustainable strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a better approach is to spend time selectively and memorize a relatively
    small set of representative ideas, while developing an understanding of how the
    ideas relate and apply to unforeseen circumstances. In this way, important broader
    patterns are identified, rather than you memorizing every detail, nuance, and
    potential application.
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work of assigning a broader meaning to stored data occurs during the **abstraction**
    process, in which raw data comes to represent a wider, more abstract concept or
    idea. This type of connection, say between an object and its representation, is
    exemplified by the famous René Magritte painting *The Treachery of Images*.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text  Description automatically generated](img/B17290_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: “This is not a pipe.” Source: http://collections.lacma.org/node/239578'
  prefs: []
  type: TYPE_NORMAL
- en: The painting depicts a tobacco pipe with the caption *Ceci n’est pas une pipe*
    (“This is not a pipe”). The point Magritte was illustrating is that a representation
    of a pipe is not truly a pipe. Yet, despite the fact that the pipe is not real,
    anybody viewing the painting easily recognizes it as a pipe. This suggests that
    observers can connect the *picture* of a pipe to the *idea* of a pipe, to a memory
    of a *physical* pipe that can be held in the hand. Abstracted connections like
    this are the basis of **knowledge representation**, the formation of logical structures
    that assist with turning raw sensory information into meaningful insight.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing this concept full circle, knowledge representation is what allows artificial
    intelligence-based tools like Midjourney ([https://www.midjourney.com](https://www.midjourney.com))
    to paint, virtually, in the style of René Magritte. The following image was generated
    entirely by artificial intelligence based on the algorithm’s understanding of
    concepts like “robot,” “pipe,” and “smoking.” If he were alive yet today, Magritte
    himself might find it surreal that his own surrealist work, which challenged human
    conceptions of reality and the connections between images and ideas, is now incorporated
    into the minds of computers and, in a roundabout way, is connecting machines’
    ideas and images to reality. Machines learned what a pipe is, in part, by viewing
    images of pipes in artwork like Magritte’s.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: “Am I a pipe?” image created by the Midjourney AI with the prompt
    of “robot smoking a pipe in the style of a René Magritte painting”'
  prefs: []
  type: TYPE_NORMAL
- en: To reify the process of knowledge representation within an algorithm, the computer
    summarizes stored raw data using a **model**, an explicit description of the patterns
    within the data. Just like Magritte’s pipe, the model representation takes on
    a life beyond the raw data. It represents an idea greater than the sum of its
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different types of models. You may already be familiar with
    some. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational diagrams, such as trees and graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical if/else rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groupings of data known as clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of model is typically not left up to the machine. Instead, the learning
    task and the type of data on hand inform model selection. Later in this chapter,
    we will discuss in more detail the methods for choosing the appropriate model
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a model to a dataset is known as **training**. When the model has been
    trained, the data has been transformed into an abstract form that summarizes the
    original information. The fact that this step is called “training” rather than
    “learning” reveals a couple of interesting aspects of the process. First, note
    that the process of learning does not end with data abstraction—the learner must
    still generalize and evaluate its training. Second, the word “training” better
    connotes the fact that the human teacher trains the machine student to use the
    data toward a specific end.
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between training and learning is subtle but important. The computer
    doesn’t “learn” a model, because this would imply that there is a single correct
    model to be learned. Of course, the computer must learn *something* about the
    data in order to complete its training, but it has some freedom in how or what
    exactly it learns. When training the learner using a given dataset, each learner
    finds its own way to model the data and identify the patterns that will be useful
    for the given task.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that a learned model does not itself provide new data,
    yet it does result in new knowledge. How can this be? The answer is that imposing
    an assumed structure on the underlying data gives insight into the unseen. It
    supposes a new concept that describes a way in which existing data elements may
    be related.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for instance, the discovery of gravity. By fitting equations to observational
    data, Sir Isaac Newton inferred the concept of gravity, but the force we now know
    as gravity was always present. It simply wasn’t recognized until Newton expressed
    it as an abstract concept that relates some data to other data—specifically, by
    becoming the *g* term in a model that explains observations of falling objects.
    The observations of the distance an object falls in various periods of time can
    be related to each other, via a simple model that applies a constant force of
    gravity to the object per unit of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Models are abstractions that explain observed data'
  prefs: []
  type: TYPE_NORMAL
- en: Most models will not result in the development of theories that shake up scientific
    thought for centuries. Still, your abstraction might result in the discovery of
    important, but previously unseen, patterns and relationships among data. A model
    trained on genomic data might find several genes that when combined are responsible
    for the onset of diabetes, a bank might discover a seemingly innocuous type of
    transaction that systematically appears prior to fraudulent activity, or a psychologist
    might identify a combination of personality characteristics indicating a new disorder.
    These underlying patterns were always present, but by presenting the information
    in a different format, a new idea is conceptualized.
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third step in the learning process is to use the abstracted knowledge for
    future action. However, among the countless underlying patterns that may be identified
    during the abstraction process and the myriad ways to model those patterns, some
    patterns will be more useful than others. Unless the production of abstractions
    is limited to the useful set, the learner will be stuck where it started, with
    a large pool of information but no actionable insight.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, the term **generalization** is defined as the process of turning abstracted
    knowledge into a form that can be utilized for future action on tasks that are
    similar, but not identical, to those the learner has seen before. It acts as a
    search through the entire set of models (that is, the theories or inferences)
    that *could* be established from the data during training. If you can imagine
    a hypothetical set containing every possible way the data might be abstracted,
    generalization involves the reduction of this set into a smaller and more manageable
    set of important findings.
  prefs: []
  type: TYPE_NORMAL
- en: In generalization, the learner is tasked with limiting the patterns it discovers
    to only those that will be most relevant to its future tasks. Normally, it is
    not feasible to reduce the number of patterns by examining them one by one and
    ranking them by future value. Instead, machine learning algorithms generally employ
    shortcuts that reduce the search space more quickly. To this end, the algorithm
    will employ **heuristics**, which are educated guesses about where to find the
    most useful inferences. Heuristics are routinely used by human beings to quickly
    generalize experience to new scenarios. If you have ever utilized your gut instinct
    to make a snap decision prior to fully evaluating your circumstances, you were
    intuitively using mental heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristics utilize approximations and other rules of thumb, which means they
    are not guaranteed to find the best model of the data. However, without taking
    these shortcuts, finding useful information in a large dataset would be infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: The incredible human ability to make quick decisions often relies not on computer-like
    logic, but rather on emotion-guided heuristics. Sometimes, this can result in
    illogical conclusions. For example, more people express fear of airline travel
    than automobile travel, despite automobiles being statistically more dangerous.
    This can be explained by the availability heuristic, which is the tendency for
    people to estimate the likelihood of an event by how easily examples can be recalled.
    Accidents involving air travel are highly publicized. Being traumatic events,
    they are likely to be recalled very easily, whereas car accidents barely warrant
    a mention in the newspaper.
  prefs: []
  type: TYPE_NORMAL
- en: The folly of misapplied heuristics is not limited to human beings. The heuristics
    employed by machine learning algorithms also sometimes result in erroneous conclusions.
    The algorithm is said to have a **bias** if the conclusions are *systematically*
    erroneous, which implies that they are wrong in a consistent or predictable manner.
    For example, suppose that a machine learning algorithm learned to identify faces
    by finding two circles representing eyes, positioned above a straight line indicating
    a mouth. The algorithm might then have trouble with, or be *biased against*, faces
    that do not conform to its model. Faces with glasses, turned at an angle, looking
    sideways, or with certain skin tones might not be detected by the algorithm. Similarly,
    it could be *biased toward* faces with other skin tones, face shapes, or characteristics
    that conform to its understanding of the world.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: The process of generalizing a learner’s experience results in a
    bias'
  prefs: []
  type: TYPE_NORMAL
- en: In modern usage, the word “bias” has come to carry quite negative connotations.
    Various forms of media frequently claim to be free from bias and claim to report
    facts objectively, untainted by emotion. Still, consider for a moment the possibility
    that a little bias might be useful. Without a bit of arbitrariness, might it be
    a little difficult to decide among several competing choices, each with distinct
    strengths and weaknesses? Indeed, studies in the field of psychology have suggested
    that individuals born with damage to the portions of the brain responsible for
    emotion may be ineffectual at decision-making and might spend hours debating simple
    decisions, such as what color shirt to wear or where to eat lunch.
  prefs: []
  type: TYPE_NORMAL
- en: Paradoxically, bias is what blinds us from some information, while also allowing
    us to utilize other information for action. It is how machine learning algorithms
    choose among the countless ways to understand a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias is a necessary evil associated with the abstraction and generalization
    processes inherent in any learning task. In order to drive action in the face
    of limitless possibility, all learning must have a bias. Consequently, each learning
    strategy has weaknesses; there is no single learning algorithm to rule them all.
    Therefore, the final step in the learning process is to evaluate its success and
    to measure the learner’s performance, despite its biases. The information gained
    in the evaluation phase can then be used to inform additional training if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve had success with one machine learning technique, you might be tempted
    to apply it to every task. It is important to resist this temptation because no
    machine learning approach is best for every circumstance. This fact is described
    by the **No Free Lunch** theorem, introduced by David Wolpert in 1996\. Based
    on the popular adage that “there is no such thing as a free lunch,” the theorem
    suggests that there is a cost or trade-off in every decision that is made—a life
    lesson that is true more generally, even outside of machine learning! For more
    information, visit: [http://www.no-free-lunch.org](http://www.no-free-lunch.org).'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, evaluation occurs after a model has been trained on an initial **training
    dataset**. Then, the model is evaluated on a separate **test dataset** to judge
    how well its characterization of the training data generalizes to new, unseen
    cases. It’s worth noting that it is exceedingly rare for a model to perfectly
    generalize to every unforeseen case—mistakes are almost always inevitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In part, models fail to generalize perfectly due to the problem of **noise**,
    a term that describes unexplained or unexplainable variations in data. Noisy data
    is caused by seemingly random events, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Measurement error due to imprecise sensors that sometimes add or subtract a
    small amount from the readings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues with human subjects, such as survey respondents reporting random answers
    to questions in order to finish more quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality problems, including missing, null, truncated, incorrectly coded,
    or corrupted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phenomena that are so complex or so little understood that they impact the data
    in ways that appear to be random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling noise is the basis of a problem called **overfitting**; because most
    noisy data is unexplainable by definition, attempts to explain the noise will
    result in models that do not generalize well to new cases. Efforts to explain
    the noise also typically result in more complex models that miss the true pattern
    the learner is trying to identify.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17290_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Modeling noise generally results in more complex models and misses
    underlying patterns'
  prefs: []
  type: TYPE_NORMAL
- en: A model that performs relatively well during training but relatively poorly
    during evaluation is said to be **overfitted** to the training dataset because
    it does not generalize well to the test dataset. In practical terms, this means
    that it has identified a pattern in the data that is not useful for future action;
    the generalization process has failed. Solutions to the problem of overfitting
    are specific to each machine learning approach. For now, the important point is
    to be aware of the issue. How well the methods handle noisy data and avoid overfitting
    is an important point of distinction among them.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve focused on how machine learning works in theory. To apply the
    learning process to real-world tasks, we’ll use a five-step process. Regardless
    of the task, each machine learning algorithm uses the following series of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: The data collection step involves gathering the learning
    material an algorithm will use to generate actionable knowledge. In most cases,
    the data will need to be combined into a single source, such as a text file, spreadsheet,
    or database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data exploration and preparation**: The quality of any machine learning project
    is based largely on the quality of its input data. Thus, it is important to learn
    more about the data and its nuances. Data preparation involves fixing or cleaning
    so-called “messy” data, eliminating unnecessary data, and re-coding the data to
    conform to the learner’s expected inputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**: By the time the data has been prepared for analysis, you
    are likely to have a sense of what you are hoping and capable of learning from
    the data. The specific machine learning task chosen will inform the selection
    of an appropriate algorithm, and the algorithm will represent the data in the
    form of a model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model evaluation**: Each machine learning model results in a biased solution
    to the learning problem, which means that it is important to evaluate how well
    the algorithm learned from its experience. Depending on the type of model used,
    you might be able to evaluate the accuracy of the model using a test dataset,
    or you may need to develop measures of performance specific to the intended application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model improvement**: If better performance is needed, it becomes necessary
    to utilize more advanced strategies to augment the model’s performance. Sometimes
    it may be necessary to switch to a different type of model altogether. You may
    need to supplement your data with additional data or perform additional preparatory
    work, as in step 2 of this process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After these steps have been completed, if the model appears to be performing
    well, it can be deployed for its intended task. You might utilize your model to
    provide score data for predictions (possibly in real time); for projections of
    financial data; to generate useful insight for marketing or research; or to automate
    tasks, such as mail delivery or flying aircraft. The successes and failures of
    the deployed model might even provide additional data to train your next-generation
    learner.
  prefs: []
  type: TYPE_NORMAL
- en: Types of input data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The practice of machine learning involves matching the characteristics of the
    input data to the biases of the available learning algorithms. Thus, before applying
    machine learning to real-world problems, it is important to understand the terminology
    that distinguishes input datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The phrase **unit of observation** is used to describe the smallest entity with
    measured properties of interest for study. Commonly, the unit of observation is
    in the form of persons, objects or things, transactions, time points, geographic
    regions, or measurements. Sometimes, units of observation are combined to form
    units, such as person-years, which denote cases where the same person is tracked
    over multiple years, and each person-year comprises a person’s data for one year.
  prefs: []
  type: TYPE_NORMAL
- en: The unit of observation is related, but not identical, to the **unit of analysis**,
    which is the smallest unit from which inference is made. Although it is often
    the case, the observed and analyzed units are not always the same. For example,
    data observed from people (the unit of observation) might be used to analyze trends
    across countries (the unit of analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets that store the units of observation and their properties can be described
    as collections of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**: Instances of the unit of observation for which properties have
    been recorded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: Recorded properties or attributes of examples that may be useful
    for learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easiest to understand features and examples through real-world scenarios.
    For instance, to build a learning algorithm to identify spam emails, the unit
    of observation could be email messages, examples would be specific individual
    messages, and the features might consist of the words used in the messages. For
    a cancer detection algorithm, the unit of observation could be patients, the examples
    might include a random sample of cancer patients, and the features may be genomic
    markers from biopsied cells in addition to patient characteristics, such as weight,
    height, or blood pressure.
  prefs: []
  type: TYPE_NORMAL
- en: People and machines differ in the types of complexity they are suited to handle
    in the input data. Humans are comfortable consuming **unstructured data**, such
    as free-form text, pictures, or sound. They are also flexible in handling cases
    in which some observations have a wealth of features, while others have very little.
    On the other hand, computers generally require data to be **structured**, which
    means that each example of the phenomenon has exactly the same set of features,
    and these features are organized in a form that a computer may understand. Using
    the brute force of a machine on large, unstructured datasets usually requires
    a transformation of the input data to a structured form.
  prefs: []
  type: TYPE_NORMAL
- en: The following spreadsheet shows data that has been gathered in **matrix format**.
    In matrix data, each row is an example and each column is a feature. Here, the
    rows indicate examples of automobiles for sale, while the columns record the automobile’s
    features, such as the price, mileage, color, and transmission type. Matrix format
    data is by far the most common form used in machine learning. As you will see
    in later chapters, when unstructured forms of data are encountered in specialized
    applications, they are ultimately transformed into a structured matrix format
    prior to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: A simple dataset in matrix format describing automobiles for sale'
  prefs: []
  type: TYPE_NORMAL
- en: A dataset’s features may come in various forms. If a feature represents a characteristic
    measured in numbers, it is unsurprisingly called **numeric**. Alternatively, if
    a feature comprises a set of categories, the feature is called **categorical**
    or **nominal**. A special type of categorical feature is called **ordinal**, which
    designates a nominal feature with categories falling in an ordered list. One example
    of an ordinal feature is clothing sizes, such as small, medium, and large; another
    is a measurement of customer satisfaction on a scale from “not at all happy” to
    “somewhat happy” to “very happy.” For any given dataset, thinking about what the
    features represent, their types, and their units will assist with determining
    an appropriate machine learning algorithm for the learning task.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning algorithms are divided into categories according to their purpose.
    Understanding the categories of learning algorithms is an essential first step
    toward using data to drive the desired action.
  prefs: []
  type: TYPE_NORMAL
- en: A **predictive model** is used for tasks that involve, as the name implies,
    the prediction of one value using other values in a dataset. The learning algorithm
    attempts to discover and model the relationship between the **target** feature
    (the feature being predicted) and the other features. Despite the common use of
    the word “prediction” to imply forecasting, predictive models need not necessarily
    foresee events in the future. For instance, a predictive model could be used to
    predict past events, such as the date of a baby’s conception using the mother’s
    present-day hormone levels. Predictive models can also be used in real time to
    control traffic lights during rush hour.
  prefs: []
  type: TYPE_NORMAL
- en: Because predictive models are given clear instructions on what they need to
    learn and how they are intended to learn it, the process of training a predictive
    model is known as **supervised learning**. This supervision does not refer to
    human involvement, but rather to the fact that the target values provide a way
    for the learner to know how well it has learned the desired task. Stated more
    formally, given a set of data, a supervised learning algorithm attempts to optimize
    a function (the model) to find the best combination of feature values, resulting
    in a target output across all rows in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The often-used supervised machine learning task of predicting which category
    an example belongs to is known as **classification**. It is easy to think of potential
    uses for a classifier. For instance, you could predict whether:'
  prefs: []
  type: TYPE_NORMAL
- en: An email message is spam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A person has cancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A football team will win or lose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An applicant will default on a loan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In classification, the target feature to be predicted is a categorical feature
    known as the **class**, which is divided into categories called **levels**. A
    class can have two or more levels, and the levels may or may not be ordinal. Classification
    is so widely used in machine learning that there are many types of classification
    algorithms, with strengths and weaknesses suited for different types of input
    data. We will see examples of these later in this chapter and many times throughout
    this book. The first real-world application of classification appears in *Chapter
    3*, *Lazy Learning – Classification Using Nearest Neighbors*, and additional examples
    appear in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*,
    and *Chapter 5*, *Divide and Conquer – Classification Using Decision Trees and
    Rules*, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learners can also be used to predict numeric data, such as income,
    laboratory values, test scores, or counts of items. To predict such numeric values,
    a common form of **numeric prediction** fits linear regression models to the input
    data. Although regression is not the only method for numeric prediction, it is
    by far the most widely used. Regression methods are widely used for forecasting,
    as they quantify in exact terms the association between the inputs and the target,
    including both the magnitude and uncertainty of the relationship. Many supervised
    learning algorithms can perform numeric prediction, but regression methods and
    numeric prediction are covered in detail in *Chapter 6*, *Forecasting Numeric
    Data – Regression Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Since it is easy to convert numbers to categories (for example, ages 13 to 19
    are teenagers) and categories to numbers (for example, assign 1 to all males and
    0 to all females), the boundary between classification models and numeric prediction
    models is not necessarily firm.
  prefs: []
  type: TYPE_NORMAL
- en: A **descriptive model** is used for tasks that would benefit from the insight
    gained from summarizing data in new and interesting ways. As opposed to predictive
    models that predict a target of interest, in a descriptive model, no single feature
    is of particular interest. Because there is no target to learn, the process of
    training a descriptive model is called **unsupervised learning**. Although it
    can be more difficult to think of applications for descriptive models—after all,
    what good is a learner that isn’t learning anything in particular—they are used
    quite regularly for data mining.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the descriptive modeling task called **pattern discovery** is used
    to identify useful associations within data. Pattern discovery is the goal of
    **market basket analysis**, which is applied to retailers’ transactional purchase
    data. Here, retailers hope to identify items that are frequently purchased together,
    such that the learned information can be used to refine marketing tactics. For
    instance, if a retailer learns that swimsuits are commonly purchased at the same
    time as sunscreen, the retailer might reposition the items more closely in the
    store or run a promotion to “up-sell” associated items to customers. The methods
    needed to perform this type of analysis are included in *Chapter 8*, *Finding
    Patterns – Market Basket Analysis Using Association Rules*.
  prefs: []
  type: TYPE_NORMAL
- en: Originally used only in retail contexts, pattern discovery is now starting to
    be used in quite innovative ways. For instance, it can be used to detect patterns
    of fraudulent behavior, screen for genetic defects, or identify hotspots for criminal
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: The descriptive modeling task of dividing a dataset into homogeneous groups
    is called **clustering**. This is sometimes used for **segmentation analysis**,
    which identifies groups of individuals with similar behavior or demographic information,
    in order to target them with advertising campaigns based on their shared characteristics.
    With this approach, a machine identifies the clusters, but human intervention
    is required to interpret them. For example, given a grocery store’s five customer
    clusters, the marketing team will need to understand the differences among the
    groups in order to create a promotion that best suits each group. Despite this
    human effort, this is still less work than creating a unique appeal for each customer.
    This type of segmentation analysis is demonstrated on a real-world dataset in
    *Chapter 9*, *Finding Groups of Data – Clustering with k-means*.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can also be used to assist with supervised learning tasks
    where labeled data is unavailable or costly to obtain. A method called **semi-supervised
    learning** uses a small amount of labeled data in conjunction with an unsupervised
    learning analysis to help categorize the unlabeled records, which can then be
    used directly in a supervised learning model. For example, if it is expensive
    for a physician to label tumor samples as cancerous or non-cancerous, only a small
    portion of the patient records may have these labels. However, after performing
    unsupervised clustering on the patient data, it may be the case that the confirmed
    cancer and non-cancer patients fall into mostly separate groups, and thus the
    unlabeled records can inherit the labels of their cluster. Thus, a predictive
    model can be built on the complete set of data rather than the small portion that
    had been manually labeled. An application of semi-supervised learning is included
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*.
  prefs: []
  type: TYPE_NORMAL
- en: An even more extreme version of this approach known as **self-supervised learning**
    requires no manually labeled data whatsoever; instead, it uses a two-step approach
    in which a sophisticated model first attempts to identify meaningful groupings
    among records, and the second model attempts to identify the key distinctions
    between the groups. This is a relatively recent innovation and is used primarily
    on large, unstructured data sources like audio, text, and image data. The building
    blocks of self-supervised learning are covered in *Chapter 7*, *Black-Box Methods
    – Neural Networks and Support Vector Machines*, and *Chapter 15*, *Making Use
    of Big Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, a class of machine learning algorithms known as **meta-learners** is
    not tied to a specific learning task, but rather is focused on learning how to
    learn more effectively. Meta-learning can be beneficial for very challenging problems
    or when a predictive algorithm’s performance needs to be as accurate as possible.
    All meta-learning algorithms use the result of past learning to inform additional
    learning. Most commonly, this encompasses algorithms that learn to work together
    in teams called **ensembles**. Just as complementary strengths and accumulated
    experiences are important factors for successful human teams, they are likewise
    valuable for machine learners, and ensembles are among the most powerful off-the-shelf
    algorithms available today. Several of the most popular ensemble learning algorithms
    are covered in *Chapter 14*, *Building Better Learners*.
  prefs: []
  type: TYPE_NORMAL
- en: A form of meta-learning involving algorithms that seem to evolve over time is
    called **reinforcement learning**. This technique involves a simulation in which
    the learner is rewarded for success or punished for failure and, over many iterations,
    seeks the highest cumulative reward. The algorithm improves at the desired learning
    task via the evolutionary pressure of rewards and punishment, applied to the simulated
    “offspring” of the learner that accumulates more beneficial random adaptations
    and jettisons the least helpful mutations over successive generations.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to supervised learning algorithms that are trained based on a labeled
    set of data, reinforcement learning can achieve performance even better than the
    human trainer on a learning task, as it is not limited to training on the data
    on hand. Stated differently, traditional supervised learners tend to mimic the
    existing data, but reinforcement learning can identify novel and unforeseen solutions
    to the task—sometimes to surprising or even comical results. For instance, reinforcement
    learners trained to play video games have discovered cheats or shortcuts to complete
    games like *Sonic the Hedgehog* much faster than humans, but in ways designers
    did not intend. Similarly, a learning algorithm trained to pilot a simulated moon
    lander discovered that a crash landing is faster than a gentle landing—apparently
    unphased by the consequences of what this would do to the fortunately hypothetical
    human passengers!
  prefs: []
  type: TYPE_NORMAL
- en: The reinforcement learning technique, although extremely powerful, is quite
    computationally expensive and is vastly different from the traditional learning
    methods described previously. It is generally applied to real-world cases in which
    the learner can perform the task quickly and repeatedly to determine its success.
    This means that it is less useful for predicting cancer, or business outcomes
    like churn and loan default, and more useful for stereotypical artificial intelligence
    applications like self-driving vehicles and other forms of automation, where success
    or failure is easily measurable, immediate, and can be simulated in a controlled
    environment. For such reasons, reinforcement learning is outside the scope of
    this book, although it is a fascinating topic to monitor closely in the future.
  prefs: []
  type: TYPE_NORMAL
- en: For more humorous stories about the unexpected solutions reinforcement learners
    have discovered, see Tom Simonite’s article, *When Bots Teach Themselves to Cheat*,
    at [https://www.wired.com/story/when-bots-teach-themselves-to-cheat/](https://www.wired.com/story/when-bots-teach-themselves-to-cheat/).
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, much of the most exciting work being done in the field of machine learning
    today is in the domain of meta-learning. In addition to ensembles and reinforcement
    learning, the promising field of **adversarial learning** involves learning about
    a model’s weaknesses to strengthen its future performance or harden it against
    malicious attacks. This may involve pitting algorithms against one another, such
    as building a reinforcement learning algorithm to produce fake photographs that
    can fool facial detection algorithms or identify transaction patterns that can
    bypass fraud detection. This is just one avenue of building better learning algorithms;
    there is also heavy investment in research and development efforts to make bigger
    and faster ensembles, which can model massive datasets using high-performance
    computers or cloud-computing environments. Starting with the base algorithms,
    we will touch on many of these fascinating innovations in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Matching input data to algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table lists the general types of machine learning algorithms covered
    in this book. Although this covers only a fraction of the entire set of learning
    algorithms, learning these methods will provide a sufficient foundation for making
    sense of any other methods you may encounter in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Learning task** | **Chapter** |'
  prefs: []
  type: TYPE_TB
- en: '| **Supervised learning algorithms** |'
  prefs: []
  type: TYPE_TB
- en: '| k-nearest neighbors | Classification | *Chapter 3* |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Bayes | Classification | *Chapter 4* |'
  prefs: []
  type: TYPE_TB
- en: '| Decision trees | Classification | *Chapter 5* |'
  prefs: []
  type: TYPE_TB
- en: '| Classification rule learners | Classification | *Chapter 5* |'
  prefs: []
  type: TYPE_TB
- en: '| Linear regression | Numeric prediction | *Chapter 6* |'
  prefs: []
  type: TYPE_TB
- en: '| Regression trees | Numeric prediction | *Chapter 6* |'
  prefs: []
  type: TYPE_TB
- en: '| Model trees | Numeric prediction | *Chapter 6* |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | Classification | *Chapter 6* |'
  prefs: []
  type: TYPE_TB
- en: '| Neural networks | Dual use | *Chapter 7* |'
  prefs: []
  type: TYPE_TB
- en: '| Support vector machines | Dual use | *Chapter 7* |'
  prefs: []
  type: TYPE_TB
- en: '| **Unsupervised learning algorithms** |'
  prefs: []
  type: TYPE_TB
- en: '| Association rules | Pattern detection | *Chapter 8* |'
  prefs: []
  type: TYPE_TB
- en: '| k-means clustering | Clustering | *Chapter 9* |'
  prefs: []
  type: TYPE_TB
- en: '| **Meta-learning algorithms** |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging | Dual use | *Chapter 14* |'
  prefs: []
  type: TYPE_TB
- en: '| Boosting | Dual use | *Chapter 14* |'
  prefs: []
  type: TYPE_TB
- en: '| Random forests | Dual use | *Chapter 14* |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient boosting | Dual use | *Chapter 14* |'
  prefs: []
  type: TYPE_TB
- en: 'To begin applying machine learning to a real-world project, you will need to
    determine which of the four learning tasks your project represents: classification,
    numeric prediction, pattern detection, or clustering. The task will drive the
    choice of algorithm. For instance, if you are undertaking pattern detection, you
    are likely to employ association rules. Similarly, a clustering problem will likely
    utilize the k-means algorithm, and numeric prediction will utilize regression
    analysis or regression trees.'
  prefs: []
  type: TYPE_NORMAL
- en: For classification, more thought is needed to match a learning problem to an
    appropriate classifier. In these cases, it is helpful to consider the various
    distinctions among the algorithms—distinctions that will only be apparent by studying
    each of the classifiers in depth. For instance, within classification problems,
    decision trees result in models that are readily understood, while the models
    of neural networks are notoriously difficult to interpret. If you were designing
    a credit scoring model, this could be an important distinction because the law
    often requires that the applicant must be notified about the reasons they were
    rejected for a loan. Even if the neural network is better at predicting loan defaults,
    if its predictions cannot be explained, then it is useless for this application.
  prefs: []
  type: TYPE_NORMAL
- en: To assist with algorithm selection, in every chapter the key strengths and weaknesses
    of each learning algorithm are listed. Although you will sometimes find that these
    characteristics exclude certain models from consideration, in many cases the choice
    of algorithm is arbitrary. When this is true, feel free to use whichever algorithm
    you are most comfortable with. Other times, when predictive accuracy is the primary
    goal, you may need to test several models and choose the one that fits best, or
    use a meta-learning approach that combines several different learners to utilize
    the strengths of each.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the algorithms needed for machine learning are not included as part
    of the base R installation. Instead, the algorithms are available via a large
    community of experts who have shared their work freely. These must be installed
    on top of base R manually. Thanks to R’s status as free open-source software,
    there is no additional charge for this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: A collection of R functions that can be shared among users is called a **package**.
    Free packages exist for each of the machine learning algorithms covered in this
    book. In fact, this book only covers a small portion of all of R’s machine learning
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in the breadth of R packages, you can view a list at the
    **Comprehensive R Archive Network** (**CRAN**), a collection of web and FTP sites
    located around the world to provide the most up-to-date versions of R software
    and packages. If you obtained the R software via download, it was most likely
    from CRAN. The CRAN website is available at [http://cran.r-project.org/index.html](http://cran.r-project.org/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you do not already have R, the CRAN website also provides installation instructions
    and information on where to find help if you have trouble.
  prefs: []
  type: TYPE_NORMAL
- en: The **Packages** link on the left side of the CRAN page will take you to a page
    where you can browse the packages in alphabetical order or sorted by publication
    date. At the time of writing, a total of 18,910 packages were available—over 35
    percent more than when the third edition of this book was written, nearly three
    times the number since the second edition, and over four times since the first
    edition roughly 10 years ago! Clearly, the R community has been thriving, and
    this trend shows no sign of slowing!
  prefs: []
  type: TYPE_NORMAL
- en: The **Task Views** link on the left side of the CRAN page provides a curated
    list of packages by subject area. The task view for machine learning, which lists
    the packages covered in this book (and many more), is available at [https://cran.r-project.org/view=MachineLearning](https://cran.r-project.org/view=MachineLearning).
  prefs: []
  type: TYPE_NORMAL
- en: Installing R packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the vast set of available R add-ons, the package format makes installation
    and use a virtually effortless process. To demonstrate the use of packages, we
    will install and load the `gmodels` package maintained by Gregory R. Warnes, which
    contains a variety of functions to aid model fitting and data analysis. We’ll
    use one of the package’s functions throughout many of this book’s chapters to
    compare model predictions to the true values. For more information on this package,
    see [https://cran.r-project.org/package=gmodels](https://cran.r-project.org/package=gmodels).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most direct way to install a package is via the `install.packages()` function.
    To install the `gmodels` package, at the R command prompt simply type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: R will then connect to CRAN and download the package in the correct format for
    your operating system. Many packages require additional packages to be installed
    before they can be used. These are called **dependencies**. By default, the installer
    will automatically download and install any dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The first time you install a package, R may ask you to choose a CRAN mirror.
    If this happens, choose the mirror residing at a location close to you. This will
    generally provide the fastest download speed.
  prefs: []
  type: TYPE_NORMAL
- en: The default installation options are appropriate for most systems. However,
    in some cases, you may want to install a package in another location. For example,
    if you do not have root or administrator privileges on your system, you may need
    to specify an alternative installation path.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be accomplished using the `lib` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The installation function also provides additional options for installing from
    a local file, installing from source, or using experimental versions. You can
    read about these options in the help file by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: More generally, the question mark operator can be used to obtain help on any
    R function. Simply type `?` before the name of the function.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and unloading R packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to conserve memory, R does not load every installed package by default.
    Instead, packages are loaded by users with the `library()` function as they are
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: The name of this function leads some people to incorrectly use the terms “library”
    and “package” interchangeably. However, to be precise, a library refers to the
    location where packages are installed and never to a package itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the `gmodels` package installed previously, you can type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Aside from `gmodels`, there are many other R packages, which will be used in
    later chapters. A reminder to install these packages will be provided as these
    packages are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To unload an R package, use the `detach()` function. For example, to unload
    the `gmodels` package shown previously, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will free up any resources used by the package.
  prefs: []
  type: TYPE_NORMAL
- en: Installing RStudio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After you have installed R from the CRAN website, it is highly recommended to
    also install the open-source **RStudio** desktop application. RStudio is an additional
    interface to R that includes functionalities that make it far easier, more convenient,
    and more interactive to work with R code.
  prefs: []
  type: TYPE_NORMAL
- en: The RStudio Open Source Edition is available free of charge from Posit ([https://www.posit.co/](https://www.posit.co/))
    alongside a paid RStudio Pro Edition that offers priority support and additional
    features for commercial organizations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17290_01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: The RStudio desktop environment makes R easier and more convenient
    to use'
  prefs: []
  type: TYPE_NORMAL
- en: The RStudio interface comprises an **integrated development environment** (**IDE**)
    including a code editor, an R command-line console, a file browser, and an R object
    browser. R code syntax is automatically colorized, and the code’s output, plots,
    and graphics are displayed directly within the environment, which makes it much
    easier to follow long or complex statements and programs. More advanced features
    allow R project and package management; integration with source control or version
    control tools, such as Git and Subversion; database connection management; and
    the compilation of R output to HTML, PDF, or Microsoft Word formats. It is even
    possible to write Python code within RStudio!
  prefs: []
  type: TYPE_NORMAL
- en: RStudio is a key reason why R is a top choice for data scientists today. It
    wraps the power of R programming and its tremendous library of machine learning
    and statistical packages in an easy-to-use and easy-to-install development interface.
    It is not only ideal for learning R but can also grow with you as you learn R’s
    more advanced functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The RStudio Desktop software is developed by a company called Posit, which was
    formerly known itself as RStudio. The rebranding, which was somewhat surprising
    to fans of RStudio, occurred in late 2022 and is intended to reflect the company’s
    broadening focus on Python as well as R. At the time of writing, the name of the
    desktop IDE software remains RStudio. For more information, see [https://posit.co/blog/rstudio-is-now-posit/](https://posit.co/blog/rstudio-is-now-posit/).
  prefs: []
  type: TYPE_NORMAL
- en: Why R and why R now?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on whom you might ask, when the first edition of this book was published
    in 2013, R probably had a slight, if not substantial, lead over Python in user
    adoption for machine learning and what is now known as data science. In the time
    since, Python usage has grown substantially, and it would be hard to argue against
    the fact that Python is the new frontrunner, although the race may be closer than
    one might expect given the enthusiasm from Python fans supporting the new and
    shiny tool with greater hype.
  prefs: []
  type: TYPE_NORMAL
- en: Granted, in the past 10 years, Python has benefitted much from the rapid maturation
    of free add-ons like the scikit-learn machine learning framework, the pandas data
    structure library, the Matplotlib plotting library, and the Jupyter notebook interface,
    among numerous other open-source libraries that made it easier than ever to do
    data science in Python. Of course, these libraries merely brought Python to feature
    parity with what R and RStudio could do already! However, these add-ons, when
    combined with the comparatively fast and memory-efficient Python code—at least
    relative to R—may have contributed to the fact that Python is now undoubtedly
    the language most often taught in formal data science degree programs and has
    rapidly gained adoption in business domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than indicating the impending death of R, the rise of Python may simply
    reflect the growth of the field. In fact, R usage is likewise growing quickly,
    and R and RStudio may be more popular than ever. Although students sometimes ask
    whether it is even worth starting with R rather than jumping straight into Python,
    there are still many good reasons one might choose to learn machine learning with
    R rather than the alternative. Note that these justifications are quite subjective
    and there is no single right answer for everyone, so I hesitate to even put this
    in writing! However, having supported this book for nearly a decade, and as someone
    who still uses R on a near-daily basis as part of my work for a large, international
    corporation, here are a few things I’ve noticed:'
  prefs: []
  type: TYPE_NORMAL
- en: R may be more intuitive and easier to learn for people with social science or
    business backgrounds (such as economics, marketing, and so on) whereas Python
    may make more sense to computer scientists and other types of engineers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R tends to be used more like a “calculator” in that you type a command, and
    something happens; in general, coding in Python tends to require more thought
    about loops and other program flow commands (this distinction is fading over time
    with the additional functionality in popular Python libraries).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R uses relatively few types of data structures (those included are tailored
    for data analysis) and the often-used spreadsheet-like data format is a built-in
    data type; comparably, Python has many specialized data structures and uses libraries
    like NumPy or pandas for the matrix data format, each of which has their own syntax
    to learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R and its packages may be easier to install and update than Python, in part
    because Python is managed by some operating systems by default, and keeping dependencies
    and environments separate is challenging (modern Python installation tools and
    package managers have addressed this while simultaneously, in some ways, making
    the problem worse!).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is typically slower and more memory-hungry than Python for data manipulation
    and iterating over large data structures, but if the data fits in memory, this
    difference is somewhat negligible; R has improved in this area (see *Chapter 12*,
    *Advanced Data Preparation*, for some ways in which R is making data preparation
    faster and easier), and for data that doesn’t fit in memory, there are workarounds
    (as described in *Chapter 15*, *Making Use of Big Data*), but this is admittedly
    one of Python’s major advantages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R has the support and vision of the team at Posit (formerly known as RStudio)
    driving innovation and making R easier and more pleasurable to use within a unified
    RStudio Desktop software environment; in contrast, Python’s innovations are occurring
    on multiple fronts, offering more “right” ways of accomplishing the same thing
    (for better or worse).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With any luck, the above reasons give you the confidence to begin your R journey!
    There’s no shame in starting here, and regardless of whether you remain with R
    for the long term, use it side by side with other languages like Python, or graduate
    to something else altogether, the foundational principles you learn in this book
    will transfer to whatever tools you choose. Although the book’s code is written
    in R, it is highly encouraged that you use the right tool for the job, whatever
    you feel that might be. You may find, as I have myself, that R and RStudio are
    your preferred tools for many real-world data science and machine learning projects—even
    if you still occasionally take advantage of Python’s unique strengths!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning originated at the intersection of statistics, database science,
    and computer science. It is a powerful tool, capable of finding actionable insight
    in large quantities of data. Still, as we have seen in this chapter, caution must
    be used in order to avoid common abuses of machine learning in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, the learning process involves the abstraction of data into a structured
    representation, and the generalization of the structure into action that can be
    evaluated for utility. In practical terms, a machine learner uses data containing
    examples and features of the concept to be learned, then summarizes this data
    in the form of a model, which is used for predictive or descriptive purposes.
    These purposes can be grouped into tasks including classification, numeric prediction,
    pattern detection, and clustering. Among the many possible methods, machine learning
    algorithms are chosen based on the input data and the learning task.
  prefs: []
  type: TYPE_NORMAL
- en: R provides support for machine learning in the form of community-authored packages.
    These powerful tools are available to download at no cost but need to be installed
    before they can be used. Each chapter in this book will introduce such packages
    as they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will further introduce the basic R commands that are
    used to manage and prepare data for machine learning. Though you might be tempted
    to skip this step and jump directly into applications, a common rule of thumb
    suggests that 80 percent or more of the time spent on typical machine learning
    projects is devoted to the step of data preparation, also known as “data wrangling.”
    As a result, investing some effort into learning how to do this effectively will
    pay dividends for you later.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
