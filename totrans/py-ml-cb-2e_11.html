<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Biometric Face Recognition</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Capturing and processing video from a webcam</li>
<li>Building a face detector using Haar cascades</li>
<li>Building eye and nose detectors</li>
<li>Performing principal component analysis</li>
<li>Performing kernel principal <span>component analysis</span></li>
<li>Performing blind source separation</li>
<li>Building a face recognizer using a local binary pattern histogram</li>
<li>Recognizing faces using a HOG-based model</li>
<li>Facial landmarks recognition</li>
<li>User authentication by face recognition</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To address the recipes in this chapter, you need the following files (available on GitHub):</p>
<ul>
<li><kbd><span>video_capture</span><span>.py</span></kbd></li>
<li><kbd><span>face_detector</span><span>.py</span></kbd></li>
<li><kbd><span>eye_nose_detector</span><span>.py</span></kbd></li>
<li><kbd><span>pca</span><span>.py</span></kbd></li>
<li><kbd><span>kpca</span><span>.py</span></kbd></li>
<li><kbd><span>blind_source_separation</span><span>.py</span></kbd></li>
<li><kbd>mixture_of_signals.txt</kbd></li>
<li><kbd><span>face_recognizer</span><span>.py</span></kbd></li>
<li><kbd><span>FaceRecognition</span><span>.py</span></kbd></li>
</ul>
<ul>
<li><kbd><span>FaceLandmarks</span><span>.py</span></kbd></li>
<li><kbd><span>UserAuthentification</span><span>.py</span></kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><strong>Face recognition</strong> refers to the task of identifying a person in a given image. This is different from face detection where we locate the face in a given image. During face detection, we don't care who the person is; we just identify the region of the image that contains the face. Therefore, in a typical biometric face recognition system, we need to determine the location of the face before we can recognize it.</p>
<p>Face recognition is very easy for humans. We seem to do it effortlessly, and we do it all the time! How do we get a machine to do the same thing? We need to understand what parts of the face we can use to uniquely identify a person. Our brain has an internal structure that seems to respond to specific features, such as edges, corners, motion, and so on. The human visual cortex combines all these features into a single coherent inference. If we want our machine to recognize faces with accuracy, we need to formulate the problem in a similar way. We need to extract features from the input image and convert them into a meaningful representation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capturing and processing video from a webcam</h1>
                </header>
            
            <article>
                
<p>Webcams are certainly not an innovative technological innovation; their appearance dates back to the beginning of the 1990s and from then on, they gained increasing popularity thanks to the spread of video chat programs, street cams, and broadband internet connections. Currently, webcams are objects for everyone and are almost always integrated into the display frames of monitors, netbooks, and notebooks. The most common uses of a webcam are to transmit video streaming and record.</p>
<p>In the first case, webcams are used in video chat programs, television broadcasting, and street cams—cameras that film a fixed point of a given location. In the second case, webcams are used for creating photos and videos that you can then upload to the internet, for example, to YouTube or social networking sites. The advantage of these types of webcams is that they can replace the more classic uses of the camera, even if they are characterized by much poorer video quality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> use a webcam to capture video data. Let's see how we can capture video footage from a webcam using OpenCV-Python.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can capture and process video from a webcam by following these steps:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the <kbd>video_capture.py</kbd></span><span> file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2 </pre>
<ol start="2">
<li>OpenCV provides a video capture object that we can use to capture images from the webcam. The <kbd>0</kbd> input argument specifies the ID of the webcam. If you connect a USB camera, then it will have a different ID:</li>
</ol>
<pre style="padding-left: 60px"># Initialize video capture object 
cap = cv2.VideoCapture(0) </pre>
<ol start="3">
<li>Define the scaling factor for frames that are captured using the webcam:</li>
</ol>
<pre style="padding-left: 60px">scaling_factor = 0.5 </pre>
<ol start="4">
<li>Start an infinite loop and keep capturing frames until you press the <em><span class="KeyPACKT">Esc</span></em> key. Read the frame from the webcam:</li>
</ol>
<pre style="padding-left: 60px"># Loop until you hit the Esc key 
while True: 
    # Capture the current frame 
    ret, frame = cap.read() </pre>
<ol start="5">
<li>Resizing the frame is optional but still a useful thing to have in your code:</li>
</ol>
<pre>    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor,  
            interpolation=cv2.INTER_AREA) </pre>
<ol start="6">
<li>Display the frame:</li>
</ol>
<pre>    cv2.imshow('Webcam', frame)</pre>
<p class="mce-root"/>
<ol start="7">
<li>Wait for 1ms before capturing the next frame:</li>
</ol>
<pre>    c = cv2.waitKey(1) 
    if c == 27: 
        break </pre>
<ol start="8">
<li>Release the video capture object:</li>
</ol>
<pre style="padding-left: 60px">cap.release() </pre>
<ol start="9">
<li>Close all active windows before exiting the code:</li>
</ol>
<pre style="padding-left: 60px">cv2.destroyAllWindows() </pre>
<p style="padding-left: 60px">If you run this code, you will see the video from the webcam.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we</span> <span>used a webcam to capture video data via OpenCV-Python. To do this, the following operations have been performed:</span></p>
<ol>
<li><span>Initialize video capture object.<br/></span></li>
<li><span>Define the image size scaling factor.<br/></span></li>
<li><span>Loop until you hit the <em>Esc</em> key:<br/></span>
<ol>
<li><span>Capture the current frame.<br/></span></li>
<li><span>Resize the frame.<br/></span></li>
<li><span>Display the image.<br/></span></li>
<li><span>Detect whether the <em>Esc</em> key has been pressed.<br/></span></li>
</ol>
</li>
<li><span>Release the video capture object.<br/></span></li>
<li><span>Close all active windows.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>OpenCV provides a very simple interface to capture live streaming with webcams. To capture a video, you need to create a <kbd>VideoCapture</kbd> object. Its argument can be the device index or the name of a video file. Then we can acquire them frame by frame. However, we must not forget to release the capture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of<span> the OpenCV library: <a href="https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html">https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a face detector using Haar cascades</h1>
                </header>
            
            <article>
                
<p>As we discussed earlier, face detection is the process of determining the location of a face in an input image. In this recipe, we will use <strong>Haar cascades</strong> for face detection. This works by extracting many simple features from the image at multiple scales. These simple features are edge, line, and rectangle features that are very easy to compute. They are then trained by creating a cascade of simple classifiers. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> learn how to determine the location of a face in the video frames that are captured by our webcam. The <strong>adaptive boosting</strong> technique is used to make this process robust. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can build a face detector using Haar cascades:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the </span><span><kbd>face_detector.py</kbd></span> <span>file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2 
import numpy as np  </pre>
<ol start="2">
<li>Load the face detector cascade file. This is a trained model that we can use as a detector:</li>
</ol>
<pre style="padding-left: 60px">face_cascade = cv2.CascadeClassifier('cascade_files/haarcascade_frontalface_alt.xml')</pre>
<ol start="3">
<li>Check whether the cascade file loaded properly:</li>
</ol>
<pre style="padding-left: 60px">if face_cascade.empty(): 
    raise IOError('Unable to load the face cascade classifier xml file')</pre>
<ol start="4">
<li>Create the video capture object:</li>
</ol>
<pre style="padding-left: 60px">cap = cv2.VideoCapture(0) </pre>
<ol start="5">
<li>Define the scaling factor for image downsampling:</li>
</ol>
<pre style="padding-left: 60px">scaling_factor = 0.5 </pre>
<ol start="6">
<li>Keep looping until you hit the <em><span class="KeyPACKT">Esc</span></em> key:</li>
</ol>
<pre style="padding-left: 60px"># Loop until you hit the Esc key 
while True: 
    # Capture the current frame and resize it 
    ret, frame = cap.read() </pre>
<ol start="7">
<li>Resize the frame:</li>
</ol>
<pre>    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor,  
            interpolation=cv2.INTER_AREA) </pre>
<ol start="8">
<li>Convert the image to grayscale. We need grayscale images to run the face detector:</li>
</ol>
<pre>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) </pre>
<ol start="9">
<li>Run the face detector on the grayscale image. The <kbd>1.3</kbd> parameter refers to the scale multiplier for each stage. The <kbd>5</kbd> parameter refers to the minimum number of neighbors that each candidate rectangle should have so that we can retain it. This candidate rectangle is basically a potential region where there is a chance of a face being detected:</li>
</ol>
<pre>    face_rects = face_cascade.detectMultiScale(gray, 1.3, 5) </pre>
<ol start="10">
<li>Draw a rectangle around each detected face region:</li>
</ol>
<pre>    for (x,y,w,h) in face_rects: 
        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3) </pre>
<ol start="11">
<li>Display the output image:</li>
</ol>
<pre>    cv2.imshow('Face Detector', frame)</pre>
<ol start="12">
<li>Wait for 1ms before going to the next iteration. If the user presses the <span class="KeyPACKT">Esc</span> key, break out of the loop:</li>
</ol>
<pre>    c = cv2.waitKey(1) 
    if c == 27: 
        break</pre>
<ol start="13">
<li>Release and destroy the objects before exiting the code:</li>
</ol>
<pre style="padding-left: 60px">cap.release() 
cv2.destroyAllWindows() </pre>
<p style="padding-left: 60px">If you run this code, you will see the face being detected in the webcam video.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we learned how to determine the location of a face in video frames that were captured by the webcam. To do this, the following operations were performed:</span></p>
<ol>
<li>Load the face cascade file.</li>
<li>Check whether the face cascade file has been loaded.</li>
<li>Initialize the video capture object.</li>
<li>Define the scaling factor.</li>
<li>Loop until you hit the <em>Esc</em> key:<br/>
<ol>
<li>Capture the current frame and resize it.</li>
<li>Convert into grayscale.</li>
<li>Run the face detector on the grayscale image.</li>
<li>Draw rectangles on the image.</li>
<li>Display the image.</li>
<li>Check if the <em>Esc</em> key has been pressed.</li>
</ol>
</li>
<li>Release the video capture object and close all windows.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Haar cascades is an approach, based on machine learning, in which a cascade function is trained by many positive and negative images. It is then used to detect objects in other images.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Face Detection using Haar Cascades</em>:<a href="https://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0"> https://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0</a></span></li>
<li><em>Rapid Object Detection using a Boosted Cascade of Simple Features</em>: <a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf">https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building eye and nose detectors</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, <em>Building a face detector using Haar cascades</em>, we used the Haar cascades method to detect <span>the location of a face in video frames that were captured by a webcam. This method can be extended </span>to detect all types of object. This is what we will be covering here. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can use the Haar cascades method to detect the eyes and nose of a person in an input video.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can build eye and nose detectors:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the <kbd>eye_nose_detector.py</kbd></span><span> file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2 
import numpy as np </pre>
<ol start="2">
<li>Load the face, eyes, and nose cascade files:</li>
</ol>
<pre style="padding-left: 60px"># Load face, eye, and nose cascade files 
face_cascade = cv2.CascadeClassifier('cascade_files/haarcascade_frontalface_alt.xml') 
eye_cascade = cv2.CascadeClassifier('cascade_files/haarcascade_eye.xml') 
nose_cascade = cv2.CascadeClassifier('cascade_files/haarcascade_mcs_nose.xml')</pre>
<ol start="3">
<li>Check whether the files have loaded correctly:</li>
</ol>
<pre style="padding-left: 60px"># Check if face cascade file has been loaded 
if face_cascade.empty(): 
    raise IOError('Unable to load the face cascade classifier xml file') 
 
# Check if eye cascade file has been loaded 
if eye_cascade.empty(): 
    raise IOError('Unable to load the eye cascade classifier xml file') 
 
# Check if nose cascade file has been loaded 
if nose_cascade.empty(): 
    raise IOError('Unable to load the nose cascade classifier xml file') </pre>
<ol start="4">
<li>Initialize the video capture object:</li>
</ol>
<pre style="padding-left: 60px"># Initialize video capture object and define scaling factor 
cap = cv2.VideoCapture(0) </pre>
<ol start="5">
<li>Define the scaling factor:</li>
</ol>
<pre style="padding-left: 60px">scaling_factor = 0.5 </pre>
<ol start="6">
<li>Keep looping until the user presses the <em><span class="KeyPACKT">Esc</span></em> key:</li>
</ol>
<pre style="padding-left: 60px">while True: 
    # Read current frame, resize it, and convert it to grayscale 
    ret, frame = cap.read() </pre>
<ol start="7">
<li>Resize the frame:</li>
</ol>
<pre style="padding-left: 30px">    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor,  
            interpolation=cv2.INTER_AREA) </pre>
<ol start="8">
<li>Convert the image into grayscale:</li>
</ol>
<pre>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) </pre>
<ol start="9">
<li>Run the face detector on the grayscale image:</li>
</ol>
<pre>    # Run face detector on the grayscale image 
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)</pre>
<p class="mce-root"/>
<ol start="10">
<li>Since we know that faces always have eyes and noses, we can run these detectors only in the face region:</li>
</ol>
<pre>    # Run eye and nose detectors within each face rectangle 
    for (x,y,w,h) in faces: </pre>
<ol start="11">
<li>Extract the face ROI:</li>
</ol>
<pre>        # Grab the current ROI in both color and grayscale images 
        roi_gray = gray[y:y+h, x:x+w] 
        roi_color = frame[y:y+h, x:x+w] </pre>
<ol start="12">
<li>Run the eye detector:</li>
</ol>
<pre>        # Run eye detector in the grayscale ROI 
        eye_rects = eye_cascade.detectMultiScale(roi_gray) </pre>
<ol start="13">
<li>Run the nose detector:</li>
</ol>
<pre>        # Run nose detector in the grayscale ROI 
        nose_rects = nose_cascade.detectMultiScale(roi_gray, 1.3, 5) </pre>
<ol start="14">
<li>Draw circles around the eyes:</li>
</ol>
<pre>        # Draw green circles around the eyes 
        for (x_eye, y_eye, w_eye, h_eye) in eye_rects: 
            center = (int(x_eye + 0.5*w_eye), int(y_eye + 0.5*h_eye)) 
            radius = int(0.3 * (w_eye + h_eye)) 
            color = (0, 255, 0) 
            thickness = 3 
            cv2.circle(roi_color, center, radius, color, thickness) </pre>
<ol start="15">
<li>Draw a rectangle around the nose:</li>
</ol>
<pre>        for (x_nose, y_nose, w_nose, h_nose) in nose_rects: 
            cv2.rectangle(roi_color, (x_nose, y_nose), (x_nose+w_nose,  
                y_nose+h_nose), (0,255,0), 3) 
            break </pre>
<ol start="16">
<li>Display the image:</li>
</ol>
<pre>    # Display the image 
    cv2.imshow('Eye and nose detector', frame)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="17">
<li>Wait for 1ms before going to the next iteration. If the user presses the <em><span class="KeyPACKT">Esc</span></em> key, then break the loop:</li>
</ol>
<pre>    # Check if Esc key has been pressed 
    c = cv2.waitKey(1) 
    if c == 27: 
        break </pre>
<ol start="18">
<li>Release and destroy the objects before exiting the code:</li>
</ol>
<pre># Release video capture object and close all windows 
cap.release() 
cv2.destroyAllWindows() </pre>
<p style="padding-left: 60px">If you run this code, you will see the eyes and nose of the person being detected in the webcam video.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we learned how to detect the eyes and nose of a person in the input video. To do this, the following operations have been performed:</span></p>
<ol>
<li>Load the face, eye, and nose cascade files.</li>
<li>Check whether the face, eye, and nose cascade files have been loaded.</li>
<li>Initialize a video capture object and define the scaling factor.</li>
<li>Loop on the frame:<br/>
<ol>
<li>Read the current frame, resize it, and convert it into grayscale.</li>
<li>Run the face detector on the grayscale image.</li>
<li>Run the eye and nose detectors within each face rectangle.</li>
<li>Display the image.</li>
<li>Check whether the <em>Esc</em> key has been pressed.</li>
</ol>
</li>
<li>Release the video capture object and close all windows.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Identifying facial elements in a webcam can be useful for recognizing subjects. Both global visual information and local characteristics (eye and nose morphology) are fundamental in the perception and recognition of the face. In fact, studies on facial recognition document that men more easily identify faces with predominant elements such as aquiline nose, squinting eyes, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Classifier case study <span>–</span> <em>Viola-Jones Face Detector</em>: <a href="http://www.cse.psu.edu/~rtc12/CSE586/lectures/violaJonesDetector.pdf">http://www.cse.psu.edu/~rtc12/CSE586/lectures/violaJonesDetector.pdf </a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing principal component analysis</h1>
                </header>
            
            <article>
                
<p><strong>Principal component analysis</strong> (<strong>PCA</strong>) is a dimensionality reduction technique that's used frequently in computer vision and machine learning. When we deal with features with large dimensionalities, training a machine learning system becomes prohibitively expensive. Therefore, we need to reduce the dimensionality of the data before we can train a system. However, when we reduce the dimensionality, we don't want to lose the information that's present in the data. This is where PCA comes into the picture! PCA identifies the important components of the data and arranges them in order of importance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can perform PCA on input data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform a PCA on some input data:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the </span><span><kbd>pca.py</kbd></span> <span>file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from sklearn import decomposition  </pre>
<ol start="2">
<li>Let's define five dimensions for our input data. The first two dimensions will be independent, but the next three dimensions will be dependent on the first two dimensions. This basically means that we can live without the last three dimensions because they do not give us any new information:</li>
</ol>
<pre style="padding-left: 60px"># Define individual features 
x1 = np.random.normal(size=250) 
x2 = np.random.normal(size=250) 
x3 = 3*x1 + 2*x2<br/>x4 = 6*x1 - 2*x2<br/>x5 = 3*x3 + x4</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Let's create a dataset with these features:</li>
</ol>
<pre style="padding-left: 60px"># Create dataset with the above features 
X = np.c_[x1, x3, x2, x5, x4] </pre>
<ol start="4">
<li>Create a PCA object:</li>
</ol>
<pre style="padding-left: 60px"># Perform Principal Component Analysis 
pca = decomposition.PCA() </pre>
<ol start="5">
<li>Fit a PCA model on the input data:</li>
</ol>
<pre style="padding-left: 60px">pca.fit(X) </pre>
<ol start="6">
<li>Print the variances of the dimensions:</li>
</ol>
<pre style="padding-left: 60px"># Print variances 
variances = pca.explained_variance_ 
print('Variances in decreasing order:\n', variances)</pre>
<ol start="7">
<li>If a particular dimension is useful, then it will have a meaningful value for the variance. Let's set a threshold and identify the important dimensions:</li>
</ol>
<pre style="padding-left: 60px"># Find the number of useful dimensions 
thresh_variance = 0.8 
num_useful_dims = len(np.where(variances &gt; thresh_variance)[0]) 
print('Number of useful dimensions:', num_useful_dims)</pre>
<ol start="8">
<li>Just like we discussed earlier, PCA has identified that only two dimensions are important in this dataset:</li>
</ol>
<pre style="padding-left: 60px"># As we can see, only the 2 first components are useful 
pca.n_components = num_useful_dims </pre>
<ol start="9">
<li>Let's convert the dataset from a five-dimensional set into a two-dimensional set:</li>
</ol>
<pre style="padding-left: 60px">XNew = pca.fit_transform(X)<br/>print('Shape before:', X.shape)<br/>print('Shape after:', XNew.shape)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="10">
<li>If you run this code, you will see the following on your Terminal:</li>
</ol>
<pre style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><strong>Variances in decreasing order:</strong><br/><strong>[2.77392134e+02 1.51557851e+01 9.54279881e-30 7.73588070e-32 </strong><strong>9.89435444e-33]</strong><br/><strong><br/>Number of useful dimensions: 2</strong><br/><strong>Shape before: (250, 5)</strong><br/><strong>Shape after: (250, 2)<br/></strong></pre>
<p style="padding-left: 60px">As we can see, the first two components contain all of the variance of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>PCA generates a new set of variables, among which there are uncorrelated variables, also known as principal components. Each main component is a linear combination of the original variables. All principal components are orthogonal to each other, so there is no redundant information.<br/>
The principal components as a whole constitute an orthogonal basis for the data space. The goal of PCA is to explain the maximum amount of variance with the lowest number of principal components. PCA is a type of multidimensional scaling wherein the variables are linearly transformed into a lower dimensional space, thereby retaining the maximum amount of information possible about the variables. A principal component is therefore a combination of the original variables after a linear transformation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The variance measures how far a set of numbers are spread out from their mean. It represents the mean of the squares of deviations of individual values from their arithmetic mean.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>sklearn.decomposition.PCA</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a></li>
<li><em>Principal components analysis</em> (by Andrew Ng from Stanford University): <a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">http://cs229.stanford.edu/notes/cs229-notes10.pdf</a></li>
</ul>
<ul>
<li><em>Principle Component Analysis</em> (from Indiana University): <a href="http://scholarwiki.indiana.edu/Z604/slides/week4-PCA.pdf">http://scholarwiki.indiana.edu/Z604/slides/week4-PCA.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing kernel principal component analysis</h1>
                </header>
            
            <article>
                
<p>PCA<span> </span>is good at reducing the number of dimensions, but it works in a linear manner. If the data is not organized in a linear fashion, PCA fails to do the required job. This is where kernel PCA enters the picture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can perform a k</span>ernel PCA on the input data and compare the result to how PCA performs on the same data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform a kernel PCA:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the </span><span><kbd>kpca.py</kbd></span> <span>file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
 
from sklearn.decomposition import PCA, KernelPCA 
from sklearn.datasets import make_circles </pre>
<ol start="2">
<li>Define the <kbd>seed</kbd> value for the random number generator. This is needed to generate data samples for analysis:</li>
</ol>
<pre style="padding-left: 60px"># Set the seed for random number generator 
np.random.seed(7) </pre>
<ol start="3">
<li>Generate data that is distributed in concentric circles to demonstrate how PCA doesn't work in this case:</li>
</ol>
<pre style="padding-left: 60px"># Generate samples 
X, y = make_circles(n_samples=500, factor=0.2, noise=0.04)</pre>
<p class="mce-root"/>
<ol start="4">
<li>Perform PCA on this data:</li>
</ol>
<pre style="padding-left: 60px"># Perform PCA 
pca = PCA() 
X_pca = pca.fit_transform(X) </pre>
<ol start="5">
<li>Perform kernel PCA on this data:</li>
</ol>
<pre style="padding-left: 60px"># Perform Kernel PCA 
kernel_pca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10) 
X_kernel_pca = kernel_pca.fit_transform(X) 
X_inverse = kernel_pca.inverse_transform(X_kernel_pca) </pre>
<ol start="6">
<li>Plot the original input data:</li>
</ol>
<pre style="padding-left: 60px"># Plot original data 
class_0 = np.where(y == 0) 
class_1 = np.where(y == 1) 
plt.figure() 
plt.title("Original data") 
plt.plot(X[class_0, 0], X[class_0, 1], "ko", mfc='none') 
plt.plot(X[class_1, 0], X[class_1, 1], "kx") 
plt.xlabel("1st dimension") 
plt.ylabel("2nd dimension") </pre>
<ol start="7">
<li>Plot the PCA-transformed data:</li>
</ol>
<pre style="padding-left: 60px"># Plot PCA projection of the data 
plt.figure() 
plt.plot(X_pca[class_0, 0], X_pca[class_0, 1], "ko", mfc='none') 
plt.plot(X_pca[class_1, 0], X_pca[class_1, 1], "kx") 
plt.title("Data transformed using PCA") 
plt.xlabel("1st principal component") 
plt.ylabel("2nd principal component") </pre>
<ol start="8">
<li>Plot the kernel PCA-transformed data:</li>
</ol>
<pre style="padding-left: 60px"># Plot Kernel PCA projection of the data 
plt.figure() 
plt.plot(X_kernel_pca[class_0, 0], X_kernel_pca[class_0, 1], "ko", mfc='none') 
plt.plot(X_kernel_pca[class_1, 0], X_kernel_pca[class_1, 1], "kx") 
plt.title("Data transformed using Kernel PCA") 
plt.xlabel("1st principal component") 
plt.ylabel("2nd principal component")</pre>
<ol start="9">
<li>Transform the data back to the original space using the Kernel method to show that the inverse is maintained:</li>
</ol>
<pre style="padding-left: 60px"># Transform the data back to original space 
plt.figure() 
plt.plot(X_inverse[class_0, 0], X_inverse[class_0, 1], "ko", mfc='none') 
plt.plot(X_inverse[class_1, 0], X_inverse[class_1, 1], "kx") 
plt.title("Inverse transform") 
plt.xlabel("1st dimension") 
plt.ylabel("2nd dimension") 
 
plt.show() </pre>
<ol start="10">
<li>The full code is given in the <kbd>kpca.py</kbd> file that's already provided to you for reference. If you run this code, you will see four diagrams. The first diagram is the original data:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1094 image-border" src="assets/9a76365f-eae9-4d62-8bd7-850626d661e4.png" style="width:33.33em;height:25.83em;"/></p>
<p style="padding-left: 60px">The second diagram depicts the data that was transformed using PCA:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1095 image-border" src="assets/67ef1a63-4853-4dcd-ac04-6f0abd5a4c19.png" style="width:30.25em;height:23.00em;"/></p>
<p style="padding-left: 60px">The third diagram depicts the data that was transformed using the kernel PCA. Note how the points are clustered on the right-hand side of the diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1096 image-border" src="assets/a2fbe42c-caea-4940-8512-59f4cd768dcd.png" style="width:31.17em;height:23.83em;"/></p>
<p style="padding-left: 60px">The fourth diagram depicts the inverse transform of the data back to the original space:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1097 image-border" src="assets/03aa5817-7804-42a6-b4c0-2183452e54eb.png" style="width:34.08em;height:25.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><strong>Kernel principal component analysis</strong> (<strong>kernel PCA</strong>) is based on PCA while using the techniques of kernel methods. In PCA, the originally linear PCA operations are performed in a reproducing kernel Hilbert space.</p>
<p>Kernel methods are a class of algorithms used for the analysis of patterns and schemes, and whose most well-known element are SVMs. Kernel methods solve a problem by mapping data into a multidimensional feature space, and in this space each coordinate corresponds to a feature of the element's data, transforming the data into a set of Euclidean space points.  Since the mapping can be general (for example, not necessarily linear), the relations that are found in this way are consequently very general.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Kernel methods are named after kernel functions, which are used to operate on the characteristic space without calculating the data coordinates in space, but rather by calculating the internal product among the images of all copies of data in the function space. Kernel methods are often computationally cheaper than the explicit calculation of coordinates. The kernel trick refers to this approach as <strong>problem resolution</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>sklearn.decomposition.KernelPCA</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html</a></li>
<li><em>Kernel Principal Components Analysis</em> (by Max Welling from the University of Toronto): <a href="https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-PCA.pdf">https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-PCA.pdf</a></li>
<li><em>KERNEL PCA</em> (by Rita Osadchy from Haifa University): <a href="http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf">http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing blind source separation</h1>
                </header>
            
            <article>
                
<p><strong>Blind source separation</strong> refers to the process of separating signals from a mixture. Let's say a bunch of different signal generators generate signals and a common receiver receives all of these signals. Now, our job is to separate these signals from this mixture using the properties of these signals. We will use <strong>independent component analysis</strong> (<strong>ICA</strong>) to achieve this. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> use the data from a <kbd>.txt</kbd> file to separate the signals contained in it using <strong>ICA</strong>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform a blind source separation:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the </span><span><kbd>blind_source_separation.py</kbd></span> <span>file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA, FastICA  </pre>
<ol start="2">
<li>We will use data from the <kbd>mixture_of_signals.txt</kbd> file that's already provided for you. Let's load the data:</li>
</ol>
<pre style="padding-left: 60px"># Load data 
input_file = 'mixture_of_signals.txt' 
X = np.loadtxt(input_file) </pre>
<ol start="3">
<li>Create the ICA object:</li>
</ol>
<pre style="padding-left: 60px"># Compute ICA 
ica = FastICA(n_components=4) </pre>
<ol start="4">
<li>Reconstruct the signals based on ICA:</li>
</ol>
<pre style="padding-left: 60px"># Reconstruct the signals 
signals_ica = ica.fit_transform(X) </pre>
<ol start="5">
<li>Extract the mixing matrix:</li>
</ol>
<pre style="padding-left: 60px"># Get estimated mixing matrix 
mixing_mat = ica.mixing_   </pre>
<ol start="6">
<li>Perform PCA for comparison:</li>
</ol>
<pre style="padding-left: 60px"># Perform PCA  
pca = PCA(n_components=4) <br/># Reconstruct signals based on orthogonal components <br/>signals_pca = pca.fit_transform(X)</pre>
<ol start="7">
<li>Define a list of signals to plot them:</li>
</ol>
<pre style="padding-left: 60px"># Specify parameters for output plots  
models = [X, signals_ica, signals_pca] </pre>
<ol start="8">
<li>Specify the colors of the plots:</li>
</ol>
<pre style="padding-left: 60px">colors = ['blue', 'red', 'black', 'green'] </pre>
<ol start="9">
<li>Plot the input signal:</li>
</ol>
<pre style="padding-left: 60px"># Plotting input signal 
plt.figure() 
plt.title('Input signal (mixture)') 
for i, (sig, color) in enumerate(zip(X.T, colors), 1): 
    plt.plot(sig, color=color) </pre>
<ol start="10">
<li>Plot the ICA-separated signals:</li>
</ol>
<pre style="padding-left: 60px"># Plotting ICA signals  
plt.figure() 
plt.title('ICA separated signals') 
plt.subplots_adjust(left=0.1, bottom=0.05, right=0.94,  
        top=0.94, wspace=0.25, hspace=0.45) </pre>
<ol start="11">
<li>Plot subplots with different colors:</li>
</ol>
<pre style="padding-left: 60px">for i, (sig, color) in enumerate(zip(signals_ica.T, colors), 1): 
    plt.subplot(4, 1, i) 
    plt.title('Signal ' + str(i)) 
    plt.plot(sig, color=color) </pre>
<ol start="12">
<li>Plot the PCA-separated signals:</li>
</ol>
<pre style="padding-left: 60px"># Plotting PCA signals   
plt.figure() 
plt.title('PCA separated signals') 
plt.subplots_adjust(left=0.1, bottom=0.05, right=0.94,  
        top=0.94, wspace=0.25, hspace=0.45) </pre>
<ol start="13">
<li>Use a different color in each subplot:</li>
</ol>
<pre style="padding-left: 60px">for i, (sig, color) in enumerate(zip(signals_pca.T, colors), 1): 
    plt.subplot(4, 1, i) 
    plt.title('Signal ' + str(i)) 
    plt.plot(sig, color=color) 
 
plt.show() </pre>
<p style="padding-left: 60px">If you run this code, you will see three diagrams. The first diagram depicts the input, which is a mixture of signals:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1098 image-border" src="assets/d6630815-f97e-4984-9dee-d55cfd91dd36.png" style="width:31.25em;height:24.33em;"/></p>
<p style="padding-left: 60px">The second diagram depicts the signals, separated using ICA:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1099 image-border" src="assets/021cd718-0d49-4cdb-9ef6-7d77696e5967.png" style="width:32.25em;height:25.50em;"/></p>
<p style="padding-left: 60px">The third diagram depicts the signals, separated using PCA:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1100 image-border" src="assets/02599066-72d7-4313-9bcd-6c8be9fdc2bd.png" style="width:31.58em;height:25.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>ICA is a computational processing method that's used to separate a multivariant signal into its additive subcomponents, assuming that there is a mutual statistical independence of the source of non-Gaussian signals. This is a special case of blind source separation. This method finds the independent components, maximizing the statistical independence of the estimated components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>An example of the application of ICA algorithms is in the field of <strong>electroencephalography</strong> (<strong>EEG</strong>), but it has also been widely exploited in the separation of the <strong>electrocardiogram</strong> (<strong>ECG</strong>) of the fetus from that of the <kbd>mother.ICA</kbd> techniques. This can be extended to the analysis of non-physical data that's either semantic or linguistic. For example, the ICA has been applied to make a computer understand the topic of discussion in a set of archives of news lists.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>sklearn.decomposition.FastICA</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html</a></li>
<li><em>BLIND SOURCE SEPARATION: Principal and Independent Component Analysis</em> (from Massachusetts Institute of Technology): <a href="http://www.mit.edu/~gari/teaching/6.555/LECTURE_NOTES/ch15_bss.pdf">http://www.mit.edu/~gari/teaching/6.555/LECTURE_NOTES/ch15_bss.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a face recognizer using a local binary patterns histogram</h1>
                </header>
            
            <article>
                
<p>We are now ready to build a face recognizer. We need a face dataset for training, so we've provided you with a folder called <kbd>faces_dataset</kbd> that contains a small number of images that are sufficient for training. This dataset is a subset of the dataset that is available at <a href="http://www.vision.caltech.edu/Image_Datasets/faces/faces.tar"><span class="URLPACKT">http://www.vision.caltech.edu/Image_Datasets/faces/faces.tar</span></a>. This dataset contains a good number of images that we can use to train a face recognition system.</p>
<p>We will use a <strong>local binary patterns histogram</strong> to build our face recognition system. In our dataset, you will see different people. Our job is to build a system that can learn to separate these people from one another. When we see an unknown image, our system will assign it to one of the existing classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can build a face recognizer using a <strong>local binary patterns histogram</strong> and use a face dataset to train the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can build a face recognizer using a local binary patterns histogram:</p>
<ol>
<li>Create a new Python file and import the following packages <span>(the full code is given in the <kbd>face_recognizer.py</kbd></span><span> file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import os 
 
import cv2 
import numpy as np 
from sklearn import preprocessing  </pre>
<ol start="2">
<li>Let's define a class to handle all of the tasks that are related to label encoding for the classes:</li>
</ol>
<pre style="padding-left: 60px"># Class to handle tasks related to label encoding 
class LabelEncoder(object): </pre>
<ol start="3">
<li>Define a method to encode the labels. In the input training data, labels are represented by words. However, we need numbers to train our system. This method will define a preprocessor object that can convert words into numbers in an organized fashion by maintaining forward and backward mapping:</li>
</ol>
<pre>    # Method to encode labels from words to numbers 
    def encode_labels(self, label_words): 
        self.le = preprocessing.LabelEncoder() 
        self.le.fit(label_words) </pre>
<ol start="4">
<li>Define a method to convert a word into a number:</li>
</ol>
<pre>    # Convert input label from word to number 
    def word_to_num(self, label_word): 
        return int(self.le.transform([label_word])[0]) </pre>
<ol start="5">
<li>Define a method to convert the number back into the original word:</li>
</ol>
<pre>    # Convert input label from number to word 
    def num_to_word(self, label_num): 
        return self.le.inverse_transform([label_num])[0] </pre>
<ol start="6">
<li>Define a method to extract the images and labels from the input folder:</li>
</ol>
<pre style="padding-left: 60px"># Extract images and labels from input path 
def get_images_and_labels(input_path): 
    label_words = [] </pre>
<ol start="7">
<li>Recursively iterate through the input folder and extract all of the image paths:</li>
</ol>
<pre>    # Iterate through the input path and append files 
    for root, dirs, files in os.walk(input_path): 
        for filename in (x for x in files if x.endswith('.jpg')): 
            filepath = os.path.join(root, filename) 
            label_words.append(filepath.split('/')[-2])</pre>
<ol start="8">
<li>Initialize the variables:</li>
</ol>
<pre>    # Initialize variables 
    images = [] 
    le = LabelEncoder() 
    le.encode_labels(label_words) 
    labels = [] </pre>
<ol start="9">
<li>Parse the input directory for training:</li>
</ol>
<pre>    # Parse the input directory 
    for root, dirs, files in os.walk(input_path): 
        for filename in (x for x in files if x.endswith('.jpg')): 
            filepath = os.path.join(root, filename) </pre>
<ol start="10">
<li>Read the current image in grayscale format:</li>
</ol>
<pre>            # Read the image in grayscale format 
            image = cv2.imread(filepath, 0)  </pre>
<ol start="11">
<li>Extract the label from the folder path:</li>
</ol>
<pre>            # Extract the label 
            name = filepath.split('/')[-2] </pre>
<ol start="12">
<li>Perform face detection on this image:</li>
</ol>
<pre>            # Perform face detection 
            faces = faceCascade.detectMultiScale(image, 1.1, 2, minSize=(100,100)) </pre>
<ol start="13">
<li>Extract the ROIs and return them, along with the label encoder:</li>
</ol>
<pre>            # Iterate through face rectangles 
            for (x, y, w, h) in faces: 
                images.append(image[y:y+h, x:x+w]) 
                labels.append(le.word_to_num(name)) 
 
    return images, labels, le </pre>
<ol start="14">
<li>Define the main function and the path to the face cascade file:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    cascade_path = "cascade_files/haarcascade_frontalface_alt.xml" 
    path_train = 'faces_dataset/train' 
    path_test = 'faces_dataset/test'</pre>
<ol start="15">
<li>Load the face cascade file:</li>
</ol>
<pre>    # Load face cascade file 
    faceCascade = cv2.CascadeClassifier(cascade_path) </pre>
<ol start="16">
<li>Create local binary patterns histogram for face recognizer objects:</li>
</ol>
<pre>    # Initialize Local Binary Patterns Histogram face recognizer 
    recognizer = cv2.face.createLBPHFaceRecognizer() </pre>
<ol start="17">
<li>Extract the images, labels, and label encoder for this input path:</li>
</ol>
<pre>    # Extract images, labels, and label encoder from training dataset 
    images, labels, le = get_images_and_labels(path_train) </pre>
<ol start="18">
<li>Train the face recognizer using the data that we extracted:</li>
</ol>
<pre>    # Train the face recognizer  
    print "\nTraining..." 
    recognizer.train(images, np.array(labels)) </pre>
<ol start="19">
<li>Test the face recognizer on unknown data:</li>
</ol>
<pre>    # Test the recognizer on unknown images 
    print '\nPerforming prediction on test images...' 
    stop_flag = False 
    for root, dirs, files in os.walk(path_test): 
        for filename in (x for x in files if x.endswith('.jpg')): 
            filepath = os.path.join(root, filename) </pre>
<ol start="20">
<li>Load the image:</li>
</ol>
<pre>            # Read the image 
            predict_image = cv2.imread(filepath, 0) </pre>
<ol start="21">
<li>Determine the location of the face using the face detector:</li>
</ol>
<pre>            # Detect faces 
            faces = faceCascade.detectMultiScale(predict_image, 1.1,  
                    2, minSize=(100,100)) </pre>
<ol start="22">
<li>For each face ROI, run the face recognizer:</li>
</ol>
<pre>            # Iterate through face rectangles 
            for (x, y, w, h) in faces: 
                # Predict the output 
                predicted_index, conf = recognizer.predict( 
                        predict_image[y:y+h, x:x+w])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="23">
<li>Convert the label into a word:</li>
</ol>
<pre>                # Convert to word label 
                predicted_person = le.num_to_word(predicted_index) </pre>
<ol start="24">
<li>Overlay the text on the output image and display it:</li>
</ol>
<pre>                # Overlay text on the output image and display it 
                cv2.putText(predict_image, 'Prediction: ' + predicted_person,  
                        (10,60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 6) 
                cv2.imshow("Recognizing face", predict_image) </pre>
<ol start="25">
<li>Check whether the user pressed the <em><span class="KeyPACKT">Esc</span></em> key. If so, break out of the loop:</li>
</ol>
<pre>            c = cv2.waitKey(0) 
            if c == 27: 
                stop_flag = True 
                break 
 
        if stop_flag: 
            break </pre>
<p style="padding-left: 60px">If you run this code, you will get an output window that displays the predicted outputs for test images. You can press the <em><span class="KeyPACKT">Space</span></em> button to keep looping. There are three different people in the test images. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The Local Binary Patterns Histogram algorithm is based on a non-parametric operator that synthesizes the local structure of an image. At a particular pixel, the LBP operator associates an ordered binary sequence of color intensity comparisons between that pixel and the pixels belonging to the considered neighborhood. In particular, if the intensity of the central pixel is greater than or equal to the intensity of the adjacent pixel, then a value of 1 is assigned. Otherwise, 0 is assigned. Therefore, for a neighborhood of 8 pixels, for example, there will be 2<sup>8</sup> possible combinations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span>To apply this operator to the face recognition problem, the idea is to divide the image into <em>m</em> local regions and extract a histogram from each of them. The vector of the features to be extracted consists of the concatenation of these local histograms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Local Binary Patterns</em> <em>Histogram</em>: <a href="http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html#local-binary-patterns-histograms">http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html#local-binary-patterns-histograms</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recognizing faces using the HOG-based model</h1>
                </header>
            
            <article>
                
<p>By face recognition, we mean the process that returns the position of the faces that are present in an image. In the <em>Building a face detector using Haar cascades</em> recipe, we already addressed this topic. In this recipe, we will use the <kbd>face_recognition</kbd> library to perform a series of operations on these faces. </p>
<p>The focal objective of face recognition consists of detecting the characteristics of a face and ignoring everything else that surrounds it. This is a feature on multiple commercial devices, and it allows you to establish when and how to apply focus in an image so that you can capture it. In the world of computer vision, it is customary to divide the family of face detection algorithms into two major categories. What distinguishes these two categories is their different uses of information, derived from a priori knowledge of the structure and properties of the face:</p>
<ul>
<li>The first category includes methods based on the extraction of specification features</li>
<li>The second category adopts a global approach to image analysis</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can use the <kbd>face_recognition</kbd> library to perform face recognition from a complex image. Before proceeding, install the <kbd>face_recognition</kbd> library. This library is based on the <kbd>dlib</kbd> library, which must be installed before we can go any further. <kbd>dlib</kbd> is a modern C++ toolkit that contains machine learning algorithms and tools for creating complex software in C++ to solve real-world problems. You can find information on installing the package at <a href="https://pypi.org/project/face_recognition/">https://pypi.org/project/face_recognition/</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can recognize faces using a HOG-based model:</p>
<ol>
<li>Create a new Python file and import the following packages<span> </span><span>(the full code is given in the</span> <span><kbd>FaceRecognition.py</kbd></span> <span>file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">from PIL import Image<br/>import face_recognition</pre>
<div style="padding-left: 60px" class="packt_infobox">The <strong>Python Imaging Library</strong> (<strong>PIL</strong>) is a free library for the Python programming language that adds support for opening, manipulating, and saving many different image file formats. <kbd>face_recognition</kbd> is a Python library that recognizes and manipulates faces from Python scripts or from the command line.</div>
<ol start="2">
<li>Let's load the <kbd>family.jpg</kbd> file into a NumPy array:</li>
</ol>
<pre style="padding-left: 60px">image = face_recognition.load_image_file("family.jpg")</pre>
<ol start="3">
<li>We will now find all of the faces in the image using the default HOG-based model:</li>
</ol>
<pre style="padding-left: 60px">face_locations = face_recognition.face_locations(image) </pre>
<ol start="4">
<li>Define a method to convert words into numbers:</li>
</ol>
<pre style="padding-left: 60px">print("Number {} face(s) recognized in this image.".format(len(face_locations)))</pre>
<ol start="5">
<li>Print the location of each face in this image:</li>
</ol>
<pre style="padding-left: 60px">for face_location in face_locations:<br/><br/>    top, right, bottom, left = face_location<br/>    print("Face location Top: {}, Left: {}, Bottom: {}, Right: {}".format(top, left, bottom, right))</pre>
<ol start="6">
<li>Finally, we need to access the actual face itself:</li>
</ol>
<pre>    face_image = image[top:bottom, left:right]<br/>    pil_image = Image.fromarray(face_image)<br/>    pil_image.show()</pre>
<p style="padding-left: 60px">A thumbnail of each recognized face will be returned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><strong>Histogram of Oriented Gradients</strong> (<strong>HOG</strong>) is a feature descriptor that's used for object recognition. The algorithm counts the occurrences of the orientation of the gradient in localized portions of an image. It differs from other techniques that are used for the same purpose (scale-invariant feature transforms, edge orientation histograms, shape contexts) because it uses a dense grid of uniformly spaced cells and uses localized superimposed normalization to improve accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The first to introduce this technology were Navneet Dalal and Bill Triggs (2005), researchers of the <strong>Institut national de recherche en informatique et en automatique</strong> (<strong>INRIA</strong>), while they were studying the problem of pedestrian detection in static images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>face_recognition</kbd> library: <a href="https://github.com/ageitgey/face_recognition">https://github.com/ageitgey/face_recognition</a> </li>
<li><em>Histograms of Oriented Gradients for Human Detection</em> (by Navneet Dalal and Bill Triggs from INRIA): <a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facial landmark recognition</h1>
                </header>
            
            <article>
                
<p>Face recognition is also complicated because of its orientation. The same face, directed in different directions from that of the observer, can induce the algorithm to identify it as a different face. To solve this problem, we can use facial landmarks, which are specific points on the face such as eyes, eyebrows, lips, nose, and so on. By using this technique, you can identify as many as 68 points on any face.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can extract facial features as facial landmarks. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform facial landmark recognition:</p>
<ol>
<li>Create a new Python file and import the following packages<span> </span><span>(the full code is given in the <kbd>FaceLandmarks.py</kbd></span><span> file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">from PIL import Image, ImageDraw<br/>import face_recognition</pre>
<ol start="2">
<li>Let's load the <kbd>ciaburro.jpg</kbd> file into a NumPy array:</li>
</ol>
<pre style="padding-left: 60px">image = face_recognition.load_image_file("ciaburro.jpg")</pre>
<ol start="3">
<li>Let's find all facial features in all of the faces in the image:</li>
</ol>
<pre style="padding-left: 60px">FaceLandmarksList = face_recognition.face_landmarks(image)</pre>
<ol start="4">
<li>Print the number of faces that were recognized in the image:</li>
</ol>
<pre style="padding-left: 60px">print("Number {} face(s) recognized in this image.".format(len(FaceLandmarksList)))</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong><span>Number 1 </span>face(s) recognized in this image</strong></pre>
<ol start="5">
<li>Create a PIL imagedraw object so that we can draw on the picture:</li>
</ol>
<pre style="padding-left: 60px">PilImage = Image.fromarray(image)<br/>DrawPilImage = ImageDraw.Draw(PilImage)</pre>
<ol start="6">
<li>At this point, we will insert a cycle that returns the position of the points for each facial feature that's included in the list and trace a line to the image:</li>
</ol>
<pre style="padding-left: 60px">for face_landmarks in FaceLandmarksList:</pre>
<ol start="7">
<li>First, we print the location of each facial feature in this image:</li>
</ol>
<pre>    for facial_feature in face_landmarks.keys():<br/>        print("{} points: {}".format(facial_feature, face_landmarks[facial_feature]))</pre>
<ol start="8">
<li>Then we trace out each facial feature in the image with a line:</li>
</ol>
<pre>    for facial_feature in face_landmarks.keys():<br/>        DrawPilImage.line(face_landmarks[facial_feature], width=5)</pre>
<ol start="9">
<li>Finally, we draw the image with the highlighted landmarks:</li>
</ol>
<pre style="padding-left: 60px">PilImage.show()</pre>
<p style="padding-left: 60px">In the following image, we can see the input image and <span>the image with highlighted landmarks:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1101 image-border" src="assets/009c45a4-9604-4d87-88ed-674d94796546.png" style="width:31.67em;height:21.25em;"/></p>
<p style="padding-left: 60px">In addition, the positions of the landmarks are printed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1102 image-border" src="assets/1aceb22e-4627-4676-ab75-4edfa0edce47.png" style="width:99.83em;height:37.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we learned<span> how to extract facial landmarks from an image and how to draw these points on the same image. The following landmarks were detected:</span></p>
<ul>
<li><kbd>chin</kbd></li>
<li><kbd>left_eyebrow</kbd></li>
<li><kbd>right_eyebrow</kbd></li>
<li><kbd>nose_bridge</kbd></li>
<li><kbd>nose_tip</kbd></li>
<li><kbd>left_eye</kbd></li>
<li><kbd>right_eye</kbd></li>
<li><kbd>top_lip</kbd></li>
<li><kbd>bottom_lip</kbd></li>
</ul>
<p>For each feature that was detected, connecting lines of the detection points were drawn to show the contours.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>To extract facial landmarks, the <kbd>face_recognition</kbd> library was used. This library performed this task by using the method that was introduced by Vahid Kazemi and Josephine Sullivan in the following paper: <em>One Millisecond Face Alignment with an Ensemble of Regression Trees</em>. To estimate the face's landmark positions, an ensemble of regression trees was used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>face_recognition</kbd> library: <a href="https://github.com/ageitgey/face_recognition">https://github.com/ageitgey/face_recognition</a> </li>
<li><em>One Millisecond Face Alignment with an Ensemble of Regression Trees</em> (by Vahid Kazemi and Josephine Sullivan): <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf">http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User authentication by face recognition</h1>
                </header>
            
            <article>
                
<p>Authentication technologies based on facial recognition have been a consolidated reality for several decades now. We no longer have to carry pocket cards, store it on the phone, or use mnemonics to remember <span>a different one </span>each time, if we are so considerate to change it often. What we need to do is authenticate that which we already had it with us. To do this, we just look at our webcam. An identification system based on facial recognition tries to identify a person by comparing the image of the face that was just acquired with those present in a database to find a possible correspondence. This leads to either allowing or prohibiting access.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will<span> see how we can build an identification system based on facial recognition using the <kbd>face_recognition</kbd>  library.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform user authentication by using face recognition:</p>
<ol>
<li>Create a new Python file and import the following packages<span> </span><span>(the full code is given in the <kbd>UserAuthentification.py</kbd></span><span> file that is provided for you)</span>:</li>
</ol>
<pre style="padding-left: 60px">import face_recognition</pre>
<ol start="2">
<li>Let's load all of the image files into NumPy arrays:</li>
</ol>
<pre style="padding-left: 60px">Image1 = face_recognition.load_image_file("giuseppe.jpg")<br/>Image2 = face_recognition.load_image_file("tiziana.jpg")<br/>UnknownImage = face_recognition.load_image_file("tiziana2.jpg")</pre>
<p style="padding-left: 60px">Three images have been loaded: the first two images refer to the faces we've already seen, while the third is the image to be compared (<kbd>tiziana</kbd>).</p>
<ol start="3">
<li>Get the face encodings for each face in each image file:</li>
</ol>
<pre style="padding-left: 60px">try:<br/>    Image1Encoding = face_recognition.face_encodings(Image1)[0]<br/>    Image2Encoding = face_recognition.face_encodings(Image2)[0]<br/>    UnknownImageEncoding = face_recognition.face_encodings(UnknownImage)[0]<br/>except IndexError:<br/>    print("Any face was located. Check the image files..")<br/>    quit()</pre>
<ol start="4">
<li>Let's define the known faces:</li>
</ol>
<pre>known_faces = [<br/>    Image1Encoding,<br/>    Image2Encoding<br/>]</pre>
<ol start="5">
<li>Let's compare the known faces with the unknown face we just loaded:</li>
</ol>
<pre style="padding-left: 60px">results = face_recognition.compare_faces(known_faces, UnknownImageEncoding)</pre>
<ol start="6">
<li>Finally, we will print the results of the comparison:</li>
</ol>
<pre style="padding-left: 60px">print("Is the unknown face a picture of Giuseppe? {}".format(results[0]))<br/>print("Is the unknown face a picture of Tiziana? {}".format(results[1]))<br/>print("Is the unknown face a new person that we've never seen before? {}".format(not True in results))</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>Is the unknown face a picture of Giuseppe? False</strong><br/><strong>Is the unknown face a picture of Tiziana? True</strong><br/><strong>Is the unknown face a new person that we've never seen before? False</strong></pre>
<p style="padding-left: 60px">As we can see, the authentication system recognized the user as Tiziana.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we learned how to build an identification system based on facial recognition. To do this, we extracted some basic measures from every face in the known database. In doing so, we were able to compare these basic measures with the basic measures of other faces that require authentication.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">These measurements were made using a deep convolutional neural network. The learning process works by analyzing three images simultaneously:</p>
<ul>
<li class="mce-root">An image that contains the face of a known person (anchor)</li>
<li class="mce-root">Another image of the same known person (positive)</li>
<li class="mce-root">An image of a completely different person (negative)</li>
</ul>
<p>At this point, the algorithm examines the measures it is generating for each of these three images. Then it adjusts the weights of the neural network to make sure that the measurements that were generated for faces 1 and 2 are slightly closer, while the measures for faces 2 and 3 are slightly more distant. This technique is called <kbd>Triplet Loss</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>So far, we have said that the secret to the success of an algorithm based on machine learning lies in the number of examples that are used in the learning phase. The greater the number, the greater the accuracy of the model. In the cases that we dealt with in this chapter, this cannot be considered valid. This is because, in the algorithms for facial recognition, the examples at our disposal were very limited.</p>
<p>Therefore, the construction and formation of a typical convolutional neural network will not work because it cannot learn the required functionality with the amount of data that's available. In these cases, we use a <strong>one-shot learning</strong> approach in which we construct a similarity function that compares two images and tells you if there is a match.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Official documentation of the </span><kbd>face_recognition</kbd> library: <a href="https://github.com/ageitgey/face_recognition">https://github.com/ageitgey/face_recognition</a></li>
<li><em>One-shot learning</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/One-shot_learning">https://en.wikipedia.org/wiki/One-shot_learning</a></li>
<li><em>One-shot learning of simple visual concepts</em> (from MIT): <a href="https://web.mit.edu/jgross/Public/lake_etal_cogsci2011.pdf">https://web.mit.edu/jgross/Public/lake_etal_cogsci2011.pdf</a></li>
<li><em>Siamese/Triplet Networks</em> (from Virginia Tech University): <a href="https://filebox.ece.vt.edu/~jbhuang/teaching/ece6554/sp17/lectures/Lecture_08_Siamese_Triplet_Networks.pdf">https://filebox.ece.vt.edu/~jbhuang/teaching/ece6554/sp17/lectures/Lecture_08_Siamese_Triplet_Networks.pdf</a>    </li>
</ul>


            </article>

            
        </section>
    </body></html>