<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Selection and Feature Engineering</h1>
                </header>
            
            <article>
                
<p>Feature engineering is the first step in a machine learning pipeline and involves all the techniques adopted to clean existing datasets, increase their signal-noise ratio, and reduce their dimensionality. Most algorithms have strong assumptions about the input data, and their performances can be negatively affected when raw datasets are used. Moreover, the data is seldom isotropic; there are often features that determine the general behavior of a sample, while others that are correlated don't provide any additional pieces of information. So, it's important to have a clear view of a dataset and know the most common algorithms used to reduce the number of features or select only the best ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">scikit-learn toy datasets</h1>
                </header>
            
            <article>
                
<p>scikit-learn provides some built-in datasets that can be used for testing purposes. They're all available in the package <kbd>sklearn.datasets</kbd> and have a common structure: the data instance variable contains the whole input set <kbd>X</kbd> while target contains the labels for classification or target values for regression. For example, considering the Boston house pricing dataset (used for regression), we have:</p>
<pre><strong>from sklearn.datasets import load_boston<br/><br/>&gt;&gt;&gt; boston = load_boston()<br/>&gt;&gt;&gt; X = boston.data<br/>&gt;&gt;&gt; Y = boston.target<br/><br/>&gt;&gt;&gt; X.shape<br/>(506, 13)<br/>&gt;&gt;&gt; Y.shape<br/>(506,)</strong></pre>
<p>In this case, we have 506 samples with 13 features and a single target value. In this book, we're going to use it for regressions and the MNIST handwritten digit dataset (<kbd>load_digits()</kbd>) for classification tasks. scikit-learn also provides functions for creating dummy datasets from scratch: <kbd>make_classification()</kbd>, <kbd>make_regression()</kbd>, and <kbd>make_blobs()</kbd> (particularly useful for testing cluster algorithms). They're very easy to use and in many cases, it's the best choice to test a model without loading more complex datasets.</p>
<div class="packt_infobox"><span>Visit <a href="http://scikit-learn.org/stable/datasets/">http://scikit-learn.org/stable/datasets/</a> for further information.</span></div>
<div class="packt_infobox"><span>The MNIST dataset provided by scikit-learn is limited for obvious reasons. If you want to experiment with the original one, refer to the website managed by Y. LeCun, C. Cortes, C. Burges: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>. Here you can download a full version made up of 70,000 handwritten digits already split into training and test sets.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating training and test sets</h1>
                </header>
            
            <article>
                
<p>When a dataset is large enough, it's a good practice to split it into training and test sets; the former to be used for training the model and the latter to test its performances. In the following figure, there's a schematic representation of this process:</p>
<div class="CDPAlignCenter CDPAlign"><img height="232" width="218" class="image-border" src="assets/4face3c7-67a7-4b70-b26d-ed1f256b89b4.png"/></div>
<p>There are two main rules in performing such an operation:</p>
<ul>
<li>Both datasets must reflect the original distribution</li>
<li>The original dataset must be randomly shuffled before the split phase in order to avoid a correlation between consequent elements</li>
</ul>
<p>With scikit-learn, this can be achieved using the <kbd>train_test_split()</kbd> function:</p>
<pre><strong><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split<br/><br/>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1000)</span></strong></pre>
<p>The parameter <kbd>test_size</kbd> (as well as <kbd>training_size</kbd>) allows specifying the percentage of elements to put into the test/training set. In this case, the ratio is 75 percent for training and 25 percent for the test phase. Another important parameter is <kbd>random_state</kbd> which can accept a NumPy <kbd>RandomState</kbd> generator or an integer seed. In many cases, it's important to provide reproducibility for the experiments, so it's also necessary to avoid using different seeds and, consequently, different random splits:</p>
<div class="packt_tip">My suggestion is to always use the same number (it can also be 0 or completely omitted), or define a global <kbd>RandomState</kbd> which can be passed to all requiring functions.</div>
<pre><strong>from sklearn.utils import check_random_state<br/><br/>&gt;&gt;&gt; rs = check_random_state(1000)<br/>&lt;mtrand.RandomState at 0x12214708&gt;<br/><br/>&gt;&gt;&gt; <span class="n">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=rs)</span></strong></pre>
<p>In this way, if the seed is kept equal, all experiments have to lead to the same results and can be easily reproduced in different environments by other scientists.</p>
<div class="packt_infobox">For further information about NumPy random number generation, visit <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing categorical data</h1>
                </header>
            
            <article>
                
<p>In many classification problems, the target dataset is made up of categorical labels which cannot immediately be processed by any algorithm. An encoding is needed and scikit-learn offers at least two valid options. Let's consider a very small dataset made of 10 categorical samples with two features each:</p>
<pre><strong>import numpy as np<br/><br/>&gt;&gt;&gt; X = np.random.uniform(0.0, 1.0, size=(10, 2))<br/>&gt;&gt;&gt; Y = np.random.choice(('Male','Female'), size=(10))<br/>&gt;&gt;&gt; X[0]<br/>array([ 0.8236887 ,  0.11975305])<br/>&gt;&gt;&gt; Y[0]<br/>'Male'</strong></pre>
<p>The first option is to use the <kbd>LabelEncoder</kbd> class, which adopts a dictionary-oriented approach, associating to each category label a progressive integer number, that is an index of an instance array called <kbd>classes_</kbd>:</p>
<pre><strong>from sklearn.preprocessing import LabelEncoder<br/><br/>&gt;&gt;&gt; le = LabelEncoder()<br/>&gt;&gt;&gt; yt = le.fit_transform(Y)<br/>&gt;&gt;&gt; print(yt)<br/>[0 0 0 1 0 1 1 0 0 1]<br/><br/>&gt;&gt;&gt; le.classes_array(['Female', 'Male'], dtype='|S6')</strong></pre>
<p>The inverse transformation can be obtained in this simple way:</p>
<pre><strong>&gt;&gt;&gt; output = [1, 0, 1, 1, 0, 0]<br/>&gt;&gt;&gt; decoded_output = [le.classes_[i] for i in output]<br/>['Male', 'Female', 'Male', 'Male', 'Female', 'Female']</strong></pre>
<p>This approach is simple and works well in many cases, but it has a drawback: all labels are turned into sequential numbers. A classifier which works with real values will then consider similar numbers according to their distance, without any concern for semantics. For this reason, it's often preferable to use so-called <strong>one-hot</strong> <strong>encod</strong><strong>ing</strong>, which binarizes the data. For labels, it can be achieved using the <kbd>LabelBinarizer</kbd> class:</p>
<pre><strong>from sklearn.preprocessing import LabelBinarizer<br/><br/>&gt;&gt;&gt; lb = LabelBinarizer()<br/>&gt;&gt;&gt; Yb = lb.fit_transform(Y)<br/>array([[1],<br/>       [0],<br/>       [1],<br/>       [1],<br/>       [1],<br/>       [1],<br/>       [0],<br/>       [1],<br/>       [1],<br/>       [1]])<br/><br/>&gt;&gt;&gt; lb.inverse_transform(Yb)<br/>array(['Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male',<br/>       'Male', 'Male'], dtype='|S6')</strong></pre>
<p>In this case, each categorical label is first turned into a positive integer and then transformed into a vector where only one feature is 1 while all the others are 0. It means, for example, that using a softmax distribution with a peak corresponding to the main class can be easily turned into a discrete vector where the only non-null element corresponds to the right class. For example:</p>
<pre><strong>import numpy as np<br/><br/>&gt;&gt;&gt; Y = lb.fit_transform(Y)<br/>array([[0, 1, 0, 0, 0],<br/>       [0, 0, 0, 1, 0],<br/>       [1, 0, 0, 0, 0]])<br/><br/>&gt;&gt;&gt; Yp = model.predict(X[0])<br/>array([[0.002, 0.991, 0.001, 0.005, 0.001]])<br/><br/>&gt;&gt;&gt; Ypr = np.round(Yp)<br/>array([[ 0.,  1.,  0.,  0.,  0.]])<br/><br/>&gt;&gt;&gt; lb.inverse_transform(Ypr)<br/>array(['Female'], dtype='|S6')</strong></pre>
<p>Another approach to categorical features can be adopted when they're structured like a list of dictionaries (not necessarily dense, they can have values only for a few features). For example:</p>
<pre><strong>data = [<br/>   { 'feature_1': 10.0, 'feature_2': 15.0 },<br/>   { 'feature_1': -5.0, 'feature_3': 22.0 },<br/>   { 'feature_3': -2.0, 'feature_4': 10.0 }<br/>]</strong></pre>
<p>In this case, scikit-learn offers the classes <kbd>DictVectorizer</kbd> and <kbd>FeatureHasher</kbd>; they both produce sparse matrices of real numbers that can be fed into any machine learning model. The latter has a limited memory consumption and adopts <strong>MurmurHash 3</strong> (read <a href="https://en.wikipedia.org/wiki/MurmurHash">https://en.wikipedia.org/wiki/MurmurHash</a>, for further information). The code for these two methods is shown as follows:</p>
<pre><strong>from sklearn.feature_extraction import DictVectorizer, FeatureHasher<br/><br/>&gt;&gt;&gt; dv = DictVectorizer()<br/>&gt;&gt;&gt; Y_dict = dv.fit_transform(data)<br/><br/>&gt;&gt;&gt; Y_dict.todense()<br/>matrix([[ 10.,  15.,   0.,   0.],<br/>        [ -5.,   0.,  22.,   0.],<br/>        [  0.,   0.,  -2.,  10.]])<br/><br/>&gt;&gt;&gt; dv.vocabulary_<br/>{'feature_1': 0, 'feature_2': 1, 'feature_3': 2, 'feature_4': 3}<br/><br/>&gt;&gt;&gt; fh = FeatureHasher()<br/>&gt;&gt;&gt; Y_hashed = fh.fit_transform(data)<br/><br/>&gt;&gt;&gt; Y_hashed.todense()<br/>matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])</strong></pre>
<p>In both cases, I suggest you read the original scikit-learn documentation to know all possible options and parameters. </p>
<p>When working with categorical features (normally converted into positive integers through <kbd>LabelEncoder</kbd>), it's also possible to filter the dataset in order to apply one-hot encoding using the <kbd>OneHotEncoder</kbd> class. In the following example, the first feature is a binary index which indicates <kbd>'Male'</kbd> or <kbd>'Female'</kbd>:</p>
<pre><strong>from sklearn.preprocessing import OneHotEncoder<br/><br/>&gt;&gt;&gt; data = [<br/>   [0, 10],<br/>   [1, 11],<br/>   [1, 8],<br/>   [0, 12],<br/>   [0, 15]<br/>]<br/><br/>&gt;&gt;&gt; oh = OneHotEncoder(categorical_features=[0])<br/>&gt;&gt;&gt; Y_oh = oh.fit_transform(data1)<br/><br/>&gt;&gt;&gt; Y_oh.todense()<br/>matrix([[  1.,   0.,  10.],<br/>        [  0.,   1.,  11.],<br/>        [  0.,   1.,   8.],<br/>        [  1.,   0.,  12.],<br/>        [  1.,   0.,  15.]])</strong></pre>
<p>Considering that these approaches increase the number of values (also exponentially with binary versions), all the classes adopt sparse matrices based on SciPy implementation. See <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html">https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html</a>  for further information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing missing features</h1>
                </header>
            
            <article>
                
<p>Sometimes a dataset can contain missing features, so there are a few options that can be taken into account:</p>
<ul>
<li>Removing the whole line</li>
<li>Creating sub-model to predict those features</li>
<li>Using an automatic strategy to input them according to the other known values</li>
</ul>
<p>The first option is the most drastic one and should be considered only when the dataset is quite large, the number of missing features is high, and any prediction could be risky. The second option is much more difficult because it's necessary to determine a supervised strategy to train a model for each feature and, finally, to predict their value. <span>Considering all pros and cons,</span> the third option is likely to be the best choice. scikit-learn offers the class <kbd>Imputer</kbd>, which is responsible for filling the holes using a strategy based on the mean (default choice), median, or frequency (the most frequent entry will be used for all the missing ones).</p>
<p>The following snippet shows an example using the three approaches (the default value for a missing feature entry is <kbd>NaN</kbd>. However, it's possible to use a different placeholder through the parameter <kbd>missing_values</kbd>):</p>
<pre><strong>from sklearn.preprocessing import Imputer<br/><br/>&gt;&gt;&gt; data = np.array([[1, np.nan, 2], [2, 3, np.nan], [-1, 4, 2]])<br/><br/>&gt;&gt;&gt; imp = Imputer(strategy='mean')<br/>&gt;&gt;&gt; imp.fit_transform(data)<br/>array([[ 1. ,  3.5,  2. ],<br/>       [ 2. ,  3. ,  2. ],<br/>       [-1. ,  4. ,  2. ]])<br/><br/>&gt;&gt;&gt; imp = Imputer(strategy='median')<br/>&gt;&gt;&gt; imp.fit_transform(data)<br/>array([[ 1. ,  3.5,  2. ],<br/>       [ 2. ,  3. ,  2. ],<br/>       [-1. ,  4. ,  2. ]])<br/><br/>&gt;&gt;&gt; imp = Imputer(strategy='most_frequent')<br/>&gt;&gt;&gt; imp.fit_transform(data)<br/>array([[ 1.,  3.,  2.],<br/>       [ 2.,  3.,  2.],<br/>       [-1.,  4.,  2.]])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data scaling and normalization</h1>
                </header>
            
            <article>
                
<p>A generic dataset (we assume here that it is always numerical) is made up of different values which can be drawn from different distributions, having different scales and, sometimes, there are also outliers. A machine learning algorithm isn't naturally able to distinguish among these various situations, and therefore, it's always preferable to standardize datasets before processing them. A very common problem derives from having a non-zero mean and a variance greater than one. In the following figure, there's a comparison between a raw dataset and the same dataset scaled and centered:</p>
<div class="CDPAlignCenter CDPAlign"><img height="204" width="507" class="image-border" src="assets/f03ba67d-26af-48a0-bc77-ea25d79ced2e.png"/></div>
<p>This result can be achieved using the <kbd>StandardScaler</kbd> class:</p>
<pre><strong>from sklearn.preprocessing import StandardScaler<br/><br/>&gt;&gt;&gt; ss = StandardScaler()<br/>&gt;&gt;&gt; scaled_data = ss.fit_transform(data)</strong></pre>
<p>It's possible to specify if the scaling process must include both mean and standard deviation using the parameters <kbd>with_mean=True/False</kbd> and <kbd>with_std=True/False</kbd> (by default they're both active). If you need a more powerful scaling feature, with a superior control on outliers and the possibility to select a quantile range, there's also the class <kbd>RobustScaler</kbd>. Here are some examples with different quantiles:</p>
<pre><strong>from sklearn.preprocessing import RubustScaler<br/><br/>&gt;&gt;&gt; rb1 = RobustScaler(quantile_range=(15, 85))<br/>&gt;&gt;&gt; scaled_data1 = rb1.fit_transform(data)<br/><br/>&gt;&gt;&gt; rb1 = RobustScaler(quantile_range=(25, 75))<br/>&gt;&gt;&gt; scaled_data1 = rb1.fit_transform(data)<br/><br/>&gt;&gt;&gt; rb2 = RobustScaler(quantile_range=(30, 60))<br/>&gt;&gt;&gt; scaled_data2 = rb2.fit_transform(data)</strong></pre>
<p>The results are shown in the following figures:</p>
<div class="CDPAlignCenter CDPAlign"><img height="393" width="398" class="image-border" src="assets/a924212e-b15b-43dc-a763-148643c16c73.png"/></div>
<p class="CDPAlignLeft CDPAlign">Other options include <kbd>MinMaxScaler</kbd> and <kbd>MaxAbsScaler</kbd>, which scale data by removing elements that don't belong to a given range (the former) or by considering a maximum absolute value (the latter).</p>
<p class="CDPAlignLeft CDPAlign">scikit-learn also provides a class for per-sample normalization, <kbd>Normalizer.</kbd> It can apply <kbd>max</kbd>, <kbd>l1</kbd> and <kbd>l2</kbd> norms to each element of a dataset. In a Euclidean space, they are defined in the following way:</p>
<div class="CDPAlignCenter CDPAlign"><img height="160" width="238" src="assets/da7716fe-b6c8-454f-8ed5-68db71930fee.png"/></div>
<p class="CDPAlignLeft CDPAlign">An example of every normalization is shown next:</p>
<pre><strong>from sklearn.preprocessing import Normalizer<br/><br/>&gt;&gt;&gt; data = np.array([1.0, 2.0])<br/><br/>&gt;&gt;&gt; n_max = Normalizer(norm='max')<br/>&gt;&gt;&gt; n_max.fit_transform(data.reshape(1, -1))<br/>[[ 0.5, 1. ]]<br/><br/>&gt;&gt;&gt; n_l1 = Normalizer(norm='l1')<br/>&gt;&gt;&gt; n_l1.fit_transform(data.reshape(1, -1))<br/>[[ 0.33333333,  0.66666667]]<br/><br/>&gt;&gt;&gt; n_l2 = Normalizer(norm='l2')<br/>&gt;&gt;&gt; n_l2.fit_transform(data.reshape(1, -1))<br/>[[ 0.4472136 ,  0.89442719]]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection and filtering</h1>
                </header>
            
            <article>
                
<p>An unnormalized dataset with many features contains information proportional to the independence of all features and their variance. Let's consider a small dataset with three features, generated with random Gaussian distributions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/cc419853-87e5-48aa-951e-75685ec5bc2b.png"/></div>
<p class="mce-root">Even without further analysis, it's obvious that the central line (with the lowest variance) is almost constant and doesn't provide any useful information. If you remember the previous chapter, the entropy H(X) is quite small, while the other two variables carry more information. A variance threshold is, therefore, a useful approach to remove all those elements whose contribution (in terms of variability and so, information) is under a predefined level. scikit-learn provides the class <kbd>VarianceThreshold</kbd> that can easily solve this problem. By applying it on the previous dataset, we get the following result:</p>
<pre><strong>from sklearn.feature_selection import VarianceThreshold<br/><br/>&gt;&gt;&gt; X[0:3, :]<br/>array([[-3.5077778 , -3.45267063,  0.9681903 ],<br/>       [-3.82581314,  5.77984656,  1.78926338],<br/>       [-2.62090281, -4.90597966,  0.27943565]])<br/><br/>&gt;&gt;&gt; vt = VarianceThreshold(threshold=1.5)<br/>&gt;&gt;&gt; X_t = vt.fit_transform(X)<br/><br/>&gt;&gt;&gt; X_t[0:3, :]<br/>array([[-0.53478521, -2.69189452],<br/>       [-5.33054034, -1.91730367],<br/>       [-1.17004376,  6.32836981]])</strong></pre>
<p class="mce-root">The third feature has been completely removed because its variance is under the selected threshold (1.5 in this case).</p>
<p class="mce-root">There are also many univariate methods that can be used in order to select the best features according to specific criteria based on F-tests and p-values, such as chi-square or ANOVA. However, their discussion is beyond the scope of this book and the reader can find further information in Freedman D., Pisani R., Purves R., <em>Statistics</em>, Norton &amp; Company.</p>
<p class="mce-root">Two examples of feature selection that use the classes <kbd>SelectKBest</kbd> (which selects the best <em>K</em> high-score features) and <kbd>SelectPercentile</kbd> (which selects only a subset of features belonging to a certain percentile) are shown next. It's possible to apply them both to regression and classification datasets, being careful to select appropriate score functions: </p>
<pre><strong>from sklearn.datasets import load_boston, load_iris<br/>from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_regression<br/><br/>&gt;&gt;&gt; regr_data = load_boston()<br/>&gt;&gt;&gt; regr_data.data.shape<br/>(506L, 13L)<br/><br/>&gt;&gt;&gt; kb_regr = SelectKBest(f_regression)<br/>&gt;&gt;&gt; X_b = kb_regr.fit_transform(regr_data.data, regr_data.target)<br/><br/>&gt;&gt;&gt; X_b.shape<br/>(506L, 10L)<br/><br/>&gt;&gt;&gt; kb_regr.scores_<br/>array([  88.15124178,   75.2576423 ,  153.95488314,   15.97151242,<br/>        112.59148028,  471.84673988,   83.47745922,   33.57957033,<br/>         85.91427767,  141.76135658,  175.10554288,   63.05422911,<br/>        601.61787111])<br/><br/>&gt;&gt;&gt; class_data = load_iris()<br/>&gt;&gt;&gt; class_data.data.shape<br/>(150L, 4L)<br/><br/>&gt;&gt;&gt; perc_class = SelectPercentile(chi2, percentile=15)<br/>&gt;&gt;&gt; X_p = perc_class.fit_transform(class_data.data, class_data.target)<br/><br/>&gt;&gt;&gt; X_p.shape<br/>(150L, 1L)<br/><br/>&gt;&gt;&gt; perc_class.scores_<br/>array([  10.81782088,    3.59449902,  116.16984746,   67.24482759])<br/><br/></strong></pre>
<div class="packt_infobox">For further details about all scikit-learn score functions and their usage, visit <a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection">http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal component analysis</h1>
                </header>
            
            <article>
                
<p>In many cases, the dimensionality of the input dataset <em>X</em> is high and so is the complexity of every related machine learning algorithm. Moreover, the information is seldom spread uniformly across all the features and, as discussed in the previous chapter, there will be high entropy features together with low entropy ones, which, of course, don't contribute dramatically to the final outcome. In general, if we consider a Euclidean space, we have:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="490" src="assets/ecde11b5-c38a-4021-ae35-78ef52874626.png"/></div>
<p class="CDPAlignLeft CDPAlign">So each point is expressed using an orthonormal basis made of <em>m</em> linearly independent vectors. Now, considering a dataset <em>X</em>, a natural question arises: is it possible to reduce <em>m</em> without a drastic loss of information? Let's consider the following figure (without any particular interpretation):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/fdb11a61-2f30-4d27-887b-2fa888fda1e4.png"/></div>
<p class="mce-root">It doesn't matter which distributions generated <em>X=(x,y)</em>, however, the variance of the horizontal component is clearly higher than the vertical one. As discussed, it means that the amount of information provided by the first component is higher and, for example, if the <em>x</em> axis is stretched horizontally keeping the vertical one fixed, the distribution becomes similar to a segment where the depth has lower and lower importance.</p>
<p class="mce-root">In order to assess how much information is brought by each component, and the correlation among them, a useful tool is the covariance matrix (if the dataset has zero mean, we can use the correlation matrix):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"> <img height="139" width="291" src="assets/a6ab6c65-7f4f-4798-88b5-bf0cf16ac842.png"/></div>
<p class="mce-root"><em>C</em> is symmetric and positive semidefinite, so all the eigenvalues are non-negative, but what's the meaning of each value? The covariance matrix for the previous example is:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="46" width="134" src="assets/6f4f0527-9563-462c-8671-a4f8a9c8c7ac.png"/></p>
<p class="mce-root">As expected, the horizontal variance is quite a bit higher than the vertical one. Moreover, the other values are close to zero. If you remember the definition and, for simplicity, remove the mean term, they represent the cross-correlation between couples of components. It's obvious that in our example, <em>X</em> and <em>Y</em> are uncorrelated (they're orthogonal), but in real-life examples, there could be features which present a residual cross-correlation. In terms of information theory, it means that knowing <em>Y</em> gives us some information about <em>X</em> (which we already know), so they share information which is indeed doubled. So our goal is also to decorrelate <em>X</em> while trying to reduce its dimensionality.</p>
<p class="mce-root">This can be achieved considering the sorted eigenvalues of <em>C</em> and selecting <em>g &lt; m</em> values:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="35" width="426" src="assets/6ac2fc1e-606d-4aa8-8d71-69865eec2f78.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="36" width="357" src="assets/c9b9db73-0c6c-4e46-99fd-b40580ae4835.png"/></div>
<p class="mce-root">So, it's possible to project the original feature vectors into this new (sub-)space, where each component carries a portion of total variance and where the new covariance matrix is decorrelated to reduce useless information sharing (in terms of correlation) among different features. In scikit-learn, there's the <kbd>PCA</kbd> class which can do all this in a very smooth way:</p>
<pre><strong>from sklearn.datasets import load_digits<br/>from sklearn.decomposition import PCA<br/><br/>&gt;&gt;&gt; digits = load_digits()</strong></pre>
<p>A figure with a few random MNIST handwritten digits is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="474" width="476" class="image-border" src="assets/6015040f-b0ca-4736-aede-87f9a9b6b00f.png"/></div>
<p class="mce-root">Each image is a vector of 64 unsigned int (8 bit) numbers (0, 255), so the initial number of components is indeed 64. However, the total amount of black pixels is often predominant and the basic signs needed to write 10 digits are similar, so it's reasonable to assume both high cross-correlation and a low variance on several components. Trying with 36 principal components, we get:</p>
<pre><strong>&gt;&gt;&gt; pca = PCA(n_components=36, whiten=True)<br/>&gt;&gt;&gt; X_pca = pca.fit_transform(digits.data / 255)</strong></pre>
<p>In order to improve performance, all integer values are normalized into the range [0, 1] and, through the parameter <kbd>whiten=True</kbd>, the variance of each component is scaled to one. As also the official scikit-learn documentation says, this process is particularly useful when an isotropic distribution is needed for many algorithms to perform efficiently. It's possible to access the explained variance ratio through the instance variable <kbd>explained_variance_ratio_</kbd><em><strong>, </strong></em>which shows which part of the total variance is carried by each single component:</p>
<pre><strong>&gt;&gt;&gt; pca.explained_variance_ratio_<br/>array([ 0.14890594,  0.13618771,  0.11794594,  0.08409979,  0.05782415,<br/>        0.0491691 ,  0.04315987,  0.03661373,  0.03353248,  0.03078806,<br/>        0.02372341,  0.02272697,  0.01821863,  0.01773855,  0.01467101,<br/>        0.01409716,  0.01318589,  0.01248138,  0.01017718,  0.00905617,<br/>        0.00889538,  0.00797123,  0.00767493,  0.00722904,  0.00695889,<br/>        0.00596081,  0.00575615,  0.00515158,  0.00489539,  0.00428887,<br/>        0.00373606,  0.00353274,  0.00336684,  0.00328029,  0.0030832 ,<br/>        0.00293778])</strong></pre>
<p>A plot for the example of MNIST digits is shown next. The left graph represents the variance ratio while the right one is the cumulative variance. It can be immediately seen how the first components are normally the most important ones in terms of information, while the following ones provide details that a classifier could also discard:</p>
<div class="CDPAlignCenter CDPAlign"><img height="197" width="488" class="image-border" src="assets/97361a4e-2c85-49b2-870c-09ee33bbae45.png"/></div>
<p>As expected, the contribution to the total variance decreases dramatically starting from the fifth component, so it's possible to reduce the original dimensionality without an unacceptable loss of information, which could drive an algorithm to learn wrong classes. In the preceding graph, there are the same handwritten digits rebuilt using the first 36 components with whitening and normalization between 0 and 1. To obtain the original images, we need to inverse-transform all new vectors and project them into the original space:</p>
<pre><strong>&gt;&gt;&gt; X_rebuilt = pca.inverse_transform(X_pca)</strong></pre>
<p>The result is shown in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="462" width="463" class="image-border" src="assets/5fbbc7f9-9929-471d-b02e-276aab96b146.png"/></div>
<p class="mce-root">This process can also partially denoise the original images by removing residual variance, which is often associated with noise or unwanted contributions (almost every calligraphy distorts some of the structural elements which are used for recognition).</p>
<p class="mce-root">I suggest the reader try different numbers of components (using the explained variance data) and also <kbd>n_components='mle'</kbd>, which implements an automatic selection of the best dimensionality (Minka T.P, <em>Automatic Choice of Dimensionality for PCA</em>, NIPS 2000: 598-604).</p>
<div class="packt_infobox">scikit-learn solves the PCA problem with <strong>SVD</strong> (<strong>Singular Value Decomposition</strong>), which can be studied in detail in Poole D., <em>Linear Algebra</em><span>, Brooks Cole</span>. It's possible to control the algorithm through the parameter <kbd>svd_solver</kbd>, whose values are <kbd>'auto', 'full', 'arpack', 'randomized'</kbd>. Arpack implements a truncated SVD. Randomized is based on an approximate algorithm which drops many singular vectors and can achieve very good performances also with high-dimensional datasets where the actual number of components is sensibly smaller.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Non-negative matrix factorization</h1>
                </header>
            
            <article>
                
<p>When the dataset is made up of non-negative elements, it's possible to use <strong>non-negative matrix factorization</strong> (<strong>NNMF</strong>) instead of standard PCA. The algorithm optimizes a loss function (alternatively on <em>W</em> and <em>H</em>) based on the Frobenius norm:</p>
<div class="CDPAlignCenter CDPAlign"><img height="60" width="346" src="assets/3b3a598d-3702-4f47-b40f-d74ad844945f.png"/></div>
<p>If <em>dim(X) = n x m</em>, then <em>dim(W) = n x p</em> and <em>dim(H) = p x m</em> with <em>p</em> equal to the number of requested components (the <kbd>n_components</kbd> parameter), which is normally smaller than the original dimensions <em>n</em> and <em>m</em>.</p>
<p class="CDPAlignLeft CDPAlign">The final reconstruction is purely additive and it has been shown that it's particularly efficient for images or text where there are normally no non-negative elements. In the following snippet, there's an example using the Iris dataset (which is non-negative). The <kbd>init</kbd> parameter can assume different values (see the documentation) which determine how the data matrix is initially processed. A random choice is for non-negative matrices which are only scaled (no SVD is performed):</p>
<pre><strong>from sklearn.datasets import load_iris</strong><br/><strong>from sklearn.decomposition import NMF</strong><br/><br/><strong>&gt;&gt;&gt; iris = load_iris()</strong><br/><strong>&gt;&gt;&gt; iris.data.shape</strong><br/><strong>(150L, 4L)</strong><br/><br/><strong>&gt;&gt;&gt; nmf = NMF(n_components=3, init='random', l1_ratio=0.1)</strong><br/><strong>&gt;&gt;&gt; Xt = nmf.fit_transform(iris.data)</strong><br/><br/><strong>&gt;&gt;&gt; nmf.reconstruction_err_</strong><br/><strong>1.8819327624141866</strong><br/><br/><strong>&gt;&gt;&gt; iris.data[0]</strong><br/><strong>array([ 5.1,  3.5,  1.4,  0.2])</strong><br/><strong>&gt;&gt;&gt; Xt[0]</strong><br/><strong>array([ 0.20668461,  1.09973772,  0.0098996 ])</strong><br/><strong>&gt;&gt;&gt; nmf.inverse_transform(Xt[0])</strong><br/><strong>array([ 5.10401653,  3.49666967,  1.3965409 ,  0.20610779])</strong></pre>
<p>NNMF, together with other factorization methods, will be very useful for more advanced techniques, such as recommendation systems and topic modeling.</p>
<div class="packt_infobox">NNMF is very sensitive to its parameters (in particular, initialization and regularization), so I suggest reading the original documentation for further information: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html.</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sparse PCA</h1>
                </header>
            
            <article>
                
<p>scikit-learn provides different PCA variants that can solve particular problems. I do suggest reading the original documentation. However, I'd like to mention <kbd>SparsePCA</kbd>, which allows exploiting the natural sparsity of data while extracting principal components. If you think about the handwritten digits or other images that must be classified, their initial dimensionality can be quite high (a 10x10 image has 100 features). However, applying a standard PCA selects only the average most important features, assuming that every sample can be rebuilt using the same components. Simplifying, this is equivalent to:</p>
<div class="CDPAlignCenter CDPAlign"><img height="30" width="213" src="assets/c71979ac-0808-4743-9c14-aa7e08345cf1.png"/></div>
<p class="CDPAlignLeft CDPAlign">On the other hand, we can always use a limited number of components, but without the limitation given by a dense projection matrix. This can be achieved by using sparse matrices (or vectors), where the number of non-zero elements is quite low. In this way, each element can be rebuilt using its specific components (in most cases, they will be always the most important), which can include elements normally discarded by a dense PCA. The previous expression now becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="23" width="455" src="assets/06c91861-72a2-4819-81ee-7fb087b0ba77.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here the non-null components have been put into the first block (they don't have the same order as the previous expression), while all the other zero terms have been separated. In terms of linear algebra, the vectorial space now has the original dimensions. However, using the power of sparse matrices (provided by <kbd>scipy.sparse</kbd>), scikit-learn can solve this problem much more efficiently than a classical PCA.</p>
<p class="CDPAlignLeft CDPAlign">The following snippet shows a sparse PCA with 60 components. In this context, they're usually called atoms and the amount of sparsity can be controlled via <em>L1</em>-norm regularization (higher <kbd>alpha</kbd> parameter values lead to more sparse results). This approach is very common in classification algorithms and will be discussed in the next chapters: </p>
<pre><strong>from sklearn.decomposition import SparsePCA</strong><br/><br/><strong>&gt;&gt;&gt; spca = SparsePCA(n_components=60, alpha=0.1)</strong><br/><strong>&gt;&gt;&gt; X_spca = spca.fit_transform(digits.data / 255)</strong><br/><br/><strong>&gt;&gt;&gt; spca.components_.shape</strong><br/><strong>(60L, 64L)</strong></pre>
<div class="CDPAlignLeft CDPAlign packt_infobox">For further information about SciPy sparse matrices, visit <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html">https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel PCA</h1>
                </header>
            
            <article>
                
<p>We're going to discuss kernel methods in <a href="82926f62-2446-4d69-b575-b6f614bb5b0d.xhtml" target="_blank">Chapter 7</a>, <em>Support Vector Machines</em>, however, it's useful to mention the class <kbd>KernelPCA</kbd>, which performs a PCA with non-linearly separable data sets. Just to understand the logic of this approach (the mathematical formulation isn't very simple), it's useful to consider a projection of each sample into a particular space where the dataset becomes linearly separable. The components of this space correspond to the first, second, ... principal components and a kernel PCA algorithm, therefore, computes the projection of our samples onto each of them.</p>
<p>Let's consider a dataset made up of a circle with a blob inside:</p>
<pre><strong>from sklearn.datasets import make_circles</strong><br/><br/><strong>&gt;&gt;&gt; Xb, Yb = make_circles(n_samples=500, factor=0.1, noise=0.05)</strong></pre>
<p class="CDPAlignLeft CDPAlign">The graphical representation is shown in the following picture. In this case, a classic PCA approach isn't able to capture the non-linear dependency of existing components (the reader can verify that the projection is equivalent to the original dataset). However, looking at the samples and using polar coordinates (therefore, a space where it's possible to project all the points), it's easy to separate the two sets, only considering the radius:</p>
<div class="CDPAlignCenter CDPAlign"><img height="444" width="469" class="image-border" src="assets/27ca52b7-07f6-499b-9a5d-7ee31b4c3eca.png"/></div>
<p>Considering the structure of the dataset, it's possible to investigate the behavior of a PCA with a radial basis function kernel. As the default value for <kbd>gamma</kbd> is 1.0/number of features (for now, consider this parameter as inversely proportional to the variance of a Gaussian), we need to increase it to capture the external circle. A value of 1.0 is enough:</p>
<pre><strong>from sklearn.decomposition import KernelPCA</strong><br/><br/><strong>&gt;&gt;&gt; kpca = KernelPCA(n_components=2, kernel='rbf', fit_inverse_transform=True, gamma=1.0)</strong><br/><strong>&gt;&gt;&gt; X_kpca = kpca.fit_transform(Xb)</strong></pre>
<p>The instance variable <kbd>X_transformed_fit_</kbd> will contain the projection of our dataset into the new space. Plotting it, we get:</p>
<p class="CDPAlignCenter CDPAlign"><img height="429" width="446" class="image-border" src="assets/eac317ef-e108-4f14-9964-378f07f6278b.png"/></p>
<p>The plot shows a separation just like expected, and it's also possible to see that the points belonging to the central blob have a curve distribution because they are more sensitive to the distance from the center.</p>
<p>Kernel PCA is a powerful instrument when we think of our dataset as made up of elements that can be a function of components (in particular, radial-basis or polynomials) but we aren't able to determine a linear relationship among them.</p>
<div class="packt_infobox">For more information about the different kernels supported by scikit-learn, visit <a href="http://scikit-learn.org/stable/modules/metrics.html#linear-kernel">http://scikit-learn.org/stable/modules/metrics.html#linear-kernel</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atom extraction and dictionary learning</h1>
                </header>
            
            <article>
                
<p>Dictionary learning is a technique which allows rebuilding a sample starting from a sparse dictionary of atoms (similar to principal components). In Mairal J., Bach F., Ponce J., Sapiro G., <em>Online Dictionary Learning for Sparse Coding</em>, Proceedings of the 29th International Conference on Machine Learning, 2009 there's a description of the same online strategy adopted by scikit-learn, which can be summarized as a double optimization problem where:</p>
<div class="CDPAlignCenter CDPAlign"><img height="49" width="274" src="assets/02c0fcbb-b224-4945-8488-0831319c3ac8.png"/></div>
<p class="CDPAlignLeft CDPAlign">Is an input dataset and the target is to find both a dictionary <em>D</em> and a set of weights for each sample:</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="327" src="assets/dec8e494-e572-4cf2-88b1-7ca9aceafa56.png"/></div>
<p class="CDPAlignLeft CDPAlign">After the training process, an input vector can be computed as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="26" width="76" src="assets/2b5629e1-50c6-46fa-98fe-b2f15b75e006.png"/></div>
<p class="CDPAlignLeft CDPAlign">The optimization problem (which involves both <em>D</em> and alpha vectors) can be expressed as the minimization of the following loss function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="46" width="231" src="assets/f0856f12-d499-4825-9430-8477d3b5feb9.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here the parameter <em>c</em> controls the level of sparsity (which is proportional to the strength of <em>L1</em> normalization). This problem can be solved by alternating the least square variable until a stable point is reached.</p>
<p class="CDPAlignLeft CDPAlign">In scikit-learn, we can implement such an algorithm with the class <kbd>DictionaryLearning</kbd><em><strong> </strong></em>(using the usual MNIST datasets), where <kbd>n_components</kbd>, as usual, determines the number of atoms:</p>
<pre><strong>from sklearn.decomposition import DictionaryLearning</strong><br/><br/><strong>&gt;&gt;&gt; dl = DictionaryLearning(n_components=36, fit_algorithm='lars', transform_algorithm='lasso_lars')</strong><br/><strong>&gt;&gt;&gt; X_dict = dl.fit_transform(digits.data)</strong></pre>
<p>A plot of each atom (component) is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="481" width="481" class="image-border" src="assets/af17080b-d4f5-4d0f-a7d3-f9cd63335a63.png"/></div>
<div class="CDPAlignLeft CDPAlign packt_infobox">This process can be very long on low-end machines. In such a case, I suggest limiting the number of samples to 20 or 30.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Freedman D., Pisani R., Purves R., <em>Statistics,</em> Norton &amp; Company</li>
<li>Gareth J., Witten D., Hastie T., Tibshirani R., <em>An Introduction to Statistical Learning: With Application in R</em>, Springer</li>
<li>Poole D., <em>Linear Algebra</em>, Brooks Cole</li>
<li>Minka T.P, <span><em>Automatic Choice of Dimensionality for PCA</em>, NIPS 2000: 598-604</span></li>
<li>Mairal J., Bach F., Ponce J., Sapiro G., <em>Online Dictionary Learning for Sparse Coding</em>, Proceedings of the 29th International Conference on Machine Learning, 2009</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Feature selection is the first (and sometimes the most important) step in a machine learning pipeline. Not all the features are useful for our purposes and some of them are expressed using different notations, so it's often necessary to preprocess our dataset before any further operations. </p>
<p>We saw how to split the data into training and test sets using a random shuffle and how to manage missing elements. Another very important section covered the techniques used to manage categorical data or labels, which are very common when a certain feature assumes only a discrete set of values.</p>
<p>Then we analyzed the problem of dimensionality. Some datasets contain many features which are correlated with each other, so they don't provide any new information but increase the computational complexity and reduce the overall performances. Principal component analysis is a method to select only a subset of features which contain the largest amount of total variance. This approach, together with its variants, allows to decorrelate the features and reduce the dimensionality without a drastic loss in terms of accuracy. Dictionary learning is another technique used to extract a limited number of building blocks from a dataset, together with the information needed to rebuild each sample. This approach is particularly useful when the dataset is made up of different versions of similar elements (such as images, letters, or digits).</p>
<p>In the next chapter, we're going to discuss linear regression, which is the most diffused and simplest supervised approach to predict continuous values. We'll also analyze how to overcome some limitations and how to solve non-linear problems using the same algorithms.</p>


            </article>

            
        </section>
    </body></html>