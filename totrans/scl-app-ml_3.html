<html><head></head><body><div class="part" title="Part&#xA0;III.&#xA0;Module 3" id="aid-6V53K1"><div class="titlepage"><div><div><h1 class="title"><a id="part03"/>Part III. Module 3</h1></div></div></div><div class="partintro" title="Module 3"><div/><div class="blockquote"><blockquote class="blockquote"><p>
<span class="strong"><strong>Mastering Scala Machine Learning</strong></span>
</p><p>
<span class="emphasis"><em>Advance your skills in efficient data analysis and data processing using the powerful tools of Scala, Spark, and Hadoop</em></span>
</p></blockquote></div></div></div>
<div class="chapter" title="Chapter&#xA0;1.&#xA0;Exploratory Data Analysis" id="aid-703K61"><div class="titlepage"><div><div><h1 class="title"><a id="ch27"/>Chapter 1. Exploratory Data Analysis</h1></div></div></div><p>Before I dive into more complex methods to analyze your data later in the book, I would like to stop at basic data exploratory tasks on which almost all data scientists spend at least 80-90% of their productive time. The data preparation, cleansing, transforming, and joining the data alone is estimated to be a $44 billion/year industry alone (<span class="emphasis"><em>Data Preparation in the Big Data Era</em></span> by <span class="emphasis"><em>Federico Castanedo</em></span> and <span class="emphasis"><em>Best Practices for Data Integration</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2015</em></span>). Given this fact, it is surprising that people only recently started spending more time on the science of developing best practices and establishing good habits, documentation, and teaching materials for the whole process of data preparation (<span class="emphasis"><em>Beautiful Data: The Stories Behind Elegant Data Solutions</em></span>, edited by <span class="emphasis"><em>Toby Segaran</em></span> and <span class="emphasis"><em>Jeff Hammerbacher</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2009</em></span> and <span class="emphasis"><em>Advanced Analytics with Spark: Patterns for Learning from Data at Scale</em></span> by <span class="emphasis"><em>Sandy Ryza et al.</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>, <span class="emphasis"><em>2015</em></span>).</p><p>Few data scientists would agree on specific tools and techniques—and there are multiple ways to perform the exploratory data analysis, ranging from Unix command line to using very popular open source and commercial ETL and visualization tools. The focus of this chapter is how to use Scala and a laptop-based environment to benefit from techniques that are commonly referred as a functional paradigm of programming. As I will discuss, these techniques can be transferred to exploratory analysis over distributed system of machines using Hadoop/Spark.</p><p>What has functional programming to do with it? Spark was developed in Scala for a good reason. Many basic principles that lie at the foundation of functional programming, such as lazy evaluation, immutability, absence of side effects, list comprehensions, and monads go really well with processing data in distributed environments, specifically, when performing the data preparation and transformation tasks on big data. Thanks to abstractions, these techniques work well on a local workstation or a laptop. As mentioned earlier, this does not preclude us from processing very large datasets up to dozens of TBs on modern laptops connected to distributed clusters of storage/processing nodes. We can do it one topic or focus area at the time, but often we even do not have to sample or filter the dataset with proper partitioning. We will use Scala as our primary tool, but will resort to other tools if required.</p><p>While Scala is complete in the sense that everything that can be implemented in other languages can be implemented in Scala, Scala is fundamentally a high-level, or even a scripting, language. One does not have to deal with low-level details of data structures and algorithm implementations that in their majority have already been tested by a plethora of applications and time, in, say, Java or C++—even though Scala has its own collections and even some basic algorithm implementations today. Specifically, in this chapter, I'll be focusing on using Scala/Spark only for high-level tasks.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Installing Scala</li><li class="listitem">Learning simple techniques for initial data exploration</li><li class="listitem">Learning how to downsample the original dataset for faster turnover</li><li class="listitem">Discussing the implementation of basic data transformation and aggregations in Scala</li><li class="listitem">Getting familiar with big data processing tools such as Spark and Spark Notebook</li><li class="listitem">Getting code for some basic visualization of datasets</li></ul></div><div class="section" title="Getting started with Scala"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec0800000"/>Getting started with Scala</h1></div></div></div><p>If you have already installed Scala, you can skip this paragraph. One can get the latest Scala download from <a class="ulink" href="http://www.scala-lang.org/download/">http://www.scala-lang.org/download/</a>. I used Scala version 2.11.7 on<a id="id0000000" class="indexterm"/> Mac OS X El Capitan 10.11.5. You can use any other <a id="id10000001" class="indexterm"/>version you like, but you might face some compatibility problems with other packages such as Spark, a common problem in open source software as the technology adoption usually lags by a few released versions.</p><div class="note" title="Note"><h3 class="title"><a id="tip02000000"/>Tip</h3><p>In most cases, you should try to maintain precise match between the recommended versions as difference in versions can lead to obscure errors and a lengthy debugging process.</p></div><p>If you installed Scala correctly, after typing <code class="literal">scala</code>, you should see something similar to the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt;</strong></span>
</pre></div><p>This is a Scala <span class="strong"><strong>read-evaluate-print-loop</strong></span> (<span class="strong"><strong>REPL</strong></span>) prompt. Although Scala programs can be compiled, the<a id="id20000001" class="indexterm"/> content of this chapter will be in REPL, as <a id="id30000001" class="indexterm"/>we are focusing on interactivity with, maybe, a few exceptions. The <code class="literal">:help</code> command provides a some utility commands available in REPL (note the colon at the start):</p><div class="mediaobject"><img src="../Images/image01626.jpeg" alt="Getting started with Scala"/></div><p style="clear:both; height: 1em;"> </p></div></div>
<div class="section" title="Distinct values of a categorical field" id="aid-7124O1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec0900000"/>Distinct values of a categorical field</h1></div></div></div><p>Now, you have a <a id="id40000001" class="indexterm"/>dataset and a computer. For convenience, I have provided you a small anonymized and obfuscated sample of clickstream data with the book repository that you can get at <a class="ulink" href="https://github.com/alexvk/ml-in-scala.git">https://github.com/alexvk/ml-in-scala.git</a>. The file in the <code class="literal">chapter01/data/clickstream</code> directory contains lines with timestamp, session ID, and some additional event information such as URL, category information, and so on at the time of the call. The first thing one would do is apply transformations to find out the distribution of values for different columns in the dataset.</p><p>
<span class="emphasis"><em>Figure 01-1 shows</em></span> screenshot shows the output of the dataset in the terminal window of the <code class="literal">gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz | less –U</code> command. The columns are tab (<code class="literal">^I</code>) separated. One can notice that, as in many real-world big data datasets, many values are missing. The first column of the dataset is recognizable as the timestamp. The file contains complex data such as arrays, structs, and maps, another feature of big data datasets.</p><p>Unix provides a few tools to dissect the datasets. Probably, <span class="strong"><strong>less</strong></span>, <span class="strong"><strong>cut</strong></span>, <span class="strong"><strong>sort</strong></span>, and <span class="strong"><strong>uniq</strong></span> are the most frequently used tools for text file manipulations. <span class="strong"><strong>Awk</strong></span>, <span class="strong"><strong>sed</strong></span>, <span class="strong"><strong>perl</strong></span>, and <span class="strong"><strong>tr</strong></span> can do more complex transformations and substitutions. Fortunately, Scala allows you to transparently use command-line tools from within Scala REPL, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image01627.jpeg" alt="Distinct values of a categorical field"/><div class="caption"><p>Figure 01-1. The clickstream file as an output of the less -U Unix command</p></div></div><p style="clear:both; height: 1em;"> </p><p>Fortunately, Scala allows you to transparently use command-line tools from within Scala REPL:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ scala</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>scala&gt; import scala.sys.process._</strong></span>
<span class="strong"><strong>import scala.sys.process._</strong></span>
<span class="strong"><strong>scala&gt; val histogram = ( "gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz"  #|  "cut -f 10" #| "sort" #|  "uniq -c" #| "sort -k1nr" ).lineStream</strong></span>
<span class="strong"><strong>histogram: Stream[String] = Stream(7731 http://www.mycompany.com/us/en_us/, ?)</strong></span>
<span class="strong"><strong>scala&gt; histogram take(10) foreach println </strong></span>
<span class="strong"><strong>7731 http://www.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>3843 http://mycompanyplus.mycompany.com/plus/</strong></span>
<span class="strong"><strong>2734 http://store.mycompany.com/us/en_us/?l=shop,men_shoes</strong></span>
<span class="strong"><strong>2400 http://m.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>1750 http://store.mycompany.com/us/en_us/?l=shop,men_mycompanyid</strong></span>
<span class="strong"><strong>1556 http://www.mycompany.com/us/en_us/c/mycompanyid?sitesrc=id_redir</strong></span>
<span class="strong"><strong>1530 http://store.mycompany.com/us/en_us/</strong></span>
<span class="strong"><strong>1393 http://www.mycompany.com/us/en_us/?cp=USNS_KW_0611081618</strong></span>
<span class="strong"><strong>1379 http://m.mycompany.com/us/en_us/?ref=http%3A%2F%2Fwww.mycompany.com%2F</strong></span>
<span class="strong"><strong>1230 http://www.mycompany.com/us/en_us/c/running</strong></span>
</pre></div><p>I used the <code class="literal">scala.sys.process</code> package to call familiar Unix commands from Scala REPL. From the<a id="id50000001" class="indexterm"/> output, we can immediately see the customers of our Webshop are mostly interested in men's shoes and running, and that most visitors are using the referral code, <span class="strong"><strong>KW_0611081618</strong></span>.</p><div class="note" title="Note"><h3 class="title"><a id="tip03000000"/>Tip</h3><p>One may wonder when we start using complex Scala types and algorithms. Just wait, a lot of highly optimized tools were created before Scala and are much more efficient for explorative data analysis. In the initial stage, the biggest bottleneck is usually just the disk I/O and slow interactivity. Later, we will discuss more iterative algorithms, which are usually more memory intensive. Also note that the UNIX pipeline operations can be implicitly parallelized on modern multi-core computer architectures, as they are in Spark (we will show it in the later chapters).</p><p>It has been shown that using compression, implicit or explicit, on input data files can actually save you the I/O time. This is particularly true for (most) modern semi-structured datasets with repetitive values and sparse content. Decompression can also be implicitly parallelized on modern fast multi-core computer architectures, removing the computational bottleneck, except, maybe in cases where compression is implemented implicitly in hardware (SSD, where we don't need to compress the files explicitly). We also recommend using directories<a id="id60000001" class="indexterm"/> rather than files as a paradigm for the dataset, where the insert operation is reduced to dropping the data file into a directory. This is how the datasets are presented in big data Hadoop tools such as Hive and Impala.</p></div></div>
<div class="section" title="Summarization of a numeric field" id="aid-720LA1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec1000000"/>Summarization of a numeric field</h1></div></div></div><p>Let's look at <a id="id70000001" class="indexterm"/>the numeric data, even though most of the columns in the dataset are either categorical or complex. The traditional way to summarize the numeric data is a five-number-summary, which is a representation of the median or mean, interquartile range, and minimum and maximum. I'll leave the computations of the median and interquartile ranges till the Spark DataFrame is introduced, as it makes these computations extremely easy; but we can compute mean, min, and max in Scala by just applying the corresponding operators:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import scala.sys.process._</strong></span>
<span class="strong"><strong>import scala.sys.process._</strong></span>
<span class="strong"><strong>scala&gt; val nums = ( "gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz"  #|  "cut -f 6" ).lineStream</strong></span>
<span class="strong"><strong>nums: Stream[String] = Stream(0, ?) </strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).min</strong></span>
<span class="strong"><strong>m: Double = 0.0</strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).sum/nums.size</strong></span>
<span class="strong"><strong>m: Double = 3.6883642764024662</strong></span>
<span class="strong"><strong>scala&gt; val m = nums.map(_.toDouble).max</strong></span>
<span class="strong"><strong>m: Double = 33.0</strong></span>
</pre></div><div class="section" title="Grepping across multiple fields"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec0700000"/>Grepping across multiple fields</h2></div></div></div><p>Sometimes one needs to get an idea of how a certain value looks across multiple fields—most common<a id="id80000001" class="indexterm"/> are IP/MAC addresses, dates, and formatted messages. For examples, if I want to see all IP addresses mentioned throughout a file or a document, I need to replace the <code class="literal">cut</code> command in the previous example by <code class="literal">grep -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}</code>, where the <code class="literal">–o</code> option instructs <code class="literal">grep</code> to print only the matching parts—a more precise regex for the IP address should be <code class="literal">grep –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)</code>, but is about 50% slower on my laptop and the original one works in most practical cases. I'll leave it as an excursive to run this command on the sample file provided with the book.</p></div></div>
<div class="section" title="Basic, stratified, and consistent sampling" id="aid-72V5S1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec1100000"/>Basic, stratified, and consistent sampling</h1></div></div></div><p>I've met quite a few data practitioners who scorn sampling. Ideally, if one can process the whole dataset, the model can only improve. In practice, the tradeoff is much more complex. First, one can build more complex models on a sampled set, particularly if the time complexity<a id="id90000001" class="indexterm"/> of the model building is non-linear—and in most<a id="id100000001" class="indexterm"/> situations, if it is at least <span class="emphasis"><em>N* log(N)</em></span>. A faster model building <a id="id110000001" class="indexterm"/>cycle allows you to iterate over models and converge on the best approach faster. In many situations, <span class="emphasis"><em>time to action</em></span> is beating the potential improvements in the prediction accuracy due to a model built on complete dataset.</p><p>Sampling may be combined with appropriate filtering—in many practical situation, focusing on a subproblem at a time leads to better understanding of the whole problem domain. In many cases, this partitioning is at the foundation of the algorithm, like in decision trees, which are considered later. Often the nature of the problem requires you to focus on the subset of original data. For example, a cyber security analysis is often focused around a specific set of IPs rather than the whole network, as it allows to iterate over hypothesis faster. Including the set of all IPs in the network may complicate things initially if not throw the modeling off the right track.</p><p>When dealing with rare events, such as clickthroughs in ADTECH, sampling the positive and negative cases with different probabilities, which is also sometimes called oversampling, often leads to better predictions in short amount of time.</p><p>Fundamentally, sampling is equivalent to just throwing a coin—or calling a random number generator—for each data row. Thus it is very much like a stream filter operation, where the filtering is on an augmented column of random numbers. Let's consider the following example:</p><div class="informalexample"><pre class="programlisting">import scala.util.Random
import util.Properties

val threshold = 0.05

val lines = scala.io.Source.fromFile("chapter01/data/iris/in.txt").getLines
val newLines = lines.filter(_ =&gt;
    Random.nextDouble() &lt;= threshold
)

val w = new java.io.FileWriter(new java.io.File("out.txt"))
newLines.foreach { s =&gt;
    w.write(s + Properties.lineSeparator)
}
w.close</pre></div><p>This is all good, but<a id="id120000001" class="indexterm"/> it has the following disadvantages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The number<a id="id130000001" class="indexterm"/> of lines in the resulting file is not known<a id="id140000001" class="indexterm"/> beforehand—even though on average it should be 5% of the original file</li><li class="listitem">The results of the sampling is non-deterministic—it is hard to rerun this process for either testing or verification</li></ul></div><p>To fix the first point, we'll need to pass a more complex object to the function, as we need to maintain the state during the original list traversal, which makes the original algorithm less functional and parallelizable (this will be discussed later):</p><div class="informalexample"><pre class="programlisting">import scala.reflect.ClassTag
import scala.util.Random
import util.Properties

def reservoirSample[T: ClassTag](input: Iterator[T],k: Int): Array[T] = {
  val reservoir = new Array[T](k)
  // Put the first k elements in the reservoir.
  var i = 0
  while (i &lt; k &amp;&amp; input.hasNext) {
    val item = input.next()
    reservoir(i) = item
    i += 1
  }

  if (i &lt; k) {
    // If input size &lt; k, trim the array size
    reservoir.take(i)
  } else {
    // If input size &gt; k, continue the sampling process.
    while (input.hasNext) {
      val item = input.next
      val replacementIndex = Random.nextInt(i)
      if (replacementIndex &lt; k) {
        reservoir(replacementIndex) = item
      }
      i += 1
    }
    reservoir
  }
}

val numLines=15
val w = new java.io.FileWriter(new java.io.File("out.txt"))
val lines = io.Source.fromFile("chapter01/data/iris/in.txt").getLines
reservoirSample(lines, numLines).foreach { s =&gt;
    w.write(s + scala.util.Properties.lineSeparator)
}
w.close</pre></div><p>This will <a id="id150000001" class="indexterm"/>output <code class="literal">numLines</code> lines. Similarly to reservoir <a id="id16000000" class="indexterm"/>sampling, stratified sampling is guaranteed to provide the same ratios <a id="id17000000" class="indexterm"/>of input/output rows for all strata defined by levels of another attribute. We can achieve this by splitting the original dataset into <span class="emphasis"><em>N</em></span> subsets corresponding to the levels, performing the reservoir sampling, and merging the results afterwards. However, MLlib library, which will be covered in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>,<span class="emphasis"><em> Working with Spark and MLlib</em></span>, already has stratified sampling implementation:</p><div class="informalexample"><pre class="programlisting">val origLinesRdd = sc.textFile("file://...")
val keyedRdd = origLines.keyBy(r =&gt; r.split(",")(0))
val fractions = keyedRdd.countByKey.keys.map(r =&gt; (r, 0.1)).toMap
val sampledWithKey = keyedRdd.sampleByKeyExact(fractions)
val sampled = sampledWithKey.map(_._2).collect</pre></div><p>The other bullet point is more subtle; sometimes we want a consistent subset of values across multiple datasets, either for reproducibility or to join with another sampled dataset. In general, if we sample two datasets, the results will contain random subsets of IDs which might have very little or no intersection. The cryptographic hashing functions come to the help here. The result of applying a hash function such as MD5 or SHA1 is a sequence of bits that is statistically uncorrelated, at least in theory. We will use the <code class="literal">MurmurHash</code> <a id="id18000000" class="indexterm"/>function, which is part of the <code class="literal">scala.util.hashing</code> package:</p><div class="informalexample"><pre class="programlisting">import scala.util.hashing.MurmurHash3._

val markLow = 0
val markHigh = 4096
val seed = 12345

def consistentFilter(s: String): Boolean = {
  val hash = stringHash(s.split(" ")(0), seed) &gt;&gt;&gt; 16
  hash &gt;= markLow &amp;&amp; hash &lt; markHigh
}

val w = new java.io.FileWriter(new java.io.File("out.txt"))
val lines = io.Source.fromFile("chapter01/data/iris/in.txt").getLines
lines.filter(consistentFilter).foreach { s =&gt;
     w.write(s + Properties.lineSeparator)
}
w.close</pre></div><p>This function is guaranteed to return exactly the same subset of records based on the value of the first field—it is either all records where the first field equals a certain value or none—and will come up with approximately one-sixteenth of the original sample; the range of <code class="literal">hash</code> is <code class="literal">0</code> to <code class="literal">65,535</code>.</p><div class="note" title="Note"><h3 class="title"><a id="note02000000"/>Note</h3><p>MurmurHash? It is not a cryptographic hash!</p><p>Unlike<a id="id19000000" class="indexterm"/> cryptographic hash functions, such as MD5 and <a id="id20000000" class="indexterm"/>SHA1, MurmurHash is not specifically <a id="id21000000" class="indexterm"/>designed to be hard to find an inverse of a hash. It is, however, really fast and efficient. This is what really matters in our use case.</p></div></div>
<div class="section" title="Working with Scala and Spark Notebooks"><div class="titlepage" id="aid-73TME2"><div><div><h1 class="title"><a id="ch01lvl1sec1200000"/>Working with Scala and Spark Notebooks</h1></div></div></div><p>Often the most frequent values or five-number summary are not sufficient to get the first understanding of the data. The<a id="id22000000" class="indexterm"/> term <span class="strong"><strong>descriptive statistics</strong></span> is very generic and may refer to very complex ways to describe the data. Quantiles, a <a id="id23000000" class="indexterm"/>
<span class="strong"><strong>Paretto</strong></span> chart or, when more than one attribute is analyzed, correlations are also examples of descriptive statistics. When sharing all these ways to look at the<a id="id24000000" class="indexterm"/> data aggregates, in many cases, it is also <a id="id25000000" class="indexterm"/>important to share the specific computations to get to them.</p><p>Scala or <a id="id26000000" class="indexterm"/>Spark Notebook <a class="ulink" href="https://github.com/Bridgewater/scala-notebook">https://github.com/Bridgewater/scala-notebook</a>, <a class="ulink" href="https://github.com/andypetrella/spark-notebook">https://github.com/andypetrella/spark-notebook</a> record the whole transformation path and the results can be shared as a JSON-based <code class="literal">*.snb</code> file. The Spark Notebook project can be downloaded from <a class="ulink" href="http://spark-notebook.io">http://spark-notebook.io</a>, and I will provide a sample <code class="literal">Chapter01.snb</code> file with the book. I will use Spark, which I will cover in more detail in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>.</p><p>For this particular example, Spark will run in the local mode. Even in the local mode Spark can utilize parallelism on your workstation, but it is limited to the number of cores and hyperthreads that can run on your laptop or workstation. With a simple configuration change, however, Spark can be pointed to a distributed set of machines and use resources across a distributed <a id="id27000000" class="indexterm"/>set of nodes.</p><p>Here is the set of <a id="id28000000" class="indexterm"/>commands to download the Spark Notebook and copy the necessary files from the code repository:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ wget http://s3.eu-central-1.amazonaws.com/spark-notebook/zip/spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ unzip -d ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ ln -sf ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/notebook/Chapter01.snb ~/spark-notebook/notebooks</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/ data/kddcup/kddcup.parquet ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ cd ~/spark-notebook</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ bin/spark-notebook </strong></span>
<span class="strong"><strong>Play server process ID is 2703</strong></span>
<span class="strong"><strong>16/04/14 10:43:35 INFO play: Application started (Prod)</strong></span>
<span class="strong"><strong>16/04/14 10:43:35 INFO play: Listening for HTTP on /0:0:0:0:0:0:0:0:9000</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now you can open the notebook at <code class="literal">http://localhost:9000</code> in your browser, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image01628.jpeg" alt="Working with Scala and Spark Notebooks"/><div class="caption"><p>Figure 01-2. The first page of the Spark Notebook with the list of notebooks</p></div></div><p style="clear:both; height: 1em;"> </p><p>Open <a id="id29000000" class="indexterm"/>the <code class="literal">Chapter01</code> notebook by clicking on it. The statements are<a id="id30000000" class="indexterm"/> organized into cells and can be executed by clicking on the small right arrow at the top, as shown in the following screenshot, or run all cells at once by navigating to <span class="strong"><strong>Cell</strong></span> | <span class="strong"><strong>Run All</strong></span>:</p><div class="mediaobject"><img src="../Images/image01629.jpeg" alt="Working with Scala and Spark Notebooks"/><div class="caption"><p>Figure 01-3. Executing the first few cells in the notebook</p></div></div><p style="clear:both; height: 1em;"> </p><p>First, we will look at the values of all or some of discrete variables. For example, to get the distribution of the labels, issue the following code:</p><div class="informalexample"><pre class="programlisting">val labelCount = df.groupBy("lbl").count().collect
labelCount.toList.map(row =&gt; (row.getString(0), row.getLong(1)))</pre></div><p>The first time<a id="id31000000" class="indexterm"/> I read the dataset, it took about a minute on <a id="id32000000" class="indexterm"/>MacBook Pro, but Spark caches the data in memory and the subsequent aggregation runs take only about a second. Spark Notebook provides you the distribution of the values, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image01630.jpeg" alt="Working with Scala and Spark Notebooks"/><div class="caption"><p>Figure 01-4. Computing the distribution of values for a categorical field</p></div></div><p style="clear:both; height: 1em;"> </p><p>I can also look at crosstab counts for pairs of discrete variables, which gives me an idea of interdependencies <a id="id33000000" class="indexterm"/>between the variables using <a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions</a>—the object does not support computing correlation <a id="id34000000" class="indexterm"/>measures<a id="id35000000" class="indexterm"/> such as chi-square yet:</p><div class="mediaobject"><img src="../Images/image01631.jpeg" alt="Working with Scala and Spark Notebooks"/><div class="caption"><p>Figure 01-5. Contingency table or crosstab</p></div></div><p style="clear:both; height: 1em;"> </p><p>However, we can see that the most popular service is private and it correlates well with the <code class="literal">SF</code> flag. Another way to analyze dependencies is to look at <code class="literal">0</code> entries. For example, the <code class="literal">S2</code> and <code class="literal">S3</code> flags are clearly related to the SMTP and FTP traffic since all other entries are <code class="literal">0</code>.</p><p>Of course, the <a id="id36000000" class="indexterm"/>most interesting correlations are with the target<a id="id37000000" class="indexterm"/> variable, but these are better discovered by supervised learning algorithms that I will cover in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib,</em></span> and <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>.</p><div class="mediaobject"><img src="../Images/image01632.jpeg" alt="Working with Scala and Spark Notebooks"/><div class="caption"><p>Figure 01-6. Computing simple aggregations using org.apache.spark.sql.DataFrameStatFunctions.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Analogously, we can compute correlations for numerical variables with the <code class="literal">dataFrame.stat.corr()</code> and <code class="literal">dataFrame.stat.cov()</code> functions (refer to <span class="emphasis"><em>Figure 01-6)</em></span>. In this case, the class supports the <span class="strong"><strong>Pearson correlation coefficient</strong></span>. Alternatively, we can use the standard <a id="id38000000" class="indexterm"/>SQL syntax on the parquet file directly:</p><div class="informalexample"><pre class="programlisting">sqlContext.sql("SELECT lbl, protocol_type, min(duration), avg(duration), stddev(duration), max(duration) FROM parquet.`kddcup.parquet` group by lbl, protocol_type")</pre></div><p>Finally, I promised you to compute percentiles. Computing percentiles usually involves sorting the whole dataset, which is expensive; however, if the tile is one of the first or the last ones, usually it is possible to optimize the computation:</p><div class="informalexample"><pre class="programlisting">val pct = sqlContext.sql("SELECT duration FROM parquet.`kddcup.parquet` where protocol_type = 'udp'").rdd.map(_.getLong(0)).cache
pct.top((0.05*pct.count).toInt).last</pre></div><p>Computing<a id="id39000000" class="indexterm"/> the exact percentiles for a more generic case is more <a id="id40000000" class="indexterm"/>computationally expensive and is provided as a part of the Spark Notebook example code.</p></div>
<div class="section" title="Basic correlations" id="aid-74S701"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec1300000"/>Basic correlations</h1></div></div></div><p>You probably <a id="id41000000" class="indexterm"/>noticed that detecting correlations from contingency tables is hard. Detecting patterns takes practice, but many people are much better at recognizing the patterns visually. Detecting actionable patterns is one of the primary goals of machine learning. While advanced supervised machine learning techniques that will be covered in <a class="link" title="Chapter 4. Supervised and Unsupervised Learning" href="part0256.xhtml#aid-7K4G02">Chapter 4</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning</em></span> and <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span> exist, initial analysis of interdependencies between variables can help with the right transformation of variables or selection of the best inference technique.</p><p>Multiple well-established visualization tools exist and there are multiple sites, such as <a class="ulink" href="http://www.kdnuggets.com">http://www.kdnuggets.com</a>, which specialize on ranking and providing recommendations on data analysis, data explorations, and visualization software. I am not going to question the validity and accuracy of such rankings in this book, and very few sites actually mention Scala as a specific way to visualize the data, even if this is possible with, say, a <code class="literal">D3.js</code> package. A good visualization is a great way to deliver your findings to a larger audience. One look is worth a thousand words.</p><p>For the purposes <a id="id42000000" class="indexterm"/>of this chapter, I will use <span class="strong"><strong>Grapher</strong></span> that is present on every Mac OS notebook. To open <span class="strong"><strong>Grapher</strong></span>, go to Utilities (<span class="emphasis"><em>shift</em></span> + <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>U</em></span> in Finder) and click on the <span class="strong"><strong>Grapher</strong></span> icon (or search by name by pressing <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>space</em></span>). Grapher presents many options, including the following <span class="strong"><strong>Log-Log</strong></span> and <span class="strong"><strong>Polar</strong></span> coordinates:</p><div class="mediaobject"><img src="../Images/image01633.jpeg" alt="Basic correlations"/><div class="caption"><p>Figure 01-7. The Grapher window</p></div></div><p style="clear:both; height: 1em;"> </p><p>Fundamentally, the amount of information that can be delivered through visualization is limited by the number of pixels on the screen, which, for most modern computers, is in millions and color variations, which arguably can also be in millions (<span class="emphasis"><em>Judd</em></span>, <span class="emphasis"><em>Deane B.</em></span>; <span class="emphasis"><em>Wyszecki</em></span>, <span class="emphasis"><em>Günter</em></span> (<span class="emphasis"><em>1975</em></span>). <span class="emphasis"><em>Color in Business, Science and Industry</em></span>. <span class="emphasis"><em>Wiley Series in Pure and Applied Optics (3rd ed.)</em></span>. New York). If I am working on a multidimensional TB dataset, the dataset first needs to be summarized, processed, and reduced to a size that can be viewed on a computer screen.</p><p>For the purpose of illustration, I will use the Iris UCI dataset that can be found at <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>. To bring the dataset into the tool, type the following code (on Mac OS):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ pbcopy &lt; chapter01/data/iris/in.txt</strong></span>
</pre></div><p>Open the new <a id="id43000000" class="indexterm"/>
<span class="strong"><strong>Point Set</strong></span> in the <span class="strong"><strong>Grapher</strong></span> (<span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>alt</em></span> + <span class="emphasis"><em>P</em></span>), press <span class="strong"><strong>Edit Points…</strong></span> and paste the data by pressing <span class="emphasis"><em>command</em></span> + <span class="emphasis"><em>V</em></span>. The tools has line-fitting capabilities with basic linear, polynomial, and exponential families and provides the popular chi-squared metric to estimate the goodness of the fit with respect to the number of free parameters:</p><div class="mediaobject"><img src="../Images/image01634.jpeg" alt="Basic correlations"/><div class="caption"><p>Figure 01-8. Fitting the Iris dataset using Grapher on Mac OS X</p></div></div><p style="clear:both; height: 1em;"> </p><p>We will cover how<a id="id44000000" class="indexterm"/> to estimate the goodness of model fit in the following chapters.</p></div>
<div class="section" title="Summary" id="aid-75QNI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec1400000"/>Summary</h1></div></div></div><p>I've tried to establish a common ground to perform a more complex data science later in the book. Don't expect these to be a complete set of exploratory techniques, as the exploratory techniques can extend to running very complex modes. However, we covered simple aggregations, sampling, file operations such as read and write, working with tools such as notebooks and Spark DataFrames, which brings familiar SQL constructs into the arsenal of an analyst working with Spark/Scala.</p><p>The next chapter will take a completely different turn by looking at the data pipelines as a part of a data-driven enterprise and cover the data discovery process from the business perspective: what are the ultimate goals we are trying to accomplish by doing the data analysis. I will cover a few traditional topics of ML, such as supervised and unsupervised learning, after this before delving into more complex representations of the data, where Scala really shows it's advantage over SQL.</p></div>
<div class="chapter" title="Chapter&#xA0;2.&#xA0;Data Pipelines and Modeling"><div class="titlepage" id="aid-76P842"><div><div><h1 class="title"><a id="ch28"/>Chapter 2. Data Pipelines and Modeling</h1></div></div></div><p>We have looked at basic hands-on tools for exploring the data in the previous chapter, thus we now can delve into more complex topics of statistical model building and optimal control or science-driven tools and problems. I will go ahead and say that we will only touch on some topics in optimal control since this book really is just about ML in Scala and not the theory of data-driven business management, which might be an exciting topic for a book on its own.</p><p>In this chapter, I will stay away from specific implementations in Scala and discuss the problem of building a data-driven enterprise at a high level. Later chapters will address how to solve these smaller pieces of the puzzle. A special emphasis will be given to handing uncertainty. Uncertainty usually comes in several flavors: first, there can be noise in the information we are provided with. Secondly, the information can be incomplete. The system may have some degree of freedom in filling the missing pieces, which results in uncertainty. Finally, there may be variations in the interpretation of the models and the resulting metrics. The final point is subtle, as most classic textbooks assume that we can measure things directly. Not only the measurements may be noisy, but the definition of the measure may change in time—try measuring satisfaction or happiness. Certainly, we can avoid the ambiguity by saying that we can optimize only measurable metrics, as people usually do, but it will significantly limit the application domain in practice. Nothing prevents the scientific machinery from handling the uncertainty in the interpretation into account as well.</p><p>The predictive models are often built just for data understanding. From the linguistic derivation, model is a simplified representation of the actual complex buildings or processes for exactly the purpose of making a point and convincing people, one or another way. The ultimate goal for predictive modeling, the modeling I am concerned about in this book and this chapter specifically, is to optimize the business processes by taking the most important factors into account in order to make the world a better place. This was certainly a sentence with a lot of uncertainty entrenched, but at least it looks like a much better goal than optimizing a click-through rate.</p><p>Let's look at a traditional business decision-making process: a traditional business might involve a set of C-level executives making decisions based on information that is usually obtained from a set of dashboards with graphical representation of the data in one or several DBs. The promise of an automated data-driven business is to be able to automatically make most of the decisions provided the uncertainties eliminating human bias. This is not to say that we no longer need C-level executives, but the C-level executives will be busy helping the machines to make the decisions instead of the other way around.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Going through the basics of influence diagrams as a tool for decision making</li><li class="listitem">Looking at variations of the pure decision making optimization in the context of adaptive <span class="strong"><strong>Markov Decision</strong></span> making process and <span class="strong"><strong>Kelly Criterion</strong></span></li><li class="listitem">Getting familiar with at least three different practical strategies for exploration-exploitation trade-off</li><li class="listitem">Describing the architecture of a data-driven enterprise</li><li class="listitem">Discussing major architectural components of a decision-making pipeline</li><li class="listitem">Getting familiar with standard tools for building data pipelines</li></ul></div><div class="section" title="Influence diagrams"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec1500000"/>Influence diagrams</h1></div></div></div><p>While the <a id="id45000000" class="indexterm"/>decision making process can have multiple facets, a book about decision making under uncertainty would be incomplete without mentioning influence diagrams (<span class="emphasis"><em>Influence Diagrams for Team Decision Analysis</em></span>, Decision Analysis 2 (4): 207–228), which help the analysis and understanding of the decision-making process. The decision may be as mundane as selection of the next news article to show to a user in a personalized environment or a complex one as detecting malware on an enterprise network or selecting the next research project.</p><p>Depending on the<a id="id46000000" class="indexterm"/> weather she can try and go on a boat trip.  We can represent the decision-making process as a diagram. Let's decide whether to take a river boat tour during her stay in Portland, Oregon:</p><div class="mediaobject"><img src="../Images/image01635.jpeg" alt="Influence diagrams"/><div class="caption"><p>Figure 02-1. A simple vacation influence diagram to represent a simple decision-making process. The diagram contains decision nodes such as Vacation Activity, observable and unobservable information nodes such as Weather Forecast and Weather, and finally the value node such as Satisfaction</p></div></div><p style="clear:both; height: 1em;"> </p><p>The preceding<a id="id47000000" class="indexterm"/> diagram represents this situation. The decision whether to participate in the activity is clearly driven by the potential to get certain satisfaction, which is a function of the decision itself and the weather at the time of the activity. While the actual weather conditions are unknown at the time of the trip planning, we believe there is a certain correlation between the weather forecast and the actual weather experienced during the trip, which is represented by the edge between the <span class="strong"><strong>Weather</strong></span> and <span class="strong"><strong>Weather Forecast</strong></span> nodes. The <span class="strong"><strong>Vacation Activity</strong></span> node is the decision node, it has only one parent as the decision is made solely based on <span class="strong"><strong>Weather Forecast</strong></span>. The final node in the DAG is <span class="strong"><strong>Satisfaction</strong></span>, which is a function of the actual whether and the decision we made during the trip planning—obviously, <span class="emphasis"><em>yes + good weather</em></span> and <span class="emphasis"><em>no + bad weather</em></span> are likely to have the highest scores. The <span class="emphasis"><em>yes + bad weather</em></span> and <span class="emphasis"><em>no + good weather</em></span> would be a bad outcome—the latter case is probably just a missed opportunity, but not necessarily a bad decision, provided an inaccurate weather forecast.</p><p>The absence of an edge carries an independence assumption. For example, we believe that <span class="strong"><strong>Satisfaction</strong></span> should not depend on <span class="strong"><strong>Weather Forecast</strong></span>, as the latter becomes irrelevant once we are on the boat. Once the vacation plan is finalized, the actual weather during the boating activity can no longer affect the decision, which was made solely based on the weather forecast; at least in our simplified model, where we exclude the option of buying a trip insurance.</p><p>The graph shows different stages of decision making and the flow of information (we will provide an actual graph implementation in Scala in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>). There is only one piece of information required to make the decision in our simplified diagram: the weather forecast. Once the decision is made, we can no longer change it, even if we have information about the actual weather at the time of the trip. The weather and the decision data can be used to model her satisfaction with the decision she has made.</p><p>Let's map this<a id="id48000000" class="indexterm"/> approach to an advertising problem as an illustration: the ultimate goal is to get user satisfaction with the targeted ads, which results in additional revenue for an advertiser. The satisfaction is the function of user-specific environmental state, which is unknown at the time of decision making. Using machine learning algorithms, however, we can forecast this state based on the user's recent Web visit history and other information that we can gather, such as geolocation, browser-agent string, time of day, category of the ad, and so on (refer to <span class="emphasis"><em>Figure 02-2</em></span>).</p><p>While we are unlikely to measure the level of dopamine in the user's brain, which will certainly fall under the realm of measurable metrics and probably reduce the uncertainty, we can measure the user satisfaction indirectly by the user's actions, either the fact that they responded to the ad or even the measure of time the user spent between the clicks to browse relevant information, which can be used to estimate the effectiveness of our modeling and algorithms. Here is an influence diagram, similar to the one for "vacation", adjusted for the advertising decision-making process:</p><div class="mediaobject"><img src="../Images/image01636.jpeg" alt="Influence diagrams"/><div class="caption"><p>Figure 02-2. The vacation influence diagram adjusted to the online advertising decision-making case. The decisions for online advertising can be made thousand times per second</p></div></div><p style="clear:both; height: 1em;"> </p><p>The actual process <a id="id49000000" class="indexterm"/>might be more complex, representing a chain of decisions, each one depending on a few previous time slices. For example, the so-called <span class="strong"><strong>Markov Chain Decision Process</strong></span>. In this case, the diagram might be repeated<a id="id50000000" class="indexterm"/> over multiple time slices.</p><p>Yet another example might be Enterprise Network Internet malware analytics system. In this case, we try to detect network connections indicative of either <span class="strong"><strong>command and control</strong></span> (<span class="strong"><strong>C2</strong></span>), lateral <a id="id51000000" class="indexterm"/>movement, or data exfiltration based on the analysis of network packets flowing through the enterprise switches. The goal is to minimize the potential impact of an outbreak with minimum impact on the functioning systems.</p><p>One of the decisions we might take is to reimage a subset of nodes or to at least isolate them. The data we collect may contain uncertainty—many benign software packages may send traffic in suspicious ways, and the models need to differentiate between them based on the risk and potential impact. One of the decisions in this specific case may be to collect additional information.</p><p>I will leave it to the reader to map this and other potential business cases to the corresponding diagram as an exercise. Let's consider a more complex optimization problem now.</p></div></div>
<div class="section" title="Sequential trials and dealing with risk"><div class="titlepage" id="aid-77NOM2"><div><div><h1 class="title"><a id="ch02lvl1sec1600000"/>Sequential trials and dealing with risk</h1></div></div></div><p>What if my <a id="id52000000" class="indexterm"/>preferences for making an extra few dollars outweigh <a id="id53000000" class="indexterm"/>the risk of losing the same amount? I will stop on why one's preferences might be asymmetric in a little while in this section, and there is scientific evidence that this asymmetry is ingrained in our minds for evolutionary reasons, but you are right, I have to optimize the expected value of the asymmetric function of the parameterized utility now, as follows:</p><div class="mediaobject"><img src="../Images/image01637.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>Why would an asymmetric function surface in the analysis? One example is repeated bets or re-investments, also known as the Kelly Criterion problem. Although originally, the Kelly Criterion<a id="id54000000" class="indexterm"/> was developed for a specific case of binary outcome as in a gambling machine and the optimization of the fraction of money to bet in each round (<span class="emphasis"><em>A New Interpretation of Information Rate</em></span>, Bell System Technical Journal 35 (4): 917–926, 1956), a more generic formulation as an re-investment problem involves a probabilistic distribution of possible returns.</p><p>The return over multiple bets is a product of individual return rates on each of the bets—the return rate is the ratio between the bankroll after the bet to the original bankroll before each individual bet, as follows:</p><div class="mediaobject"><img src="../Images/image01638.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>This does not help us much to optimize the total return as we don't know how to optimize the product of <span class="emphasis"><em>i.i.d</em></span>. random variables. However, we can convert the product to a sum using log<a id="id55000000" class="indexterm"/> transformation and apply the <span class="strong"><strong>central limit theorem</strong></span> (<span class="strong"><strong>CLT</strong></span>) to approximate the sum of <span class="emphasis"><em>i.i.d</em></span>. variables (provided that the distribution of <span class="emphasis"><em>r</em></span>
<span class="emphasis"><em><sub>i</sub></em></span> is subect to CLT conditions, for example, has a finite mean and variance), as follows:</p><div class="mediaobject"><img src="../Images/image01639.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>Thus, the cumulative result of making <span class="emphasis"><em>N</em></span> bets would look like the result of making <span class="emphasis"><em>N</em></span> bets with expected return of <span class="inlinemediaobject"><img src="../Images/image01640.jpeg" alt="Sequential trials and dealing with risk"/></span>, and not <span class="inlinemediaobject"><img src="../Images/image01641.jpeg" alt="Sequential trials and dealing with risk"/></span>
</p><p>As I mentioned <a id="id56000000" class="indexterm"/>before, the problem is most often applied for the case <a id="id57000000" class="indexterm"/>of binary bidding, although it can be easily generalized, in which case there is an additional parameter: <span class="emphasis"><em>x</em></span>, the amount of money to bid in each round. Let's say I make a profit of <span class="emphasis"><em>W</em></span> with probability <span class="emphasis"><em>p</em></span> or completely lose my bet otherwise with the probability <span class="emphasis"><em>(1-p)</em></span>. Optimizing the expected return with respect to the following additional parameter:</p><div class="mediaobject"><img src="../Images/image01642.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01643.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01644.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>The last equation is the Kelly Criterion ratio and gives you the optimal amount to bet.</p><p>The reason that one might bet less than the total amount is that even if the average return is positive, there is still a possibility to lose the whole bankroll, particularly, in highly skewed situations. For example, even if the probability of making <span class="emphasis"><em>10 x</em></span> on your bet is <span class="emphasis"><em>0.105</em></span> (<span class="emphasis"><em>W = 10</em></span>, the expected return is <span class="emphasis"><em>5%)</em></span>, the combinatorial analysis show that even after <span class="emphasis"><em>60</em></span> bets, there is roughly a <span class="emphasis"><em>50%</em></span> chance that the overall return will be negative, and there is an <span class="emphasis"><em>11%</em></span> chance, in particular, of losing <span class="emphasis"><em>(57 - 10 x 3) = 27</em></span> times your bet or more:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.27 </strong></span>

<span class="strong"><strong>scala&gt; def logFactorial(n: Int) = { (1 to n).map(Math.log(_)).sum }</strong></span>
<span class="strong"><strong>logFactorial: (n: Int)Double</strong></span>

<span class="strong"><strong>scala&gt; def cmnp(m: Int, n: Int, p: Double) = {</strong></span>
<span class="strong"><strong>     |   Math.exp(logFactorial(n) -</strong></span>
<span class="strong"><strong>     |   logFactorial(m) +</strong></span>
<span class="strong"><strong>     |   m*Math.log(p) -</strong></span>
<span class="strong"><strong>     |   logFactorial(n-m) +</strong></span>
<span class="strong"><strong>     |   (n-m)*Math.log(1-p))</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>cmnp: (m: Int, n: Int, p: Double)Double</strong></span>

<span class="strong"><strong>scala&gt; val p = 0.105</strong></span>
<span class="strong"><strong>p: Double = 0.105</strong></span>

<span class="strong"><strong>scala&gt; val n = 60</strong></span>
<span class="strong"><strong>n: Int = 60</strong></span>

<span class="strong"><strong>scala&gt; var cumulative = 0.0</strong></span>
<span class="strong"><strong>cumulative: Double = 0.0</strong></span>

<span class="strong"><strong>scala&gt; for(i &lt;- 0 to 14) {</strong></span>
<span class="strong"><strong>     |   val prob = cmnp(i,n,p)</strong></span>
<span class="strong"><strong>     |   cumulative += prob</strong></span>
<span class="strong"><strong>     |   println(f"We expect $i wins with $prob%.6f probability $cumulative%.3f cumulative (n = $n, p = $p).")</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>We expect 0 wins with 0.001286 probability 0.001 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 1 wins with 0.009055 probability 0.010 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 2 wins with 0.031339 probability 0.042 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 3 wins with 0.071082 probability 0.113 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 4 wins with 0.118834 probability 0.232 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 5 wins with 0.156144 probability 0.388 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 6 wins with 0.167921 probability 0.556 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 7 wins with 0.151973 probability 0.708 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 8 wins with 0.118119 probability 0.826 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 9 wins with 0.080065 probability 0.906 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 10 wins with 0.047905 probability 0.954 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 11 wins with 0.025546 probability 0.979 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 12 wins with 0.012238 probability 0.992 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 13 wins with 0.005301 probability 0.997 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 14 wins with 0.002088 probability 0.999 cumulative (n = 60, p = 0.105).</strong></span>
</pre></div><p>Note that to recover the <span class="emphasis"><em>27 x</em></span> amount, one would need to play only <span class="inlinemediaobject"><img src="../Images/image01645.jpeg" alt="Sequential trials and dealing with risk"/></span> additional rounds on average with these favourable odds, but one must have<a id="id58000000" class="indexterm"/> something to bet to start with. The Kelly Criterion <a id="id59000000" class="indexterm"/>provides that the optimal is to bet only <span class="emphasis"><em>1.55%</em></span> of our bankroll. Note that if I bet the whole bankroll, I would lose all my money with 89.5% certainty in the first round (the probability of a win is only <span class="emphasis"><em>0.105</em></span>). If I bet only a fraction of the bankroll, the chances of staying in the game are infinitely better, but the overall returns are smaller. The plot of expected log of return is shown in <span class="emphasis"><em>Figure 02-3</em></span> as a function of the portions of the bankroll to bet, <span class="emphasis"><em>x</em></span>, and possible distribution of outcomes in 60 bets that I just computed. In 24% of the games we'll do worse than the lower curve, in 39% worse than the next curve, in about half—44%—a gambler we'll do the same or better than the black curve in the middle, and in 30% of cases better than the top one. The optimal Kelly Criterion value for <span class="emphasis"><em>x</em></span> is <span class="emphasis"><em>0.0155</em></span>, which will eventually optimize the overall return over infinitely many rounds:</p><div class="mediaobject"><img src="../Images/image01646.jpeg" alt="Sequential trials and dealing with risk"/><div class="caption"><p>Figure 02-3. The expected log of return as a function of the bet amount and possible outcomes in 60 rounds (see equation (2.2))</p></div></div><p style="clear:both; height: 1em;"> </p><p>The Kelly Criterion has been criticized for being both too aggressive (gamblers tend to overestimate their winning potential/ratio and underestimate the probability of a ruin), as well as for being too conservative (the value at risk should be the total available capital, not just the bankroll), but it demonstrates one of the examples where we need to compensate our<a id="id60000000" class="indexterm"/> intuitive understanding of the "benefit" with <a id="id61000000" class="indexterm"/>some additional transformations.</p><p>From the financial point of view, the Kelly Criterion is a much better description of risk than the standard definition as volatility or variance of the returns. For a generic parametrized payoff distribution, <span class="emphasis"><em>y(z)</em></span>, with a probability distribution function, <span class="emphasis"><em>f(z)</em></span>, the equation (2.3) can be reformulated as follows. after the substitution <span class="emphasis"><em>r(x) = 1 + x y(z)</em></span>, where <span class="emphasis"><em>x</em></span> is still the amount to bet:</p><div class="mediaobject"><img src="../Images/image01647.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01648.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>It can also be written in the following manner in the discrete case:</p><div class="mediaobject"><img src="../Images/image01649.jpeg" alt="Sequential trials and dealing with risk"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the denominator emphasizes the contributions from the regions with negative payoffs. Specifically, the possibility of losing all your bankroll is exactly where the denominator <span class="inlinemediaobject"><img src="../Images/image01650.jpeg" alt="Sequential trials and dealing with risk"/></span> is zero.</p><p>As I mentioned before, interestingly, risk aversion is engrained in our intuitions and there seems to be a natural risk-aversion system of preferences encoded in both humans and primates (<span class="emphasis"><em>A Monkey Economy as Irrational as Ours</em></span> by Laurie Santos, TED talk, 2010). Now enough about monkeys and risk, let's get into another rather controversial subject—the <a id="id62000000" class="indexterm"/>exploration-exploitation trade-off, where one might not <a id="id63000000" class="indexterm"/>even know the payoff trade-offs initially.</p></div>
<div class="section" title="Exploration and exploitation" id="aid-78M981"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec1700000"/>Exploration and exploitation</h1></div></div></div><p>The exploration-exploitation trade-off is another problem that has its apparent origin within<a id="id64000000" class="indexterm"/> gambling, even though the real applications range from allocation of funding to research projects to self-driving cars. The traditional formulation is a multi-armed bandit problem, which refers to an imaginary slot machine with one or more arms. Sequential plays of each arm generate <span class="emphasis"><em>i.i.d</em></span>
<code class="literal">.</code> returns with unknown probabilities for each arm; the successive plays are independent in the simplified models. The rewards are assumed to be independent across the arms. The goal is to maximize the reward—for example, the amount of money won, and to minimize the learning loss, or the amount spend on the arms with less than optimal winning rate, provided an agreed upon arm selection policy. The obvious trade-off is between the <span class="strong"><strong>exploration</strong></span> in search of an arm that produces the best return and <span class="strong"><strong>exploitation</strong></span> of the best-known arm with optimal return:</p><div class="mediaobject"><img src="../Images/image01651.jpeg" alt="Exploration and exploitation"/></div><p style="clear:both; height: 1em;"> </p><p>The <span class="strong"><strong>pseudo-regret</strong></span><a id="id65000000" class="indexterm"/> is then the difference:</p><div class="mediaobject"><img src="../Images/image01652.jpeg" alt="Exploration and exploitation"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01653.jpeg" alt="Exploration and exploitation"/></span> is the <span class="emphasis"><em>i<sup>th</sup></em></span> arm selection out of <span class="emphasis"><em>N</em></span> trials. The multi-armed bandit problem was extensively studied in the 1930s and again during the early 2000s, with the application in finance and ADTECH. While in general, due to stochastic nature of the problem, it is not possible to provide a bound on the expected regret better than the square root of <span class="emphasis"><em>N</em></span>, the pseudo-regret can be controlled so that we are able to bound it by a log of <span class="emphasis"><em>N</em></span> (<span class="emphasis"><em>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</em></span> by Sebastien Bubeck and Nicolo Cesa-Bianchi, <a class="ulink" href="http://arxiv.org/pdf/1204.5721.pdf">http://arxiv.org/pdf/1204.5721.pdf</a>).</p><p>One of the most common strategies used in practice is epsilon strategies, where the optimal arm is chosen with the probability of <span class="inlinemediaobject"><img src="../Images/image01654.jpeg" alt="Exploration and exploitation"/></span> and one of the other arms with the remaining probability. The drawback of this approach is that we might spend a lot of exploration resources on the arms that are never going to provide any rewards. The UCB strategy improves the epsilon strategy by choosing an arm with the largest estimate of the return, plus some multiple or fraction of the standard deviation of the return estimates. The approach<a id="id66000000" class="indexterm"/> needs the recomputation of the best arm to pull at each round and suffers from approximations made to estimate the mean and standard deviation. Besides, UCB requires the recomputation of the estimates for each successive pull, which might be a scalability problem.</p><p>Finally, the Thompson sampling strategy uses a fixed random sample from Beta-Bernoulli posterior estimates and assigns the next arm to the one that gives the minimal expected regret, for which real data can be used to avoid parameter recomputation. Although the specific numbers may depend on the assumptions, one available comparison for these model performances is provided in the following diagram:</p><div class="mediaobject"><img src="../Images/image01655.jpeg" alt="Exploration and exploitation"/><div class="caption"><p>Figure 02-3. The simulation results for different exploration exploitation strategies for K = 5, one-armed bandits, and different strategies.</p></div></div><p style="clear:both; height: 1em;"> </p><p>
<span class="emphasis"><em>Figure 02-3</em></span> shows simulation results for different strategies (taken from the Rich Relevance website at <a class="ulink" href="http://engineering.richrelevance.com/recommendations-thompson-sampling">http://engineering.richrelevance.com/recommendations-thompson-sampling</a>). The <span class="strong"><strong>Random</strong></span> strategy just allocates the arms at random and corresponds to pure exploration. The <span class="strong"><strong>Naive</strong></span> strategy is random up to a certain threshold and than switches to pure Exxploitation mode. <span class="strong"><strong>Upper Confidence Bound</strong></span> (<span class="strong"><strong>UCB</strong></span>) with 95% confidence level. UCB1 is a modification of UCB to take into account the log-normality of the distributions. Finally the Thompson sampling strategy makes a random sample from actual posterior distribution to optimize the regret.</p><p>Exploration/exploitation models are known to be very sensitive to the initial conditions and outliers, particularly on the low-response side. One can spend enormous amount of trials on the arms that are essentially dead.</p><p>Other<a id="id67000000" class="indexterm"/> improvements on the strategies are possible by estimating better priors based on additional information, such as location, or limiting the set of arms to explore—<span class="emphasis"><em>K</em></span>—due to such additional information, but these aspects are more domain-specific (such as personalization or online advertising).</p></div>
<div class="section" title="Unknown unknowns" id="aid-79KPQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec1800000"/>Unknown unknowns</h1></div></div></div><p>Unknown unknowns<a id="id68000000" class="indexterm"/> have been largely made famous due to a phrase from a response the United States Secretary of Defense, Donald Rumsfeld, gave to a question at a United States <span class="strong"><strong>Department of Defense</strong></span> (<span class="strong"><strong>DoD</strong></span>) news briefing on February 12, 2002 about the lack of evidence linking the government of Iraq with the supply of weapons of mass destruction to terrorist groups, and books by Nassim Taleb (<span class="emphasis"><em>The Black Swan: The Impact of the Highly Improbable</em></span> by Nassim Taleb, Random House, 2007).</p><div class="note" title="Note"><h3 class="title"><a id="note03000000"/>Note</h3><p>
<span class="strong"><strong>Turkey paradox</strong></span>
</p><p>Arguably, the <a id="id69000000" class="indexterm"/>unknown unknown is better explained by the turkey paradox. Suppose you have a family of turkeys playing in the backyard and enjoying protection and free food. Across the fence, there is another family of turkeys. This all works day after day, and month after month, until Thanksgiving comes—Thanksgiving Day is a national holiday celebrated in Canada and the United States, where it's customary to roast the turkeys in an oven. The turkeys are very likely to be harvested and consumed at this point, although from the turkey's point of view, there is no discernable signal that anything will happen on the second Monday of October in Canada and the fourth Thursday of November in the United States. No amount of modeling on the within-the-year data can fix this prediction problem from the turkey's point of view besides the additional year-over-year information.</p></div><p>The unknown unknown is something that is not in the model and cannot be anticipated to be in the model. In reality, the only unknown unknowns that are of interest are the ones that affect the model so significantly that the results that were previously virtually impossible, or possible with infinitesimal probability, now become the reality. Given that most of the practical distributions are from exponential family with really thin tails, the deviation from normal does not have to be more than a few sigmas to have devastating results on the standard model assumptions. While one has still to come up with an actionable strategy of <a id="id70000000" class="indexterm"/>how to include the unknown factors in the model—a few ways have been proposed, including fractals, but few if any are actionable—the practitioners have to be aware of the risks, and here the definition of the risk is exactly the possibility of delivering the models useless. Of course, the difference between the known unknown and unknown unknown is exactly that we understand the risks and what needs to be explored.</p><p>As we looked at the basic scope of problems that the decision-making systems are facing, let's look at the data pipelines, the software systems that provide information for making the decisions, and more practical aspects of designing the data pipeline for a data-driven system.</p></div>
<div class="section" title="Basic components of a data-driven system"><div class="titlepage" id="aid-7AJAC2"><div><div><h1 class="title"><a id="ch02lvl1sec1900000"/>Basic components of a data-driven system</h1></div></div></div><p>In short, a data-driven architecture contains the following components—at least all the systems I've seen have them—or can be reduced to these components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Data ingest</strong></span>: We need to collect the data from systems and devices. Most of the <a id="id71000000" class="indexterm"/>systems have logs, or at least an option to <a id="id72000000" class="indexterm"/>write files into a local filesystem. Some can have capabilities to report information to network-based interfaces such as syslog, but the absence of persistence layer usually means potential data loss, if not absence of audit information.</li><li class="listitem"><span class="strong"><strong>Data transformation layer</strong></span>: It was also historically called <span class="strong"><strong>extract, transform, and load</strong></span> (<span class="strong"><strong>ETL</strong></span>). Today the data transformation layer can also be used to<a id="id73000000" class="indexterm"/> have real-time processing, where<a id="id74000000" class="indexterm"/> the aggregates are computed <a id="id75000000" class="indexterm"/>on the most recent data. The data transformation layer is also traditionally used to reformat and index the data to be efficiently accessed by a UI component of algorithms down the pipeline.</li><li class="listitem"><span class="strong"><strong>Data analytics and machine learning engine</strong></span>: The reason this is not part of the standard<a id="id76000000" class="indexterm"/> data transformation layer is<a id="id77000000" class="indexterm"/> usually that this layer requires<a id="id78000000" class="indexterm"/> quite different skills. The mindset of people who<a id="id79000000" class="indexterm"/> build reasonable statistical models is usually different from people who make terabytes of data move fast, even though occasionally I can find people with both skills. Usually, these unicorns are called data scientists, but the skills in any specific field are usually inferior to ones who specialize in a particular field. We need more of <a id="id80000000" class="indexterm"/>either, though. Another reason is that<a id="id81000000" class="indexterm"/> machine learning, and to a certain extent, data <a id="id82000000" class="indexterm"/>analysis, requires multiple<a id="id83000000" class="indexterm"/> aggregations and passes over the same data, which as opposed to a more stream-like ETL transformations, requires a different engine.</li><li class="listitem"><span class="strong"><strong>UI component</strong></span>: Yes, UI stands for user interface, which most often is a set of components<a id="id84000000" class="indexterm"/> that allow you to communicate <a id="id85000000" class="indexterm"/>with the system via a browser (it used to be a native GUI, but these days the web-based JavaScript or Scala-based frameworks are much more powerful and portable). From the data pipeline and modeling perspective, this component offers an API to access internal representation of data and models.</li><li class="listitem"><span class="strong"><strong>Actions engine</strong></span>: This is usually a configurable rules engine to optimize the provided<a id="id86000000" class="indexterm"/> metrics based on insights. The <a id="id87000000" class="indexterm"/>actions may be either real-time, like in online advertising, in which case the engine should be able to supply real-time scoring information, or a recommendation for a user action, which can take the form of an e-mail alert.</li><li class="listitem"><span class="strong"><strong>Correlation engine</strong></span>: This is an emerging component that may analyze the output of data <a id="id88000000" class="indexterm"/>analysis and machine learning <a id="id89000000" class="indexterm"/>engine to infer additional insights into data or model behavior. The actions might also be triggered by an output from this layer.</li><li class="listitem"><span class="strong"><strong>Monitoring</strong></span>: This is a <a id="id90000000" class="indexterm"/>complex system will be<a id="id91000000" class="indexterm"/> incomplete without logging, monitoring, and some way to change system parameters. The purpose of monitoring is to have a nested decision-making system regarding the optimal health of the system and either to mitigate the problem(<span class="emphasis"><em>s</em></span>) automatically or to alert the system administrators about the problem(<span class="emphasis"><em>s</em></span>).</li></ul></div><p>Let's discuss each of the components in detail in the following sections.</p><div class="section" title="Data ingest"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec0800000"/>Data ingest</h2></div></div></div><p>With the <a id="id92000000" class="indexterm"/>proliferation of smart devices, information gathering has become less of a problem and more of a necessity for any business that does more than a type-written text. For the purpose of this chapter, I will assume that the device or devices are connected to the Internet or have some way of passing this information via home dialing or direct network connection.</p><p>The major purpose of this component is to collect all relevant information that can be relevant for further data-driven decision making. The following table provides details on the most common implementations of the data ingest:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Framework</p>
</th><th valign="bottom">
<p>When used</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<span class="strong"><strong>Syslog</strong></span>
</p>
</td><td valign="top">
<p>Syslog is <a id="id93000000" class="indexterm"/>one of the most common standards to pass <a id="id94000000" class="indexterm"/>messages between the machines on Unix. Syslog usually listens on port 514 and the transport protocol can be configured either with UDP (unreliable) or with TCP. The latest enhanced implementation on CentOS and Red Hat Linux is rsyslog, which includes many advanced options such as regex-based filtering that is useful for system-performance tuning and debugging. Apart from slightly inefficient raw message representation—plain text, which might be inefficient for long messages with repeated strings—the syslog system can support tens of thousands of messages per second.</p>
</td><td valign="top">
<p>Syslog is one of the oldest protocols developed in the 1980s by Eric Allman as part of Sendmail. While it does not guarantee delivery or durability, particularly for distributed systems, it is one of the most widespread protocols for message passing. Some of the later frameworks, such as Flume and Kafka, have syslog interfaces as well. </p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Rsync</strong></span>
</p>
</td><td valign="top">
<p>Rsync is a younger framework developed in the 1990s. If the data is <a id="id95000000" class="indexterm"/>put in the flat files on a local filesystem, rsync might be an option. While rsync is more traditionally used to synchronize two directories, it also <a id="id96000000" class="indexterm"/>can be run periodically to transfer log data in batches. Rsync uses a recursive algorithm invented by an Australian computer programmer, Andrew Tridgell, for efficiently detecting the differences and transmitting a structure (such as a file) across a communication link when the receiving computer already has a similar, but not identical, version of the same structure. While it incurs extra communication, it is better from the point of durability, as the original copy can always be retrieved. It is particularly appropriate if the log data is known to arrive in batches in the first place (such as uploads or downloads).</p>
</td><td valign="top">
<p>Rsync <a id="id97000000" class="indexterm"/>has been known to be hampered by network bottlenecks, as it ultimately passes more information over the network when comparing the directory structures. However, the transferred files may be compressed when passed over the network. The network bandwidth can be limited per command-line flags.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Flume</strong></span>
</p>
</td><td valign="top">
<p>Flume is one of the youngest frameworks developed by Cloudera in 2009-2011 and open sourced. Flume—we refer to the more popular flume-ng implementation as Flume as opposed to an older regular Flume—consists of sources, pipes, and sinks that may be <a id="id98000000" class="indexterm"/>configured on multiple nodes for high availability and redundancy purposes. Flume was designed to err on the reliability side at the expense of possible duplication of data. Flume passes the messages in the <span class="strong"><strong>Avro</strong></span> format, which is also open sourced and the transfer protocol, as well as messages can be encoded and compressed.</p>
</td><td valign="top">
<p>While Flume originally was developed just to ship records from a file or a set of files, it can<a id="id99000000" class="indexterm"/> also be configured to listen to a port, or even grab the records from a database. Flume has multiple adapters including the preceding syslog.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Kafka</strong></span>
</p>
</td><td valign="top">
<p>Kafka is the latest addition to the <a id="id100000000" class="indexterm"/>log-processing framework developed by<a id="id101000000" class="indexterm"/> LinkedIn and is open sourced. Kafka, compared to the previous frameworks, is more like a distributed reliable message queue. Kafka keeps a partitioned, potentially between multiple distributed machines; buffer and one can subscribe to or unsubscribe from<a id="id102000000" class="indexterm"/> getting messages for a particular<a id="id103000000" class="indexterm"/> topic. Kafka was built with strong reliability guarantees in mind, which is achieved through replication and consensus protocol.</p>
</td><td valign="top">
<p>Kafka might not be appropriate for small systems (&lt; five nodes) as the benefits of the fully distributed system might be evident only at larger scales. Kafka is commercially supported by Confluent.</p>
</td></tr></tbody></table></div><p>The transfer of information usually occurs in batches, or micro batches if the requirements are close to real time. Usually the information first ends up in a file, traditionally called log, in a device's local filesystem, and then is transferred to a central location. Recently<a id="id104000000" class="indexterm"/> developed Kafka and Flume are often used to manage these transfers, together with a more traditional syslog, rsync, or netcat. Finally, the data can be placed into a local or distributed storage such as HDFS, Cassandra, or Amazon S3.</p></div><div class="section" title="Data transformation layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec0900000"/>Data transformation layer</h2></div></div></div><p>After the data <a id="id105000000" class="indexterm"/>ends up in HDFS or other storage, the data needs to be made available for processing. Traditionally, the data is processed on a schedule and ends up partitioned by time-based buckets. The processing can happen daily or hourly, or even on a sub-minute basis with the new Scala streaming framework, depending on the latency requirements. The processing may involve some preliminary feature construction or vectorization, even though it is traditionally considered a machine-learning task. The following table summarizes some available frameworks:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Framework</p>
</th><th valign="bottom">
<p>When used</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<span class="strong"><strong>Oozie</strong></span>
</p>
</td><td valign="top">
<p>This is<a id="id106000000" class="indexterm"/> one of the oldest open source frameworks developed by Yahoo. This has good integration with big data Hadoop tools. It has limited UI that lists the job history.</p>
</td><td valign="top">
<p>The whole workflow is put into <a id="id107000000" class="indexterm"/>one big XML file, which might be considered a disadvantage from the modularity point of view.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Azkaban</strong></span>
</p>
</td><td valign="top">
<p>This is <a id="id108000000" class="indexterm"/>an alternative open source workflow-scheduling framework developed by LinkedIn. Compared to Oozie, this arguably has a better UI. The <a id="id109000000" class="indexterm"/>disadvantage is that all high-level tasks are executed locally, which might present a scalability problem.</p>
</td><td valign="top">
<p>The idea behind Azkaban is to create<a id="id110000000" class="indexterm"/> a fully modularized drop-in architecture where the new jobs/tasks can be added with as few <a id="id111000000" class="indexterm"/>modifications as possible.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>StreamSets</strong></span>
</p>
</td><td valign="top">
<p>StreamSets is the latest addition build by the former Informix<a id="id112000000" class="indexterm"/> and Cloudera developers. It has a very <a id="id113000000" class="indexterm"/>developed UI and supports a much richer set of input sources and output destinations.</p>
</td><td valign="top">
<p>This is a fully UI-driven tool with an emphasis on data curation, for example, constantly monitoring the data stream for problems and abnormalities.</p>
</td></tr></tbody></table></div><p>Separate attention should be given to stream-processing frameworks, where the latency requirements are reduced to one or a few records at a time. First, stream processing usually requires much more resources dedicated to processing, as it is more expensive to process individual records at a time as opposed to batches of records, even if it is tens or hundreds of records. So, the architect needs to justify the additional costs based on the value of more recent result, which is not always warranted. Second, stream processing requires a few adjustments to the architecture as handling the more recent data becomes a priority; for example, a delta architecture where the more recent data is handled by a separate substream <a id="id114000000" class="indexterm"/>or a <a id="id115000000" class="indexterm"/>set of nodes became very popular recently with systems such as <span class="strong"><strong>Druid</strong></span> (<a class="ulink" href="http://druid.io">http://druid.io</a>).</p></div><div class="section" title="Data analytics and machine learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1000000"/>Data analytics and machine learning</h2></div></div></div><p>For the purpose of this chapter, <span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) is any algorithm that can compute aggregates or summaries <a id="id116000000" class="indexterm"/>that are actionable. We will cover more complex algorithms from <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span> to <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>, but in some cases, a simple sliding-window average and deviation from the average <a id="id117000000" class="indexterm"/>may be sufficient signal for taking an action. In the past few years, it just works in A/B testing somehow became a <a id="id118000000" class="indexterm"/>convincing argument for model building and deployment. I am not speculating that solid scientific principles might or might not apply, but many fundamental assumptions such as <span class="emphasis"><em>i.i.d.</em></span>, balanced designs, and the thinness of the tail just fail to hold for many big data situation. Simpler models tend to be faster and to have better performance and stability.</p><p>For example, in online advertising, one might just track average performance of a set of ads over a certain similar properties over times to make a decision whether to have this ad displayed. The information about anomalies, or deviation from the previous behavior, may signal a new unknown unknown, which signals that the old data no longer applies, in <a id="id119000000" class="indexterm"/>which case, the system has no choice but to start the new <a id="id120000000" class="indexterm"/>exploration cycle.</p><p>I will talk about more complex non-structured, graph, and pattern mining later in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>, <a class="link" title="Chapter 8. Integrating Scala with R and Python" href="part0288.xhtml#aid-8IL202">Chapter 8</a>, <span class="emphasis"><em>Integrating Scala with R and Python</em></span> and <a class="link" title="Chapter 9. NLP in Scala" href="part0291.xhtml#aid-8LGJM2">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p></div><div class="section" title="UI component"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1100000"/>UI component</h2></div></div></div><p>Well, UI is<a id="id121000000" class="indexterm"/> for wimps! Just joking...maybe it's too harsh, but in reality, UI usually presents a syntactic sugar that is necessary to convince the population beyond the data scientists. A good analyst should probably be able to figure out t-test probabilities by just looking at a table with numbers.</p><p>However, one should probably apply the same methodologies we used at the beginning of the chapter, assessing the usefulness of different components and the amount of cycles put into them. The presence of a good UI is often justified, but depends on the target audience.</p><p>First, there are a number of existing UIs and reporting frameworks. Unfortunately, most of them are not aligned with the functional programming methodologies. Also, the presence of complex/semi-structured data, which I will describe in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span> in more detail, presents a new twist that many frameworks are not ready to deal with without implementing some kind of DSL. Here are a few frameworks for building the UI in a Scala project that I find particularly worthwhile:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Framework</p>
</th><th valign="bottom">
<p>When used</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<span class="strong"><strong>Scala Swing</strong></span>
</p>
</td><td valign="top">
<p>If you <a id="id122000000" class="indexterm"/>used Swing<a id="id123000000" class="indexterm"/> components in Java and are proficient with them, Scala Swing is a good choice for you. Swing component is arguably the least portable component of Java, so your mileage can vary on different platforms.</p>
</td><td valign="top">
<p>The <code class="literal">Scala.swing</code> package uses the standard Java Swing library under the hood, but it has some nice additions. Most notably, as it's made for Scala, it can be used in a much more concise way than the standard Swing.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Lift</strong></span>
</p>
</td><td valign="top">
<p>Lift is a <a id="id124000000" class="indexterm"/>secure, developer-centric, scalable, and interactive<a id="id125000000" class="indexterm"/> framework written in Scala. Lift is open sourced under Apache 2.0 license.</p>
</td><td valign="top">
<p>The open source Lift framework was launched in 2007 by David Polak, who was dissatisfied with certain aspects of the Ruby on Rails framework. Any existing Java library and web container can be used in running Lift applications. Lift web applications are thus packaged as WAR files and deployed on any servlet 2.4 engine (for example, Tomcat 5.5.xx, Jetty 6.0, and so on). Lift programmers may use the standard Scala/Java development toolchain, including IDEs such as Eclipse, NetBeans, and IDEA. Dynamic web content is authored via templates using standard HTML5 or XHTML editors. Lift applications also benefit from native support for advanced web development techniques, such as Comet and Ajax.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Play</strong></span>
</p>
</td><td valign="top">
<p>Play is arguably better aligned with <a id="id126000000" class="indexterm"/>Scala as a functional language than any other<a id="id127000000" class="indexterm"/> platform—it is officially supported by Typesafe, the commercial company behind Scala. The Play framework 2.0 builds on Scala, Akka, and sbt to deliver superior asynchronous request handling, fast and reliable. Typesafe templates, and a powerful build system with flexible deployment options. Play is open sourced under Apache 2.0 license.</p>
</td><td valign="top">
<p>The open source Play framework was created in 2007 by Guillaume Bort, who sought to bring a fresh web development experience inspired by modern web frameworks like Ruby on Rails to the long-suffering Java web development community. Play follows a familiar stateless model-view-controller architectural pattern, with a philosophy of convention-over-configuration and an emphasis on developer productivity. Unlike traditional Java web frameworks with their tedious compile-package-deploy-restart cycles, updates to Play applications are instantly visible with a simple browser refresh.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Dropwizard</strong></span>
</p>
</td><td valign="top">
<p>The dropwizard (<a class="ulink" href="http://www.dropwizard.io">www.dropwizard.io</a>) project is an attempt to build a generic RESTful framework in both <a id="id128000000" class="indexterm"/>Java and Scala, even though one might <a id="id129000000" class="indexterm"/>end up using more Java than Scala. What is nice about this framework is that it is flexible enough to be used with arbitrary complex data (including semi-structured).This is licensed under Apache License 2.0.</p>
</td><td valign="top">
<p>RESTful API assumes state, while <a id="id130000000" class="indexterm"/>functional languages shy away from using state. Unless you are flexible enough to deviate from a pure functional approach, this framework is probably not good enough for you.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>Slick</strong></span>
</p>
</td><td valign="top">
<p>While Slick is not a UI component, it is Typesafe's modern <a id="id131000000" class="indexterm"/>database query<a id="id132000000" class="indexterm"/> and access library for Scala, which can serve as a UI backend. It allows you to work with the stored data almost as if you were using Scala collections, while at the same time, giving you full control over when a database access occurs and what data is transferred. You can also use SQL directly. Use it if all of your data is purely relational. This is open sourced under BSD-Style license.</p>
</td><td valign="top">
<p>Slick was started in 2012 by Stefan Zeiger and maintained mainly by Typesafe. It is useful for mostly relational data.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>NodeJS</strong></span>
</p>
</td><td valign="top">
<p>Node.js is a JavaScript runtime, built on Chrome's V8 JavaScript<a id="id133000000" class="indexterm"/> engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js' package ecosystem, npm, is the largest ecosystem of open source libraries<a id="id134000000" class="indexterm"/> in the world. It is open sourced under MIT License.</p>
</td><td valign="top">
<p>Node.js <a id="id135000000" class="indexterm"/>was first introduced in 2009 by Ryan Dahl and other developers working at Joyent. Originally Node.js supported only Linux, but now it runs on OS X<a id="id136000000" class="indexterm"/> and Windows.</p>
</td></tr><tr><td valign="top">
<p>
<span class="strong"><strong>AngularJS</strong></span>
</p>
</td><td valign="top">
<p>AngularJS (<a class="ulink" href="https://angularjs.org">https://angularjs.org</a>) is a <a id="id137000000" class="indexterm"/>frontend development framework, built to simplify development of one-page web applications. This is open sourced under MIT License.</p>
</td><td valign="top">
<p>AngularJS <a id="id138000000" class="indexterm"/>was originally developed in 2009 by <a id="id139000000" class="indexterm"/>Misko Hevery at Brat Tech LLC. AngularJS is mainly maintained by Google and by a community of individual developers and corporations, and thus is specifically for Android platform (support for IE8 is dropped in versions 1.3 and later).</p>
</td></tr></tbody></table></div></div><div class="section" title="Actions engine"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1200000"/>Actions engine</h2></div></div></div><p>While this is <a id="id140000000" class="indexterm"/>the heart of the data-oriented system pipeline, it is also arguably the easiest one. Once the system of metrics and values is known, the system decides, based on the known equations, whether to take a certain set of actions or not, based on the information provided. While the triggers based on a threshold is the most common implementation, the significance of probabilistic approaches that present the user with a set of possibilities and associated probabilities is emerging—or just presenting the user with the top <span class="emphasis"><em>N</em></span> relevant choices like a search engine does.</p><p>The management of the rules might become pretty involved. It used to be that managing the rules with a rule engine, such as <span class="strong"><strong>Drools</strong></span> (<a class="ulink" href="http://www.drools.org">http://www.drools.org</a>), was sufficient. However, managing complex <a id="id141000000" class="indexterm"/>rules becomes an issue that often requires development of a DSL (<span class="emphasis"><em>Domain-Specific Languages</em></span> by Martin Fowler, Addison-Wesley, 2010). Scala is particularly fitting language for the development of such an actions engine.</p></div><div class="section" title="Correlation engine"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1300000"/>Correlation engine</h2></div></div></div><p>The more complex the decision-making system is, the more it requires a secondary decision-making system to optimize its management. DevOps is turning into DataOps (<span class="emphasis"><em>Getting Data Right</em></span> by Michael Stonebraker et al., Tamr, 2015). Data collected about the performance of a <a id="id142000000" class="indexterm"/>data-driven system are used to detect anomalies and semi-automated maintenance.</p><p>Models are often subject to time drift, where the performance might deteriorate either due to the changes in the data collection layers or the behavioral changes in the population (I will cover model drift in <a class="link" title="Chapter 10. Advanced Model Monitoring" href="part0297.xhtml#aid-8R7N21">Chapter 10</a>, <span class="emphasis"><em>Advanced Model Monitoring</em></span>). Another aspect of model management is to track model performance, and in some cases, use "collective" intelligence of the models by various consensus schemes.</p></div><div class="section" title="Monitoring"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1400000"/>Monitoring</h2></div></div></div><p>Monitoring a <a id="id143000000" class="indexterm"/>system involves collecting information about system performance either for audit, diagnostic, or performance-tuning purposes. While it is related to the issues raised in the previous sections, monitoring solution often incorporates diagnostic and historical storage solutions and persistence of critical data, such as a black box on an airplane. In the Java and, thus, Scala world, a popular tool of choice is Java performance beans, which can be monitored in the Java Console. While Java natively supports MBean for exposing JVM information over JMX, <span class="strong"><strong>Kamon</strong></span> (<a class="ulink" href="http://kamon.io">http://kamon.io</a>) is <a id="id144000000" class="indexterm"/>an open source library that uses this mechanism to<a id="id145000000" class="indexterm"/> specifically expose Scala and Akka metrics.</p><p>Some other<a id="id146000000" class="indexterm"/> popular monitoring open source solutions are <span class="strong"><strong>Ganglia</strong></span> (<a class="ulink" href="http://ganglia.sourceforge.net/">http://ganglia.sourceforge.net/</a>) and <a id="id147000000" class="indexterm"/>
<span class="strong"><strong>Graphite</strong></span> (<a class="ulink" href="http://graphite.wikidot.com">http://graphite.wikidot.com</a>).</p><p>I will stop here, as I will address system and model monitoring in more detail in <a class="link" title="Chapter 10. Advanced Model Monitoring" href="part0297.xhtml#aid-8R7N21">Chapter 10</a>, <span class="emphasis"><em>Advanced Model Monitoring</em></span>.</p></div></div>
<div class="section" title="Optimization and interactivity" id="aid-7BHQU1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec2000000"/>Optimization and interactivity</h1></div></div></div><p>While the <a id="id148000000" class="indexterm"/>data collected can be just used for understanding the business, the <a id="id149000000" class="indexterm"/>final goal of any data-driven business is to optimize the business behavior by automatically making data-based and model-based decisions. We want to reduce human intervention to minimum. The following simplified diagram can be depicted as a cycle:</p><div class="mediaobject"><img src="../Images/image01656.jpeg" alt="Optimization and interactivity"/><div class="caption"><p>Figure 02-4. The predictive model life cycle</p></div></div><p style="clear:both; height: 1em;"> </p><p>The cycle is<a id="id150000000" class="indexterm"/> repeated over and over for new information coming into the <a id="id151000000" class="indexterm"/>system. The parameters of the system may be tuned to improve the overall system performance.</p><div class="section" title="Feedback loops"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1500000"/>Feedback loops</h2></div></div></div><p>While humans are still likely to be kept in the loop for most of the systems, last few years saw an emergence of systems that can manage the complete feedback loop on their own—ranging <a id="id152000000" class="indexterm"/>from advertisement systems to self-driving cars.</p><p>The classical formulation of this problem is the optimal control theory, which is also an optimization problem to minimize cost functional, given a set of differential equations describing the system. An optimal control is a set of control policies to minimize the cost functional given constraints. For example, the problem might be to find a way to drive the car to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Another control problem is to maximize profit for showing ads on a website, provided the inventory and time constraints. Most software packages for <a id="id153000000" class="indexterm"/>optimal control are written in other languages such as C or MATLAB (PROPT, SNOPT, RIOTS, DIDO, DIRECT, and GPOPS), but can be interfaced with Scala.</p><p>However, in many cases, the parameters for the optimization or the state transition, or differential equations, are not known with certainty. <span class="strong"><strong>Markov Decision Processes</strong></span> (<span class="strong"><strong>MDPs</strong></span>) provide a mathematical framework to model decision making in situations where outcomes are partly random and partly under the control of the decision maker. In MDPs, we deal with a discrete set of possible states and a set of actions. The "rewards" and state transitions depend both on the state and actions. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning.</p></div></div>
<div class="section" title="Summary" id="aid-7CGBG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec2100000"/>Summary</h1></div></div></div><p>In this chapter, I described a high-level architecture and approach to design a data-driven enterprise. I also introduced you to influence diagrams, a tool for understanding how the decisions are made in traditional and data-driven enterprises. I stopped on a few key models, such as Kelly Criterion and multi-armed bandit, essential to demonstrate the issues from the mathematical point of view. I built on top of this to introduce some Markov decision process approaches where we deal with decision policies based on the results of the previous decisions and observations. I delved into more practical aspects of building a data pipeline for decision-making, describing major components and frameworks that can be used to built them. I also discussed the issues of communicating the data and modeling results between different stages and nodes, presenting the results to the user, feedback loop, and monitoring.</p><p>In the next chapter, I will describe MLlib, a library for machine learning over distributed set of nodes written in Scala.</p></div>
<div class="chapter" title="Chapter&#xA0;3.&#xA0;Working with Spark and MLlib" id="aid-7DES21"><div class="titlepage"><div><div><h1 class="title"><a id="ch29"/>Chapter 3. Working with Spark and MLlib</h1></div></div></div><p>Now that we are powered with the knowledge of where and how statistics and machine learning fits in the global data-driven enterprise architecture, let's stop at the specific implementations in Spark and MLlib, a machine learning library on top of Spark. Spark is a relatively new member of the big data ecosystem that is optimized for memory usage as opposed to disk. The data can be still spilled to disk as necessary, but Spark does the spill only if instructed to do so explicitly, or if the active dataset does not fit into the memory. Spark stores lineage information to recompute the active dataset if a node goes down or the information is erased from memory for some other reason. This is in contrast to the traditional MapReduce approach, where the data is persisted to the disk after each map or reduce task.</p><p>Spark is particularly suited for iterative or statistical machine learning algorithms over a distributed set of nodes and can scale out of core. The only limitation is the total memory and disk space available across all Spark nodes and the network speed. I will cover the basics of Spark architecture and implementation in this chapter.</p><p>One can direct Spark to execute data pipelines either on a single node or across a set of nodes with a simple change in the configuration parameters. Of course, this flexibility comes at a cost of slightly heavier framework and longer setup times, but the framework is very parallelizable and as most of modern laptops are already multithreaded and sufficiently powerful, this usually does not present a big issue.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Installing and configuring Spark if you haven't done so yet</li><li class="listitem">Learning the basics of Spark architecture and why it is inherently tied to the Scala language</li><li class="listitem">Learning why Spark is the next technology after sequential implementations and Hadoop MapReduce</li><li class="listitem">Learning about Spark components</li><li class="listitem">Looking at the simple implementation of word count in Scala and Spark</li><li class="listitem">Looking at the streaming word count implementation</li><li class="listitem">Seeing how to create Spark DataFrames from either a distributed file or a distributed database</li><li class="listitem">Learning about Spark performance tuning</li></ul></div><div class="section" title="Setting up Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec2200000"/>Setting up Spark</h1></div></div></div><p>If you<a id="id154000000" class="indexterm"/> haven't done so yet, you can download the pre-build Spark package <a id="id155000000" class="indexterm"/>from <a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>. The latest release at the time of writing is <span class="strong"><strong>1.6.1</strong></span>:</p><div class="mediaobject"><img src="../Images/image01657.jpeg" alt="Setting up Spark"/><div class="caption"><p>Figure 03-1. The download site at http://spark.apache.org with recommended selections for this chapter</p></div></div><p style="clear:both; height: 1em;"> </p><p>Alternatively, you can build the Spark by downloading the full source distribution from <a class="ulink" href="https://github.com/apache/spark">https://github.com/apache/spark</a>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ git clone https://github.com/apache/spark.git</strong></span>
<span class="strong"><strong>Cloning into 'spark'...</strong></span>
<span class="strong"><strong>remote: Counting objects: 301864, done.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ cd spark</strong></span>
<span class="strong"><strong>$sh ./ dev/change-scala-version.sh 2.11</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$./make-distribution.sh --name alex-build-2.6-yarn --skip-java-test --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-2.11 -Phadoop-2.6</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The command will download the necessary dependencies and create the <code class="literal">spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz</code> file in the Spark directory; the version is 2.0.0, as it is the next release version at the time of writing. In general, you do not want to build from trunk unless you are interested in the latest features. If you want a released version, you can checkout the corresponding tag. Full list of available versions is available via the <code class="literal">git branch –r</code> command. The <code class="literal">spark*.tgz</code> file is all you need to run Spark on <a id="id156000000" class="indexterm"/>any machine that has Java JRE.</p><p>The distribution comes with the <code class="literal">docs/building-spark.md</code> document that describes other options for building Spark and their descriptions, including incremental Scala compiler, zinc. Full Scala 2.11 support is in the works for the next Spark 2.0.0 release.</p></div></div>
<div class="section" title="Understanding Spark architecture"><div class="titlepage" id="aid-7EDCK2"><div><div><h1 class="title"><a id="ch03lvl1sec2300000"/>Understanding Spark architecture</h1></div></div></div><p>A parallel<a id="id157000000" class="indexterm"/> execution involves splitting the workload into subtasks that<a id="id158000000" class="indexterm"/> are executed in different threads or on different nodes. Let's see how Spark does this and how it manages execution and communication between the subtasks.</p><div class="section" title="Task scheduling"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec1600000"/>Task scheduling</h2></div></div></div><p>Spark <a id="id159000000" class="indexterm"/>workload splitting is determined by the number <a id="id160000000" class="indexterm"/>of partitions for <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>), the<a id="id161000000" class="indexterm"/> basic abstraction in Spark, and the pipeline structure. An RDD represents an immutable, partitioned collection of elements that can be operated on in parallel. While the specifics might depend on the mode in which Spark runs, the following diagram captures the Spark task/resource scheduling:</p><div class="mediaobject"><img src="../Images/image01658.jpeg" alt="Task scheduling"/><div class="caption"><p>Figure 03-2. A generic Spark task scheduling diagram. While not shown explicitly in the figure, Spark Context opens an HTTP UI, usually on port 4040 (the concurrent contexts will open 4041, 4042, and so on), which is present during a task execution. Spark Master UI is usually 8080 (although it is changed to 18080 in CDH) and Worker UI is usually 7078. Each node can run multiple executors, and each executor can run multiple tasks.</p></div></div><p style="clear:both; height: 1em;"> </p><div class="note" title="Note"><h3 class="title"><a id="tip04000000"/>Tip</h3><p>You will find that Spark, as well as Hadoop, has a lot of parameters. Some of them are specified as environment variables (refer to the <code class="literal">$SPARK_HOME/conf/spark-env.sh</code> file), and yet some can be given as a command-line parameter. Moreover, some files with pre-defined names can contain parameters that will change the Spark behavior, such as <code class="literal">core-site.xml</code>. This might be confusing, and I will cover as much as possible in this and the following chapters. If you are working with <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), then the <code class="literal">core-site.xml</code> and <code class="literal">hdfs-site.xml</code> files will<a id="id162000000" class="indexterm"/> contain the pointer and specifications for the HDFS master. The requirement for picking this file is that it has to be on <code class="literal">CLASSPATH</code> Java process, which, again, may be set by either specifying <code class="literal">HADOOP_CONF_DIR</code> or <code class="literal">SPARK_CLASSPATH</code> environment variables. As is usual with open source, you need to grep the code sometimes to understand how various parameters work, so having a copy of the source tree on your laptop is a good idea.</p></div><p>Each node<a id="id163000000" class="indexterm"/> in the cluster can run one or more executors, and each<a id="id164000000" class="indexterm"/> executor can schedule a sequence of tasks to perform the Spark operations. Spark driver is responsible for scheduling the execution and works with the cluster scheduler, such as Mesos or YARN to schedule the available resources. Spark driver usually runs on the client machine, but in the latest release, it can also run in the cluster under the cluster manager. YARN and Mesos have the ability to dynamically manage the number of executors that run concurrently on each node, provided the resource constraints.</p><p>In the Standalone mode, <span class="strong"><strong>Spark Master</strong></span> does the work of the cluster scheduler—it might be less efficient in <a id="id165000000" class="indexterm"/>allocating resources, but it's better than nothing in the absence of preconfigured Mesos or YARN. Spark standard distribution contains shell scripts to start Spark in Standalone mode in the <code class="literal">sbin</code> directory. Spark Master and driver communicate directly with one or several Spark workers that run on individual nodes. Once the master is running, you can start Spark shell with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell --master spark://&lt;master-address&gt;:7077</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="tip05000000"/>Tip</h3><p>Note that you can always run Spark in local mode, which means that all tasks will be executed in a single JVM, by specifying <code class="literal">--master local[2]</code>, where <code class="literal">2</code> is the number of threads that have to be at least <code class="literal">2</code>. In fact, we will be using the local mode very often in this book for running small examples.</p></div><p>Spark shell is an application from the Spark point of view. Once you start a Spark application, you will see it under <span class="strong"><strong>Running Applications</strong></span> in the Spark Master UI (or in the corresponding cluster manager), which can redirect you to the Spark application HTTP UI at port 4040, where one can see the subtask execution timeline and other important properties such as environment setting, classpath, parameters passed to the JVM, and information on resource usage (refer to <span class="emphasis"><em>Figure 3-3</em></span>):</p><div class="mediaobject"><img src="../Images/image01659.jpeg" alt="Task scheduling"/><div class="caption"><p>Figure 03-3. Spark Driver UI in Standalone mode with time decomposition</p></div></div><p style="clear:both; height: 1em;"> </p><p>As we <a id="id166000000" class="indexterm"/>saw, with Spark, one can easily switch between local <a id="id167000000" class="indexterm"/>and cluster mode by providing the <code class="literal">--master</code> command-line option, setting a <code class="literal">MASTER</code> environment variable, or modifying <code class="literal">spark-defaults.conf</code>, which should be on the classpath during the execution, or even set explicitly using the <code class="literal">setters</code> method on the <code class="literal">SparkConf</code> object directly in Scala, which will be covered later:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Cluster Manager</p>
</th><th valign="bottom">
<p>MASTER env variable</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>Local (single node, multiple threads)</p>
</td><td valign="top">
<p>
<code class="literal">local[n]</code>
</p>
</td><td valign="top">
<p>
<span class="emphasis"><em>n</em></span> is the number of threads to use, should be greater than or equal to <span class="emphasis"><em>2</em></span>. If you want Spark to communicate with other Hadoop tools such as Hive, you still need to point it to the cluster by either setting the <code class="literal">HADOOP_CONF_DIR</code> environment variable or copying the Hadoop <code class="literal">*-site.xml</code> configuration files into the <code class="literal">conf</code> subdirectory.</p>
</td></tr><tr><td valign="top">
<p>Standalone (Daemons running on the nodes)</p>
</td><td valign="top">
<p>
<code class="literal">spark:// master-address&gt;:7077</code>
</p>
</td><td valign="top">
<p>This has a set of start/stop scripts in the <code class="literal">$SPARK_HOME/sbin</code> directory. This also supports the HA mode. More <a id="id168000000" class="indexterm"/>details can be found at <a class="ulink" href="https://spark.apache.org/docs/latest/spark-standalone.html">https://spark.apache.org/docs/latest/spark-standalone.html</a>.</p>
</td></tr><tr><td valign="top">
<p>Mesos</p>
</td><td valign="top">
<p>
<code class="literal">mesos://host:5050</code> or <code class="literal">mesos://zk://host:2181</code>
</p>
<p> (multimaster)</p>
</td><td valign="top">
<p>Here, you need to set <code class="literal">MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libmesos.so&gt;</code> and <code class="literal">SPARK_EXECUTOR_URI=&lt;URL of spark-1.5.0.tar.gz&gt;</code>. The default is fine-grained mode, where each Spark task runs as a separate Mesos task. Alternatively, the user can specify the coarse-grained mode, where the Mesos tasks persists for the duration of the application. The advantage is lower total start-up costs. This can use dynamic allocation (refer to the following URL) in coarse-grained mode. More <a id="id169000000" class="indexterm"/>details can be found at <a class="ulink" href="https://spark.apache.org/docs/latest/running-on-mesos.html">https://spark.apache.org/docs/latest/running-on-mesos.html</a>.</p>
</td></tr><tr><td valign="top">
<p>YARN</p>
</td><td valign="top">
<p>
<code class="literal">yarn</code>
</p>
</td><td valign="top">
<p>Spark driver can run either in the cluster or on the client node, which is managed by the <code class="literal">--deploy-mode</code> parameter (cluster or client, shell can only run in the client mode). Set <code class="literal">HADOOP_CONF_DIR</code> or <code class="literal">YARN_CONF_DIR</code> to point to the YARN config files. Use the <code class="literal">--num-executors</code> flag or <code class="literal">spark.executor.instances</code> property to set a fixed number of executors (default).</p>
<p>Set <code class="literal">spark.dynamicAllocation.enabled</code> to <code class="literal">true</code> to dynamically create/kill executors depending on the application demand. More details are available <a id="id170000000" class="indexterm"/>at <a class="ulink" href="https://spark.apache.org/docs/latest/running-on-yarn.html">https://spark.apache.org/docs/latest/running-on-yarn.html</a>.</p>
</td></tr></tbody></table></div><p>The most <a id="id171000000" class="indexterm"/>common ports are 8080, the master UI, and 4040, the <a id="id172000000" class="indexterm"/>application UI. Other Spark ports are summarized in the following table:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/><col/></colgroup><thead><tr><th colspan="2" valign="bottom" style="text-align: center">
<p>Standalone ports</p>
</th><th valign="bottom"> </th><th valign="bottom"> </th><th valign="bottom"> </th></tr><tr><th valign="bottom">
<p>From</p>
</th><th valign="bottom">
<p>To</p>
</th><th valign="bottom">
<p>Default Port</p>
</th><th valign="bottom">
<p>Purpose</p>
</th><th valign="bottom">
<p>Configuration Setting</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>Browser</p>
</td><td valign="top">
<p>Standalone Master</p>
</td><td valign="top">
<p>8080</p>
</td><td valign="top">
<p>Web UI</p>
</td><td valign="top">
<p>
<code class="literal">spark.master.ui.port /SPARK_MASTER_WEBUI_PORT</code>
</p>
</td></tr><tr><td valign="top">
<p>Browser</p>
</td><td valign="top">
<p>Standalone worker</p>
</td><td valign="top">
<p>8081</p>
</td><td valign="top">
<p>Web UI</p>
</td><td valign="top">
<p>
<code class="literal">spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT</code>
</p>
</td></tr><tr><td valign="top">
<p>Driver / Standalone worker</p>
</td><td valign="top">
<p>Standalone Master</p>
</td><td valign="top">
<p>7077</p>
</td><td valign="top">
<p>Submit job to cluster / Join cluster</p>
</td><td valign="top">
<p>
<code class="literal">SPARK_MASTER_PORT</code>
</p>
</td></tr><tr><td valign="top">
<p>Standalone master</p>
</td><td valign="top">
<p>Standalone worker</p>
</td><td valign="top">
<p>(random)</p>
</td><td valign="top">
<p>Schedule executors</p>
</td><td valign="top">
<p>
<code class="literal">SPARK_WORKER_PORT</code>
</p>
</td></tr><tr><td valign="top">
<p>Executor / Standalone master</p>
</td><td valign="top">
<p>Driver</p>
</td><td valign="top">
<p>(random)</p>
</td><td valign="top">
<p>Connect to application / Notify executor state changes</p>
</td><td valign="top">
<p>
<code class="literal">spark.driver.port</code>
</p>
</td></tr><tr><td colspan="2" valign="bottom" style="text-align: center">
<p><span class="strong"><strong>Other ports</strong></span></p>
</td><td colspan="3" valign="top" style="text-align: center"> </td></tr><tr><td valign="bottom">
<p><span class="strong"><strong>From</strong></span></p>
</td><td valign="bottom">
<p><span class="strong"><strong>To</strong></span></p>
</td><td valign="bottom">
<p><span class="strong"><strong>Default Port</strong></span></p>
</td><td valign="bottom">
<p><span class="strong"><strong>Purpose</strong></span></p>
</td><td valign="bottom">
<p><span class="strong"><strong>Configuration Setting</strong></span></p>
</td></tr><tr><td valign="top">
<p>Browser</p>
</td><td valign="top">
<p>Application</p>
</td><td valign="top">
<p>4040</p>
</td><td valign="top">
<p>Web UI</p>
</td><td valign="top">
<p>
<code class="literal">spark.ui.port</code>
</p>
</td></tr><tr><td valign="top">
<p>Browser</p>
</td><td valign="top">
<p>History server</p>
</td><td valign="top">
<p>18080</p>
</td><td valign="top">
<p>Web UI</p>
</td><td valign="top">
<p>
<code class="literal">spark.history.ui.port</code>
</p>
</td></tr><tr><td valign="top">
<p>Driver</p>
</td><td valign="top">
<p>Executor</p>
</td><td valign="top">
<p>(random)</p>
</td><td valign="top">
<p>Schedule tasks</p>
</td><td valign="top">
<p>
<code class="literal">spark.executor.port</code>
</p>
</td></tr><tr><td valign="top">
<p>Executor</p>
</td><td valign="top">
<p>Driver</p>
</td><td valign="top">
<p>(random)</p>
</td><td valign="top">
<p>File server for files and jars</p>
</td><td valign="top">
<p>
<code class="literal">spark.fileserver.port</code>
</p>
</td></tr><tr><td valign="top">
<p>Executor</p>
</td><td valign="top">
<p>Driver</p>
</td><td valign="top">
<p>(random)</p>
</td><td valign="top">
<p>HTTP broadcast</p>
</td><td valign="top">
<p>
<code class="literal">spark.broadcast.port</code>
</p>
</td></tr></tbody></table></div><p>Also, some <a id="id173000000" class="indexterm"/>of the documentation is available with the source <a id="id174000000" class="indexterm"/>distribution in the <code class="literal">docs</code> subdirectory, but may be out of date.</p></div><div class="section" title="Spark components"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec1700000"/>Spark components</h2></div></div></div><p>Since the<a id="id175000000" class="indexterm"/> emergence of Spark, multiple applications that<a id="id176000000" class="indexterm"/> benefit from Spark's ability to cache RDDs have been written: Shark, Spork (Pig on Spark), graph libraries (GraphX, GraphFrames), streaming, MLlib, and so on; some <a id="id177000000" class="indexterm"/>of these <a id="id178000000" class="indexterm"/>will be covered here and in later chapters.</p><p>In this section, I will cover major architecture components to collect, store, and analyze the data in Spark. While I will cover a more complete data life cycle architecture in <a class="link" title="Chapter 2. Data Pipelines and Modeling" href="part0242.xhtml#aid-76P842">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>, here are Spark-specific components:</p><div class="mediaobject"><img src="../Images/image01660.jpeg" alt="Spark components"/><div class="caption"><p>Figure 03-4.  Spark architecture and components.</p></div></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="MQTT, ZeroMQ, Flume, and Kafka"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec1800000"/>MQTT, ZeroMQ, Flume, and Kafka</h2></div></div></div><p>All of these <a id="id179000000" class="indexterm"/>are different ways to reliably move data from one place to<a id="id180000000" class="indexterm"/> another without loss and duplication. They <a id="id181000000" class="indexterm"/>usually implement a publish-subscribe <a id="id182000000" class="indexterm"/>model, where multiple writers and readers can write <a id="id183000000" class="indexterm"/>and<a id="id184000000" class="indexterm"/> read from the same queues with<a id="id185000000" class="indexterm"/> different<a id="id186000000" class="indexterm"/> guarantees. Flume stands out as a first distributed log and event management implementation, but it is slowly replaced by Kafka, a fully functional publish-subscribe distributed message queue optionally persistent across a distributed set of nodes developed at LinkedIn. We covered Flume and Kafka briefly in the previous chapter. Flume configuration is file-based and is traditionally used to deliver messages from a Flume source to one or several Flume sinks. One of the popular sources is <code class="literal">netcat</code>—listening on raw data over a port. For example, the following configuration describes an agent receiving data and then writing them to HDFS every 30 seconds (default):</p><div class="informalexample"><pre class="programlisting"># Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 4987

# Describe the sink (the instructions to configure and start HDFS are provided in the Appendix)
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://localhost:8020/flume/netcat/data
a1.sinks.k1.hdfs.filePrefix=chapter03.example
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1</pre></div><p>This<a id="id187000000" class="indexterm"/> file is<a id="id188000000" class="indexterm"/> included as part of the code <a id="id189000000" class="indexterm"/>provided <a id="id190000000" class="indexterm"/>with this book in the <code class="literal">chapter03/conf</code> directory. Let's <a id="id191000000" class="indexterm"/>download<a id="id192000000" class="indexterm"/> and <a id="id193000000" class="indexterm"/>start Flume agent (check the MD5 sum <a id="id194000000" class="indexterm"/>with one provided at <a class="ulink" href="http://flume.apache.org/download.html">http://flume.apache.org/download.html</a>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget http://mirrors.ocf.berkeley.edu/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz</strong></span>
<span class="strong"><strong>$ md5sum apache-flume-1.6.0-bin.tar.gz</strong></span>
<span class="strong"><strong>MD5 (apache-flume-1.6.0-bin.tar.gz) = defd21ad8d2b6f28cc0a16b96f652099</strong></span>
<span class="strong"><strong>$ tar xf apache-flume-1.6.0-bin.tar.gz</strong></span>
<span class="strong"><strong>$ cd apache-flume-1.6.0-bin</strong></span>
<span class="strong"><strong>$ ./bin/flume-ng agent -Dlog.dir=. -Dflume.log.level=DEBUG,console -n a1 -f ../chapter03/conf/flume.conf</strong></span>
<span class="strong"><strong>Info: Including Hadoop libraries found via (/Users/akozlov/hadoop-2.6.4/bin/hadoop) for HDFS access</strong></span>
<span class="strong"><strong>Info: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-api-1.7.5.jar from classpath</strong></span>
<span class="strong"><strong>Info: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar from classpath</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now, in a separate window, you can type a <code class="literal">netcat</code> command to send text to the Flume agent:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ nc localhost 4987</strong></span>
<span class="strong"><strong>Hello</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>World</strong></span>
<span class="strong"><strong>OK</strong></span>

<span class="strong"><strong>...</strong></span>
</pre></div><p>The<a id="id195000000" class="indexterm"/> Flume <a id="id196000000" class="indexterm"/>agent will first<a id="id197000000" class="indexterm"/> create a <code class="literal">*.tmp</code> file and then rename it to a file without <a id="id198000000" class="indexterm"/>extension (the file extension can be used to filter <a id="id199000000" class="indexterm"/>out <a id="id200000000" class="indexterm"/>files being written to):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/hdfs dfs -text /flume/netcat/data/chapter03.example.1463052301372</strong></span>
<span class="strong"><strong>16/05/12 04:27:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong></span>
<span class="strong"><strong>1463052302380  Hello</strong></span>
<span class="strong"><strong>1463052304307  World</strong></span>
</pre></div><p>Here, each row<a id="id201000000" class="indexterm"/> is a <a id="id202000000" class="indexterm"/>Unix time in milliseconds and data received. In this case, we put the data into HDFS, from where they can be analyzed by a Spark/Scala program, we can exclude the files being written to by the <code class="literal">*.tmp</code> filename pattern. However, if you are really interested in up-to-the-last-minute values, Spark, as well as some other platforms, supports streaming, which I will cover in a few sections.</p></div><div class="section" title="HDFS, Cassandra, S3, and Tachyon"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec1900000"/>HDFS, Cassandra, S3, and Tachyon</h2></div></div></div><p>HDFS, Cassandra, S3, and Tachyon are the different ways to get the data into persistent storage and <a id="id203000000" class="indexterm"/>compute nodes as necessary with different guarantees. HDFS<a id="id204000000" class="indexterm"/> is a distributed storage implemented as a<a id="id205000000" class="indexterm"/> part of Hadoop, which serves as the backend for<a id="id206000000" class="indexterm"/> many products in the Hadoop ecosystem. HDFS <a id="id207000000" class="indexterm"/>divides <a id="id208000000" class="indexterm"/>each file into blocks, which <a id="id209000000" class="indexterm"/>are <a id="id210000000" class="indexterm"/>128 MB in size by default, and stores each block on at least three nodes. Although HDFS is reliable and supports HA, a general complaint about HDFS storage is that it is slow, particularly for machine learning purposes. Cassandra is a general-purpose key/value storage that also stores multiple copies of a row and can be configured to support different levels of consistency to optimize read or write speeds. The advantage that Cassandra over HDFS model is that it does not have a central master node; the reads and writes are completed based on the consensus algorithm. This, however, may sometimes reflect on the Cassandra stability. S3 is the <a id="id211000000" class="indexterm"/>Amazon <a id="id212000000" class="indexterm"/>storage: The data is stored<a id="id213000000" class="indexterm"/> off-cluster, which<a id="id214000000" class="indexterm"/> affects the I/O speed. Finally, the <a id="id215000000" class="indexterm"/>recently <a id="id216000000" class="indexterm"/>developed <a id="id217000000" class="indexterm"/>Tachyon claims to utilize node's memory to<a id="id218000000" class="indexterm"/> optimize access to working sets across the nodes.</p><p>Additionally, new backends<a id="id219000000" class="indexterm"/> are being constantly developed, for example, Kudu from Cloudera (<a class="ulink" href="http://getkudu.io/kudu.pdf">http://getkudu.io/kudu.pdf</a>) and <span class="strong"><strong>Ignite File System</strong></span> (<span class="strong"><strong>IGFS</strong></span>) from <a id="id220000000" class="indexterm"/>GridGain (<a class="ulink" href="http://apacheignite.gridgain.org/v1.0/docs/igfs">http://apacheignite.gridgain.org/v1.0/docs/igfs)</a>. Both are open source and Apache-licensed.</p></div><div class="section" title="Mesos, YARN, and Standalone"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2000000"/>Mesos, YARN, and Standalone</h2></div></div></div><p>As we <a id="id221000000" class="indexterm"/>mentioned before, Spark can run under different <a id="id222000000" class="indexterm"/>cluster resource schedulers. These are various<a id="id223000000" class="indexterm"/> implementations to schedule Spark's containers <a id="id224000000" class="indexterm"/>and tasks on the cluster. The schedulers can be viewed as cluster<a id="id225000000" class="indexterm"/> kernels, performing functions similar to the operating system kernel: resource <a id="id226000000" class="indexterm"/>allocation, scheduling, I/O optimization, application services, and UI.</p><p>Mesos is one of the original cluster managers and is built using the same principles as the Linux kernel, only at a different level of abstraction. A Mesos slave runs on every machine and provides API's for resource management and scheduling across entire datacenter and cloud environments. Mesos is written in C++.</p><p>YARN is a more recent cluster manager developed by Yahoo. Each node in YARN runs a <span class="strong"><strong>Node Manager</strong></span>, which <a id="id227000000" class="indexterm"/>communicates with the <span class="strong"><strong>Resource Manager</strong></span> which may run on a separate node<a id="id228000000" class="indexterm"/>. The resource manager schedules the task to satisfy memory and CPU constraints. The Spark driver itself can run either in the cluster, which is called the cluster mode for YARN. Otherwise, in the client mode, only Spark executors run in the cluster and the driver that schedules Spark pipelines runs on the same machine that runs Spark shell or submit program. The Spark executors will talk to the local host over a random open port in this case. YARN is written in Java with the consequences of unpredictable GC pauses, which might make latency's long tail fatter.</p><p>Finally, if none of these resource schedulers are available, the standalone deployment mode starts a <code class="literal">org.apache.spark.deploy.worker.Worker</code> process on each node that communicates with the Spark Master process run as <code class="literal">org.apache.spark.deploy.master.Master</code>. The worker process is completely managed by the master and can run multiple executors and tasks (refer to <span class="emphasis"><em>Figure 3-2</em></span>).</p><p>In practical<a id="id229000000" class="indexterm"/> implementations, it is advised to track the program parallelism and <a id="id230000000" class="indexterm"/>required resources through driver's UI and adjust the parallelism <a id="id231000000" class="indexterm"/>and available memory, increasing the parallelism<a id="id232000000" class="indexterm"/> if necessary. In the following section, we will start <a id="id233000000" class="indexterm"/>looking at how Scala and Scala in Spark address <a id="id234000000" class="indexterm"/>different problems.</p></div></div>
<div class="section" title="Applications"><div class="titlepage" id="aid-7FBT62"><div><div><h1 class="title"><a id="ch03lvl1sec2400000"/>Applications</h1></div></div></div><p>Let's consider a few practical examples and libraries in Spark/Scala starting with a very traditional problem of word counting.</p><div class="section" title="Word count"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2100000"/>Word count</h2></div></div></div><p>Most modern<a id="id235000000" class="indexterm"/> machine learning algorithms require multiple passes over data. If the data fits in the memory of a single machine, the data is readily available and this does not present a performance bottleneck. However, if the data becomes too large to fit into RAM, one has a choice of either dumping pieces of the data on disk (or database), which is about 100 times slower, but has a much larger capacity, or splitting the dataset between multiple machines across the network and transferring the results. While there are still ongoing debates, for most practical systems, analysis shows that storing the data over a set of network connected nodes has a slight advantage over repeatedly storing and reading it from hard disks on a single node, particularly if we can split the workload effectively between multiple CPUs.</p><div class="note" title="Note"><h3 class="title"><a id="tip06000000"/>Tip</h3><p>An average disk has bandwidth of about 100 MB/sec and transfers with a few mms latency, depending on the rotation speed and caching. This is about 100 times slower than reading the data from memory, depending on the data size and caching implementation again. Modern data bus can transfer data at over 10 GB/sec. While the network speed still lags behind the direct memory access, particularly with standard TCP/IP kernel networking layer overhead, specialized hardware can reach tens of GB/sec and if run in parallel, it can be potentially as fast as reading from the memory. In practice, the network-transfer speeds are somewhere between 1 to 10 GB/sec, but still faster than the disk in most practical systems. Thus, we can potentially fit the data into combined memory of all the cluster nodes and perform iterative machine learning algorithms across a system of them.</p></div><p>One problem with<a id="id236000000" class="indexterm"/> memory, however, is that it is does not persist across node failures and reboots. A popular big data framework, Hadoop, made possible with the help of the original Dean/Ghemawat paper (Jeff Dean and Sanjay Ghemawat, <span class="emphasis"><em>MapReduce: Simplified Data Processing on Large Clusters</em></span>, OSDI, 2004.), is using exactly the disk layer persistence to guarantee fault tolerance and store intermediate results. A Hadoop MapReduce program would first run a <code class="literal">map</code> function on each row of a dataset, emitting one or more key/value pairs. These key/value pairs then would be sorted, grouped, and aggregated by key so that the records with the same key would end up being processed together on the same reducer, which might be running on same or another node. The reducer applies a <code class="literal">reduce</code> function that traverses all the values that were emitted for the same key and aggregates them accordingly. The persistence of intermediate results would guarantee that if a reducer fails for one or another reason, the partial computations can be discarded and the reduce computation can be restarted from the checkpoint-saved results. Many simple ETL-like applications traverse the dataset only once with very little information preserved as state from one record to another.</p><p>For example, one of the traditional applications of MapReduce is word count. The program needs to count the number of occurrences of each word in a document consisting of lines of text. In Scala, the word count is readily expressed as an application of the <code class="literal">foldLeft</code> method on a sorted list of words:</p><div class="informalexample"><pre class="programlisting">val lines = scala.io.Source.fromFile("...").getLines.toSeq
val counts = lines.flatMap(line =&gt; line.split("\\W+")).sorted.
  foldLeft(List[(String,Int)]()){ (r,c) =&gt;
    r match {
      case (key, count) :: tail =&gt;
        if (key == c) (c, count+1) :: tail
        else (c, 1) :: r
        case Nil =&gt;
          List((c, 1))
  }
}</pre></div><p>If I run this program, the output will be a list of (word, count) tuples. The program splits the lines into words, sorts the words, and then matches each word with the latest entry in the list of (word, count) tuples. The same computation in MapReduce would be expressed as follows:</p><div class="informalexample"><pre class="programlisting">val linesRdd = sc.textFile("hdfs://...")
val counts = linesRdd.flatMap(line =&gt; line.split("\\W+"))
    .map(_.toLowerCase)
    .map(word =&gt; (word, 1)).
    .reduceByKey(_+_)
counts.collect</pre></div><p>First, we need to <a id="id237000000" class="indexterm"/>process each line of the text by splitting the line into words and generation <code class="literal">(word, 1)</code> pairs. This task is easily parallelized. Then, to parallelize the global count, we need to split the counting part by assigning a task to do the count for a subset of words. In Hadoop, we compute the hash of the word and divide the work based on the value of the hash.</p><p>Once the map task finds all the entries for a given hash, it can send the key/value pairs to the reducer, the sending part is usually called shuffle in MapReduce vernacular. A reducer waits until it receives all the key/value pairs from all the mappers, combines the values—a partial combine can also happen on the mapper, if possible—and computes the overall aggregate, which in this case is just sum. A single reducer will see all the values for a given word.</p><p>Let's look at the log output of the word count operation in Spark (Spark is very verbose by default, you can manage the verbosity level by modifying the <code class="literal">conf/log4j.properties</code> file by replacing <code class="literal">INFO</code> with <code class="literal">ERROR</code> or <code class="literal">FATAL</code>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget http://mirrors.sonic.net/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ tar xvf spark-1.6.1-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ cd spark-1.6.1-bin-hadoop2.6</strong></span>
<span class="strong"><strong>$ mkdir leotolstoy</strong></span>
<span class="strong"><strong>$ (cd leotolstoy; wget http://www.gutenberg.org/files/1399/1399-0.txt)</strong></span>
<span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>
<span class="strong"><strong>scala&gt; val linesRdd = sc.textFile("leotolstoy", minPartitions=10)</strong></span>
<span class="strong"><strong>linesRdd: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[3] at textFile at &lt;console&gt;:27</strong></span>
</pre></div><p>At this stage, the only thing that happened is metadata manipulations, Spark has not touched the data itself. Spark estimates that the size of the dataset and the number of partitions. By default, this is the number of HDFS blocks, but we can specify the minimum number<a id="id238000000" class="indexterm"/> of partitions explicitly with the <code class="literal">minPartitions</code> parameter:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val countsRdd = linesRdd.flatMap(line =&gt; line.split("\\W+")).</strong></span>
<span class="strong"><strong>     | map(_.toLowerCase).</strong></span>
<span class="strong"><strong>     | map(word =&gt; (word, 1)).</strong></span>
<span class="strong"><strong>     | reduceByKey(_+_)</strong></span>
<span class="strong"><strong>countsRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at &lt;console&gt;:31</strong></span>
</pre></div><p>We just defined another RDD derived from the original <code class="literal">linesRdd</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; countsRdd.collect.filter(_._2 &gt; 99)</strong></span>
<span class="strong"><strong>res3: Array[(String, Int)] = Array((been,1061), (them,841), (found,141), (my,794), (often,105), (table,185), (this,1410), (here,364), (asked,320), (standing,132), ("",13514), (we,592), (myself,140), (is,1454), (carriage,181), (got,277), (won,153), (girl,117), (she,4403), (moment,201), (down,467), (me,1134), (even,355), (come,667), (new,319), (now,872), (upon,207), (sister,115), (veslovsky,110), (letter,125), (women,134), (between,138), (will,461), (almost,124), (thinking,159), (have,1277), (answer,146), (better,231), (men,199), (after,501), (only,654), (suddenly,173), (since,124), (own,359), (best,101), (their,703), (get,304), (end,110), (most,249), (but,3167), (was,5309), (do,846), (keep,107), (having,153), (betsy,111), (had,3857), (before,508), (saw,421), (once,334), (side,163), (ough...</strong></span>
</pre></div><p>Word count over 2 GB of text data—40,291 lines and 353,087 words—took under a second to read, split, and group by words.</p><p>With extended logging, you could see the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Spark opens a few ports to communicate with the executors and users</li><li class="listitem">Spark UI runs on port 4040 on <code class="literal">http://localhost:4040</code></li><li class="listitem">You can read the file either from local or distributed storage (HDFS, Cassandra, and S3)</li><li class="listitem">Spark will connect to Hive if Spark is built with Hive support</li><li class="listitem">Spark uses lazy evaluation and executes the pipeline only when necessary or when output is required</li><li class="listitem">Spark uses internal scheduler to split the job into tasks, optimize the execution, and execute the tasks</li><li class="listitem">The results are stored into RDDs, which can either be saved or brought into RAM of the node executing the shell with collect method</li></ul></div><p>The art of <a id="id239000000" class="indexterm"/>parallel performance tuning is to split the workload between different nodes or threads so that the overhead is relatively small and the workload is balanced.</p></div><div class="section" title="Streaming word count"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2200000"/>Streaming word count</h2></div></div></div><p>Spark supports listening on incoming streams, partitioning it, and computing aggregates close to <a id="id240000000" class="indexterm"/>real-time. Currently supported sources are Kafka, Flume, HDFS/S3, Kinesis, Twitter, as well as the traditional MQs such as ZeroMQ and MQTT. In Spark, streaming is implemented as micro-batches. Internally, Spark divides input data into micro-batches, usually from subseconds to minutes in size and performs RDD aggregation operations on these micro-batches.</p><p>For example, let's extend the Flume example that we covered earlier. We'll need to modify the Flume configuration file to create a Spark polling sink. Instead of HDFS, replace the sink section:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># The sink is Spark</strong></span>
<span class="strong"><strong>a1.sinks.k1.type=org.apache.spark.streaming.flume.sink.SparkSink</strong></span>
<span class="strong"><strong>a1.sinks.k1.hostname=localhost</strong></span>
<span class="strong"><strong>a1.sinks.k1.port=4989</strong></span>
</pre></div><p>Now, instead of writing to HDFS, Flume will wait for Spark to poll for data:</p><div class="informalexample"><pre class="programlisting">
object FlumeWordCount {
  def main(args: Array[String]) {
    // Create the context with a 2 second batch size
    val sparkConf = new SparkConf().setMaster("local[2]").setAppName("FlumeWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(2))
    ssc.checkpoint("/tmp/flume_check")
    val hostPort=args(0).split(":")
    System.out.println("Opening a sink at host: [" + hostPort(0) + "] port: [" + hostPort(1).toInt + "]")
    val lines = FlumeUtils.createPollingStream(ssc, hostPort(0), hostPort(1).toInt, StorageLevel.MEMORY_ONLY)
    val words = lines
      .map(e =&gt; new String(e.event.getBody.array)).map(_.toLowerCase).flatMap(_.split("\\W+"))
      .map(word =&gt; (word, 1L))
      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print
    ssc.start()
    ssc.awaitTermination()
  }
}</pre></div><p>To run the program, start the Flume agent in one window:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ ./bin/flume-ng agent -Dflume.log.level=DEBUG,console -n a1 –f ../chapter03/conf/flume-spark.conf</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Then run the <code class="literal">FlumeWordCount</code> object in another:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cd ../chapter03</strong></span>
<span class="strong"><strong>$ sbt "run-main org.akozlov.chapter03.FlumeWordCount localhost:4989</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now, any<a id="id241000000" class="indexterm"/> text typed to the <code class="literal">netcat</code> connection will be split into words and counted every two seconds for a six second sliding window:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ echo "Happy families are all alike; every unhappy family is unhappy in its own way" | nc localhost 4987</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1464161488000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(are,1)</strong></span>
<span class="strong"><strong>(is,1)</strong></span>
<span class="strong"><strong>(its,1)</strong></span>
<span class="strong"><strong>(family,1)</strong></span>
<span class="strong"><strong>(families,1)</strong></span>
<span class="strong"><strong>(alike,1)</strong></span>
<span class="strong"><strong>(own,1)</strong></span>
<span class="strong"><strong>(happy,1)</strong></span>
<span class="strong"><strong>(unhappy,2)</strong></span>
<span class="strong"><strong>(every,1)</strong></span>
<span class="strong"><strong>...</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1464161490000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(are,1)</strong></span>
<span class="strong"><strong>(is,1)</strong></span>
<span class="strong"><strong>(its,1)</strong></span>
<span class="strong"><strong>(family,1)</strong></span>
<span class="strong"><strong>(families,1)</strong></span>
<span class="strong"><strong>(alike,1)</strong></span>
<span class="strong"><strong>(own,1)</strong></span>
<span class="strong"><strong>(happy,1)</strong></span>
<span class="strong"><strong>(unhappy,2)</strong></span>
<span class="strong"><strong>(every,1)</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Spark/Scala allows to seamlessly switch between the streaming sources. For example, the same program <a id="id242000000" class="indexterm"/>for Kafka publish/subscribe topic model looks similar to the following:</p><div class="informalexample"><pre class="programlisting">object KafkaWordCount {
  def main(args: Array[String]) {
    // Create the context with a 2 second batch size
    val sparkConf = new SparkConf().setMaster("local[2]").setAppName("KafkaWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(2))
    ssc.checkpoint("/tmp/kafka_check")
    System.out.println("Opening a Kafka consumer at zk:[" + args(0) + "] for group group-1 and topic example")
    val lines = KafkaUtils.createStream(ssc, args(0), "group-1", Map("example" -&gt; 1), StorageLevel.MEMORY_ONLY)
    val words = lines
      .flatMap(_._2.toLowerCase.split("\\W+"))
      .map(word =&gt; (word, 1L))
      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print
    ssc.start()
    ssc.awaitTermination()
  }
}</pre></div><p>To start the Kafka broker, first download the latest binary distribution and start ZooKeeper. ZooKeeper is a distributed-services coordinator and is required by Kafka even in a single-node deployment:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget http://apache.cs.utah.edu/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ tar xf kafka_2.11-0.9.0.1.tgz</strong></span>
<span class="strong"><strong>$ bin/zookeeper-server-start.sh config/zookeeper.properties</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>In another window, start the Kafka server:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/kafka-server-start.sh config/server.properties</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Run the <code class="literal">KafkaWordCount</code> object:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sbt "run-main org.akozlov.chapter03.KafkaWordCount localhost:2181"</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now, publishing the stream of words into the Kafka topic will produce the window counts:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ echo "Happy families are all alike; every unhappy family is unhappy in its own way" | ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic example</strong></span>
<span class="strong"><strong>...</strong></span>

<span class="strong"><strong>$ sbt "run-main org.akozlov.chapter03.FlumeWordCount localhost:4989</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1464162712000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(are,1)</strong></span>
<span class="strong"><strong>(is,1)</strong></span>
<span class="strong"><strong>(its,1)</strong></span>
<span class="strong"><strong>(family,1)</strong></span>
<span class="strong"><strong>(families,1)</strong></span>
<span class="strong"><strong>(alike,1)</strong></span>
<span class="strong"><strong>(own,1)</strong></span>
<span class="strong"><strong>(happy,1)</strong></span>
<span class="strong"><strong>(unhappy,2)</strong></span>
<span class="strong"><strong>(every,1)</strong></span>
</pre></div><p>As you see, the <a id="id243000000" class="indexterm"/>programs output every two seconds. Spark streaming is sometimes called <span class="strong"><strong>micro-batch processing</strong></span>. Streaming has many<a id="id244000000" class="indexterm"/> other applications (and frameworks), but this is too big of a topic to be entirely considered here and needs to be covered separately. I'll cover some ML on streams of data in <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>. Now, let's get back to more traditional SQL-like interfaces.</p></div><div class="section" title="Spark SQL and DataFrame"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2300000"/>Spark SQL and DataFrame</h2></div></div></div><p>DataFrame was a<a id="id245000000" class="indexterm"/> relatively recent addition to Spark, introduced<a id="id246000000" class="indexterm"/> in version 1.3, allowing <a id="id247000000" class="indexterm"/>one to use the standard SQL language for data analysis. We already<a id="id248000000" class="indexterm"/> used some SQL commands in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span> for the exploratory data analysis. SQL is really great for simple exploratory analysis and data aggregations.</p><p>According to the latest poll results, about 70% of Spark users use DataFrame. Although DataFrame recently became the most popular framework for working with tabular data, it is a relatively heavyweight object. The pipelines that use DataFrames may execute much slower than the ones that are based on Scala's vector or LabeledPoint, which will be discussed in the next chapter. The evidence from different developers is that the response times can be driven to tens or hundreds of milliseconds depending on the query, from submillisecond on simpler objects.</p><p>Spark implements its own shell for SQL, which can be invoked in addition to the standard Scala REPL shell: <code class="literal">./bin/spark-sql</code> can be used to access the existing Hive/Impala or relational DB tables:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ ./bin/spark-sql</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>spark-sql&gt; select min(duration), max(duration), avg(duration) from kddcup;</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>0  58329  48.34243046395876</strong></span>
<span class="strong"><strong>Time taken: 11.073 seconds, Fetched 1 row(s)</strong></span>
</pre></div><p>In <a id="id249000000" class="indexterm"/>standard<a id="id250000000" class="indexterm"/> Spark's REPL, the<a id="id251000000" class="indexterm"/> same query can be performed by running the <a id="id252000000" class="indexterm"/>following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ ./bin/spark-shell</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>scala&gt; val df = sqlContext.sql("select min(duration), max(duration), avg(duration) from kddcup"</strong></span>
<span class="strong"><strong>16/05/12 13:35:34 INFO parse.ParseDriver: Parsing command: select min(duration), max(duration), avg(duration) from alex.kddcup_parquet</strong></span>
<span class="strong"><strong>16/05/12 13:35:34 INFO parse.ParseDriver: Parse Completed</strong></span>
<span class="strong"><strong>df: org.apache.spark.sql.DataFrame = [_c0: bigint, _c1: bigint, _c2: double]</strong></span>
<span class="strong"><strong>scala&gt; df.collect.foreach(println)</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>16/05/12 13:36:32 INFO scheduler.DAGScheduler: Job 2 finished: collect at &lt;console&gt;:22, took 4.593210 s</strong></span>
<span class="strong"><strong>[0,58329,48.34243046395876]</strong></span>
</pre></div></div></div>
<div class="section" title="ML libraries"><div class="titlepage" id="aid-7GADO2"><div><div><h1 class="title"><a id="ch03lvl1sec2500000"/>ML libraries</h1></div></div></div><p>Spark, particularly<a id="id253000000" class="indexterm"/> with memory-based storage systems, claims to substantially improve the speed of data access within and between nodes. ML seems to be a natural fit, as many algorithms require multiple passes over the data, or repartitioning. MLlib is the open source library of choice, although private companies are catching, up with their own proprietary implementations.</p><p>As I will chow in <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>, most of the standard machine learning algorithms can be expressed as an optimization problem. For example, classical linear regression minimizes the sum of squares of <span class="emphasis"><em>y</em></span> distance between the regression line and the actual value of <span class="emphasis"><em>y</em></span>:</p><div class="mediaobject"><img src="../Images/image01661.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01662.jpeg" alt="ML libraries"/></span> are the predicted values according to the linear expression:</p><div class="mediaobject"><img src="../Images/image01663.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p><p>
<span class="emphasis"><em>A</em></span> is commonly <a id="id254000000" class="indexterm"/>called the slope, and <span class="emphasis"><em>B</em></span> the intercept. In a more generalized formulation, a linear optimization problem is to minimize an additive function:</p><div class="mediaobject"><img src="../Images/image01664.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01665.jpeg" alt="ML libraries"/></span> is a loss function and <span class="inlinemediaobject"><img src="../Images/image01666.jpeg" alt="ML libraries"/></span> is a regularization function. The regularization function is an increasing function of model complexity, for example, the number of parameters (or a natural logarithm thereof). Most common loss functions are given in the following table:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom"> </th><th valign="bottom">
<p>Loss function L</p>
</th><th valign="bottom">
<p>Gradient</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>Linear</p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01667.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01668.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td></tr><tr><td valign="top">
<p>Logistic </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01669.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01670.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td></tr><tr><td valign="top">
<p>Hinge</p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01671.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image01672.jpeg" alt="ML libraries"/></span>
</td></tr></tbody></table></div><p>The purpose of the <a id="id255000000" class="indexterm"/>regularizer is to penalize more complex models to avoid overfitting and improve generalization error: more MLlib currently supports the following regularizers:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom"> </th><th valign="bottom">
<p>Regularizer R</p>
</th><th valign="bottom">
<p>Gradient</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>L2</p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01673.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01674.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td></tr><tr><td valign="top">
<p>L1</p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01675.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01676.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td></tr><tr><td valign="top">
<p>Elastic net</p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01677.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td><td valign="top">
<div class="mediaobject"><img src="../Images/image01678.jpeg" alt="ML libraries"/></div><p style="clear:both; height: 1em;"> </p>
</td></tr></tbody></table></div><p>Here, <span class="emphasis"><em>sign(w)</em></span> is the vector of the signs of all entries of <span class="emphasis"><em>w</em></span>.</p><p>Currently, MLlib includes implementation of the following algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Basic statistics:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Summary statistics</li><li class="listitem">Correlations</li><li class="listitem">Stratified sampling</li><li class="listitem">Hypothesis testing</li><li class="listitem">Streaming significance testing</li><li class="listitem">Random data generation</li></ul></div></li><li class="listitem">Classification and regression:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Linear models (SVMs, logistic regression, and linear regression)</li><li class="listitem">Naive Bayes</li><li class="listitem">Decision trees</li><li class="listitem">Ensembles of trees (Random Forests and Gradient-Boosted Trees)</li><li class="listitem">Isotonic regression</li></ul></div></li><li class="listitem">Collaborative <a id="id256000000" class="indexterm"/>filtering:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Alternating least squares</strong></span> (<span class="strong"><strong>ALS</strong></span>)</li></ul></div></li><li class="listitem">Clustering:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">k-means</li><li class="listitem">Gaussian<a id="id257000000" class="indexterm"/> mixture</li><li class="listitem"><span class="strong"><strong>Power Iteration Clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>)</li><li class="listitem"><span class="strong"><strong>Latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</li><li class="listitem">Bisecting <a id="id258000000" class="indexterm"/>k-means</li><li class="listitem">Streaming k-means</li></ul></div></li><li class="listitem">Dimensionality <a id="id259000000" class="indexterm"/>reduction:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>)</li><li class="listitem"><span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>)</li></ul></div></li><li class="listitem">Feature <a id="id260000000" class="indexterm"/>extraction and transformation</li><li class="listitem">Frequent <a id="id261000000" class="indexterm"/>pattern mining:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">FP-growth?Association rules</li><li class="listitem">PrefixSpan</li></ul></div></li><li class="listitem">Optimization:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>SGD</strong></span>)</li><li class="listitem"><span class="strong"><strong>Limited-Memory BFGS</strong></span> (<span class="strong"><strong>L-BFGS</strong></span>)</li></ul></div></li></ul></div><p>I will go over <a id="id262000000" class="indexterm"/>some of the algorithms in <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>. More complex non-structured machine learning methods <a id="id263000000" class="indexterm"/>will be considered in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>.</p><div class="section" title="SparkR"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2400000"/>SparkR</h2></div></div></div><p>R is an<a id="id264000000" class="indexterm"/> implementation of popular S programming language <a id="id265000000" class="indexterm"/>created by John Chambers while working at Bell Labs. R is currently supported by the <span class="strong"><strong>R Foundation for Statistical Computing</strong></span>. R's popularity has increased in recent years according to polls. SparkR provides a lightweight frontend to use Apache Spark from R. Starting with Spark 1.6.0, SparkR provides a distributed DataFrame implementation that supports operations such as selection, filtering, aggregation, and so on, which is similar to R DataFrames, dplyr, but on very large datasets. SparkR also supports distributed machine learning using MLlib.</p><p>SparkR required R version 3 or higher, and can be invoked via the <code class="literal">./bin/sparkR</code> shell. I will cover SparkR in <a class="link" title="Chapter 8. Integrating Scala with R and Python" href="part0288.xhtml#aid-8IL202">Chapter 8</a>, <span class="emphasis"><em>Integrating Scala with R and Python</em></span>.</p></div><div class="section" title="Graph algorithms – GraphX and GraphFrames"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec2500000"/>Graph algorithms – GraphX and GraphFrames</h2></div></div></div><p>Graph algorithms<a id="id266000000" class="indexterm"/> are one of the hardest to correctly<a id="id267000000" class="indexterm"/> distribute between nodes, unless the graph itself is naturally <a id="id268000000" class="indexterm"/>partitioned, that is, it can be represented by a set of<a id="id269000000" class="indexterm"/> disconnected subgraphs. Since the social networking analysis <a id="id270000000" class="indexterm"/>on a multi-million node scale became popular due to <a id="id271000000" class="indexterm"/>companies such as Facebook, Google, and LinkedIn, researches have been coming up with new approaches to formalize the graph representations, algorithms, and types of questions asked.</p><p>GraphX is a modern framework for graph computations described in a 2013 paper (<span class="emphasis"><em>GraphX: A Resilient Distributed Graph System on Spark</em></span> by Reynold Xin, Joseph Gonzalez, Michael Franklin, and Ion Stoica, GRADES (SIGMOD workshop), 2013). It has graph-parallel frameworks such as Pregel, and PowerGraph as predecessors. The graph is represented by two RDDs: one for vertices and another one for edges. Once the RDDs are joined, GraphX supports either Pregel-like API or MapReduce-like API, where the map function is applied to the node's neighbors and reduce is the aggregation step on top of the map results.</p><p>At the time of writing, GraphX includes the implementation for the following graph algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">PageRank</li><li class="listitem">Connected components</li><li class="listitem">Triangle counting</li><li class="listitem">Label propagation</li><li class="listitem">SVD++ (collaborative filtering)</li><li class="listitem">Strongly connected components</li></ul></div><p>As GraphX is an open source library, changes to the list are expected. GraphFrames is a new implementation <a id="id272000000" class="indexterm"/>from Databricks that fully supports the following <a id="id273000000" class="indexterm"/>three <a id="id274000000" class="indexterm"/>languages: Scala, Java, and <a id="id275000000" class="indexterm"/>Python, and is build on top <a id="id276000000" class="indexterm"/>of DataFrames. I'll discuss specific implementations in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>.</p></div></div>
<div class="section" title="Spark performance tuning" id="aid-7H8UA1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec2600000"/>Spark performance tuning</h1></div></div></div><p>While <a id="id277000000" class="indexterm"/>efficient execution of the data pipeline is prerogative of the task scheduler, which is part of the Spark driver, sometimes Spark needs hints. Spark scheduling is primarily driven by the two parameters: CPU and memory. Other resources, such as disk and network I/O, of course, play an important part in Spark performance as well, but neither Spark, Mesos or YARN can currently do anything to actively manage them.</p><p>The first parameter to watch is the number of RDD partitions, which can be specified explicitly when reading the RDD from a file. Spark usually errs on the side of too many partitions as it provides more parallelism, and it does work in many cases as the task setup/teardown times are relatively small. However, one might experiment with decreasing the number of partitions, especially if one does aggregations.</p><p>The default number of partitions per RDD and the level of parallelism is determined by the <code class="literal">spark.default.parallelism</code> parameter, defined in the <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code> configuration file. The number of partitions for a specific RDD can also be explicitly changed by the <code class="literal">coalesce()</code> or <code class="literal">repartition()</code> methods.</p><p>The total number of cores and available memory is often the reason for deadlocks as the tasks cannot proceed further. One can specify the number of cores for each executor with the <code class="literal">--executor-cores</code> flag when invoking spark-submit, spark-shell, or PySpark from the command line. Alternatively, one can set the corresponding parameters in the <code class="literal">spark-defaults.conf</code> file discussed earlier. If the number of cores is set too high, the scheduler will not be able to allocate resources on the nodes and will deadlock.</p><p>In a similar way, <code class="literal">--executor-memory</code> (or the <code class="literal">spark.executor.memory</code> property) specifies the requested heap size for all the tasks (the default is 1g). If the executor memory is specified too high, again, the scheduler may be deadlocked or will be able to schedule only a limited number of executors on a node.</p><p>The implicit assumption in Standalone mode when counting the number of cores and memory is that Spark is the only running application—which may or may not be true. When running under Mesos or YARN, it is important to configure the cluster scheduler that it has the resources available to schedule the executors requested by the Spark Driver. The relevant YARN properties are: <code class="literal">yarn.nodemanager.resource.cpu-vcores</code> and <code class="literal">yarn.nodemanager.resource.memory-mb</code>. YARN may round the requested memory up a little. YARN's <code class="literal">yarn.scheduler.minimum-allocation-mb</code> and <code class="literal">yarn.scheduler.increment-allocation-mb</code> properties control the minimum and increment request values respectively.</p><p>JVMs can <a id="id278000000" class="indexterm"/>also use some memory off heap, for example, for interned strings and direct byte buffers. The value of the <code class="literal">spark.yarn.executor.memoryOverhead</code> property is added to the executor memory to determine the full memory request to YARN for each executor. It defaults to max (<span class="emphasis"><em>384, .07 * spark.executor.memory</em></span>).</p><p>Since Spark can internally transfer the data between executors and client node, efficient serialization is very important. I will consider different serialization frameworks in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>, but Spark uses Kryo serialization by default, which requires the classes to be registered explicitly in a static method. If you see a serialization error in your code, it is likely because the corresponding class has not been registered or Kryo does not support it, as it happens with too nested and complex data types. In general, it is recommended to avoid complex objects to be passed between the executors unless the object serialization can be done very efficiently.</p><p>Driver has similar parameters: <code class="literal">spark.driver.cores</code>, <code class="literal">spark.driver.memory</code>, and <code class="literal">spark.driver.maxResultSize</code>. The latter one sets the limit for the results collected from all the executors with the <code class="literal">collect</code> method. It is important to protect the driver process from out-of-memory exceptions. The other way to avoid out-of-memory exceptions and consequent problems are to either modify the pipeline to return aggregated or filtered results or use the <code class="literal">take</code> method instead.</p></div>
<div class="section" title="Running Hadoop HDFS"><div class="titlepage" id="aid-7I7ES2"><div><div><h1 class="title"><a id="ch03lvl1sec2700000"/>Running Hadoop HDFS</h1></div></div></div><p>A distributed <a id="id279000000" class="indexterm"/>processing framework wouldn't be complete without distributed storage. One of them is HDFS. Even if Spark is run on local mode, it can still use a distributed file system at the backend. Like Spark breaks computations into subtasks, HDFS breaks a file into blocks and stores them across a set of machines. For HA, HDFS stores multiple copies of each block, the number of copies is called replication level, three by default (refer to <span class="emphasis"><em>Figure 3-5</em></span>).</p><p>
<span class="strong"><strong>NameNode</strong></span><a id="id280000000" class="indexterm"/> is managing the HDFS storage by remembering the block locations and other metadata such as owner, file permissions, and block size, which are file-specific. <span class="strong"><strong>Secondary Namenode</strong></span> is a slight misnomer: its function is to merge the <a id="id281000000" class="indexterm"/>metadata modifications, edits, into fsimage, or a file that serves as a metadata database. The merge is required, as it is more practical to write modifications of fsimage to a separate file instead of applying each modification to the disk image of the fsimage directly (in addition to applying the corresponding changes in memory). Secondary <span class="strong"><strong>Namenode</strong></span> cannot serve as a second copy of the <span class="strong"><strong>Namenode</strong></span>. A <span class="strong"><strong>Balancer</strong></span> is <a id="id282000000" class="indexterm"/>run to move the blocks to maintain approximately equal disk usage across the servers—the initial block assignment to the nodes is supposed to be random, if enough space is available and the client is not run within the cluster. Finally, the <span class="strong"><strong>Client</strong></span><a id="id283000000" class="indexterm"/> communicates with the <span class="strong"><strong>Namenode</strong></span> to get the metadata and block locations, but after that, either reads or writes the data directly to the node, where a copy of the block resides. The client is the only component that can be run outside the HDFS cluster, but it needs network connectivity with all the nodes in the cluster.</p><p>If any of the node<a id="id284000000" class="indexterm"/> dies or disconnects from the network, the <span class="strong"><strong>Namenode</strong></span> notices the change, as it constantly maintains the contact with the nodes via heartbeats. If the node does not reconnect to the <span class="strong"><strong>Namenode</strong></span> within 10 minutes (by default), the <span class="strong"><strong>Namenode</strong></span> will start replicating the blocks in order to achieve the required replication level for the blocks that were lost on the node. A separate block scanner thread in the <span class="strong"><strong>Namenode</strong></span> will scan the blocks for possible bit rot—each block maintains a checksum—and will delete corrupted and orphaned blocks:</p><div class="mediaobject"><img src="../Images/image01679.jpeg" alt="Running Hadoop HDFS"/><div class="caption"><p>Figure 03-5. This is the HDFS architecture. Each block is stored in three separate locations (the replication level).</p></div></div><p style="clear:both; height: 1em;"> </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">To start HDFS<a id="id285000000" class="indexterm"/> on your machine (with replication level 1), download a <a id="id286000000" class="indexterm"/>Hadoop distribution, for example, from <a class="ulink" href="http://hadoop.apache.org">http://hadoop.apache.org</a>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/h/hadoop-2.6.4.tar.gz</strong></span>
<span class="strong"><strong>--2016-05-12 00:10:55--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz</strong></span>
<span class="strong"><strong>           =&gt; 'hadoop-2.6.4.tar.gz.1'</strong></span>
<span class="strong"><strong>Resolving apache.cs.utah.edu... 155.98.64.87</strong></span>
<span class="strong"><strong>Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.</strong></span>
<span class="strong"><strong>Logging in as anonymous ... Logged in!</strong></span>
<span class="strong"><strong>==&gt; SYST ... done.    ==&gt; PWD ... done.</strong></span>
<span class="strong"><strong>==&gt; TYPE I ... done.  ==&gt; CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.</strong></span>
<span class="strong"><strong>==&gt; SIZE hadoop-2.6.4.tar.gz ... 196015975</strong></span>
<span class="strong"><strong>==&gt; PASV ... done.    ==&gt; RETR hadoop-2.6.4.tar.gz ... done.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds</strong></span>
<span class="strong"><strong>--2016-05-12 00:13:58--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds</strong></span>
<span class="strong"><strong>           =&gt; 'hadoop-2.6.4.tar.gz.mds'</strong></span>
<span class="strong"><strong>Resolving apache.cs.utah.edu... 155.98.64.87</strong></span>
<span class="strong"><strong>Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.</strong></span>
<span class="strong"><strong>Logging in as anonymous ... Logged in!</strong></span>
<span class="strong"><strong>==&gt; SYST ... done.    ==&gt; PWD ... done.</strong></span>
<span class="strong"><strong>==&gt; TYPE I ... done.  ==&gt; CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.</strong></span>
<span class="strong"><strong>==&gt; SIZE hadoop-2.6.4.tar.gz.mds ... 958</strong></span>
<span class="strong"><strong>==&gt; PASV ... done.    ==&gt; RETR hadoop-2.6.4.tar.gz.mds ... done.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ shasum -a 512 hadoop-2.6.4.tar.gz</strong></span>
<span class="strong"><strong>493cc1a3e8ed0f7edee506d99bfabbe2aa71a4776e4bff5b852c6279b4c828a0505d4ee5b63a0de0dcfecf70b4bb0ef801c767a068eaeac938b8c58d8f21beec  hadoop-2.6.4.tar.gz</strong></span>
<span class="strong"><strong>$ cat !$.mds</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz:    MD5 = 37 01 9F 13 D7 DC D8 19  72 7B E1 58 44 0B 94 42</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz:   SHA1 = 1E02 FAAC 94F3 35DF A826  73AC BA3E 7498 751A 3174</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz: RMD160 = 2AA5 63AF 7E40 5DCD 9D6C  D00E EBB0 750B D401 2B1F</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz: SHA224 = F4FDFF12 5C8E754B DAF5BCFC 6735FCD2 C6064D58</strong></span>
<span class="strong"><strong>                              36CB9D80 2C12FC4D</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz: SHA256 = C58F08D2 E0B13035 F86F8B0B 8B65765A B9F47913</strong></span>
<span class="strong"><strong>                              81F74D02 C48F8D9C EF5E7D8E</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz: SHA384 = 87539A46 B696C98E 5C7E352E 997B0AF8 0602D239</strong></span>
<span class="strong"><strong>                              5591BF07 F3926E78 2D2EF790 BCBB6B3C EAF5B3CF</strong></span>
<span class="strong"><strong>                              ADA7B6D1 35D4B952</strong></span>
<span class="strong"><strong>hadoop-2.6.4.tar.gz: SHA512 = 493CC1A3 E8ED0F7E DEE506D9 9BFABBE2 AA71A477</strong></span>
<span class="strong"><strong>                              6E4BFF5B 852C6279 B4C828A0 505D4EE5 B63A0DE0</strong></span>
<span class="strong"><strong>                              DCFECF70 B4BB0EF8 01C767A0 68EAEAC9 38B8C58D</strong></span>
<span class="strong"><strong>                              8F21BEEC</strong></span>

<span class="strong"><strong>$ tar xf hadoop-2.6.4.tar.gz</strong></span>
<span class="strong"><strong>$ cd hadoop-2.6.4</strong></span>
</pre></div></li><li class="listitem">To get the <a id="id287000000" class="indexterm"/>minimal HDFS configuration, modify the <code class="literal">core-site.xml</code> and <code class="literal">hdfs-site.xml</code> files, as follows:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cat &lt;&lt; EOF &gt; etc/hadoop/core-site.xml</strong></span>
<span class="strong"><strong>&lt;configuration&gt;</strong></span>
<span class="strong"><strong>    &lt;property&gt;</strong></span>
<span class="strong"><strong>        &lt;name&gt;fs.defaultFS&lt;/name&gt;</strong></span>
<span class="strong"><strong>        &lt;value&gt;hdfs://localhost:8020&lt;/value&gt;</strong></span>
<span class="strong"><strong>    &lt;/property&gt;</strong></span>
<span class="strong"><strong>&lt;/configuration&gt;</strong></span>
<span class="strong"><strong>EOF</strong></span>
<span class="strong"><strong>$ cat &lt;&lt; EOF &gt; etc/hadoop/hdfs-site.xml</strong></span>
<span class="strong"><strong>&lt;configuration&gt;</strong></span>
<span class="strong"><strong>   &lt;property&gt;</strong></span>
<span class="strong"><strong>        &lt;name&gt;dfs.replication&lt;/name&gt;</strong></span>
<span class="strong"><strong>        &lt;value&gt;1&lt;/value&gt;</strong></span>
<span class="strong"><strong>    &lt;/property&gt;</strong></span>
<span class="strong"><strong>&lt;/configuration&gt;</strong></span>
<span class="strong"><strong>EOF</strong></span>
</pre></div><p>This will put the Hadoop HDFS metadata and data directories under the <code class="literal">/tmp/hadoop-$USER</code> directories. To make this more permanent, we can add the <code class="literal">dfs.namenode.name.dir</code>,<code class="literal"> dfs.namenode.edits.dir</code>, and <code class="literal">dfs.datanode.data.dir</code> parameters, but we will leave these out for now. For a more customized distribution, one can download a Cloudera<a id="id288000000" class="indexterm"/> version from <a class="ulink" href="http://archive.cloudera.com/cdh">http://archive.cloudera.com/cdh</a>.</p></li><li class="listitem">First, we<a id="id289000000" class="indexterm"/> need to write an empty metadata:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/hdfs namenode -format</strong></span>
<span class="strong"><strong>16/05/12 00:55:40 INFO namenode.NameNode: STARTUP_MSG: </strong></span>
<span class="strong"><strong>/************************************************************</strong></span>
<span class="strong"><strong>STARTUP_MSG: Starting NameNode</strong></span>
<span class="strong"><strong>STARTUP_MSG:   host = alexanders-macbook-pro.local/192.168.1.68</strong></span>
<span class="strong"><strong>STARTUP_MSG:   args = [-format]</strong></span>
<span class="strong"><strong>STARTUP_MSG:   version = 2.6.4</strong></span>
<span class="strong"><strong>STARTUP_MSG:   classpath =</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div></li><li class="listitem">Then start the <code class="literal">namenode</code>, <code class="literal">secondarynamenode</code>, and <code class="literal">datanode</code> Java processes (I usually open three different command-line windows to see the logs, but in a production environment, these are usually daemonized):<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/hdfs namenode &amp;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ bin/hdfs secondarynamenode &amp;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ bin/hdfs datanode &amp;</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div></li><li class="listitem">We are now ready to create the first HDFS file:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ date | bin/hdfs dfs –put – date.txt</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ bin/hdfs dfs –ls</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 akozlov supergroup 29 2016-05-12 01:02 date.txt</strong></span>
<span class="strong"><strong>$ bin/hdfs dfs -text date.txt</strong></span>
<span class="strong"><strong>Thu May 12 01:02:36 PDT 2016</strong></span>
</pre></div></li><li class="listitem">Of course, in <a id="id290000000" class="indexterm"/>this particular case, the actual file is stored only on one node, which is the same node we run <code class="literal">datanode</code> on (localhost). In my case, it is the following:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cat /tmp/hadoop-akozlov/dfs/data/current/BP-1133284427-192.168.1.68-1463039756191/current/finalized/subdir0/subdir0/blk_1073741827</strong></span>
<span class="strong"><strong>Thu May 12 01:02:36 PDT 2016</strong></span>
</pre></div></li><li class="listitem">The <a id="id291000000" class="indexterm"/>Namenode UI can be found at <code class="literal">http://localhost:50070</code> and displays a host of information, including the HDFS usage and the list of DataNodes, the slaves of the HDFS Master node as follows:<div class="mediaobject"><img src="../Images/image01680.jpeg" alt="Running Hadoop HDFS"/><div class="caption"><p>Figure 03-6.  A snapshot of HDFS NameNode UI.</p></div></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><p>The preceding <a id="id292000000" class="indexterm"/>figure shows HDFS Namenode HTTP UI in a single node deployment (usually, <code class="literal">http://&lt;namenode-address&gt;:50070</code>). The <span class="strong"><strong>Utilities</strong></span> | <span class="strong"><strong>Browse the file system</strong></span> tab allows you to browse and download the files from HDFS. Nodes can be added by starting DataNodes on a different node and pointing to the Namenode with the <code class="literal">fs.defaultFS=&lt;namenode-address&gt;:8020</code> parameter. The Secondary Namenode HTTP UI is usually at <code class="literal">http:&lt;secondarynamenode-address&gt;:50090</code>.</p><p>Scala/Spark by default will use the local file system. However, if the <code class="literal">core-site/xml</code> file is on the classpath or placed in the <code class="literal">$SPARK_HOME/conf</code> directory, Spark will use HDFS as the default.</p></div>
<div class="section" title="Summary" id="aid-7J5VE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec2800000"/>Summary</h1></div></div></div><p>In this chapter, I covered the Spark/Hadoop and their relationship with Scala and functional programming at a very high level. I considered a classic word count example and it's implementation in Scala and Spark. I also provided high-level components of Spark ecosystem with specific examples of word count and streaming. I now have all the components to start looking at the specific implementation of classic machine learning algorithms in Scala/Spark. In the next chapter, I will start by covering supervised and unsupervised learning—a traditional division of learning algorithms for structured data.</p></div>
<div class="chapter" title="Chapter&#xA0;4.&#xA0;Supervised and Unsupervised Learning"><div class="titlepage" id="aid-7K4G02"><div><div><h1 class="title"><a id="ch30"/>Chapter 4. Supervised and Unsupervised Learning</h1></div></div></div><p>I covered the basics of the MLlib library in the previous chapter, but MLlib, at least at the time of writing this book, is more like a fast-moving target that is gaining the lead rather than a well-structured implementation that everyone uses in production or even has a consistent and tested documentation. In this situation, as people say, rather than giving you the fish, I will try to focus on well-established concepts behind the libraries and teach the process of fishing in this book in order to avoid the need to drastically modify the chapters with each new MLlib release. For better or worse, this increasingly seems to be a skill that a data scientist needs to possess.</p><p>Statistics and machine learning inherently deal with uncertainty, due to one or another reason we covered in <a class="link" title="Chapter 2. Data Pipelines and Modeling" href="part0242.xhtml#aid-76P842">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>. While some datasets might be completely random, the goal here is to find trends, structure, and patterns beyond what a random number generator will provide you. The fundamental value of ML is that we can generalize these patterns and improve on at least some metrics. Let's see what basic tools are available within Scala/Spark.</p><p>In this chapter, I am covering supervised and unsupervised leaning, the two historically different approaches. Supervised learning is traditionally used when we have a specific goal to predict a label, or a specific attribute of a dataset. Unsupervised learning can be used to understand internal structure and dependencies between any attributes of a dataset, and is often used to group the records or attributes in meaningful clusters.  In practice, both methods can be used to complement and aid each other.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Learning standard models for supervised learning – decision trees and logistic regression</li><li class="listitem">Discussing the staple of unsupervised learning – k-means clustering and its derivatives</li><li class="listitem">Understanding metrics and methods to evaluate the effectiveness of the above algorithms</li><li class="listitem">Having a glimpse of extending the above methods on special cases of streaming data, sparse data, and non-structured data</li></ul></div><div class="section" title="Records and supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec2900000"/>Records and supervised learning</h1></div></div></div><p>For the purpose <a id="id293000000" class="indexterm"/>of this chapter, a record is an observation or measurement of one or several attributes. We assume that the observations might contain noise <span class="inlinemediaobject"><img src="../Images/image01681.jpeg" alt="Records and supervised learning"/></span> (or be inaccurate for one or other reason):</p><div class="mediaobject"><img src="../Images/image01682.jpeg" alt="Records and supervised learning"/></div><p style="clear:both; height: 1em;"> </p><p>While we believe that there is some pattern or correlation between the attributes, the one that we are after and want to uncover, the noise is uncorrelated across either the attributes or the records. In statistical terms, we say that the values for each record are drawn from the same distribution and are independent (or <span class="emphasis"><em>i.i.d</em></span>. in statistical terms). The order of records does not matter. One of the attributes, usually the first, might be designated to be the label.</p><p>Supervised learning is when the goal is to predict the label <span class="emphasis"><em>yi</em></span>:</p><div class="mediaobject"><img src="../Images/image01683.jpeg" alt="Records and supervised learning"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>N</em></span> is the number of remaining attributes. In other words, the goal is to generalize the patterns so that we can predict the label by just knowing the other attributes, whether because we cannot physically get the measurement or just want to explore the structure of the dataset without having the immediate goal to predict the label.</p><p>The unsupervised learning is when we don't use the label—we just try to explore the structure and correlations to understand the dataset to, potentially, predict the label better. The number of problems in this latter category has increased recently with the emergence of learning for unstructured data and streams, each of which, I'll be covering later in the book in separate chapters.</p><div class="section" title="Iris dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec2600000"/>Iris dataset</h2></div></div></div><p>I will demonstrate<a id="id294000000" class="indexterm"/> the concept of records and labels based on<a id="id295000000" class="indexterm"/> one of the most famous datasets in machine learning, the Iris dataset (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>). The Iris dataset <a id="id296000000" class="indexterm"/>contains 50 records for each of the three types of Iris flower, 150 lines of total five fields. Each line is a measurement of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Sepal length in cm</li><li class="listitem">Sepal width in cm</li><li class="listitem">Petal length in cm</li><li class="listitem">Petal width in cm</li></ul></div><p>With the final field being the type of the flower (<span class="emphasis"><em>setosa</em></span>, <span class="emphasis"><em>versicolor</em></span>, or <span class="emphasis"><em>virginica</em></span>). The classic problem is to predict the label, which, in this case, is a categorical attribute with three possible values as a function of the first four attributes:</p><div class="mediaobject"><img src="../Images/image01684.jpeg" alt="Iris dataset"/></div><p style="clear:both; height: 1em;"> </p><p>One option would be to draw a plane in the four-dimensional space that separates all four labels. Unfortunately, as one can find out, while one of the classes is clearly separable, the remaining two are not, as shown in the following multidimensional scatterplot (we have used Data Desk software to create it):</p><div class="mediaobject"><img src="../Images/image01685.jpeg" alt="Iris dataset"/><div class="caption"><p>Figure 04-1. The Iris dataset as a three-dimensional plot. The Iris setosa records, shown by crosses, can be separated from the other two types based on petal length and width.</p></div></div><p style="clear:both; height: 1em;"> </p><p>The colors and shapes are assigned according to the following table:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Label</p>
</th><th valign="bottom">
<p>Color</p>
</th><th valign="bottom">
<p>Shape</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<span class="emphasis"><em>Iris setosa</em></span>
</p>
</td><td valign="top">
<p>Blue</p>
</td><td valign="top">
<p>x</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>Iris versicolor</em></span>
</p>
</td><td valign="top">
<p>Green</p>
</td><td valign="top">
<p>Vertical bar</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>Iris virginica</em></span>
</p>
</td><td valign="top">
<p>Purple</p>
</td><td valign="top">
<p>Horizontal bar</p>
</td></tr></tbody></table></div><p>The <span class="emphasis"><em>Iris setosa</em></span> is separable because it happens to have a very short petal length and width compared to the two other types.</p><p>Let's see how <a id="id297000000" class="indexterm"/>we can use MLlib to find that separating <a id="id298000000" class="indexterm"/>multidimensional plane.</p></div><div class="section" title="Labeled point"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec2700000"/>Labeled point</h2></div></div></div><p>The labeled datasets <a id="id299000000" class="indexterm"/>used to have a very special place<a id="id300000000" class="indexterm"/> in ML—we will discuss unsupervised learning later in the chapter, where we do not need a label, so MLlib has a special data type to represent a record <a id="id301000000" class="indexterm"/>with a <code class="literal">org.apache.spark.mllib.regression.LabeledPoint</code> label (refer to <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point</a>). To read the Iris dataset from a text file, we need to transform the original UCI repository file into the so-called LIBSVM text format. While there are plenty of converters from CSV to LIBSVM format, I'd like to use a simple AWK script to do the job:</p><div class="informalexample"><pre class="programlisting">awk -F, '/setosa/ {print "0 1:"$1" 2:"$2" 3:"$3" 4:"$4;}; /versicolor/ {print "1 1:"$1" 2:"$2" 3:"$3" 4:"$4;}; /virginica/ {print "1 1:"$1" 2:"$2" 3:"$3" 4:"$4;};' iris.csv &gt; iris-libsvm.txt
</pre></div><div class="note" title="Note"><h3 class="title"><a id="note04000000"/>Note</h3><p>
<span class="strong"><strong>Why do we need LIBSVM format?</strong></span>
</p><p>LIBSVM is <a id="id302000000" class="indexterm"/>the format that many libraries use. First, LIBSVM takes only continuous attributes. While a lot of datasets in the real world contain discrete or categorical attributes, internally they are always converted to a numerical representation for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical attribute does not make much sense in the unordered discrete values. Second, the LIBSVM format allows for efficient sparse data representation. While the Iris dataset is not sparse, almost all of the modern big data sources are sparse, and the format allows for efficient storage by only storing the provided values. Many modern big data key-value and traditional RDBMS databases actually do the same for efficiency reasons.</p></div><p>The code<a id="id303000000" class="indexterm"/> might be more complex for missing values, but we know<a id="id304000000" class="indexterm"/> that the Iris dataset is not sparse—otherwise we'd complement our code with a bunch of if statements. We mapped the last two labels to 1 for our purpose now.</p></div><div class="section" title="SVMWithSGD"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec2800000"/>SVMWithSGD</h2></div></div></div><p>Now, let's<a id="id305000000" class="indexterm"/> run the <span class="strong"><strong>Linear Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>) <a id="id306000000" class="indexterm"/>SVMWithSGD code<a id="id307000000" class="indexterm"/> from MLlib:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.6, 0.4), seed = 123L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 100</strong></span>
<span class="strong"><strong>numIterations: Int = 100</strong></span>
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(training, numIterations)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.clearThreshold()</strong></span>
<span class="strong"><strong>res0: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = None</strong></span>
<span class="strong"><strong>scala&gt; val scoreAndLabels = test.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val score = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (score, point.label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at &lt;console&gt;:36</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new BinaryClassificationMetrics(scoreAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@692e4a35</strong></span>
<span class="strong"><strong>scala&gt; val auROC = metrics.areaUnderROC()</strong></span>
<span class="strong"><strong>auROC: Double = 1.0</strong></span>

<span class="strong"><strong>scala&gt; println("Area under ROC = " + auROC)</strong></span>
<span class="strong"><strong>Area under ROC = 1.0</strong></span>
<span class="strong"><strong>scala&gt; model.save(sc, "model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>So, you just<a id="id308000000" class="indexterm"/> run one of the most complex algorithms in the <a id="id309000000" class="indexterm"/>machine learning toolbox: SVM. The result is a separating plane that distinguishes <span class="emphasis"><em>Iris setosa</em></span> flowers from the other two types. The model in this case is exactly the intercept and the coefficients of the plane that best separates the labels:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; model.intercept</strong></span>
<span class="strong"><strong>res5: Double = 0.0</strong></span>

<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>res6: org.apache.spark.mllib.linalg.Vector = [-0.2469448809675877,-1.0692729424287566,1.7500423423258127,0.8105712661836376]</strong></span>
</pre></div><p>If one looks under the hood, the model is stored in a <code class="literal">parquet</code> file, which can be dumped using <code class="literal">parquet-tool</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ parquet-tools dump model/data/part-r-00000-7a86b825-569d-4c80-8796-8ee6972fd3b1.gz.parquet</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>DOUBLE weights.values.array </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:3 V:-0.2469448809675877</strong></span>
<span class="strong"><strong>value 2: R:1 D:3 V:-1.0692729424287566</strong></span>
<span class="strong"><strong>value 3: R:1 D:3 V:1.7500423423258127</strong></span>
<span class="strong"><strong>value 4: R:1 D:3 V:0.8105712661836376</strong></span>

<span class="strong"><strong>DOUBLE intercept </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 1 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:1 V:0.0</strong></span>
<span class="strong"><strong>…</strong></span>
</pre></div><p>The <span class="strong"><strong>Receiver Operating Characteristic</strong></span> (<span class="strong"><strong>ROC</strong></span>) is a common measure of the classifier to be<a id="id310000000" class="indexterm"/> able to correctly rank the records <a id="id311000000" class="indexterm"/>according to their numeric label. We will consider precision metrics in more detail in <a class="link" title="Chapter 9. NLP in Scala" href="part0291.xhtml#aid-8LGJM2">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p><div class="note" title="Note"><h3 class="title"><a id="tip07000000"/>Tip</h3><p>
<span class="strong"><strong>What is ROC?</strong></span>
</p><p>ROC has <a id="id312000000" class="indexterm"/>emerged in signal processing with the first application to<a id="id313000000" class="indexterm"/> measure the accuracy of analog radars. The common measure of accuracy is area under ROC, which, shortly, is the probability of two randomly chosen points to be ranked correctly according to their labels (the <span class="emphasis"><em>0</em></span> label should always have a lower rank than the <span class="emphasis"><em>1</em></span> label). AUROC has a number of attractive characteristics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The value, at least theoretically, does not depend on the oversampling rate, that is, the rate at which we see <span class="emphasis"><em>0</em></span> labels as opposed to <span class="emphasis"><em>1</em></span> labels.</li><li class="listitem">The value does not depend on the sample size, excluding the expected variance due to the limited sample size.</li><li class="listitem">Adding a constant to the final score does not change the ROC, thus the intercept can always be set to <span class="emphasis"><em>0</em></span>. Computing the ROC requires a sort with respect to the generated score.</li></ul></div></div><p>Of course, separating the remaining two labels is a harder problem since the plane that separated <span class="emphasis"><em>Iris versicolor</em></span> from <span class="emphasis"><em>Iris virginica</em></span> does not exist: the AUROC score will be less than <span class="emphasis"><em>1.0</em></span>. However, the SVM method will find the plane that best differentiates between the latter two classes.</p></div><div class="section" title="Logistic regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec2900000"/>Logistic regression</h2></div></div></div><p>Logistic regression<a id="id314000000" class="indexterm"/> is one of the oldest classification<a id="id315000000" class="indexterm"/> methods. The outcome of the logistic regression is also a set of weights, which define the hyperplane, but the loss function is logistic loss instead of <span class="emphasis"><em>L2</em></span>:</p><div class="mediaobject"><img src="../Images/image01686.jpeg" alt="Logistic regression"/></div><p style="clear:both; height: 1em;"> </p><p>Logit function is a frequent choice when the label is binary (as <span class="emphasis"><em>y = +/- 1</em></span> in the above equation):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.MulticlassMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.MulticlassMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.6, 0.4))</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29</strong></span>
<span class="strong"><strong>scala&gt; val model = new LogisticRegressionWithLBFGS().setNumClasses(3).run(training)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 8, numClasses = 3, threshold = 0.5</strong></span>
<span class="strong"><strong>scala&gt; val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(features)</strong></span>
<span class="strong"><strong>     |   (prediction, label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[67] at map at &lt;console&gt;:37</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new MulticlassMetrics(predictionAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6d5254f3</strong></span>
<span class="strong"><strong>scala&gt; val precision = metrics.precision</strong></span>
<span class="strong"><strong>precision: Double = 0.9516129032258065</strong></span>
<span class="strong"><strong>scala&gt; println("Precision = " + precision)</strong></span>
<span class="strong"><strong>Precision = 0.9516129032258065</strong></span>
<span class="strong"><strong>scala&gt; model.intercept</strong></span>
<span class="strong"><strong>res5: Double = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>res7: org.apache.spark.mllib.linalg.Vector = [10.644978886788556,-26.850171485157578,3.852594349297618,8.74629386938248,4.288703063075211,-31.029289381858273,9.790312529377474,22.058196856491996]</strong></span>
</pre></div><p>The<a id="id316000000" class="indexterm"/> labels in this case can be any integer in the<a id="id317000000" class="indexterm"/> range <span class="emphasis"><em>[0, k)</em></span>, where <span class="emphasis"><em>k</em></span> is the total number of classes (the correct class will be determined by building multiple binary logistic regression models against the pivot class, which in this case, is the class with the <span class="emphasis"><em>0</em></span> label) (<span class="emphasis"><em>The Elements of Statistical Learning</em></span> by <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Jerome Friedman</em></span>, <span class="emphasis"><em>Springer Series in Statistics</em></span>).</p><p>The accuracy metric is precision, or the percentage of records predicted correctly (which is 95% in our case).</p></div><div class="section" title="Decision tree"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec3000000"/>Decision tree</h2></div></div></div><p>The <a id="id318000000" class="indexterm"/>preceding two methods describe linear models. Unfortunately, the <a id="id319000000" class="indexterm"/>linear approach does not always work for complex interactions between attributes. Assume that the label looks like an exclusive <span class="emphasis"><em>OR: 0</em></span> if <span class="emphasis"><em>X ? Y</em></span> and <span class="emphasis"><em>1</em></span> if <span class="emphasis"><em>X = Y</em></span>:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>X</p>
</th><th valign="bottom">
<p>Y</p>
</th><th valign="bottom">
<p>Label</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>1</p>
</td><td valign="top">
<p>0</p>
</td><td valign="top">
<p>0</p>
</td></tr><tr><td valign="top">
<p>0</p>
</td><td valign="top">
<p>1</p>
</td><td valign="top">
<p>0</p>
</td></tr><tr><td valign="top">
<p>1</p>
</td><td valign="top">
<p>1</p>
</td><td valign="top">
<p>1</p>
</td></tr><tr><td valign="top">
<p>0</p>
</td><td valign="top">
<p>0</p>
</td><td valign="top">
<p>1</p>
</td></tr></tbody></table></div><p>There is no hyperplane that can differentiate between the two labels in the <span class="emphasis"><em>XY</em></span> space. Recursive split solution, where the split on each level is made on only one variable or a linear combination thereof might work a bit better in these case. Decision trees are also known to <a id="id320000000" class="indexterm"/>work well with sparse and interaction-rich <a id="id321000000" class="indexterm"/>datasets:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Algo.Classification</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.configuration.Algo.Classification</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3), 11L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:30, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:30)</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
<span class="strong"><strong>trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:30</strong></span>
<span class="strong"><strong>testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:30</strong></span>
<span class="strong"><strong>scala&gt; val strategy = new Strategy(Classification, Gini, 10, 3, 10)</strong></span>
<span class="strong"><strong>strategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@4110e631</strong></span>
<span class="strong"><strong>scala&gt; val dt = new DecisionTree(strategy)</strong></span>
<span class="strong"><strong>dt: org.apache.spark.mllib.tree.DecisionTree = org.apache.spark.mllib.tree.DecisionTree@33d89052</strong></span>
<span class="strong"><strong>scala&gt; val model = dt.run(trainingData)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 6 with 21 nodes</strong></span>
<span class="strong"><strong>scala&gt; val labelAndPreds = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (point.label, prediction)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>labelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[32] at map at &lt;console&gt;:36</strong></span>
<span class="strong"><strong>scala&gt; val testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count.toDouble / testData.count()</strong></span>
<span class="strong"><strong>testErr: Double = 0.02631578947368421</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
<span class="strong"><strong>Test Error = 0.02631578947368421</strong></span>

<span class="strong"><strong>scala&gt; println("Learned classification tree model:\n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned classification tree model:</strong></span>
<span class="strong"><strong>DecisionTreeModel classifier of depth 6 with 21 nodes</strong></span>
<span class="strong"><strong>  If (feature 3 &lt;= 0.4)</strong></span>
<span class="strong"><strong>   Predict: 0.0</strong></span>
<span class="strong"><strong>  Else (feature 3 &gt; 0.4)</strong></span>
<span class="strong"><strong>   If (feature 3 &lt;= 1.7)</strong></span>
<span class="strong"><strong>    If (feature 2 &lt;= 4.9)</strong></span>
<span class="strong"><strong>     If (feature 0 &lt;= 5.3)</strong></span>
<span class="strong"><strong>      If (feature 1 &lt;= 2.8)</strong></span>
<span class="strong"><strong>       If (feature 2 &lt;= 3.9)</strong></span>
<span class="strong"><strong>        Predict: 1.0</strong></span>
<span class="strong"><strong>       Else (feature 2 &gt; 3.9)</strong></span>
<span class="strong"><strong>        Predict: 2.0</strong></span>
<span class="strong"><strong>      Else (feature 1 &gt; 2.8)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 5.3)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 2 &gt; 4.9)</strong></span>
<span class="strong"><strong>     If (feature 0 &lt;= 6.0)</strong></span>
<span class="strong"><strong>      If (feature 1 &lt;= 2.4)</strong></span>
<span class="strong"><strong>       Predict: 2.0</strong></span>
<span class="strong"><strong>      Else (feature 1 &gt; 2.4)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 6.0)</strong></span>
<span class="strong"><strong>      Predict: 2.0</strong></span>
<span class="strong"><strong>   Else (feature 3 &gt; 1.7)</strong></span>
<span class="strong"><strong>    If (feature 2 &lt;= 4.9)</strong></span>
<span class="strong"><strong>     If (feature 1 &lt;= 3.0)</strong></span>
<span class="strong"><strong>      Predict: 2.0</strong></span>
<span class="strong"><strong>     Else (feature 1 &gt; 3.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 2 &gt; 4.9)</strong></span>
<span class="strong"><strong>     Predict: 2.0</strong></span>
<span class="strong"><strong>scala&gt; model.save(sc, "dt-model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>As you<a id="id322000000" class="indexterm"/> can see, the error (misprediction) rate on hold-out 30% sample is only 2.6%. The 30% sample of 150 is only 45 records, which means we missed <a id="id323000000" class="indexterm"/>only 1 record from the whole test set. Certainly, the result might and will change with a different seed, and we need a more rigorous cross-validation technique to prove the accuracy of the model, but this is enough for a rough estimate of model performance.</p><p>Decision tree generalizes on regression case, that is, when the label is continuous in nature. In this case, the splitting criterion is minimization of weighted variance, as opposed to entropy gain or gini in the case of classification. I will talk more about the differences in <a class="link" title="Chapter 5. Regression and Classification" href="part0260.xhtml#aid-7NUI81">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>.</p><p>There are a number of parameters, which can be tuned to improve the performance:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Parameter</p>
</th><th valign="bottom">
<p>Description</p>
</th><th valign="bottom">
<p>Recommended value</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<code class="literal">maxDepth</code>
</p>
</td><td valign="top">
<p>This is<a id="id324000000" class="indexterm"/> the maximum depth of the tree. Deep trees are costly and usually are more likely to overfit. Shallow trees are more efficient and better for bagging/boosting algorithms such as AdaBoost.</p>
</td><td valign="top">
<p>This depends on the size of the original dataset. It is worth experimenting and plotting the accuracy of the resulting tree versus the parameter to find out the optimum.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">minInstancesPerNode</code>
</p>
</td><td valign="top">
<p>This<a id="id325000000" class="indexterm"/> also limits the size of the tree: once the number of instances falls under this threshold, no further splitting occurs.</p>
</td><td valign="top">
<p>The value is usually 10-100, depending on the complexity of the original dataset and the number of potential labels.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">maxBins</code>
</p>
</td><td valign="top">
<p>This is<a id="id326000000" class="indexterm"/> used only for continuous attributes: the number of bins to split the original range.</p>
</td><td valign="top">
<p>Large number of bins increase computation and communication cost. One can also consider the option of pre-discretizing the attribute based on domain knowledge.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">minInfoGain</code>
</p>
</td><td valign="top">
<p>This<a id="id327000000" class="indexterm"/> is the amount of information gain (entropy), impurity (gini), or variance (regression) gain to split a node.</p>
</td><td valign="top">
<p>The default is <span class="emphasis"><em>0</em></span>, but you can increase the default to limit the tree size and reduce the risk of overfitting.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">maxMemoryInMB</code>
</p>
</td><td valign="top">
<p>This is<a id="id328000000" class="indexterm"/> the amount of memory to be used for collecting sufficient statistics.</p>
</td><td valign="top">
<p>The default value is conservatively chosen to be 256 MB to allow the decision algorithm to work in most scenarios. Increasing <code class="literal">maxMemoryInMB</code> can lead to faster training (if the memory is available) by allowing fewer passes over the data. However, there may be decreasing returns as <code class="literal">maxMemoryInMB</code> grows, as the amount of communication on each iteration can be proportional to <code class="literal">maxMemoryInMB</code>.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">subsamplingRate</code>
</p>
</td><td valign="top">
<p>This is <a id="id329000000" class="indexterm"/>the fraction of the training data used for learning the decision tree.</p>
</td><td valign="top">
<p>This parameter is most relevant for training ensembles of trees (using <code class="literal">RandomForest</code> and <code class="literal">GradientBoostedTrees</code>), where it can be useful to subsample the original data. For training a single decision tree, this parameter is less useful since the number of training instances is generally not the main constraint.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">useNodeIdCache</code>
</p>
</td><td valign="top">
<p>If this <a id="id330000000" class="indexterm"/>is set to true, the algorithm will avoid passing the current model (tree or trees) to executors on each iteration.</p>
</td><td valign="top">
<p>This can be useful with deep trees (speeding up computation on workers) and for large random forests (reducing communication on each iteration).</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">checkpointDir:</code>
</p>
</td><td valign="top">
<p>This is <a id="id331000000" class="indexterm"/>the directory for checkpointing the node ID cache RDDs.</p>
</td><td valign="top">
<p>This is an optimization to save intermediate results to avoid recomputation in case of node failure. Set it in large clusters or with unreliable nodes.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">checkpointInterval</code>
</p>
</td><td valign="top">
<p>This is<a id="id332000000" class="indexterm"/> the frequency for checkpointing the node ID cache RDDs.</p>
</td><td valign="top">
<p>Setting this too low will cause extra overhead from writing to HDFS and setting this too high can cause problems if executors fail and the RDD needs to be recomputed.</p>
</td></tr></tbody></table></div></div><div class="section" title="Bagging and boosting – ensemble learning methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec3100000"/>Bagging and boosting – ensemble learning methods</h2></div></div></div><p>As a portfolio of stocks has better characteristics compared to individual equities, models can be<a id="id333000000" class="indexterm"/> combined to produce better <a id="id334000000" class="indexterm"/>classifiers. Usually, these methods work really well with decision trees as the training technique can be modified to produce models with large variations. One way is to train the model on random subsets of the original data or random subsets of attributes, which is called random forest. Another way is to generate a sequence of models, where misclassified instances are reweighted to get a larger weight in each subsequent iteration. It has been shown that this method has a relation to gradient descent methods in the model parameter space. While these are valid and interesting techniques, they usually require much more space in terms of model storage and <a id="id335000000" class="indexterm"/>are less interpretable compared to<a id="id336000000" class="indexterm"/> bare decision tree models. For Spark, the <a id="id337000000" class="indexterm"/>ensemble models are currently under development—the umbrella issue is SPARK-3703 (<a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-3703">https://issues.apache.org/jira/browse/SPARK-3703</a>).</p></div></div></div>
<div class="section" title="Unsupervised learning"><div class="titlepage" id="aid-7L30I2"><div><div><h1 class="title"><a id="ch04lvl1sec3000000"/>Unsupervised learning</h1></div></div></div><p>If we get rid <a id="id338000000" class="indexterm"/>of the label in the Iris dataset, it would be nice if some algorithm could recover the original grouping, maybe without the exact label names—<span class="emphasis"><em>setosa</em></span>, <span class="emphasis"><em>versicolor</em></span>, and <span class="emphasis"><em>virginica</em></span>. Unsupervised learning has multiple applications in compression and encoding, CRM, recommendation engines, and security to uncover internal structure without actually having the exact labels. The labels sometimes can be given base on the singularity in attribute value distributions. For example, <span class="emphasis"><em>Iris setosa</em></span> can be described as a <span class="emphasis"><em>Flower with Small Leaves</em></span>.</p><p>While a supervised learning problem can always be cast as unsupervised by disregarding the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation problem by assigning label <span class="emphasis"><em>1</em></span> to all vectors and generating random vectors with label <span class="emphasis"><em>0</em></span> (<span class="emphasis"><em>The Elements of Statistical Learning</em></span> by <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Jerome Friedman</em></span>, <span class="emphasis"><em>Springer Series in Statistics</em></span>). The difference between the two is formal and it's even fuzzier with non-structured and nested data. Often, running unsupervised algorithms in labeled datasets leads to a better understanding of the dependencies and thus a better selection and performance of the supervised algorithm.</p><p>One of the most popular algorithms for clustering and unsupervised learning in k-means (and its variants, k-median and k-center, will be described later):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell</strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>
<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; val iris = sc.textFile("iris.txt")</strong></span>
<span class="strong"><strong>iris: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at &lt;console&gt;:23</strong></span>

<span class="strong"><strong>scala&gt; val vectors = data.map(s =&gt; Vectors.dense(s.split('\t').map(_.toDouble))).cache()</strong></span>
<span class="strong"><strong>vectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[5] at map at &lt;console&gt;:25</strong></span>

<span class="strong"><strong>scala&gt; val numClusters = 3</strong></span>
<span class="strong"><strong>numClusters: Int = 3</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 20</strong></span>
<span class="strong"><strong>numIterations: Int = 20</strong></span>
<span class="strong"><strong>scala&gt; val clusters = KMeans.train(vectors, numClusters, numIterations)</strong></span>
<span class="strong"><strong>clusters: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@5dc9cb99</strong></span>
<span class="strong"><strong>scala&gt; val centers = clusters.clusterCenters</strong></span>
<span class="strong"><strong>centers: Array[org.apache.spark.mllib.linalg.Vector] = Array([5.005999999999999,3.4180000000000006,1.4640000000000002,0.2439999999999999], [6.8538461538461535,3.076923076923076,5.715384615384614,2.0538461538461537], [5.883606557377049,2.740983606557377,4.388524590163936,1.4344262295081966])</strong></span>
<span class="strong"><strong>scala&gt; val SSE = clusters.computeCost(vectors)</strong></span>
<span class="strong"><strong>WSSSE: Double = 78.94506582597859</strong></span>
<span class="strong"><strong>scala&gt; vectors.collect.map(x =&gt; clusters.predict(x))</strong></span>
<span class="strong"><strong>res18: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2)</strong></span>
<span class="strong"><strong>scala&gt; println("Sum of Squared Errors = " + SSE)</strong></span>
<span class="strong"><strong>Sum of Squared Errors = 78.94506582597859</strong></span>
<span class="strong"><strong>scala&gt; clusters.save(sc, "model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>One can see that the first center, the one with index <code class="literal">0</code>, has petal length and width of <code class="literal">1.464</code> and <code class="literal">0.244</code>, which is much shorter than the other two—<code class="literal">5.715</code> and <code class="literal">2.054</code>, <code class="literal">4.389</code> and <code class="literal">1.434</code>). The prediction completely matches the first cluster, corresponding to <span class="emphasis"><em>Iris setosa</em></span>, but has a few mispredictions for the other two.</p><p>The measure <a id="id339000000" class="indexterm"/>of cluster quality might depend on the (desired) labels if we want to achieve a desired classification result, but since the algorithm has no information about the labeling, a more common measure is the sum of distances from centroids to the points in each of the clusters. Here is a graph of <code class="literal">WSSSE</code>, depending on the number of clusters:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; 1.to(10).foreach(i =&gt; println("i: " + i + " SSE: " + KMeans.train(vectors, i, numIterations).computeCost(vectors)))</strong></span>
<span class="strong"><strong>i: 1 WSSSE: 680.8244</strong></span>
<span class="strong"><strong>i: 2 WSSSE: 152.3687064773393</strong></span>
<span class="strong"><strong>i: 3 WSSSE: 78.94506582597859</strong></span>
<span class="strong"><strong>i: 4 WSSSE: 57.47327326549501</strong></span>
<span class="strong"><strong>i: 5 WSSSE: 46.53558205128235</strong></span>
<span class="strong"><strong>i: 6 WSSSE: 38.9647878510374</strong></span>
<span class="strong"><strong>i: 7 WSSSE: 34.311167589868646</strong></span>
<span class="strong"><strong>i: 8 WSSSE: 32.607859500805034</strong></span>
<span class="strong"><strong>i: 9 WSSSE: 28.231729411088438</strong></span>
<span class="strong"><strong>i: 10 WSSSE: 29.435054384424078</strong></span>
</pre></div><p>As expected, the average distance is decreasing as more clusters are configured. A common method to determine the optimal number of clusters—in our example, we know that there are three types of flowers—is to add a penalty function. A common penalty is the log of the number of clusters as we expect a convex function. What would be the coefficient in front of log? If each vector is associated with its own cluster, the sum of all distances will be zero, so if we would like a metric that achieves approximately the same value at both ends of the set of possible values, <code class="literal">1</code> to <code class="literal">150</code>, the coefficient should be <code class="literal">680.8244/log(150)</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; for (i &lt;- 1.to(10)) println(i + " -&gt; " + ((KMeans.train(vectors, i, numIterations).computeCost(vectors)) + 680 * scala.math.log(i) / scala.math.log(150)))</strong></span>
<span class="strong"><strong>1 -&gt; 680.8244</strong></span>
<span class="strong"><strong>2 -&gt; 246.436635016484</strong></span>
<span class="strong"><strong>3 -&gt; 228.03498068120865</strong></span>
<span class="strong"><strong>4 -&gt; 245.48126639400738</strong></span>
<span class="strong"><strong>5 -&gt; 264.9805962616268</strong></span>
<span class="strong"><strong>6 -&gt; 285.48857890531764</strong></span>
<span class="strong"><strong>7 -&gt; 301.56808340425164</strong></span>
<span class="strong"><strong>8 -&gt; 315.321639004243</strong></span>
<span class="strong"><strong>9 -&gt; 326.47262191671723</strong></span>
<span class="strong"><strong>10 -&gt; 344.87130979355675</strong></span>
</pre></div><p>Here is how the sum of the squared distances with penalty looks as a graph:</p><div class="mediaobject"><img src="../Images/image01687.jpeg" alt="Unsupervised learning"/><div class="caption"><p>Figure 04-2. The measure of the clustering quality as a function of the number of clusters</p></div></div><p style="clear:both; height: 1em;"> </p><p>Besides <a id="id340000000" class="indexterm"/>k-means clustering, MLlib also has implementations of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Gaussian mixture</li><li class="listitem"><span class="strong"><strong>Power Iteration Clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>)</li><li class="listitem"><span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</li><li class="listitem">Streaming k-means</li></ul></div><p>The<a id="id341000000" class="indexterm"/> Gaussian mixture is another classical mechanism, particularly<a id="id342000000" class="indexterm"/> known for spectral analysis. Gaussian <a id="id343000000" class="indexterm"/>mixture decomposition is appropriate, where the attributes are continuous and we know that they are likely to come from a set of Gaussian distributions. For example, while the potential groups of points corresponding to clusters may have the average for all attributes, say <span class="strong"><strong>Var1</strong></span> and <span class="strong"><strong>Var2</strong></span>, the points might be centered around two intersecting hyperplanes, as shown in the following diagram:</p><div class="mediaobject"><img src="../Images/image01688.jpeg" alt="Unsupervised learning"/><div class="caption"><p>Figure 04-3. A mixture of two Gaussians that cannot be properly described by k-means clustering</p></div></div><p style="clear:both; height: 1em;"> </p><p>This renders the k-means algorithm ineffective as it will not be able to distinguish between the two (of course a simple non-linear transformation such as a distance to one of the<a id="id344000000" class="indexterm"/> hyperplanes will solve the problem, but this is where domain knowledge and expertise as a data scientist are handy).</p><p>PIC is using<a id="id345000000" class="indexterm"/> clustering vertices of a graph provided pairwise similarity measures given as edge properties. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. MLlib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (<code class="literal">srcId</code>, <code class="literal">dstId</code>, similarity) tuples and outputs a model with the clustering assignments. The similarities must be non-negative. PIC assumes that the similarity measure is symmetric. A pair (<code class="literal">srcId</code>, <code class="literal">dstId</code>) regardless of the ordering should appear at most once in the input data. If a pair is missing from the input, their similarity is treated as zero.</p><p>LDA can be <a id="id346000000" class="indexterm"/>used for clustering documents based on keyword frequencies. Rather than estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated.</p><p>Finally, streaming k-means is a modification of the k-means algorithm, where the clusters can be adjusted with <a id="id347000000" class="indexterm"/>new batches of data. For each batch of data, we assign all points to their nearest cluster, compute new cluster centers based on the assignment, and then update each cluster parameters using the equations:</p><div class="mediaobject"><img src="../Images/image01689.jpeg" alt="Unsupervised learning"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01690.jpeg" alt="Unsupervised learning"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>c</em></span>
<span class="emphasis"><em>t</em></span> and <span class="emphasis"><em>c'</em></span>
<span class="emphasis"><em>t</em></span> are the centers of from the old model and the ones computed for the new batch and <span class="emphasis"><em>n</em></span>
<span class="emphasis"><em>t</em></span> and <span class="emphasis"><em>n'</em></span>
<span class="emphasis"><em>t</em></span> are the number of vectors from the old model and for the new batch. By changing the <span class="emphasis"><em>a</em></span> parameter, we can control how much information from the old runs can<a id="id348000000" class="indexterm"/> influence the clustering—<span class="emphasis"><em>0</em></span> means the new cluster centers are totally based on the points in the new batch, while <span class="emphasis"><em>1</em></span> means that we accommodate for all points that we have seen so far.</p><p>k-means clustering<a id="id349000000" class="indexterm"/> has many modifications. For example, k-medians computes the cluster centers as medians of the attribute values, not mean, which works much better for some distributions and with <span class="emphasis"><em>L1</em></span> target distance metric (absolute value of the difference) as opposed to <span class="emphasis"><em>L2</em></span> (the sum of squares). K-medians centers are not necessarily present as a specific point in the dataset. K-medoids is another algorithm from the same family, where the resulting cluster center has to be an actual instance in the input set and we actually do not need to have the global sort, only the pairwise distances between the points. Many variations of the techniques exist on how to choose the original seed cluster centers and converge on the optimal number of clusters (besides the simple log trick I have shown).</p><p>Another big class of clustering algorithms is hierarchical clustering. Hierarchical clustering is either done from the top—akin to the decision tree algorithms—or from the bottom; we first find the closest neighbors, pair them, and continue the pairing process up the hierarchy until all records are merged. The advantage of hierarchical clustering is that it can be made deterministic and relatively fast, even though the cost of one iteration in k-means is probably going to be better. However, as mentioned, the unsupervised problem can actually <a id="id350000000" class="indexterm"/>be converted to a density-estimation supervised problem, with all the supervised learning techniques available. So have fun understanding the data!</p></div>
<div class="section" title="Problem dimensionality"><div class="titlepage" id="aid-7M1H42"><div><div><h1 class="title"><a id="ch04lvl1sec3100000"/>Problem dimensionality</h1></div></div></div><p>The larger the attribute space or the number of dimensions, the harder it is to usually predict the label for a given combination of attribute values. This is mostly due to the fact that the total number of possible distinct combinations of attributes increases exponentially <a id="id351000000" class="indexterm"/>with the dimensionality of the attribute space—at least in the case of discrete variables (in case of continuous variables, the situation is more complex and depends on the metrics used), and it is becoming harder to generalize.</p><p>The effective dimensionality of the problem might be different from the dimensionality of the input space. For example, if the label depends only on the linear combination of the (continuous) input attributes, the problem is called linearly separable and its internal dimensionality is one—we still have to find the coefficients for this linear combination like in logistic regression though.</p><p>This idea is also<a id="id352000000" class="indexterm"/> sometimes referred to as a <span class="strong"><strong>Vapnik–Chervonenkis</strong></span> (<span class="strong"><strong>VC</strong></span>) dimension of a problem, model, or algorithm—the expressive power of the model depending on how complex the dependencies that it can solve, or shatter, might be. More complex problems require algorithms with higher VC dimensions and larger training sets. However, using an algorithm with higher VC dimension on a simple problem can lead to overfitting and worse generalization to new data.</p><p>If the units of input attributes are comparable, say all of them are meters or units of time, PCA, or more generally, kernel methods, can be used to reduce the dimensionality of the input space:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.PCA</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.PCA</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val pca = new PCA(2).fit(data.map(_.features))</strong></span>
<span class="strong"><strong>pca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@4eee0b1a</strong></span>

<span class="strong"><strong>scala&gt; val reduced = data.map(p =&gt; p.copy(features = pca.transform(p.features)))</strong></span>
<span class="strong"><strong>reduced: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[311] at map at &lt;console&gt;:39</strong></span>
<span class="strong"><strong>scala&gt; reduced.collect().take(10)</strong></span>
<span class="strong"><strong>res4: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[-2.827135972679021,-5.641331045573367]), (0.0,[-2.7959524821488393,-5.145166883252959]), (0.0,[-2.621523558165053,-5.177378121203953]), (0.0,[-2.764905900474235,-5.0035994150569865]), (0.0,[-2.7827501159516546,-5.6486482943774305]), (0.0,[-3.231445736773371,-6.062506444034109]), (0.0,[-2.6904524156023393,-5.232619219784292]), (0.0,[-2.8848611044591506,-5.485129079769268]), (0.0,[-2.6233845324473357,-4.743925704477387]), (0.0,[-2.8374984110638493,-5.208032027056245]))</strong></span>

<span class="strong"><strong>scala&gt; import scala.language.postfixOps</strong></span>
<span class="strong"><strong>import scala.language.postfixOps</strong></span>

<span class="strong"><strong>scala&gt; pca pc</strong></span>
<span class="strong"><strong>res24: org.apache.spark.mllib.linalg.DenseMatrix = </strong></span>
<span class="strong"><strong>-0.36158967738145065  -0.6565398832858496  </strong></span>
<span class="strong"><strong>0.08226888989221656   -0.7297123713264776  </strong></span>
<span class="strong"><strong>-0.856572105290527    0.17576740342866465  </strong></span>
<span class="strong"><strong>-0.35884392624821626  0.07470647013502865</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; val splits = reduced.randomSplit(Array(0.6, 0.4), seed = 1L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[312] at randomSplit at &lt;console&gt;:44, MapPartitionsRDD[313] at randomSplit at &lt;console&gt;:44)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[312] at randomSplit at &lt;console&gt;:44</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[313] at randomSplit at &lt;console&gt;:44</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 100</strong></span>
<span class="strong"><strong>numIterations: Int = 100</strong></span>
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(training, numIterations)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.clearThreshold()</strong></span>
<span class="strong"><strong>res30: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = None</strong></span>
<span class="strong"><strong>scala&gt; val scoreAndLabels = test.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val score = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (score, point.label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[517] at map at &lt;console&gt;:54</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new BinaryClassificationMetrics(scoreAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27f49b8c</strong></span>

<span class="strong"><strong>scala&gt; val auROC = metrics.areaUnderROC()</strong></span>
<span class="strong"><strong>auROC: Double = 1.0</strong></span>
<span class="strong"><strong>scala&gt; println("Area under ROC = " + auROC)</strong></span>
<span class="strong"><strong>Area under ROC = 1.0</strong></span>
</pre></div><p>Here, we <a id="id353000000" class="indexterm"/>reduced the original four-dimensional problem to two-dimensional. Like averaging, computing linear combinations of input attributes and selecting only those that describe most of the variance helps to reduce noise.</p></div>
<div class="section" title="Summary" id="aid-7N01M1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec3200000"/>Summary</h1></div></div></div><p>In this chapter, we looked at supervised and unsupervised learning and a few examples of how to run them in Spark/Scala. We considered SVM, logistic regression, decision tree, and k-means in the example of UCI Iris dataset. This is in no way a complete guide, and many other libraries either exist or are being made as we speak, but I would bet that you can solve 99% of the immediate data analysis problems just with these tools.</p><p>This will give you a very fast shortcut on how to start being productive with a new dataset. There are many other ways to look at the datasets, but before we get into more advanced topics, let's discuss regression and classification in the next chapter, that is, how to predict continuous and discrete labels.</p></div>
<div class="chapter" title="Chapter&#xA0;5.&#xA0;Regression and Classification" id="aid-7NUI81"><div class="titlepage"><div><div><h1 class="title"><a id="ch31"/>Chapter 5. Regression and Classification</h1></div></div></div><p>In the previous chapter, we got familiar with supervised and unsupervised learning. Another standard taxonomy of the machine learning methods is based on the label is from continuous or discrete space. Even if the discrete labels are ordered, there is a significant difference, particularly how the goodness of fit metrics is evaluated.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Learning about the origin of the word regression</li><li class="listitem">Learning metrics for evaluating the goodness of fit in continuous and discrete space</li><li class="listitem">Discussing how to write simple code in Scala for linear and logistic regression</li><li class="listitem">Learning about advanced concepts such as regularization, multiclass predictions, and heteroscedasticity</li><li class="listitem">Discussing an example of MLlib application for regression tree analysis</li><li class="listitem">Learning about the different ways of evaluating classification models</li></ul></div><div class="section" title="What regression stands for?"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec3300000"/>What regression stands for?</h1></div></div></div><p>While the word<a id="id354000000" class="indexterm"/> classification is intuitively clear, the word regression does not seem to imply a predictor of a continuous label. According to the Webster dictionary, regression is:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"a return to a former or less developed state."</em></span></p></blockquote></div><p>It does also mention a special definition for statistics as <span class="emphasis"><em>a measure of the relation between the mean value of one variable (for example, output) and corresponding values of other variables (for example, time and cost)</em></span>, which is actually correct these days. However, historically, the regression coefficient was meant to signify the hereditability of certain characteristics, such as weight and size, from one generation to another, with the hint of planned gene selection, including humans (<a class="ulink" href="http://www.amstat.org/publications/jse/v9n3/stanton.html">http://www.amstat.org/publications/jse/v9n3/stanton.html</a>). More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished 19th-century scientist in his own right, which was also widely criticized for the promotion of eugenics, had distributed packets of sweet pea seeds to seven friends. Each friend received seeds of uniform weight, but with substantial variation across the seven packets. Galton's friends were supposed to harvest the next generation seeds and ship them back to him. Galton then proceeded to analyze the statistical properties of the seeds within each group, and one of the analysis was to plot the regression line, which always appeared to <a id="id355000000" class="indexterm"/>have the slope less than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance (5th ed.), New York: Macmillan and Company), as opposed to either <span class="emphasis"><em>0</em></span>, in the case of no correlation and no inheritance; or <span class="emphasis"><em>1</em></span>, in the case the total replication of the parent's characteristics in the descendants. We will discuss why the coefficient of the regression line should always be less than <span class="emphasis"><em>1</em></span> in the presence of noise in the data, even if the correlation is perfect. However, beyond the discussion and details, the origin of the term regression is partly due to planned breeding of plants and humans. Of course, Galton did not have access to PCA, Scala, or any other computing machinery at the time, which might shed more light on the differences between correlation and the slope of the regression line.</p></div></div>
<div class="section" title="Continuous space and metrics"><div class="titlepage" id="aid-7OT2Q2"><div><div><h1 class="title"><a id="ch05lvl1sec3400000"/>Continuous space and metrics</h1></div></div></div><p>As most of this chapter's content will be dealing with trying to predict or optimize continuous variables, let's first understand how to measure the difference in a continuous space. Unless <a id="id356000000" class="indexterm"/>a drastically new discovery is made pretty soon, the space we <a id="id357000000" class="indexterm"/>live in is a three-dimensional Euclidian space. Whether we like it or not, this is the world we are mostly comfortable with today. We can completely specify our location with three continuous numbers. The difference in locations is usually measured by distance, or a metric, which is a function of a two arguments that returns a single positive real number. Naturally, the distance, <span class="inlinemediaobject"><img src="../Images/image01691.jpeg" alt="Continuous space and metrics"/></span>, between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> should always be equal or smaller than the sum of distances between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Z</em></span> and <span class="emphasis"><em>Y</em></span> and <span class="emphasis"><em>Z</em></span>:</p><div class="mediaobject"><img src="../Images/image01692.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><p>For any <span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span>, and <span class="emphasis"><em>Z</em></span>, which is also <a id="id358000000" class="indexterm"/>called triangle inequality. The two other properties of a metric is symmetry:</p><div class="mediaobject"><img src="../Images/image01693.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><p>Non-negativity of distance:</p><div class="mediaobject"><img src="../Images/image01694.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01695.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the <a id="id359000000" class="indexterm"/>metric is <code class="literal">0</code> if, and only if, <span class="emphasis"><em>X=Y</em></span>. The <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Continuous space and metrics"/></span> distance is the<a id="id360000000" class="indexterm"/> distance as we understand it in everyday life, the square root of the sum of the squared differences along each of the dimensions. A generalization of our physical distance is p-norm (<span class="emphasis"><em>p = 2</em></span> for the <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Continuous space and metrics"/></span> distance):</p><div class="mediaobject"><img src="../Images/image01697.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the sum is the overall components of the <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> vectors. If <span class="emphasis"><em>p=1</em></span>, the 1-norm is the sum of absolute differences, or Manhattan distance, as if the only path from point <span class="emphasis"><em>X</em></span> to point <span class="emphasis"><em>Y</em></span> would be to move only along one of the components. This distance is also often referred to as <span class="inlinemediaobject"><img src="../Images/image01698.jpeg" alt="Continuous space and metrics"/></span> distance:</p><div class="mediaobject"><img src="../Images/image01699.jpeg" alt="Continuous space and metrics"/><div class="caption"><p>Figure 05-1. The <span class="inlinemediaobject"><img src="../Images/image01698.jpeg" alt="Continuous space and metrics"/></span> circle in two-dimensional space (the set of points exactly one unit from the origin (0, 0))</p></div></div><p style="clear:both; height: 1em;"> </p><p>Here is a<a id="id361000000" class="indexterm"/> representation <a id="id362000000" class="indexterm"/>of a circle in a two-dimensional space:</p><div class="mediaobject"><img src="../Images/image01700.jpeg" alt="Continuous space and metrics"/><div class="caption"><p>Figure 05-2. <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Continuous space and metrics"/></span> circle in two-dimensional space (the set of points equidistant from the origin (0, 0)), which actually looks like a circle in our everyday understanding of distance.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Another <a id="id363000000" class="indexterm"/>frequently <a id="id364000000" class="indexterm"/>used special case is <span class="inlinemediaobject"><img src="../Images/image01701.jpeg" alt="Continuous space and metrics"/></span>, the limit when <span class="inlinemediaobject"><img src="../Images/image01702.jpeg" alt="Continuous space and metrics"/></span>, which is the maximum deviation along any of the components, as follows:</p><div class="mediaobject"><img src="../Images/image01703.jpeg" alt="Continuous space and metrics"/></div><p style="clear:both; height: 1em;"> </p><p>The equidistant circle for the <span class="inlinemediaobject"><img src="../Images/image01701.jpeg" alt="Continuous space and metrics"/></span> distance is shown in <span class="emphasis"><em>Figure 05-3</em></span>:</p><div class="mediaobject"><img src="../Images/image01704.jpeg" alt="Continuous space and metrics"/><div class="caption"><p>Figure 05-3. <span class="inlinemediaobject"><img src="../Images/image01701.jpeg" alt="Continuous space and metrics"/></span> circle in two-dimensional space (the set of points equidistant from the origin (0, 0)). This is a square as the <span class="inlinemediaobject"><img src="../Images/image01701.jpeg" alt="Continuous space and metrics"/></span> metric is the maximum distance along any of the components.</p></div></div><p style="clear:both; height: 1em;"> </p><p>I'll consider the <span class="strong"><strong>Kullback-Leibler</strong></span> (<span class="strong"><strong>KL</strong></span>) distance later when I talk about classification, which measures<a id="id365000000" class="indexterm"/> the difference between two probability distributions, but it is an example of distance that is not symmetric and thus it is not a metric.</p><p>The metric <a id="id366000000" class="indexterm"/>properties make it easier to decompose the problem. Due to<a id="id367000000" class="indexterm"/> the triangle inequality, one can potentially reduce a difficult problem of optimizing a goal by substituting it by a set of problems by optimizing along a number of dimensional components of the problem separately.</p></div>
<div class="section" title="Linear regression"><div class="titlepage" id="aid-7PRJC2"><div><div><h1 class="title"><a id="ch05lvl1sec3500000"/>Linear regression</h1></div></div></div><p>As explained in <a class="link" title="Chapter 2. Data Pipelines and Modeling" href="part0242.xhtml#aid-76P842">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>, most complex machine learning problems <a id="id368000000" class="indexterm"/>can be reduced to optimization as our final goal is to optimize the whole process where the machine is involved as an intermediary or the complete solution. The metric can be explicit, such as error rate, or more indirect, such as <span class="strong"><strong>Monthly Active Users</strong></span> (<span class="strong"><strong>MAU</strong></span>), but the effectiveness of an algorithm is<a id="id369000000" class="indexterm"/> finally judged by how it improves some metrics and processes in our lives. Sometimes, the goals may consist of multiple subgoals, or other metrics such as maintainability and stability might eventually be considered, but essentially, we need to either maximize or minimize a continuous metric in one or other way.</p><p>For the rigor of the flow, let's show how the linear regression can be formulated as an optimization problem. The classical linear regression needs to optimize the cumulative <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Linear regression"/></span> error rate:</p><div class="mediaobject"><img src="../Images/image01705.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01706.jpeg" alt="Linear regression"/></span> is the estimate given by a model, which, in the case of linear regression, is as follows:</p><div class="mediaobject"><img src="../Images/image01707.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>(Other potential <span class="strong"><strong>loss functions</strong></span><a id="id370000000" class="indexterm"/> have been enumerated in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>). As the <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Linear regression"/></span> metric is a differentiable convex function of <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>, the extreme value can be found by equating the derivative of the cumulative error rate to <code class="literal">0</code>:</p><div class="mediaobject"><img src="../Images/image01708.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>Computing the <a id="id371000000" class="indexterm"/>derivatives is straightforward in this case and leads to the following equation:</p><div class="mediaobject"><img src="../Images/image01709.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01710.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>This can be solved to give:</p><div class="mediaobject"><img src="../Images/image01711.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01712.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>avg()</em></span> denotes the average overall input records. Note that if <span class="emphasis"><em>avg(x)=0</em></span> the preceding equation is reduced to the following:</p><div class="mediaobject"><img src="../Images/image01713.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01714.jpeg" alt="Linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>So, we can quickly compute the linear regression coefficients using basic Scala operators (we can always make <span class="emphasis"><em>avg(x)</em></span> to be zero by performing a <span class="inlinemediaobject"><img src="../Images/image01715.jpeg" alt="Linear regression"/></span>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ scala</strong></span>

<span class="strong"><strong>Welcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import scala.util.Random</strong></span>
<span class="strong"><strong>import scala.util.Random</strong></span>

<span class="strong"><strong>scala&gt; val x = -5 to 5</strong></span>
<span class="strong"><strong>x: scala.collection.immutable.Range.Inclusive = Range(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)</strong></span>

<span class="strong"><strong>scala&gt; val y = x.map(_ * 2 + 4 + Random.nextGaussian)</strong></span>
<span class="strong"><strong>y: scala.collection.immutable.IndexedSeq[Double] = Vector(-4.317116812989753, -4.4056031270948015, -2.0376543660274713, 0.0184679796245639, 1.8356532746253016, 3.2322795591658644, 6.821999810895798, 7.7977904139852035, 10.288549406814154, 12.424126535332453, 13.611442206874917)</strong></span>

<span class="strong"><strong>scala&gt; val a = (x, y).zipped.map(_ * _).sum / x.map(x =&gt; x * x).sum</strong></span>
<span class="strong"><strong>a: Double = 1.9498665133868092</strong></span>

<span class="strong"><strong>scala&gt; val b = y.sum / y.size</strong></span>
<span class="strong"><strong>b: Double = 4.115448625564203</strong></span>
</pre></div><p>Didn't I inform <a id="id372000000" class="indexterm"/>you previously that Scala is a very concise language? We just did linear regression with five lines of code, three of which were just data-generation statements.</p><p>Although there are libraries written in Scala for performing (multivariate) linear regression, such <a id="id373000000" class="indexterm"/>as Breeze (<a class="ulink" href="https://github.com/scalanlp/breeze">https://github.com/scalanlp/breeze</a>), which provides a more extensive functionality, it is nice to be able to use pure Scala functionality to get some simple statistical results.</p><p>Let's look at the problem of Mr. Galton, where he found that the regression line always has the slope of less than one, which implies that we should always regress to some predefined mean. I will generate the same points as earlier, but they will be distributed along the horizontal line with some predefined noise. Then, I will rotate the line by <span class="emphasis"><em>45</em></span> degrees by doing a linear rotation transformation in the <span class="emphasis"><em>xy</em></span>-space. Intuitively, it should be clear that if <a id="id374000000" class="indexterm"/>anything, <span class="emphasis"><em>y</em></span> is strongly correlated with x and absent, the <span class="emphasis"><em>y</em></span> noise should be nothing else but <span class="emphasis"><em>x</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import scala.util.Random.nextGaussian</strong></span>
<span class="strong"><strong>import scala.util.Random.nextGaussian</strong></span>

<span class="strong"><strong>scala&gt; val x0 = Vector.fill(201)(100 * nextGaussian)</strong></span>
<span class="strong"><strong>x0: scala.collection.immutable.IndexedSeq[Double] = Vector(168.28831870102465, -40.56031270948016, -3.7654366027471324, 1.84679796245639, -16.43467253746984, -76.77204408341358, 82.19998108957988, -20.22095860147962, 28.854940681415442, 42.41265353324536, -38.85577931250823, -17.320873680820082, 64.19368427702135, -8.173507833084892, -198.6064655461397, 40.73700995880357, 32.36849515282444, 0.07758364225363915, -101.74032407199553, 34.789280276495646, 46.29624756866302, 35.54024768650289, 24.7867839701828, -11.931948933554782, 72.12437623460166, 30.51440227306552, -80.20756177356768, 134.2380548346385, 96.14401034937691, -205.48142161773896, -73.48186022765427, 2.7861465340245215, 39.49041527572774, 12.262899592863906, -118.30408039749234, -62.727048950163855, -40.58557796128219, -23.42...</strong></span>
<span class="strong"><strong>scala&gt; val y0 = Vector.fill(201)(30 * nextGaussian)</strong></span>
<span class="strong"><strong>y0: scala.collection.immutable.IndexedSeq[Double] = Vector(-51.675658534203876, 20.230770706186128, 32.47396891906855, -29.35028743620815, 26.7392929946199, 49.85681312583139, 24.226102932450917, 31.19021547086266, 26.169544117916704, -4.51435617676279, 5.6334117227063985, -59.641661744341775, -48.83082934374863, 29.655750956280304, 26.000847703123497, -17.43319605936741, 0.8354318740518344, 11.44787080976254, -26.26312164695179, 88.63863939038357, 45.795968719043785, 88.12442528090506, -29.829048945601635, -1.0417034396751037, -27.119245702417494, -14.055969115249258, 6.120344305721601, 6.102779172838027, -6.342516875566529, 0.06774080659895702, 46.364626315486014, -38.473161588561, -43.25262339890197, 19.77322736359687, -33.78364440355726, -29.085765762613683, 22.87698648100551, 30.53...</strong></span>
<span class="strong"><strong>scala&gt; val x1 = (x0, y0).zipped.map((a,b) =&gt; 0.5 * (a + b) )</strong></span>
<span class="strong"><strong>x1: scala.collection.immutable.IndexedSeq[Double] = Vector(58.30633008341039, -10.164771001647015, 14.354266158160707, -13.75174473687588, 5.152310228575029, -13.457615478791094, 53.213042011015396, 5.484628434691521, 27.51224239966607, 18.949148678241286, -16.611183794900917, -38.48126771258093, 7.681427466636357, 10.741121561597705, -86.3028089215081, 11.651906949718079, 16.601963513438136, 5.7627272260080895, -64.00172285947366, 61.71395983343961, 46.0461081438534, 61.83233648370397, -2.5211324877094174, -6.486826186614943, 22.50256526609208, 8.229216578908131, -37.04360873392304, 70.17041700373827, 44.90074673690519, -102.70684040557, -13.558616956084126, -17.843507527268237, -1.8811040615871129, 16.01806347823039, -76.0438624005248, -45.90640735638877, -8.85429574013834, 3.55536787...</strong></span>
<span class="strong"><strong>scala&gt; val y1 = (x0, y0).zipped.map((a,b) =&gt; 0.5 * (a - b) )</strong></span>
<span class="strong"><strong>y1: scala.collection.immutable.IndexedSeq[Double] = Vector(109.98198861761426, -30.395541707833143, -18.11970276090784, 15.598542699332269, -21.58698276604487, -63.31442860462248, 28.986939078564482, -25.70558703617114, 1.3426982817493691, 23.463504855004075, -22.244595517607316, 21.160394031760845, 56.51225681038499, -18.9146293946826, -112.3036566246316, 29.08510300908549, 15.7665316393863, -5.68514358375445, -37.73860121252187, -26.924679556943964, 0.2501394248096176, -26.292088797201085, 27.30791645789222, -5.445122746939839, 49.62181096850958, 22.28518569415739, -43.16395303964464, 64.06763783090022, 51.24326361247172, -102.77458121216895, -59.92324327157014, 20.62965406129276, 41.37151933731485, -3.755163885366482, -42.26021799696754, -16.820641593775086, -31.73128222114385, -26.9...</strong></span>
<span class="strong"><strong>scala&gt; val a = (x1, y1).zipped.map(_ * _).sum / x1.map(x =&gt; x * x).sum</strong></span>
<span class="strong"><strong>a: Double = 0.8119662470457414</strong></span>
</pre></div><p>The slope is<a id="id375000000" class="indexterm"/> only <code class="literal">0.81</code>! Note that if one runs PCA on the <code class="literal">x1</code> and <code class="literal">y1</code> data, the first principal component is correctly along the diagonal.</p><p>For completeness, I am giving a plot of (<span class="emphasis"><em>x1, y1</em></span>) zipped here:</p><div class="mediaobject"><img src="../Images/image01716.jpeg" alt="Linear regression"/><div class="caption"><p>Figure 05-4. The regression curve slope of a seemingly perfectly correlated dataset is less than one. This has to do with the metric the regression problem optimizes (y-distance).</p></div></div><p style="clear:both; height: 1em;"> </p><p>I will leave<a id="id376000000" class="indexterm"/> it to the reader to find the reason why the slope is less than one, but it has to do with the specific question the regression problem is supposed to answer and the metric it optimizes.</p></div>
<div class="section" title="Logistic regression" id="aid-7QQ3U1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec3600000"/>Logistic regression</h1></div></div></div><p>Logistic regression optimizes<a id="id377000000" class="indexterm"/> the logit loss function with respect to <span class="emphasis"><em>w</em></span>:</p><div class="mediaobject"><img src="../Images/image01717.jpeg" alt="Logistic regression"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>y</em></span> is binary (in this case plus or minus one). While there is no closed-form solution for the error minimization problem like there was in the previous case of linear regression, logistic function is differentiable and allows iterative algorithms that converge very fast.</p><p>The gradient is as follows:</p><div class="mediaobject"><img src="../Images/image01718.jpeg" alt="Logistic regression"/></div><p style="clear:both; height: 1em;"> </p><p>Again, we can quickly concoct a Scala program that uses the gradient to converge to the value, where <span class="inlinemediaobject"><img src="../Images/image01719.jpeg" alt="Logistic regression"/></span> (we use the MLlib <code class="literal">LabeledPoint</code> data structure only for convenience of reading the data):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vector</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vector</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.util._</strong></span>
<span class="strong"><strong>import org.apache.spark.util._</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util._</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util._</strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "data/iris/iris-libsvm.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[291] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; var w = Vector.random(4)</strong></span>
<span class="strong"><strong>w: org.apache.spark.util.Vector = (0.9515155226069267, 0.4901713461728122, 0.4308861351586426, 0.8030814804136821)</strong></span>

<span class="strong"><strong>scala&gt; for (i &lt;- 1.to(10)) println { val gradient = data.map(p =&gt; ( - p.label / (1+scala.math.exp(p.label*(Vector(p.features.toDense.values) dot w))) * Vector(p.features.toDense.values) )).reduce(_+_); w -= 0.1 * gradient; w }</strong></span>
<span class="strong"><strong>(-24.056553839570114, -16.585585503253142, -6.881629923278653, -0.4154730884796032)</strong></span>
<span class="strong"><strong>(38.56344616042987, 12.134414496746864, 42.178370076721365, 16.344526911520397)</strong></span>
<span class="strong"><strong>(13.533446160429868, -4.95558550325314, 34.858370076721364, 15.124526911520398)</strong></span>
<span class="strong"><strong>(-11.496553839570133, -22.045585503253143, 27.538370076721364, 13.9045269115204)</strong></span>
<span class="strong"><strong>(-4.002010810020908, -18.501520148476196, 32.506256310962314, 15.455945245916512)</strong></span>
<span class="strong"><strong>(-4.002011353029471, -18.501520429824225, 32.50625615219947, 15.455945209971787)</strong></span>
<span class="strong"><strong>(-4.002011896036225, -18.501520711171313, 32.50625599343715, 15.455945174027184)</strong></span>
<span class="strong"><strong>(-4.002012439041171, -18.501520992517463, 32.506255834675365, 15.455945138082699)</strong></span>
<span class="strong"><strong>(-4.002012982044308, -18.50152127386267, 32.50625567591411, 15.455945102138333)</strong></span>
<span class="strong"><strong>(-4.002013525045636, -18.501521555206942, 32.506255517153384, 15.455945066194088)</strong></span>

<span class="strong"><strong>scala&gt; w *= 0.24 / 4</strong></span>
<span class="strong"><strong>w: org.apache.spark.util.Vector = (-0.24012081150273815, -1.1100912933124165, 1.950375331029203, 0.9273567039716453)</strong></span>
</pre></div><p>The logistic regression was reduced to only one line of Scala code! The last line was to normalize the weights—only the relative values are important to define the separating plane—to <a id="id378000000" class="indexterm"/>compare them to the one obtained with the MLlib in previous chapter.</p><p>The <span class="strong"><strong>Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>SGD</strong></span>) algorithm used in the actual implementation is essentially<a id="id379000000" class="indexterm"/> the same gradient descent, but optimized in the following ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The actual gradient is computed on a subsample of records, which may lead to faster conversion due to less rounding noise and avoid local minima.</li><li class="listitem">
The step—a fixed <span class="emphasis"><em>0.1</em></span> in our case—is a monotonically decreasing function of the iteration as <span class="inlinemediaobject"><img src="../Images/image01720.jpeg" alt="Logistic regression"/></span>, which might also lead to better conversion.
</li><li class="listitem">It incorporates regularization; instead of minimizing just the loss function, you minimize the sum of the loss function, plus some penalty metric, which is a function of model complexity. I will discuss this in the following section.</li></ul></div></div>
<div class="section" title="Regularization" id="aid-7ROKG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec3700000"/>Regularization</h1></div></div></div><p>The regularization<a id="id380000000" class="indexterm"/> was originally developed to cope with ill-poised problems, where the problem was underconstrained—allowed multiple solutions given the data—or the data and the solution that contained too much noise (<span class="emphasis"><em>A.N. Tikhonov</em></span>, <span class="emphasis"><em>A.S. Leonov</em></span>, <span class="emphasis"><em>A.G. Yagola. Nonlinear Ill-Posed Problems</em></span>, <span class="emphasis"><em>Chapman and Hall</em></span>, <span class="emphasis"><em>London</em></span>, <span class="emphasis"><em>Weinhe</em></span>). Adding additional penalty function that skews a solution if it does not have a desired property, such as the smoothness in curve fitting or spectral analysis, usually solves the problem.</p><p>The choice of the penalty function is somewhat arbitrary, but it should reflect a desired skew in the solution. If the penalty function is differentiable, it can be incorporated into the gradient descent process; ridge regression is an example where the penalty is the <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Regularization"/></span>metric for the weights or the sum of squares of the coefficients.</p><p>MLlib currently implements <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Regularization"/></span>, <span class="inlinemediaobject"><img src="../Images/image01698.jpeg" alt="Regularization"/></span>, and a <a id="id381000000" class="indexterm"/>mixture thereof called <span class="strong"><strong>Elastic Net</strong></span>, as was shown in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>. The <span class="inlinemediaobject"><img src="../Images/image01698.jpeg" alt="Regularization"/></span> regularization effectively penalizes for the number of non-zero entries in the regression weights, but <a id="id382000000" class="indexterm"/>has been known to have slower convergence. <span class="strong"><strong>Least Absolute Shrinkage and Selection Operator</strong></span> (<span class="strong"><strong>LASSO</strong></span>) uses the <span class="inlinemediaobject"><img src="../Images/image01698.jpeg" alt="Regularization"/></span> regularization.</p><p>Another way<a id="id383000000" class="indexterm"/> to reduce the uncertainty in underconstrained problems is to take the prior information that may be coming from domain experts into account. This can be done using Bayesian analysis and introducing additional factors into the posterior probability—the probabilistic rules are generally expressed as multiplication rather than sum. However, since the goal is often minimizing the log likelihood, the Bayesian correction can often be expressed as standard regularizer as well.</p></div>
<div class="section" title="Multivariate regression" id="aid-7SN521"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec3800000"/>Multivariate regression</h1></div></div></div><p>It is possible to <a id="id384000000" class="indexterm"/>minimize multiple metrics at the same time. While Spark only has a few multivariate analysis tools, other more traditional well-established<a id="id385000000" class="indexterm"/> packages come <a id="id386000000" class="indexterm"/>with <span class="strong"><strong>Multivariate Analysis of Variance</strong></span> (<span class="strong"><strong>MANOVA</strong></span>), a generalization of <span class="strong"><strong>Analysis of Variance</strong></span> (<span class="strong"><strong>ANOVA</strong></span>) method. I will cover ANOVA and MANOVA in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>.</p><p>For a practical analysis, we first need to understand if the target variables are correlated, for which we can use the PCA Spark implementation covered in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>. If the dependent variables are strongly correlated, maximizing one leads to maximizing the other, and we can just maximize the first principal component (and potentially build a regression model on the second component to understand what drives the difference).</p><p>If the targets are uncorrelated, building a separate model for each of them can pinpoint the important variables that drive either and whether these two sets are disjoint. In the latter case, we could build two separate models to predict each of the targets independently.</p></div>
<div class="section" title="Heteroscedasticity" id="aid-7TLLK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec3900000"/>Heteroscedasticity</h1></div></div></div><p>One of the<a id="id387000000" class="indexterm"/> fundamental assumptions in regression approach is that the target variance is not correlated with either independent (attributes) or dependent (target) variables. An example where this assumption might break is counting data, which is generally described by <span class="strong"><strong>Poisson distribution</strong></span>. For Poisson distribution, the <a id="id388000000" class="indexterm"/>variance is proportional to the expected value, and the higher values can contribute more to the final variance of the weights.</p><p>While heteroscedasticity may or may not significantly skew the resulting weights, one practical way to compensate for heteroscedasticity is to perform a log transformation, which will <a id="id389000000" class="indexterm"/>compensate for it in the case of Poisson distribution:</p><div class="mediaobject"><img src="../Images/image01721.jpeg" alt="Heteroscedasticity"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image01722.jpeg" alt="Heteroscedasticity"/></div><p style="clear:both; height: 1em;"> </p><p>Some <a id="id390000000" class="indexterm"/>other (parametrized) transformations are the <span class="strong"><strong>Box-Cox transformation</strong></span>:</p><div class="mediaobject"><img src="../Images/image01723.jpeg" alt="Heteroscedasticity"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01724.jpeg" alt="Heteroscedasticity"/></span> is a parameter (the log transformation is a partial case, where <span class="inlinemediaobject"><img src="../Images/image01725.jpeg" alt="Heteroscedasticity"/></span>) and Tuckey's lambda transformation (for attributes between <span class="emphasis"><em>0</em></span> and <span class="emphasis"><em>1</em></span>):</p><div class="mediaobject"><img src="../Images/image01726.jpeg" alt="Heteroscedasticity"/></div><p style="clear:both; height: 1em;"> </p><p>These compensate for Poisson binomial distributed attributes or the estimates of the probability of success in a sequence of trails with potentially a mix of <span class="emphasis"><em>n</em></span> Bernoulli distributions.</p><p>Heteroscedasticity is one of the main reasons that logistic function minimization works better than linear regression with <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Heteroscedasticity"/></span> minimization in a binary prediction problem. Let's consider discrete labels in more details.</p></div>
<div class="section" title="Regression trees" id="aid-7UK661"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4000000"/>Regression trees</h1></div></div></div><p>We have seen<a id="id391000000" class="indexterm"/> classification trees in the previous chapter. One can build a recursive split-and-concur structure for a regression problem, where a split is chosen to minimize the remaining variance. Regression trees are less popular than decision trees or classical ANOVA analysis; however, let's provide an example of a regression tree here as a part of MLlib:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.DecisionTree</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>

<span class="strong"><strong>scala&gt; // Load and parse the data file.</strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; // Split the data into training and test sets (30% held out for testing)</strong></span>

<span class="strong"><strong>scala&gt; val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26</strong></span>

<span class="strong"><strong>scala&gt; val categoricalFeaturesInfo = Map[Int, Int]()</strong></span>
<span class="strong"><strong>categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()</strong></span>

<span class="strong"><strong>scala&gt; val impurity = "variance"</strong></span>
<span class="strong"><strong>impurity: String = variance</strong></span>

<span class="strong"><strong>scala&gt; val maxDepth = 5</strong></span>
<span class="strong"><strong>maxDepth: Int = 5</strong></span>

<span class="strong"><strong>scala&gt; val maxBins = 32</strong></span>
<span class="strong"><strong>maxBins: Int = 32</strong></span>

<span class="strong"><strong>scala&gt; val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 2 with 5 nodes</strong></span>

<span class="strong"><strong>scala&gt; val labelsAndPredictions = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (point.label, prediction)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>labelsAndPredictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[20] at map at &lt;console&gt;:36</strong></span>

<span class="strong"><strong>scala&gt; val testMSE = labelsAndPredictions.map{ case(v, p) =&gt; math.pow((v - p), 2)}.mean()</strong></span>
<span class="strong"><strong>testMSE: Double = 0.07407407407407407</strong></span>

<span class="strong"><strong>scala&gt; println(s"Test Mean Squared Error = $testMSE")</strong></span>
<span class="strong"><strong>Test Mean Squared Error = 0.07407407407407407</strong></span>

<span class="strong"><strong>scala&gt; println("Learned regression tree model:\n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned regression tree model:</strong></span>
<span class="strong"><strong>DecisionTreeModel regressor of depth 2 with 5 nodes</strong></span>
<span class="strong"><strong>  If (feature 378 &lt;= 71.0)</strong></span>
<span class="strong"><strong>   If (feature 100 &lt;= 165.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
<span class="strong"><strong>   Else (feature 100 &gt; 165.0)</strong></span>
<span class="strong"><strong>    Predict: 1.0</strong></span>
<span class="strong"><strong>  Else (feature 378 &gt; 71.0)</strong></span>
<span class="strong"><strong>   Predict: 1.0</strong></span>
</pre></div><p>The splits at<a id="id392000000" class="indexterm"/> each level are made to minimize the variance, as follows:</p><div class="mediaobject"><img src="../Images/image01727.jpeg" alt="Regression trees"/></div><p style="clear:both; height: 1em;"> </p><p>which is equivalent to minimizing the <span class="inlinemediaobject"><img src="../Images/image01696.jpeg" alt="Regression trees"/></span> distances between the label values and their mean within each leaf summed over all the leaves of the node.</p></div>
<div class="section" title="Classification metrics" id="aid-7VIMO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4100000"/>Classification metrics</h1></div></div></div><p>If the label is<a id="id393000000" class="indexterm"/> discrete, the prediction problem is called classification. In general, the target can take only one of the values for each record (even though multivalued targets are possible, particularly for text classification problems to be considered in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>).</p><p>If the discrete values are ordered and the ordering makes sense, such as <span class="emphasis"><em>Bad</em></span>, <span class="emphasis"><em>Worse</em></span>, <span class="emphasis"><em>Good</em></span>, the discrete labels can be cast into integer or double, and the problem is reduced to regression (we believe if you are between <span class="emphasis"><em>Bad</em></span> and <span class="emphasis"><em>Worse</em></span>, you are definitely farther away from being <span class="emphasis"><em>Good</em></span> than <span class="emphasis"><em>Worse</em></span>).</p><p>A generic metric to optimize is the misclassification rate is as follows:</p><div class="mediaobject"><img src="../Images/image01728.jpeg" alt="Classification metrics"/></div><p style="clear:both; height: 1em;"> </p><p>However, if the algorithm can predict the distribution of possible values for the target, a more general metric such as the KL divergence or Manhattan can be used.</p><p>KL divergence is a measure of information loss when probability distribution <span class="inlinemediaobject"><img src="../Images/image01729.jpeg" alt="Classification metrics"/></span> is used to approximate probability distribution <span class="inlinemediaobject"><img src="../Images/image01730.jpeg" alt="Classification metrics"/></span>:</p><div class="mediaobject"><img src="../Images/image01731.jpeg" alt="Classification metrics"/></div><p style="clear:both; height: 1em;"> </p><p>It is closely <a id="id394000000" class="indexterm"/>related to entropy gain split criteria used in the decision tree induction, as the latter is the sum of KL divergences of the node probability distribution to the leaf probability distribution over all leaf nodes.</p></div>
<div class="section" title="Multiclass problems" id="aid-80H7A1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4200000"/>Multiclass problems</h1></div></div></div><p>If the number<a id="id395000000" class="indexterm"/> of possible outcomes for target is larger than two, in general, we have to predict either the expected probability distribution of the target values or at least the list of ordered values—hopefully augmented by a rank variable, which can be used for additional analysis.</p><p>While some algorithms, such as decision trees, can natively predict multivalued attributes. A common technique is to reduce the prediction of one of the <span class="emphasis"><em>K</em></span> target values to <span class="emphasis"><em>(K-1)</em></span> binary classification problems by choosing one of the values as the base and building <span class="emphasis"><em>(K-1)</em></span> binary classifiers. It is usually a good idea to select the most populated level as the base.</p></div>
<div class="section" title="Perceptron" id="aid-81FNS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4300000"/>Perceptron</h1></div></div></div><p>In the early days <a id="id396000000" class="indexterm"/>of machine learning, researchers were trying to imitate the functionality of the human brain. At the beginning of the 20th century, people thought that the human brain consisted entirely of cells that are called neurons—cells with long appendages called axons that were able to transmit signals by means of electric impulses. The AI researchers were trying to replicate the functionality of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted sum of its input values:</p><div class="mediaobject"><img src="../Images/image01732.jpeg" alt="Perceptron"/></div><p style="clear:both; height: 1em;"> </p><p>This is a very simplistic representation of the processes in the human brain—biologists have since then discovered other ways in which information is transferred besides electric impulses such as chemical ones. Moreover, they have found over 300 different types of cells that may be classified as neurons (<a class="ulink" href="http://neurolex.org/wiki/Category:Neuron">http://neurolex.org/wiki/Category:Neuron</a>). Also, the process of neuron firing is more complex than just linear transmission of voltages as it involves complex time patterns as well. Nevertheless, the concept turned out to be very productive, and multiple algorithms and techniques were developed for neural nets, or the sets of perceptions connected to each other in layers. Specifically, it can be shown that the neural network, with certain <a id="id397000000" class="indexterm"/>modification, where the step function is replaced by a logistic function in the firing equation, can approximate an arbitrary differentiable function with any desired precision.</p><p>MLlib <a id="id398000000" class="indexterm"/>implements <span class="strong"><strong>Multilayer Perceptron Classifier</strong></span> (<span class="strong"><strong>MLCP</strong></span>) as an <code class="literal">org.apache.spark.ml.classification.MultilayerPerceptronClassifier </code>class:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.ml.classification.MultilayerPerceptronClassifier</strong></span>
<span class="strong"><strong>import org.apache.spark.ml.classification.MultilayerPerceptronClassifier</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</strong></span>
<span class="strong"><strong>import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt").toDF()</strong></span>
<span class="strong"><strong>data: org.apache.spark.sql.DataFrame = [label: double, features: vector]        </strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val Array(train, test) = data.randomSplit(Array(0.6, 0.4), seed = 13L)</strong></span>
<span class="strong"><strong>train: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>
<span class="strong"><strong>test: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>

<span class="strong"><strong>scala&gt; // specify layers for the neural network: </strong></span>

<span class="strong"><strong>scala&gt; // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)</strong></span>

<span class="strong"><strong>scala&gt; val layers = Array(4, 5, 4, 3)</strong></span>
<span class="strong"><strong>layers: Array[Int] = Array(4, 5, 4, 3)</strong></span>

<span class="strong"><strong>scala&gt; // create the trainer and set its parameters</strong></span>

<span class="strong"><strong>scala&gt; val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(13L).setMaxIter(100)</strong></span>
<span class="strong"><strong>trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_b5f2c25196f9</strong></span>

<span class="strong"><strong>scala&gt; // train the model</strong></span>

<span class="strong"><strong>scala&gt; val model = trainer.fit(train)</strong></span>
<span class="strong"><strong>model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_b5f2c25196f9</strong></span>

<span class="strong"><strong>scala&gt; // compute precision on the test set</strong></span>

<span class="strong"><strong>scala&gt; val result = model.transform(test)</strong></span>
<span class="strong"><strong>result: org.apache.spark.sql.DataFrame = [label: double, features: vector, prediction: double]</strong></span>

<span class="strong"><strong>scala&gt; val predictionAndLabels = result.select("prediction", "label")</strong></span>
<span class="strong"><strong>predictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]</strong></span>

<span class="strong"><strong>scala&gt; val evaluator = new MulticlassClassificationEvaluator().setMetricName("precision")</strong></span>
<span class="strong"><strong>evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_55757d35e3b0</strong></span>

<span class="strong"><strong>scala&gt; println("Precision = " + evaluator.evaluate(predictionAndLabels))</strong></span>
<span class="strong"><strong>Precision = 0.9375</strong></span>
</pre></div></div>
<div class="section" title="Generalization error and overfitting" id="aid-82E8E1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4400000"/>Generalization error and overfitting</h1></div></div></div><p>So, how do <a id="id399000000" class="indexterm"/>we know that the model we have discussed is <a id="id400000000" class="indexterm"/>good? One obvious and ultimate criterion is its performance in practice.</p><p>One common problem that plagues the more complex models, such as decision trees and neural nets, is overfitting. The model can minimize the desired metric on the provided data, but does a very poor job on a slightly different dataset in practical deployments, Even a standard technique, when we split the dataset into training and test, the training for deriving the model and test for validating that the model works well on a hold-out data, may not capture all the changes that are in the deployments. For example, linear models such as ANOVA, logistic, and linear regression are usually relatively stable and less of a subject to overfitting. However, you might find that any particular technique either works or doesn't work for your specific domain.</p><p>Another case when generalization may fail is time-drift. The data may change over time significantly so that the model trained on the old data no longer generalizes on the new data in a deployment. In practice, it is always a good idea to have several models in production and constantly monitor their relative performance.</p><p>I will consider standard ways to avoid overfitting such as hold out datasets and cross-validation in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span> and model monitoring in <a class="link" title="Chapter 9. NLP in Scala" href="part0291.xhtml#aid-8LGJM2">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p></div>
<div class="section" title="Summary" id="aid-83CP01"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec4500000"/>Summary</h1></div></div></div><p>We now have all the necessary tools to look at more complex problems that are more commonly called the big data problems. Armed with standard statistical algorithms—I understand that I have not covered many details and I am completely ready to accept the criticism—there is an entirely new ground to explore where we do not have clearly defined records, the variables in the datasets may be sparse and nested, and we have to cover a lot of ground and do a lot of preparatory work just to get to the stage where we can apply the standard statistical models. This is where Scala shines best.</p><p>In the next chapter, we will look more at working with unstructured data.</p></div>
<div class="chapter" title="Chapter&#xA0;6.&#xA0;Working with Unstructured Data"><div class="titlepage" id="aid-84B9I2"><div><div><h1 class="title"><a id="ch32"/>Chapter 6. Working with Unstructured Data</h1></div></div></div><p>I am very excited to introduce you to this chapter. Unstructured data is what, in reality, makes big data different from the old data, it also makes Scala to be the new paradigm for processing the data. To start with, unstructured data at first sight seems a lot like a derogatory term. Notwithstanding, every sentence in this book is unstructured data: it does not have the traditional record / row / column semantics. For most people, however, this is the easiest thing to read rather than the book being presented as a table or spreadsheet.</p><p>In practice, the unstructured data means nested and complex data. An XML document or a photograph are good examples of unstructured data, which have very rich structure to them. My guess is that the originators of the term meant that the new data, the data that engineers at social interaction companies such as Google, Facebook, and Twitter saw, had a different structure to it as opposed to a traditional flat table that everyone used to see. These indeed did not fit the traditional RDBMS paradigm. Some of them can be flattened, but the underlying storage would be too inefficient as the RDBMSs were not optimized to handle them and also be hard to parse not only for humans, but for the machines as well.</p><p>A lot of techniques introduced in this chapter were created as an emergency Band-Aid to deal with the need to just process the data.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Learning about the serialization, popular serialization frameworks, and language in which the machines talk to each other</li><li class="listitem">Learning about Avro-Parquet encoding for nested data</li><li class="listitem">Learning how RDBMs try to incorporate nested structures in modern SQL-like languages to work with them</li><li class="listitem">Learning how you can start working with nested structures in Scala</li><li class="listitem">Seeing a practical example of sessionization—one of the most frequent use cases for unstructured data</li><li class="listitem">Seeing how Scala traits and match/case statements can simplify path analysis</li><li class="listitem">Learning where the nested structures can benefit your analysis</li></ul></div><div class="section" title="Nested data"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec4600000"/>Nested data</h1></div></div></div><p>You already <a id="id401000000" class="indexterm"/>saw unstructured data in the previous chapters, the data was an array <a id="id402000000" class="indexterm"/>of <span class="strong"><strong>LabeledPoint</strong></span>, which is a tuple <span class="strong"><strong>(label: Double, features: Vector)</strong></span>. The label is just a number of type <span class="strong"><strong>Double</strong></span>. <span class="strong"><strong>Vector</strong></span> is a sealed trait <a id="id403000000" class="indexterm"/>with two subclasses: <span class="strong"><strong>SparseVector</strong></span> and <span class="strong"><strong>DenseVector</strong></span>. The <a id="id404000000" class="indexterm"/>class <a id="id405000000" class="indexterm"/>diagram is as follows:</p><div class="mediaobject"><img src="../Images/image01733.jpeg" alt="Nested data"/><div class="caption"><p>Figure 1: The LabeledPoint class structure is a tuple of label and features, where features is a trait with two inherited subclasses {Dense,Sparse}Vector.  DenseVector is an array of double, while SparseVector stores only size and non-default elements by index and value.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Each observation is a tuple of label and features, and features can be sparse. Definitely, if there are no missing values, the whole row can be represented as vector. A dense vector representation requires (<span class="emphasis"><em>8 x size + 8</em></span>) bytes. If most of the elements are missing—or equal to some default value—we can store only the non-default elements. In this case, we would require (<span class="emphasis"><em>12 x non_missing_size + 20</em></span>) bytes, with small variations depending on the JVM implementation. So, the threshold for switching between one or another, from the storage point of view, is when the size is greater than <span class="emphasis"><em>1.5 x</em></span> ( <span class="emphasis"><em>non_missing_size + 1</em></span> ), or if roughly at least 30% of elements are non-default. While the computer languages are good at representing the complex structures via pointers, we need some convenient form to exchange<a id="id406000000" class="indexterm"/> these data between JVMs or machines. First, let's see first how Spark/Scala does it, specifically persisting the data in the Parquet format:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>Wha</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>     |    LabeledPoint(0.0, Vectors.sparse(3, Array(1), Array(1.0))),</strong></span>
<span class="strong"><strong>     |    LabeledPoint(1.0, Vectors.dense(0.0, 2.0, 0.0)),</strong></span>
<span class="strong"><strong>     |    LabeledPoint(2.0, Vectors.sparse(3, Array((1, 3.0)))),</strong></span>
<span class="strong"><strong>     |    LabeledPoint.parse("(3.0,[0.0,4.0,0.0])"));</strong></span>
<span class="strong"><strong>pts: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val rdd = sc.parallelize(points)</strong></span>
<span class="strong"><strong>rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:25</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val df = rdd.repartition(1).toDF</strong></span>
<span class="strong"><strong>df: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>

<span class="strong"><strong>scala&gt; df.write.parquet("points")</strong></span>
</pre></div><p>What we did was create a new RDD dataset from command line, or we could use <code class="literal">org.apache.spark.mllib.util.MLUtils</code> to load a text file, converted it to a DataFrames and <a id="id407000000" class="indexterm"/>create a serialized representation of it in the Parquet file under the <code class="literal">points</code> directory.</p><div class="note" title="Note"><h3 class="title"><a id="note05000000"/>Note</h3><p>
<span class="strong"><strong>What Parquet stands for?</strong></span>
</p><p>Apache Parquet <a id="id408000000" class="indexterm"/>is a columnar storage format, jointly developed by Cloudera and Twitter for big data. Columnar storage allows for better compression of values in the datasets and is more efficient if only a subset of columns need to be retrieved from the disk. Parquet was built from the ground up with complex nested data structures in mind and uses the record shredding and <a id="id409000000" class="indexterm"/>assembly algorithm described in the Dremel paper (<a class="ulink" href="https://blog.twitter.com/2013/dremel-made-simple-with-parquet">https://blog.twitter.com/2013/dremel-made-simple-with-parquet</a>). Dremel/Parquet encoding uses definition/repetition fields to denote the level in the hierarchy the data is coming from, which covers most of the immediate encoding needs, as it is sufficient to store optional fields, nested arrays, and maps. Parquet stores the data by chunks, thus probably the name Parquet, which means flooring composed of wooden blocks arranged in a geometric pattern. Parquet can be optimized for reading only a subset of blocks from disk, depending on the subset of columns to be read and the index used (although it very much depends on whether the specific implementation is aware of these features). The<a id="id410000000" class="indexterm"/> values in the columns can use dictionary and <span class="strong"><strong>Run-Length Encoding</strong></span> (<span class="strong"><strong>RLE</strong></span>), which provides exceptionally good compression for columns with many duplicate entries, a frequent use case in big data.</p></div><p>Parquet file<a id="id411000000" class="indexterm"/> is a binary format, but you might look at the information<a id="id412000000" class="indexterm"/> in it using <code class="literal">parquet-tools</code>, which are downloadable from <a class="ulink" href="http://archive.cloudera.com/cdh5/cdh/5">http://archive.cloudera.com/cdh5/cdh/5</a>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ wget -O - http://archive.cloudera.com/cdh5/cdh/5/parquet-1.5.0-cdh5.5.0.tar.gz | tar xzvf -</strong></span>

<span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ cd parquet-1.5.0-cdh5.5.0/parquet-tools</strong></span>

<span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ tar xvf xvf parquet-1.5.0-cdh5.5.0/parquet-tools/target/parquet-tools-1.5.0-cdh5.5.0-bin.tar.gz</strong></span>

<span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ cd parquet-tools-1.5.0-cdh5.5.0</strong></span>

<span class="strong"><strong>akozlov@Alexanders-MacBook-Pro $ ./parquet-schema ~/points/*.parquet </strong></span>
<span class="strong"><strong>message spark_schema {</strong></span>
<span class="strong"><strong>  optional double label;</strong></span>
<span class="strong"><strong>  optional group features {</strong></span>
<span class="strong"><strong>    required int32 type (INT_8);</strong></span>
<span class="strong"><strong>    optional int32 size;</strong></span>
<span class="strong"><strong>    optional group indices (LIST) {</strong></span>
<span class="strong"><strong>      repeated group list {</strong></span>
<span class="strong"><strong>        required int32 element;</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    optional group values (LIST) {</strong></span>
<span class="strong"><strong>      repeated group list {</strong></span>
<span class="strong"><strong>        required double element;</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Let's look at the<a id="id413000000" class="indexterm"/> schema, which is very close to the structure depicted in <span class="emphasis"><em>Figure 1</em></span>: first member is the label of type double and the second and last one is features of composite type. The keyword optional is another way of saying that the value can be null (absent) in the record for one or another reason. The lists or arrays are encoded as a repeated field. As the whole array may be absent (it is possible for all features to be absent), it is wrapped into optional groups (indices and values). Finally, the type encodes whether it is a sparse or dense representation:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro $ ./parquet-dump ~/points/*.parquet </strong></span>
<span class="strong"><strong>row group 0 </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>label:       DOUBLE GZIP DO:0 FPO:4 SZ:78/79/1.01 VC:4 ENC:BIT_PACKED,PLAIN,RLE</strong></span>
<span class="strong"><strong>features:   </strong></span>
<span class="strong"><strong>.type:       INT32 GZIP DO:0 FPO:82 SZ:101/63/0.62 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE</strong></span>
<span class="strong"><strong>.size:       INT32 GZIP DO:0 FPO:183 SZ:97/59/0.61 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE</strong></span>
<span class="strong"><strong>.indices:   </strong></span>
<span class="strong"><strong>..list:     </strong></span>
<span class="strong"><strong>...element:  INT32 GZIP DO:0 FPO:280 SZ:100/65/0.65 VC:4 ENC:PLAIN_DICTIONARY,RLE</strong></span>
<span class="strong"><strong>.values:    </strong></span>
<span class="strong"><strong>..list:     </strong></span>
<span class="strong"><strong>...element:  DOUBLE GZIP DO:0 FPO:380 SZ:125/111/0.89 VC:8 ENC:PLAIN_DICTIONARY,RLE</strong></span>

<span class="strong"><strong>    label TV=4 RL=0 DL=1</strong></span>
<span class="strong"><strong>    ------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:38 VC:4</strong></span>

<span class="strong"><strong>    features.type TV=4 RL=0 DL=1 DS:                 2 DE:PLAIN_DICTIONARY</strong></span>
<span class="strong"><strong>    ------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4</strong></span>

<span class="strong"><strong>    features.size TV=4 RL=0 DL=2 DS:                 1 DE:PLAIN_DICTIONARY</strong></span>
<span class="strong"><strong>    ------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4</strong></span>

<span class="strong"><strong>    features.indices.list.element TV=4 RL=1 DL=3 DS: 1 DE:PLAIN_DICTIONARY</strong></span>
<span class="strong"><strong>    ------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:15 VC:4</strong></span>

<span class="strong"><strong>    features.values.list.element TV=8 RL=1 DL=3 DS:  5 DE:PLAIN_DICTIONARY</strong></span>
<span class="strong"><strong>    ------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:17 VC:8</strong></span>

<span class="strong"><strong>DOUBLE label </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:1 V:0.0</strong></span>
<span class="strong"><strong>value 2: R:0 D:1 V:1.0</strong></span>
<span class="strong"><strong>value 3: R:0 D:1 V:2.0</strong></span>
<span class="strong"><strong>value 4: R:0 D:1 V:3.0</strong></span>

<span class="strong"><strong>INT32 features.type </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:1 V:0</strong></span>
<span class="strong"><strong>value 2: R:0 D:1 V:1</strong></span>
<span class="strong"><strong>value 3: R:0 D:1 V:0</strong></span>
<span class="strong"><strong>value 4: R:0 D:1 V:1</strong></span>

<span class="strong"><strong>INT32 features.size </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:2 V:3</strong></span>
<span class="strong"><strong>value 2: R:0 D:1 V:&lt;null&gt;</strong></span>
<span class="strong"><strong>value 3: R:0 D:2 V:3</strong></span>
<span class="strong"><strong>value 4: R:0 D:1 V:&lt;null&gt;</strong></span>

<span class="strong"><strong>INT32 features.indices.list.element </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:3 V:1</strong></span>
<span class="strong"><strong>value 2: R:0 D:1 V:&lt;null&gt;</strong></span>
<span class="strong"><strong>value 3: R:0 D:3 V:1</strong></span>
<span class="strong"><strong>value 4: R:0 D:1 V:&lt;null&gt;</strong></span>

<span class="strong"><strong>DOUBLE features.values.list.element </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 8 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:3 V:1.0</strong></span>
<span class="strong"><strong>value 2: R:0 D:3 V:0.0</strong></span>
<span class="strong"><strong>value 3: R:1 D:3 V:2.0</strong></span>
<span class="strong"><strong>value 4: R:1 D:3 V:0.0</strong></span>
<span class="strong"><strong>value 5: R:0 D:3 V:3.0</strong></span>
<span class="strong"><strong>value 6: R:0 D:3 V:0.0</strong></span>
<span class="strong"><strong>value 7: R:1 D:3 V:4.0</strong></span>
<span class="strong"><strong>value 8: R:1 D:3 V:0.0</strong></span>
</pre></div><p>You are probably a bit confused about the <code class="literal">R</code>: and <code class="literal">D</code>: in the output. These are the repetition and definition<a id="id414000000" class="indexterm"/> levels as described in the Dremel paper and they are necessary to efficiently encode the values in the nested structures. Only repeated fields increment the repetition level and only non-required fields increment the definition level. Drop in <code class="literal">R</code> signifies the end of the list(array). For every non-required level in the hierarchy tree, one needs a new definition level. Repetition and definition level values are small by design and can be efficiently stored in a serialized form.</p><p>What is best, if there are many duplicate entries, they will all be placed together. The case for which the compression algorithm (by default, it is gzip) are optimized. Parquet also implements other algorithms exploiting repeated values such as dictionary encoding or RLE compression.</p><p>This is a simple and efficient serialization out of the box. We have been able to write a set of complex objects to a file, each column stored in a separate block, representing all values in the records and nested structures.</p><p>Let's now read the file and recover RDD. The Parquet format does not know anything about the <code class="literal">LabeledPoint</code> class, so we'll have to do some typecasting and trickery here. When we read the file, we'll see a collection of <code class="literal">org.apache.spark.sql.Row</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; val df = sqlContext.read.parquet("points")</strong></span>
<span class="strong"><strong>df: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>

<span class="strong"><strong>scala&gt; val df = sqlContext.read.parquet("points").collect</strong></span>
<span class="strong"><strong>df: Array[org.apache.spark.sql.Row] = Array([0.0,(3,[1],[1.0])], [1.0,[0.0,2.0,0.0]], [2.0,(3,[1],[3.0])], [3.0,[0.0,4.0,0.0]])</strong></span>

<span class="strong"><strong>scala&gt; val rdd = df.map(x =&gt; LabeledPoint(x(0).asInstanceOf[scala.Double], x(1).asInstanceOf[org.apache.spark.mllib.linalg.Vector]))</strong></span>
<span class="strong"><strong>rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[16] at map at &lt;console&gt;:25</strong></span>

<span class="strong"><strong>scala&gt; rdd.collect</strong></span>
<span class="strong"><strong>res12: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))</strong></span>

<span class="strong"><strong>scala&gt; rdd.filter(_.features(1) &lt;= 2).collect</strong></span>
<span class="strong"><strong>res13: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]))</strong></span>
</pre></div><p>Personally, I think that this is pretty cool: without any compilation, we can encode and decide <a id="id415000000" class="indexterm"/>complex objects. One can easily create their own objects in REPL. Let's consider that we want to track user's behavior on the web:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; case class Person(id: String, visits: Array[String]) { override def toString: String = { val vsts = visits.mkString(","); s"($id -&gt; $vsts)" } }</strong></span>
<span class="strong"><strong>defined class Person</strong></span>

<span class="strong"><strong>scala&gt; val p1 = Person("Phil", Array("http://www.google.com", "http://www.facebook.com", "http://www.linkedin.com", "http://www.homedepot.com"))</strong></span>
<span class="strong"><strong>p1: Person = (Phil -&gt; http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com)</strong></span>

<span class="strong"><strong>scala&gt; val p2 = Person("Emily", Array("http://www.victoriassecret.com", "http://www.pacsun.com", "http://www.abercrombie.com/shop/us", "http://www.orvis.com"))</strong></span>
<span class="strong"><strong>p2: Person = (Emily -&gt; http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com)</strong></span>

<span class="strong"><strong>scala&gt; sc.parallelize(Array(p1,p2)).repartition(1).toDF.write.parquet("history")</strong></span>

<span class="strong"><strong>scala&gt; import scala.collection.mutable.WrappedArray</strong></span>
<span class="strong"><strong>import scala.collection.mutable.WrappedArray</strong></span>

<span class="strong"><strong>scala&gt; val df = sqlContext.read.parquet("history")</strong></span>
<span class="strong"><strong>df: org.apache.spark.sql.DataFrame = [id: string, visits: array&lt;string&gt;]</strong></span>

<span class="strong"><strong>scala&gt; val rdd = df.map(x =&gt; Person(x(0).asInstanceOf[String], x(1).asInstanceOf[WrappedArray[String]].toArray[String]))</strong></span>
<span class="strong"><strong>rdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[27] at map at &lt;console&gt;:28</strong></span>

<span class="strong"><strong>scala&gt; rdd.collect</strong></span>
<span class="strong"><strong>res9: Array[Person] = Array((Phil -&gt; http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com), (Emily -&gt; http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com))</strong></span>
</pre></div><p>As a matter <a id="id416000000" class="indexterm"/>of good practice, we need to register the newly created classes with the <code class="literal">Kryo</code> <code class="literal">serializer</code>—Spark will use another serialization mechanism to pass the objects between tasks and executors. If the class is not registered, Spark will use default Java serialization, which might be up to <span class="emphasis"><em>10 x</em></span> slower:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; :paste</strong></span>
<span class="strong"><strong>// Entering paste mode (ctrl-D to finish)</strong></span>

<span class="strong"><strong>import com.esotericsoftware.kryo.Kryo</strong></span>
<span class="strong"><strong>import org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}</strong></span>

<span class="strong"><strong>class MyKryoRegistrator extends KryoRegistrator {</strong></span>
<span class="strong"><strong>  override def registerClasses(kryo: Kryo) {</strong></span>
<span class="strong"><strong>    kryo.register(classOf[Person])</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>}</strong></span>

<span class="strong"><strong>object MyKryoRegistrator {</strong></span>
<span class="strong"><strong>  def register(conf: org.apache.spark.SparkConf) {</strong></span>
<span class="strong"><strong>    conf.set("spark.serializer", classOf[KryoSerializer].getName)</strong></span>
<span class="strong"><strong>    conf.set("spark.kryo.registrator", classOf[MyKryoRegistrator].getName)</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>^D</strong></span>

<span class="strong"><strong>// Exiting paste mode, now interpreting.</strong></span>

<span class="strong"><strong>import com.esotericsoftware.kryo.Kryo</strong></span>
<span class="strong"><strong>import org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}</strong></span>
<span class="strong"><strong>defined class MyKryoRegistrator</strong></span>
<span class="strong"><strong>defined module MyKryoRegistrator</strong></span>

<span class="strong"><strong>scala&gt;</strong></span>
</pre></div><p>If you are <a id="id417000000" class="indexterm"/>deploying the code on a cluster, the recommendation is to put this code in a jar on the classpath.</p><p>I've certainly seen examples of up to 10 level deep nesting in production. Although this might be an overkill for performance reasons, nesting is required in more and more production business use cases. Before we go into the specifics of constructing a nested object in the example of sessionization, let's get an overview of serialization in general.</p></div></div>
<div class="section" title="Other serialization formats"><div class="titlepage" id="aid-859Q42"><div><div><h1 class="title"><a id="ch06lvl1sec4700000"/>Other serialization formats</h1></div></div></div><p>I do recommend the Parquet format for storing the data. However, for completeness, I need to at least mention other serialization formats, some of them like Kryo will be used implicitly <a id="id418000000" class="indexterm"/>for you during Spark computations without your knowledge and there is obviously a default Java serialization.</p><div class="note" title="Note"><h3 class="title"><a id="tip08000000"/>Tip</h3><p>
<span class="strong"><strong>Object-oriented approach versus functional approach</strong></span>
</p><p>Objects in <a id="id419000000" class="indexterm"/>object-oriented<a id="id420000000" class="indexterm"/> approach are characterized by state and behavior. Objects are the cornerstone of object-oriented programming. A class is a template for objects with fields that represent the state, and methods that may represent the behavior. Abstract method implementation may depend on the instance of the class. In functional approach, the state is usually frowned upon; in pure programming languages, there should be no state, no<a id="id421000000" class="indexterm"/> side effects, and<a id="id422000000" class="indexterm"/> every invocation should return the same result. The behaviors may be expressed though additional function parameters and higher order functions (functions over functions, such as currying), but should be explicit unlike the abstract methods. Since Scala is a mix of object-oriented and functional language, some of the preceding constraints are violated, but this does not mean that you have to use them unless absolutely necessary. It is best practice to store the code in jar packages while storing the data, particularly for the big data, separate from code in data files (in a serialized form); but again, people often store data/configurations in jar files, and it is less common, but possible to store code in the data files.</p></div><p>The serialization has been an issue since the need to persist data on disk or transfer object from one JVM <a id="id423000000" class="indexterm"/>or machine to another over network appeared. Really, the purpose of serialization is to make complex nested objects be represented as a series of bytes, understandable by machines, and as you can imagine, this might be language-dependent. Luckily, serialization frameworks converge on a set of common data structures they can handle.</p><p>One of the most popular serialization mechanisms, but not the most efficient, is to dump an object in an ASCII file: CSV, XML, JSON, YAML, and so on. They do work for more complex nested data like structures, arrays, and maps, but are inefficient from the storage space perspective. For example, a Double represents a continuous number with 15-17 significant digits that will, without rounding or trivial ratios, take 15-17 bytes to represent in US ASCII, while the binary representation takes only 8 bytes. Integers may be stored even more efficiently, particularly if they are small, as we can compress/remove zeroes.</p><p>One advantage of text encoding is that they are much easier to visualize with simple command-line tools, but any advanced serialization framework now comes with a set of tools to work with raw records such as <code class="literal">avro</code>
<span class="emphasis"><em>-</em></span> or <code class="literal">parquet-tools</code>.</p><p>The following table provides an overview for most common serialization frameworks:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Serialization Format</p>
</th><th valign="bottom">
<p>When developed</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>XML, JSON, YAML</p>
</td><td valign="top">
<p>This was <a id="id424000000" class="indexterm"/>a direct<a id="id425000000" class="indexterm"/> response <a id="id426000000" class="indexterm"/>to the necessity to encode nested structures and exchange the <a id="id427000000" class="indexterm"/>data between machines.</p>
</td><td valign="top">
<p>While <a id="id428000000" class="indexterm"/>grossly inefficient, these are still used <a id="id429000000" class="indexterm"/>in many places, particularly in web services. The only advantage is that they are relatively easy to parse without machines.</p>
</td></tr><tr><td valign="top">
<p>Protobuf</p>
</td><td valign="top">
<p>Developed<a id="id430000000" class="indexterm"/> by Google in the early 2000s. This implements the Dremel encoding scheme and supports multiple languages (Scala is not officially supported yet, even though some code exists).</p>
</td><td valign="top">
<p>The <a id="id431000000" class="indexterm"/>main advantage is that Protobuf can generate native classes in many languages. C++, Java, and Python are officially supported. There are ongoing projects in C, C#, Haskell, Perl, Ruby, Scala, and more. Run-time can call native code to inspect/serialize/deserialize the objects and binary representations.</p>
</td></tr><tr><td valign="top">
<p>Avro</p>
</td><td valign="top">
<p>Avro<a id="id432000000" class="indexterm"/> was developed by <a id="id433000000" class="indexterm"/>Doug Cutting while he was working at Cloudera. The main objective was to separate the encoding from a specific implementation and language, allowing better schema evolution.</p>
</td><td valign="top">
<p>While the arguments whether Protobuf or Avro are more efficient are still ongoing, Avro supports a larger number of complex structures, say unions and maps out of the box, compared to Protobuf. Scala support is still to be strengthened to the production level. Avro files have schema encoded with every file, which has its pros and cons.</p>
</td></tr><tr><td valign="top">
<p>Thrift</p>
</td><td valign="top">
<p>The <a id="id434000000" class="indexterm"/>Apache Thrift <a id="id435000000" class="indexterm"/>was developed at Facebook for the same purpose Protobuf was developed. It probably has the widest selection of supported languages: C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml, Delphi, and other languages. Again, Twitter is hard at work for making the<a id="id436000000" class="indexterm"/> Thrift code generation in Scala (<a class="ulink" href="https://twitter.github.io/scrooge/">https://twitter.github.io/scrooge/</a>).</p>
</td><td valign="top">
<p>Apache Thrift is often described as a framework for cross-language services development and is most frequently used as <span class="strong"><strong>Remote Procedure Call</strong></span> (<span class="strong"><strong>RPC</strong></span>). Even<a id="id437000000" class="indexterm"/> though it can be used directly for serialization/deserialization, other frameworks just happen to be more popular.</p>
</td></tr><tr><td valign="top">
<p>Parquet</p>
</td><td valign="top">
<p>Parquet <a id="id438000000" class="indexterm"/>was developed<a id="id439000000" class="indexterm"/> in a joint effort between Twitter and Cloudera. Compared to the Avro format, which is row-oriented, Parquet is columnar storage that results in better compression and performance if only a few columns are to be selected. The interval encoding is Dremel or Protobuf-based, even though the records are presented as Avro records; thus, it is often called <a id="id440000000" class="indexterm"/>
<span class="strong"><strong>AvroParquet</strong></span>.</p>
</td><td valign="top">
<p>Advances features such as indices, dictionary encoding, and RLE compression potentially make it very efficient for pure disk storage. Writing the files may be slower as Parquet requires some preprocessing and index building before it can be committed to the disk.</p>
</td></tr><tr><td valign="top">
<p>Kryo</p>
</td><td valign="top">
<p>This <a id="id441000000" class="indexterm"/>is a framework <a id="id442000000" class="indexterm"/>for encoding arbitrary classes in Java. However, not all built-in Java collection classes can be serialized.</p>
</td><td valign="top">
<p>If one avoids non-serializable exceptions, such as priority queues, Kryo can be very efficient. Direct support in Scala is also under way.</p>
</td></tr></tbody></table></div><p>Certainly, Java has a built-in serialization framework, but as it has to support all Java cases, and therefore<a id="id443000000" class="indexterm"/> is overly general, the Java serialization is far less efficient than any of the preceding methods. I have certainly seen other companies implement their own proprietary serialization earlier, which would beat any of the preceding serialization for the specific cases. Nowadays, it is no longer necessary, as the maintenance costs definitely overshadow the converging inefficiency of the existing frameworks.</p></div>
<div class="section" title="Hive and Impala"><div class="titlepage" id="aid-868AM2"><div><div><h1 class="title"><a id="ch06lvl1sec4800000"/>Hive and Impala</h1></div></div></div><p>One of the <a id="id444000000" class="indexterm"/>design considerations for a new framework is always the compatibility<a id="id445000000" class="indexterm"/> with the old frameworks. For better or worse, most data analysts still work with SQL. The roots of the SQL go to an influential relational modeling paper (<span class="emphasis"><em>Codd, Edgar F</em></span> (June 1970). <span class="emphasis"><em>A Relational Model of Data for Large Shared Data Banks</em></span>. <span class="emphasis"><em>Communications of the ACM (Association for Computing Machinery) 13 (6): 377–87</em></span>). All modern databases implement one or another version of SQL.</p><p>While the relational model was influential and important for bringing the database performance, particularly for <span class="strong"><strong>Online Transaction Processing</strong></span> (<span class="strong"><strong>OLTP</strong></span>) to the competitive<a id="id446000000" class="indexterm"/> levels, the significance of normalization for analytic workloads, where one needs to perform aggregations, and for situations where relations themselves change and are subject to analysis, is less critical. This section will cover the extensions of standard SQL language for analysis engines traditionally used for big data analytics: Hive and Impala. Both of them are currently Apache licensed projects. The following table summarizes the complex types:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Type</p>
</th><th valign="bottom">
<p>Hive support since version</p>
</th><th valign="bottom">
<p>Impala support since version</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<code class="literal">ARRAY</code>
</p>
</td><td valign="top">
<p>This is<a id="id447000000" class="indexterm"/> supported since 0.1.0, but the use of non-constant index expressions is allowed only as of 0.14.</p>
</td><td valign="top">
<p>This is supported since 2.3.0 (only for Parquet tables).</p>
</td><td valign="top">
<p>This can be an array of any type, including complex. The index is <code class="literal">int</code> in Hive (<code class="literal">bigint</code> in Impala) and access is via array notation, for example, <code class="literal">element[1]</code> only in Hive (<code class="literal">array.pos</code> and <code class="literal">item pseudocolumns</code> in Impala).</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">MAP</code>
</p>
</td><td valign="top">
<p>This is<a id="id448000000" class="indexterm"/> supported since 0.1.0, but the use of non-constant index expressions is allowed only as of 0.14.</p>
</td><td valign="top">
<p>This is supported since 2.3.0 (only for Parquet tables).</p>
</td><td valign="top">
<p>The key should be of primitive type. Some libraries support keys of the string type only. Fields are accessed using array notation, for example, <code class="literal">map["key"]</code> only in Hive (map key and value pseudocolumns in Impala).</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">STRUCT</code>
</p>
</td><td valign="top">
<p>This is<a id="id449000000" class="indexterm"/> supported since 0.5.0.</p>
</td><td valign="top">
<p>This is supported since 2.3.0 (only for Parquet tables).</p>
</td><td valign="top">
<p>Access is using dot notation, for example, <code class="literal">struct.element</code>.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">UNIONTYPE</code>
</p>
</td><td valign="top">
<p>This is<a id="id450000000" class="indexterm"/> supported since 0.7.0.</p>
</td><td valign="top">
<p>This is not supported in Impala.</p>
</td><td valign="top">
<p>Support is incomplete: queries that reference <code class="literal">UNIONTYPE</code> fields in <code class="literal">JOIN</code> (HIVE-2508), <code class="literal">WHERE</code>, and <code class="literal">GROUP BY</code> clauses will fail, and Hive does not define the syntax to extract the tag or value fields of <code class="literal">UNIONTYPE</code>. This means that <code class="literal">UNIONTYPEs</code> are effectively look-at-only.</p>
</td></tr></tbody></table></div><p>While Hive/Impala tables can be created on top of many underlying file formats (Text, Sequence, ORC, Avro, Parquet, and even custom format) and multiple serializations, in most practical instances, Hive is used to read lines of text in ASCII files. The underlying serialization/deserialization format is <code class="literal">LazySimpleSerDe</code> (<span class="strong"><strong>Serialization</strong></span>/<span class="strong"><strong>Deserialization</strong></span> (<span class="strong"><strong>SerDe</strong></span>)). The format defines several levels of separators, as follows:</p><div class="informalexample"><pre class="programlisting">row_format
  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
    [NULL DEFINED AS char]</pre></div><p>The<a id="id451000000" class="indexterm"/> default for separators are <code class="literal">'\001'</code> or <code class="literal">^A</code>, <code class="literal">'\002'</code> or <code class="literal">^B</code>, and <code class="literal">'\003'</code> or <code class="literal">^B</code>. In <a id="id452000000" class="indexterm"/>other words, it's using the new separator at each level of the hierarchy as opposed to the definition/repetition indicator in the Dremel encoding. For example, to encode the <code class="literal">LabeledPoint</code> table that we used before, we need to create a file, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cat data</strong></span>
<span class="strong"><strong>0^A1^B1^D1.0$</strong></span>
<span class="strong"><strong>2^A1^B1^D3.0$</strong></span>
<span class="strong"><strong>1^A0^B0.0^C2.0^C0.0$</strong></span>
<span class="strong"><strong>3^A0^B0.0^C4.0^C0.0$</strong></span>
</pre></div><p>Download <a id="id453000000" class="indexterm"/>Hive from <a class="ulink" href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz</a> and perform the follow:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ tar xf hive-1.1.0-cdh5.5.0.tar.gz </strong></span>
<span class="strong"><strong>$ cd hive-1.1.0-cdh5.5.0</strong></span>
<span class="strong"><strong>$ bin/hive</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>hive&gt; CREATE TABLE LABELED_POINT ( LABEL INT, VECTOR UNIONTYPE&lt;ARRAY&lt;DOUBLE&gt;, MAP&lt;INT,DOUBLE&gt;&gt; ) STORED AS TEXTFILE;</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>Time taken: 0.453 seconds</strong></span>
<span class="strong"><strong>hive&gt; LOAD DATA LOCAL INPATH './data' OVERWRITE INTO TABLE LABELED_POINT;</strong></span>
<span class="strong"><strong>Loading data to table alexdb.labeled_point</strong></span>
<span class="strong"><strong>Table labeled_point stats: [numFiles=1, numRows=0, totalSize=52, rawDataSize=0]</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>Time taken: 0.808 seconds</strong></span>
<span class="strong"><strong>hive&gt; select * from labeled_point;</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>0  {1:{1:1.0}}</strong></span>
<span class="strong"><strong>2  {1:{1:3.0}}</strong></span>
<span class="strong"><strong>1  {0:[0.0,2.0,0.0]}</strong></span>
<span class="strong"><strong>3  {0:[0.0,4.0,0.0]}</strong></span>
<span class="strong"><strong>Time taken: 0.569 seconds, Fetched: 4 row(s)</strong></span>
<span class="strong"><strong>hive&gt;</strong></span>
</pre></div><p>In Spark, select from a relational table is supported via the <code class="literal">sqlContext.sql</code> method, but unfortunately the Hive union types are not directly supported as of Spark 1.6.1; it does support maps <a id="id454000000" class="indexterm"/>and arrays though. The supportability of complex objects in <a id="id455000000" class="indexterm"/>other BI and data analysis tools still remains the biggest obstacle to their adoption. Supporting everything as a rich data structure in Scala is one of the options to converge on nested data representation.</p></div>
<div class="section" title="Sessionization"><div class="titlepage" id="aid-876R82"><div><div><h1 class="title"><a id="ch06lvl1sec4900000"/>Sessionization</h1></div></div></div><p>I will demonstrate<a id="id456000000" class="indexterm"/> the use of the complex or nested structures in the example of sessionization. In sessionization, we want to find the behavior of an entity, identified by some ID over a period of time. While the original records may come in any order, we want to summarize the behavior over time to derive trends.</p><p>We already analyzed web server logs in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span>. We found out how often different web pages are accessed over a period of time. We could dice and slice this information, but without analyzing the sequence of pages visited, it would be hard to understand each individual user interaction with the website. In this chapter, I would like to give this analysis more individual flavor by tracking the user navigation throughout the website. Sessionization is a common tool for website personalization and advertising, IoT tracking, telemetry, and enterprise security, in fact anything to do with entity behavior.</p><p>Let's assume the data comes as tuples of three elements (fields <code class="literal">1</code>, <code class="literal">5</code>, <code class="literal">11</code> in the original dataset in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span>):</p><div class="informalexample"><pre class="programlisting">(id, timestamp, path)</pre></div><p>Here, <code class="literal">id</code> is a unique entity ID, timestamp is an event <code class="literal">timestamp</code> (in any sortable format: Unix timestamp or an ISO8601 date format), and <code class="literal">path</code> is some indication of the location on the web server page hierarchy.</p><p>For people familiar with SQL, sessionization, or at least a subset of it, is better known as a windowing analytics function:</p><div class="informalexample"><pre class="programlisting">SELECT id, timestamp, path 
  ANALYTIC_FUNCTION(path) OVER (PARTITION BY id ORDER BY timestamp) AS agg
FROM log_table;</pre></div><p>Here <code class="literal">ANALYTIC_FUNCTION</code> is some transformation on the sequence of paths for a given <code class="literal">id</code>. While this approach works for a relatively simple function, such as first, last, lag, average, expressing a complex function over a sequence of paths is usually very convoluted (for example, nPath <a id="id457000000" class="indexterm"/>from Aster Data (<a class="ulink" href="https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf">https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf</a>)). Besides, without additional preprocessing and partitioning, these approaches usually result in big data transfers across multiple nodes in a distributed setting.</p><p>While in a pure<a id="id458000000" class="indexterm"/> functional approach, one would just have to design a function—or a sequence of function applications—to produce the desired answers from the original set of tuples, I will create two helper objects that will help us to simplify working with the concept of a user session. As an additional benefit, the new nested structures can be persisted on a disk to speed up getting answers on additional questions.</p><p>Let's see how it's done in Spark/Scala using case classes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell</strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; :paste</strong></span>
<span class="strong"><strong>// Entering paste mode (ctrl-D to finish)</strong></span>

<span class="strong"><strong>import java.io._</strong></span>

<span class="strong"><strong>// a basic page view structure</strong></span>
<span class="strong"><strong>@SerialVersionUID(123L)</strong></span>
<span class="strong"><strong>case class PageView(ts: String, path: String) extends Serializable with Ordered[PageView] {</strong></span>
<span class="strong"><strong>  override def toString: String = {</strong></span>
<span class="strong"><strong>    s"($ts :$path)"</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  def compare(other: PageView) = ts compare other.ts</strong></span>
<span class="strong"><strong>}</strong></span>

<span class="strong"><strong>// represent a session</strong></span>
<span class="strong"><strong>@SerialVersionUID(456L)</strong></span>
<span class="strong"><strong>case class Session[A  &lt;: PageView](id: String, visits: Seq[A]) extends Serializable {</strong></span>
<span class="strong"><strong>  override def toString: String = {</strong></span>
<span class="strong"><strong>    val vsts = visits.mkString("[", ",", "]")</strong></span>
<span class="strong"><strong>    s"($id -&gt; $vsts)"</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>}^D</strong></span>
<span class="strong"><strong>// Exiting paste mode, now interpreting.</strong></span>

<span class="strong"><strong>import java.io._</strong></span>
<span class="strong"><strong>defined class PageView</strong></span>
<span class="strong"><strong>defined class Session</strong></span>
</pre></div><p>The first class <a id="id459000000" class="indexterm"/>will represent a single page view with a timestamp, which, in this case, is an ISO8601 <code class="literal">String</code>, while the second a sequence of page views. Could we do it by encoding both members as a <code class="literal">String</code> with a object separator? Absolutely, but representing the fields as members of a class gives us nice access semantics, together with offloading some of the work that we need to perform on the compiler, which is always nice.</p><p>Let's read the previously described log files and construct the objects:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val rdd = sc.textFile("log.csv").map(x =&gt; { val z = x.split(",",3); (z(1), new PageView(z(0), z(2))) } ).groupByKey.map( x =&gt; { new Session(x._1, x._2.toSeq.sorted) } ).persist</strong></span>
<span class="strong"><strong>rdd: org.apache.spark.rdd.RDD[Session] = MapPartitionsRDD[14] at map at &lt;console&gt;:31</strong></span>

<span class="strong"><strong>scala&gt; rdd.take(3).foreach(println)</strong></span>
<span class="strong"><strong>(189.248.74.238 -&gt; [(2015-08-23 23:09:16 :mycompanycom&gt;homepage),(2015-08-23 23:11:00 :mycompanycom&gt;homepage),(2015-08-23 23:11:02 :mycompanycom&gt;running:slp),(2015-08-23 23:12:01 :mycompanycom&gt;running:slp),(2015-08-23 23:12:03 :mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp),(2015-08-23 23:12:42 :mycompanycom&gt;running:slp),(2015-08-23 23:13:25 :mycompanycom&gt;homepage),(2015-08-23 23:14:00 :mycompanycom&gt;homepage),(2015-08-23 23:14:06 :mycompanycom:mobile&gt;mycompany photoid&gt;landing),(2015-08-23 23:14:56 :mycompanycom&gt;men&gt;shoes:segmentedgrid),(2015-08-23 23:15:10 :mycompanycom&gt;homepage)])</strong></span>
<span class="strong"><strong>(82.166.130.148 -&gt; [(2015-08-23 23:14:27 :mycompanycom&gt;homepage)])</strong></span>
<span class="strong"><strong>(88.234.248.111 -&gt; [(2015-08-23 22:36:10 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:36:20 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:36:28 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:36:30 :mycompanycom&gt;plus&gt;onepluspdp&gt;sport band),(2015-08-23 22:36:52 :mycompanycom&gt;onsite search&gt;results found),(2015-08-23 22:37:19 :mycompanycom&gt;plus&gt;onepluspdp&gt;sport band),(2015-08-23 22:37:21 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:37:39 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:37:43 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:37:46 :mycompanycom&gt;plus&gt;onepluspdp&gt;sport watch),(2015-08-23 22:37:50 :mycompanycom&gt;gear&gt;mycompany+ sportwatch:standardgrid),(2015-08-23 22:38:14 :mycompanycom&gt;homepage),(2015-08-23 22:38:35 :mycompanycom&gt;homepage),(2015-08-23 22:38:37 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:39:01 :mycompanycom&gt;homepage),(2015-08-23 22:39:24 :mycompanycom&gt;homepage),(2015-08-23 22:39:26 :mycompanycom&gt;plus&gt;whatismycompanyfuel)])</strong></span>
</pre></div><p>Bingo! We <a id="id460000000" class="indexterm"/>have an RDD of Sessions, one per each unique IP address. The IP <code class="literal">189.248.74.238</code> has a session that lasted from <code class="literal">23:09:16</code> to <code class="literal">23:15:10</code>, and seemingly ended after browsing for men's shoes. The session for IP <code class="literal">82.166.130.148</code> contains only one hit. The last session concentrated on sports watch and lasted for over three minutes from <code class="literal">2015-08-23 22:36:10</code> to <code class="literal">2015-08-23 22:39:26</code>. Now, we can easily ask questions involving specific navigation path patterns. For example, we want analyze all the sessions that resulted in checkout (the path contains <code class="literal">checkout</code>) and see the number of hits and the distribution of times after the last hit on homepage:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import java.time.ZoneOffset</strong></span>
<span class="strong"><strong>import java.time.ZoneOffset</strong></span>

<span class="strong"><strong>scala&gt; import java.time.LocalDateTime</strong></span>
<span class="strong"><strong>import java.time.LocalDateTime</strong></span>

<span class="strong"><strong>scala&gt; import java.time.format.DateTimeFormatter</strong></span>
<span class="strong"><strong>import java.time.format.DateTimeFormatter</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>
<span class="strong"><strong>scala&gt; def toEpochSeconds(str: String) : Long = { LocalDateTime.parse(str, DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")).toEpochSecond(ZoneOffset.UTC) }</strong></span>
<span class="strong"><strong>toEpochSeconds: (str: String)Long</strong></span>

<span class="strong"><strong>scala&gt; val checkoutPattern = ".*&gt;checkout.*".r.pattern</strong></span>
<span class="strong"><strong>checkoutPattern: java.util.regex.Pattern = .*&gt;checkout.*</strong></span>

<span class="strong"><strong>scala&gt; val lengths = rdd.map(x =&gt; { val pths = x.visits.map(y =&gt; y.path); val pchs = pths.indexWhere(checkoutPattern.matcher(_).matches); (x.id, x.visits.map(y =&gt; y.ts).min, x.visits.map(y =&gt; y.ts).max, x.visits.lastIndexWhere(_ match { case PageView(ts, "mycompanycom&gt;homepage") =&gt; true; case _ =&gt; false }, pchs), pchs, x.visits) } ).filter(_._4&gt;0).filter(t =&gt; t._5&gt;t._4).map(t =&gt; (t._5 - t._4, toEpochSeconds(t._6(t._5).ts) - toEpochSeconds(t._6(t._4).ts)))</strong></span>

<span class="strong"><strong>scala&gt; lengths.toDF("cnt", "sec").agg(avg($"cnt"),min($"cnt"),max($"cnt"),avg($"sec"),min($"sec"),max($"sec")).show</strong></span>
<span class="strong"><strong>+-----------------+--------+--------+------------------+--------+--------+</strong></span>

<span class="strong"><strong>|         avg(cnt)|min(cnt)|max(cnt)|          avg(sec)|min(sec)|max(sec)|</strong></span>
<span class="strong"><strong>+-----------------+--------+--------+------------------+--------+--------+</strong></span>
<span class="strong"><strong>|19.77570093457944|       1|     121|366.06542056074767|      15|    2635|</strong></span>
<span class="strong"><strong>+-----------------+--------+--------+------------------+--------+--------+</strong></span>


<span class="strong"><strong>scala&gt; lengths.map(x =&gt; (x._1,1)).reduceByKey(_+_).sortByKey().collect</strong></span>
<span class="strong"><strong>res18: Array[(Int, Int)] = Array((1,1), (2,8), (3,2), (5,6), (6,7), (7,9), (8,10), (9,4), (10,6), (11,4), (12,4), (13,2), (14,3), (15,2), (17,4), (18,6), (19,1), (20,1), (21,1), (22,2), (26,1), (27,1), (30,2), (31,2), (35,1), (38,1), (39,2), (41,1), (43,2), (47,1), (48,1), (49,1), (65,1), (66,1), (73,1), (87,1), (91,1), (103,1), (109,1), (121,1))</strong></span>
</pre></div><p>The sessions last<a id="id461000000" class="indexterm"/> from 1 to 121 hits with a mode at 8 hits and from 15 to 2653 seconds (or about 45 minutes). Why would you be interested in this information? Long sessions might indicate that there was a problem somewhere in the middle of the session: a long delay or non-responsive call. It does not have to be: the person might just have taken a long lunch break or a call to discuss his potential purchase, but there might be something of interest here. At least one should agree that this is an outlier and needs to be carefully analyzed.</p><p>Let's talk about persisting this data to the disk. As you've seen, our transformation is written as a long pipeline, so there is nothing in the result that one could not compute from the raw data. This is a functional approach, the data is immutable. Moreover, if there is an error in our processing, let's say I want to change the homepage to some other anchor page, I can always modify the function as opposed to data. You may be content or not with this fact, but there is absolutely no additional piece of information in the result—transformations only increase the disorder and entropy. They might make it more palatable for humans, but this is only because humans are a very inefficient data-processing apparatus.</p><div class="note" title="Note"><h3 class="title"><a id="tip09000000"/>Tip</h3><p>
<span class="strong"><strong>Why rearranging the data makes the analysis faster?</strong></span>
</p><p>Sessionization<a id="id462000000" class="indexterm"/> seems just a simple rearranging of data—we just put the pages that were accessed in sequence together. Yet, in many cases, it makes practical data analysis run 10 to 100 times faster. The reason is data locality. The analysis, like filtering or path matching, most often tends to happen on the pages in one session at a time. Deriving user features requires all page views or interactions of the user to be in one place on disk and memory. This often beats other inefficiencies such as the overhead of encoding/decoding the nested structures as this can happen in local L1/L2 cache as opposed to data transfers from RAM or disk, which are much more expensive in modern multithreaded CPUs. This very much depends on the complexity of the analysis, of course.</p></div><p>There is a reason<a id="id463000000" class="indexterm"/> to persist the new data to the disk, and we can do it with either CSV, Avro, or Parquet format. The reason is that we do not want to reprocess the data if we want to look at them again. The new representation might be more compact and more efficient to retrieve and show to my manager. Really, humans like side effects and, fortunately, Scala/Spark allows you to do this as was described in the previous section.</p><p>Well, well, well...will say the people familiar with sessionization. This is only a part of the story. We want to split the path sequence into multiple sessions, run path analysis, compute conditional probabilities for page transitions, and so on. This is exactly where the functional paradigm shines. Write the following function:</p><div class="informalexample"><pre class="programlisting">def splitSession(session: Session[PageView]) : Seq[Session[PageView]] = { … }</pre></div><p>Then run the following code:</p><div class="informalexample"><pre class="programlisting">val newRdd = rdd.flatMap(splitSession)</pre></div><p>Bingo! The result is the session's split. I intentionally left the implementation out; it's the implementation that is user-dependent, not the data, and every analyst might have it's own way to split the sequence of page visits into sessions.</p><p>Another use case to apply the function is feature generation for applying machine learning…well, this is already hinting at the side effect: we want to modify the state of the world to make it more personalized and user-friendly. I guess one cannot avoid it after all.</p></div>
<div class="section" title="Working with traits" id="aid-885BQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec5000000"/>Working with traits</h1></div></div></div><p>As we saw, case <a id="id464000000" class="indexterm"/>classes significantly simplify handling of new nested data structures that we want to construct. The case class definition is probably the most convincing reason to move from Java (and SQL) to Scala. Now, what about the methods? How do we quickly add methods to a class without expensive recompilation? Scala allows you to do this transparently with traits!</p><p>A fundamental feature of functional programming is that functions are a first class citizen on par with objects. In the previous section, we defined the two <code class="literal">EpochSeconds</code> functions that transform the ISO8601 format to epoch time in seconds. We also suggested the <code class="literal">splitSession</code> function that provides a multi-session view for a given IP. How do we associate this or other behavior with a given class?</p><p>First, let's define a desired behavior:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; trait Epoch {</strong></span>
<span class="strong"><strong>     |   this: PageView =&gt;</strong></span>
<span class="strong"><strong>     |   def epoch() : Long = { LocalDateTime.parse(ts, DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")).toEpochSecond(ZoneOffset.UTC) }</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>defined trait Epoch</strong></span>
</pre></div><p>This basically creates a <code class="literal">PageView</code>-specific function that converts a string representation for datetime to epoch time in seconds. Now, if we just make the following transformation:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val rddEpoch = rdd.map(x =&gt; new Session(x.id, x.visits.map(x =&gt; new PageView(x.ts, x.path) with Epoch)))</strong></span>
<span class="strong"><strong>rddEpoch: org.apache.spark.rdd.RDD[Session[PageView with Epoch]] = MapPartitionsRDD[20] at map at &lt;console&gt;:31</strong></span>
</pre></div><p>We now have a new RDD of page views with additional behavior. For example, if we want to find out what is the time spent on each individual page in a session is, we will run a pipeline, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; rddEpoch.map(x =&gt; (x.id, x.visits.zip(x.visits.tail).map(x =&gt; (x._2.path, x._2.epoch - x._1.epoch)).mkString("[", ",", "]"))).take(3).foreach(println)</strong></span>
<span class="strong"><strong>(189.248.74.238,[(mycompanycom&gt;homepage,104),(mycompanycom&gt;running:slp,2),(mycompanycom&gt;running:slp,59),(mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp,2),(mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp,5),(mycompanycom&gt;running&gt;stories&gt;2013&gt;04&gt;themycompanyfreestore:cdp,0),(mycompanycom&gt;running:slp,34),(mycompanycom&gt;homepage,43),(mycompanycom&gt;homepage,35),(mycompanycom:mobile&gt;mycompany photoid&gt;landing,6),(mycompanycom&gt;men&gt;shoes:segmentedgrid,50),(mycompanycom&gt;homepage,14)])</strong></span>
<span class="strong"><strong>(82.166.130.148,[])</strong></span>
<span class="strong"><strong>(88.234.248.111,[(mycompanycom&gt;plus&gt;home,10),(mycompanycom&gt;plus&gt;home,8),(mycompanycom&gt;plus&gt;onepluspdp&gt;sport band,2),(mycompanycom&gt;onsite search&gt;results found,22),(mycompanycom&gt;plus&gt;onepluspdp&gt;sport band,27),(mycompanycom&gt;plus&gt;home,2),(mycompanycom&gt;plus&gt;home,18),(mycompanycom&gt;plus&gt;home,4),(mycompanycom&gt;plus&gt;onepluspdp&gt;sport watch,3),(mycompanycom&gt;gear&gt;mycompany+ sportwatch:standardgrid,4),(mycompanycom&gt;homepage,24),(mycompanycom&gt;homepage,21),(mycompanycom&gt;plus&gt;products landing,2),(mycompanycom&gt;homepage,24),(mycompanycom&gt;homepage,23),(mycompanycom&gt;plus&gt;whatismycompanyfuel,2)])</strong></span>
</pre></div><p>Multiple traits can be added at the same time without affecting either the original class definitions or <a id="id465000000" class="indexterm"/>original data. No recompilation is required.</p></div>
<div class="section" title="Working with pattern matching"><div class="titlepage" id="aid-893SC2"><div><div><h1 class="title"><a id="ch06lvl1sec5100000"/>Working with pattern matching</h1></div></div></div><p>No Scala book would be complete without mentioning the match/case statements. Scala has a very rich <a id="id466000000" class="indexterm"/>pattern-matching mechanism. For instance, let's say we want to find all instances of a sequence of page views that start with a homepage followed by a products page—we really want to filter out the determined buyers. This may be accomplished with a new function, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; def findAllMatchedSessions(h: Seq[Session[PageView]], s: Session[PageView]) : Seq[Session[PageView]] = {</strong></span>
<span class="strong"><strong>     |     def matchSessions(h: Seq[Session[PageView]], id: String, p: Seq[PageView]) : Seq[Session[PageView]] = {</strong></span>
<span class="strong"><strong>     |       p match {</strong></span>
<span class="strong"><strong>     |         case Nil =&gt; Nil</strong></span>
<span class="strong"><strong>     |         case PageView(ts1, "mycompanycom&gt;homepage") :: PageView(ts2, "mycompanycom&gt;plus&gt;products landing") :: tail =&gt;</strong></span>
<span class="strong"><strong>     |           matchSessions(h, id, tail).+:(new Session(id, p))</strong></span>
<span class="strong"><strong>     |         case _ =&gt; matchSessions(h, id, p.tail)</strong></span>
<span class="strong"><strong>     |       }</strong></span>
<span class="strong"><strong>     |     }</strong></span>
<span class="strong"><strong>     |    matchSessions(h, s.id, s.visits)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>findAllSessions: (h: Seq[Session[PageView]], s: Session[PageView])Seq[Session[PageView]]</strong></span>
</pre></div><p>Note that we explicitly put <code class="literal">PageView</code> constructors in the case statement! Scala will traverse the <code class="literal">visits</code> sequence and generate new sessions that match the specified two <code class="literal">PageViews</code>, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; rdd.flatMap(x =&gt; findAllMatchedSessions(Nil, x)).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(88.234.248.111 -&gt; [(2015-08-23 22:38:35 :mycompanycom&gt;homepage),(2015-08-23 22:38:37 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:39:01 :mycompanycom&gt;homepage),(2015-08-23 22:39:24 :mycompanycom&gt;homepage),(2015-08-23 22:39:26 :mycompanycom&gt;plus&gt;whatismycompanyfuel)])</strong></span>
<span class="strong"><strong>(148.246.218.251 -&gt; [(2015-08-23 22:52:09 :mycompanycom&gt;homepage),(2015-08-23 22:52:16 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:52:23 :mycompanycom&gt;homepage),(2015-08-23 22:52:32 :mycompanycom&gt;homepage),(2015-08-23 22:52:39 :mycompanycom&gt;running:slp)])</strong></span>
<span class="strong"><strong>(86.30.116.229 -&gt; [(2015-08-23 23:15:00 :mycompanycom&gt;homepage),(2015-08-23 23:15:02 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:15:12 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:15:18 :mycompanycom&gt;language tunnel&gt;load),(2015-08-23 23:15:23 :mycompanycom&gt;language tunnel&gt;geo selected),(2015-08-23 23:15:24 :mycompanycom&gt;homepage),(2015-08-23 23:15:27 :mycompanycom&gt;homepage),(2015-08-23 23:15:30 :mycompanycom&gt;basketball:slp),(2015-08-23 23:15:38 :mycompanycom&gt;basketball&gt;lebron-10:cdp),(2015-08-23 23:15:50 :mycompanycom&gt;basketball&gt;lebron-10:cdp),(2015-08-23 23:16:05 :mycompanycom&gt;homepage),(2015-08-23 23:16:09 :mycompanycom&gt;homepage),(2015-08-23 23:16:11 :mycompanycom&gt;basketball:slp),(2015-08-23 23:16:29 :mycompanycom&gt;onsite search&gt;results found),(2015-08-23 23:16:39 :mycompanycom&gt;onsite search&gt;no results)])</strong></span>
<span class="strong"><strong>(204.237.0.130 -&gt; [(2015-08-23 23:26:23 :mycompanycom&gt;homepage),(2015-08-23 23:26:27 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:26:35 :mycompanycom&gt;plus&gt;fuelband activity&gt;summary&gt;wk)])</strong></span>
<span class="strong"><strong>(97.82.221.34 -&gt; [(2015-08-23 22:36:24 :mycompanycom&gt;homepage),(2015-08-23 22:36:32 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:37:09 :mycompanycom&gt;plus&gt;plus activity&gt;summary&gt;wk),(2015-08-23 22:37:39 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:44:17 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:44:33 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:44:34 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:44:36 :mycompanycom&gt;plus&gt;home),(2015-08-23 22:44:43 :mycompanycom&gt;plus&gt;home)])</strong></span>
<span class="strong"><strong>(24.230.204.72 -&gt; [(2015-08-23 22:49:58 :mycompanycom&gt;homepage),(2015-08-23 22:50:00 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 22:50:30 :mycompanycom&gt;homepage),(2015-08-23 22:50:38 :mycompanycom&gt;homepage),(2015-08-23 22:50:41 :mycompanycom&gt;training:cdp),(2015-08-23 22:51:56 :mycompanycom&gt;training:cdp),(2015-08-23 22:51:59 :mycompanycom&gt;store locator&gt;start),(2015-08-23 22:52:28 :mycompanycom&gt;store locator&gt;landing)])</strong></span>
<span class="strong"><strong>(62.248.72.18 -&gt; [(2015-08-23 23:14:27 :mycompanycom&gt;homepage),(2015-08-23 23:14:30 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:14:33 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:14:40 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:14:47 :mycompanycom&gt;store homepage),(2015-08-23 23:14:50 :mycompanycom&gt;store homepage),(2015-08-23 23:14:55 :mycompanycom&gt;men:clp),(2015-08-23 23:15:08 :mycompanycom&gt;men:clp),(2015-08-23 23:15:15 :mycompanycom&gt;men:clp),(2015-08-23 23:15:16 :mycompanycom&gt;men:clp),(2015-08-23 23:15:24 :mycompanycom&gt;men&gt;sportswear:standardgrid),(2015-08-23 23:15:41 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:49 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:50 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:56 :mycompanycom&gt;men&gt;sportswear:standardgrid),(2015-08-23 23:18:41 :mycompanycom&gt;pdp&gt;mycompany bruin low men's shoe),(2015-08-23 23:18:42 :mycompanycom&gt;pdp&gt;mycompany bruin low men's shoe),(2015-08-23 23:18:53 :mycompanycom&gt;pdp&gt;mycompany bruin low men's shoe),(2015-08-23 23:18:55 :mycompanycom&gt;pdp&gt;mycompany bruin low men's shoe),(2015-08-23 23:18:57 :mycompanycom&gt;pdp&gt;mycompany bruin low men's shoe),(2015-08-23 23:19:04 :mycompanycom&gt;men&gt;sportswear:standardgrid),(2015-08-23 23:20:12 :mycompanycom&gt;men&gt;sportswear&gt;silver:standardgrid),(2015-08-23 23:28:20 :mycompanycom&gt;onsite search&gt;no results),(2015-08-23 23:28:33 :mycompanycom&gt;onsite search&gt;no results),(2015-08-23 23:28:36 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:40 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:41 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom&gt;pdp&gt;mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:29:00 :mycompanycom&gt;pdp:mycompanyid&gt;mycompany blazer low id shoe)])</strong></span>
<span class="strong"><strong>(46.5.127.21 -&gt; [(2015-08-23 22:58:00 :mycompanycom&gt;homepage),(2015-08-23 22:58:01 :mycompanycom&gt;plus&gt;products landing)])</strong></span>
<span class="strong"><strong>(200.45.228.1 -&gt; [(2015-08-23 23:07:33 :mycompanycom&gt;homepage),(2015-08-23 23:07:39 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:07:42 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:07:45 :mycompanycom&gt;language tunnel&gt;load),(2015-08-23 23:07:59 :mycompanycom&gt;homepage),(2015-08-23 23:08:15 :mycompanycom&gt;homepage),(2015-08-23 23:08:26 :mycompanycom&gt;onsite search&gt;results found),(2015-08-23 23:08:43 :mycompanycom&gt;onsite search&gt;no results),(2015-08-23 23:08:49 :mycompanycom&gt;onsite search&gt;results found),(2015-08-23 23:08:53 :mycompanycom&gt;language tunnel&gt;load),(2015-08-23 23:08:55 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:09:04 :mycompanycom&gt;homepage),(2015-08-23 23:11:34 :mycompanycom&gt;running:slp)])</strong></span>
<span class="strong"><strong>(37.78.203.213 -&gt; [(2015-08-23 23:18:10 :mycompanycom&gt;homepage),(2015-08-23 23:18:12 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:18:14 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:18:22 :mycompanycom&gt;plus&gt;products landing),(2015-08-23 23:18:25 :mycompanycom&gt;store homepage),(2015-08-23 23:18:31 :mycompanycom&gt;store homepage),(2015-08-23 23:18:34 :mycompanycom&gt;men:clp),(2015-08-23 23:18:50 :mycompanycom&gt;store homepage),(2015-08-23 23:18:51 :mycompanycom&gt;footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom&gt;men&gt;footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom&gt;men&gt;footwear:segmentedgrid),(2015-08-23 23:19:26 :mycompanycom&gt;men&gt;footwear&gt;new releases:standardgrid),(2015-08-23 23:19:26 :mycompanycom&gt;men&gt;footwear&gt;new releases:standardgrid),(2015-08-23 23:19:35 :mycompanycom&gt;pdp&gt;mycompany cheyenne 2015 men's shoe),(2015-08-23 23:19:40 :mycompanycom&gt;men&gt;footwear&gt;new releases:standardgrid)])</strong></span>
</pre></div><p>I leave it to the <a id="id467000000" class="indexterm"/>reader to write a function that also filters only those sessions where the user spent less than 10 seconds before going to the products page. The epoch trait or the previously defined to the <code class="literal">EpochSeconds</code> function may be useful.</p><p>The match/case function can be also used for feature generation and return a vector of features over a session.</p></div>
<div class="section" title="Other uses of unstructured data" id="aid-8A2CU1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec5200000"/>Other uses of unstructured data</h1></div></div></div><p>The personalization and device diagnostic obviously are not the only uses of unstructured data. The preceding case is a good example as we started from structured record and quickly converged <a id="id468000000" class="indexterm"/>on the need to construct an unstructured data structure to simplify the analysis.</p><p>In fact, there are many more unstructured data than there are structured; it is just the convenience of having the flat structure for the traditional statistical analysis that makes us to present the data as a set of records. Text, images, and music are the examples of semi-structured data.</p><p>One example of non-structured data is denormalized data. Traditionally the record data are normalized mostly for performance reasons as the RDBMSs have been optimized to work with structured data. This leads to foreign key and lookup tables, but these are very hard to maintain if the dimensions change. Denormalized data does not have this problem as the lookup table can be stored with each record—it is just an additional table object associated with a row, but may be less storage-efficient.</p></div>
<div class="section" title="Probabilistic structures" id="aid-8B0TG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec5300000"/>Probabilistic structures</h1></div></div></div><p>Another use case is the probabilistic structures. Usually people assume that answering a question is <a id="id469000000" class="indexterm"/>deterministic. As I showed in <a class="link" title="Chapter 2. Data Pipelines and Modeling" href="part0242.xhtml#aid-76P842">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>, in many cases, the true answer has some uncertainty associated with it. One of the most popular ways to encode uncertainty is probability, which is a frequentist approach, meaning that the simple count of when the answer does happen to be the true answer, divided by the total number of attempts—the probability also can encode our beliefs. I will touch on probabilistic analysis and models in the following chapters, but probabilistic analysis requires storing each possible outcome with some measure of probability, which happens to be a nested structure.</p></div>
<div class="section" title="Projections" id="aid-8BVE21"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec5400000"/>Projections</h1></div></div></div><p>One way to deal <a id="id470000000" class="indexterm"/>with high dimensionality is projections on a lower dimensional space. The fundamental basis for why projections might work is Johnson-Lindenstrauss lemma. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. We will touch on random and other projections when we talk about NLP in <a class="link" title="Chapter 9. NLP in Scala" href="part0291.xhtml#aid-8LGJM2">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>, but the random projections work well for nested structures and functional programming language, as in many cases, generating a random projection is the question of applying a function to a compactly encoded data rather than flattening the data explicitly. In other words, the Scala definition for a random projection may look like functional paradigm shines. Write the following function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>def randomeProjecton(data: NestedStructure) : Vector = { … }</strong></span>
</pre></div><p>Here, <code class="literal">Vector</code> is in low dimensional space.</p><p>The map used for embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</p></div>
<div class="section" title="Summary" id="aid-8CTUK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec5500000"/>Summary</h1></div></div></div><p>In this chapter, we saw examples of how to represent and work with complex and nested data in Scala. Obviously, it would be hard to cover all the cases as the world of unstructured data is much larger than the nice niche of structured row-by-row simplification of the real world and is still under construction. Pictures, music, and spoken and written language have a lot of nuances that are hard to capture in a flat representation.</p><p>While for ultimate data analysis, we eventually convert the datasets to the record-oriented flat representation, at least at the time of collection, one needs to be careful to store that data as it is and not throw away useful information that might be contained in data or metadata. Extending the databases and storage with a way to record this useful information is the first step. The next one is to use languages that can effectively analyze this information; which is definitely Scala.</p><p>In the next chapter we'll look at somewhat related topic of working with graphs, a specific example of non-structured data.</p></div>
<div class="chapter" title="Chapter&#xA0;7.&#xA0;Working with Graph Algorithms" id="aid-8DSF61"><div class="titlepage"><div><div><h1 class="title"><a id="ch33"/>Chapter 7. Working with Graph Algorithms</h1></div></div></div><p>In this chapter, I'll delve into graph libraries and algorithm implementations in Scala. In particular, I will introduce Graph for Scala (<a class="ulink" href="http://www.scala-graph.org">http://www.scala-graph.org</a>), an open source project that was <a id="id471000000" class="indexterm"/>started in 2011 in the EPFL Scala incubator. Graph for Scala does not support distributed computing yet—the distributed computing aspects of popular graph algorithms is available in GraphX, which is a part of MLlib library that is part of Spark project (<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-guide.html">http://spark.apache.org/docs/latest/mllib-guide.html</a>). Both, Spark and MLlib were started as class projects at UC Berkeley around or after 2009. I considered Spark in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span> and introduced an RDD. In GraphX, a graph is a pair of RDDs, each of which is partitioned among executors and tasks, represents vertices and edges in a graph.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Configuring <a id="id472000000" class="indexterm"/><span class="strong"><strong>Simple Build Tool</strong></span> (<span class="strong"><strong>SBT</strong></span>) to use the material in this chapter interactively</li><li class="listitem">Learning basic operations on graphs supported by Graph for Scala</li><li class="listitem">Learning how to enforce graph constraints</li><li class="listitem">Learning how to import/export graphs in JSON</li><li class="listitem">Performing connected components, triangle count, and strongly connected components running on Enron e-mail data</li><li class="listitem">Performing PageRank computations on Enron e-mail data</li><li class="listitem">Learning how to use SVD++</li></ul></div><div class="section" title="A quick introduction to graphs"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec5600000"/>A quick introduction to graphs</h1></div></div></div><p>What is a <a id="id473000000" class="indexterm"/>graph? A graph is <a id="id474000000" class="indexterm"/>a set of <span class="strong"><strong>vertices</strong></span> where some pairs of these vertices are linked<a id="id475000000" class="indexterm"/> with <span class="strong"><strong>edges</strong></span>. If every vertex is linked with every other vertex, we say the graph is a complete graph. On the contrary, if it has no edges, the graph is said to be empty. These are, of course, extremes that are rarely encountered in practice, as graphs have varying degrees of density; the more edges it has proportional to the number of vertices, the more dense we say it is.</p><p>Depending on <a id="id476000000" class="indexterm"/>what algorithms we intend to run on a graph and how dense is it expected to be, we can choose how to appropriately represent the graph in memory. If the graph is really dense, it pays off to store it as a square <span class="emphasis"><em>N x N</em></span> matrix, where <span class="emphasis"><em>0</em></span> in the <span class="emphasis"><em>n</em></span>th row and <span class="emphasis"><em>m</em></span>th column means that the <span class="emphasis"><em>n</em></span> vertex is not connected to the <span class="emphasis"><em>m</em></span> vertex. A diagonal entry expresses a node connection to itself. This representation is called the adjacency matrix.</p><p>If there are not many edges and we need to traverse the whole edge set without distinction, often it pays off to store it as a simple container of pairs. This structure is called an <a id="id477000000" class="indexterm"/>
<span class="strong"><strong>edge list</strong></span>.</p><p>In practice, we can model many real-life situations and events as graphs. We could imagine cities as vertices and plane routes as edges. If there is no flight between two cities, there is no edge between them. Moreover, if we add the numerical costs of plane tickets to the edges, we say that the <a id="id478000000" class="indexterm"/>graph is <span class="strong"><strong>weighted</strong></span>. If there are some edges where only travels in one direction exist, we can represent that by making a graph directed as opposed to an undirected graph. So, for an undirected graph, it is true that the graph is symmetric, that is, if <span class="emphasis"><em>A</em></span> is connected to <span class="emphasis"><em>B</em></span>, then <span class="emphasis"><em>B</em></span> is also connected to <span class="emphasis"><em>A</em></span>—that is not necessarily true for a directed graph.</p><p>Graphs without cycles are called acyclic. Multigraph can contain multiple edges, potentially of different type, between the nodes. Hyperedges can connect arbitrary number of nodes.</p><p>The most popular algorithm on the undirected graphs is probably <span class="strong"><strong>connected components</strong></span>, or <a id="id479000000" class="indexterm"/>partitioning of a graph into subgraph, in which any two vertices are connected to each other by paths. Partitioning is important to parallelize the operations on the graphs.</p><p>Google and other search engines made PageRank popular. According to Google, PageRank estimates of how important the website is by counting the number and quality of links to a page. The underlying assumption is that more important websites are likely to receive more links from other websites, especially more highly ranked ones. PageRank can be applied to many problems outside of websites ranking and is equivalent to finding eigenvectors and the most significant eigenvalue of the connectivity matrix.</p><p>The most basic, nontrivial subgraph, consists of three nodes. Triangle counting finds all the possible fully connected (or complete) triples of nodes and is another well-known algorithm used in community detection and CAD.</p><p>A <span class="strong"><strong>clique</strong></span><a id="id480000000" class="indexterm"/> is a fully connected subgraph. A strongly connected component is an analogous notion for a directed graph: every vertex in a subgraph is reachable from every other vertex. GraphX provides an implementation for both.</p><p>Finally, a recommender graph is a graph connecting two types of nodes: users and items. The edges can additionally contain the strength of a recommendation or a measure of satisfaction. The<a id="id481000000" class="indexterm"/> goal of a recommender is to predict the satisfaction for potentially missing edges. Multiple algorithms have been developed for a recommendation engine, such as SVD and SVD++, which are considered at the end of this chapter.</p></div></div>
<div class="section" title="SBT"><div class="titlepage" id="aid-8EQVO2"><div><div><h1 class="title"><a id="ch07lvl1sec5700000"/>SBT</h1></div></div></div><p>Everyone likes <a id="id482000000" class="indexterm"/>Scala REPL. REPL is the command line for Scala. It allows you to type Scala expressions that are evaluated immediately and try and explore things. As you saw in the previous chapters, one can simply type <code class="literal">scala</code> at the command prompt and start developing complex data pipelines. What is even more convenient is that one can press <span class="emphasis"><em>tab</em></span> to have auto-completion, a required feature of any fully developed modern IDE (such as Eclipse or IntelliJ, <span class="emphasis"><em>Ctrl +</em></span>. or <span class="emphasis"><em>Ctrl + Space</em></span>) by keeping track of the namespace and using reflection mechanisms. Why would we need one extra tool or framework for builds, particularly that other builds management frameworks such as Ant, Maven, and Gradle exist in addition to IDEs? As the SBT authors argue, even though one might compile Scala using the preceding tools, all of them have inefficiencies, as it comes to interactivity and reproducibility of Scala builds (<span class="emphasis"><em>SBT in Action</em></span> by <span class="emphasis"><em>Joshua Suereth</em></span> and <span class="emphasis"><em>Matthew Farwell</em></span>, Nov 2015).</p><p>One of the main SBT features for me is interactivity and the ability to seamlessly work with multiple versions of Scala and dependent libraries. In the end, what is critical for software development is the speed with which one can prototype and test new ideas. I used to work on mainframes using punch cards, where the programmers were waiting to execute their programs and ideas, sometimes for hours and days. The efficiency of the computers mattered more, as this was the bottleneck. These days are gone, and a personal laptop is probably having more computing power than rooms full of servers a few decades back. To take advantage of this efficiency, we need to utilize human time more efficiently by speeding <a id="id483000000" class="indexterm"/>up the program development cycle, which also means interactivity and more versions in the repositories.</p><p>Apart from the ability to<a id="id484000000" class="indexterm"/> handle multiple versions and REPL, SBT's main features are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Native support for compiling Scala code and integrating with many test frameworks, including JUnit, ScalaTest, and Selenium</li><li class="listitem">Build descriptions written in Scala using a DSL</li><li class="listitem">Dependency management using Ivy (which also supports Maven-format repositories)</li><li class="listitem">Continuous execution, compilation, testing, and deployment</li><li class="listitem">Integration with the Scala interpreter for rapid iteration and debugging</li><li class="listitem">Support for mixed Java/Scala projects</li><li class="listitem">Support for testing and deployment frameworks</li><li class="listitem">Ability to complement the tool with custom plugins</li><li class="listitem">Parallel execution of tasks</li></ul></div><p>SBT is written in Scala and uses SBT to build itself (bootstrapping or dogfooding). SBT became the de facto build tool for the <a id="id485000000" class="indexterm"/>Scala community, and is used by the <span class="strong"><strong>Lift</strong></span> and <a id="id486000000" class="indexterm"/>
<span class="strong"><strong>Play</strong></span> frameworks.</p><p>While you can download <a id="id487000000" class="indexterm"/>SBT directly from <a class="ulink" href="http://www.scala-sbt.org/download">http://www.scala-sbt.org/download</a>, the easiest way to install SBT on Mac is to run MacPorts:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ port install sbt</strong></span>
</pre></div><p>You can also run Homebrew:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ brew install sbt</strong></span>
</pre></div><p>While other tools exist to create SBT projects, the most straightforward way is to run the <code class="literal">bin/create_project.sh</code> script in the GitHub book project repository provided for each chapter:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/create_project.sh</strong></span>
</pre></div><p>This will create main and test source subdirectories (but not the code). The project directory contains project-wide settings (refer to <code class="literal">project/build.properties</code>). The target will contain compiled classes and build packages (the directory will contain different subdirectories for different versions of Scala, for example, 2.10 and 2.11). Finally, any jars or libraries put into the <code class="literal">lib</code> directory will be available across the project (I personally recommend using the <code class="literal">libraryDependencies</code> mechanism in the <code class="literal">build.sbt</code> file, but <a id="id488000000" class="indexterm"/>not all libraries are available via centralized repositories). This is the minimal setup, and the directory structure may potentially contain multiple subprojects. The Scalastyle plugin<a id="id489000000" class="indexterm"/> will even check the syntax for you (<a class="ulink" href="http://www.scalastyle.org/sbt.html">http://www.scalastyle.org/sbt.html</a>). Just add <code class="literal">project/plugin.sbt</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cat &gt;&gt; project.plugin.sbt &lt;&lt; EOF</strong></span>
<span class="strong"><strong>addSbtPlugin("org.scalastyle" %% "scalastyle-sbt-plugin" % "0.8.0")</strong></span>
<span class="strong"><strong>EOF</strong></span>
</pre></div><p>Finally, the SBT creates Scaladoc documentation with the <code class="literal">sdbt doc</code> command.</p><div class="note" title="Note"><h3 class="title"><a id="note06000000"/>Note</h3><p>
<span class="strong"><strong>Blank lines and other settings in build.sbt</strong></span>
</p><p>Probably most<a id="id490000000" class="indexterm"/> of the <code class="literal">build.sbt</code> files out there are double spaced: this is a remnant of old versions. You no longer need them. As of version 0.13.7, the definitions do not require extra lines.</p><p>There are <a id="id491000000" class="indexterm"/>many other settings that you can use on <code class="literal">build.sbt</code> or <code class="literal">build.properties</code>, the up-to-date documentation is available at <a class="ulink" href="http://www.scala-sbt.org/documentation.html">http://www.scala-sbt.org/documentation.html</a>.</p></div><p>When run from the command line, the tool will automatically download and use the dependencies, in this case, <code class="literal">graph-{core,constrained,json}</code> and <code class="literal">lift-json</code>. In order to run the project, simply type <code class="literal">sbt run</code>.</p><p>In continuous mode, SBT will automatically detect changes to the source file and rerun the command(s). In order to continuously compile and run the code, type <code class="literal">~~ run</code> after starting REPL with <code class="literal">sbt</code>.</p><p>To get help on the<a id="id492000000" class="indexterm"/> commands, run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sbt</strong></span>
<span class="strong"><strong> [info] Loading global plugins from /Users/akozlov/.sbt/0.13/plugins</strong></span>
<span class="strong"><strong>[info] Set current project to My Graph Project (in build file:/Users/akozlov/Scala/graph/)</strong></span>
<span class="strong"><strong>&gt; help</strong></span>

<span class="strong"><strong>  help                                    Displays this help message or prints detailed help on requested commands (run 'help &lt;command&gt;').</strong></span>
<span class="strong"><strong>For example, `sbt package` will build a Java jar, as follows:</strong></span>
<span class="strong"><strong>$  sbt package</strong></span>
<span class="strong"><strong>[info] Loading global plugins from /Users/akozlov/.sbt/0.13/plugins</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Scala/graph/project</strong></span>
<span class="strong"><strong>[info] Set current project to My Graph Project (in build file:/Users/akozlov/Scala/graph/)</strong></span>
<span class="strong"><strong>[info] Updating {file:/Users/akozlov/Scala/graph/}graph...</strong></span>
<span class="strong"><strong>[info] Resolving jline#jline;2.12.1 ...</strong></span>
<span class="strong"><strong>[info] Done updating.</strong></span>
<span class="strong"><strong>$ ls -1 target/scala-2.11/</strong></span>
<span class="strong"><strong>classes</strong></span>
<span class="strong"><strong>my-graph-project_2.11-1.0.jar</strong></span>
</pre></div><p>While SBT will be sufficient for our use even with a<a id="id493000000" class="indexterm"/> simple editor such as <span class="strong"><strong>vi</strong></span> or <a id="id494000000" class="indexterm"/>
<span class="strong"><strong>Emacs</strong></span>, the <code class="literal">sbteclipse</code> project <a id="id495000000" class="indexterm"/>at <a class="ulink" href="https://github.com/typesafehub/sbteclipse">https://github.com/typesafehub/sbteclipse</a> will create the necessary project files to work with your Eclipse IDE.</p></div>
<div class="section" title="Graph for Scala"><div class="titlepage" id="aid-8FPGA2"><div><div><h1 class="title"><a id="ch07lvl1sec5800000"/>Graph for Scala</h1></div></div></div><p>For this<a id="id496000000" class="indexterm"/> project, I will create a <code class="literal">src/main/scala/InfluenceDiagram.scala</code> file. For demo purpose, I will just recreate the graph from <a class="link" title="Chapter 2. Data Pipelines and Modeling" href="part0242.xhtml#aid-76P842">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>:</p><div class="informalexample"><pre class="programlisting">import scalax.collection.Graph
import scalax.collection.edge._
import scalax.collection.GraphPredef._
import scalax.collection.GraphEdge._

import scalax.collection.edge.Implicits._

object InfluenceDiagram extends App {
  var g = Graph[String, LDiEdge](("'Weather'"~+&gt;"'Weather Forecast'")("Forecast"), ("'Weather Forecast'"~+&gt;"'Vacation Activity'")("Decision"), ("'Vacation Activity'"~+&gt;"'Satisfaction'")("Deterministic"), ("'Weather'"~+&gt;"'Satisfaction'")("Deterministic"))
  println(g.mkString(";"))
  println(g.isDirected)
  println(g.isAcyclic)
}</pre></div><p>The <code class="literal">~+&gt;</code> operator<a id="id497000000" class="indexterm"/> is used to create a directed labeled edge between two nodes defined in <code class="literal">scalax/collection/edge/Implicits.scala</code>, which, in our case, are of the <code class="literal">String</code> type. The list of other edge types and operators is provided in the following table:</p><p>
<span class="emphasis"><em>The following table shows</em></span> graph<a id="id498000000" class="indexterm"/> edges from <code class="literal">scalax.collection.edge.Implicits</code> (from <a class="ulink" href="http://www.scala-graph.org/guides/core-initializing.html">http://www.scala-graph.org/guides/core-initializing.html</a>)</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Edge Class</p>
</th><th valign="bottom">
<p>Shortcut/Operator</p>
</th><th valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td colspan="3" valign="top" style="text-align: center">
<p>
<span class="strong"><strong>Hyperedges</strong></span>
</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">HyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~</code>
</p>
</td><td valign="top">
<p>hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%</code>
</p>
</td><td valign="top">
<p>weighted hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#</code></p>
</td><td valign="top">
<p>key-weighted hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+</code>
</p>
</td><td valign="top">
<p>labeled hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LkHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+#</code>
</p>
</td><td valign="top">
<p>key-labeled hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+</code>
</p>
</td><td valign="top">
<p>weighted labeled hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+</code>
</p>
</td><td valign="top">
<p>key-weighted labeled hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLkHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+#</code>
</p>
</td><td valign="top">
<p>weighted key-labeled hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLkHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+#</code>
</p>
</td><td valign="top">
<p>key-weighted key-labeled hyperedge</p>
</td></tr><tr><td colspan="3" valign="top" style="text-align: center">
<p>
<span class="strong"><strong>Directed hyperedges</strong></span>
</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">DiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~&gt;</code>
</p>
</td><td valign="top">
<p>directed <a id="id499000000" class="indexterm"/>hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%&gt;</code>
</p>
</td><td valign="top">
<p>weighted directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+&gt;</code>
</p>
</td><td valign="top">
<p>labeled directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LkDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+#&gt;</code>
</p>
</td><td valign="top">
<p>key-labeled directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+&gt;</code>
</p>
</td><td valign="top">
<p>weighted labeled directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted labeled directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLkDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+#&gt;</code>
</p>
</td><td valign="top">
<p>weighted key-labeled directed hyperedge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLkDiHyperEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+#&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted key-labeled directed hyperedge</p>
</td></tr><tr><td colspan="3" valign="top" style="text-align: center">
<p>
<span class="strong"><strong>Undirected edges</strong></span>
</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">UnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~</code>
</p>
</td><td valign="top">
<p>undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%</code>
</p>
</td><td valign="top">
<p>weighted undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#</code>
</p>
</td><td valign="top">
<p>key-weighted undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+</code>
</p>
</td><td valign="top">
<p>labeled <a id="id500000000" class="indexterm"/>undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LkUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+#</code>
</p>
</td><td valign="top">
<p>key-labeled undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+</code>
</p>
</td><td valign="top">
<p>weighted labeled undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+</code>
</p>
</td><td valign="top">
<p>key-weighted labeled undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLkUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+#</code>
</p>
</td><td valign="top">
<p>weighted key-labeled undirected edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLkUnDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+#</code>
</p>
</td><td valign="top">
<p>key-weighted key-labeled undirected edge</p>
</td></tr><tr><td colspan="3" valign="top" style="text-align: center">
<p>
<span class="strong"><strong>Directed edges</strong></span>
</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">DiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~&gt;</code>
</p>
</td><td valign="top">
<p>directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%&gt;</code>
</p>
</td><td valign="top">
<p>weighted directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+&gt;</code>
</p>
</td><td valign="top">
<p>labeled directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">LkDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~+#&gt;</code>
</p>
</td><td valign="top">
<p>key-labeled directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+&gt;</code>
</p>
</td><td valign="top">
<p>weighted labeled directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted labeled directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WLkDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%+#&gt;</code>
</p>
</td><td valign="top">
<p>weighted key-labeled directed edge</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">WkLkDiEdge</code>
</p>
</td><td valign="top">
<p>
<code class="literal">~%#+#&gt;</code>
</p>
</td><td valign="top">
<p>key-weighted key-labeled directed edge</p>
</td></tr></tbody></table></div><p>You saw the power <a id="id501000000" class="indexterm"/>of graph for Scala: the edges can be weighted and we may potentially construct a multigraph (key-labeled edges allow multiple edges for a pair of source and destination nodes).</p><p>If you run SBT on the preceding project with the Scala file in the <code class="literal">src/main/scala</code> directory, the output will be as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project</strong></span>
<span class="strong"><strong>[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)</strong></span>
<span class="strong"><strong>&gt; run</strong></span>
<span class="strong"><strong>[warn] Multiple main classes detected.  Run 'show discoveredMainClasses' to see the list</strong></span>

<span class="strong"><strong>Multiple main classes detected, select one to run:</strong></span>

<span class="strong"><strong> [1] org.akozlov.chapter07.ConstranedDAG</strong></span>
<span class="strong"><strong> [2] org.akozlov.chapter07.EnronEmail</strong></span>
<span class="strong"><strong> [3] org.akozlov.chapter07.InfluenceDiagram</strong></span>
<span class="strong"><strong> [4] org.akozlov.chapter07.InfluenceDiagramToJson</strong></span>

<span class="strong"><strong>Enter number: 3</strong></span>

<span class="strong"><strong>[info] Running org.akozlov.chapter07.InfluenceDiagram </strong></span>
<span class="strong"><strong>'Weather';'Vacation Activity';'Satisfaction';'Weather Forecast';'Weather'~&gt;'Weather Forecast' 'Forecast;'Weather'~&gt;'Satisfaction' 'Deterministic;'Vacation Activity'~&gt;'Satisfaction' 'Deterministic;'Weather Forecast'~&gt;'Vacation Activity' 'Decision</strong></span>
<span class="strong"><strong>Directed: true</strong></span>
<span class="strong"><strong>Acyclic: true</strong></span>
<span class="strong"><strong>'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~&gt;'Weather Forecast' 'Forecast;'Weather'~&gt;'Satisfaction' 'Deterministic;'Vacation Activity'~&gt;'Satisfaction' 'Deterministic;'Satisfaction'~&gt;'Recommend to a Friend' 'Probabilistic;'Weather Forecast'~&gt;'Vacation Activity' 'Decision</strong></span>
<span class="strong"><strong>Directed: true</strong></span>
<span class="strong"><strong>Acyclic: true</strong></span>
</pre></div><p>If continuous <a id="id502000000" class="indexterm"/>compilation is enabled, the main method will be run as soon as SBT detects that the file has changed (in the case of multiple classes having the main method, SBT will ask you which one to run, which is not great for interactivity; so you might want to limit the number of executable classes).</p><p>I will cover different output formats in a short while, but let's first see how to perform simple operations on the graph.</p><div class="section" title="Adding nodes and edges"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3200000"/>Adding nodes and edges</h2></div></div></div><p>First, we <a id="id503000000" class="indexterm"/>already know that the graph is directed and acyclic, which<a id="id504000000" class="indexterm"/> is a required property for all decision<a id="id505000000" class="indexterm"/> diagrams<a id="id506000000" class="indexterm"/> so that we know we did not make a mistake. Let's say that I want to make the graph more complex and add a node that will indicate the likelihood of me recommending a vacation in Portland, Oregon to another person. The only thing I need to add is the following line:</p><div class="informalexample"><pre class="programlisting">g += ("'Satisfaction'" ~+&gt; "'Recommend to a Friend'")("Probabilistic")</pre></div><p>If you have continuous compilation/run enabled, you will immediately see the changes after pressing the <span class="strong"><strong>Save File</strong></span> button:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~&gt;'Weather Forecast' 'Forecast;'Weather'~&gt;'Satisfaction' 'Deterministic;'Vacation Activity'~&gt;'Satisfaction' 'Deterministic;'Satisfaction'~&gt;'Recommend to a Friend' 'Probabilistic;'Weather Forecast'~&gt;'Vacation Activity' 'Decision</strong></span>
<span class="strong"><strong>Directed: true</strong></span>
<span class="strong"><strong>Acyclic: true</strong></span>
</pre></div><p>Now, if we want to know the parents of the newly introduced node, we can simply run the following code:</p><div class="informalexample"><pre class="programlisting">println((g get "'Recommend to a Friend'").incoming)

Set('Satisfaction'~&gt;'Recommend to a Friend' 'Probabilistic)</pre></div><p>This will give us a set of parents for a specific node—and thus drive the decision making process. If we add a cycle, the acyclic method will automatically detect it:</p><div class="informalexample"><pre class="programlisting">g += ("'Satisfaction'" ~+&gt; "'Weather'")("Cyclic")
println(g.mkString(";")) println("Directed: " + g.isDirected)
println("Acyclic: " + g.isAcyclic)

'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~&gt;'Weather Forecast' 'Forecast;'Weather'~&gt;'Satisfaction' 'Deterministic;'Vacation Activity'~&gt;'Satisfaction' 'Deterministic;'Satisfaction'~&gt;'Recommend to a Friend' 'Probabilistic;'Satisfaction'~&gt;'Weather' 'Cyclic;'Weather Forecast'~&gt;'Vacation Activity' 'Decision
Directed: true
Acyclic: false</pre></div><p>Note that you can create the graphs completely programmatically:</p><div class="informalexample"><pre class="programlisting"> var n, m = 0; val f = Graph.fill(45){ m = if (m &lt; 9) m + 1 else { n = if (n &lt; 8) n + 1 else 8; n + 1 }; m ~ n }

  println(f.nodes)
  println(f.edges)
  println(f)

  println("Directed: " + f.isDirected)
  println("Acyclic: " + f.isAcyclic)

NodeSet(0, 9, 1, 5, 2, 6, 3, 7, 4, 8)
EdgeSet(9~0, 9~1, 9~2, 9~3, 9~4, 9~5, 9~6, 9~7, 9~8, 1~0, 5~0, 5~1, 5~2, 5~3, 5~4, 2~0, 2~1, 6~0, 6~1, 6~2, 6~3, 6~4, 6~5, 3~0, 3~1, 3~2, 7~0, 7~1, 7~2, 7~3, 7~4, 7~5, 7~6, 4~0, 4~1, 4~2, 4~3, 8~0, 8~1, 8~2, 8~3, 8~4, 8~5, 8~6, 8~7)
Graph(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1~0, 2~0, 2~1, 3~0, 3~1, 3~2, 4~0, 4~1, 4~2, 4~3, 5~0, 5~1, 5~2, 5~3, 5~4, 6~0, 6~1, 6~2, 6~3, 6~4, 6~5, 7~0, 7~1, 7~2, 7~3, 7~4, 7~5, 7~6, 8~0, 8~1, 8~2, 8~3, 8~4, 8~5, 8~6, 8~7, 9~0, 9~1, 9~2, 9~3, 9~4, 9~5, 9~6, 9~7, 9~8)
Directed: false
Acyclic: false</pre></div><p>Here, the<a id="id507000000" class="indexterm"/> element<a id="id508000000" class="indexterm"/> computation <a id="id509000000" class="indexterm"/>provided as the second parameter to the fill method is<a id="id510000000" class="indexterm"/> repeated <code class="literal">45</code> times (the first parameter). The graph connects every node to all of its predecessors, which is also known as a clique in the graph theory.</p></div><div class="section" title="Graph constraints"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3300000"/>Graph constraints</h2></div></div></div><p>Graph for Scala <a id="id511000000" class="indexterm"/>enables us to set constraints that cannot be violated by any future graph update. This comes in handy when we want to preserve some detail in the graph structure. For example, a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) should <a id="id512000000" class="indexterm"/>not contain cycles. Two constraints are currently implemented as a part of the <code class="literal">scalax.collection.constrained.constraints</code> package—connected and acyclic, as follows:</p><div class="informalexample"><pre class="programlisting">package org.akozlov.chapter07

import scalax.collection.GraphPredef._, scalax.collection.GraphEdge._
import scalax.collection.constrained.{Config, ConstraintCompanion, Graph =&gt; DAG}
import scalax.collection.constrained.constraints.{Connected, Acyclic}

object AcyclicWithSideEffect extends ConstraintCompanion[Acyclic] {
  def apply [N, E[X] &lt;: EdgeLikeIn[X]] (self: DAG[N,E]) =
    new Acyclic[N,E] (self) {
      override def onAdditionRefused(refusedNodes: Iterable[N],
        refusedEdges: Iterable[E[N]],
        graph:        DAG[N,E]) = {
          println("Addition refused: " + "nodes = " + refusedNodes + ", edges = " + refusedEdges)
          true
        }
    }
}

object ConnectedWithSideEffect extends ConstraintCompanion[Connected] {
  def apply [N, E[X] &lt;: EdgeLikeIn[X]] (self: DAG[N,E]) =
    new Connected[N,E] (self) {
      override def onSubtractionRefused(refusedNodes: Iterable[DAG[N,E]#NodeT],
        refusedEdges: Iterable[DAG[N,E]#EdgeT],
        graph:        DAG[N,E]) = {
          println("Subtraction refused: " + "nodes = " + refusedNodes + ", edges = " + refusedEdges)
        true
      }
    }
}

class CycleException(msg: String) extends IllegalArgumentException(msg)
object ConstranedDAG extends App {
  implicit val conf: Config = ConnectedWithSideEffect &amp;&amp; AcyclicWithSideEffect
  val g = DAG(1~&gt;2, 1~&gt;3, 2~&gt;3, 3~&gt;4) // Graph()
  println(g ++ List(1~&gt;4, 3~&gt;1))
  println(g - 2~&gt;3)
  println(g - 2)
  println((g + 4~&gt;5) - 3)
}</pre></div><p>Here is the command <a id="id513000000" class="indexterm"/>to run the program that tries to add or remove nodes that violate the constraints:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt "run-main org.akozlov.chapter07.ConstranedDAG"</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project</strong></span>
<span class="strong"><strong>[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)</strong></span>
<span class="strong"><strong>[info] Running org.akozlov.chapter07.ConstranedDAG </strong></span>
<span class="strong"><strong>Addition refused: nodes = List(), edges = List(1~&gt;4, 3~&gt;1)</strong></span>
<span class="strong"><strong>Graph(1, 2, 3, 4, 1~&gt;2, 1~&gt;3, 2~&gt;3, 3~&gt;4)</strong></span>
<span class="strong"><strong>Subtraction refused: nodes = Set(), edges = Set(2~&gt;3)</strong></span>
<span class="strong"><strong>Graph(1, 2, 3, 4, 1~&gt;2, 1~&gt;3, 2~&gt;3, 3~&gt;4)</strong></span>
<span class="strong"><strong>Graph(1, 3, 4, 1~&gt;3, 3~&gt;4)</strong></span>
<span class="strong"><strong>Subtraction refused: nodes = Set(3), edges = Set()</strong></span>
<span class="strong"><strong>Graph(1, 2, 3, 4, 5, 1~&gt;2, 1~&gt;3, 2~&gt;3, 3~&gt;4, 4~&gt;5)</strong></span>
<span class="strong"><strong>[success] Total time: 1 s, completed May 1, 2016 1:53:42 PM </strong></span>
</pre></div><p>Adding or<a id="id514000000" class="indexterm"/> subtracting nodes that violate one of the constraints is rejected. The programmer can also specify a side effect if an attempt to add or subtract a node that violates the condition is made.</p></div><div class="section" title="JSON"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3400000"/>JSON</h2></div></div></div><p>Graph for Scala <a id="id515000000" class="indexterm"/>supports importing/exporting graphs to JSON, as <a id="id516000000" class="indexterm"/>follows:</p><div class="informalexample"><pre class="programlisting">object InfluenceDiagramToJson extends App {

  val g = Graph[String,LDiEdge](("'Weather'" ~+&gt; "'Weather Forecast'")("Forecast"), ("'Weather Forecast'" ~+&gt; "'Vacation Activity'")("Decision"), ("'Vacation Activity'" ~+&gt; "'Satisfaction'")("Deterministic"), ("'Weather'" ~+&gt; "'Satisfaction'")("Deterministic"), ("'Satisfaction'" ~+&gt; "'Recommend to a Friend'")("Probabilistic"))

  import scalax.collection.io.json.descriptor.predefined.{LDi}
  import scalax.collection.io.json.descriptor.StringNodeDescriptor
  import scalax.collection.io.json._

  val descriptor = new Descriptor[String](
    defaultNodeDescriptor = StringNodeDescriptor,
    defaultEdgeDescriptor = LDi.descriptor[String,String]("Edge")
  )

  val n = g.toJson(descriptor)
  println(n)
  import net.liftweb.json._
  println(Printer.pretty(JsonAST.render(JsonParser.parse(n))))
}</pre></div><p>To produce a JSON representation for a sample graph, run:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[kozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt "run-main org.akozlov.chapter07.InfluenceDiagramToJson"</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project</strong></span>
<span class="strong"><strong>[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)</strong></span>
<span class="strong"><strong>[info] Running org.akozlov.chapter07.InfluenceDiagramToJson </strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "nodes":[["'Recommend to a Friend'"],["'Satisfaction'"],["'Vacation Activity'"],["'Weather Forecast'"],["'Weather'"]],</strong></span>
<span class="strong"><strong>  "edges":[{</strong></span>
<span class="strong"><strong>    "n1":"'Weather'",</strong></span>
<span class="strong"><strong>    "n2":"'Weather Forecast'",</strong></span>
<span class="strong"><strong>    "label":"Forecast"</strong></span>
<span class="strong"><strong>  },{</strong></span>
<span class="strong"><strong>    "n1":"'Vacation Activity'",</strong></span>
<span class="strong"><strong>    "n2":"'Satisfaction'",</strong></span>
<span class="strong"><strong>    "label":"Deterministic"</strong></span>
<span class="strong"><strong>  },{</strong></span>
<span class="strong"><strong>    "n1":"'Weather'",</strong></span>
<span class="strong"><strong>    "n2":"'Satisfaction'",</strong></span>
<span class="strong"><strong>    "label":"Deterministic"</strong></span>
<span class="strong"><strong>  },{</strong></span>
<span class="strong"><strong>    "n1":"'Weather Forecast'",</strong></span>
<span class="strong"><strong>    "n2":"'Vacation Activity'",</strong></span>
<span class="strong"><strong>    "label":"Decision"</strong></span>
<span class="strong"><strong>  },{</strong></span>
<span class="strong"><strong>    "n1":"'Satisfaction'",</strong></span>
<span class="strong"><strong>    "n2":"'Recommend to a Friend'",</strong></span>
<span class="strong"><strong>    "label":"Probabilistic"</strong></span>
<span class="strong"><strong>  }]</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>[success] Total time: 1 s, completed May 1, 2016 1:55:30 PM</strong></span>
</pre></div><p>For more<a id="id517000000" class="indexterm"/> complex structures, one might need to write custom <a id="id518000000" class="indexterm"/>descriptors, serializers, and deserializers (refer<a id="id519000000" class="indexterm"/> to <a class="ulink" href="http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package">http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package</a>).</p></div></div>
<div class="section" title="GraphX"><div class="titlepage" id="aid-8GO0S2"><div><div><h1 class="title"><a id="ch07lvl1sec5900000"/>GraphX</h1></div></div></div><p>While graph<a id="id520000000" class="indexterm"/> for Scala may be considered a DSL for graph operations and querying, one should go to GraphX for scalability. GraphX is build on top of a powerful Spark framework. As an example of Spark/GraphX operations, I'll use the CMU Enron e-mail dataset (about 2 GB). The actual semantic analysis of the e-mail content is not going to be important to us until the next chapters. The dataset can be downloaded from the CMU site. It has e-mail from mailboxes of 150 users, primarily Enron managers, and about 517,401 e-mails between them. The e-mails may be considered as an indication of a relation (edge) between two people: Each email is an edge between a source (<code class="literal">From:</code>) and a destination (<code class="literal">To:</code>) vertices.</p><p>Since GraphX requires the data in RDD format, I'll have to do some preprocessing. Luckily, it is extremely easy with Scala—this is why Scala is the perfect language for semi-structured data. Here is the code:</p><div class="informalexample"><pre class="programlisting">package org.akozlov.chapter07

import scala.io.Source

import scala.util.hashing.{MurmurHash3 =&gt; Hash}
import scala.util.matching.Regex

import java.util.{Date =&gt; javaDateTime}

import java.io.File
import net.liftweb.json._
import Extraction._
import Serialization.{read, write}

object EnronEmail {

  val emailRe = """[a-zA-Z0-9_.+\-]+@enron.com""".r.unanchored

  def emails(s: String) = {
    for (email &lt;- emailRe findAllIn s) yield email
  }

  def hash(s: String) = {
    java.lang.Integer.MAX_VALUE.toLong + Hash.stringHash(s)
  }

  val messageRe =
    """(?:Message-ID:\s+)(&lt;[A-Za-z0-9_.+\-@]+&gt;)(?s)(?:.*?)(?m)
      |(?:Date:\s+)(.*?)$(?:.*?)
      |(?:From:\s+)([a-zA-Z0-9_.+\-]+@enron.com)(?:.*?)
      |(?:Subject: )(.*?)$""".stripMargin.r.unanchored

  case class Relation(from: String, fromId: Long, to: String, toId: Long, source: String, messageId: String, date: javaDateTime, subject: String)

  implicit val formats = Serialization.formats(NoTypeHints)

  def getFileTree(f: File): Stream[File] =
    f #:: (if (f.isDirectory) f.listFiles().toStream.flatMap(getFileTree) else Stream.empty)

  def main(args: Array[String]) {
    getFileTree(new File(args(0))).par.map {
      file =&gt; {
        "\\.$".r findFirstIn file.getName match {
          case Some(x) =&gt;
          try {
            val src = Source.fromFile(file, "us-ascii")
            val message = try src.mkString finally src.close()
            message match {
              case messageRe(messageId, date, from , subject) =&gt;
              val fromLower = from.toLowerCase
              for (to &lt;- emails(message).filter(_ != fromLower).toList.distinct)
              println(write(Relation(fromLower, hash(fromLower), to, hash(to), file.toString, messageId, new javaDateTime(date), subject)))
                case _ =&gt;
            }
          } catch {
            case e: Exception =&gt; System.err.println(e)
          }
          case _ =&gt;
        }
      }
    }
  }
}</pre></div><p>First, we use <a id="id521000000" class="indexterm"/>the <code class="literal">MurmurHash3</code> class to generate node IDs, which are of type <code class="literal">Long</code>, as they are required for each node in GraphX. The <code class="literal">emailRe</code> and <code class="literal">messageRe</code> are used to match the file content to find the required content. Scala allows you to parallelize the programs without much work.</p><p>Note the <code class="literal">par</code> call on line 50, <code class="literal">getFileTree(new File(args(0))).par.map</code>. This will make the loop parallel. If processing the whole Enron dataset can take up to an hour even on 3 GHz processor, adding parallelization reduces it by about 8 minutes on a 32-core Intel Xeon E5-2630 2.4 GHz CPU Linux machine (it took 15 minutes on an Apple MacBook Pro with 2.3 GHz Intel Core i7).</p><p>Running the code will produce a set of JSON records that can be loaded into Spark (to run it, you'll need to<a id="id522000000" class="indexterm"/> put <span class="strong"><strong>joda-time</strong></span> and <span class="strong"><strong>lift-json</strong></span> library jars on the classpath), as<a id="id523000000" class="indexterm"/> follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># (mkdir Enron; cd Enron; wget -O - http://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz | tar xzvf -)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong># sbt --error "run-main org.akozlov.chapter07.EnronEmail Enron/maildir" &gt; graph.json</strong></span>

<span class="strong"><strong># spark --driver-memory 2g --executor-memory 2g</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; val df = sqlContext.read.json("graph.json")</strong></span>
<span class="strong"><strong>df: org.apache.spark.sql.DataFrame = [[date: string, from: string, fromId: bigint, messageId: string, source: string, subject: string, to: string, toId: bigint]</strong></span>
</pre></div><p>Nice! Spark was able to figure out the fields and types on it's own. If Spark was not able to parse all the records, one would have a <code class="literal">_corrupt_record</code> field containing the unparsed records (one of them is the <code class="literal">[success]</code> line at the end of the dataset, which can be filtered out <a id="id524000000" class="indexterm"/>with a <code class="literal">grep -Fv [success]</code>). You can see them with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; df.select("_corrupt_record").collect.foreach(println)</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The nodes (people) and edges (relations) datasets can be extracted with the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.RDD</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; val people: RDD[(VertexId, String)] = df.select("fromId", "from").unionAll(df.select("toId", "to")).na.drop.distinct.map( x =&gt; (x.get(0).toString.toLong, x.get(1).toString))</strong></span>
<span class="strong"><strong>people: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[146] at map at &lt;console&gt;:28</strong></span>

<span class="strong"><strong>scala&gt; val relationships = df.select("fromId", "toId", "messageId", "subject").na.drop.distinct.map( x =&gt; Edge(x.get(0).toString.toLong, x.get(1).toString.toLong, (x.get(2).toString, x.get(3).toString)))</strong></span>
<span class="strong"><strong>relationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[(String, String)]] = MapPartitionsRDD[156] at map at &lt;console&gt;:28</strong></span>

<span class="strong"><strong>scala&gt; val graph = Graph(people, relationships).cache</strong></span>
<span class="strong"><strong>graph: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@7b59aa7b</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="note07000000"/>Note</h3><p>
<span class="strong"><strong>Node IDs in GraphX</strong></span>
</p><p>As we saw in Graph for Scala, specifying the edges is sufficient for defining the nodes and the graph. In Spark/GraphX, nodes need to be extracted explicitly, and each node needs to be associated with <span class="emphasis"><em>n</em></span> id of the <code class="literal">Long</code> type. While this potentially<a id="id525000000" class="indexterm"/> limits the flexibility and the number of unique nodes, it enhances the efficiency. In this particular example, generating node ID as a hash of the e-mail string was sufficient as no collisions were detected, but the generation of unique IDs is usually a hard problem to parallelize.</p></div><p>The first<a id="id526000000" class="indexterm"/> GraphX graph is ready!! It took a bit more work than Scala for Graph, but now it's totally ready for distributed processing. A few things to note: first, we needed to explicitly convert the fields to <code class="literal">Long</code> and <code class="literal">String</code> as the <code class="literal">Edge</code> constructor needed help in figuring out the types. Second, Spark might need to optimize the number of partitions (likely, it created too many):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; graph.vertices.getNumPartitions</strong></span>
<span class="strong"><strong>res1: Int = 200</strong></span>

<span class="strong"><strong>scala&gt; graph.edges.getNumPartitions</strong></span>
<span class="strong"><strong>res2: Int = 200</strong></span>
</pre></div><p>To repartition, there are two calls: repartition and coalesce. The latter tries to avoid shuffle, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val graph = Graph(people.coalesce(6), relationships.coalesce(6))</strong></span>
<span class="strong"><strong>graph: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@5dc7d016</strong></span>

<span class="strong"><strong>scala&gt; graph.vertices.getNumPartitions</strong></span>
<span class="strong"><strong>res10: Int = 6</strong></span>

<span class="strong"><strong>scala&gt; graph.edges.getNumPartitions</strong></span>
<span class="strong"><strong>res11: Int = 6</strong></span>
</pre></div><p>However, this might limit parallelism if one performs computations over a large cluster. Finally, it's a good idea to use <code class="literal">cache</code> method that pins the data structure in memory:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; graph.cache</strong></span>
<span class="strong"><strong>res12: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@5dc7d016</strong></span>
</pre></div><p>It took a few more commands to construct a graph in Spark, but four is not too bad. Let's compute some<a id="id527000000" class="indexterm"/> statistics (and show the power of Spark/GraphX, in the following table:</p><p>Computing basic statistics on Enron e-mail graph.</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Statistics</p>
</th><th valign="bottom">
<p>Spark command</p>
</th><th valign="bottom">
<p>Value for Enron</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>Total # of relations (pairwise communications)</p>
</td><td valign="top">
<p>
<code class="literal">graph.numEdges</code>
</p>
</td><td valign="top">
<p>3,035,021</p>
</td></tr><tr><td valign="top">
<p>Number of e-mails (message IDs)</p>
</td><td valign="top">
<p>
<code class="literal">graph.edges.map( e =&gt; e.attr._1 ).distinct.count</code>
</p>
</td><td valign="top">
<p>371,135</p>
</td></tr><tr><td valign="top">
<p>Number of connected pairs</p>
</td><td valign="top">
<p>
<code class="literal">graph.edges.flatMap( e =&gt; List((e.srcId, e.dstId), (e.dstId, e.srcId))).distinct.count / 2</code>
</p>
</td><td valign="top">
<p>217,867</p>
</td></tr><tr><td valign="top">
<p>Number of one-way communications</p>
</td><td valign="top">
<p>
<code class="literal">graph.edges.flatMap( e =&gt; List((e.srcId, e.dstId), (e.dstId, e.srcId))).distinct.count - graph.edges.map( e =&gt; (e.srcId, e.dstId)).distinct.count</code>
</p>
</td><td valign="top">
<p>193,183</p>
</td></tr><tr><td valign="top">
<p>Number of distinct subject lines</p>
</td><td valign="top">
<p>
<code class="literal">graph.edges.map( e =&gt; e.attr._2 ).distinct.count</code>
</p>
</td><td valign="top">
<p>110,273</p>
</td></tr><tr><td valign="top">
<p>Total # of nodes</p>
</td><td valign="top">
<p>
<code class="literal">graph.numVertices</code>
</p>
</td><td valign="top">
<p>23,607</p>
</td></tr><tr><td valign="top">
<p>Number of destination-only nodes</p>
</td><td valign="top">
<p>
<code class="literal">graph. numVertices - graph.edges.map( e =&gt; e.srcId).distinct.count</code>
</p>
</td><td valign="top">
<p>17,264</p>
</td></tr><tr><td valign="top">
<p>Number of source-only nodes</p>
</td><td valign="top">
<p>
<code class="literal">graph. numVertices - graph.edges.map( e =&gt; e.dstId).distinct.count</code>
</p>
</td><td valign="top">
<p>611</p>
</td></tr></tbody></table></div><div class="section" title="Who is getting e-mails?"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3500000"/>Who is getting e-mails?</h2></div></div></div><p>One of the <a id="id528000000" class="indexterm"/>most straightforward ways to estimate people's importance<a id="id529000000" class="indexterm"/> in an organization is to look at the number of connections or the number of incoming and outgoing communicates. The GraphX graph has built-in <code class="literal">inDegrees</code> and <code class="literal">outDegrees</code> methods. To rank the emails with respect to the number of incoming emails, run:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; people.join(graph.inDegrees).sortBy(_._2._2, ascending=false).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(268746271,(richard.shapiro@enron.com,18523))</strong></span>
<span class="strong"><strong>(1608171805,(steven.kean@enron.com,15867))</strong></span>
<span class="strong"><strong>(1578042212,(jeff.dasovich@enron.com,13878))</strong></span>
<span class="strong"><strong>(960683221,(tana.jones@enron.com,13717))</strong></span>
<span class="strong"><strong>(3784547591,(james.steffes@enron.com,12980))</strong></span>
<span class="strong"><strong>(1403062842,(sara.shackleton@enron.com,12082))</strong></span>
<span class="strong"><strong>(2319161027,(mark.taylor@enron.com,12018))</strong></span>
<span class="strong"><strong>(969899621,(mark.guzman@enron.com,10777))</strong></span>
<span class="strong"><strong>(1362498694,(geir.solberg@enron.com,10296))</strong></span>
<span class="strong"><strong>(4151996958,(ryan.slinger@enron.com,10160))</strong></span>
</pre></div><p>To rank the emails with respect to the number of egressing emails, run:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; people.join(graph.outDegrees).sortBy(_._2._2, ascending=false).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(1578042212,(jeff.dasovich@enron.com,139786))</strong></span>
<span class="strong"><strong>(2822677534,(veronica.espinoza@enron.com,106442))</strong></span>
<span class="strong"><strong>(3035779314,(pete.davis@enron.com,94666))</strong></span>
<span class="strong"><strong>(2346362132,(rhonda.denton@enron.com,90570))</strong></span>
<span class="strong"><strong>(861605621,(cheryl.johnson@enron.com,74319))</strong></span>
<span class="strong"><strong>(14078526,(susan.mara@enron.com,58797))</strong></span>
<span class="strong"><strong>(2058972224,(jae.black@enron.com,58718))</strong></span>
<span class="strong"><strong>(871077839,(ginger.dernehl@enron.com,57559))</strong></span>
<span class="strong"><strong>(3852770211,(lorna.brennan@enron.com,50106))</strong></span>
<span class="strong"><strong>(241175230,(mary.hain@enron.com,40425))</strong></span>
<span class="strong"><strong>…</strong></span>
</pre></div><p>Let's <a id="id530000000" class="indexterm"/>apply <a id="id531000000" class="indexterm"/>some more complex algorithms to the Enron dataset.</p></div><div class="section" title="Connected components"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3600000"/>Connected components</h2></div></div></div><p>Connected components<a id="id532000000" class="indexterm"/> determine whether the graph is naturally<a id="id533000000" class="indexterm"/> partitioned into several parts. In the Enron relationship graph, this would mean that two or several groups communicate mostly between each other:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val groups = org.apache.spark.graphx.lib.ConnectedComponents.run(graph).vertices.map(_._2).distinct.cache</strong></span>
<span class="strong"><strong>groups: org.apache.spark.rdd.RDD[org.apache.spark.graphx.VertexId] = MapPartitionsRDD[2404] at distinct at &lt;console&gt;:34</strong></span>

<span class="strong"><strong>scala&gt; groups.count</strong></span>
<span class="strong"><strong>res106: Long = 18</strong></span>

<span class="strong"><strong>scala&gt; people.join(groups.map( x =&gt; (x, x))).map(x =&gt; (x._1, x._2._1)).sortBy(_._1).collect.foreach(println)</strong></span>
<span class="strong"><strong>(332133,laura.beneville@enron.com)</strong></span>
<span class="strong"><strong>(81833994,gpg.me-q@enron.com)</strong></span>
<span class="strong"><strong>(115247730,dl-ga-enron_debtor@enron.com)</strong></span>
<span class="strong"><strong>(299810291,gina.peters@enron.com)</strong></span>
<span class="strong"><strong>(718200627,techsupport.notices@enron.com)</strong></span>
<span class="strong"><strong>(847455579,paul.de@enron.com)</strong></span>
<span class="strong"><strong>(919241773,etc.survey@enron.com)</strong></span>
<span class="strong"><strong>(1139366119,enron.global.services.-.us@enron.com)</strong></span>
<span class="strong"><strong>(1156539970,shelley.ariel@enron.com)</strong></span>
<span class="strong"><strong>(1265773423,dl-ga-all_ews_employees@enron.com)</strong></span>
<span class="strong"><strong>(1493879606,chairman.ees@enron.com)</strong></span>
<span class="strong"><strong>(1511379835,gary.allen.-.safety.specialist@enron.com)</strong></span>
<span class="strong"><strong>(2114016426,executive.robert@enron.com)</strong></span>
<span class="strong"><strong>(2200225669,ken.board@enron.com)</strong></span>
<span class="strong"><strong>(2914568776,ge.americas@enron.com)</strong></span>
<span class="strong"><strong>(2934799198,yowman@enron.com)</strong></span>
<span class="strong"><strong>(2975592118,tech.notices@enron.com)</strong></span>
<span class="strong"><strong>(3678996795,mail.user@enron.com)</strong></span>
</pre></div><p>We see<a id="id534000000" class="indexterm"/> 18 groups. Each one of the groups can be counted and <a id="id535000000" class="indexterm"/>extracted by filtering the ID. For instance, the group associated with <code class="email">&lt;<a class="email" href="mailto:etc.survey@enron.com">etc.survey@enron.com</a>&gt;</code> can be found by running a SQL query on DataFrame:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; df.filter("fromId = 919241773 or toId = 919241773").select("date","from","to","subject","source").collect.foreach(println)</strong></span>
<span class="strong"><strong>[2000-09-19T18:40:00.000Z,survey.test@enron.com,etc.survey@enron.com,NO ACTION REQUIRED - TEST,Enron/maildir/dasovich-j/all_documents/1567.]</strong></span>
<span class="strong"><strong>[2000-09-19T18:40:00.000Z,survey.test@enron.com,etc.survey@enron.com,NO ACTION REQUIRED - TEST,Enron/maildir/dasovich-j/notes_inbox/504.]</strong></span>
</pre></div><p>This group is based on a single e-mail sent on September 19, 2000, from <code class="email">&lt;<a class="email" href="mailto:survey.test@enron.com">survey.test@enron.com</a>&gt;</code> to <code class="email">&lt;<a class="email" href="mailto:etc.survey@enron">etc.survey@enron</a>&gt;</code>. The e-mail is listed twice, only because it ended up in two different folders (and has two distinct message IDs). Only the first group, the largest subgraph, contains more than two e-mail addresses in the organization.</p></div><div class="section" title="Triangle counting"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3700000"/>Triangle counting</h2></div></div></div><p>The <a id="id536000000" class="indexterm"/>triangle counting algorithm is relatively straightforward<a id="id537000000" class="indexterm"/> and can be computed in the following three steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Compute the set of neighbors for each vertex.</li><li class="listitem">For each edge, compute the intersection of the sets and send the count to both vertices.</li><li class="listitem">Compute the sum at each vertex and divide by two, as each triangle is counted twice.</li></ol><div style="height:10px; width: 1px"/></div><p>We need to convert the multigraph to an undirected graph with <code class="literal">srcId &lt; dstId</code>, which is a precondition for the algorithm:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val unedges = graph.edges.map(e =&gt; if (e.srcId &lt; e.dstId) (e.srcId, e.dstId) else (e.dstId, e.srcId)).map( x =&gt; Edge(x._1, x._2, 1)).cache</strong></span>
<span class="strong"><strong>unedges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[87] at map at &lt;console&gt;:48</strong></span>

<span class="strong"><strong>scala&gt; val ungraph = Graph(people, unedges).partitionBy(org.apache.spark.graphx.PartitionStrategy.EdgePartition1D, 10).cache</strong></span>
<span class="strong"><strong>ungraph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@77274fff</strong></span>

<span class="strong"><strong>scala&gt; val triangles = org.apache.spark.graphx.lib.TriangleCount.run(ungraph).cache</strong></span>
<span class="strong"><strong>triangles: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@6aec6da1</strong></span>

<span class="strong"><strong>scala&gt; people.join(triangles.vertices).map(t =&gt; (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(31761,sally.beck@enron.com)</strong></span>
<span class="strong"><strong>(24101,louise.kitchen@enron.com)</strong></span>
<span class="strong"><strong>(23522,david.forster@enron.com)</strong></span>
<span class="strong"><strong>(21694,kenneth.lay@enron.com)</strong></span>
<span class="strong"><strong>(20847,john.lavorato@enron.com)</strong></span>
<span class="strong"><strong>(18460,david.oxley@enron.com)</strong></span>
<span class="strong"><strong>(17951,tammie.schoppe@enron.com)</strong></span>
<span class="strong"><strong>(16929,steven.kean@enron.com)</strong></span>
<span class="strong"><strong>(16390,tana.jones@enron.com)</strong></span>
<span class="strong"><strong>(16197,julie.clyatt@enron.com)</strong></span>
</pre></div><p>While there<a id="id538000000" class="indexterm"/> is no direct relationship between the<a id="id539000000" class="indexterm"/> triangle count and the importance of people in the organization, the people with higher triangle count arguably are more social—even though a clique or a strongly connected component count might be a better measure.</p></div><div class="section" title="Strongly connected components"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3800000"/>Strongly connected components</h2></div></div></div><p>In the <a id="id540000000" class="indexterm"/>mathematical theory of directed graphs, a subgraph is<a id="id541000000" class="indexterm"/> said to be strongly connected if every vertex is reachable from every other vertex. It could happen that the whole graph is just one strongly connected component, but on the other end of the spectrum, each vertex could be its own connected component.</p><p>If you contract each connected component to a single vertex, you get a new directed graph that has a property to be without cycles—acyclic.</p><p>The algorithm for SCC detection is already built into GraphX:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val components = org.apache.spark.graphx.lib.StronglyConnectedComponents.run(graph, 100).cache</strong></span>
<span class="strong"><strong>components: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@55913bc7</strong></span>

<span class="strong"><strong>scala&gt; components.vertices.map(_._2).distinct.count</strong></span>
<span class="strong"><strong>res2: Long = 17980</strong></span>

<span class="strong"><strong>scala&gt; people.join(components.vertices.map(_._2).distinct.map( x =&gt; (x, x))).map(x =&gt; (x._1, x._2._1)).sortBy(_._1).collect.foreach(println)</strong></span>
<span class="strong"><strong>(332133,laura.beneville@enron.com)                                              </strong></span>
<span class="strong"><strong>(466265,medmonds@enron.com)</strong></span>
<span class="strong"><strong>(471258,.jane@enron.com)</strong></span>
<span class="strong"><strong>(497810,.kimberly@enron.com)</strong></span>
<span class="strong"><strong>(507806,aleck.dadson@enron.com)</strong></span>
<span class="strong"><strong>(639614,j..bonin@enron.com)</strong></span>
<span class="strong"><strong>(896860,imceanotes-hbcamp+40aep+2ecom+40enron@enron.com)</strong></span>
<span class="strong"><strong>(1196652,enron.legal@enron.com)</strong></span>
<span class="strong"><strong>(1240743,thi.ly@enron.com)</strong></span>
<span class="strong"><strong>(1480469,ofdb12a77a.a6162183-on86256988.005b6308@enron.com)</strong></span>
<span class="strong"><strong>(1818533,fran.i.mayes@enron.com)</strong></span>
<span class="strong"><strong>(2337461,michael.marryott@enron.com)</strong></span>
<span class="strong"><strong>(2918577,houston.resolution.center@enron.com)</strong></span>
</pre></div><p>There <a id="id542000000" class="indexterm"/>are 18,200 strongly connected components <a id="id543000000" class="indexterm"/>with only an average 23,787/18,200 = 1.3 users per group.</p></div><div class="section" title="PageRank"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec3900000"/>PageRank</h2></div></div></div><p>The <a id="id544000000" class="indexterm"/>PageRank algorithm gives us an estimate<a id="id545000000" class="indexterm"/> of how important a person by analysing the links, which are the emails in this case.  For example, let's run PageRank on Enron email graph:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val ranks = graph.pageRank(0.001).vertices</strong></span>
<span class="strong"><strong>ranks: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[955] at RDD at VertexRDD.scala:57</strong></span>

<span class="strong"><strong>scala&gt; people.join(ranks).map(t =&gt; (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)</strong></span>

<span class="strong"><strong>scala&gt; val ranks = graph.pageRank(0.001).vertices</strong></span>
<span class="strong"><strong>ranks: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[955] at RDD at VertexRDD.scala:57</strong></span>

<span class="strong"><strong>scala&gt; people.join(ranks).map(t =&gt; (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(32.073722548483325,tana.jones@enron.com)</strong></span>
<span class="strong"><strong>(29.086568868043248,sara.shackleton@enron.com)</strong></span>
<span class="strong"><strong>(28.14656912897315,louise.kitchen@enron.com)</strong></span>
<span class="strong"><strong>(26.57894933459292,vince.kaminski@enron.com)</strong></span>
<span class="strong"><strong>(25.865486865014493,sally.beck@enron.com)</strong></span>
<span class="strong"><strong>(23.86746232662471,john.lavorato@enron.com)</strong></span>
<span class="strong"><strong>(22.489814482022275,jeff.skilling@enron.com)</strong></span>
<span class="strong"><strong>(21.968039409295585,mark.taylor@enron.com)</strong></span>
<span class="strong"><strong>(20.903053536275547,kenneth.lay@enron.com)</strong></span>
<span class="strong"><strong>(20.39124651779771,gerald.nemec@enron.com)</strong></span>
</pre></div><p>Ostensibly, these are the go-to people. PageRank tends to emphasize the incoming edges, and Tana Jones returns to the top of the list compared to the 9th place in the triangle counting.</p></div><div class="section" title="SVD++"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec4000000"/>SVD++</h2></div></div></div><p>SVD++ is a<a id="id546000000" class="indexterm"/> recommendation engine algorithm, developed specifically for<a id="id547000000" class="indexterm"/> Netflix competition by Yahuda Koren and team in 2008—the original paper is still out there in the public domain and can be Googled as <code class="literal">kdd08koren.pdf</code>. The specific implementation comes from the .NET <span class="emphasis"><em>MyMediaLite</em></span> library<a id="id548000000" class="indexterm"/> by ZenoGarther (<a class="ulink" href="https://github.com/zenogantner/MyMediaLite">https://github.com/zenogantner/MyMediaLite</a>), who granted Apache 2 license to the Apache Foundation. Let's assume I have a set of users (on the left) and items (on the right):</p><div class="mediaobject"><img src="../Images/image01734.jpeg" alt="SVD++"/><div class="caption"><p>Figure 07-1. A graphical representation of a recommendation problem as a bipartite graph.</p></div></div><p style="clear:both; height: 1em;"> </p><p>The preceding diagram is a graphical representation of the recommendation problem. The nodes on the left represent users. The nodes on the right represent items. User <span class="strong"><strong>1</strong></span> recommends items <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>C</strong></span>, while users <span class="strong"><strong>2</strong></span> and <span class="strong"><strong>3</strong></span> recommend only a single item <span class="strong"><strong>A</strong></span>. The rest of the edges are missing. The common question is to find recommendation ranking of the rest of the<a id="id549000000" class="indexterm"/> items, the edges may also have a weight or recommendation <a id="id550000000" class="indexterm"/>strength attached to them. The graph is usually sparse. Such graph is also often called bipartite, as the edges only go from one set of nodes to another set of nodes (the user does not recommend another user).</p><p>For the recommendation engine, we typically need two types of nodes—users and items. The recommendations are based on the rating matrix of (user, item, and rating) tuples. One of the implementation of the recommendation algorithm is based on <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) of the preceding matrix. The final scoring has four components: the baseline, which<a id="id551000000" class="indexterm"/> is the sum of average for the whole matrix, average for the users, and average for the items, as follows:</p><div class="mediaobject"><img src="../Images/image01735.jpeg" alt="SVD++"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the <span class="inlinemediaobject"><img src="../Images/image01736.jpeg" alt="SVD++"/></span>, <span class="inlinemediaobject"><img src="../Images/image01737.jpeg" alt="SVD++"/></span>, and <span class="inlinemediaobject"><img src="../Images/image01738.jpeg" alt="SVD++"/></span> can be understood as the averages for the whole population, user (among all user recommendations), and item (among all the users). The final part is the Cartesian product of two rows:</p><div class="mediaobject"><img src="../Images/image01739.jpeg" alt="SVD++"/></div><p style="clear:both; height: 1em;"> </p><p>The <a id="id552000000" class="indexterm"/>problem is <a id="id553000000" class="indexterm"/>posed as a minimization problem (refer to <a class="link" title="Chapter 4. Supervised and Unsupervised Learning" href="part0256.xhtml#aid-7K4G02">Chapter 4</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning</em></span>):</p><div class="mediaobject"><img src="../Images/image01740.jpeg" alt="SVD++"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image01741.jpeg" alt="SVD++"/></span> is a regularization coefficient also discussed in <a class="link" title="Chapter 4. Supervised and Unsupervised Learning" href="part0256.xhtml#aid-7K4G02">Chapter 4</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning</em></span>. So, each user is associated with a set of numbers (<span class="inlinemediaobject"><img src="../Images/image01742.jpeg" alt="SVD++"/></span>, and each item with <span class="inlinemediaobject"><img src="../Images/image01743.jpeg" alt="SVD++"/></span>, <span class="inlinemediaobject"><img src="../Images/image01744.jpeg" alt="SVD++"/></span>. In this particlar implementation, the optimal coefficients are found by gradient descent. This is the basic of SVD optimization. In linear algebra, SVD takes an arbitrary <span class="inlinemediaobject"><img src="../Images/image01745.jpeg" alt="SVD++"/></span> matrix <span class="emphasis"><em>A</em></span> and represents it as a product of an orthogonal <span class="inlinemediaobject"><img src="../Images/image01745.jpeg" alt="SVD++"/></span> matrix <span class="emphasis"><em>U</em></span>, a diagonal <span class="inlinemediaobject"><img src="../Images/image01745.jpeg" alt="SVD++"/></span> matrix <span class="inlinemediaobject"><img src="../Images/image01746.jpeg" alt="SVD++"/></span>, and a <span class="inlinemediaobject"><img src="../Images/image01745.jpeg" alt="SVD++"/></span> unitary matrix <span class="emphasis"><em>V</em></span>, for example, the columns are mutually orthogonal. Arguably, if one takes the largest <span class="inlinemediaobject"><img src="../Images/image01747.jpeg" alt="SVD++"/></span> entries of the <span class="inlinemediaobject"><img src="../Images/image01746.jpeg" alt="SVD++"/></span> matrix, the product is reduced to the product of a very tall <span class="inlinemediaobject"><img src="../Images/image01748.jpeg" alt="SVD++"/></span> matrix and a wide <span class="inlinemediaobject"><img src="../Images/image01749.jpeg" alt="SVD++"/></span> matric, where <span class="inlinemediaobject"><img src="../Images/image01747.jpeg" alt="SVD++"/></span> is called the rank of decomposition. If the remaining values are small, the new <span class="inlinemediaobject"><img src="../Images/image01750.jpeg" alt="SVD++"/></span> numbers approximate the original <span class="inlinemediaobject"><img src="../Images/image01745.jpeg" alt="SVD++"/></span> numbers <a id="id554000000" class="indexterm"/>for the relation, <span class="emphasis"><em>A</em></span>. If <span class="emphasis"><em>m</em></span> and <span class="emphasis"><em>n</em></span> are large to start with, and in practical<a id="id555000000" class="indexterm"/> online shopping situations, <span class="emphasis"><em>m</em></span> is the items and can be in hundreds of thousands, and <span class="emphasis"><em>n</em></span> is the users and can be hundreds of millions, the saving can be substantial. For example, for <span class="emphasis"><em>r=10</em></span>, <span class="emphasis"><em>m=100,000</em></span>, and <span class="emphasis"><em>n=100,000,000</em></span>, the savings are as follows:</p><div class="mediaobject"><img src="../Images/image01751.jpeg" alt="SVD++"/></div><p style="clear:both; height: 1em;"> </p><p>SVD can also be viewed as PCA for matrices with <span class="inlinemediaobject"><img src="../Images/image01752.jpeg" alt="SVD++"/></span>. In the Enron case, we can treat senders as <a id="id556000000" class="indexterm"/>users <a id="id557000000" class="indexterm"/>and recipients as items (we'll need to reassign the node IDs), as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val rgraph = graph.partitionBy(org.apache.spark.graphx.PartitionStrategy.EdgePartition1D, 10).mapEdges(e =&gt; 1).groupEdges(_+_).cache</strong></span>
<span class="strong"><strong>rgraph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@2c1a48d6</strong></span>

<span class="strong"><strong>scala&gt; val redges = rgraph.edges.map( e =&gt; Edge(-e.srcId, e.dstId, Math.log(e.attr.toDouble)) ).cache</strong></span>
<span class="strong"><strong>redges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[57] at map at &lt;console&gt;:36</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.graphx.lib.SVDPlusPlus</strong></span>
<span class="strong"><strong>import org.apache.spark.graphx.lib.SVDPlusPlus</strong></span>

<span class="strong"><strong>scala&gt; implicit val conf = new SVDPlusPlus.Conf(10, 50, 0.0, 10.0, 0.007, 0.007, 0.005, 0.015)</strong></span>
<span class="strong"><strong>conf: org.apache.spark.graphx.lib.SVDPlusPlus.Conf = org.apache.spark.graphx.lib.SVDPlusPlus$Conf@15cdc117</strong></span>

<span class="strong"><strong>scala&gt; val (svd, mu) = SVDPlusPlus.run(redges, conf)</strong></span>
<span class="strong"><strong>svd: org.apache.spark.graphx.Graph[(Array[Double], Array[Double], Double, Double),Double] = org.apache.spark.graphx.impl.GraphImpl@3050363d</strong></span>
<span class="strong"><strong>mu: Double = 1.3773578970633769</strong></span>

<span class="strong"><strong>scala&gt; val svdRanks = svd.vertices.filter(_._1 &gt; 0).map(x =&gt; (x._2._3, x._1))</strong></span>
<span class="strong"><strong>svdRanks: org.apache.spark.rdd.RDD[(Double, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1517] at map at &lt;console&gt;:31</strong></span>

<span class="strong"><strong>scala&gt; val svdRanks = svd.vertices.filter(_._1 &gt; 0).map(x =&gt; (x._1, x._2._3))</strong></span>
<span class="strong"><strong>svdRanks: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = MapPartitionsRDD[1520] at map at &lt;console&gt;:31</strong></span>

<span class="strong"><strong>scala&gt; people.join(svdRanks).sortBy(_._2._2, ascending=false).map(x =&gt; (x._2._2, x._2._1)).take(10).foreach(println)</strong></span>
<span class="strong"><strong>(8.864218804309887,jbryson@enron.com)</strong></span>
<span class="strong"><strong>(5.935146713012661,dl-ga-all_enron_worldwide2@enron.com)</strong></span>
<span class="strong"><strong>(5.740242927715701,houston.report@enron.com)</strong></span>
<span class="strong"><strong>(5.441934324464593,a478079f-55e1f3b0-862566fa-612229@enron.com)</strong></span>
<span class="strong"><strong>(4.910272928389445,pchoi2@enron.com)</strong></span>
<span class="strong"><strong>(4.701529779800544,dl-ga-all_enron_worldwide1@enron.com)</strong></span>
<span class="strong"><strong>(4.4046392452058045,eligible.employees@enron.com)</strong></span>
<span class="strong"><strong>(4.374738019256556,all_ena_egm_eim@enron.com)</strong></span>
<span class="strong"><strong>(4.303078586979311,dl-ga-all_enron_north_america@enron.com)</strong></span>
<span class="strong"><strong>(3.8295412053860867,the.mailout@enron.com)</strong></span>
</pre></div><p>The <code class="literal">svdRanks</code> is the user-part of the <span class="inlinemediaobject"><img src="../Images/image01753.jpeg" alt="SVD++"/></span> prediction. The distribution lists take a priority as<a id="id558000000" class="indexterm"/> this is usually used for mass e-mailing. To get the user-specific part, we<a id="id559000000" class="indexterm"/> need to provide the user ID:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import com.github.fommil.netlib.BLAS.{getInstance =&gt; blas}</strong></span>

<span class="strong"><strong>scala&gt; def topN(uid: Long, num: Int) = {</strong></span>
<span class="strong"><strong>     |    val usr = svd.vertices.filter(uid == -_._1).collect()(0)._2</strong></span>
<span class="strong"><strong>     |    val recs = svd.vertices.filter(_._1 &gt; 0).map( v =&gt; (v._1, mu + usr._3 + v._2._3 + blas.ddot(usr._2.length, v._2._1, 1, usr._2, 1)))</strong></span>
<span class="strong"><strong>     |    people.join(recs).sortBy(_._2._2, ascending=false).map(x =&gt; (x._2._2, x._2._1)).take(num)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>topN: (uid: Long, num: Int)Array[(Double, String)]</strong></span>

<span class="strong"><strong>scala&gt; def top5(x: Long) : Array[(Double, String)] = topN(x, 5)</strong></span>
<span class="strong"><strong>top5: (x: Long)Array[(Double, String)]</strong></span>

<span class="strong"><strong>scala&gt; people.join(graph.inDegrees).sortBy(_._2._2, ascending=false).map(x =&gt; (x._1, x._2._1)).take(10).toList.map(t =&gt; (t._2, top5(t._1).toList)).foreach(println)</strong></span>
<span class="strong"><strong>(richard.shapiro@enron.com,List((4.866184418005094E66,anne.bertino@enron.com), (3.9246829664352734E66,kgustafs@enron.com), (3.9246829664352734E66,gweiss@enron.com), (3.871029763863491E66,hill@enron.com), (3.743135924382312E66,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(steven.kean@enron.com,List((2.445163626935533E66,anne.bertino@enron.com), (1.9584692804232504E66,hill@enron.com), (1.9105427465629028E66,kgustafs@enron.com), (1.9105427465629028E66,gweiss@enron.com), (1.8931872324048717E66,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(jeff.dasovich@enron.com,List((2.8924566115596135E66,anne.bertino@enron.com), (2.3157345904446663E66,hill@enron.com), (2.2646318970030287E66,gweiss@enron.com), (2.2646318970030287E66,kgustafs@enron.com), (2.2385865127706285E66,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(tana.jones@enron.com,List((6.1758464471309754E66,elizabeth.sager@enron.com), (5.279291610047078E66,tana.jones@enron.com), (4.967589820856654E66,tim.belden@enron.com), (4.909283344915057E66,jeff.dasovich@enron.com), (4.869177440115682E66,mark.taylor@enron.com)))</strong></span>
<span class="strong"><strong>(james.steffes@enron.com,List((5.7702834706832735E66,anne.bertino@enron.com), (4.703038082326939E66,gweiss@enron.com), (4.703038082326939E66,kgustafs@enron.com), (4.579565962089777E66,hill@enron.com), (4.4298763869135494E66,george@enron.com)))</strong></span>
<span class="strong"><strong>(sara.shackleton@enron.com,List((9.198688613290757E67,louise.kitchen@enron.com), (8.078107057848099E67,john.lavorato@enron.com), (6.922806078209984E67,greg.whalley@enron.com), (6.787266892881456E67,elizabeth.sager@enron.com), (6.420473603137515E67,sally.beck@enron.com)))</strong></span>
<span class="strong"><strong>(mark.taylor@enron.com,List((1.302856119148208E66,anne.bertino@enron.com), (1.0678968544568682E66,hill@enron.com), (1.031255083546722E66,fraser@enron.com), (1.009319696608474E66,george@enron.com), (9.901391892701356E65,brad@enron.com)))</strong></span>
<span class="strong"><strong>(mark.guzman@enron.com,List((9.770393472845669E65,anne.bertino@enron.com), (7.97370292724488E65,kgustafs@enron.com), (7.97370292724488E65,gweiss@enron.com), (7.751983820970696E65,hill@enron.com), (7.500175024539423E65,george@enron.com)))</strong></span>
<span class="strong"><strong>(geir.solberg@enron.com,List((6.856103529420811E65,anne.bertino@enron.com), (5.611272903720188E65,gweiss@enron.com), (5.611272903720188E65,kgustafs@enron.com), (5.436280144720843E65,hill@enron.com), (5.2621103015001885E65,george@enron.com)))</strong></span>
<span class="strong"><strong>(ryan.slinger@enron.com,List((5.0579114162531735E65,anne.bertino@enron.com), (4.136838933824579E65,kgustafs@enron.com), (4.136838933824579E65,gweiss@enron.com), (4.0110663808847004E65,hill@enron.com), (3.8821438267917902E65,george@enron.com)))</strong></span>

<span class="strong"><strong>scala&gt; people.join(graph.outDegrees).sortBy(_._2._2, ascending=false).map(x =&gt; (x._1, x._2._1)).take(10).toList.map(t =&gt; (t._2, top5(t._1).toList)).foreach(println)</strong></span>
<span class="strong"><strong>(jeff.dasovich@enron.com,List((2.8924566115596135E66,anne.bertino@enron.com), (2.3157345904446663E66,hill@enron.com), (2.2646318970030287E66,gweiss@enron.com), (2.2646318970030287E66,kgustafs@enron.com), (2.2385865127706285E66,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(veronica.espinoza@enron.com,List((3.135142195254243E65,gweiss@enron.com), (3.135142195254243E65,kgustafs@enron.com), (2.773512892785554E65,anne.bertino@enron.com), (2.350799070225962E65,marcia.a.linton@enron.com), (2.2055288158758267E65,robert@enron.com)))</strong></span>
<span class="strong"><strong>(pete.davis@enron.com,List((5.773492048248794E66,louise.kitchen@enron.com), (5.067434612038159E66,john.lavorato@enron.com), (4.389028076992449E66,greg.whalley@enron.com), (4.1791711984241975E66,sally.beck@enron.com), (4.009544764149938E66,elizabeth.sager@enron.com)))</strong></span>
<span class="strong"><strong>(rhonda.denton@enron.com,List((2.834710591578977E68,louise.kitchen@enron.com), (2.488253676819922E68,john.lavorato@enron.com), (2.1516048969715738E68,greg.whalley@enron.com), (2.0405329247770104E68,sally.beck@enron.com), (1.9877213034021861E68,elizabeth.sager@enron.com)))</strong></span>
<span class="strong"><strong>(cheryl.johnson@enron.com,List((3.453167402163105E64,mary.dix@enron.com), (3.208849221485621E64,theresa.byrne@enron.com), (3.208849221485621E64,sandy.olofson@enron.com), (3.0374270093157086E64,hill@enron.com), (2.886581252384442E64,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(susan.mara@enron.com,List((5.1729089729525785E66,anne.bertino@enron.com), (4.220843848723133E66,kgustafs@enron.com), (4.220843848723133E66,gweiss@enron.com), (4.1044435240204605E66,hill@enron.com), (3.9709951893268635E66,george@enron.com)))</strong></span>
<span class="strong"><strong>(jae.black@enron.com,List((2.513139130001457E65,anne.bertino@enron.com), (2.1037756300035247E65,hill@enron.com), (2.0297519350719265E65,fraser@enron.com), (1.9587139280519927E65,george@enron.com), (1.947164483486155E65,brad@enron.com)))</strong></span>
<span class="strong"><strong>(ginger.dernehl@enron.com,List((4.516267307013845E66,anne.bertino@enron.com), (3.653408921875843E66,gweiss@enron.com), (3.653408921875843E66,kgustafs@enron.com), (3.590298037045689E66,hill@enron.com), (3.471781765250177E66,fraser@enron.com)))</strong></span>
<span class="strong"><strong>(lorna.brennan@enron.com,List((2.0719309635087482E66,anne.bertino@enron.com), (1.732651408857978E66,kgustafs@enron.com), (1.732651408857978E66,gweiss@enron.com), (1.6348480059915056E66,hill@enron.com), (1.5880693846486309E66,george@enron.com)))</strong></span>
<span class="strong"><strong>(mary.hain@enron.com,List((5.596589595417286E66,anne.bertino@enron.com), (4.559474243930487E66,kgustafs@enron.com), (4.559474243930487E66,gweiss@enron.com), (4.4421474044331</strong></span>
</pre></div><p>Here, we <a id="id560000000" class="indexterm"/>computed <a id="id561000000" class="indexterm"/>the top five recommended e-mail-to list for top in-degree and out-degree users.</p><p>SVD has only 159 lines of code in Scala and can be the basis for some further improvements. SVD++ includes<a id="id562000000" class="indexterm"/> a part based on implicit user feedback and item similarity <a id="id563000000" class="indexterm"/>information. Finally, the Netflix winning solution had also taken into consideration the fact that user preferences are time-dependent, but this part has not been implemented in GraphX yet.</p></div></div>
<div class="section" title="Summary" id="aid-8HMHE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec6000000"/>Summary</h1></div></div></div><p>While one can easily create their own data structures for graph problems, Scala's support for graphs comes from both semantic layer—Graph for Scala is effectively a convenient, interactive, and expressive language for working with graphs—and scalability via Spark and distributed computing. I hope that some of the material exposed in this chapter will be useful for implementing algorithms on top of Scala, Spark, and GraphX. It is worth mentioning that bot libraries are still under active development.</p><p>In the next chapter, we'll step down from from our flight in the the skies and look at Scala integration with traditional data analysis frameworks such as statistical language R and Python, which are often used for data munching. Later, in <a class="link" title="Chapter 9. NLP in Scala" href="part0291.xhtml#aid-8LGJM2">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>. I'll look at NLP Scala tools, which leverage complex data structures extensively.</p></div>
<div class="chapter" title="Chapter&#xA0;8.&#xA0;Integrating Scala with R and Python"><div class="titlepage" id="aid-8IL202"><div><div><h1 class="title"><a id="ch34"/>Chapter 8. Integrating Scala with R and Python</h1></div></div></div><p>While Spark provides MLlib as a library for machine learning, in many practical situations, R or Python present a more familiar and time-tested interface for statistical computations. In particular, R's extensive statistical library includes very popular ANOVA/MANOVA methods of analyzing variance and variable dependencies/independencies, sets of statistical tests, and random number generators that are not currently present in MLlib. The interface from R to Spark is available under SparkR project. Finally, data analysts know Python's NumPy and SciPy linear algebra implementations for their efficiency as well as other time-series, optimization, and signal processing packages. With R/Python integration, all these familiar functionalities can be exposed to Scala/Spark users until the Spark/MLlib interfaces stabilize and the libraries make their way into the new framework while benefiting the users with Spark's ability to execute workflows in a distributed way across multiple machines.</p><p>When people program in R or Python, or with any statistical or linear algebra packages for this matter, they are usually not specifically focusing on the functional programming aspects. As I mentioned in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span>, Scala should be treated as a high-level language and this is where it shines. Integration with highly efficient C and Fortran implementations, for <a id="id564000000" class="indexterm"/>example, of the freely available <span class="strong"><strong>Basic Linear Algebra Subprograms</strong></span> (<span class="strong"><strong>BLAS</strong></span>), <span class="strong"><strong>Linear Algebra </strong></span><a id="id565000000" class="indexterm"/>
<span class="strong"><strong>Package</strong></span> (<span class="strong"><strong>LAPACK</strong></span>), and <span class="strong"><strong>Arnoldi Package</strong></span> (<span class="strong"><strong>ARPACK</strong></span>), is <a id="id566000000" class="indexterm"/>known to find its way into Java and thus <a id="id567000000" class="indexterm"/>Scala (<a class="ulink" href="http://www.netlib.org">http://www.netlib.org</a>, <a class="ulink" href="https://github.com/fommil/netlib-java">https://github.com/fommil/netlib-java</a>). I would like to leave Scala at what it's doing best. In this chapter, however, I will focus on how to use these languages with Scala/Spark.</p><p>I will use the publicly<a id="id568000000" class="indexterm"/> available United States Department of Transportation flights dataset for this chapter (<a class="ulink" href="http://www.transtats.bts.gov">http://www.transtats.bts.gov</a>).</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Installing R and configuring SparkR if you haven't done so yet</li><li class="listitem">Learning about R (and Spark) DataFrames</li><li class="listitem">Performing linear regression and ANOVA analysis with R</li><li class="listitem">Performing <a id="id569000000" class="indexterm"/><span class="strong"><strong>Generalized Linear Model</strong></span> (<span class="strong"><strong>GLM</strong></span>) modeling with SparkR</li><li class="listitem">Installing Python if you haven't done so yet</li><li class="listitem">Learning how to use PySpark and call Python from Scala</li></ul></div><div class="section" title="Integrating with R"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec6100000"/>Integrating with R</h1></div></div></div><p>As with many<a id="id570000000" class="indexterm"/> advanced and carefully designed technologies, people usually either love or hate R as a language. One of the reason being that R was one of the first language implementations that tries to manipulate complex objects, even though most of them turn out to be just a list as opposed to struct or map as in more mature modern implementations. R was originally created at the University of Auckland by Ross Ihaka and Robert Gentleman around 1993, and had its roots in the S language developed at Bell Labs around 1976, when most of the commercial programming was still <a id="id571000000" class="indexterm"/>done in Fortran. While R incorporates some functional features such as passing functions as a parameter and map/apply, it conspicuously misses some others such as lazy evaluation and list comprehensions. With all this said, R has a very good help system, and if someone says that they never had to go back to the <code class="literal">help(…)</code> command to figure out how to run a certain data transformation or model better, they are either lying or just starting in R.</p><div class="section" title="Setting up R and SparkR"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4100000"/>Setting up R and SparkR</h2></div></div></div><p>To run SparkR, you'll need <a id="id572000000" class="indexterm"/>R version 3.0 or later. Follow the given instructions for the installation, depending <a id="id573000000" class="indexterm"/>on you operating system.</p><div class="section" title="Linux"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0100000"/>Linux</h3></div></div></div><p>On a Linux system, detailed<a id="id574000000" class="indexterm"/> installation documentation<a id="id575000000" class="indexterm"/> is available at <a class="ulink" href="https://cran.r-project.org/bin/linux">https://cran.r-project.org/bin/linux</a>. However, for example, on a <a id="id576000000" class="indexterm"/>Debian system, one installs it by running the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># apt-get update</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong># apt-get install r-base r-base-dev</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>To list installed/available packages on the Linux repository site, perform the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># apt-cache search "^r-.*" | sort</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>R packages, which<a id="id577000000" class="indexterm"/> are a part of <code class="literal">r-base</code> and <code class="literal">r-recommended</code>, are installed into the <code class="literal">/usr/lib/R/library</code> directory. These can be updated using the usual package maintenance tools such as <code class="literal">apt-get</code> or aptitude. The other R packages available as precompiled Debian packages, <code class="literal">r-cran-*</code> and <code class="literal">r-bioc-*</code>, are installed into <code class="literal">/usr/lib/R/site-library</code>. The following command shows all packages that depend on <code class="literal">r-base-core</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># apt-cache rdepends r-base-core</strong></span>
</pre></div><p>This comprises of a large number of contributed packages from CRAN and other repositories. If you want to install R packages that are not provided as package, or if you want to use newer<a id="id578000000" class="indexterm"/> versions, you need to build them from source that requires the <code class="literal">r-base-dev</code> development package that can be installed by the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># apt-get install r-base-dev</strong></span>
</pre></div><p>This pulls in the basic requirements to compile R packages, such as the development tools group install. R packages may then be installed by the local user/admin from the CRAN source packages, typically from inside R using the <code class="literal">R&gt; install.packages()</code> function or <code class="literal">R CMD INSTALL</code>. For example, to install the R <code class="literal">ggplot2</code> package, run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; install.packages("ggplot2")</strong></span>
<span class="strong"><strong>--- Please select a CRAN mirror for use in this session ---</strong></span>
<span class="strong"><strong>also installing the dependencies 'stringi', 'magrittr', 'colorspace', 'Rcpp', 'stringr', 'RColorBrewer', 'dichromat', 'munsell', 'labeling', 'digest', 'gtable', 'plyr', 'reshape2', 'scales'</strong></span>
</pre></div><p>This will download and optionally compile the package and its dependencies from one of the available sites. Sometime R is confused about the repositories; in this case, I recommend creating a <code class="literal">~/.Rprofile</code> file in the home directory pointing to the closest CRAN repository:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ cat &gt;&gt; ~/.Rprofile &lt;&lt; EOF</strong></span>
<span class="strong"><strong>r = getOption("repos") # hard code the Berkeley repo for CRAN</strong></span>
<span class="strong"><strong>r["CRAN"] = "http://cran.cnr.berkeley.edu"</strong></span>
<span class="strong"><strong>options(repos = r)</strong></span>
<span class="strong"><strong>rm(r)</strong></span>

<span class="strong"><strong>EOF</strong></span>
</pre></div><p>
<code class="literal">~/.Rprofile</code> contains commands to customize your sessions. One of the commands I recommend to put<a id="id579000000" class="indexterm"/> in there is <code class="literal">options (prompt="R&gt; ")</code> to be<a id="id580000000" class="indexterm"/> able to distinguish the shell you are working in by the prompt, following the tradition of most tools in this book. The list of known <a id="id581000000" class="indexterm"/>mirrors is available at <a class="ulink" href="https://cran.r-project.org/mirrors.html">https://cran.r-project.org/mirrors.html</a>.
</p><p>Also, it is good practice to specify the directory to install <code class="literal">system/site/user</code> packages via the following command, unless your OS setup does it already by putting these commands into <code class="literal">~/.bashrc</code> or system <code class="literal">/etc/profile</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ export R_LIBS_SITE=${R_LIBS_SITE:-/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library}</strong></span>
<span class="strong"><strong>$ export R_LIBS_USER=${R_LIBS_USER:-$HOME/R/$(uname -i)-library/$( R --version | grep -o -E [0-9]+\.[</strong></span>
<span class="strong"><strong>0-9]+ | head -1)}</strong></span>
</pre></div></div><div class="section" title="Mac OS"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0200000"/>Mac OS</h3></div></div></div><p>R for Mac OS can be<a id="id582000000" class="indexterm"/> downloaded, for example, from <a class="ulink" href="http://cran.r-project.org/bin/macosx">http://cran.r-project.org/bin/macosx</a>. The latest version at the time of the writing is 3.2.3. Always <a id="id583000000" class="indexterm"/>check the consistency of the downloaded <a id="id584000000" class="indexterm"/>package. To do so, run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ pkgutil --check-signature R-3.2.3.pkg</strong></span>
<span class="strong"><strong>Package "R-3.2.3.pkg":</strong></span>
<span class="strong"><strong>   Status: signed by a certificate trusted by Mac OS X</strong></span>
<span class="strong"><strong>   Certificate Chain:</strong></span>
<span class="strong"><strong>    1. Developer ID Installer: Simon Urbanek</strong></span>
<span class="strong"><strong>       SHA1 fingerprint: B7 EB 39 5E 03 CF 1E 20 D1 A6 2E 9F D3 17 90 26 D8 D6 3B EF</strong></span>
<span class="strong"><strong>       -----------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    2. Developer ID Certification Authority</strong></span>
<span class="strong"><strong>       SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86</strong></span>
<span class="strong"><strong>       -----------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>    3. Apple Root CA</strong></span>
<span class="strong"><strong>       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60</strong></span>
</pre></div><p>The environment <a id="id585000000" class="indexterm"/>settings in the preceding subsection also apply to the Mac OS setup.</p></div><div class="section" title="Windows"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0300000"/>Windows</h3></div></div></div><p>R for Windows <a id="id586000000" class="indexterm"/>can be downloaded from <a class="ulink" href="https://cran.r-project.org/bin/windows/">https://cran.r-project.org/bin/windows/</a> as an exe installer. Run this executable as an administrator to install R.</p><p>One can usually <a id="id587000000" class="indexterm"/>edit the environment setting for <span class="strong"><strong>System/User</strong></span> by<a id="id588000000" class="indexterm"/> following the <span class="strong"><strong>Control Panel</strong></span> | <span class="strong"><strong>System and Security</strong></span> | <span class="strong"><strong>System</strong></span> | <span class="strong"><strong>Advanced system settings</strong></span> | <span class="strong"><strong>Environment Variables</strong></span> path from the Windows menu.</p></div><div class="section" title="Running SparkR via scripts"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0400000"/>Running SparkR via scripts</h3></div></div></div><p>To run SparkR, one <a id="id589000000" class="indexterm"/>needs to run install the <code class="literal">R/install-dev.sh</code> script that comes with the Spark git tree. In fact, one only needs the shell script and the content of the <code class="literal">R/pkg</code> directory, which is not always included with the compiled Spark distributions:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ git clone https://github.com/apache/spark.git</strong></span>
<span class="strong"><strong>Cloning into 'spark'...</strong></span>
<span class="strong"><strong>remote: Counting objects: 301864, done.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ cp –r R/{install-dev.sh,pkg) $SPARK_HOME/R</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ cd $SPARK_HOME</strong></span>
<span class="strong"><strong>$ ./R/install-dev.sh</strong></span>
<span class="strong"><strong>* installing *source* package 'SparkR' ...</strong></span>
<span class="strong"><strong>** R</strong></span>
<span class="strong"><strong>** inst</strong></span>
<span class="strong"><strong>** preparing package for lazy loading</strong></span>
<span class="strong"><strong>Creating a new generic function for 'colnames' in package 'SparkR'</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ bin/sparkR</strong></span>

<span class="strong"><strong>R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"</strong></span>
<span class="strong"><strong>Copyright (C) 2015 The R Foundation for Statistical Computing</strong></span>
<span class="strong"><strong>Platform: x86_64-redhat-linux-gnu (64-bit)</strong></span>

<span class="strong"><strong>R is free software and comes with ABSOLUTELY NO WARRANTY.</strong></span>
<span class="strong"><strong>You are welcome to redistribute it under certain conditions.</strong></span>
<span class="strong"><strong>Type 'license()' or 'licence()' for distribution details.</strong></span>

<span class="strong"><strong>  Natural language support but running in an English locale</strong></span>

<span class="strong"><strong>R is a collaborative project with many contributors.</strong></span>
<span class="strong"><strong>Type 'contributors()' for more information and</strong></span>
<span class="strong"><strong>'citation()' on how to cite R or R packages in publications.</strong></span>

<span class="strong"><strong>Type 'demo()' for some demos, 'help()' for on-line help, or</strong></span>
<span class="strong"><strong>'help.start()' for an HTML browser interface to help.</strong></span>
<span class="strong"><strong>Type 'q()' to quit R.</strong></span>

<span class="strong"><strong>Launching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   "sparkr-shell" /tmp/RtmpgdTfmU/backend_port22446d0391e8 </strong></span>

<span class="strong"><strong> Welcome to</strong></span>
<span class="strong"><strong>    ____              __ </strong></span>
<span class="strong"><strong>   / __/__  ___ _____/ /__ </strong></span>
<span class="strong"><strong>  _\ \/ _ \/ _ `/ __/  '_/ </strong></span>
<span class="strong"><strong> /___/ .__/\_,_/_/ /_/\_\   version  1.6.1 </strong></span>
<span class="strong"><strong>    /_/ </strong></span>


<span class="strong"><strong> Spark context is available as sc, SQL context is available as sqlContext</strong></span><span class="strong"><strong>&gt;</strong></span>
</pre></div></div><div class="section" title="Running Spark via R's command line"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0500000"/>Running Spark via R's command line</h3></div></div></div><p>Alternatively, we can also<a id="id590000000" class="indexterm"/> initialize Spark from the R command line <a id="id591000000" class="indexterm"/>directly (or from RStudio at <a class="ulink" href="http://rstudio.org/">http://rstudio.org/</a>) using the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>R&gt; sc &lt;- sparkR.init(master = Sys.getenv("SPARK_MASTER"), sparkEnvir = list(spark.driver.memory="1g"))</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>R&gt; sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
</pre></div><p>As described previously in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>, the <code class="literal">SPARK_HOME</code> environment variable needs to point to your local Spark installation directory and <code class="literal">SPARK_MASTER</code> and <code class="literal">YARN_CONF_DIR</code> to the desired cluster manager (local, standalone, mesos, and YARN) and YARN configuration directory if one is using Spark with the YARN cluster manager.</p><p>Although most all<a id="id592000000" class="indexterm"/> of the distributions come with a UI, in the tradition of this book and for the purpose of this chapter I'll use the command line.</p></div></div><div class="section" title="DataFrames"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4200000"/>DataFrames</h2></div></div></div><p>The<a id="id593000000" class="indexterm"/> DataFrames originally came from R and Python, so it is natural to see them in SparkR.</p><div class="note" title="Note"><h3 class="title"><a id="note08000000"/>Note</h3><p>Please note that the implementation of DataFrames in SparkR is on top of RDDs, so they work differently than the R DataFrames.</p></div><p>The question of when and where to store and apply the schema and other metadata like types has been a topic of active debate recently. On one hand, providing the schema early with the data enables thorough data validation and potentially optimization. On the other hand, it may be too restrictive for the original data ingest, whose goal is just to capture as much data as possible and perform data formatting/cleansing later on, the approach often referred as schema on read. The latter approach recently won more ground with the tools to work with evolving schemas such as Avro and automatic schema discovery tools, but for the purpose of this chapter, I'll assume that we have done the schema discovery part and can start working with a DataFrames.</p><p>Let's first download and extract a flight delay dataset from the United States Department of Transportation, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip</strong></span>
<span class="strong"><strong>--2016-01-23 15:40:02--  http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip</strong></span>
<span class="strong"><strong>Resolving www.transtats.bts.gov... 204.68.194.70</strong></span>
<span class="strong"><strong>Connecting to www.transtats.bts.gov|204.68.194.70|:80... connected.</strong></span>
<span class="strong"><strong>HTTP request sent, awaiting response... 200 OK</strong></span>
<span class="strong"><strong>Length: 26204213 (25M) [application/x-zip-compressed]</strong></span>
<span class="strong"><strong>Saving to: "On_Time_On_Time_Performance_2015_7.zip"</strong></span>

<span class="strong"><strong>100%[====================================================================================================================================================================================&gt;] 26,204,213   966K/s   in 27s     </strong></span>

<span class="strong"><strong>2016-01-23 15:40:29 (956 KB/s) - "On_Time_On_Time_Performance_2015_7.zip" saved [26204213/26204213]</strong></span>

<span class="strong"><strong>$ unzip -d flights On_Time_On_Time_Performance_2015_7.zip</strong></span>
<span class="strong"><strong>Archive:  On_Time_On_Time_Performance_2015_7.zip</strong></span>
<span class="strong"><strong>  inflating: flights/On_Time_On_Time_Performance_2015_7.csv  </strong></span>
<span class="strong"><strong>  inflating: flights/readme.html</strong></span>
</pre></div><p>If you have Spark running on the <a id="id594000000" class="indexterm"/>cluster, you want to copy the file in HDFS:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ hadoop fs –put flights .</strong></span>
</pre></div><p>The <code class="literal">flights/readme.html</code> files gives you detailed metadata information, as shown in the following image:</p><div class="mediaobject"><img src="../Images/image01754.jpeg" alt="DataFrames"/><div class="caption"><p>Figure 08-1: Metadata provided with the On-Time Performance dataset released by the US Department of Transportation (for demo purposes only)</p></div></div><p style="clear:both; height: 1em;"> </p><p>Now, I want you<a id="id595000000" class="indexterm"/> to analyze the delays of <code class="literal">SFO</code> returning flights and possibly find the factors contributing to the delay. Let's start with the R <code class="literal">data.frame</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/sparkR --master local[8]</strong></span>

<span class="strong"><strong>R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"</strong></span>
<span class="strong"><strong>Copyright (C) 2015 The R Foundation for Statistical Computing</strong></span>
<span class="strong"><strong>Platform: x86_64-apple-darwin13.4.0 (64-bit)</strong></span>

<span class="strong"><strong>R is free software and comes with ABSOLUTELY NO WARRANTY.</strong></span>
<span class="strong"><strong>You are welcome to redistribute it under certain conditions.</strong></span>
<span class="strong"><strong>Type 'license()' or 'licence()' for distribution details.</strong></span>

<span class="strong"><strong>  Natural language support but running in an English locale</strong></span>

<span class="strong"><strong>R is a collaborative project with many contributors.</strong></span>
<span class="strong"><strong>Type 'contributors()' for more information and</strong></span>
<span class="strong"><strong>'citation()' on how to cite R or R packages in publications.</strong></span>

<span class="strong"><strong>Type 'demo()' for some demos, 'help()' for on-line help, or</strong></span>
<span class="strong"><strong>'help.start()' for an HTML browser interface to help.</strong></span>
<span class="strong"><strong>Type 'q()' to quit R.</strong></span>

<span class="strong"><strong>[Previously saved workspace restored]</strong></span>

<span class="strong"><strong>Launching java with spark-submit command /Users/akozlov/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   "--master" "local[8]" "sparkr-shell" /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T//RtmpD42eTz/backend_port682e58e2c5db </strong></span>

<span class="strong"><strong> Welcome to</strong></span>
<span class="strong"><strong>    ____              __ </strong></span>
<span class="strong"><strong>   / __/__  ___ _____/ /__ </strong></span>
<span class="strong"><strong>  _\ \/ _ \/ _ `/ __/  '_/ </strong></span>
<span class="strong"><strong> /___/ .__/\_,_/_/ /_/\_\   version  1.6.1 </strong></span>
<span class="strong"><strong>    /_/ </strong></span>


<span class="strong"><strong> Spark context is available as sc, SQL context is available as sqlContext</strong></span>
<span class="strong"><strong>&gt; flights &lt;- read.table(unz("On_Time_On_Time_Performance_2015_7.zip", "On_Time_On_Time_Performance_2015_7.csv"), nrows=1000000, header=T, quote="\"", sep=",")</strong></span>
<span class="strong"><strong>&gt; sfoFlights &lt;- flights[flights$Dest == "SFO", ]</strong></span>
<span class="strong"><strong>&gt; attach(sfoFlights)</strong></span>
<span class="strong"><strong>&gt; delays &lt;- aggregate(ArrDelayMinutes ~ DayOfWeek + Origin + UniqueCarrier, FUN=mean, na.rm=TRUE)</strong></span>
<span class="strong"><strong>&gt; tail(delays[order(delays$ArrDelayMinutes), ])</strong></span>
<span class="strong"><strong>    DayOfWeek Origin UniqueCarrier ArrDelayMinutes</strong></span>
<span class="strong"><strong>220         4    ABQ            OO           67.60</strong></span>
<span class="strong"><strong>489         4    TUS            OO           71.80</strong></span>
<span class="strong"><strong>186         5    IAH            F9           77.60</strong></span>
<span class="strong"><strong>696         3    RNO            UA           79.50</strong></span>
<span class="strong"><strong>491         6    TUS            OO          168.25</strong></span>
<span class="strong"><strong>84          7    SLC            AS          203.25</strong></span>
</pre></div><p>If you were flying from Salt Lake City on Sunday with Alaska Airlines in July 2015, consider yourself unlucky (we have only done simple analysis so far, so one shouldn't attach too much significance to this result). There may be multiple other random factors contributing to the delay.</p><p>Even though we<a id="id596000000" class="indexterm"/> ran the example in SparkR, we still used the R <code class="literal">data.frame</code>. If we want to analyze data across multiple months, we will need to distribute the load across multiple nodes. This is where the SparkR distributed DataFrame comes into play, as it can be distributed across multiple threads even on a single node. There is a direct way to convert the R DataFrame to SparkR DataFrame (and thus to RDD):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; sparkDf &lt;- createDataFrame(sqlContext, flights)</strong></span>
</pre></div><p>If I run it on a laptop, I will run out of memory. The overhead is large due to the fact that I need to transfer the data between multiple threads/nodes, we want to filter it as soon as possible:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sparkDf &lt;- createDataFrame(sqlContext, subset(flights, select = c("ArrDelayMinutes", "DayOfWeek", "Origin", "Dest", "UniqueCarrier")))</strong></span>
</pre></div><p>This will run even on my laptop. There is, of course, a reverse conversion from Spark's DataFrame to R's <code class="literal">data.frame</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; rDf &lt;- as.data.frame(sparkDf)</strong></span>
</pre></div><p>Alternatively, I can use the <code class="literal">spark-csv</code> package to read it from the <code class="literal">.csv</code> file, which, if the original <code class="literal">.csv</code> file is in a distributed filesystem such as HDFS, will avoid shuffling the data over network in a cluster setting. The only drawback, at least currently, is that Spark cannot read from the <code class="literal">.zip</code> files directly:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; $ ./bin/sparkR --packages com.databricks:spark-csv_2.10:1.3.0 --master local[8]</strong></span>

<span class="strong"><strong>R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"</strong></span>
<span class="strong"><strong>Copyright (C) 2015 The R Foundation for Statistical Computing</strong></span>
<span class="strong"><strong>Platform: x86_64-redhat-linux-gnu (64-bit)</strong></span>

<span class="strong"><strong>R is free software and comes with ABSOLUTELY NO WARRANTY.</strong></span>
<span class="strong"><strong>You are welcome to redistribute it under certain conditions.</strong></span>
<span class="strong"><strong>Type 'license()' or 'licence()' for distribution details.</strong></span>

<span class="strong"><strong>  Natural language support but running in an English locale</strong></span>

<span class="strong"><strong>R is a collaborative project with many contributors.</strong></span>
<span class="strong"><strong>Type 'contributors()' for more information and</strong></span>
<span class="strong"><strong>'citation()' on how to cite R or R packages in publications.</strong></span>

<span class="strong"><strong>Type 'demo()' for some demos, 'help()' for on-line help, or</strong></span>
<span class="strong"><strong>'help.start()' for an HTML browser interface to help.</strong></span>
<span class="strong"><strong>Type 'q()' to quit R.</strong></span>

<span class="strong"><strong>Warning: namespace 'SparkR' is not available and has been replaced</strong></span>
<span class="strong"><strong>by .GlobalEnv when processing object 'sparkDf'</strong></span>
<span class="strong"><strong>[Previously saved workspace restored]</strong></span>

<span class="strong"><strong>Launching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   "--master" "local[8]" "--packages" "com.databricks:spark-csv_2.10:1.3.0" "sparkr-shell" /tmp/RtmpfhcUXX/backend_port1b066bea5a03 </strong></span>
<span class="strong"><strong>Ivy Default Cache set to: /home/alex/.ivy2/cache</strong></span>
<span class="strong"><strong>The jars for the packages stored in: /home/alex/.ivy2/jars</strong></span>
<span class="strong"><strong>:: loading settings :: url = jar:file:/home/alex/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml</strong></span>
<span class="strong"><strong>com.databricks#spark-csv_2.10 added as a dependency</strong></span>
<span class="strong"><strong>:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0</strong></span>
<span class="strong"><strong>  confs: [default]</strong></span>
<span class="strong"><strong>  found com.databricks#spark-csv_2.10;1.3.0 in central</strong></span>
<span class="strong"><strong>  found org.apache.commons#commons-csv;1.1 in central</strong></span>
<span class="strong"><strong>  found com.univocity#univocity-parsers;1.5.1 in central</strong></span>
<span class="strong"><strong>:: resolution report :: resolve 189ms :: artifacts dl 4ms</strong></span>
<span class="strong"><strong>  :: modules in use:</strong></span>
<span class="strong"><strong>  com.databricks#spark-csv_2.10;1.3.0 from central in [default]</strong></span>
<span class="strong"><strong>  com.univocity#univocity-parsers;1.5.1 from central in [default]</strong></span>
<span class="strong"><strong>  org.apache.commons#commons-csv;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |                  |            modules            ||   artifacts   |</strong></span>
<span class="strong"><strong>  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>:: retrieving :: org.apache.spark#spark-submit-parent</strong></span>
<span class="strong"><strong>  confs: [default]</strong></span>
<span class="strong"><strong>  0 artifacts copied, 3 already retrieved (0kB/7ms)</strong></span>

<span class="strong"><strong> Welcome to</strong></span>
<span class="strong"><strong>    ____              __ </strong></span>
<span class="strong"><strong>   / __/__  ___ _____/ /__ </strong></span>
<span class="strong"><strong>  _\ \/ _ \/ _ `/ __/  '_/ </strong></span>
<span class="strong"><strong> /___/ .__/\_,_/_/ /_/\_\   version  1.6.1 </strong></span>
<span class="strong"><strong>    /_/ </strong></span>


<span class="strong"><strong> Spark context is available as sc, SQL context is available as sqlContext</strong></span>
<span class="strong"><strong>&gt; sparkDf &lt;- read.df(sqlContext, "./flights", "com.databricks.spark.csv", header="true", inferSchema = "false")</strong></span>
<span class="strong"><strong>&gt; sfoFlights &lt;- select(filter(sparkDf, sparkDf$Dest == "SFO"), "DayOfWeek", "Origin", "UniqueCarrier", "ArrDelayMinutes")</strong></span>
<span class="strong"><strong>&gt; aggs &lt;- agg(group_by(sfoFlights, "DayOfWeek", "Origin", "UniqueCarrier"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes))</strong></span>
<span class="strong"><strong>&gt; head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)</strong></span>
<span class="strong"><strong>   DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes)   </strong></span>
<span class="strong"><strong>1          7    SLC            AS                      4               203.25</strong></span>
<span class="strong"><strong>2          6    TUS            OO                      4               168.25</strong></span>
<span class="strong"><strong>3          3    RNO            UA                      8                79.50</strong></span>
<span class="strong"><strong>4          5    IAH            F9                      5                77.60</strong></span>
<span class="strong"><strong>5          4    TUS            OO                      5                71.80</strong></span>
<span class="strong"><strong>6          4    ABQ            OO                      5                67.60</strong></span>
<span class="strong"><strong>7          2    ABQ            OO                      4                66.25</strong></span>
<span class="strong"><strong>8          1    IAH            F9                      4                61.25</strong></span>
<span class="strong"><strong>9          4    DAL            WN                      5                59.20</strong></span>
<span class="strong"><strong>10         3    SUN            OO                      5                59.00</strong></span>
</pre></div><p>Note that we<a id="id597000000" class="indexterm"/> loaded the additional <code class="literal">com.databricks:spark-csv_2.10:1.3.0</code> package by supplying the <code class="literal">--package</code> flag on the command line; we can easily go distributed by using a Spark instance over a cluster of nodes or even analyze a larger dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ for i in $(seq 1 6); do wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_$i.zip; unzip -d flights On_Time_On_Time_Performance_2015_$i.zip; hadoop fs -put -f flights/On_Time_On_Time_Performance_2015_$i.csv flights; done</strong></span>

<span class="strong"><strong>$ hadoop fs -ls flights</strong></span>
<span class="strong"><strong>Found 7 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  211633432 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_1.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  192791767 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_2.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  227016932 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_3.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  218600030 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_4.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  224003544 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_5.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  227418780 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_6.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 alex eng  235037955 2016-02-15 21:56 flights/On_Time_On_Time_Performance_2015_7.csv</strong></span>
</pre></div><p>This will download and put the on-time performance data in the flight's directory (remember, as we discussed in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span>, we would like to treat directories as big data datasets). We can now run the same analysis over the whole period of 2015 (for the available data):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; sparkDf &lt;- read.df(sqlContext, "./flights", "com.databricks.spark.csv", header="true")</strong></span>
<span class="strong"><strong>&gt; sfoFlights &lt;- select(filter(sparkDf, sparkDf$Dest == "SFO"), "DayOfWeek", "Origin", "UniqueCarrier", "ArrDelayMinutes")</strong></span>
<span class="strong"><strong>&gt; aggs &lt;- cache(agg(group_by(sfoFlights, "DayOfWeek", "Origin", "UniqueCarrier"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes)))</strong></span>
<span class="strong"><strong>&gt; head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)</strong></span>
<span class="strong"><strong>   DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes)   </strong></span>
<span class="strong"><strong>1          6    MSP            UA                      1            122.00000</strong></span>
<span class="strong"><strong>2          3    RNO            UA                      8             79.50000</strong></span>
<span class="strong"><strong>3          1    MSP            UA                     13             68.53846</strong></span>
<span class="strong"><strong>4          7    SAT            UA                      1             65.00000</strong></span>
<span class="strong"><strong>5          7    STL            UA                      9             64.55556</strong></span>
<span class="strong"><strong>6          1    ORD            F9                     13             55.92308</strong></span>
<span class="strong"><strong>7          1    MSO            OO                      4             50.00000</strong></span>
<span class="strong"><strong>8          2    MSO            OO                      4             48.50000</strong></span>
<span class="strong"><strong>9          5    CEC            OO                     28             45.86957</strong></span>
<span class="strong"><strong>10         3    STL            UA                     13             43.46154</strong></span>
</pre></div><p>Note that we used a <code class="literal">cache()</code> call to pin the dataset to the memory as we will use it again later. This time<a id="id598000000" class="indexterm"/> it's Minneapolis/United on Saturday! However, you probably already know why: there is only one record for this combination of <code class="literal">DayOfWeek</code>, <code class="literal">Origin</code>, and <code class="literal">UniqueCarrier</code>; it's most likely an outlier. The average over about <code class="literal">30</code> flights for the previous outlier was reduced to <code class="literal">30</code> minutes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; head(arrange(filter(filter(aggs, aggs$Origin == "SLC"), aggs$UniqueCarrier == "AS"), c('avg(ArrDelayMinutes)'), decreasing = TRUE), 100)</strong></span>
<span class="strong"><strong>  DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes)</strong></span>
<span class="strong"><strong>1         7    SLC            AS                     30            32.600000</strong></span>
<span class="strong"><strong>2         2    SLC            AS                     30            10.200000</strong></span>
<span class="strong"><strong>3         4    SLC            AS                     31             9.774194</strong></span>
<span class="strong"><strong>4         1    SLC            AS                     30             9.433333</strong></span>
<span class="strong"><strong>5         3    SLC            AS                     30             5.866667</strong></span>
<span class="strong"><strong>6         5    SLC            AS                     31             5.516129</strong></span>
<span class="strong"><strong>7         6    SLC            AS                     30             2.133333</strong></span>
</pre></div><p>Sunday still remains a problem in terms of delays. The limit to the amount of data we can analyze now is <a id="id599000000" class="indexterm"/>only the number of cores on the laptop and nodes in the cluster. Let's look at more complex machine learning models now.</p></div><div class="section" title="Linear models"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4300000"/>Linear models</h2></div></div></div><p>Linear methods play <a id="id600000000" class="indexterm"/>an important role in statistical modeling. As the name suggests, linear model assumes that the dependent variable is a weighted combination of independent variables. In R, the <code class="literal">lm</code> function performs a linear regression and reports the coefficients, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; attach(iris)</strong></span>
<span class="strong"><strong>R&gt; lm(Sepal.Length ~ Sepal.Width)</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lm(formula = Sepal.Length ~ Sepal.Width)</strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>(Intercept)  Sepal.Width</strong></span>
<span class="strong"><strong>     6.5262      -0.2234</strong></span>
</pre></div><p>The <code class="literal">summary</code> function provides even more information:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; model &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)</strong></span>
<span class="strong"><strong>R&gt; summary(model)</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)</strong></span>

<span class="strong"><strong>Residuals:</strong></span>
<span class="strong"><strong>     Min       1Q   Median       3Q      Max </strong></span>
<span class="strong"><strong>-0.82816 -0.21989  0.01875  0.19709  0.84570 </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>             Estimate Std. Error t value Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept)   1.85600    0.25078   7.401 9.85e-12 ***</strong></span>
<span class="strong"><strong>Sepal.Width   0.65084    0.06665   9.765  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>Petal.Length  0.70913    0.05672  12.502  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>Petal.Width  -0.55648    0.12755  -4.363 2.41e-05 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>Residual standard error: 0.3145 on 146 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:  0.8586,  Adjusted R-squared:  0.8557 </strong></span>
<span class="strong"><strong>F-statistic: 295.5 on 3 and 146 DF,  p-value: &lt; 2.2e-16</strong></span>
</pre></div><p>While we considered generalized linear models in <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>, and we will also consider the <code class="literal">glm</code> implementation in R and SparkR shortly, linear models provide <a id="id601000000" class="indexterm"/>more information in general and are an excellent tool for working with noisy data and selecting the relevant attribute for further analysis.</p><div class="note" title="Note"><h3 class="title"><a id="note09000000"/>Note</h3><p>
<span class="strong"><strong>Data analysis life cycle</strong></span>
</p><p>While most of the<a id="id602000000" class="indexterm"/> statistical books focus on the analysis and best use of available data, the results of statistical analysis in general should also affect the search for the new sources of information. In the complete data life cycle, discussed at the end of <a class="link" title="Chapter 3. Working with Spark and MLlib" href="part0249.xhtml#aid-7DES21">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>, a data scientist should always transform the latest variable importance results into the theories of how to collect data. For example, if the ink usage analysis for home printers points to an increase in ink usage for photos, one could potentially collect more information about the format of the pictures, sources of digital images, and paper the user prefers to use. This approach turned out to be very productive in a real business situation even though not fully automated.</p></div><p>Specifically, here is a<a id="id603000000" class="indexterm"/> short description of the output that linear models provide:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Residuals</strong></span>: These are <a id="id604000000" class="indexterm"/>statistics for the difference between the actual and predicted values. A lot of techniques exist to detect the problems with the models on patterns of the residual distribution, but this is out of scope of this book. A detailed residual table can be obtained with the <code class="literal">resid(model)</code> function.</li><li class="listitem"><span class="strong"><strong>Coefficients</strong></span>: These are the actual linear combination coefficients; the t-value represents the ratio of the value of the coefficient to the estimate of the standard error: higher <a id="id605000000" class="indexterm"/>values mean a higher likelihood that this coefficient has a non-trivial effect on the dependent variable. The coefficients can also be obtained with <code class="literal">coef(model)</code> functions.</li><li class="listitem"><span class="strong"><strong>Residual standard error</strong></span>: This reports the standard mean square error, the metric that is the<a id="id606000000" class="indexterm"/> target of optimization in a straightforward linear regression.</li><li class="listitem"><span class="strong"><strong>Multiple R-squared</strong></span>: This is the fraction of the dependent variable variance that is explained by the model. The adjusted value accounts for the number of parameters in your model and is considered to be a better metric to avoid overfitting if the <a id="id607000000" class="indexterm"/>number of observations does not justify the complexity of the models, which happens even for big data problems.</li><li class="listitem"><span class="strong"><strong>F-statistic</strong></span>: The measure <a id="id608000000" class="indexterm"/>of model quality. In plain terms, it measures how all the parameters in the model explain the dependent variable. The p-value provides the probability that the model explains the dependent variable just due to random chance. The values under 0.05 (or 5%) are, in general, considered satisfactory. While in general, a high value probably means that the model is probably not statistically valid and "nothing else matters, the low F-statistic does not always mean that the model will work well in practice, so it cannot be directly applied as a model acceptance criterion.</li></ul></div><p>Once the linear models are applied, usually more complex <code class="literal">glm</code> or recursive models, such as decision trees and the <code class="literal">rpart</code> function, are applied to find interesting variable interactions. Linear models are good for establishing baseline on the other models that can improve.</p><p>Finally, ANOVA is a standard technique to study the variance if the independent variables are discrete:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; aov &lt;- aov(Sepal.Length ~ Species)</strong></span>
<span class="strong"><strong>R&gt; summary(aov)</strong></span>
<span class="strong"><strong>             Df Sum Sq Mean Sq F value Pr(&gt;F)    </strong></span>
<span class="strong"><strong>Species       2  63.21  31.606   119.3 &lt;2e-16 ***</strong></span>
<span class="strong"><strong>Residuals   147  38.96   0.265                   </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><p>The measure of the model quality is F-statistics. While one can always run R algorithms with RDD using the pipe <a id="id609000000" class="indexterm"/>mechanism with <code class="literal">Rscript</code>, I will partially cover this functionality with respect to <span class="strong"><strong>Java Specification Request</strong></span> (<span class="strong"><strong>JSR</strong></span>) 223 Python<a id="id610000000" class="indexterm"/> integration later. In this section, I would like to explore specifically a generalized linear regression <code class="literal">glm</code> function that is implemented both in R and SparkR natively.</p></div><div class="section" title="Generalized linear model"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4400000"/>Generalized linear model</h2></div></div></div><p>Once again, you can<a id="id611000000" class="indexterm"/> run either R <code class="literal">glm</code> or SparkR <code class="literal">glm</code>. The list of possible link and optimization functions for R implementation is provided in the following table:</p><p>The following list shows possible options for R <code class="literal">glm</code> implementation:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Family</p>
</th><th valign="bottom">
<p>Variance</p>
</th><th valign="bottom">
<p>Link</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>gaussian</p>
</td><td valign="top">
<p>gaussian</p>
</td><td valign="top">
<p>identity</p>
</td></tr><tr><td valign="top">
<p>binomial</p>
</td><td valign="top">
<p>binomial</p>
</td><td valign="top">
<p>logit, probit or cloglog</p>
</td></tr><tr><td valign="top">
<p>poisson</p>
</td><td valign="top">
<p>poisson</p>
</td><td valign="top">
<p>log, identity or sqrt</p>
</td></tr><tr><td valign="top">
<p>Gamma</p>
</td><td valign="top">
<p>Gamma</p>
</td><td valign="top">
<p>inverse, identity or log</p>
</td></tr><tr><td valign="top">
<p>inverse.gaussian</p>
</td><td valign="top">
<p>inverse.gaussian</p>
</td><td valign="top">
<p>1/mu^2</p>
</td></tr><tr><td valign="top">
<p>quasi</p>
</td><td valign="top">
<p>user-defined</p>
</td><td valign="top">
<p>user-defined</p>
</td></tr></tbody></table></div><p>I will use a binary target, <code class="literal">ArrDel15</code>, which indicates whether the plane was more than 15 minutes late for the arrival. The independent variables will be <code class="literal">DepDel15</code>, <code class="literal">DayOfWeek</code>, <code class="literal">Month</code>, <code class="literal">UniqueCarrier</code>, <code class="literal">Origin</code>, and <code class="literal">Dest</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; flights &lt;- read.table(unz("On_Time_On_Time_Performance_2015_7.zip", "On_Time_On_Time_Performance_2015_7.csv"), nrows=1000000, header=T, quote="\"", sep=",")</strong></span>
<span class="strong"><strong>R&gt; flights$DoW_ &lt;- factor(flights$DayOfWeek,levels=c(1,2,3,4,5,6,7), labels=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))</strong></span>
<span class="strong"><strong>R&gt; attach(flights)</strong></span>
<span class="strong"><strong>R&gt; system.time(model &lt;- glm(ArrDel15 ~ UniqueCarrier + DoW_ + Origin + Dest, flights, family="binomial"))</strong></span>
</pre></div><p>While you wait for the<a id="id612000000" class="indexterm"/> results, open another shell and run <code class="literal">glm</code> in the <code class="literal">SparkR</code> mode on the full seven months of data:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sparkR&gt; cache(sparkDf &lt;- read.df(sqlContext, "./flights", "com.databricks.spark.csv", header="true", inferSchema="true"))</strong></span>
<span class="strong"><strong>DataFrame[Year:int, Quarter:int, Month:int, DayofMonth:int, DayOfWeek:int, FlightDate:string, UniqueCarrier:string, AirlineID:int, Carrier:string, TailNum:string, FlightNum:int, OriginAirportID:int, OriginAirportSeqID:int, OriginCityMarketID:int, Origin:string, OriginCityName:string, OriginState:string, OriginStateFips:int, OriginStateName:string, OriginWac:int, DestAirportID:int, DestAirportSeqID:int, DestCityMarketID:int, Dest:string, DestCityName:string, DestState:string, DestStateFips:int, DestStateName:string, DestWac:int, CRSDepTime:int, DepTime:int, DepDelay:double, DepDelayMinutes:double, DepDel15:double, DepartureDelayGroups:int, DepTimeBlk:string, TaxiOut:double, WheelsOff:int, WheelsOn:int, TaxiIn:double, CRSArrTime:int, ArrTime:int, ArrDelay:double, ArrDelayMinutes:double, ArrDel15:double, ArrivalDelayGroups:int, ArrTimeBlk:string, Cancelled:double, CancellationCode:string, Diverted:double, CRSElapsedTime:double, ActualElapsedTime:double, AirTime:double, Flights:double, Distance:double, DistanceGroup:int, CarrierDelay:double, WeatherDelay:double, NASDelay:double, SecurityDelay:double, LateAircraftDelay:double, FirstDepTime:int, TotalAddGTime:double, LongestAddGTime:double, DivAirportLandings:int, DivReachedDest:double, DivActualElapsedTime:double, DivArrDelay:double, DivDistance:double, Div1Airport:string, Div1AirportID:int, Div1AirportSeqID:int, Div1WheelsOn:int, Div1TotalGTime:double, Div1LongestGTime:double, Div1WheelsOff:int, Div1TailNum:string, Div2Airport:string, Div2AirportID:int, Div2AirportSeqID:int, Div2WheelsOn:int, Div2TotalGTime:double, Div2LongestGTime:double, Div2WheelsOff:string, Div2TailNum:string, Div3Airport:string, Div3AirportID:string, Div3AirportSeqID:string, Div3WheelsOn:string, Div3TotalGTime:string, Div3LongestGTime:string, Div3WheelsOff:string, Div3TailNum:string, Div4Airport:string, Div4AirportID:string, Div4AirportSeqID:string, Div4WheelsOn:string, Div4TotalGTime:string, Div4LongestGTime:string, Div4WheelsOff:string, Div4TailNum:string, Div5Airport:string, Div5AirportID:string, Div5AirportSeqID:string, Div5WheelsOn:string, Div5TotalGTime:string, Div5LongestGTime:string, Div5WheelsOff:string, Div5TailNum:string, :string]</strong></span>
<span class="strong"><strong>sparkR&gt; noNulls &lt;- cache(dropna(selectExpr(filter(sparkDf, sparkDf$Cancelled == 0), "ArrDel15", "UniqueCarrier", "format_string('%d', DayOfWeek) as DayOfWeek", "Origin", "Dest"), "any"))</strong></span>
<span class="strong"><strong>sparkR&gt; sparkModel = glm(ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest, noNulls, family="binomial")</strong></span>
</pre></div><p>Here we try to build a<a id="id613000000" class="indexterm"/> model explaining delays as an effect of carrier, day of week, and origin on destination airports, which is captured by the formular construct <code class="literal">ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest</code>.</p><div class="note" title="Note"><h3 class="title"><a id="note10000000"/>Note</h3><p>
<span class="strong"><strong>Nulls, big data, and Scala</strong></span>
</p><p>Note that in the SparkR case of <code class="literal">glm</code>, I had to explicitly filter out the non-cancelled flights and removed the NA—or nulls in the C/Java lingo. While R does this for you by<a id="id614000000" class="indexterm"/> default, NAs in big data are very common as the datasets are typically sparse and shouldn't be treated lightly. The fact that we have to deal with nulls explicitly in MLlib warns us about some additional information in the dataset and is definitely a welcome feature. The presence of an NA can carry <a id="id615000000" class="indexterm"/>information about the way the data was collected. Ideally, each NA should be accompanied by a small <code class="literal">get_na_info</code> method as to why this particular value was not available or collected, which leads us to the <code class="literal">Either</code> type in Scala.</p><p>Even though nulls are inherited from Java and a part of Scala, the <code class="literal">Option</code> and <code class="literal">Either</code> types are new and more robust mechanism to deal with special cases where nulls were traditionally used. Specifically, <code class="literal">Either</code> can provide a value or exception message as to why it was not computed; while <code class="literal">Option</code> can either provide a value or be <code class="literal">None</code>, which can be readily captured by the Scala pattern-matching framework.</p></div><p>One thing you will notice is that SparkR will run multiple threads, and even on a single node, it will consume CPU time from multiple cores and returns much faster even with a larger size of data. In my experiment on a 32-core machine, it was able to finish in under a minute (as opposed to 35 minutes for R <code class="literal">glm</code>). To get the results, as in the R model case, we need to run the <code class="literal">summary()</code> method:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(sparkModel)</strong></span>
<span class="strong"><strong>$coefficients</strong></span>
<span class="strong"><strong>                     Estimate</strong></span>
<span class="strong"><strong>(Intercept)      -1.518542340</strong></span>
<span class="strong"><strong>UniqueCarrier_WN  0.382722232</strong></span>
<span class="strong"><strong>UniqueCarrier_DL -0.047997652</strong></span>
<span class="strong"><strong>UniqueCarrier_OO  0.367031995</strong></span>
<span class="strong"><strong>UniqueCarrier_AA  0.046737727</strong></span>
<span class="strong"><strong>UniqueCarrier_EV  0.344539788</strong></span>
<span class="strong"><strong>UniqueCarrier_UA  0.299290120</strong></span>
<span class="strong"><strong>UniqueCarrier_US  0.069837542</strong></span>
<span class="strong"><strong>UniqueCarrier_MQ  0.467597761</strong></span>
<span class="strong"><strong>UniqueCarrier_B6  0.326240578</strong></span>
<span class="strong"><strong>UniqueCarrier_AS -0.210762769</strong></span>
<span class="strong"><strong>UniqueCarrier_NK  0.841185903</strong></span>
<span class="strong"><strong>UniqueCarrier_F9  0.788720078</strong></span>
<span class="strong"><strong>UniqueCarrier_HA -0.094638586</strong></span>
<span class="strong"><strong>DayOfWeek_5       0.232234937</strong></span>
<span class="strong"><strong>DayOfWeek_4       0.274016179</strong></span>
<span class="strong"><strong>DayOfWeek_3       0.147645473</strong></span>
<span class="strong"><strong>DayOfWeek_1       0.347349366</strong></span>
<span class="strong"><strong>DayOfWeek_2       0.190157420</strong></span>
<span class="strong"><strong>DayOfWeek_7       0.199774806</strong></span>
<span class="strong"><strong>Origin_ATL       -0.180512251</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The worst performer is <code class="literal">NK</code> (Spirit Airlines). Internally, SparkR uses limited-memory BFGS, which is a<a id="id616000000" class="indexterm"/> limited-memory quasi-Newton optimization method that is similar to the results obtained with R <code class="literal">glm</code> on the July data:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; summary(model)</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>glm(formula = ArrDel15 ~ UniqueCarrier + DoW + Origin + Dest, </strong></span>
<span class="strong"><strong>    family = "binomial", data = dow)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-1.4205  -0.7274  -0.6132  -0.4510   2.9414  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                  Estimate Std. Error z value Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)     -1.817e+00  2.402e-01  -7.563 3.95e-14 ***</strong></span>
<span class="strong"><strong>UniqueCarrierAS -3.296e-01  3.413e-02  -9.658  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierB6  3.932e-01  2.358e-02  16.676  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierDL -6.602e-02  1.850e-02  -3.568 0.000359 ***</strong></span>
<span class="strong"><strong>UniqueCarrierEV  3.174e-01  2.155e-02  14.728  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierF9  6.754e-01  2.979e-02  22.668  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierHA  7.883e-02  7.058e-02   1.117 0.264066    </strong></span>
<span class="strong"><strong>UniqueCarrierMQ  2.175e-01  2.393e-02   9.090  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierNK  7.928e-01  2.702e-02  29.343  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierOO  4.001e-01  2.019e-02  19.817  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierUA  3.982e-01  1.827e-02  21.795  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>UniqueCarrierVX  9.723e-02  3.690e-02   2.635 0.008423 ** </strong></span>
<span class="strong"><strong>UniqueCarrierWN  6.358e-01  1.700e-02  37.406  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>dowTue           1.365e-01  1.313e-02  10.395  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>dowWed           1.724e-01  1.242e-02  13.877  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>dowThu           4.593e-02  1.256e-02   3.656 0.000256 ***</strong></span>
<span class="strong"><strong>dowFri          -2.338e-01  1.311e-02 -17.837  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>dowSat          -2.413e-01  1.458e-02 -16.556  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>dowSun          -3.028e-01  1.408e-02 -21.511  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>OriginABI       -3.355e-01  2.554e-01  -1.314 0.188965    </strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Other parameters of SparkR <code class="literal">glm</code> implementation are provided in the following table:</p><p>The following table<a id="id617000000" class="indexterm"/> shows a list of parameters for SparkR <code class="literal">glm</code> implementation:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Parameter</p>
</th><th valign="bottom">
<p>Possible Values</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<code class="literal">formula</code>
</p>
</td><td valign="top">
<p>A symbolic description like in R</p>
</td><td valign="top">
<p>Currently<a id="id618000000" class="indexterm"/> only a subset of formula operators are supported: '<code class="literal">~</code>', '<code class="literal">.</code>', '<code class="literal">:</code>', '<code class="literal">+</code>', and '<code class="literal">-</code>'</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">family</code>
</p>
</td><td valign="top">
<p>gaussian or binomial</p>
</td><td valign="top">
<p>Needs to be in <a id="id619000000" class="indexterm"/>quotes: gaussian -&gt; linear regression, binomial -&gt; logistic regression</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">data</code>
</p>
</td><td valign="top">
<p>DataFrame</p>
</td><td valign="top">
<p>Needs to be<a id="id620000000" class="indexterm"/> SparkR DataFrame, not <code class="literal">data.frame</code>
</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">lambda</code>
</p>
</td><td valign="top">
<p>positive</p>
</td><td valign="top">
<p>Regularization<a id="id621000000" class="indexterm"/> coefficient</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">alpha</code>
</p>
</td><td valign="top">
<p>positive</p>
</td><td valign="top">
<p>Elastic-net <a id="id622000000" class="indexterm"/>mixing parameter (refer to glmnet's documentation for details)</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">standardize</code>
</p>
</td><td valign="top">
<p>TRUE or <a id="id623000000" class="indexterm"/>FALSE</p>
</td><td valign="top">
<p>User-defined</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">solver</code>
</p>
</td><td valign="top">
<p>l-bfgs, normal or auto</p>
</td><td valign="top">
<p>auto<a id="id624000000" class="indexterm"/> will choose the algorithm automatically, l-bfgs means limited-memory BFGS, normal means using normal equation as an analytical solution to the linear regression problem</p>
</td></tr></tbody></table></div></div><div class="section" title="Reading JSON files in SparkR"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4500000"/>Reading JSON files in SparkR</h2></div></div></div><p>Schema on Read is<a id="id625000000" class="indexterm"/> one of the convenient features of big data. The DataFrame class has the ability to figure out the schema of a text file containing a JSON<a id="id626000000" class="indexterm"/> record per line:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ cat examples/src/main/resources/people.json </strong></span>
<span class="strong"><strong>{"name":"Michael"}</strong></span>
<span class="strong"><strong>{"name":"Andy", "age":30}</strong></span>
<span class="strong"><strong>{"name":"Justin", "age":19}</strong></span>

<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/sparkR</strong></span>
<span class="strong"><strong>...</strong></span>

<span class="strong"><strong>&gt; people = read.json(sqlContext, "examples/src/main/resources/people.json")</strong></span>
<span class="strong"><strong>&gt; dtypes(people)</strong></span>
<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>[1] "age"    "bigint"</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>[1] "name"   "string"</strong></span>

<span class="strong"><strong>&gt; schema(people)</strong></span>
<span class="strong"><strong>StructType</strong></span>
<span class="strong"><strong>|-name = "age", type = "LongType", nullable = TRUE</strong></span>
<span class="strong"><strong>|-name = "name", type = "StringType", nullable = TRUE</strong></span>
<span class="strong"><strong>&gt; showDF(people)</strong></span>
<span class="strong"><strong>+----+-------+</strong></span>
<span class="strong"><strong>| age|   name|</strong></span>
<span class="strong"><strong>+----+-------+</strong></span>
<span class="strong"><strong>|null|Michael|</strong></span>
<span class="strong"><strong>|  30|   Andy|</strong></span>
<span class="strong"><strong>|  19| Justin|</strong></span>
<span class="strong"><strong>+----+-------+</strong></span>
</pre></div></div><div class="section" title="Writing Parquet files in SparkR"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4600000"/>Writing Parquet files in SparkR</h2></div></div></div><p>As we<a id="id627000000" class="indexterm"/> mentioned in the previous chapter, the Parquet format is an efficient storage format, particularly for low cardinality columns. Parquet files can be<a id="id628000000" class="indexterm"/> read/written directly from R:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; write.parquet(sparkDf, "parquet")</strong></span>
</pre></div><p>You can see that the new Parquet file is 66 times smaller that the original zip file downloaded from the DoT:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ ls –l On_Time_On_Time_Performance_2015_7.zip parquet/ flights/</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff  26204213 Sep  9 12:21 /Users/akozlov/spark/On_Time_On_Time_Performance_2015_7.zip</strong></span>

<span class="strong"><strong>flights/:</strong></span>
<span class="strong"><strong>total 459088</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff  235037955 Sep  9 12:20 On_Time_On_Time_Performance_2015_7.csv</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff      12054 Sep  9 12:20 readme.html</strong></span>

<span class="strong"><strong>parquet/:</strong></span>
<span class="strong"><strong>total 848</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff       0 Jan 24 22:50 _SUCCESS</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff   10000 Jan 24 22:50 _common_metadata</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff   23498 Jan 24 22:50 _metadata</strong></span>
<span class="strong"><strong>-rw-r--r--  1 akozlov  staff  394418 Jan 24 22:50 part-r-00000-9e2d0004-c71f-4bf5-aafe-90822f9d7223.gz.parquet</strong></span>
</pre></div></div><div class="section" title="Invoking Scala from R"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4700000"/>Invoking Scala from R</h2></div></div></div><p>Let's assume that one <a id="id629000000" class="indexterm"/>has an exceptional implementation of a numeric method in Scala that we want to call from R. One way of doing this would be to use the R <code class="literal">system()</code> function that invokes <code class="literal">/bin/sh</code> on Unix-like systems. However, the <code class="literal">rscala</code> package is a more efficient way that starts a Scala interpreter and maintains communication over TCP/IP network connection.</p><p>Here, the Scala interpreter maintains the state (memoization) between the calls. Similarly, one can define functions, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>R&gt; scala &lt;- scalaInterpreter()</strong></span>
<span class="strong"><strong>R&gt; scala %~% 'def pri(i: Stream[Int]): Stream[Int] = i.head #:: pri(i.tail filter  { x =&gt; { println("Evaluating " + x + "%" + i.head); x % i.head != 0 } } )'</strong></span>
<span class="strong"><strong>ScalaInterpreterReference... engine: javax.script.ScriptEngine</strong></span>
<span class="strong"><strong>R&gt; scala %~% 'val primes = pri(Stream.from(2))'</strong></span>
<span class="strong"><strong>ScalaInterpreterReference... primes: Stream[Int]</strong></span>
<span class="strong"><strong>R&gt; scala %~% 'primes take 5 foreach println'</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>Evaluating 3%2</strong></span>
<span class="strong"><strong>3</strong></span>
<span class="strong"><strong>Evaluating 4%2</strong></span>
<span class="strong"><strong>Evaluating 5%2</strong></span>
<span class="strong"><strong>Evaluating 5%3</strong></span>
<span class="strong"><strong>5</strong></span>
<span class="strong"><strong>Evaluating 6%2</strong></span>
<span class="strong"><strong>Evaluating 7%2</strong></span>
<span class="strong"><strong>Evaluating 7%3</strong></span>
<span class="strong"><strong>Evaluating 7%5</strong></span>
<span class="strong"><strong>7</strong></span>
<span class="strong"><strong>Evaluating 8%2</strong></span>
<span class="strong"><strong>Evaluating 9%2</strong></span>
<span class="strong"><strong>Evaluating 9%3</strong></span>
<span class="strong"><strong>Evaluating 10%2</strong></span>
<span class="strong"><strong>Evaluating 11%2</strong></span>
<span class="strong"><strong>Evaluating 11%3</strong></span>
<span class="strong"><strong>Evaluating 11%5</strong></span>
<span class="strong"><strong>Evaluating 11%7</strong></span>
<span class="strong"><strong>11</strong></span>
<span class="strong"><strong>R&gt; scala %~% 'primes take 5 foreach println'</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>3</strong></span>
<span class="strong"><strong>5</strong></span>
<span class="strong"><strong>7</strong></span>
<span class="strong"><strong>11</strong></span>
<span class="strong"><strong>R&gt; scala %~% 'primes take 7 foreach println'</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>3</strong></span>
<span class="strong"><strong>5</strong></span>
<span class="strong"><strong>7</strong></span>
<span class="strong"><strong>11</strong></span>
<span class="strong"><strong>Evaluating 12%2</strong></span>
<span class="strong"><strong>Evaluating 13%2</strong></span>
<span class="strong"><strong>Evaluating 13%3</strong></span>
<span class="strong"><strong>Evaluating 13%5</strong></span>
<span class="strong"><strong>Evaluating 13%7</strong></span>
<span class="strong"><strong>Evaluating 13%11</strong></span>
<span class="strong"><strong>13</strong></span>
<span class="strong"><strong>Evaluating 14%2</strong></span>
<span class="strong"><strong>Evaluating 15%2</strong></span>
<span class="strong"><strong>Evaluating 15%3</strong></span>
<span class="strong"><strong>Evaluating 16%2</strong></span>
<span class="strong"><strong>Evaluating 17%2</strong></span>
<span class="strong"><strong>Evaluating 17%3</strong></span>
<span class="strong"><strong>Evaluating 17%5</strong></span>
<span class="strong"><strong>Evaluating 17%7</strong></span>
<span class="strong"><strong>Evaluating 17%11</strong></span>
<span class="strong"><strong>Evaluating 17%13</strong></span>
<span class="strong"><strong>17</strong></span>
<span class="strong"><strong>R&gt; </strong></span>
</pre></div><p>R from Scala can<a id="id630000000" class="indexterm"/> be invoked using the <code class="literal">!</code> or <code class="literal">!!</code> Scala operators and <code class="literal">Rscript</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ cat &lt;&lt; EOF &gt; rdate.R</strong></span>
<span class="strong"><strong>&gt; #!/usr/local/bin/Rscript</strong></span>
<span class="strong"><strong>&gt; </strong></span>
<span class="strong"><strong>&gt; write(date(), stdout())</strong></span>
<span class="strong"><strong>&gt; EOF</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ chmod a+x rdate.R</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import sys.process._</strong></span>
<span class="strong"><strong>import sys.process._</strong></span>

<span class="strong"><strong>scala&gt; val date = Process(Seq("./rdate.R")).!!</strong></span>
<span class="strong"><strong>date: String =</strong></span>
<span class="strong"><strong>"Wed Feb 24 02:20:09 2016</strong></span>
<span class="strong"><strong>"</strong></span>
</pre></div><div class="section" title="Using Rserve"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0600000"/>Using Rserve</h3></div></div></div><p>A more efficient way is to use the similar TCP/IP binary transport protocol to communicate with R <a id="id631000000" class="indexterm"/>with <code class="literal">Rsclient/Rserve</code> (<a class="ulink" href="http://www.rforge.net/Rserve">http://www.rforge.net/Rserve</a>). To start <code class="literal">Rserve</code> on a<a id="id632000000" class="indexterm"/> node that has R installed, perform the following action:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ wget http://www.rforge.net/Rserve/snapshot/Rserve_1.8-5.tar.gz</strong></span>

<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar</strong></span>
<span class="strong"><strong>.gz</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar.gz</strong></span>

<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro ~]$ $ R -q CMD Rserve</strong></span>

<span class="strong"><strong>R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"</strong></span>
<span class="strong"><strong>Copyright (C) 2015 The R Foundation for Statistical Computing</strong></span>
<span class="strong"><strong>Platform: x86_64-apple-darwin13.4.0 (64-bit)</strong></span>

<span class="strong"><strong>R is free software and comes with ABSOLUTELY NO WARRANTY.</strong></span>
<span class="strong"><strong>You are welcome to redistribute it under certain conditions.</strong></span>
<span class="strong"><strong>Type 'license()' or 'licence()' for distribution details.</strong></span>

<span class="strong"><strong>  Natural language support but running in an English locale</strong></span>

<span class="strong"><strong>R is a collaborative project with many contributors.</strong></span>
<span class="strong"><strong>Type 'contributors()' for more information and</strong></span>
<span class="strong"><strong>'citation()' on how to cite R or R packages in publications.</strong></span>

<span class="strong"><strong>Type 'demo()' for some demos, 'help()' for on-line help, or</strong></span>
<span class="strong"><strong>'help.start()' for an HTML browser interface to help.</strong></span>
<span class="strong"><strong>Type 'q()' to quit R.</strong></span>

<span class="strong"><strong>Rserv started in daemon mode.</strong></span>
</pre></div><p>By default, <code class="literal">Rserv</code> opens a connection on <code class="literal">localhost:6311</code>. The advantage of the binary network protocol is<a id="id633000000" class="indexterm"/> that it is platform-independent and multiple clients can communicate with the server. The clients can connect to <code class="literal">Rserve</code>.</p><p>Note that, while passing the results as a binary object has its advantages, you have to be careful with the type mappings between R and Scala. <code class="literal">Rserve</code> supports other clients, including Python, but I will also cover JSR 223-compliant scripting at the end of this chapter.</p></div></div></div></div>
<div class="section" title="Integrating with Python"><div class="titlepage" id="aid-8JJII2"><div><div><h1 class="title"><a id="ch08lvl1sec6200000"/>Integrating with Python</h1></div></div></div><p>Python has slowly<a id="id634000000" class="indexterm"/> established ground as a de-facto tool for data science. It has <a id="id635000000" class="indexterm"/>a command-line interface and decent visualization<a id="id636000000" class="indexterm"/> via matplotlib and ggplot, which is based on R's ggplot2. Recently, Wes McKinney, the creator of Pandas, the time series data-analysis package, has joined Cloudera to pave way for Python in big data.</p><div class="section" title="Setting up Python"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4800000"/>Setting up Python</h2></div></div></div><p>Python is usually part <a id="id637000000" class="indexterm"/>of the default installation. Spark requires version 2.7.0+.</p><p>If you don't have <a id="id638000000" class="indexterm"/>Python on Mac OS, I recommend installing the Homebrew package manager from <a class="ulink" href="http://brew.sh">http://brew.sh</a>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark(master)]$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</strong></span>
<span class="strong"><strong>==&gt; This script will install:</strong></span>
<span class="strong"><strong>/usr/local/bin/brew</strong></span>
<span class="strong"><strong>/usr/local/Library/...</strong></span>
<span class="strong"><strong>/usr/local/share/man/man1/brew.1</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark(master)]$ brew install python</strong></span>
<span class="strong"><strong>…</strong></span>
</pre></div><p>Otherwise, on a Unix-like system, Python can be compiled from the source distribution:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ export PYTHON_VERSION=2.7.11</strong></span>
<span class="strong"><strong>$ wget -O - https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz | tar xzvf -</strong></span>
<span class="strong"><strong>$ cd $HOME/Python-$PYTHON_VERSION</strong></span>
<span class="strong"><strong>$ ./configure--prefix=/usr/local --enable-unicode=ucs4--enable-shared LDFLAGS="-Wl,-rpath /usr/local/lib"</strong></span>
<span class="strong"><strong>$ make; sudo make altinstall</strong></span>
<span class="strong"><strong>$ sudo ln -sf /usr/local/bin/python2.7 /usr/local/bin/python</strong></span>
</pre></div><p>It is good practice to place it in a directory different from the default Python installation. It is normal to have multiple versions of Python on a single system, which usually does not lead to <a id="id639000000" class="indexterm"/>problems as Python separates the installation directories. For the purpose of this chapter, as for many machine learning takes, I'll also need a few packages. The packages and specific versions may differ across installations:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget https://bootstrap.pypa.io/ez_setup.py</strong></span>
<span class="strong"><strong>$ sudo /usr/local/bin/python ez_setup.py</strong></span>
<span class="strong"><strong>$ sudo /usr/local/bin/easy_install-2.7 pip</strong></span>
<span class="strong"><strong>$ sudo /usr/local/bin/pip install --upgrade avro nose numpy scipy pandas statsmodels scikit-learn iso8601 python-dateutil python-snappy</strong></span>
</pre></div><p>If everything compiles—SciPy uses a Fortran compiler and libraries for linear algebra—we are ready to use Python 2.7.11!</p><div class="note" title="Note"><h3 class="title"><a id="note11000000"/>Note</h3><p>Note that if one wants to use Python with the <code class="literal">pipe</code> command in a distributed environment, Python needs to be installed on every node in the network.</p></div></div><div class="section" title="PySpark"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec4900000"/>PySpark</h2></div></div></div><p>As <code class="literal">bin/sparkR</code> launches<a id="id640000000" class="indexterm"/> R with preloaded Spark context, <code class="literal">bin/pyspark</code> launches Python shell with preloaded Spark context and Spark driver running. The <a id="id641000000" class="indexterm"/>
<code class="literal">PYSPARK_PYTHON</code> environment variable can be used to point to a specific Python version:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ export PYSPARK_PYTHON=/usr/local/bin/python</strong></span>
<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/pyspark </strong></span>
<span class="strong"><strong>Python 2.7.11 (default, Jan 23 2016, 20:14:24) </strong></span>
<span class="strong"><strong>[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin</strong></span>
<span class="strong"><strong>Type "help", "copyright", "credits" or "license" for more information.</strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /__ / .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Python version 2.7.11 (default, Jan 23 2016 20:14:24)</strong></span>
<span class="strong"><strong>SparkContext available as sc, HiveContext available as sqlContext.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;</strong></span>
</pre></div><p>PySpark<a id="id642000000" class="indexterm"/> directly <a id="id643000000" class="indexterm"/>supports most of MLlib functionality on Spark RDDs (<a class="ulink" href="http://spark.apache.org/docs/latest/api/python">http://spark.apache.org/docs/latest/api/python</a>), but it is known to lag a few releases behind the <a id="id644000000" class="indexterm"/>Scala API (<a class="ulink" href="http://spark.apache.org/docs/latest/api/python">http://spark.apache.org/docs/latest/api/python</a>). As of the 1.6.0+ release, it also supports <a id="id645000000" class="indexterm"/>DataFrames (<a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; sfoFlights = sqlContext.sql("SELECT Dest, UniqueCarrier, ArrDelayMinutes FROM parquet.parquet")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sfoFlights.groupBy(["Dest", "UniqueCarrier"]).agg(func.avg("ArrDelayMinutes"), func.count("ArrDelayMinutes")).sort("avg(ArrDelayMinutes)", ascending=False).head(5)</strong></span>
<span class="strong"><strong>[Row(Dest=u'HNL', UniqueCarrier=u'HA', avg(ArrDelayMinutes)=53.70967741935484, count(ArrDelayMinutes)=31), Row(Dest=u'IAH', UniqueCarrier=u'F9', avg(ArrDelayMinutes)=43.064516129032256, count(ArrDelayMinutes)=31), Row(Dest=u'LAX', UniqueCarrier=u'DL', avg(ArrDelayMinutes)=39.68691588785047, count(ArrDelayMinutes)=214), Row(Dest=u'LAX', UniqueCarrier=u'WN', avg(ArrDelayMinutes)=29.704453441295545, count(ArrDelayMinutes)=247), Row(Dest=u'MSO', UniqueCarrier=u'OO', avg(ArrDelayMinutes)=29.551724137931036, count(ArrDelayMinutes)=29)]</strong></span>
</pre></div></div><div class="section" title="Calling Python from Java/Scala"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec5000000"/>Calling Python from Java/Scala</h2></div></div></div><p>As this is really a <a id="id646000000" class="indexterm"/>book about Scala, we should also mention that one can call Python code and its interpreter directly from Scala (or Java). There are a few <a id="id647000000" class="indexterm"/>options available that will be discussed in this chapter.</p><div class="section" title="Using sys.process._"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0700000"/>Using sys.process._</h3></div></div></div><p>Scala, as well as <a id="id648000000" class="indexterm"/>Java, can call OS processes via spawning a separate thread, which we already used for interactive analysis in <a class="link" title="Chapter 1. Exploratory Data Analysis" href="part0235.xhtml#aid-703K61">Chapter 1</a>, <span class="emphasis"><em>Exploratory Data Analysis</em></span>: the <code class="literal">.!</code> method will start the process and return the exit code, while <code class="literal">.!!</code> will return the string that contains the output:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import sys.process._</strong></span>
<span class="strong"><strong>import sys.process._</strong></span>

<span class="strong"><strong>scala&gt; val retCode = Process(Seq("/usr/local/bin/python", "-c", "import socket; print(socket.gethostname())")).!</strong></span>
<span class="strong"><strong>Alexanders-MacBook-Pro.local</strong></span>
<span class="strong"><strong>retCode: Int = 0</strong></span>

<span class="strong"><strong>scala&gt; val lines = Process(Seq("/usr/local/bin/python", "-c", """from datetime import datetime, timedelta; print("Yesterday was {}".format(datetime.now()-timedelta(days=1)))""")).!!</strong></span>
<span class="strong"><strong>lines: String =</strong></span>
<span class="strong"><strong>"Yesterday was 2016-02-12 16:24:53.161853</strong></span>
<span class="strong"><strong>"</strong></span>
</pre></div><p>Let's try a more complex SVD computation (similar to the one we used in SVD++ recommendation engine, but this time, it invokes BLAS C-libraries at the backend). I created a Python executable that takes a string representing a matrix and the required rank as an input and outputs an SVD approximation with the provided rank:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env python

import sys
import os
import re

import numpy as np
from scipy import linalg
from scipy.linalg import svd

np.set_printoptions(linewidth=10000)

def process_line(input):
    inp = input.rstrip("\r\n")
    if len(inp) &gt; 1:
        try:
            (mat, rank) = inp.split("|")
            a = np.matrix(mat)
            r = int(rank)
        except:
            a = np.matrix(inp)
            r = 1
        U, s, Vh = linalg.svd(a, full_matrices=False)
        for i in xrange(r, s.size):
            s[i] = 0
        S = linalg.diagsvd(s, s.size, s.size)
        print(str(np.dot(U, np.dot(S, Vh))).replace(os.linesep, ";"))

if __name__ == '__main__':
    map(process_line, sys.stdin)</pre></div><p>Let's call it <code class="literal">svd.py</code> and <a id="id649000000" class="indexterm"/>put in in the current directory. Given a matrix and rank as an input, it produces an approximation of a given rank:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ echo -e "1,2,3;2,1,2;3,2,1;7,8,9|3" | ./svd.py</strong></span>
<span class="strong"><strong>[[ 1.  2.  3.]; [ 2.  1.  2.]; [ 3.  2.  1.]; [ 7.  8.  9.]]</strong></span>
</pre></div><p>To call it from Scala, let's define the following <code class="literal">#&lt;&lt;&lt;</code> method in our DSL:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; implicit class RunCommand(command: String) {</strong></span>
<span class="strong"><strong>     |   def #&lt;&lt;&lt; (input: String)(implicit buffer: StringBuilder) =  {</strong></span>
<span class="strong"><strong>     |     val process = Process(command)</strong></span>
<span class="strong"><strong>     |     val io = new ProcessIO (</strong></span>
<span class="strong"><strong>     |       in  =&gt; { in.write(input getBytes "UTF-8"); in.close},</strong></span>
<span class="strong"><strong>     |       out =&gt; { buffer append scala.io.Source.fromInputStream(out).getLines.mkString("\n"); buffer.append("\n"); out.close() },</strong></span>
<span class="strong"><strong>     |       err =&gt; { scala.io.Source.fromInputStream(err).getLines().foreach(System.err.println) })</strong></span>
<span class="strong"><strong>     |     (process run io).exitValue</strong></span>
<span class="strong"><strong>     |   }</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>defined class RunCommand</strong></span>
</pre></div><p>Now, we can use the <code class="literal">#&lt;&lt;&lt;</code> operator to call Python's SVD method:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; implicit val buffer = new StringBuilder()</strong></span>
<span class="strong"><strong>buffer: StringBuilder =</strong></span>

<span class="strong"><strong>scala&gt; if ("./svd.py" #&lt;&lt;&lt; "1,2,3;2,1,2;3,2,1;7,8,9|1" == 0)  Some(buffer.toString) else None</strong></span>
<span class="strong"><strong>res77: Option[String] = Some([[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]])</strong></span>
</pre></div><p>Note that as we requested<a id="id650000000" class="indexterm"/> the resulting matrix rank to be one, all rows and columns are linearly dependent. We can even pass several lines of input at a time, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; if ("./svd.py" #&lt;&lt;&lt; """</strong></span>
<span class="strong"><strong>     | 1,2,3;2,1,2;3,2,1;7,8,9|0</strong></span>
<span class="strong"><strong>     | 1,2,3;2,1,2;3,2,1;7,8,9|1</strong></span>
<span class="strong"><strong>     | 1,2,3;2,1,2;3,2,1;7,8,9|2</strong></span>
<span class="strong"><strong>     | 1,2,3;2,1,2;3,2,1;7,8,9|3""" == 0) Some(buffer.toString) else None</strong></span>
<span class="strong"><strong>res80: Option[String] =</strong></span>
<span class="strong"><strong>Some([[ 0.  0.  0.]; [ 0.  0.  0.]; [ 0.  0.  0.]; [ 0.  0.  0.]]</strong></span>
<span class="strong"><strong>[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]</strong></span>
<span class="strong"><strong>[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]</strong></span>
<span class="strong"><strong>[[ 1.  2.  3.]; [ 2.  1.  2.]; [ 3.  2.  1.]; [ 7.  8.  9.]])</strong></span>
</pre></div></div><div class="section" title="Spark pipe"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0800000"/>Spark pipe</h3></div></div></div><p>SVD decomposition is usually a pretty heavy operation, so the relative overhead of calling Python in this case is small. We can avoid this overhead if we keep the process running and supply <a id="id651000000" class="indexterm"/>several lines at a time, like we did in the last example. Both Hadoop MR and Spark implement this approach. For example, in Spark, the whole computation will take only one line, as shown in the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; sc.parallelize(List("1,2,3;2,1,2;3,2,1;7,8,9|0", "1,2,3;2,1,2;3,2,1;7,8,9|1", "1,2,3;2,1,2;3,2,1;7,8,9|2", "1,2,3;2,1,2;3,2,1;7,8,9|3"),4).pipe("./svd.py").collect.foreach(println)</strong></span>
<span class="strong"><strong>[[ 0.  0.  0.]; [ 0.  0.  0.]; [ 0.  0.  0.]; [ 0.  0.  0.]]</strong></span>
<span class="strong"><strong>[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]</strong></span>
<span class="strong"><strong>[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]</strong></span>
<span class="strong"><strong>[[ 1.  2.  3.]; [ 2.  1.  2.]; [ 3.  2.  1.]; [ 7.  8.  9.]]</strong></span>
</pre></div><p>The whole pipeline is ready to be distributed across a cluster of multicore workstations! I think you will be in love with Scala/Spark already.</p><p>Note that debugging the pipelined executions might be tricky as the data is passed from one process to another using OS pipes.</p></div><div class="section" title="Jython and JSR 223"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec0900000"/>Jython and JSR 223</h3></div></div></div><p>For completeness, we<a id="id652000000" class="indexterm"/> need to mention Jython, a Java<a id="id653000000" class="indexterm"/> implementation of Python (as opposed to a more familiar C implementation, also called CPython). Jython avoids the problem of passing input/output via OS pipelines by allowing the users to compile Python source code to Java byte codes, and running the resulting bytecodes on any Java virtual machine. As Scala also runs in Java virtual machine, it can use the Jython classes directly, although the reverse is not true in general; Scala classes sometimes are not compatible to be used by Java/Jython.</p><div class="note" title="Note"><h3 class="title"><a id="note12000000"/>Note</h3><p>
<span class="strong"><strong>JSR 223</strong></span>
</p><p>In this <a id="id654000000" class="indexterm"/>particular case, the request is for "Scripting for the JavaTM Platform" and was originally filed on Nov 15th 2004 (<a class="ulink" href="https://www.jcp.org/en/jsr/detail?id=223">https://www.jcp.org/en/jsr/detail?id=223</a>). At the beginning, it was targeted towards the ability of the Java servlet to work with multiple scripting languages. The<a id="id655000000" class="indexterm"/> specification requires the scripting language maintainers to provide a Java JAR with corresponding implementations. Portability issues hindered practical implementations, particularly when platforms require complex interaction with OS, such as dynamic linking in C or Fortran. Currently, only a handful languages are supported, with R and Python being supported, but in incomplete form.</p></div><p>Since Java 6, JSR 223: Scripting for Java added the <code class="literal">javax.script</code> package that allows multiple scripting languages to be called through the same API as long as the language provides a script engine. To add the Jython scripting language, download the latest Jython JAR from the<a id="id656000000" class="indexterm"/> Jython site at <a class="ulink" href="http://www.jython.org/downloads.html">http://www.jython.org/downloads.html</a>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ wget -O jython-standalone-2.7.0.jar http://search.maven.org/remotecontent?filepath=org/python/jython-standalone/2.7.0/jython-standalone-2.7.0.jar</strong></span>

<span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro Scala]$ scala -cp jython-standalone-2.7.0.jar </strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import javax.script.ScriptEngine;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; import javax.script.ScriptEngineManager;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; import javax.script.ScriptException;</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; val manager = new ScriptEngineManager();</strong></span>
<span class="strong"><strong>manager: javax.script.ScriptEngineManager = javax.script.ScriptEngineManager@3a03464</strong></span>

<span class="strong"><strong>scala&gt; val engines = manager.getEngineFactories();</strong></span>
<span class="strong"><strong>engines: java.util.List[javax.script.ScriptEngineFactory] = [org.python.jsr223.PyScriptEngineFactory@4909b8da, jdk.nashorn.api.scripting.NashornScriptEngineFactory@68837a77, scala.tools.nsc.interpreter.IMain$Factory@1324409e]</strong></span>
</pre></div><p>Now, I can use the Jython/Python scripting engine:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val engine = new ScriptEngineManager().getEngineByName("jython");</strong></span>
<span class="strong"><strong>engine: javax.script.ScriptEngine = org.python.jsr223.PyScriptEngine@6094de13</strong></span>

<span class="strong"><strong>scala&gt; engine.eval("from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))")</strong></span>
<span class="strong"><strong>res15: Object = null</strong></span>

<span class="strong"><strong>scala&gt; engine.get("yesterday")</strong></span>
<span class="strong"><strong>res16: Object = 2016-02-12 23:26:38.012000</strong></span>
</pre></div><p>It is worth giving a disclaimer here that not all Python modules are available in Jython. Modules that require a C/Fortran dynamic linkage for the library that doesn't exist in Java are not likely to work in Jython. Specifically, NumPy and SciPy are not supported in Jython as they rely on C/Fortran. If you discover some other missing modules, you can try copying the <code class="literal">.py</code> file from a Python distribution to a <code class="literal">sys.path</code> Jython directory—if this works, consider <a id="id657000000" class="indexterm"/>yourself lucky.</p><p>Jython has the <a id="id658000000" class="indexterm"/>advantage of accessing Python-rich modules without the necessity of starting the Python runtime on each call, which might result in a significant performance saving:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val startTime = System.nanoTime</strong></span>
<span class="strong"><strong>startTime: Long = 54384084381087</strong></span>

<span class="strong"><strong>scala&gt; for (i &lt;- 1 to 100) {</strong></span>
<span class="strong"><strong>     |   engine.eval("from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))")</strong></span>
<span class="strong"><strong>     |   val yesterday = engine.get("yesterday")</strong></span>
<span class="strong"><strong>     | }</strong></span>

<span class="strong"><strong>scala&gt; val elapsed = 1e-9 * (System.nanoTime - startTime)</strong></span>
<span class="strong"><strong>elapsed: Double = 0.270837934</strong></span>

<span class="strong"><strong>scala&gt; val startTime = System.nanoTime</strong></span>
<span class="strong"><strong>startTime: Long = 54391560460133</strong></span>

<span class="strong"><strong>scala&gt; for (i &lt;- 1 to 100) {</strong></span>
<span class="strong"><strong>     |   val yesterday = Process(Seq("/usr/local/bin/python", "-c", """from datetime import datetime, timedelta; print(datetime.now()-timedelta(days=1))""")).!!</strong></span>
<span class="strong"><strong>     | }</strong></span>

<span class="strong"><strong>scala&gt; val elapsed = 1e-9 * (System.nanoTime - startTime)</strong></span>
<span class="strong"><strong>elapsed: Double = 2.221937263</strong></span>
</pre></div><p>Jython JSR 223 call is 10 times faster!</p></div></div></div>
<div class="section" title="Summary" id="aid-8KI341"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec6300000"/>Summary</h1></div></div></div><p>R and Python are like bread and butter for a data scientist. Modern frameworks tend to be interoperable and borrow from each other's strength. In this chapter, I went over the plumbing of interoperability with R and Python. Both of them have packages (R) and modules (Python) that became very popular and extend the current Scala/Spark functionality. Many consider R and Python existing libraries to be crucial for their implementations.</p><p>This chapter demonstrated a few ways to integrate these packages and provide the tradeoffs of using these integrations so that we can proceed on to the next chapter, looking at the NLP, where functional programming has been traditionally used from the start.</p></div>
<div class="chapter" title="Chapter&#xA0;9.&#xA0;NLP in Scala"><div class="titlepage" id="aid-8LGJM2"><div><div><h1 class="title"><a id="ch35"/>Chapter 9. NLP in Scala</h1></div></div></div><p>This chapter describes a few common techniques of <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>), specifically, the ones that can benefit from Scala. There are some NLP packages in the open <a id="id659000000" class="indexterm"/>source out there. The most famous of them is probably <a id="id660000000" class="indexterm"/>NLTK (<a class="ulink" href="http://www.nltk.org">http://www.nltk.org</a>), which is written in Python, and ostensibly even a larger number of proprietary software solutions emphasizing<a id="id661000000" class="indexterm"/> different aspects of NLP. It is worth mentioning Wolf (<a class="ulink" href="https://github.com/wolfe-pack">https://github.com/wolfe-pack</a>), FACTORIE (<a class="ulink" href="http://factorie.cs.umass.edu">http://factorie.cs.umass.edu</a>), and ScalaNLP (<a class="ulink" href="http://www.scalanlp.org">http://www.scalanlp.org</a>), and skymind (<a class="ulink" href="http://www.skymind.io">http://www.skymind.io</a>), which is partly proprietary. However, few open source <a id="id662000000" class="indexterm"/>projects<a id="id663000000" class="indexterm"/> in this area remain<a id="id664000000" class="indexterm"/> active for a long period of time for one or another reason. Most projects are being eclipsed by Spark and MLlib capabilities, particularly, in the scalability aspect.</p><p>Instead of giving a detailed description of each of the NLP projects, which also might include speech-to-text, text-to-speech, and language translators, I will provide a few basic techniques focused on leveraging Spark MLlib in this chapter. The chapter comes very naturally as the last analytics chapter in this book. Scala is a very natural-language looking computer language and this chapter will leverage the techniques I developed earlier.</p><p>NLP arguably is the core of AI. Originally, the AI was created to mimic the humans, and natural language parsing and understanding is an indispensable part of it. Big data techniques has started to penetrate NLP, even though traditionally, NLP is very computationally intensive and is regarded as a small data problem. NLP often requires extensive deep learning techniques, and the volume of data of all written texts appears to be not so large compared to the logs volumes generated by all the machines today and analyzed by the big data machinery.</p><p>Even though the Library of Congress counts millions of documents, most of them can be digitized in PBs of actual digital data, a volume that any social websites is able to collect, store, and analyze within a few seconds. Complete works of most prolific authors can be stored within a few MBs of files (refer to <span class="emphasis"><em>Table 09-1</em></span>). Nonetheless, the social network and ADTECH companies parse text from millions of users and in hundreds of contexts every day.</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>The complete works of</p>
</th><th valign="bottom">
<p>When lived</p>
</th><th valign="bottom">
<p>Size</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<span class="emphasis"><em>Plato</em></span>
</p>
</td><td valign="top">
<p>428/427 (or 424/423) - 348/347 BC</p>
</td><td valign="top">
<p>2.1 MB</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>William Shakespeare</em></span>
</p>
</td><td valign="top">
<p>26 April 1564 (baptized) - 23 April 1616</p>
</td><td valign="top">
<p>3.8 MB</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>Fyodor Dostoevsky</em></span>
</p>
</td><td valign="top">
<p>11 November 1821 - 9 February 1881</p>
</td><td valign="top">
<p>5.9 MB</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>Leo Tolstoy</em></span>
</p>
</td><td valign="top">
<p>9 September 1828 - 20 November 1910</p>
</td><td valign="top">
<p>6.9 MB</p>
</td></tr><tr><td valign="top">
<p>
<span class="emphasis"><em>Mark Twain</em></span>
</p>
</td><td valign="top">
<p>November 30, 1835 - April 21, 1910</p>
</td><td valign="top">
<p>13 MB</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 09-1. Complete Works collections of some famous writers (most can be acquired on Amazon.com today for a few dollars, later authors, although readily digitized, are more expensive)</p></blockquote></div><p>The natural language is a dynamic concept that changes over time, technology, and generations. We saw the appearance of emoticons, three-letter abbreviations, and so on. Foreign languages tend to borrow from each other; describing this dynamic ecosystem is a challenge on itself.</p><p>As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate the language analysis rather than rewriting the tools in Scala. As the topic is so large, I will not claim to cover all aspects of NLP here.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Discussing NLP with the example of text processing pipeline and stages</li><li class="listitem">Learning techniques for simple text analysis in terms of bags</li><li class="listitem">Learning about <span class="strong"><strong>Term Frequency Inverse Document Frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>) technique<a id="id665000000" class="indexterm"/> that goes beyond simple bag analysis and de facto the standard in <a id="id666000000" class="indexterm"/><span class="strong"><strong>Information Retrieval</strong></span> (<span class="strong"><strong>IR</strong></span>)</li><li class="listitem">Learning<a id="id667000000" class="indexterm"/> about document clustering with the example of the <span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>) approach</li><li class="listitem">Performing semantic analysis using word2vec n-gram-based algorithms</li></ul></div><div class="section" title="Text analysis pipeline"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec6400000"/>Text analysis pipeline</h1></div></div></div><p>Before we <a id="id668000000" class="indexterm"/>proceed to detailed algorithms, let's look at a generic text-processing pipeline depicted in <span class="emphasis"><em>Figure 9-1</em></span>. In text analysis, the input is usually presented as a stream of characters (depending on the specific language).</p><p>Lexical analysis has to do with breaking this stream into a sequence of words (or lexemes in linguistic analysis). Often <a id="id669000000" class="indexterm"/>it is also called tokenization (and the words called the tokens). <span class="strong"><strong>ANother Tool for Language Recognition</strong></span> (<span class="strong"><strong>ANTLR</strong></span>) (<a class="ulink" href="http://www.antlr.org/">http://www.antlr.org/</a>) and Flex (<a class="ulink" href="http://flex.sourceforge.net">http://flex.sourceforge.net</a>) are probably the most<a id="id670000000" class="indexterm"/> famous in the open source community. One of the <a id="id671000000" class="indexterm"/>classical examples of ambiguity is lexical ambiguity. For example, in the phrase <span class="emphasis"><em>I saw a bat.</em></span> <span class="emphasis"><em>bat</em></span> can mean either an animal or a <a id="id672000000" class="indexterm"/>baseball bat. We usually need context to figure this out, which we will discuss next:</p><div class="mediaobject"><img src="../Images/image01755.jpeg" alt="Text analysis pipeline"/><div class="caption"><p>Figure 9-1. Typical stages of an NLP process.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Syntactic analysis, or parsing, traditionally deals with matching the structure of the text with grammar rules. This is relatively more important for computer languages that do not allow any ambiguity. In natural languages, this process is usually called chunking and tagging. In many cases, the meaning of the word in human language can be subject to context, intonation, or even body language or facial expression. The value of such analysis, as opposed to the big data approach, where the volume of data trumps complexity is still a contentious topic—one example of the latter is the word2vec approach, which will be described later.</p><p>Semantic<a id="id673000000" class="indexterm"/> analysis is the process of extracting language-independent meaning from the syntactic structures. As much as possible, it also involves removing features specific to particular cultural and linguistic contexts, to the extent that such a project is possible. The sources of ambiguity at this stage are: phrase attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal speech, and so on. Again, word2vec partially deals with these issues.</p><p>Disclosure integration partially deals with the issue of the context: the meaning of a sentence or an idiom can depend on the sentences or paragraphs before that. Syntactic analysis and cultural background play an important role here.</p><p>Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret what is said in terms of what the intention was. How does this change the state of the world? Is it actionable?</p><div class="section" title="Simple text analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec5100000"/>Simple text analysis</h2></div></div></div><p>The straightforward representation of the document is a bag of words. Scala, and Spark, provides an <a id="id674000000" class="indexterm"/>excellent paradigm to perform analysis on the word distributions. First, we read the whole collection of texts, and then count the unique words:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  ''_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; val leotolstoy = sc.textFile("leotolstoy").cache</strong></span>
<span class="strong"><strong>leotolstoy: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[1] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; leotolstoy.flatMap(_.split("\\W+")).count</strong></span>
<span class="strong"><strong>res1: Long = 1318234 </strong></span>

<span class="strong"><strong>scala&gt; val shakespeare = sc.textFile("shakespeare").cache</strong></span>
<span class="strong"><strong>shakespeare: org.apache.spark.rdd.RDD[String] = shakespeare MapPartitionsRDD[7] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; shakespeare.flatMap(_.split("\\W+")).count</strong></span>
<span class="strong"><strong>res2: Long = 1051958</strong></span>
</pre></div><p>This gives <a id="id675000000" class="indexterm"/>us just an estimate of the number of distinct words in the repertoire of quite different authors. The simplest way to find intersection between the two corpuses is to find the common vocabulary (which will be quite different as <span class="emphasis"><em>Leo Tolstoy</em></span> wrote in Russian and French, while <span class="emphasis"><em>Shakespeare</em></span> was an English-writing author):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; :silent</strong></span>

<span class="strong"><strong>scala&gt; val shakespeareBag = shakespeare.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>

<span class="strong"><strong>scala&gt; val leotolstoyBag = leotolstoy.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>
<span class="strong"><strong>leotolstoyBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at map at &lt;console&gt;:29</strong></span>

<span class="strong"><strong>scala&gt; println("The bags intersection is " + leotolstoyBag.intersection(shakespeareBag).count)</strong></span>
<span class="strong"><strong>The bags intersection is 11552</strong></span>
</pre></div><p>A few thousands word indices are manageable with the current implementations. For any new story, we can determine whether it is more likely to be written by Leo Tolstoy or <span class="emphasis"><em>William Shakespeare</em></span>. Let's take a look at <span class="emphasis"><em>The King James Version of the Bible</em></span>, which also can be <a id="id676000000" class="indexterm"/>downloaded from Project Gutenberg (<a class="ulink" href="https://www.gutenberg.org/files/10/10-h/10-h.htm">https://www.gutenberg.org/files/10/10-h/10-h.htm</a>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ (mkdir bible; cd bible; wget http://www.gutenberg.org/cache/epub/10/pg10.txt)</strong></span>

<span class="strong"><strong>scala&gt; val bible = sc.textFile("bible").cache</strong></span>

<span class="strong"><strong>scala&gt; val bibleBag = bible.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>

<span class="strong"><strong>scala&gt;:silent</strong></span>

<span class="strong"><strong>scala&gt; bibleBag.intersection(shakespeareBag).count</strong></span>
<span class="strong"><strong>res5: Long = 7250</strong></span>

<span class="strong"><strong>scala&gt; bibleBag.intersection(leotolstoyBag).count</strong></span>
<span class="strong"><strong>res24: Long = 6611</strong></span>
</pre></div><p>This seems<a id="id677000000" class="indexterm"/> reasonable as the religious language was popular during the Shakespearean time. On the other hand, plays by <span class="emphasis"><em>Anton Chekhov</em></span> have a larger intersection with the <span class="emphasis"><em>Leo Tolstoy</em></span> vocabulary:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ (mkdir chekhov; cd chekhov;</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/7986/pg7986.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/1756/pg1756.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/1754/1754.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/13415/pg13415.txt)</strong></span>

<span class="strong"><strong>scala&gt; val chekhov = sc.textFile("chekhov").cache</strong></span>
<span class="strong"><strong>chekhov: org.apache.spark.rdd.RDD[String] = chekhov MapPartitionsRDD[61] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; val chekhovBag = chekhov.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>
<span class="strong"><strong>chekhovBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at distinct at &lt;console&gt;:29</strong></span>

<span class="strong"><strong>scala&gt; chekhovBag.intersection(leotolstoyBag).count</strong></span>
<span class="strong"><strong>res8: Long = 8263</strong></span>

<span class="strong"><strong>scala&gt; chekhovBag.intersection(shakespeareBag).count</strong></span>
<span class="strong"><strong>res9: Long = 6457 </strong></span>
</pre></div><p>This is a very simple approach that works, but there are a number of commonly known improvements we can make. First, a common technique is to stem the words. In many languages, words have a common part, often called root, and a changeable prefix or suffix, which may depend on the context, gender, time, and so on. Stemming is the process of improving the distinct count and intersection by approximating this flexible word form to the root, base, or a stem form in general. The stem form does not need to be identical to the morphological root of the word, it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid grammatical root. Secondly, we probably should account for the frequency of the words—while we will describe more elaborate methods in the next section, for the purpose of this exercise, we'll exclude the words with very high count, that usually are present in any document such as articles and possessive pronouns, which are usually called stop words, and the words with very low count. Specifically, I'll use the<a id="id678000000" class="indexterm"/> optimized <span class="strong"><strong>Porter Stemmer</strong></span> implementation that I described in more detail at the end of the chapter.</p><div class="note" title="Note"><h3 class="title"><a id="note13000000"/>Note</h3><p>The <a class="ulink" href="http://tartarus.org/martin/PorterStemmer/">http://tartarus.org/martin/PorterStemmer/</a> site contains some of the <a id="id679000000" class="indexterm"/>Porter Stemmer implementations in Scala and other languages, including a highly optimized ANSI C, which may be more efficient, but here I will provide another optimized Scala version that can be used immediately with Spark.</p></div><p>The<a id="id680000000" class="indexterm"/> Stemmer example will stem the words and count the relative intersections between them, removing the stop words:</p><div class="informalexample"><pre class="programlisting">def main(args: Array[String]) {

    val stemmer = new Stemmer

    val conf = new SparkConf().
      setAppName("Stemmer").
      setMaster(args(0))

    val sc = new SparkContext(conf)

    val stopwords = scala.collection.immutable.TreeSet(
      "", "i", "a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "from", "had", "has", "he", "her", "him", "his", "in", "is", "it", "its", "my", "not", "of", "on", "she", "that", "the", "to", "was", "were", "will", "with", "you"
    ) map { stemmer.stem(_) }

    val bags = for (name &lt;- args.slice(1, args.length)) yield {
      val rdd = sc.textFile(name).map(_.toLowerCase)
      if (name == "nytimes" || name == "nips" || name == "enron")
        rdd.filter(!_.startsWith("zzz_")).flatMap(_.split("_")).map(stemmer.stem(_)).distinct.filter(!stopwords.contains(_)).cache
      else {
        val withCounts = rdd.flatMap(_.split("\\W+")).map(stemmer.stem(_)).filter(!stopwords.contains(_)).map((_, 1)).reduceByKey(_+_)
        val minCount = scala.math.max(1L, 0.0001 * withCounts.count.toLong)
        withCounts.filter(_._2 &gt; minCount).map(_._1).cache
      }
    }

    val cntRoots = (0 until { args.length - 1 }).map(i =&gt; Math.sqrt(bags(i).count.toDouble))

    for(l &lt;- 0 until { args.length - 1 }; r &lt;- l until { args.length - 1 }) {
      val cnt = bags(l).intersection(bags(r)).count
      println("The intersect " + args(l+1) + " x " + args(r+1) + " is: " + cnt + " (" + (cnt.toDouble / cntRoots(l) / cntRoots(r)) + ")")
    }

    sc.stop
    }
}</pre></div><p>When<a id="id681000000" class="indexterm"/> one runs the main class example from the command line, it outputs the stemmed bag sizes and intersection for datasets specified as parameters (these are directories in the home filesystem with documents):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sbt "run-main org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible"</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter09/project</strong></span>
<span class="strong"><strong>[info] Set current project to NLP in Scala (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter09/)</strong></span>
<span class="strong"><strong>[info] Running org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible</strong></span>
<span class="strong"><strong>The intersect shakespeare x shakespeare is: 10533 (1.0)</strong></span>
<span class="strong"><strong>The intersect shakespeare x leotolstoy is: 5834 (0.5293670391596142)</strong></span>
<span class="strong"><strong>The intersect shakespeare x chekhov is: 3295 (0.4715281914492153)</strong></span>
<span class="strong"><strong>The intersect shakespeare x nytimes is: 7207 (0.4163369701270161)</strong></span>
<span class="strong"><strong>The intersect shakespeare x nips is: 2726 (0.27457329089479504)</strong></span>
<span class="strong"><strong>The intersect shakespeare x enron is: 5217 (0.34431535832271265)</strong></span>
<span class="strong"><strong>The intersect shakespeare x bible is: 3826 (0.45171392986714726)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x leotolstoy is: 11531 (0.9999999999999999)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x chekhov is: 4099 (0.5606253333241973)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x nytimes is: 8657 (0.47796976891152176)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x nips is: 3231 (0.3110369262979765)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x enron is: 6076 (0.38326210407266764)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x bible is: 3455 (0.3898604013063757)</strong></span>
<span class="strong"><strong>The intersect chekhov x chekhov is: 4636 (1.0)</strong></span>
<span class="strong"><strong>The intersect chekhov x nytimes is: 3843 (0.33463022711780555)</strong></span>
<span class="strong"><strong>The intersect chekhov x nips is: 1889 (0.28679311682962116)</strong></span>
<span class="strong"><strong>The intersect chekhov x enron is: 3213 (0.31963226496874225)</strong></span>
<span class="strong"><strong>The intersect chekhov x bible is: 2282 (0.40610513998395287)</strong></span>
<span class="strong"><strong>The intersect nytimes x nytimes is: 28449 (1.0)</strong></span>
<span class="strong"><strong>The intersect nytimes x nips is: 4954 (0.30362042173997206)</strong></span>
<span class="strong"><strong>The intersect nytimes x enron is: 11273 (0.45270741164576034)</strong></span>
<span class="strong"><strong>The intersect nytimes x bible is: 3655 (0.2625720159205085)</strong></span>
<span class="strong"><strong>The intersect nips x nips is: 9358 (1.0000000000000002)</strong></span>
<span class="strong"><strong>The intersect nips x enron is: 4888 (0.3422561629856124)</strong></span>
<span class="strong"><strong>The intersect nips x bible is: 1615 (0.20229053645165143)</strong></span>
<span class="strong"><strong>The intersect enron x enron is: 21796 (1.0)</strong></span>
<span class="strong"><strong>The intersect enron x bible is: 2895 (0.23760453654690084)</strong></span>
<span class="strong"><strong>The intersect bible x bible is: 6811 (1.0)</strong></span>
<span class="strong"><strong>[success] Total time: 12 s, completed May 17, 2016 11:00:38 PM</strong></span>
</pre></div><p>This, in <a id="id682000000" class="indexterm"/>this case, just confirms the hypothesis that Bible's vocabulary is closer to <span class="emphasis"><em>William Shakespeare</em></span> than to Leo Tolstoy and other sources. Interestingly, modern vocabularies of <span class="emphasis"><em>NY Times</em></span> articles and Enron's e-mails from the previous chapters are much closer to <span class="emphasis"><em>Leo Tolstoy's</em></span>, which is probably more an indication of the translation quality.</p><p>Another thing to notice is that the pretty complex analysis took about <span class="emphasis"><em>40</em></span> lines of Scala code (not counting the libraries, specifically the Porter Stemmer, which is about ~ <span class="emphasis"><em>100</em></span> lines) and about 12 seconds. The power of Scala is that it can leverage other libraries very efficiently to write concise code.</p><div class="note" title="Note"><h3 class="title"><a id="note14000000"/>Note</h3><p>
<span class="strong"><strong>Serialization</strong></span>
</p><p>We already<a id="id683000000" class="indexterm"/> talked about serialization in <a class="link" title="Chapter 6. Working with Unstructured Data" href="part0273.xhtml#aid-84B9I2">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>. As Spark's tasks are executed in different threads and potentially JVMs, Spark does a lot of serialization/deserialization when passing the objects. Potentially, I could use <code class="literal">map { val stemmer = new Stemmer; stemmer.stem(_) }</code> instead of <code class="literal">map { stemmer.stem(_) }</code>, but the latter reuses the object for multiple iterations and seems to be linguistically more appealing. One suggested performance optimization is to use <span class="emphasis"><em>Kryo serializer</em></span>, which is less flexible than the Java serializer, but more performant. However, for integrative purpose, it is much easier to just make every object in the pipeline serializable and use default Java serialization.</p></div><p>As another example, let's compute the distribution of word frequencies, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val bags = for (name &lt;- List("shakespeare", "leotolstoy", "chekhov", "nytimes", "enron", "bible")) yield {</strong></span>
<span class="strong"><strong>     |     sc textFile(name) flatMap { _.split("\\W+") } map { _.toLowerCase } map { stemmer.stem(_) } filter { ! stopwords.contains(_) } cache()</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>bags: List[org.apache.spark.rdd.RDD[String]] = List(MapPartitionsRDD[93] at filter at &lt;console&gt;:36, MapPartitionsRDD[98] at filter at &lt;console&gt;:36, MapPartitionsRDD[103] at filter at &lt;console&gt;:36, MapPartitionsRDD[108] at filter at &lt;console&gt;:36, MapPartitionsRDD[113] at filter at &lt;console&gt;:36, MapPartitionsRDD[118] at filter at &lt;console&gt;:36)</strong></span>

<span class="strong"><strong>scala&gt; bags reduceLeft { (a, b) =&gt; a.union(b) } map { (_, 1) } reduceByKey { _+_ } collect() sortBy(- _._2) map { x =&gt; scala.math.log(x._2) }</strong></span>
<span class="strong"><strong>res18: Array[Double] = Array(10.27759958298627, 10.1152465449837, 10.058652004037477, 10.046635061754612, 9.999615579630348, 9.855399641729074, 9.834405391348684, 9.801233318497372, 9.792667717430884, 9.76347807952779, 9.742496866444002, 9.655474810542554, 9.630365631415676, 9.623244409181346, 9.593355351246755, 9.517604459155686, 9.515837804297965, 9.47231994707559, 9.45930760329985, 9.441531454869693, 9.435561763085358, 9.426257878198653, 9.378985497953893, 9.355997944398545, 9.34862295977619, 9.300820725104558, 9.25569607369698, 9.25320827220336, 9.229162126216771, 9.20391980417326, 9.19917830726999, 9.167224080902555, 9.153875834995056, 9.137877200242468, 9.129889247578555, 9.090430075303626, 9.090091799380007, 9.083075020930307, 9.077722847361343, 9.070273383079064, 9.0542711863262...</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The<a id="id684000000" class="indexterm"/> distribution of relative frequencies on the log-log scale is presented in the following diagram. With the exception of the first few tokens, the dependency of frequency on rank is almost linear:</p><div class="mediaobject"><img src="../Images/image01756.jpeg" alt="Simple text analysis"/><div class="caption"><p>Figure 9-2. A typical distribution of word relative frequencies on log-log scale (Zipf's Law)</p></div></div><p style="clear:both; height: 1em;"> </p></div></div></div>
<div class="section" title="MLlib algorithms in Spark"><div class="titlepage" id="aid-8MF482"><div><div><h1 class="title"><a id="ch09lvl1sec6500000"/>MLlib algorithms in Spark</h1></div></div></div><p>Let's halt at <a id="id685000000" class="indexterm"/>MLlib that complements other NLP libraries written in Scala. MLlib is primarily important because of scalability, and thus supports a few of the data preparation and text processing algorithms, particularly in the area of <a id="id686000000" class="indexterm"/>feature construction (<a class="ulink" href="http://spark.apache.org/docs/latest/ml-features.html">http://spark.apache.org/docs/latest/ml-features.html</a>).</p><div class="section" title="TF-IDF"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec5200000"/>TF-IDF</h2></div></div></div><p>Although <a id="id687000000" class="indexterm"/>the preceding analysis can already give a powerful insight, the piece of information<a id="id688000000" class="indexterm"/> that is missing from the analysis is term frequency information. The term frequencies are relatively more important in information retrieval, where the collection of documents need to be searched and ranked in relation to a few terms. The top documents are usually returned to the user.</p><p>TF-IDF is a standard technique where term frequencies are offset by the frequencies of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses a hash function to identify the terms. This approach avoids the need to compute a global term-to-index map, but can be subject to potential hash collisions, the probability of which is determined by the number of buckets of the hash table. The default feature dimension is <span class="emphasis"><em>2^20=1,048,576</em></span>.</p><p>In the Spark implementation, each document is a line in the dataset. We can convert it into to an RDD of iterables and compute the hashing by the following code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.HashingTF</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.HashingTF</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vector</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vector</strong></span>

<span class="strong"><strong>scala&gt; val hashingTF = new HashingTF</strong></span>
<span class="strong"><strong>hashingTF: org.apache.spark.mllib.feature.HashingTF = org.apache.spark.mllib.feature.HashingTF@61b975f7</strong></span>

<span class="strong"><strong>scala&gt; val documents: RDD[Seq[String]] = sc.textFile("shakepeare").map(_.split("\\W+").toSeq)</strong></span>
<span class="strong"><strong>documents: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[263] at map at &lt;console&gt;:34</strong></span>

<span class="strong"><strong>scala&gt; val tf = hashingTF transform documents</strong></span>
<span class="strong"><strong>tf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[264] at map at HashingTF.scala:76</strong></span>
</pre></div><p>When computing <code class="literal">hashingTF</code>, we only need a single pass over the data, applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies <a id="id689000000" class="indexterm"/>by<a id="id690000000" class="indexterm"/> IDF:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; tf.cache</strong></span>
<span class="strong"><strong>res26: tf.type = MapPartitionsRDD[268] at map at HashingTF.scala:76</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.IDF</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.IDF</strong></span>

<span class="strong"><strong>scala&gt; val idf = new IDF(minDocFreq = 2) fit tf</strong></span>
<span class="strong"><strong>idf: org.apache.spark.mllib.feature.IDFModel = org.apache.spark.mllib.feature.IDFModel@514bda2d</strong></span>

<span class="strong"><strong>scala&gt; val tfidf = idf transform tf</strong></span>
<span class="strong"><strong>tfidf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[272] at mapPartitions at IDF.scala:178</strong></span>

<span class="strong"><strong>scala&gt; tfidf take(10) foreach println</strong></span>
<span class="strong"><strong>(1048576,[3159,3543,84049,582393,787662,838279,928610,961626,1021219,1021273],[3.9626355004005083,4.556357737874695,8.380602528651274,8.157736974683708,11.513471982269106,9.316247404932888,10.666174121881904,11.513471982269106,8.07948477778396,11.002646358503116])</strong></span>
<span class="strong"><strong>(1048576,[267794,1021219],[8.783442874448122,8.07948477778396])</strong></span>
<span class="strong"><strong>(1048576,[0],[0.5688129477150906])</strong></span>
<span class="strong"><strong>(1048576,[3123,3370,3521,3543,96727,101577,114801,116103,497275,504006,508606,843002,962509,980206],[4.207164322003765,2.9674322162952897,4.125144122691999,2.2781788689373474,2.132236195047438,3.2951341639027754,1.9204575904855747,6.318664992090735,11.002646358503116,3.1043838099579815,5.451238364272918,11.002646358503116,8.43769700104158,10.30949917794317])</strong></span>
<span class="strong"><strong>(1048576,[0,3371,3521,3555,27409,89087,104545,107877,552624,735790,910062,943655,962421],[0.5688129477150906,3.442878442319589,4.125144122691999,4.462482535201062,5.023254392629403,5.160262034409286,5.646060083831103,4.712188947797486,11.002646358503116,7.006282204641219,6.216822672821767,11.513471982269106,8.898512204232908])</strong></span>
<span class="strong"><strong>(1048576,[3371,3543,82108,114801,149895,279256,582393,597025,838279,915181],[3.442878442319589,2.2781788689373474,6.017670811187438,3.8409151809711495,7.893585399642122,6.625632265652778,8.157736974683708,10.414859693600997,9.316247404932888,11.513471982269106])</strong></span>
<span class="strong"><strong>(1048576,[3123,3555,413342,504006,690950,702035,980206],[4.207164322003765,4.462482535201062,3.4399651117812313,3.1043838099579815,11.513471982269106,11.002646358503116,10.30949917794317])</strong></span>
<span class="strong"><strong>(1048576,[0],[0.5688129477150906])</strong></span>
<span class="strong"><strong>(1048576,[97,1344,3370,100898,105489,508606,582393,736902,838279,1026302],[2.533299776544098,23.026943964538212,2.9674322162952897,0.0,11.225789909817326,5.451238364272918,8.157736974683708,10.30949917794317,9.316247404932888,11.513471982269106])</strong></span>
<span class="strong"><strong>(1048576,[0,1344,3365,114801,327690,357319,413342,692611,867249,965170],[4.550503581720725,23.026943964538212,2.7455719545259836,1.9204575904855747,8.268278849083533,9.521041817578901,3.4399651117812313,0.0,6.661441718349489,0.0])</strong></span>
</pre></div><p>Here<a id="id691000000" class="indexterm"/> we see<a id="id692000000" class="indexterm"/> each document represented by a set of terms and their scores.</p></div><div class="section" title="LDA"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec5300000"/>LDA</h2></div></div></div><p>LDA in <a id="id693000000" class="indexterm"/>Spark MLlib is a clustering <a id="id694000000" class="indexterm"/>mechanism, where the feature vectors represent the counts of words in a document. The model maximizes the probability of observing the word counts, given the assumption that each document is a mixture of topics<a id="id695000000" class="indexterm"/> and the words in the documents are generated based on <span class="strong"><strong>Dirichlet distribution</strong></span> (a generalization of beta distribution on multinomial case) for each of the topic independently. The goal is to derive the (latent) distribution of the topics and the parameters of the words generation statistical model.</p><p>The MLlib<a id="id696000000" class="indexterm"/> implementation is based on 2009 LDA paper (<a class="ulink" href="http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf">http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf</a>) and uses GraphX to implement a distributed <span class="strong"><strong>Expectation Maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) algorithm for assigning<a id="id697000000" class="indexterm"/> topics to the documents.</p><p>Let's take the Enron e-mail corpus discussed in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>, where we tried to figure out communications graph. For e-mail clustering, we need to extract the body of the e-mail and place is as a single line in the training file:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ mkdir enron</strong></span>
<span class="strong"><strong>$ cat /dev/null &gt; enron/all.txt</strong></span>
<span class="strong"><strong>$ for f in $(find maildir -name \*\. -print); do cat $f | sed '1,/^$/d;/^$/d' | tr "\n\r" "  " &gt;&gt; enron/all.txt; echo "" &gt;&gt; enron/all.txt; done</strong></span>
<span class="strong"><strong>$</strong></span>
</pre></div><p>Now, let's use Scala/Spark to construct a corpus dataset containing the document ID, followed by a dense array of word counts in the bag:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ spark-shell --driver-memory 8g --executor-memory 8g --packages com.github.fommil.netlib:all:1.1.2</strong></span>
<span class="strong"><strong>Ivy Default Cache set to: /home/alex/.ivy2/cache</strong></span>
<span class="strong"><strong>The jars for the packages stored in: /home/alex/.ivy2/jars</strong></span>
<span class="strong"><strong>:: loading settings :: url = jar:file:/opt/cloudera/parcels/CDH-5.5.2-1.cdh5.5.2.p0.4/jars/spark-assembly-1.5.0-cdh5.5.2-hadoop2.6.0-cdh5.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml</strong></span>
<span class="strong"><strong>com.github.fommil.netlib#all added as a dependency</strong></span>
<span class="strong"><strong>:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0</strong></span>
<span class="strong"><strong>  confs: [default]</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#all;1.1.2 in central</strong></span>
<span class="strong"><strong>  found net.sourceforge.f2j#arpack_combined_all;0.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#core;1.1.2 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#native_ref-java;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil#jniloader;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#native_system-java;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-win-i686;1.1 in central</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1-javadoc.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] net.sourceforge.f2j#arpack_combined_all;0.1!arpack_combined_all.jar (513ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (18ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-osx-x86_64/1.1/netlib-native_ref-osx-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1!netlib-native_ref-osx-x86_64.jar (167ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-x86_64/1.1/netlib-native_ref-linux-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1!netlib-native_ref-linux-x86_64.jar (159ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-i686/1.1/netlib-native_ref-linux-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1!netlib-native_ref-linux-i686.jar (131ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-x86_64/1.1/netlib-native_ref-win-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1!netlib-native_ref-win-x86_64.jar (210ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-i686/1.1/netlib-native_ref-win-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-i686;1.1!netlib-native_ref-win-i686.jar (167ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-armhf/1.1/netlib-native_ref-linux-armhf-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1!netlib-native_ref-linux-armhf.jar (110ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-osx-x86_64/1.1/netlib-native_system-osx-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1!netlib-native_system-osx-x86_64.jar (54ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-x86_64/1.1/netlib-native_system-linux-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1!netlib-native_system-linux-x86_64.jar (47ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-i686/1.1/netlib-native_system-linux-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-i686;1.1!netlib-native_system-linux-i686.jar (44ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-armhf/1.1/netlib-native_system-linux-armhf-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>[SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1!netlib-native_system-linux-armhf.jar (35ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-x86_64/1.1/netlib-native_system-win-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1!netlib-native_system-win-x86_64.jar (62ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-i686/1.1/netlib-native_system-win-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-i686;1.1!netlib-native_system-win-i686.jar (55ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_ref-java/1.1/native_ref-java-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#native_ref-java;1.1!native_ref-java.jar (24ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/jniloader/1.1/jniloader-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil#jniloader;1.1!jniloader.jar (3ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_system-java/1.1/native_system-java-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#native_system-java;1.1!native_system-java.jar (7ms)</strong></span>
<span class="strong"><strong>:: resolution report :: resolve 3366ms :: artifacts dl 1821ms</strong></span>
<span class="strong"><strong>  :: modules in use:</strong></span>
<span class="strong"><strong>  com.github.fommil#jniloader;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#all;1.1.2 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#core;1.1.2 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#native_ref-java;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#native_system-java;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-win-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]</strong></span>
<span class="strong"><strong>  :: evicted modules:</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#core;1.1 by [com.github.fommil.netlib#core;1.1.2] in [default]</strong></span>
<span class="strong"><strong>  --------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |                  |            modules            ||   artifacts   |</strong></span>
<span class="strong"><strong>  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |      default     |   19  |   18  |   18  |   1   ||   17  |   17  |</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; val enron = sc textFile("enron")</strong></span>
<span class="strong"><strong>enron: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:21</strong></span>

<span class="strong"><strong>scala&gt; enron.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct.count</strong></span>
<span class="strong"><strong>res0: Long = 529199                                                             </strong></span>

<span class="strong"><strong>scala&gt; val stopwords = scala.collection.immutable.TreeSet ("", "i", "a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "from", "had", "has", "he", "her", "him", "his", "in", "is", "it", "its", "not", "of", "on", "she", "that", "the", "to", "was", "were", "will", "with", "you")</strong></span>
<span class="strong"><strong>stopwords: scala.collection.immutable.TreeSet[String] = TreeSet(, a, an, and, are, as, at, be, but, by, for, from, had, has, he, her, him, his, i, in, is, it, its, not, of, on, she, that, the, to, was, were, will, with, you)</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val terms = enron.flatMap(x =&gt; if (x.length &lt; 8192) x.toLowerCase.split("\\W+") else Nil).filterNot(stopwords).map(_,1).reduceByKey(_+_).collect.sortBy(- _._2).slice(0, 1000).map(_._1)</strong></span>
<span class="strong"><strong>terms: Array[String] = Array(enron, ect, com, this, hou, we, s, have, subject, or, 2001, if, your, pm, am, please, cc, 2000, e, any, me, 00, message, 1, corp, would, can, 10, our, all, sent, 2, mail, 11, re, thanks, original, know, 12, 713, http, may, t, do, 3, time, 01, ees, m, new, my, they, no, up, information, energy, us, gas, so, get, 5, about, there, need, what, call, out, 4, let, power, should, na, which, one, 02, also, been, www, other, 30, email, more, john, like, these, 03, mark, 04, attached, d, enron_development, their, see, 05, j, forwarded, market, some, agreement, 09, day, questions, meeting, 08, when, houston, doc, contact, company, 6, just, jeff, only, who, 8, fax, how, deal, could, 20, business, use, them, date, price, 06, week, here, net, 15, 9, 07, group, california,...</strong></span>
<span class="strong"><strong>scala&gt; def getBagCounts(bag: Seq[String]) = { for(term &lt;- terms) yield { bag.count(_==term) } }</strong></span>
<span class="strong"><strong>getBagCounts: (bag: Seq[String])Array[Int]</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>

<span class="strong"><strong>scala&gt; val corpus = enron.map(x =&gt; { if (x.length &lt; 8192) Some(x.toLowerCase.split("\\W+").toSeq) else None } ).map(x =&gt; { Vectors.dense(getBagCounts(x.getOrElse(Nil)).map(_.toDouble).toArray) }).zipWithIndex.map(_.swap).cache</strong></span>
<span class="strong"><strong>corpus: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[14] at map at &lt;console&gt;:30</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>

<span class="strong"><strong>scala&gt; val ldaModel = new LDA().setK(10).run(corpus)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; ldaModel.topicsMatrix.transpose</strong></span>
<span class="strong"><strong>res2: org.apache.spark.mllib.linalg.Matrix = </strong></span>
<span class="strong"><strong>207683.78495933366  79745.88417942637   92118.63972404732   ... (1000 total)</strong></span>
<span class="strong"><strong>35853.48027575886   4725.178508682296   111214.8860582083   ...</strong></span>
<span class="strong"><strong>135755.75666585402  54736.471356209106  93289.65563593085   ...</strong></span>
<span class="strong"><strong>39445.796099155996  6272.534431534215   34764.02707696523   ...</strong></span>
<span class="strong"><strong>329786.21570967307  602782.9591026317   42212.22143362559   ...</strong></span>
<span class="strong"><strong>62235.09960154089   12191.826543794878  59343.24100019015   ...</strong></span>
<span class="strong"><strong>210049.59592560542  160538.9650732507   40034.69756641789   ...</strong></span>
<span class="strong"><strong>53818.14660186875   6351.853448001488   125354.26708575874  ...</strong></span>
<span class="strong"><strong>44133.150537842856  4342.697652158682   154382.95646078113  ...</strong></span>
<span class="strong"><strong>90072.97362336674   21132.629704311104  93683.40795807641   ...</strong></span>
</pre></div><p>We can<a id="id698000000" class="indexterm"/> also list the words and their<a id="id699000000" class="indexterm"/> relative importance for the topic in the descending order:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; ldaModel.describeTopics foreach { x : (Array[Int], Array[Double]) =&gt; { print(x._1.slice(0,10).map(terms(_)).mkString(":")); print("-&gt; "); print(x._2.slice(0,10).map(_.toFloat).mkString(":")); println } }</strong></span>
<span class="strong"><strong>com:this:ect:or:if:s:hou:2001:00:we-&gt;0.054606363:0.024220783:0.02096761:0.013669214:0.0132700335:0.012969772:0.012623918:0.011363528:0.010114557:0.009587474</strong></span>
<span class="strong"><strong>s:this:hou:your:2001:or:please:am:com:new-&gt;0.029883621:0.027119286:0.013396418:0.012856948:0.01218803:0.01124849:0.010425644:0.009812181:0.008742722:0.0070441025</strong></span>
<span class="strong"><strong>com:this:s:ect:hou:or:2001:if:your:am-&gt;0.035424445:0.024343235:0.015182628:0.014283071:0.013619815:0.012251413:0.012221165:0.011411696:0.010284024:0.009559739</strong></span>
<span class="strong"><strong>would:pm:cc:3:thanks:e:my:all:there:11-&gt;0.047611523:0.034175437:0.022914853:0.019933242:0.017208714:0.015393614:0.015366959:0.01393391:0.012577525:0.011743208</strong></span>
<span class="strong"><strong>ect:com:we:can:they:03:if:also:00:this-&gt;0.13815293:0.0755843:0.065043546:0.015290086:0.0121941045:0.011561104:0.011326733:0.010967959:0.010653805:0.009674695</strong></span>
<span class="strong"><strong>com:this:s:hou:or:2001:pm:your:if:cc-&gt;0.016605735:0.015834121:0.01289918:0.012708308:0.0125788655:0.011726159:0.011477625:0.010578845:0.010555539:0.009609056</strong></span>
<span class="strong"><strong>com:ect:we:if:they:hou:s:00:2001:or-&gt;0.05537054:0.04231919:0.023271963:0.012856676:0.012689817:0.012186356:0.011350313:0.010887237:0.010778923:0.010662295</strong></span>
<span class="strong"><strong>this:s:hou:com:your:2001:or:please:am:if-&gt;0.030830953:0.016557815:0.014236835:0.013236604:0.013107091:0.0126846135:0.012257128:0.010862533:0.01027849:0.008893094</strong></span>
<span class="strong"><strong>this:s:or:pm:com:your:please:new:hou:2001-&gt;0.03981197:0.013273305:0.012872894:0.011672661:0.011380969:0.010689667:0.009650983:0.009605533:0.009535899:0.009165275</strong></span>
<span class="strong"><strong>this:com:hou:s:or:2001:if:your:am:please-&gt;0.024562683:0.02361607:0.013770585:0.013601272:0.01269994:0.012360005:0.011348433:0.010228578:0.009619628:0.009347991</strong></span>
</pre></div><p>To find out the top documents per topic or top topics per document, we need to convert this model to <code class="literal">DistributedLDA</code> or <code class="literal">LocalLDAModel</code>, which extend <code class="literal">LDAModel</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; ldaModel.save(sc, "ldamodel")</strong></span>

<span class="strong"><strong>scala&gt; val sameModel = DistributedLDAModel.load(sc, "ldamode2l")</strong></span>

<span class="strong"><strong>scala&gt; sameModel.topDocumentsPerTopic(10) foreach { x : (Array[Long], Array[Double]) =&gt; { print(x._1.mkString(":")); print("-&gt; "); print(x._2.map(_.toFloat).mkString(":")); println } }</strong></span>
<span class="strong"><strong>59784:50745:52479:60441:58399:49202:64836:52490:67936:67938-&gt; 0.97146696:0.9713364:0.9661418:0.9661132:0.95249915:0.9519995:0.94945914:0.94944507:0.8977366:0.8791358</strong></span>
<span class="strong"><strong>233009:233844:233007:235307:233842:235306:235302:235293:233020:233857-&gt; 0.9962034:0.9962034:0.9962034:0.9962034:0.9962034:0.99620336:0.9954057:0.9954057:0.9954057:0.9954057</strong></span>
<span class="strong"><strong>14909:115602:14776:39025:115522:288507:4499:38955:15754:200876-&gt; 0.83963907:0.83415157:0.8319566:0.8303818:0.8291597:0.8281472:0.82739806:0.8272517:0.82579833:0.8243338</strong></span>
<span class="strong"><strong>237004:71818:124587:278308:278764:278950:233672:234490:126637:123664-&gt; 0.99929106:0.9968135:0.9964454:0.99644524:0.996445:0.99644494:0.99644476:0.9964447:0.99644464:0.99644417</strong></span>
<span class="strong"><strong>156466:82237:82252:82242:341376:82501:341367:340197:82212:82243-&gt; 0.99716955:0.94635135:0.9431836:0.94241136:0.9421047:0.9410431:0.94075173:0.9406304:0.9402021:0.94014835</strong></span>
<span class="strong"><strong>335708:336413:334075:419613:417327:418484:334157:335795:337573:334160-&gt; 0.987011:0.98687994:0.9865438:0.96953565:0.96953565:0.96953565:0.9588571:0.95852506:0.95832515:0.9581657</strong></span>
<span class="strong"><strong>243971:244119:228538:226696:224833:207609:144009:209548:143066:195299-&gt; 0.7546907:0.7546907:0.59146744:0.59095955:0.59090924:0.45532238:0.45064417:0.44945204:0.4487876:0.44833568</strong></span>
<span class="strong"><strong>242260:214359:126325:234126:123362:233304:235006:124195:107996:334829-&gt; 0.89615464:0.8961442:0.8106028:0.8106027:0.8106023:0.8106023:0.8106021:0.8106019:0.76834095:0.7570231</strong></span>
<span class="strong"><strong>209751:195546:201477:191758:211002:202325:197542:193691:199705:329052-&gt; 0.913124:0.9130985:0.9130918:0.9130672:0.5525752:0.5524637:0.5524494:0.552405:0.55240136:0.5026157</strong></span>
<span class="strong"><strong>153326:407544:407682:408098:157881:351230:343651:127848:98884:129351-&gt; 0.97206575:0.97206575:0.97206575:0.97206575:0.97206575:0.9689198:0.968068:0.9659192:0.9657442:0.96553063</strong></span>
</pre></div></div></div>
<div class="section" title="Segmentation, annotation, and chunking" id="aid-8NDKQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec6600000"/>Segmentation, annotation, and chunking</h1></div></div></div><p>When the<a id="id700000000" class="indexterm"/> text is presented in digital form, it is relatively easy to find words as<a id="id701000000" class="indexterm"/> we can split the stream on non-word characters. This <a id="id702000000" class="indexterm"/>becomes more complex in spoken language analysis. In this case, segmenters try to optimize a metric, for example, to minimize the number of distinct words in the lexicon and the length or complexity of the phrase (<span class="emphasis"><em>Natural Language Processing with Python</em></span> by <span class="emphasis"><em>Steven Bird et al</em></span>, <span class="emphasis"><em>O'Reilly Media Inc</em></span>, 2009).</p><p>Annotation usually refers to parts-of-speech tagging. In English, these are nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions, and interjections. For example, in the phrase <span class="emphasis"><em>we saw the yellow dog</em></span>, <span class="emphasis"><em>we</em></span> is a pronoun, <span class="emphasis"><em>saw</em></span> is a verb, <span class="emphasis"><em>the</em></span> is an article, <span class="emphasis"><em>yellow</em></span> is an adjective, and <span class="emphasis"><em>dog</em></span> is a noun.</p><p>In some languages, the chunking and annotation depends on context. For example, in Chinese, <span class="emphasis"><em>????</em></span> literally translates to <span class="emphasis"><em>love country person</em></span> and can mean either <span class="emphasis"><em>country-loving person</em></span> or <span class="emphasis"><em>love country-person</em></span>. In Russian, <span class="emphasis"><em>??????? ?????? ??????????</em></span>, literally translating to <span class="emphasis"><em>execute not pardon</em></span>, can mean <span class="emphasis"><em>execute, don't pardon</em></span>, or <span class="emphasis"><em>don't execute, pardon</em></span>. While in written language, this can be disambiguated using commas, in a spoken language this is usually it is very hard to recognize the difference, even though sometimes the intonation can help to segment the phrase properly.</p><p>For techniques based on word frequencies in the bags, some extremely common words, which are of little value in helping select documents, are explicitly excluded from the vocabulary. These words are called stop words. There is no good general strategy for determining a stop list, but in many cases, this is to exclude very frequent words that appear in almost every document and do not help to differentiate between them for classification or information retrieval purposes.</p></div>
<div class="section" title="POS tagging"><div class="titlepage" id="aid-8OC5C2"><div><div><h1 class="title"><a id="ch09lvl1sec6700000"/>POS tagging</h1></div></div></div><p>POS tagging probabilistically annotates each word with it's grammatical function—noun, verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic and <a id="id703000000" class="indexterm"/>semantic analysis. Let's demonstrate POS tagging<a id="id704000000" class="indexterm"/> on the FACTORIE toolkit example, a software library written in Scala (<a class="ulink" href="http://factorie.cs.umass.edu">http://factorie.cs.umass.edu</a>). To start, you need to <a id="id705000000" class="indexterm"/>download the binary image or source files from <a class="ulink" href="https://github.com/factorie/factorie.git">https://github.com/factorie/factorie.git</a> and build it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ git clone https://github.com/factorie/factorie.git</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ cd factorie</strong></span>
<span class="strong"><strong>$ git checkout factorie_2.11-1.2</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ mvn package -Pnlp-jar-with-dependencies</strong></span>
</pre></div><p>After the build, which also includes model training, the following command will start a network server on <code class="literal">port 3228</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ $ bin/fac nlp --wsj-forward-pos --conll-chain-ner</strong></span>
<span class="strong"><strong>java -Xmx6g -ea -Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -classpath ./src/main/resources:./target/classes:./target/factorie_2.11-1.2-nlp-jar-with-dependencies.jar</strong></span>
<span class="strong"><strong>found model</strong></span>
<span class="strong"><strong>18232</strong></span>
<span class="strong"><strong>Listening on port 3228</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now, all<a id="id706000000" class="indexterm"/> traffic to <code class="literal">port 3228</code> will be interpreted (as text), and the output will be tokenized and annotated:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ telnet localhost 3228</strong></span>
<span class="strong"><strong>Trying ::1...</strong></span>
<span class="strong"><strong>Connected to localhost.</strong></span>
<span class="strong"><strong>Escape character is '^]'.</strong></span>
<span class="strong"><strong>But I warn you, if you don't tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist--I really believe he is Antichrist--I will have nothing more to do with you and you are no longer my friend, no longer my 'faithful slave,' as you call yourself! But how do you do? I see I have frightened you--sit down and tell me all the news.</strong></span>

<span class="strong"><strong>1  1  But  CC  O</strong></span>
<span class="strong"><strong>2  2  I    PRP  O</strong></span>
<span class="strong"><strong>3  3  warn    VBP  O</strong></span>
<span class="strong"><strong>4  4  you    PRP  O</strong></span>
<span class="strong"><strong>5  5  ,      O</strong></span>
<span class="strong"><strong>6  6  if    IN  O</strong></span>
<span class="strong"><strong>7  7  you    PRP  O</strong></span>
<span class="strong"><strong>8  8  do    VBP  O</strong></span>
<span class="strong"><strong>9  9  n't    RB  O</strong></span>
<span class="strong"><strong>10  10  tell    VB  O</strong></span>
<span class="strong"><strong>11  11  me    PRP  O</strong></span>
<span class="strong"><strong>12  12  that    IN  O</strong></span>
<span class="strong"><strong>13  13  this    DT  O</strong></span>
<span class="strong"><strong>14  14  means    VBZ  O</strong></span>
<span class="strong"><strong>15  15  war    NN  O</strong></span>
<span class="strong"><strong>16  16  ,    ,  O</strong></span>
<span class="strong"><strong>17  17  if    IN  O</strong></span>
<span class="strong"><strong>18  18  you  PRP  O</strong></span>
<span class="strong"><strong>19  19  still    RB  O</strong></span>
<span class="strong"><strong>20  20  try    VBP  O</strong></span>
<span class="strong"><strong>21  21  to    TO  O</strong></span>
<span class="strong"><strong>22  22  defend    VB  O</strong></span>
<span class="strong"><strong>23  23  the    DT  O</strong></span>
<span class="strong"><strong>24  24  infamies    NNS  O</strong></span>
<span class="strong"><strong>25  25  and    CC  O</strong></span>
<span class="strong"><strong>26  26  horrors    NNS  O</strong></span>
<span class="strong"><strong>27  27  perpetrated    VBN  O</strong></span>
<span class="strong"><strong>28  28  by    IN  O</strong></span>
<span class="strong"><strong>29  29  that    DT  O</strong></span>
<span class="strong"><strong>30  30  Antichrist    NNP  O</strong></span>
<span class="strong"><strong>31  31  --    :  O</strong></span>
<span class="strong"><strong>32  1  I  PRP  O</strong></span>
<span class="strong"><strong>33  2  really    RB  O</strong></span>
<span class="strong"><strong>34  3  believe    VBP  O</strong></span>
<span class="strong"><strong>35  4  he    PRP  O</strong></span>
<span class="strong"><strong>36  5  is    VBZ  O</strong></span>
<span class="strong"><strong>37  6  Antichrist    NNP  U-MISC</strong></span>
<span class="strong"><strong>38  7  --    :  O</strong></span>
<span class="strong"><strong>39  1  I    PRP  O</strong></span>
<span class="strong"><strong>40  2  will    MD  O</strong></span>
<span class="strong"><strong>41  3  have    VB  O</strong></span>
<span class="strong"><strong>42  4  nothing    NN  O</strong></span>
<span class="strong"><strong>43  5  more    JJR  O</strong></span>
<span class="strong"><strong>44  6  to    TO  O</strong></span>
<span class="strong"><strong>45  7  do    VB  O</strong></span>
<span class="strong"><strong>46  8  with    IN  O</strong></span>
<span class="strong"><strong>47  9  you    PRP  O</strong></span>
<span class="strong"><strong>48  10  and    CC  O</strong></span>
<span class="strong"><strong>49  11  you    PRP  O</strong></span>
<span class="strong"><strong>50  12  are    VBP  O</strong></span>
<span class="strong"><strong>51  13  no    RB  O</strong></span>
<span class="strong"><strong>52  14  longer    RBR  O</strong></span>
<span class="strong"><strong>53  15  my    PRP$  O</strong></span>
<span class="strong"><strong>54  16  friend    NN  O</strong></span>
<span class="strong"><strong>55  17  ,    ,  O</strong></span>
<span class="strong"><strong>56  18  no    RB  O</strong></span>
<span class="strong"><strong>57  19  longer    RB  O</strong></span>
<span class="strong"><strong>58  20  my  PRP$  O</strong></span>
<span class="strong"><strong>59  21  '    POS  O</strong></span>
<span class="strong"><strong>60  22  faithful    NN  O</strong></span>
<span class="strong"><strong>61  23  slave    NN  O</strong></span>
<span class="strong"><strong>62  24  ,    ,  O</strong></span>
<span class="strong"><strong>63  25  '    ''  O</strong></span>
<span class="strong"><strong>64  26  as    IN  O</strong></span>
<span class="strong"><strong>65  27  you    PRP  O</strong></span>
<span class="strong"><strong>66  28  call    VBP  O</strong></span>
<span class="strong"><strong>67  29  yourself    PRP  O</strong></span>
<span class="strong"><strong>68  30  !    .  O</strong></span>
<span class="strong"><strong>69  1  But    CC  O</strong></span>
<span class="strong"><strong>70  2  how    WRB  O</strong></span>
<span class="strong"><strong>71  3  do    VBP  O</strong></span>
<span class="strong"><strong>72  4  you    PRP  O</strong></span>
<span class="strong"><strong>73  5  do    VB  O</strong></span>
<span class="strong"><strong>74  6  ?    .  O</strong></span>
<span class="strong"><strong>75  1  I    PRP  O</strong></span>
<span class="strong"><strong>76  2  see    VBP  O</strong></span>
<span class="strong"><strong>77  3  I    PRP  O</strong></span>
<span class="strong"><strong>78  4  have    VBP  O</strong></span>
<span class="strong"><strong>79  5  frightened    VBN  O</strong></span>
<span class="strong"><strong>80  6  you    PRP  O</strong></span>
<span class="strong"><strong>81  7  --    :  O</strong></span>
<span class="strong"><strong>82  8  sit    VB  O</strong></span>
<span class="strong"><strong>83  9  down    RB  O</strong></span>
<span class="strong"><strong>84  10  and    CC  O</strong></span>
<span class="strong"><strong>85  11  tell    VB  O</strong></span>
<span class="strong"><strong>86  12  me    PRP  O</strong></span>
<span class="strong"><strong>87  13  all    DT  O</strong></span>
<span class="strong"><strong>88  14  the    DT  O</strong></span>
<span class="strong"><strong>89  15  news    NN  O</strong></span>
<span class="strong"><strong>90  16  .    .  O</strong></span>
</pre></div><p>This POS is<a id="id707000000" class="indexterm"/> a single-path left-right tagger that can process the text as a stream. Internally, the algorithm uses probabilistic techniques to find the most probable assignment. Let's also look at other techniques that do not use grammatical analysis and yet proved to be very useful for language understanding and interpretation.</p></div>
<div class="section" title="Using word2vec to find word relationships"><div class="titlepage" id="aid-8PALU2"><div><div><h1 class="title"><a id="ch09lvl1sec6800000"/>Using word2vec to find word relationships</h1></div></div></div><p>Word2vec has <a id="id708000000" class="indexterm"/>been developed by Tomas Mikolov at Google, around 2012. The original idea behind word2vec was to demonstrate that one might improve efficiency by trading the model's complexity for efficiency. Instead of representing a document as bags of words, word2vec takes each word context into account by trying to analyze n-grams or skip-grams (a set of surrounding tokens with potential the token in question skipped). The words and word contexts themselves are represented by an array of floats/doubles <span class="inlinemediaobject"><img src="../Images/image01757.jpeg" alt="Using word2vec to find word relationships"/></span>. The objective function is to maximize log likelihood:</p><div class="mediaobject"><img src="../Images/image01758.jpeg" alt="Using word2vec to find word relationships"/></div><p style="clear:both; height: 1em;"> </p><p>Where:</p><div class="mediaobject"><img src="../Images/image01759.jpeg" alt="Using word2vec to find word relationships"/></div><p style="clear:both; height: 1em;"> </p><p>By choosing the optimal <span class="inlinemediaobject"><img src="../Images/image01757.jpeg" alt="Using word2vec to find word relationships"/></span> and <a id="id709000000" class="indexterm"/>to get a comprehensive word representation (also called <span class="strong"><strong>map optimization</strong></span>). Similar words are found based on cosine similarity<a id="id710000000" class="indexterm"/> metric (dot product) of <span class="inlinemediaobject"><img src="../Images/image01757.jpeg" alt="Using word2vec to find word relationships"/></span>. Spark implementation uses hierarchical softmax, which reduces the complexity of computing the conditional probability to <span class="inlinemediaobject"><img src="../Images/image01760.jpeg" alt="Using word2vec to find word relationships"/></span>, or log of the vocabulary size <span class="emphasis"><em>V</em></span>, as opposed to <span class="inlinemediaobject"><img src="../Images/image01761.jpeg" alt="Using word2vec to find word relationships"/></span>, or proportional to <span class="emphasis"><em>V</em></span>. The training is still linear in the dataset size, but is amenable to big data parallelization techniques.</p><p>
<code class="literal">Word2vec</code> is traditionally used to predict the most likely word given context or find similar words with a similar meaning (synonyms). The following code trains in <code class="literal">word2vec</code> model on <span class="emphasis"><em>Leo Tolstoy's Wars and Peace</em></span>, and finds synonyms for the word <span class="emphasis"><em>circle</em></span>. I had to convert the Gutenberg's representation of <span class="emphasis"><em>War and Peace</em></span> to a single-line format by running the <code class="literal">cat 2600.txt | tr "\n\r" "  " &gt; warandpeace.txt</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val word2vec = new Word2Vec</strong></span>
<span class="strong"><strong>word2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@58bb4dd</strong></span>

<span class="strong"><strong>scala&gt; val model = word2vec.fit(sc.textFile("warandpeace").map(_.split("\\W+").toSeq)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@6f61b9d7</strong></span>

<span class="strong"><strong>scala&gt; val synonyms = model.findSynonyms("life", 10)</strong></span>
<span class="strong"><strong>synonyms: Array[(String, Double)] = Array((freedom,1.704344822168997), (universal,1.682276637692245), (conception,1.6776193389148586), (relation,1.6760497906519414), (humanity,1.67601036253831), (consists,1.6637604144872544), (recognition,1.6526169382380496), (subjection,1.6496559771230317), (activity,1.646671198014248), (astronomy,1.6444424059160712))</strong></span>

<span class="strong"><strong>scala&gt; synonyms foreach println</strong></span>
<span class="strong"><strong>(freedom,1.704344822168997)</strong></span>
<span class="strong"><strong>(universal,1.682276637692245)</strong></span>
<span class="strong"><strong>(conception,1.6776193389148586)</strong></span>
<span class="strong"><strong>(relation,1.6760497906519414)</strong></span>
<span class="strong"><strong>(humanity,1.67601036253831)</strong></span>
<span class="strong"><strong>(consists,1.6637604144872544)</strong></span>
<span class="strong"><strong>(recognition,1.6526169382380496)</strong></span>
<span class="strong"><strong>(subjection,1.6496559771230317)</strong></span>
<span class="strong"><strong>(activity,1.646671198014248)</strong></span>
<span class="strong"><strong>(astronomy,1.6444424059160712)</strong></span>
</pre></div><p>While in <a id="id711000000" class="indexterm"/>general, it is hard to some with an objective function, and <code class="literal">freedom</code> is not listed as a synonym to <code class="literal">life</code> in the English Thesaurus, the results do make sense.</p><p>Each word in the word2vec model is represented as an array of doubles. Another interesting application is to find associations <span class="emphasis"><em>a to b is the same as c to ?</em></span> by performing subtraction <span class="emphasis"><em>vector(a) - vector(b) + vector(c)</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val a = model.getVectors.filter(_._1 == "monarchs").map(_._2).head</strong></span>
<span class="strong"><strong>a: Array[Float] = Array(-0.0044642715, -0.0013227836, -0.011506443, 0.03691717, 0.020431392, 0.013427449, -0.0036369907, -0.013460356, -3.8938568E-4, 0.02432113, 0.014533845, 0.004130258, 0.00671316, -0.009344602, 0.006229065, -0.005442078, -0.0045390734, -0.0038824948, -6.5973646E-4, 0.021729799, -0.011289608, -0.0030690092, -0.011423801, 0.009100784, 0.011765533, 0.0069619063, 0.017540144, 0.011198071, 0.026103685, -0.017285397, 0.0045515243, -0.0044477824, -0.0074411617, -0.023975836, 0.011371289, -0.022625357, -2.6478301E-5, -0.010510282, 0.010622139, -0.009597833, 0.014937023, -0.01298345, 0.0016747514, 0.01172987, -0.001512275, 0.022340108, -0.009758578, -0.014942565, 0.0040697413, 0.0015349758, 0.010246878, 0.0021413323, 0.008739062, 0.007845526, 0.006857361, 0.01160148, 0.008595...</strong></span>
<span class="strong"><strong>scala&gt; val b = model.getVectors.filter(_._1 == "princess").map(_._2).head</strong></span>
<span class="strong"><strong>b: Array[Float] = Array(0.13265875, -0.04882792, -0.08409957, -0.04067986, 0.009084379, 0.121674284, -0.11963971, 0.06699862, -0.20277102, 0.26296946, -0.058114383, 0.076021515, 0.06751665, -0.17419271, -0.089830205, 0.2463593, 0.062816426, -0.10538805, 0.062085453, -0.2483566, 0.03468293, 0.20642486, 0.3129267, -0.12418643, -0.12557726, 0.06725172, -0.03703333, -0.10810595, 0.06692443, -0.046484336, 0.2433963, -0.12762263, -0.18473054, -0.084376186, 0.0037174677, -0.0040220995, -0.3419341, -0.25928706, -0.054454487, 0.09521076, -0.041567303, -0.13727514, -0.04826158, 0.13326299, 0.16228828, 0.08495835, -0.18073058, -0.018380836, -0.15691829, 0.056539804, 0.13673553, -0.027935665, 0.081865616, 0.07029694, -0.041142456, 0.041359138, -0.2304657, -0.17088272, -0.14424285, -0.0030700471, -0...</strong></span>
<span class="strong"><strong>scala&gt; val c = model.getVectors.filter(_._1 == "individual").map(_._2).head</strong></span>
<span class="strong"><strong>c: Array[Float] = Array(-0.0013353615, -0.01820516, 0.007949033, 0.05430816, -0.029520465, -0.030641818, -6.607431E-4, 0.026548808, 0.04784935, -0.006470232, 0.041406438, 0.06599842, 0.0074243015, 0.041538745, 0.0030222891, -0.003932073, -0.03154199, -0.028486902, 0.022139633, 0.05738223, -0.03890591, -0.06761177, 0.0055152955, -0.02480924, -0.053222697, -0.028698998, -0.005315235, 0.0582403, -0.0024816995, 0.031634405, -0.027884213, 6.0290704E-4, 1.9750209E-4, -0.05563172, 0.023785716, -0.037577976, 0.04134448, 0.0026664822, -0.019832063, -0.0011898747, 0.03160933, 0.031184288, 0.0025268437, -0.02718441, -0.07729341, -0.009460656, 0.005344515, -0.05110715, 0.018468754, 0.008984449, -0.0053139487, 0.0053904117, -0.01322933, -0.015247412, 0.009819351, 0.038043085, 0.044905875, 0.00402788...</strong></span>
<span class="strong"><strong>scala&gt; model.findSynonyms(new DenseVector((for(i &lt;- 0 until 100) yield (a(i) - b(i) + c(i)).toDouble).toArray), 10) foreach println</strong></span>
<span class="strong"><strong>(achievement,0.9432423663884002)</strong></span>
<span class="strong"><strong>(uncertainty,0.9187759184842362)</strong></span>
<span class="strong"><strong>(leader,0.9163721499105207)</strong></span>
<span class="strong"><strong>(individual,0.9048367510621271)</strong></span>
<span class="strong"><strong>(instead,0.8992079672038455)</strong></span>
<span class="strong"><strong>(cannon,0.8947818781378154)</strong></span>
<span class="strong"><strong>(arguments,0.8883634101905679)</strong></span>
<span class="strong"><strong>(aims,0.8725107984356915)</strong></span>
<span class="strong"><strong>(ants,0.8593842583047755)</strong></span>
<span class="strong"><strong>(War,0.8530727227924755)</strong></span>
</pre></div><p>This<a id="id712000000" class="indexterm"/> can be used to find relationships in the language.</p><div class="section" title="A Porter Stemmer implementation of the code"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec5400000"/>A Porter Stemmer implementation of the code</h2></div></div></div><p>Porter Stemmer <a id="id713000000" class="indexterm"/>was first developed around the 1980s and there<a id="id714000000" class="indexterm"/> are many implementations. The detailed steps and original reference are provided<a id="id715000000" class="indexterm"/> at <a class="ulink" href="http://tartarus.org/martin/PorterStemmer/def.txt">http://tartarus.org/martin/PorterStemmer/def.txt</a>. It consists of roughly 6-9 steps of suffix/endings replacements, some of which are conditional on prefix or stem. I will provide a Scala-optimized version with the book code repository. For example, step 1 covers the majority of stemming cases and consists of 12 substitutions: the last 8 of which are <a id="id716000000" class="indexterm"/>conditional on the number of syllables<a id="id717000000" class="indexterm"/> and the presence of vowels in the stem:</p><div class="informalexample"><pre class="programlisting">  def step1(s: String) = {
    b = s
    // step 1a
    processSubList(List(("sses", "ss"), ("ies","i"),("ss","ss"), ("s", "")), _&gt;=0)
    // step 1b
    if (!(replacer("eed", "ee", _&gt;0)))
    {
      if ((vowelInStem("ed") &amp;&amp; replacer("ed", "", _&gt;=0)) || (vowelInStem("ing") &amp;&amp; replacer("ing", "", _&gt;=0)))
      {
        if (!processSubList(List(("at", "ate"), ("bl","ble"), ("iz","ize")), _&gt;=0 ) )
        {
          // if this isn't done, then it gets more confusing.
          if (doublec() &amp;&amp; b.last != 'l' &amp;&amp; b.last != 's' &amp;&amp; b.last != 'z') { b = b.substring(0, b.length - 1) }
          else
            if (calcM(b.length) == 1 &amp;&amp; cvc("")) { b = b + "e" }
        }
      }
    }
    // step 1c
    (vowelInStem("y") &amp;&amp; replacer("y", "i", _&gt;=0))
    this
  }</pre></div><p>The complete<a id="id718000000" class="indexterm"/> code is available at <a class="ulink" href="https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala">https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala</a>.</p></div></div>
<div class="section" title="Summary" id="aid-8Q96G1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec6900000"/>Summary</h1></div></div></div><p>In this chapter, I described basic NLP concepts and demonstrated a few basic techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed and tested in a few lines of Scala code. This is definitely just the tip of the iceberg as a lot of NLP techniques are being developed now, including the ones based on in-CPU parallelization as part of GPUs. (refer to, for example, <span class="strong"><strong>Puck</strong></span> at <a class="ulink" href="https://github.com/dlwh/puck">https://github.com/dlwh/puck</a>). I also gave a flavor of major Spark MLlib NLP implementations.</p><p>In the next chapter, which will be the final chapter of this book, I'll cover systems and model monitoring.</p></div>
<div class="chapter" title="Chapter&#xA0;10.&#xA0;Advanced Model Monitoring" id="aid-8R7N21"><div class="titlepage"><div><div><h1 class="title"><a id="ch36"/>Chapter 10. Advanced Model Monitoring</h1></div></div></div><p>Even though this is the last chapter of the book, it can hardly be an afterthought even though monitoring in general often is in practical situations, quite unfortunately. Monitoring is a vital deployment component for any long execution cycle component and thus is part of the finished product. Monitoring can significantly enhance product experience and define future success as it improves problem diagnostic and is essential to determine the improvement path.</p><p>One of the primary rules of successful software engineering is to create systems as if they were targeted for personal use when possible, which fully applies to monitoring, diagnostic, and debugging—quite hapless name for fixing existing issues in software products. Diagnostic and debugging of complex systems, particularly distributed systems, is hard, as the events often can be arbitrary interleaved and program executions subject to race conditions. While there is a lot of research going in the area of distributed system devops and maintainability, this chapter will scratch the service and provide guiding principle to design a maintainable complex distributed system.</p><p>To start with, a pure functional approach, which Scala claims to follow, spends a lot of time avoiding side effects. While this idea is useful in a number of aspects, it is hard to imagine a useful program that has no effect on the outside world, the whole idea of a data-driven application is to have a positive effect on the way the business is conducted, a well-defined side effect.</p><p>Monitoring clearly falls in the side effect category. Execution needs to leave a trace that the user can later parse in order to understand where the design or implementation went awry. The trace of the execution can be left by either writing something on a console or into a file, usually called a log, or returning an object that contains the trace of the program execution, and the intermediate results. The latter approach, which is actually more in line with functional programming and monadic philosophy, is actually more appropriate for the distributed programming but often overlooked. This would have been an interesting topic for research, but unfortunately the space is limited and I have to discuss the practical aspects of monitoring in contemporary systems that is almost always done by logging. Having the monadic approach of carrying an object with the execution trace on each call can certainly increase the overhead of the interprocess or inter-machine communication, but saves a lot of time in stitching different pieces of information together.</p><p>Let's list the naive approaches to debugging that everyone who needed to find a bug in the code tried:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Analyzing program output, particularly logs produced by simple print statements or built-in logback, java.util.logging, log4j, or the slf4j façade</li><li class="listitem">Attaching a (remote) debugger</li><li class="listitem">Monitoring CPU, disk I/O, memory (to resolve higher level resource-utilization issues)</li></ul></div><p>More or less, all these approaches fail if we have a multithreaded or distributed system—and Scala is inherently multithreaded as Spark is inherently distributed. Collecting logs over a set of nodes is not scalable (even though a few successful commercial systems exist that do this). Attaching a remote debugger is not always possible due to security and network restrictions. Remote debugging can also induce substantial overhead and interfere with the program execution, particularly for ones that use synchronization. Setting the debug level to the <code class="literal">DEBUG</code> or <code class="literal">TRACE</code> level helps sometimes, but leaves you at the mercy of the developer who may or may not have thought of a particular corner case you are dealing with right at the moment. The approach we take in this book is to open a servlet with enough information to glean into program execution and application methods real-time, as much as it is possible with the current state of Scala and Scalatra.</p><p>Enough about the overall issues of debugging the program execution. Monitoring is somewhat different, as it is concerned with only high-level issue identification. Intersection with issue investigation or resolution happens, but usually is outside of monitoring. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Understanding major areas for monitoring and monitoring goals</li><li class="listitem">Learning OS tools for Scala/Java monitoring to support issue identification and debugging</li><li class="listitem">Learning about MBeans and MXBeans</li><li class="listitem">Understanding model performance drift</li><li class="listitem">Understanding A/B testing</li></ul></div><div class="section" title="System monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec7000000"/>System monitoring</h1></div></div></div><p>While there <a id="id719000000" class="indexterm"/>are other types of monitoring dealing specifically with ML-targeted tasks, such as monitoring the performance of the models, let me start with basic system monitoring. Traditionally, system monitoring is a subject of operating system maintenance, but it is becoming a vital component of any complex application, specifically running over a set of distributed workstations. The primary components of the OS are CPU, disk, memory, network, and energy on battery-powered machines. The <a id="id720000000" class="indexterm"/>traditional OS-like tools for monitoring system performance are provided in the following table. We limit them to Linux tools as this is the platform for most Scala applications, even though other OS vendors provide OS monitoring tools<a id="id721000000" class="indexterm"/> such as <span class="strong"><strong>Activity Monitor</strong></span>. As Scala runs in Java JVM, I also added Java-specific monitoring tools that are specific to JVMs:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Area</p>
</th><th valign="bottom">
<p>Programs</p>
</th><th valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>CPU</p>
</td><td valign="top">
<p>
<code class="literal">htop,</code> <code class="literal">top</code>, <code class="literal">sar-u</code>
</p>
</td><td valign="top">
<p>
<code class="literal">top</code> has been the most often used performance diagnostic tool, as CPU and memory have been the most constraint resources. With the advent of distributed programming, network and disk tend to be the most constraint.</p>
</td></tr><tr><td valign="top">
<p>Disk</p>
</td><td valign="top">
<p>
<code class="literal">iostat</code>, <code class="literal">sar -d</code>, <code class="literal">lsof</code>
</p>
</td><td valign="top">
<p>The number of open files, provided by <code class="literal">lsof</code>, is often a constraining resource as many big data applications and daemons tend to keep multiple files open.</p>
</td></tr><tr><td valign="top">
<p>Memory</p>
</td><td valign="top">
<p>
<code class="literal">top</code>, <code class="literal">free</code>, <code class="literal">vmstat</code>, <code class="literal">sar -r</code>
</p>
</td><td valign="top">
<p>Memory is used by OS in multiple ways, for example to maintain disk I/O buffers so that having extra buffered and cached memory helps performance.</p>
</td></tr><tr><td valign="top">
<p>Network</p>
</td><td valign="top">
<p>
<code class="literal">ifconfig</code>, <code class="literal">netstat</code>, <code class="literal">tcpdump</code>, <code class="literal">nettop</code>, <code class="literal">iftop</code>, <code class="literal">nmap</code>
</p>
</td><td valign="top">
<p>Network is how the distributed systems talk and is an important OS component. From the application point of view, watch for errors, collisions, and dropped packets as an indicator of problems.</p>
</td></tr><tr><td valign="top">
<p>Energy</p>
</td><td valign="top">
<p>
<code class="literal">powerstat</code>
</p>
</td><td valign="top">
<p>While power consumption is traditionally not a part of OS monitoring, it is nevertheless a shared resource, which recently became one of the major costs for maintaining a working system.</p>
</td></tr><tr><td valign="top">
<p>Java</p>
</td><td valign="top">
<p>
<code class="literal">jconsole</code>, <code class="literal">jinfo</code>, <code class="literal">jcmd</code>, <code class="literal">jmc</code>
</p>
</td><td valign="top">
<p>All these tools allow you to examine configuration and run-time properties of an application. <span class="strong"><strong>Java Mission Control</strong></span> (<span class="strong"><strong>JMC</strong></span>) is shipped with<a id="id722000000" class="indexterm"/> JDK starting with version 7u40.</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 10.1. Common Linux OS monitoring tools</p></blockquote></div><p>In many cases, the tools are redundant. For example, the CPU and memory information can be obtained with <code class="literal">top</code>, <code class="literal">sar</code>, and <code class="literal">jmc</code> commands.</p><p>There are a <a id="id723000000" class="indexterm"/>few tools for collecting this information over a set of distributed nodes. Ganglia<a id="id724000000" class="indexterm"/> is a BSD-licensed scalable distributed monitoring <a id="id725000000" class="indexterm"/>system (<a class="ulink" href="http://ganglia.info">http://ganglia.info</a>). It is based on a hierarchical design and is very careful about data structure and algorithm designs. It is known to scale to 10,000s of nodes. It consists of a gmetad daemon that is collects information from multiple hosts and presents it in a web interface, and gmond daemons running on each individual host. The communication happens on the 8649 port by default, which spells Unix. By default, gmond sends information about CPU, memory, and network, but multiple plugins exist for other metrics (or can be created). Gmetad can aggregate the information and pass it up the hierarchy chain to another gmetad daemon.  Finally, the data is presented in a Ganglia web interface.</p><p>Graphite is another monitoring tool that stores numeric time-series data and renders graphs of this data on demand. The web app provides a /render endpoint to generate graphs and retrieve raw data via a RESTful API. Graphite has a pluggable backend (although it has it's own default implementation). Most of the modern metrics implementations, including scala-metrics used in this chapter, support sending data to Graphite.</p></div></div>
<div class="section" title="Process monitoring"><div class="titlepage" id="aid-8S67K2"><div><div><h1 class="title"><a id="ch10lvl1sec7100000"/>Process monitoring</h1></div></div></div><p>The tools described in<a id="id726000000" class="indexterm"/> the previous section are not application-specific. For a long-running process, it often necessary to provide information about the internal state to either a monitoring a graphing solution such as Ganglia or Graphite, or just display it in a servlet. Most of these solutions are read-only, but in some cases, the commands give the control to the users to modify the state, such as log levels, or to trigger garbage collection.</p><p>Monitoring, in general is supposed to do the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Provide high-level information about program execution and application-specific metrics</li><li class="listitem">Potentially, perform health-checks for critical components</li><li class="listitem">Might incorporate alerting and thresholding on some critical metrics</li></ul></div><p>I have also seen monitoring to include update operations to either update the logging parameters or test components, such as trigger model scoring with predefined parameters. The latter can be considered as a part of parameterized health check.</p><p>Let's see how it <a id="id727000000" class="indexterm"/>works on the example of a simple <code class="literal">Hello World</code> web application that accepts REST-like requests and assigns a unique ID for different users written in the <a id="id728000000" class="indexterm"/>Scalatra framework (<a class="ulink" href="http://scalatra.org">http://scalatra.org</a>), a lightweight web-application development framework in Scala. The application is supposed to respond to CRUD HTTP requests to create a unique numeric ID for a user. To implement the service in Scalatra, we need just to provide a <code class="literal">Scalate</code> template. The full <a id="id729000000" class="indexterm"/>documentation can be found at <a class="ulink" href="http://scalatra.org/2.4/guides/views/scalate.html">http://scalatra.org/2.4/guides/views/scalate.html</a>, the source code is provided with the book and can be found in <code class="literal">chapter10</code> subdirectory:</p><div class="informalexample"><pre class="programlisting">class SimpleServlet extends Servlet {
  val logger = LoggerFactory.getLogger(getClass)
  var hwCounter: Long = 0L
  val hwLookup: scala.collection.mutable.Map[String,Long] = scala.collection.mutable.Map() 
  val defaultName = "Stranger"
  def response(name: String, id: Long) = { "Hello %s! Your id should be %d.".format(if (name.length &gt; 0) name else defaultName, id) }
  get("/hw/:name") {
    val name = params("name")
    val startTime = System.nanoTime
    val retVal = response(name, synchronized { hwLookup.get(name) match { case Some(id) =&gt; id; case _ =&gt; hwLookup += name -&gt; { hwCounter += 1; hwCounter } ; hwCounter } } )
    logger.info("It took [" + name + "] " + (System.nanoTime - startTime) + " " + TimeUnit.NANOSECONDS)
    retVal
  }
}</pre></div><p>First, the code gets the <code class="literal">name</code> parameter from the request (REST-like parameter parsing is also supported). Then, it checks the internal HashMap for existing entries, and if the entry does not exist, it creates a new index using a synchronized call to increment <code class="literal">hwCounter</code> (in a real-world application, this information should be persistent in a database such as HBase, but I'll skip this layer in this section for the purpose of simplicity). To run the application, one needs to download the code, start <code class="literal">sbt</code>, and type <code class="literal">~;jetty:stop;jetty:start</code> to enable continuous run/compilation as in <a class="link" title="Chapter 7. Working with Graph Algorithms" href="part0283.xhtml#aid-8DSF61">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>. The modifications to the file will be immediately picked up by the<a id="id730000000" class="indexterm"/> build tool and the jetty server will restart:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro chapter10]$ sbt</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter10/project</strong></span>
<span class="strong"><strong>[info] Compiling 1 Scala source to /Users/akozlov/Src/Book/ml-in-scala/chapter10/project/target/scala-2.10/sbt-0.13/classes...</strong></span>
<span class="strong"><strong>[info] Set current project to Advanced Model Monitoring (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/)</strong></span>
<span class="strong"><strong>&gt; ~;jetty:stop;jetty:start</strong></span>
<span class="strong"><strong>[success] Total time: 0 s, completed May 15, 2016 12:08:31 PM</strong></span>
<span class="strong"><strong>[info] Compiling Templates in Template Directory: /Users/akozlov/Src/Book/ml-in-scala/chapter10/src/main/webapp/WEB-INF/templates</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
<span class="strong"><strong>[info] starting server ...</strong></span>
<span class="strong"><strong>[success] Total time: 1 s, completed May 15, 2016 12:08:32 PM</strong></span>
<span class="strong"><strong>1. Waiting for source changes... (press enter to interrupt)</strong></span>
<span class="strong"><strong>2016-05-15 12:08:32.578:INFO::main: Logging initialized @119ms</strong></span>
<span class="strong"><strong>2016-05-15 12:08:32.586:INFO:oejr.Runner:main: Runner</strong></span>
<span class="strong"><strong>2016-05-15 12:08:32.666:INFO:oejs.Server:main: jetty-9.2.1.v20140609</strong></span>
<span class="strong"><strong>2016-05-15 12:08:34.650:WARN:oeja.AnnotationConfiguration:main: ServletContainerInitializers: detected. Class hierarchy: empty</strong></span>
<span class="strong"><strong>2016-15-05 12:08:34.921: [main] INFO  o.scalatra.servlet.ScalatraListener - The cycle class name from the config: ScalatraBootstrap</strong></span>
<span class="strong"><strong>2016-15-05 12:08:34.973: [main] INFO  o.scalatra.servlet.ScalatraListener - Initializing life cycle class: ScalatraBootstrap</strong></span>
<span class="strong"><strong>2016-15-05 12:08:35.213: [main] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T/scalate-6339535024071976693-workdir</strong></span>
<span class="strong"><strong>2016-05-15 12:08:35.216:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@1ef7fe8e{/,file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/,AVAILABLE}{file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/}</strong></span>
<span class="strong"><strong>2016-05-15 12:08:35.216:WARN:oejsh.RequestLogHandler:main: !RequestLog</strong></span>
<span class="strong"><strong>2016-05-15 12:08:35.237:INFO:oejs.ServerConnector:main: Started ServerConnector@68df9280{HTTP/1.1}{0.0.0.0:8080}</strong></span>
<span class="strong"><strong>2016-05-15 12:08:35.237:INFO:oejs.Server:main: Started @2795ms2016-15-05 12:03:52.385: [main] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T/scalate-3504767079718792844-workdir</strong></span>
<span class="strong"><strong>2016-05-15 12:03:52.387:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@1ef7fe8e{/,file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/,AVAILABLE}{file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/}</strong></span>
<span class="strong"><strong>2016-05-15 12:03:52.388:WARN:oejsh.RequestLogHandler:main: !RequestLog</strong></span>
<span class="strong"><strong>2016-05-15 12:03:52.408:INFO:oejs.ServerConnector:main: Started ServerConnector@68df9280{HTTP/1.1}{0.0.0.0:8080}</strong></span>
<span class="strong"><strong>2016-05-15 12:03:52.408:INFO:oejs.Server:main: Started @2796mss</strong></span>
</pre></div><p>When the <a id="id731000000" class="indexterm"/>servlet is started on port 8080, issue a browser request:</p><div class="note" title="Note"><h3 class="title"><a id="tip10000000"/>Tip</h3><p>I pre-created the project for this book, but if you want to create a Scalatra project from scratch, there is a <code class="literal">gitter</code> command in <code class="literal">chapter10/bin/create_project.sh</code>. Gitter will create a <code class="literal">project/build.scala</code> file with a Scala object, extending build that will set project parameters and enable the Jetty plugin for the SBT.</p></div><p>
<code class="literal">http://localhost:8080/hw/Joe</code>. </p><p>The output should look similar to the following screenshot:</p><div class="mediaobject"><img src="../Images/image01762.jpeg" alt="Process monitoring"/><div class="caption"><p>Figure 10-1: The servlet web page.</p></div></div><p style="clear:both; height: 1em;"> </p><p>If you call the servlet with a different name, it will assign a distinct ID, which will be persistent across the lifetime of the application.</p><p>As we also enabled console logging, you will also see something similar to the following command on the console:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>2016-15-05 13:10:06.240: [qtp1747585824-26] INFO  o.a.examples.ServletWithMetrics - It took [Joe] 133225 NANOSECONDS</strong></span>
</pre></div><p>While retrieving and analyzing logs, which can be redirected to a file, is an option and there are multiple systems to collect, search, and analyze logs from a set of distributed servers, it is often also important to have a simple way to introspect the running code. One way to accomplish this is to create a separate template with metrics, however, Scalatra provides metrics and health support to enable basic implementations for counts, histograms, rates, and so on.</p><p>I will use the Scalatra metrics support. The <code class="literal">ScalatraBootstrap</code> class has to implement the <code class="literal">MetricsBootstrap</code> trait. The <code class="literal">org.scalatra.metrics.MetricsSupport</code> and <code class="literal">org.scalatra.metrics.HealthChecksSupport</code> traits provide templating similar to the Scalate<a id="id732000000" class="indexterm"/> templates, as shown in the following code.</p><p>The following is the content of the <code class="literal">ScalatraTemplate.scala</code> file:</p><div class="informalexample"><pre class="programlisting">import org.akozlov.examples._

import javax.servlet.ServletContext
import org.scalatra.LifeCycle
import org.scalatra.metrics.MetricsSupportExtensions._
import org.scalatra.metrics._
 
class ScalatraBootstrap extends LifeCycle with MetricsBootstrap {
  override def init(context: ServletContext) = {
    context.mount(new ServletWithMetrics, "/")
    context.mountMetricsAdminServlet("/admin")
    context.mountHealthCheckServlet("/health")
    context.installInstrumentedFilter("/*")
  }
}</pre></div><p>The following is the content of the <code class="literal">ServletWithMetrics.scala</code> file:</p><div class="informalexample"><pre class="programlisting">package org.akozlov.examples

import org.scalatra._
import scalate.ScalateSupport
import org.scalatra.ScalatraServlet
import org.scalatra.metrics.{MetricsSupport, HealthChecksSupport}
import java.util.concurrent.atomic.AtomicLong
import java.util.concurrent.TimeUnit
import org.slf4j.{Logger, LoggerFactory}

class ServletWithMetrics extends Servlet with MetricsSupport with HealthChecksSupport {
  val logger = LoggerFactory.getLogger(getClass)
  val defaultName = "Stranger"
  var hwCounter: Long = 0L
  val hwLookup: scala.collection.mutable.Map[String,Long] = scala.collection.mutable.Map()  val hist = histogram("histogram")
  val cnt =  counter("counter")
  val m = meter("meter")
  healthCheck("response", unhealthyMessage = "Ouch!") { response("Alex", 2) contains "Alex" }
  def response(name: String, id: Long) = { "Hello %s! Your id should be %d.".format(if (name.length &gt; 0) name else defaultName, id) }

  get("/hw/:name") {
    cnt += 1
    val name = params("name")
    hist += name.length
    val startTime = System.nanoTime
    val retVal = response(name, synchronized { hwLookup.get(name) match { case Some(id) =&gt; id; case _ =&gt; hwLookup += name -&gt; { hwCounter += 1; hwCounter } ; hwCounter } } )s
    val elapsedTime = System.nanoTime - startTime
    logger.info("It took [" + name + "] " + elapsedTime + " " + TimeUnit.NANOSECONDS)
    m.mark(1)
    retVal
  }</pre></div><p>If you run the <a id="id733000000" class="indexterm"/>server again, the <code class="literal">http://localhost:8080/admin</code> page will show a set of links for operational information, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image01763.jpeg" alt="Process monitoring"/><div class="caption"><p>Figure 10-2: The admin servlet web page</p></div></div><p style="clear:both; height: 1em;"> </p><p>The <span class="strong"><strong>Metrics</strong></span> link will lead to the metrics servlet depicted in <span class="emphasis"><em>Figure 10-3</em></span>. The <code class="literal">org.akozlov.exampes.ServletWithMetrics.counter</code> will have a global count of requests, and <code class="literal">org.akozlov.exampes.ServletWithMetrics.histogram</code> will show the distribution <a id="id734000000" class="indexterm"/>of accumulated values, in this case, the name lengths. More importantly, it will compute <code class="literal">50</code>, <code class="literal">75</code>, <code class="literal">95</code>, <code class="literal">98</code>, <code class="literal">99</code>, and <code class="literal">99.9</code> percentiles. The meter counter will show rates for the last <code class="literal">1</code>, <code class="literal">5</code>, and <code class="literal">15</code> minutes:</p><div class="mediaobject"><img src="../Images/image01764.jpeg" alt="Process monitoring"/><div class="caption"><p>Figure 10-3: The metrics servlet web page</p></div></div><p style="clear:both; height: 1em;"> </p><p>Finally, one can <a id="id735000000" class="indexterm"/>write health checks. In this case, I will just check whether the result of the response function contains the string that it has been passed as a parameter. Refer to the following <span class="emphasis"><em>Figure 10.4</em></span>:</p><div class="mediaobject"><img src="../Images/image01765.jpeg" alt="Process monitoring"/><div class="caption"><p>Figure 10-4: The health check servlet web page.</p></div></div><p style="clear:both; height: 1em;"> </p><p>The metrics <a id="id736000000" class="indexterm"/>can be configured to report to Ganglia or Graphite data collection servers or periodically dump information into a log file.</p><p>Endpoints do not have to be read-only. One of the pre-configured components is the timer, which measures the time to complete a task—which can be used for measuring scoring performance. Let's put the code in the <code class="literal">ServletWithMetrics</code> class:</p><div class="informalexample"><pre class="programlisting">  get("/time") {
    val sleepTime = scala.util.Random.nextInt(1000)
    val startTime = System.nanoTime
    timer("timer") {
      Thread.sleep(sleepTime)
      Thread.sleep(sleepTime)
      Thread.sleep(sleepTime)
    }
    logger.info("It took [" + sleepTime + "] " + (System.nanoTime - startTime) + " " + TimeUnit.NANOSECONDS)
    m.mark(1)
  }</pre></div><p>Accessing <code class="literal">http://localhost:8080/time</code> will trigger code execution, which will be timed with a timer in metrics.</p><p>Analogously, the put operation, which can be created with the <code class="literal">put()</code> template, can be used to either adjust the run-time parameters or execute the code in-situ—which, depending on the code, might need to be secured in production environments.</p><div class="note" title="Note"><h3 class="title"><a id="note15000000"/>Note</h3><p>
<span class="strong"><strong>JSR 110</strong></span>
</p><p>JSR 110 is <a id="id737000000" class="indexterm"/>another <span class="strong"><strong>Java Specification Request</strong></span> (<span class="strong"><strong>JSR</strong></span>), commonly<a id="id738000000" class="indexterm"/> known as <span class="strong"><strong>Java Management Extensions</strong></span> (<span class="strong"><strong>JMX</strong></span>). JSR 110 specifies a number of APIs and protocols in <a id="id739000000" class="indexterm"/>order to be able to monitor the JVM executions remotely. A common way to access JMX Services is via the <code class="literal">jconsole</code> command that will connect to one of the local processes by default. To connect to a remote host, you need to provide the <code class="literal">-Dcom.sun.management.jmxremote.port=portNum</code> property on the Java command line. It is also advisable to enable security (SSL or password-based authentication). In practice, other monitoring tools use JMX for monitoring, as well as managing the JVM, as JMX allows callbacks to manage the system state.</p><p>You can provide your own metrics that are exposed via JMX. While Scala runs in JVM, the implementation of JMX (via MBeans) is very Java-specific, and it is not clear how well the mechanism will play with Scala. JMX Beans can certainly be exposed as a servlet in Scala though.</p></div><p>The<a id="id740000000" class="indexterm"/> JMX MBeans can usually be examined in JConsole, but we can also expose it as <code class="literal">/jmx servlet</code>, the code provided in the book repository (<a class="ulink" href="https://github.com/alexvk/ml-in-scala">https://github.com/alexvk/ml-in-scala</a>).</p></div>
<div class="section" title="Model monitoring" id="aid-8T4O61"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec7200000"/>Model monitoring</h1></div></div></div><p>We have covered basic<a id="id741000000" class="indexterm"/> system and application metrics. Lately, a new direction evolved for using monitoring components to monitor statistical model performance. The statistical model performance covers the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">How the model performance evolved over time</li><li class="listitem">When is the time to retire the model</li><li class="listitem">Model health check</li></ul></div><div class="section" title="Performance over time"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec5500000"/>Performance over time</h2></div></div></div><p>ML models deteriorate with time, or 'age': While this process is not still well understood, the model performance tends to change<a id="id742000000" class="indexterm"/> with time, if even due to concept drift, where the definition of the attributes change, or the changes in the underlying dependencies. Unfortunately, model performance rarely improves, at least in my practice. Thus, it is imperative to keep track of models. One way to do this is by monitoring the metrics that the model is intended to optimize, as in many cases, we do not have a ready-labeled set of data.</p><p>In many cases, the model performance deterioration is not related directly to the quality of the statistical modeling, even though simpler models such as linear and logistic regression tend to be more stable than more complex models such as decision trees. Schema evolution or unnoticed renaming of attributes may cause the model to not perform well.</p><p>Part of model monitoring should be running the health check, where a model periodically scores either a few records or a known scored set of data.</p></div><div class="section" title="Criteria for model retiring"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec5600000"/>Criteria for model retiring</h2></div></div></div><p>A very<a id="id743000000" class="indexterm"/> common case in practical deployments is that data scientists come with better sets of models every few weeks. However, if this does not happen, one needs come up with a set of criteria to retire a model. As real-world traffic rarely comes with the scored data, for example, the data that is already scored, the usual way to measure model performance is via a proxy, which is the metric that the model is supposed to improve.</p></div><div class="section" title="A/B testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec5700000"/>A/B testing</h2></div></div></div><p>A/B testing is a<a id="id744000000" class="indexterm"/> specific case of controlled experiment in e-commerce setting. A/B testing is usually applied to versions of a web page where we<a id="id745000000" class="indexterm"/> direct completely independent subset of users to each of the versions. The dependent variable to test is usually the response rate. Unless any specific information is available about users, and in many cases, it is not unless a cookie is placed in the computer, the split is random. Often the split is based on unique userID, but this is known not to work too well across multiple devices. A/B testing is subject to the same assumptions the controlled experiments are subject to: the tests should be completely independent and the distribution of the dependent variable should be <code class="literal">i.i.d.</code>. Even though it is hard to imagine that all people are truly <code class="literal">i.i.d.</code>, the A/B test has been shown to work for practical problems.</p><p>In modeling, we split the traffic to be scored into two or multiple channels to be scored by two or multiple models. Further, we need to measure the cumulative performance metric for each of the channels together with estimated variance. Usually, one of the models is treated as a baseline and is associated with the null hypothesis, and for the rest of the models, we run a t-test, comparing the ratio of the difference to the standard deviation.</p></div></div>
<div class="section" title="Summary" id="aid-8U38O1"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec7300000"/>Summary</h1></div></div></div><p>This chapter described system, application, and model monitoring goals together with the existing monitoring solutions for Scala, and specifically Scalatra. Many metrics overlap with standard OS or Java monitoring, but we also discussed how to create application-specific metrics and health checks. We talked about a new emerging field of model monitoring in an ML application, where statistical models are subject to deterioration, health, and performance monitoring. I also touched on monitoring distributed systems, a topic that really deserves much more space, which unfortunately, I did not have.</p><p>This is the end of the book, but in no way is it the end of the journey. I am sure, new frameworks and applications are being written as we speak. Scala has been a pretty awesome and succinct development tool in my practice, with which I've been able to achieve results in hours instead of days, which is the case with more traditional tools, but it is yet to win the popular support, which I am pretty sure it. We just need to emphasize its advantages in the modern world of interactive analysis, complex data, and distributed processing.</p></div></body></html>