- en: 'Chapter 2: Graph Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：图机器学习
- en: '**Machine learning** is a subset of artificial intelligence that aims to provide
    systems with the ability to *learn* and improve from data. It has achieved impressive
    results in many different applications, especially where it is difficult or unfeasible
    to explicitly define rules to solve a specific task. For instance, we can train
    algorithms to recognize spam emails, translate sentences into other languages,
    recognize objects in an image, and so on.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是人工智能的一个子集，旨在为系统提供从数据中*学习*和改进的能力。它在许多不同的应用中取得了令人印象深刻的成果，尤其是在难以或无法明确定义规则来解决特定任务的情况下。例如，我们可以训练算法来识别垃圾邮件，将句子翻译成其他语言，识别图像中的物体，等等。'
- en: In recent years, there has been an increasing interest in applying machine learning
    to *graph-structured data*. Here, the primary objective is to automatically learn
    suitable representations to make predictions, discover new patterns, and understand
    complex dynamics in a better manner with respect to "traditional" machine learning
    approaches.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，应用机器学习到*图结构数据*的兴趣日益增加。在这里，主要目标是自动学习合适的表示，以便进行预测，发现新的模式，并更好地理解相对于“传统”机器学习方法的复杂动态。
- en: This chapter will first review some of the basic machine learning concepts.
    Then, an introduction to graph machine learning will be provided, with a particular
    focus on **representation learning**. We will then analyze a practical example
    to guide you through the comprehension of the theoretical concepts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先回顾一些基本的机器学习概念。然后，将提供关于图机器学习的介绍，特别关注**表示学习**。接着，我们将分析一个实际案例，以指导您理解理论概念。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: A refresher on machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的复习
- en: What is machine learning on graphs and why is it important?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图上机器学习是什么，为什么它很重要？
- en: A general taxonomy to navigate among graph machine learning algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图机器学习算法之间导航的一般分类法
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter notebooks with *Python 3.8* for all of our exercises.
    The following is a list of the Python libraries that need to be installed for
    this chapter using `pip`. For example, run `pip install networkx==2.5` on the
    command line, and so on:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带有*Python 3.8*的Jupyter笔记本来完成所有练习。以下是需要使用`pip`安装的Python库列表，以便本章使用。例如，在命令行中运行`pip
    install networkx==2.5`，等等：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All the code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所有相关的代码文件均可在[https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter02)找到。
- en: Understanding machine learning on graphs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在图上理解机器学习
- en: 'Of the branches of artificial intelligence, **machine learning** is one that
    has attracted the most attention in recent years. It refers to a class of computer
    algorithms that automatically learn and improve their skills through experience
    *without being explicitly programmed*. Such an approach takes inspiration from
    nature. Imagine an athlete who faces a novel movement for the first time: they
    start slowly, carefully imitating the gesture of a coach, trying, making mistakes,
    and trying again. Eventually, they will improve, becoming more and more confident.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的各个分支中，**机器学习**是近年来最受关注的领域之一。它指的是一类计算机算法，这些算法通过经验自动学习和改进其技能，而不需要明确编程。这种方法从自然界中汲取灵感。想象一下，一位运动员第一次面对一个新颖的动作：他们开始慢慢地、仔细地模仿教练的姿势，尝试、犯错，然后再次尝试。最终，他们会变得越来越好，越来越自信。
- en: Now, how does this concept translate to machines? It is essentially an optimization
    problem. The goal is to find a mathematical model that is able to achieve the
    best possible performance on a particular task. Performance can be measured using
    a specific performance metric (also known as a **loss function** or **cost function**).
    In a common learning task, the algorithm is provided with data, possibly lots
    of it. The algorithm uses this data to iteratively make decisions or predictions
    for the specific task. At each iteration, decisions are evaluated using the loss
    function. The resulting *error* is used to update the model parameters in a way
    that, hopefully, means the model will perform better. This process is commonly
    called **training**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个概念如何转化为机器？这本质上是一个优化问题。目标是找到一个数学模型，能够在特定任务上实现最佳性能。性能可以使用特定的性能指标（也称为 **损失函数**
    或 **成本函数**）来衡量。在常见的学习任务中，算法被提供数据，可能有很多数据。算法使用这些数据来对特定任务进行迭代决策或预测。在每次迭代中，决策使用损失函数进行评估。产生的
    *误差* 用于以某种方式更新模型参数，希望模型将表现得更好。这个过程通常被称为 **训练**。
- en: More formally, let's consider a particular task, *T*, and a performance metric,
    *P*, which allows us to quantify how good an algorithm is performing on *T*. According
    to Mitchell (Mitchell et al., 1997), an algorithm is said to learn from experience,
    *E*, if its performance at task *T*, measured by *P*, improves with experience
    *E*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，让我们考虑一个特定的任务 *T* 和一个性能指标 *P*，它允许我们量化算法在 *T* 上的表现有多好。根据 Mitchell（Mitchell
    et al., 1997）的说法，如果一个算法在经验 *E* 上，通过 *P* 衡量的任务 *T* 的性能随着经验 *E* 的增加而提高，那么这个算法就被说成是从经验中学习的。
- en: Basic principles of machine learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的基本原理
- en: Machine learning algorithms fall into three main categories, known as *supervised*,
    *unsupervised*, and *semi-supervised* learning. These learning paradigms depend
    on the way data is provided to the algorithm and how performance is evaluated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法分为三大类，称为 *监督学习*、*无监督学习* 和 *半监督学习*。这些学习范式取决于数据提供给算法的方式以及性能如何评估。
- en: '**Supervised learning** is the learning paradigm used when we know the answer
    to the problem. In this scenario, the dataset is composed of samples of pairs
    of the form *<x,y>*, where *x* is the input (for example, an image or a voice
    signal) and *y* is the corresponding desired output (for example, what the image
    represents or what the voice is saying). The input variables are also known as
    *features*, while the output is usually referred to as *labels*, *targets*, and
    *annotations*. In supervised settings, performance is often evaluated using a
    *distance function*. This function measures the differences between the prediction
    and the expected output. According to the type of labels, supervised learning
    can be further divided into the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是在我们知道问题答案时使用的学习范式。在这种情况下，数据集由形式为 *<x,y>* 的样本对组成，其中 *x* 是输入（例如，图像或声音信号），而
    *y* 是相应的期望输出（例如，图像所代表的内容或声音所表达的内容）。输入变量也被称为 *特征*，而输出通常被称为 *标签*、*目标*和*注释*。在监督学习设置中，性能通常使用
    *距离函数* 来评估。此函数衡量预测与期望输出之间的差异。根据标签的类型，监督学习可以进一步分为以下几种：'
- en: '**Classification**: Here, the labels are discrete and refer to the "class"
    the input belongs to. Examples of classification are determining the object in
    a photo or predicting whether an email is spam or not.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：在这里，标签是离散的，指的是输入所属的“类别”。分类的例子包括确定照片中的物体或预测一封电子邮件是否为垃圾邮件。'
- en: '**Regression**: The target is continuous. Examples of regression problems are
    predicting the temperature in a building or predicting the selling price of any
    particular product.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：目标是连续的。回归问题的例子包括预测建筑中的温度或预测任何特定产品的销售价格。'
- en: '**Unsupervised learning** differs from supervised learning since the answer
    to the problem is not known. In this context, we do not have any labels and only
    the inputs, *<x>*, are provided. The goal is thus deducing structures and patterns,
    attempting to find similarities.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**与监督学习不同，因为问题答案未知。在这种情况下，我们没有标签，只有输入 *<x>*。因此，目标是推断结构和模式，试图找到相似性。'
- en: Discovering groups of similar examples (clustering) is one of these problems,
    as well as giving new representations of the data in a high-dimensional space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 发现相似示例的组（聚类）是这类问题之一，以及在高维空间中给出数据的新表示。
- en: In **semi-supervised learning**, the algorithm is trained using a combination
    of labeled and unlabeled data. Usually, to direct the research of structures present
    in the unlabeled input data, a limited amount of labeled data is used.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在**半监督学习**中，算法使用标记数据和未标记数据的组合进行训练。通常，为了指导未标记输入数据中存在的结构的研究，会使用有限数量的标记数据。
- en: It is also worth mentioning that **reinforcement learning** is used for training
    machine learning models to make a sequence of decisions. The artificial intelligence
    algorithm faces a game-like situation, getting *penalties* or *rewards* based
    on the actions performed. The role of the algorithm is to understand how to act
    in order to maximize rewards and minimize penalties.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，**强化学习**用于训练机器学习模型以做出一系列决策。人工智能算法面临类似游戏的情境，根据执行的动作获得*惩罚*或*奖励*。算法的作用是理解如何行动以最大化奖励并最小化惩罚。
- en: 'Minimizing the error on the training data is not enough. The keyword in machine
    learning is *learning*. It means that algorithms must be able to achieve the same
    level of performance even on unseen data. The most common way of evaluating the
    generalization capabilities of machine learning algorithms is to divide the dataset
    into two parts: the **training set** and the **test set**. The model is trained
    on the training set, where the loss function is computed and used to update the
    parameters. After training, the model''s performance is evaluated on the test
    set. Moreover, when more data is available, the test set can be further divided
    into **validation** and **test** sets. The validation set is commonly used for
    assessing the model''s performance during training.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在训练数据上最小化错误是不够的。机器学习的关键词是*学习*。这意味着算法必须能够在未见过的数据上达到相同的性能水平。评估机器学习算法泛化能力的最常见方式是将数据集分为两部分：**训练集**和**测试集**。模型在训练集上训练，其中计算损失函数并用于更新参数。训练后，模型在测试集上的性能得到评估。此外，当有更多数据可用时，测试集可以进一步分为**验证集**和**测试集**。验证集通常用于评估训练期间模型的表现。
- en: 'When training a machine learning algorithm, three situations can be observed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练机器学习算法时，可以观察到三种情况：
- en: In the first situation, the model reaches a low level of performance over the
    training set. This situation is commonly known as **underfitting**, meaning that
    the model is not powerful enough to address the task.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一种情况下，模型在训练集上的表现水平较低。这种情况通常被称为**欠拟合**，意味着模型没有足够的能力来处理任务。
- en: In the second situation, the model achieves a high level of performance over
    the training set but struggles at generalizing over testing data. This situation
    is known as **overfitting**. In this case, the model is simply memorizing the
    training data, without actually understanding the true relations among them.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种情况下，模型在训练集上达到高水平的表现，但在测试数据的泛化上遇到困难。这种情况被称为**过拟合**。在这种情况下，模型只是简单地记忆训练数据，而没有真正理解它们之间的真实关系。
- en: Finally, the ideal situation is when the model is able to achieve (possibly)
    the highest level of performance over both training and testing data.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，理想的情况是模型能够在训练和测试数据上达到（可能）最高的性能水平。
- en: 'An example of overfitting and underfitting is given by the risk curve shown
    in *Figure 2.1*. From the figure, it is possible to see how the performances on
    the training and test sets change according to the complexity of the model (the
    number of parameters to be fitted):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合的例子可以通过*图2.1*中显示的风险曲线给出。从图中可以看出，训练集和测试集的性能如何根据模型的复杂性（要拟合的模型参数数量）而变化：
- en: '![Figure 2.1 – Risk curve describing the prediction error on training and test
    set error in the function of the model complexity (number of parameters of the
    model)](img/B16069_02_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 描述模型复杂度（模型参数数量）函数上的训练集和测试集预测错误的风险曲线](img/B16069_02_01.jpg)'
- en: Figure 2.1 – Risk curve describing the prediction error on training and test
    set error in the function of the model complexity (number of parameters of the
    model)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 描述模型复杂度（模型参数数量）函数上的训练集和测试集预测错误的风险曲线
- en: 'Overfitting is one of the main problems that affect machine learning practitioners.
    It can occur due to several reasons. Some of the reasons can be as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是影响机器学习实践者的主要问题之一。它可能由几个原因引起。以下是一些可能的原因：
- en: The dataset can be ill-defined or not sufficiently representative of the task.
    In this case, adding more data could help to mitigate the problem.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集可能定义不明确或不足以代表任务。在这种情况下，添加更多数据可以帮助减轻问题。
- en: The mathematical model used for addressing the problem is too powerful for the
    task. In this case, proper constraints can be added to the loss function in order
    to reduce the model's "power." Such constraints are called **regularization**
    terms.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于解决问题的数学模型对于任务来说过于强大。在这种情况下，可以向损失函数中添加适当的约束来降低模型的力量。这些约束被称为**正则化**项。
- en: Machine learning has achieved impressive results in many fields, becoming one
    of the most diffused and effective approaches in computer vision, pattern recognition,
    and natural language processing, among others.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在许多领域取得了令人印象深刻的成果，成为计算机视觉、模式识别和自然语言处理等领域中最广泛和最有效的途径之一。
- en: The benefit of machine learning on graphs
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图机器学习的益处
- en: Several machine learning algorithms have been developed, each with its own advantages
    and limitations. Among those, it is worth mentioning regression algorithms (for
    example, linear and logistic regression), instance-based algorithms (for example,
    k-nearest neighbor or support vector machines), decision tree algorithms, Bayesian
    algorithms (for example, naïve Bayes), clustering algorithms (for example, k-means),
    and artificial neural networks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出多种机器学习算法，每种算法都有其自身的优点和局限性。其中，值得提及的是回归算法（例如，线性回归和逻辑回归）、基于实例的算法（例如，k近邻或支持向量机）、决策树算法、贝叶斯算法（例如，朴素贝叶斯）、聚类算法（例如，k均值）和人工神经网络。
- en: But what is the key to all of this success?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一切成功的秘诀是什么？
- en: 'Essentially, one thing: machine learning can automatically address tasks that
    are easy for humans to do. These tasks can be too complex to describe using traditional
    computer algorithms and, in some cases, they have shown even better capabilities
    than human beings. This is especially true when dealing with graphs—they can differ
    in several more ways than an image or audio signal because of their complex structure.
    By using graph machine learning, we can create algorithms to automatically detect
    and interpret recurring latent patterns.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，只有一件事：机器学习可以自动处理人类容易完成的任务。这些任务可能过于复杂，无法用传统的计算机算法描述，在某些情况下，它们甚至显示出比人类更好的能力。这尤其适用于处理图——由于它们的复杂结构，它们可以以比图像或音频信号更多的方式有所不同。通过使用图机器学习，我们可以创建算法来自动检测和解释重复出现的潜在模式。
- en: For these reasons, there has been an increasing interest in *learning representations*
    for graph-structured data and many machine learning algorithms have been developed
    for handling graphs. For example, we might be interested in determining the role
    of a protein in a biological interaction graph, predicting the evolution of a
    collaboration network, recommending new products to a user in a social network,
    and many more (we will discuss these and more applications in [*Chapter 10*](B16069_10_Final_JM_ePub.xhtml#_idTextAnchor150),
    *The Future of Graphs*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，对图结构数据的表示学习越来越感兴趣，并且已经开发出许多机器学习算法来处理图。例如，我们可能对确定蛋白质在生物相互作用图中的作用、预测合作网络的演变、向社交网络中的用户推荐新产品等感兴趣（我们将在[*第10章*](B16069_10_Final_JM_ePub.xhtml#_idTextAnchor150)，*图的未来*)。
- en: 'Due to their nature, graphs can be analyzed at different levels of granularity:
    at the node, edge, and graph level (the whole graph), as depicted in *Figure 2.2*.
    For each of those levels, different problems could be faced and, as a consequence,
    specific algorithms should be used:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的本质，图可以在不同的粒度级别上进行分析：在节点、边和图级别（整个图），如图*图2.2*所示。对于这些级别中的每一个，都可能遇到不同的问题，因此应该使用特定的算法：
- en: '![Figure 2.2 – Visual representation of the three different levels of granularity
    in graphs'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.2 – 图的三种不同粒度级别的视觉表示'
- en: '](img/B16069_02_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16069_02_02.jpg]'
- en: Figure 2.2 – Visual representation of the three different levels of granularity
    in graphs
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 图的三种不同粒度级别的视觉表示
- en: 'In the following bullet points, we will give some examples of machine learning
    problems that could be faced for each of those levels:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的项目符号中，我们将给出一些针对这些级别的机器学习问题的例子：
- en: '**Node level**: Given a (possibly large) graph, ![](img/Formula_02_001.png),
    the goal is to classify each vertex, ![](img/Formula_02_002.png), into the right
    class. In this setting, the dataset includes *G* and a list of pairs, *< vi,yi
    >*, where *vi* is a node of graph *G* and *y**i* is the class to which the node
    belongs.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点级别**：给定一个（可能很大的）图，![](img/Formula_02_001.png)，目标是将每个顶点，![](img/Formula_02_002.png)，分类到正确的类别。在这种情况下，数据集包括*G*和一系列成对元素，*<
    vi,yi >*，其中*vi*是图*G*的一个节点，*yi*是该节点所属的类别。'
- en: '**Edge level**: Given a (possibly large) graph, ![](img/Formula_02_003.png),
    the goal is to classify each edge, ![](img/Formula_02_004.png), into the right
    class. In this setting, the dataset includes *G* and a list of pairs, *< ei,yi
    >*, where *ei* is an edge of graph *G* and *yi* is the class to which the edge
    belongs. Another typical task for this level of granularity is **link prediction**,
    the problem of predicting the existence of a link between two existing nodes in
    a graph.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边级别**：给定一个（可能很大的）图，![](img/Formula_02_003.png)，目标是将每条边，![](img/Formula_02_004.png)，分类到正确的类别。在这种情况下，数据集包括*G*和一系列成对元素，*<
    ei,yi >*，其中*ei*是图*G*的一条边，*yi*是该边所属的类别。在这个粒度级别上，另一个典型任务是**链接预测**，即预测图中两个现有节点之间是否存在链接的问题。'
- en: '**Graph level**: Given a dataset with *m* different graphs, the task is to
    build a machine learning algorithm capable of classifying a graph into the right
    class. We can then see this problem as a classification problem, where the dataset
    is defined by a list of pairs, *<Gi,yi**>*, where *Gi* is a graph and *yi* is
    the class the graph belongs to.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图级别**：给定一个包含*m*个不同图的数据库，任务是构建一个能够将图分类到正确类别的机器学习算法。然后我们可以将这个问题视为一个分类问题，其中数据集由一系列成对元素定义，*<Gi,yi**>，其中*Gi*是一个图，*yi*是该图所属的类别。'
- en: In this section, we discussed some basic concepts of machine learning. Moreover,
    we have enriched our description by introducing some of the common machine learning
    problems when dealing with graphs. Having those theoretical principles as a basis,
    we will now introduce some more complex concepts relating to graph machine learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了机器学习的一些基本概念。此外，我们通过介绍处理图时的一些常见机器学习问题来丰富了我们的描述。有了这些理论原则作为基础，我们现在将介绍一些与图机器学习相关的更复杂的概念。
- en: The generalized graph embedding problem
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广义图嵌入问题
- en: In classical machine learning applications, a common way to process the input
    data is to build from a set of features, in a process called **feature engineering**,
    which is capable of giving a compact and meaningful representation of each instance
    present in the dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的机器学习应用中，处理输入数据的一种常见方式是通过一组特征进行构建，这个过程称为**特征工程**，它能够为数据集中每个实例提供一个紧凑且具有意义的表示。
- en: The dataset obtained from the feature engineering step will be then used as
    input for the machine learning algorithm. If this process usually works well for
    a large range of problems, it may not be the optimal solution when we are dealing
    with graphs. Indeed, due to their well-defined structure, finding a suitable representation
    capable of incorporating all the useful information might not be an easy task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从特征工程步骤获得的数据库将作为机器学习算法的输入。如果这个过程通常对大量问题都有效，那么当我们处理图时，它可能不是最佳解决方案。确实，由于它们的结构定义良好，找到一个能够包含所有有用信息的适当表示可能并不容易。
- en: The first, and most straightforward, way of creating features capable of representing
    structural information from graphs is the *extraction of certain statistics*.
    For instance, a graph could be represented by its degree distribution, efficiency,
    and all the metrics we described in the previous chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种，也是最直接的方法，是创建能够从图中提取结构信息的特征，即*提取某些统计数据*。例如，一个图可以通过其度分布、效率和我们在上一章中描述的所有指标来表示。
- en: A more complex procedure consists of applying specific kernel functions or,
    in other cases, engineering-specific features that are capable of incorporating
    the desired properties into the final machine learning model. However, as you
    can imagine, this process could be really time-consuming and, in certain cases,
    the features used in the model could represent just a subset of the information
    that is really needed to get the best performance for the final model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更复杂的程序包括应用特定的核函数，或者在其他情况下，工程特定的特征，这些特征能够将所需属性纳入最终的机器学习模型中。然而，正如你可以想象的那样，这个过程可能非常耗时，并且在某些情况下，模型中使用的特征可能只代表所需信息的子集，以获得最终模型的最佳性能。
- en: In the last decade, a lot of work has been done in order to define new approaches
    for creating meaningful and compact representations of graphs. The general idea
    behind all these approaches is to create algorithms capable of *learning* a good
    representation of the original dataset such that geometric relationships in the
    new space reflect the structure of the original graph. We usually call the process
    of learning a good representation of a given graph **representation learning**
    or **network embedding**. We will provide a more formal definition as follows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，为了定义创建有意义的紧凑图表示的新方法，已经做了大量工作。所有这些方法背后的通用思想是创建能够*学习*原始数据集良好表示的算法，使得新空间中的几何关系反映了原始图的结构。我们通常将学习给定图的良好表示的过程称为**表示学习**或**网络嵌入**。我们将在以下内容中提供更正式的定义。
- en: '**Representation learning** (**network embedding**) is the task that aims to
    learn a mapping function, ![](img/Formula_02_005.png), from a discrete graph to
    a continuous domain. Function ![](img/Formula_02_006.png) will be capable of performing
    a low-dimensional vector representation such that the properties (local and global)
    of graph ![](img/Formula_02_007.png) are preserved.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**表示学习**（**网络嵌入**）的任务是学习从离散图到连续域的映射函数，![](img/Formula_02_005.png)。函数![](img/Formula_02_006.png)将能够执行低维向量表示，使得图![](img/Formula_02_007.png)的属性（局部和全局）得到保留。'
- en: 'Once mapping ![](img/Formula_02_008.png) is learned, it could be applied to
    the graph and the resulting mapping could be used as a feature set for a machine
    learning algorithm. A graphical example of this process is visible in *Figure
    2.3*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习到映射 ![](img/Formula_02_008.png)，它就可以应用于图，并且得到的映射可以用作机器学习算法的特征集。这个过程的一个图形示例可见于*图2.3*：
- en: '![Figure 2.3 – Example of a workflow for a network embedding algorithm'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 网络嵌入算法的工作流程示例'
- en: '](img/B16069_02_03.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_02_03.jpg)'
- en: Figure 2.3 – Example of a workflow for a network embedding algorithm
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 网络嵌入算法的工作流程示例
- en: Mapping function ![](img/Formula_02_009.png) can also be applied in order to
    learn the vector representation for nodes and edges. As we already mentioned,
    machine learning problems on graphs could occur at different levels of granularity.
    As a consequence, different embedding algorithms have been developed in order
    to learn functions to generate the vectorial representation of nodes (![](img/Formula_02_010.png)
    (also known as **node embedding**) or edges (![](img/Formula_02_011.png)) (also
    known as **edge embedding**). Those mapping functions try to build a vector space
    such that the geometric relationships in the new space reflect the structure of
    the original graph, node, or edges. As a result, we will see that graphs, nodes,
    or edges that are similar in the original space will also be similar in the new
    space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 映射函数![](img/Formula_02_009.png)也可以应用于学习节点和边的向量表示。正如我们之前提到的，图上的机器学习问题可能发生在不同的粒度级别。因此，已经开发了不同的嵌入算法来学习生成节点向量表示的函数（![](img/Formula_02_010.png)，也称为**节点嵌入**）或边的向量表示（![](img/Formula_02_011.png)，也称为**边嵌入**）。这些映射函数试图构建一个向量空间，使得新空间中的几何关系反映了原始图、节点或边的结构。因此，我们将看到，在原始空间中相似的结构在新空间中也将是相似的。
- en: In other words, in the space generated by the embedding function, similar structures
    will have *a small Euclidean distance*, while dissimilar structures will have
    *a large Euclidean distance*. It is important to highlight that while most embedding
    algorithms generate a mapping in Euclidean vector spaces, there has recently been
    an interest in non-Euclidean mapping functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在嵌入函数生成的空间中，相似的结构将具有*较小的欧几里得距离*，而不同的结构将具有*较大的欧几里得距离*。重要的是要强调，尽管大多数嵌入算法在欧几里得向量空间中生成映射，但最近对非欧几里得映射函数产生了兴趣。
- en: 'Let''s now see a practical example of what an embedding space looks like, and
    how similarity can be seen in the new space. In the following code block, we show
    an example using a particular embedding algorithm known as **Node to Vector**
    (**Node2Vec**). We will describe how it works in the next chapter. At the moment,
    we will just say that the algorithm will map each node of graph *G* in a vector:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个嵌入空间的实际例子，以及如何在新的空间中看到相似性。在下面的代码块中，我们展示了使用一种称为**节点到向量**（**Node2Vec**）的特定嵌入算法的示例。我们将在下一章中描述其工作原理。目前，我们只需说该算法将图*G*中的每个节点映射到一个向量：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we have done the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们已经做了以下工作：
- en: We generated a barbell graph (described in the previous chapter).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成了一个哑铃图（在上一章中描述）。
- en: The Node2Vec embedding algorithm is then used in order to map each node of the
    graph in a vector of two dimensions.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用Node2Vec嵌入算法将图中的每个节点映射到二维向量。
- en: Finally, the two-dimensional vectors generated by the embedding algorithm, representing
    the nodes of the original graph, are plotted.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，嵌入算法生成的二维向量，代表原始图中的节点，被绘制出来。
- en: 'The result is shown in *Figure 2.4*:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如*图2.4*所示：
- en: '![Figure 2.4 – Application of the Node2Vec algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 将Node2Vec算法应用于图（左）以生成其节点的嵌入向量（右）]'
- en: '](img/B16069_02_04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_02_04.jpg)'
- en: Figure 2.4 – Application of the Node2Vec algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 将Node2Vec算法应用于图（左）以生成其节点的嵌入向量（右）
- en: From *Figure 2.4*, it is easy to see that nodes that have a similar structure
    are close to each other and are distant from nodes that have dissimilar structures.
    It is also interesting to observe how good Node2Vec is at discriminating group
    1 from group 3\. Since the algorithm uses neighboring information of each node
    to generate the representation, the clear discrimination of those two groups is
    possible.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图2.4*中，很容易看出具有相似结构的节点彼此靠近，并且与具有不同结构的节点相距较远。观察Node2Vec如何很好地区分第1组和第3组也非常有趣。由于该算法使用每个节点的邻接信息来生成表示，因此这两个组之间的清晰区分是可能的。
- en: 'Another example on the same graph can be performed using the **Edge to Vector**
    (**Edge2Vec**) algorithm in order to generate a mapping for the edges for the
    same graph, *G*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一张图上，可以使用**边到向量**（**Edge2Vec**）算法进行另一个示例，以便为同一图*G*生成边的映射：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code, we have done the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们已经做了以下工作：
- en: We generated a barbell graph (described in the previous chapter).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成了一个哑铃图（在上一章中描述）。
- en: The `HadamardEmbedder` embedding algorithm is applied to the result of the Node2Vec
    algorithm (`keyed_vectors=model.wv`) used in order to map each edge of the graph
    in a vector of two dimensions.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`HadamardEmbedder`嵌入算法应用于Node2Vec算法的结果（`keyed_vectors=model.wv`），以便将图中的每个边映射到二维向量。
- en: Finally, the two-dimensional vectors generated by the embedding algorithm, representing
    the nodes of the original graph, are plotted.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，嵌入算法生成的二维向量，代表原始图中的节点，被绘制出来。
- en: 'The results are shown in *Figure 2.5*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如*图2.5*所示：
- en: '![Figure 2.5 – Application of the Hadamard algorithm to a graph (left) to generate
    the embedding vector of its edges (right)](img/B16069_02_05.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 将Hadamard算法应用于图（左）以生成其边的嵌入向量（右）](img/B16069_02_05.jpg)'
- en: Figure 2.5 – Application of the Hadamard algorithm to a graph (left) to generate
    the embedding vector of its edges (right)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 将Hadamard算法应用于图（左）以生成其边的嵌入向量（右）
- en: As for node embedding, in *Figure 2.5*, we reported the results of the edge
    embedding algorithm. From the figure, it is easy to see that the edge embedding
    algorithm clearly identifies similar edges. As expected, edges belonging to groups
    1, 2, and 3 are clustered in well-defined and well-grouped regions. Moreover,
    the (6,7) and (10,11) edges, belonging to groups 4 and 5, respectively, are well
    clustered in specific groups.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于节点嵌入，在*图2.5*中，我们报告了边嵌入算法的结果。从图中可以看出，边嵌入算法清楚地识别了相似的边。正如预期的那样，属于第1、2和3组的边在定义良好且分组清晰的区域内聚集。此外，属于第4组的(6,7)边和属于第5组的(10,11)边在特定的组内很好地聚集。
- en: 'Finally, we will provide an example of a **Graph to Vector** (**Grap2Vec**)
    embedding algorithm. This algorithm maps a single graph in a vector. As for another
    example, we will discuss this algorithm in more detail in the next chapter. In
    the following code block, we provide a Python example showing how to use the Graph2Vec
    algorithm in order to generate the embedding representation on a set of graphs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将提供一个**图到向量**（**Grap2Vec**）嵌入算法的示例。该算法将单个图映射到向量。至于另一个示例，我们将在下一章中更详细地讨论此算法。在下面的代码块中，我们提供了一个Python示例，说明如何使用Graph2Vec算法在图集上生成嵌入表示：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, the following has been done:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，已经做了以下工作：
- en: 20 Watts-Strogatz graphs (described in the previous chapter) have been generated
    with random parameters.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 20个Watts-Strogatz图（在上一章中描述）已使用随机参数生成。
- en: We have then executed the graph embedding algorithm in order to generate a two-dimensional
    vector representation of each graph.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经执行了图嵌入算法，以生成每个图的二维向量表示。
- en: Finally, the generated vectors are plotted in their Euclidean space.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，生成的向量被绘制在其欧几里得空间中。
- en: 'The results of this example are shown in *Figure 2.6*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的结果在*图2.6*中显示：
- en: '![Figure 2.6 – Plot of two embedding vectors generated by the Graph2Vec algorithm
    applied to 20 randomly generated Watts-Strogatz graphs (left). Extraction of two
    graphs with a large Euclidean distance (Graph 12 and Graph 8 on the top right)
    and two graphs with a low Euclidean distance (Graph 14 and Graph 4 on the bottom
    right) is shown](img/B16069_02_06.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6 – 使用Graph2Vec算法应用于20个随机生成的Watts-Strogatz图生成的两个嵌入向量的图（左）。展示了两个具有较大欧几里得距离的图（图12和图8在右上角）以及两个具有较低欧几里得距离的图（图14和图4在右下角）](img/B16069_02_06.jpg)'
- en: Figure 2.6 – Plot of two embedding vectors generated by the Graph2Vec algorithm
    applied to 20 randomly generated Watts-Strogatz graphs (left). Extraction of two
    graphs with a large Euclidean distance (Graph 12 and Graph 8 at the top right)
    and two graphs with a low Euclidean distance (Graph 14 and Graph 4 at the bottom
    right) is shown
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 使用Graph2Vec算法应用于20个随机生成的Watts-Strogatz图生成的两个嵌入向量的图（左）。展示了两个具有较大欧几里得距离的图（图12和图8在右上角）以及两个具有较低欧几里得距离的图（图14和图4在右下角）
- en: As we can see from *Figure 2.6*, graphs with a large Euclidean distance, such
    as graph 12 and graph 8, have a different structure. The former is generated with
    the `nx.watts_strogatz_graph(20,20,0.2857)` parameter and the latter with the
    `nx.watts_strogatz_graph(13,6,0.8621)` parameter. In contrast, a graph with a
    low Euclidean distance, such as graph 14 and graph 8, has a similar structure.
    Graph 14 is generated with the `nx.watts_strogatz_graph(9,9,0.5091)` command,
    while graph 4 is generated with `nx.watts_strogatz_graph(10,5,0.5659)`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2.6*所示，具有较大欧几里得距离的图，例如图12和图8，具有不同的结构。前者使用`nx.watts_strogatz_graph(20,20,0.2857)`参数生成，后者使用`nx.watts_strogatz_graph(13,6,0.8621)`参数生成。相比之下，具有较低欧几里得距离的图，例如图14和图8，具有相似的结构。图14使用`nx.watts_strogatz_graph(9,9,0.5091)`命令生成，而图4使用`nx.watts_strogatz_graph(10,5,0.5659)`生成。
- en: 'In the scientific literature, a plethora of embedding methods has been developed.
    We will describe in detail and use some of them in the next section of this book.
    These methods are usually classified into two main types: *transductive* and *inductive*,
    depending on the update procedure of the function when new samples are added.
    If new nodes are provided, transductive methods update the model (for example,
    re-train) to infer information about the nodes, while in inductive methods, models
    are expected to generalize to new nodes, edges, or graphs that were not observed
    during training.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在科学文献中，已经开发出大量的嵌入方法。我们将在本书的下一节中详细描述并使用其中的一些方法。这些方法通常分为两大类：*归纳*和*演绎*，这取决于在添加新样本时函数的更新过程。如果提供了新节点，归纳方法会更新模型（例如，重新训练）以推断有关节点的信息，而在归纳方法中，模型预期可以推广到训练期间未观察到的新的节点、边或图。
- en: The taxonomy of graph embedding machine learning algorithms
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图嵌入机器学习算法的分类
- en: 'A wide variety of methods to generate a compact space for graph representation
    have been developed. In recent years, a trend has been observed of researchers
    and machine learning practitioners converging toward a unified notation to provide
    a common definition to describe such algorithms. In this section, we will be introduced
    to a simplified version of the taxonomy defined in the paper *Machine Learning
    on Graphs: A Model and Comprehensive Taxonomy* ([https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出多种方法来为图表示生成紧凑的空间。近年来，研究人员和机器学习从业者趋向于统一符号，以提供一个共同的定义来描述此类算法。在本节中，我们将介绍论文《图上的机器学习：一个模型和综合分类》中定义的分类的简化版本（[https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)）。
- en: In this formal representation, every graph, node, or edge embedding method can
    be described by two fundamental components, named the encoder and the decoder.
    The **encoder** (**ENC**) maps the input into the embedding space, while the **decoder**
    (**DEC**) decodes structural information about the graph from the learned embedding
    (*Figure 2.7*).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种形式表示中，每个图、节点或边嵌入方法都可以用两个基本组件来描述，称为编码器和解码器。**编码器**（**ENC**）将输入映射到嵌入空间，而**解码器**（**DEC**）从学习到的嵌入中解码关于图的结构信息（*图2.7*）。
- en: 'The framework described in the paper follows an intuitive idea: if we are able
    to encode a graph such that the decoder is able to retrieve all the necessary
    information, then the embedding must contain a compressed version of all this
    information and can be used to downstream machine learning tasks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中描述的框架遵循一个直观的想法：如果我们能够将一个图编码，使得解码器能够检索所有必要的信息，那么嵌入必须包含所有这些信息的压缩版本，并且可以用于下游机器学习任务：
- en: '![Figure 2.7 – Generalized encoder (ENC) and decoder (DEC) architecture for
    embedding algorithms'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7 – 嵌入算法的通用编码器（ENC）和解码器（DEC）架构'
- en: '](img/B16069_02_07.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_02_07.jpg)'
- en: Figure 2.7 – Generalized encoder (ENC) and decoder (DEC) architecture for embedding
    algorithms
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 嵌入算法的通用编码器（ENC）和解码器（DEC）架构
- en: In many graph-based machine learning algorithms for representation learning,
    the decoder is usually designed to map pairs of node embeddings to a real value,
    usually representing the proximity (distance) of the nodes in the original graphs.
    For example, it is possible to implement the decoder such that, given the embedding
    representation of two nodes, ![](img/Formula_02_012.png) and ![](img/Formula_02_013.png),
    ![](img/Formula_02_014.png) if in the input graph an edge connecting the two nodes,
    ![](img/Formula_02_015.png), exists. In practice, more effective *proximity functions*
    can be used to measure the similarity between nodes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多基于图的机器学习算法中，用于表示学习，解码器通常被设计为将节点嵌入对映射到实值，通常表示原始图中节点的邻近度（距离）。例如，可以实现解码器，给定两个节点的嵌入表示，![](img/Formula_02_012.png)和![](img/Formula_02_013.png)，![](img/Formula_02_014.png)如果输入图中存在连接两个节点的边，![](img/Formula_02_015.png)。在实践中，更有效的*邻近度函数*可以用来衡量节点之间的相似性。
- en: The categorization of embedding algorithms
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入算法的分类
- en: 'Inspired by the general framework depicted in *Figure 2.7*, we will now provide
    a categorization of the various embedding algorithms into four main groups. Moreover,
    in order to help you to better understand this categorization, we shall provide
    simple code snapshots in pseudo-code. In our pseudo-code formalism, we denote
    `G` as a generic `networkx` graph, with `graphs_list` as a list of `networkx`
    graphs and `model` as a generic embedding algorithm:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 受到*图2.7*中描述的通用框架的启发，我们现在将对各种嵌入算法进行分类，分为四大主要类别。此外，为了帮助您更好地理解这种分类，我们将提供简单的伪代码代码片段。在我们的伪代码形式中，我们用`G`表示一个通用的`networkx`图，`graphs_list`表示`networkx`图的列表，`model`表示一个通用的嵌入算法：
- en: '`graphs_list` (line 2). Unsupervised and supervised shallow embedding methods
    will be described, respectively, in [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Unsupervised Graph Learning*, and [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`graphs_list`（第2行）。无监督和监督浅层嵌入方法将分别在[*第3章*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046)，“无监督图学习”和[*第4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)，“监督图学习”中描述。'
- en: '`graphs_list` (line 1). Once the model is fitted on the input training set,
    it is possible to use it to generate the embedding vector of a new unseen graph,
    `G`. Graph autoencoding methods will be described in [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Unsupervised Graph Learning*.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`graphs_list`（第1行）。一旦模型在输入训练集上拟合，就可以用它来生成一个未见过的图`G`的嵌入向量。图自动编码方法将在[*第3章*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046)，“无监督图学习”中描述。'
- en: '**Neighborhood aggregation methods**: These algorithms can be used to extract
    embeddings at the graph level, where nodes are labeled with some properties. Moreover,
    as for the graph autoencoding methods, the algorithms belonging to this class
    are able to learn a general mapping function, ![](img/Formula_02_017.png), also
    capable of generating the embedding vector for unseen instances.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**邻域聚合方法**：这些算法可以用于在图级别提取嵌入，其中节点被标记为某些属性。此外，对于图自动编码方法，属于此类别的算法能够学习一个通用的映射函数，![](img/Formula_02_017.png)，也能够为未见实例生成嵌入向量。'
- en: A nice property of those algorithms is the possibility to build an embedding
    space where not only the internal structure of the graph is taken into account
    but also some external information, defined as properties of its nodes. For instance,
    with this method, we can have an embedding space capable of identifying, at the
    same time, graphs with similar structures and different properties on nodes. Unsupervised
    and supervised neighborhood aggregation methods will be described in [*Chapter
    3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046), *Unsupervised Graph Learning*,
    and [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064), *Supervised
    Graph Learning*, respectively.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些算法的一个良好特性是能够构建一个嵌入空间，不仅考虑了图的内部结构，还考虑了一些外部信息，定义为节点属性。例如，使用这种方法，我们可以有一个能够同时识别具有相似结构和节点上不同属性的图的嵌入空间。无监督和监督的邻域聚合方法将分别在[*第3章*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046)
    *无监督图学习*和[*第4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064) *监督图学习*中描述。
- en: '**Graph regularization methods**: Methods based on graph regularization are
    slightly different from the ones listed in the preceding points. Here, we do not
    have a graph as input. Instead, the objective is to learn from a set of features
    by exploiting their "interaction" to regularize the process. In more detail, a
    graph can be constructed from the features by considering feature similarities.
    The main idea is based on the assumption that nearby nodes in a graph are likely
    to have the same labels. Therefore, the loss function is designed to constrain
    the labels to be consistent with the graph structure. For example, regularization
    might constrain neighboring nodes to share similar embeddings, in terms of their
    distance in the L2 norm. For this reason, the encoder only uses *X* node features
    as input.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图正则化方法**：基于图正则化的方法与前面列出的方法略有不同。在这里，我们没有图作为输入。相反，目标是通过对特征集的“交互”来学习，从而正则化过程。更详细地说，可以通过考虑特征相似性从特征构建一个图。主要思想基于这样的假设：图中相邻的节点很可能具有相同的标签。因此，损失函数被设计成约束标签与图结构的一致性。例如，正则化可能约束相邻节点在L2范数距离上共享相似的嵌入。因此，编码器只使用*X*节点特征作为输入。'
- en: The algorithms belonging to this family learn a function, ![](img/Formula_02_018.png),
    that maps a specific set of features (![](img/Formula_02_019.png)) to an embedding
    vector. As for the graph autoencoding and neighborhood aggregation methods, this
    algorithm is also able to apply the learned function to new, unseen features.
    Graph regularization methods will be described in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属于这个家族的算法学习一个函数，![](img/Formula_02_018.png)，它将一组特定的特征(![](img/Formula_02_019.png))映射到一个嵌入向量。至于图自动编码和邻域聚合方法，这个算法也能够将学习到的函数应用于新的、未见过的特征。图正则化方法将在[*第4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)
    *监督图学习*中描述。
- en: For algorithms belonging to the group of shallow embedding methods and neighborhood
    aggregation methods, it is possible to define an *unsupervised* and *supervised*
    version. The ones belonging to graph autoencoding methods are suitable in unsupervised
    tasks, while the algorithms belonging to graph regularization methods are used
    in semi-supervised/supervised settings.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于属于浅层嵌入方法和邻域聚合方法的算法，可以定义一个*无监督*和*监督*版本。属于图自动编码方法的算法适用于无监督任务，而属于图正则化方法的算法用于半监督/监督设置。
- en: For unsupervised algorithms, the embedding of a specific dataset is performed
    only using the information contained in the input dataset, such as nodes, edges,
    or graphs. For the supervised setting, external information is used to guide the
    embedding process. That information is usually classed as a label, such as a pair,
    *<Gi,yi>*, that assigns to each graph a specific class. This process is more complex
    than the unsupervised one since the model tries to find the best vectorial representation
    in order to find the best assignment of a label to an instance. In order to clarify
    this concept, we can think, for instance, of the *convolutional neural networks*
    for image classification. During their training process, neural networks try to
    classify each image into the right class by performing the fitting of various
    convolutional filters at the same time. The goal of those convolutional filters
    is to find a compact representation of the input data in order to maximize the
    prediction performances. The same concept is also valid for supervised graph embedding,
    where the algorithm tries to find the best graph representation in order to maximize
    the performance of a class assignment task.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无监督算法，特定数据集的嵌入仅使用输入数据集中包含的信息执行，例如节点、边或图。对于监督设置，使用外部信息来指导嵌入过程。这些信息通常被归类为标签，例如一对*<Gi,yi>*)，将每个图分配给一个特定的类别。这个过程比无监督过程更复杂，因为模型试图找到最佳的向量表示，以便找到对实例的最佳标签分配。为了阐明这个概念，我们可以以图像分类中的*卷积神经网络*为例。在它们的训练过程中，神经网络试图通过同时拟合各种卷积滤波器来将每个图像分类到正确的类别。这些卷积滤波器的目标是找到输入数据的紧凑表示，以最大化预测性能。同样的概念也适用于监督图嵌入，其中算法试图找到最佳的图表示，以最大化类别分配任务的性能。
- en: 'From a more mathematical perspective, all these models are trained with a proper
    loss function. This function can be generalized using two terms:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从更数学的角度来看，所有这些模型都是通过适当的损失函数进行训练的。这个函数可以使用两个术语进行泛化：
- en: The first is used in supervised settings to minimize the difference between
    the prediction and the target.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个用于监督设置中，以最小化预测值与目标值之间的差异。
- en: The second is used to evaluate the similarity between the input graph and the
    one reconstructed after the ENC + DEC steps (which is the structure reconstruction
    error).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个用于评估输入图与经过ENC + DEC步骤后重建的图之间的相似性（即结构重建误差）。
- en: 'Formally, it can be defined as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，它可以定义为以下内容：
- en: '![](img/Formula_02_020.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/Formula_02_020.jpg)'
- en: Here, ![](img/Formula_02_021.png) is the loss function in the supervised settings.
    The model is optimized to minimize, for each instance, the error between the right
    (![](img/Formula_02_022.png)) and the predicted class (![](img/Formula_02_023.png)).
    ![](img/Formula_02_024.png) is the loss function representing the reconstruction
    error between the input graph (![](img/Formula_02_025.png)) and the one obtained
    after the ENC + DEC process (![](img/Formula_02_026.png)). For unsupervised settings,
    we have the same loss but ![](img/Formula_02_027.png), since we do not have a
    target variable to use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式图片](img/Formula_02_021.png)是监督设置中的损失函数。模型被优化以最小化每个实例中正确值(![公式图片](img/Formula_02_022.png))与预测类别(![公式图片](img/Formula_02_023.png))之间的误差。![公式图片](img/Formula_02_024.png)是表示输入图(![公式图片](img/Formula_02_025.png))与经过ENC
    + DEC过程后获得的图(![公式图片](img/Formula_02_026.png))之间重建误差的损失函数。对于无监督设置，我们有相同的损失函数，但![公式图片](img/Formula_02_027.png)，因为我们没有目标变量可以使用。
- en: It is important to highlight the main role that these algorithms play when we
    try to solve a machine learning problem on a graph. They can be used *passively*
    in order to transform a graph into a feature vector suitable for a classical machine
    learning algorithm or for data visualization tasks. But they can also be used
    *actively* during the learning process, where the machine learning algorithm finds
    a compact and meaningful solution to a specific problem.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图在图上解决机器学习问题时，这些算法扮演着非常重要的角色。它们可以被*被动地*使用，以便将图转换为适合经典机器学习算法或数据可视化任务的特性向量。但它们也可以在学习过程中*主动地*使用，此时机器学习算法会找到一个紧凑且具有意义的解决方案来解决特定问题。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we refreshed some basic *machine learning* concepts and discovered
    how they can be applied to graphs. We defined basic *graph machine learning* terminology
    with a particular focus on *graph representation learning*. A taxonomy of the
    main graph machine learning algorithms was presented in order to clarify what
    differentiates the various ranges of solutions developed over the years. Finally,
    practical examples were provided to begin understanding how the theory can be
    applied to practical problems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们刷新了一些基本的机器学习概念，并探讨了它们如何应用于图。我们定义了基本的*图机器学习*术语，特别关注*图表示学习*。为了阐明多年来开发的各个解决方案的不同之处，我们提出了主要图机器学习算法的分类。最后，提供了实际例子，以开始理解理论如何应用于实际问题。
- en: In the next chapter, we will revise the main graph-based machine learning algorithms.
    We will analyze their behavior and see how they can be used in practice.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾主要的基于图的机器学习算法。我们将分析它们的行为，并了解它们在实际中的应用方式。
