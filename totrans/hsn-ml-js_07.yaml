- en: Forecasting with Regression Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to take a brief look at forecasting using regression
    algorithms. We'll additionally discuss time-series analysis and how we can use
    techniques from digital-signal processing to aid in our analysis. By the end of
    the chapter, you will have seen a number of patterns commonly found in time-series
    and continuous-valued data and will have an understanding of which types of regressions
    fit on which types of data. Additionally, you will have learned a few digital
    signal processing techniques, such as filtering, seasonality analysis, and Fourier
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forecasting is a very broad concept that covers many types of tasks. This chapter
    will provide you with an initial toolbox of concepts and algorithms that apply
    broadly to time-series data. We will focus on the fundamentals, and discuss the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression versus classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear, exponential, and polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-series analysis basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-pass and high-pass filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonality and subtractive analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourier analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These concepts build an essential toolbox that you can use when working with
    real-world forecasting and analysis problems. There are many other tools that
    apply to specific situations, but I consider these topics to be the absolute essentials.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by looking at the similarities—and differences—between regression
    and classification in **machine learning** (**ML**).
  prefs: []
  type: TYPE_NORMAL
- en: Regression versus classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much of this book has been involved with classification tasks, where the objective
    of the analysis is to fit a data point to one of a number of predefined classes
    or labels. When classifying data, you are able to judge your algorithm's accuracy
    by comparing predictions to true values; a guessed label is either correct or
    incorrect. In classification tasks, you can often determine the likelihood or
    probability that a guessed label fits the data, and you typically choose the label
    with the maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare and contrast classification tasks to regression tasks. Both are
    similar in that the ultimate goal is to make a prediction, informed by prior knowledge
    or data. Both are similar in that we want to create some kind of function or logic
    that maps input values to output values, and make that mapping function both as
    accurate and as generalized as possible. However, the major difference between
    regression and classification is that in regression, your goal is to determine
    the *quantity* of a value rather than its label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you have historical data about the processing load over time on a server
    that you manage. This data is *time-series*, because the data evolves over time.
    The data is also *continuous* (as opposed to *discrete*), because the output values
    can be any real number: 1, or 2.3, or 2.34353, and so on. The goal in time-series
    analysis or regression analysis is not to label the data, but rather to predict
    what your server load will be next Thursday evening at 20:15 p.m., for instance.
    To accomplish this goal, you must analyze the time-series data and attempt to
    extract patterns from it, and then use those patterns to make a future prediction.
    Your prediction will also be a real and continuous number, such as *I predict
    the server load next Thursday night will be 2.75*.'
  prefs: []
  type: TYPE_NORMAL
- en: In classification tasks, you can judge the accuracy of your algorithm by comparing
    predictions to true values and counting how many predictions were correct or incorrect.
    Because regression tasks involve themselves with continuous values, one cannot
    simply determine whether a prediction was correct or incorrect. If you predict
    that server load will be 2.75 and it ends up truly being 2.65, can you say that
    the prediction was correct? Or incorrect? What about if it ends up being 2.74?
    When classifying *spam* or *not spam*, you either get the prediction right or
    you get it wrong. When you compare continuous values, however, you can only determine
    how close you got the prediction and therefore must use some other metric to define
    the accuracy of your algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you will use a different set of algorithms to analyze continuous
    or time-series data than you would use for classification tasks. However, there
    are some ML algorithms that can handle both regression and classification tasks
    with minor modifications. Most notably, decision trees, random forests, and neural
    networks can all be used both for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Least-squares regressions techniques, such as linear regression, polynomial
    regression, power law regression, and others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trend analysis or smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonality analysis or pattern subtraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing regression analysis, there are two primary and overarching goals.
    First, we want to determine and identify any underlying, systemic patterns in
    the data. If we can identify the systemic patterns, we may be able to identify
    the phenomena underlying the patterns and develop a deeper understanding of the
    system as a whole. If, through your analysis, you find that there is a pattern
    that repeats itself every 16 hours, you will be in a much better position to figure
    out what phenomenon is causing the pattern and take action. As with all ML tasks,
    that 16-hour pattern may be buried deep within the data and may not be identifiable
    at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: The second major goal is to use the knowledge of the underlying patterns to
    make future predictions. The predictions that you make will only be as good as
    the analysis that powers the predictions. If there are four different systemic
    patterns in your data, and you've only identified and modeled three of them, your
    predictions may be inaccurate since you haven't fully modeled the real-world phenomena
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving both of these goals relies on your ability to identify and formally
    (that is, mathematically) describe the patterns and phenomena. In some cases,
    you may not be able to fully identify the root cause of a pattern; even then,
    if the pattern is reliable and your analysis is good, you will still be able to
    predict the future behavior of the system even if you don't fully understand the
    cause. This is the case with all ML problems; ML ultimately analyzes behaviors
    and results—the things we can measure—but having a deep understanding of the causes
    can only help.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all ML problems, we must also contend with noise. In classification problems,
    noise can take many forms, such as missing or incorrect values, or undefinable
    human behavior. Noise can take many forms in regression problems as well: sensors
    may be susceptible to environmental noise, there can be random fluctuations in
    the underlying processes, or noise can be caused by many small, hard-to-predict,
    systemic factors.'
  prefs: []
  type: TYPE_NORMAL
- en: Noise always makes patterns more difficult to identify, whether you're performing
    regression or classification analysis. In regression analysis, your goal is to
    be able to separate the systemic behaviors (the actual patterns) from the random
    sources of noise in the data. In some cases, it's important to also model the
    noise as a behavior, because the noise itself can have a significant effect on
    your predictions; in other cases, the noise can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the difference between systemic patterns and noise, consider
    the following dataset. There are no units on the graph, as this is just a conceptual
    example of some dependent parameter, *Y*, that varies with some independent parameter, *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/342e53f9-ac57-4b05-9286-d8b22306ed87.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we can clearly see the difference between the systemic pattern
    and the noise. The systemic pattern is the steady, linear growth—the Y-values
    generally increase as the X-values increase, despite some fluctuations from point
    to point due to noise. By modeling the systemic pattern in this data, we would
    be able to make a reasonable prediction for what the Y-value will be when the
    X-value is 75, or 100, or -20\. Whether or not the noise is significant will depend
    on the specific application; you can either ignore the noise, or you can model
    it and include it in your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 1](370a39e1-e618-475d-9722-9b4315a9a6ee.xhtml), *Exploring the
    Potential of JavaScript*, we learned about one technique for dealing with noise:
    smoothing with a moving average. Instead of graphing individual points, we can
    take groups of three points together and plot their averages. If the noise is
    truly random and distributed evenly (that is, the average of all the effects of
    noise comes out close to zero), a moving average will tend to cancel out some
    of the noise. If you are averaging three points, and the effect due to noise on
    each of those points adds +1, -2, and +1.2 to each point, respectively, the moving
    average will reduce the total effect of the noise to +0.2\. When we graph the
    moving average, we will typically find a smoother pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ee9697-e607-458f-9c5c-fa5dc5bd941d.png)'
  prefs: []
  type: TYPE_IMG
- en: The moving average has reduced the effect of the noise, and helps us focus on
    the systemic pattern a bit more—but we are not much closer to being able to predict
    future values, such as when *X* is 75\. The moving average only helps us reduce
    the effect of noise on the data points within the dataset. When you look at the
    Y-value when *X = 4*, for instance, the measured value is around 21 while the
    smoothed value is 28\. In this case, the smoothed value of 28 better represents
    the *systemic pattern* at *X = 4*, even though the actual measured value at this
    point was 21\. Most likely, the big difference between the measurement and the
    systemic pattern was caused by a significant source of random noise when this
    measurement was taken.
  prefs: []
  type: TYPE_NORMAL
- en: Be cautious when dealing with noise. It's important to recognize that, in the
    preceding example, the actual measured Y-value was indeed 21 at *X = 4*. The smoothed,
    moving average version is an idealization. It is our attempt to cut through the
    noise in order to see the signal, but we cannot forget that the actual measurement
    was significantly affected by noise. Whether this fact is important to your analysis
    depends significantly on the problem you are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: How, then, do we approach the problem of predicting a future value of this data?
    The moving average may help us when *interpolating* data, but not when *extrapolating*
    to a future X-value. You can, of course, make a guess as to what the value will
    be when *X = 75*, since this example is simple and easy to visualize. However,
    since this is a book about ML, we can assume that real-world problems will not
    be so easy to analyze by eye, and we will need to introduce new tools.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is the *regression.* As with all predictive ML
    problems, we want to create some kind of abstract function that can map input
    values to output values, and use that function to make predictions. In classification
    tasks, that mapping function may be a Bayesian predictor or a heuristic based
    on random forests. In regression tasks, the mapping function will often be a mathematical
    function that describes a line, or a polynomial, or some other kind of shape that
    fits the data well.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''ve ever graphed data in Excel or Google Sheets, there''s a good chance
    you have already used linear regressions. The *trendline* feature of these programs
    performs a linear regression in order to determine a mapping function that best
    fits the data. The following graph is a trendline determined by a *linear regression*,
    which is a type of algorithm that is used to find the mathematical line that best
    fits the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a85c0ab2-389f-42fe-b705-a3715dc8ecf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, Excel gives us another piece of information, called the **R² value**,
    which is a representation of how well the trendline fits the data. An R² value
    closer to 1.0 indicates that the trendline explains much of the variance between
    points; a low R² value indicates that the model does not explain the variance.
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between the trendline and the moving average we saw earlier
    is that the trendline is an actual mathematical model. When you find a trendline
    with a linear regression, you will have a mathematical formula that describes
    the entire line. The moving average only exists where the data points exist; we
    can only have a moving average between X = 0 and X = 50\. The trendline, on the
    other hand, is described by the mathematical formula for a straight line, and
    it extends out to infinity both to the left and to the right. If you know the
    formula for the trendline, you can plug in *any* value of *X* to that formula
    and get a prediction for the value of *Y*. If, for example, you find that the
    formula for a line is *Y = 2.5 x X + 22*, you can plug in *X = 75* and you will
    get a prediction of *Y = 2.5 x 75 + 22*, or *Y = 209.5*. There is no way to get
    such a prediction from a moving average.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is just one type of regression algorithm, specifically used
    to find a straight line that fits the data. In this chapter, we will explore several
    other types of regression algorithms, each with a different shape. In all cases,
    you can use a metric that describes how well the regression fits the data. Typically,
    this metric will be **root mean squared error** (**RMSE**), which is the square
    root of the average of the squared error for each point compared to the trendline.
    Most regression algorithms are *least-squares* regressions, which aim to find
    the trendline that minimizes the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at several examples of regression shapes and how to fit them
    to data in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into the first example, let''s take a minute to set up our project
    folder and dependencies. Create a new folder called `Ch7-Regression`, and inside
    that folder add the following `package.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then run the `yarn install` command from the command line to install all dependencies.
    Next, create a folder called `src`, and add an empty file called `index.js`. Finally,
    download the `data.js` file from the book's GitHub repository into the `src` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we''re going to work on the noisy linear data from the previous
    section. As a reminder, the data itself looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c6d5b17-4315-459f-8800-ff764c663ad3.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal is to find the formula for a line that fits the data and make a prediction
    for a future value when *X = 75*. We'll use Tom Alexander's `regression` library,
    which can perform a number of types of regressions and also provides the capability
    to make predictions based on the resultant regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `index.js` file, add the following import statements to the top of the
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As with all ML problems, you should start by visualizing your data and trying
    to understand the overall shape of the data before you choose the algorithm. In
    this case, we can see that the data follows a linear trend so we will choose the
    linear regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear regression, the goal is to determine the parameters for the formula
    for a line that best fits the data. The formula for a straight line has the following
    form: *y = mx + b*, sometimes written as *y = ax + b*, where *x* is the input
    variable or independent variable, *y* is the target or dependent variable, *m*
    (or *a*) is the *slope* or *gradient* of the line, and *b* is the *y-intercept*
    of the line (the Y-value of the line when *X = 0*). Therefore, the minimum requirements
    for the output of a linear regression are the values for *a* and *b*, the only
    two parameters that determine the shape of the line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following import lines to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Performing a linear regression on the data will return a model; the model essentially
    encapsulates the values of *a* and *b*, or the slope and the intercept of the
    line. This particular library not only returns the line's parameters in the `linearModel.equation`
    property, but also gives us a string representation of the line formula, calculates
    the R² fit of the regression, and gives us a method, called `predict`, which we
    can use to plug a new X-value into the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the code by issuing the `yarn start` command from the command line. You
    should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The regression has determined that the formula for the line that best fits
    our data is *y = 2.47x + 22.6*. The original formula I used to create this test
    data was *y = 2.5x + 22*. The slight difference between the determined equation
    and the actual equation is due to the effect of the random noise I added to the
    dataset. As you can see, the linear regression has done a good job of looking
    past the noise and discovering the underlying pattern. If we chart these results,
    we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec75338a-7bc4-4a74-9a00-aebf94f1a9c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of the regression, as seen in the preceding graph, are exactly the
    same as the results given by the trendline feature of Excel or Google Sheets,
    with the difference being that we generated the trendline in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: When asked to predict a future value where *X = 75*, the regression returns
    *Y = 207.85*. Using my original formula, the true value would have been 209.5\.
    The amount of noise I added to the data amounts to a random and uniform noise
    level of +/- 12.5 for any given point, so the predicted value turns out to be
    very close to the actual value when you account for the uncertainty due to noise.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted, however, that errors in regression compound as you make
    predictions further away from the domain of the original data. When predicting
    for X = 75, the error between the prediction and the actual value is only 1.65\.
    If we were to predict for X = 1000, on the other hand, the true formula would
    return 2,522, but the regression would predict 2,492.6\. The error between the
    actual value and the prediction at X = 1000 is now 29.4, nearly 30, far beyond
    the uncertainty due to noise. Regressions are very useful predictors, but you
    must always keep in mind that these errors can compound and therefore the predictions
    will become less accurate as you get further away from the domain of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this type of prediction error lies in the regression for the
    slope of the equation. The slope of the line in the original equation is 2.5\.
    This means that for every unit change in the *X* value, we should expect a change
    of 2.5 units in the *Y* value. The regression, on the other hand, determines a
    slope of 2.47\. Therefore, for every unit change in the *X* value, the regression
    is inheriting a slight error of -0.03\. Predicted values will be slightly lower
    than actual values by that amount, multiplied by the *X* distance for your prediction.
    For every 10 units of *X*, the regression inherits a total error of -0.3\. For
    every 100 units of *X*, the regression inherits an error of -3.0, and so on. When
    we extrapolate out to X=1000, we've inherited an error of -30, because that slight
    per-unit error of -0.03 gets multiplied by the distance we travel along the *x*
    axis.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at values within our data domain—values between X = 0 and X = 50—we
    only get very small prediction errors due to this slight difference in slope.
    Within our data domain, the regression has corrected for the error in slope by
    slightly increasing the y-intercept value (the original is +22, the regression
    returned +22.6). Due to the noise in our data, the regression formula of *y =
    2.47x + 22.6* is a better fit than the actual formula of *y = 2.5x + 22*. The
    regression finds a slightly less-steep slope and makes up for it by raising the
    entire line by 0.6 units (the difference in y-intercepts), because this fits both
    the data and the noise better. This model fits the data very well between X =
    0 and X = 50, but when we try to predict the value when X = 1000, the slight +0.6
    modification in the y-intercept is no longer enough to make up for the decreased
    slope over such a vast distance.
  prefs: []
  type: TYPE_NORMAL
- en: Linear trends like the one found in this example are very common. There are
    many types of data that exhibit linear relationships and can be modeled simply
    yet accurately as long as you don't attempt to over-extrapolate the data. In the
    next example, we'll take a look at exponential regressions.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – exponential regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common trend in continuous data patterns is *exponential growth**,*
    which is also commonly seen as *exponential decay. *In exponential growth, a future
    value is proportionally related to the current value. The general formula for
    this type of growth can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = y[0] (1 + r) x*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *y[0]* is the quantity's initial value (when *x* = 0), and *r* is the
    growth rate of the quantity.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you are investing money in the stock market and expect a 5%
    rate of return per year (*r = 0.05*) with an initial investment of $10,000, after
    five years you can expect $12,763\. The exponential growth formula applies here
    because the amount of money you have next year is proportionally related to the
    amount of money you have this year, and the amount of money you have two years
    from now is related to the money you have next year, and so on. This only applies
    if you reinvest your returns, causing the amount of money you're actively investing
    to increase with each passing year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another form for the exponential growth equation is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = ae^(bx)*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *b = ln(1 + r)*, *a* is the initial value *y[0]*, and *e* is Euler's constant
    of approximately 2.718\. This slight transformation in form is easier to manipulate
    mathematically and is typically the preferred form used by mathematicians for
    analysis. In our stock market investment example, we can rewrite the formula for
    five year growth as *y = 10000*e^(ln(1.05)*5)*, and we will get the same result
    of $12,763.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exponential growth is sometimes called **hockey-stick growth** due to the shape
    of the curve resembling the outline of a hockey stick:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dd1b88a-4d6a-472b-aaeb-21d67069bb13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some examples of exponential growth include:'
  prefs: []
  type: TYPE_NORMAL
- en: Population growth; that is, world population or bacterial culture growth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viral growth, such as analysis of disease infections or the viral spread of
    YouTube videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive feedback loops in mechanics or signal processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Economic growth, including compound interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing power of computers under Moore's law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that, in almost all circumstances, exponential growth
    is unsustainable. For example, if you are predicting the growth of a bacterial
    colony in a Petri dish, you may observe exponential growth for a short time, however
    once the Petri dish runs out of food and space, other factors will take over and
    the growth will no longer be exponential. Similarly, if your website incentivizes
    new users to invite their friends, you may see exponential growth in your membership
    for a while, but eventually you will saturate the market and growth will slow.
    Therefore, you must be cautious in analyzing exponential growth models and understand
    that the conditions that fuel exponential growth may ultimately change. Similar
    to linear regression, an exponential regression will only apply to modest extrapolations
    of your data. Your website's membership may grow exponentially for a year, but
    not for ten years; you cannot have 20 billion members if there are only seven
    billion people on Earth.
  prefs: []
  type: TYPE_NORMAL
- en: If the growth rate, *r*, or the parameter *k* (called the **growth constant**),
    is negative, you will have exponential decay rather than exponential growth. Exponential
    decay is still exponential growth in the sense that future values are proportional
    to the current value, however in exponential decay, future values are proportionately
    *smaller* than the current value.
  prefs: []
  type: TYPE_NORMAL
- en: One real-world use of exponential decay is in carbon-dating analysis. Because
    the radioactive carbon-14 isotope decays into non-radioactive carbon-12 with a
    half-life of 5,730 years—meaning that, in aggregate, half the carbon-14 decays
    into carbon-12 every 5,730 years—scientists can use the exponential decay formula
    to figure out how old an object must be in order to have the appropriate ratio
    of carbon-14 to carbon-12.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential decay is also seen in physics and mechanics, particularly in the
    spring-mass-damper problem. It can also be used by coroners and medical examiners
    to determine the time of death of a subject, based on the fact that a warm body
    will cool down and approach the ambient temperature of the room in an exponentially
    decaying fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In exponential regression, our goal is to determine the values of the parameters
    *a* and *b*—the initial value and the growth constant. Let''s try this in JavaScript.
    The data we wish to analyze is exponentially decaying, with random sensor noise
    added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/928b67bb-e521-4c2a-af8a-64d5cad67500.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding chart exhibits some quantity that starts near 100 and decays down
    to approximately 0\. This could represent, for instance, the number of visitors
    over time to a post that was shared on Facebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attempting to fit a trendline with Excel or Google Sheets does not help us
    in this case. The linear trendline does not fit the exponential curve, and the
    inappropriateness of the fit is indicated by the poor R² value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6682428-e2cf-4d50-8bae-007dd6ce7e70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now use JavaScript to find the regression for this data and also make
    a prediction of the value for one second before the dataset started. Add the following
    code to `index.js`; it is the linear regression code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program with `yarn start` and you should see the following output,
    following the output for the linear regression example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can immediately see the high R² value of 0.99, indicating that the regression
    has found a good fit to the data. If we chart this regression along with the original
    data, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bfcc118-4465-45e4-9359-e1ac6b4c739e.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, we get a prediction for X = -1 of 103, which fits our data well.
    The original parameters for the equation I used to generate the test data were
    *a = 100* and *b = -0.1*, while the predicted parameters were *a = 94.5* and *b
    = -0.09*. The presence of noise has made a significant impact on the starting
    value, which would have been 100 if there were no noise in the system, but was
    actually measured at 96\. When comparing the regressed value for *a* to the actual
    value of *a*, you must also consider the fact that the regressed value of *a*
    was close to the measured, noisy value, even though it is pretty far from the
    systemic value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at the polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The polynomial regression can be considered a more generalized form of the
    linear regression. A polynomial relationship has the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = a[0] + a[1]x¹ + a[2]x² + a[3]x³ + ... + a[n]x^n*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A polynomial can have any number of terms, which is called the **degree** of
    the polynomial. For each degree of the polynomial, the independent variable, *x*,
    is multiplied by some parameter, *a[n]*,, and the X-value is raised to the power
    *n*. A straight line is considered a polynomial of degree *1*; if you update the
    preceding polynomial formula to remove all degrees above one, you are left with:'
  prefs: []
  type: TYPE_NORMAL
- en: '* y = a[0] + a[1]x*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *a[0]* is the y-intercept and *a[1]* is the slope of the line. Despite
    the slight difference in notation, this is equivalent to *y = mx + b*.
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic equations, which you may recall from high school math, are simply
    polynomials of degree *2*, or *y = a[0] + a[1]x + a[2]x²*. Cubic equations are
    polynomials of degree 3, quadratic equations are polynomials of degree 4, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The property of polynomials and polynomial regressions that makes them so powerful
    is the fact that nearly any shape can be described by a polynomial of sufficient
    degree, within a limited range of values. Polynomial regressions can even fit
    sinusoidal shapes, as long as you don't try to extrapolate too far. Polynomial
    regressions exhibit properties similar to other machine learning algorithms in
    the sense that they can overfit and become very inaccurate for new data points
    if you attempt to extrapolate too far.
  prefs: []
  type: TYPE_NORMAL
- en: Because polynomials can be of any degree, you must also configure the regression
    with an additional parameter; this parameter can be guessed or you can search
    for the degree that maximizes the R² fit. This approach is similar to the approach
    we used for k-means when you don't know the number of clusters in advance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data we wish to fit, when graphed, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bcfab46-a8df-43ab-b29e-87973f606582.png)'
  prefs: []
  type: TYPE_IMG
- en: This small window of data looks sinusoidal but is in fact polynomial; remember
    that polynomial equations can reproduce many types of shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the bottom of `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we have configured the regression with `{order: 2}`, that is, we
    are attempting to fit the data with a quadratic formula. Run the program with
    `yarn start` to see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The R² fit for this data is quite low, at `0.75`, indicating that we have probably
    used an incorrect value for the `order` parameter. Try increasing the order to
    `{order: 4}` and re-run the program to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The regression is a much better fit now, at the expense of having added extra
    polynomial terms to the equation. If we graph this regression against the original
    data, we will see the following output, which indeed fits the data very well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/135d82e1-3f14-42d7-a7cd-29ae566248cb.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will explore some other types of analyses that can be
    performed on time-series data, including low-pass and high-pass filters, and seasonality
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Other time-series analysis techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regressions are a great starting point for analyzing continuous data, however,
    there are many other techniques one can employ when analyzing time-series data
    specifically. While regressions can be used for any continuous data mapping, time-series
    analysis is specifically geared toward continuous data that evolves over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many examples of time-series data, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Server load over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stock prices over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User activity over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather patterns over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective when analyzing time-series data is similar to the objective in
    analyzing continuous data with regressions. We wish to identify and describe the
    various factors that influence the changing value over time. This section will
    describe a number of techniques above and beyond regressions that you can use
    to analyze time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at techniques that come from the field of digital
    signal processing, which has applications in electronics, sensor analysis, and
    audio signals. While your specific time-series problem may not be related to any
    of these fields, the tools used in digital signal processing applications can
    be applied to any problem domain that deals with digital signals. Among the most
    significant tools and techniques are filtering, seasonality detection, and frequency
    analysis. We'll discuss these techniques, but I will leave it up to you to implement
    your own examples and experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filtering, in a digital signal processing context, is a technique used to filter
    out either high-frequency or low-frequency components of a signal. These are called
    **low-pass filters** and **high-pass filters**, respectively; a low-pass filter
    allows low-frequency signals to *pass* while removing high-frequency components
    from the signal. There are also *band-pass* and *notch* filters, which either
    allow a range of frequencies to pass or cut a range of frequencies from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: In electronics, filters are designed by using capacitors, resistors, and other
    simple electronic components in order to allow only frequencies above or below
    a *cut-off* *frequency* to pass through the circuit. In digital signal processing,
    the same effect can be achieved with an *infinite impulse response* filter, which
    is an algorithm that can reproduce the effects of an electronic circuit on time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, consider the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f3482fb-e2bf-4a0b-ae03-693641f263d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This data was generated by combining two sinusoidal signals, one low-frequency
    signal and one high-frequency signal. If we chart the two signals individually,
    we can see how they combine to create the overall signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c71d815-d1a7-4dd8-8731-5ecdc504f319.png)'
  prefs: []
  type: TYPE_IMG
- en: When filtering the overall signal, the goal is to extract either the low-frequency
    or the high-frequency component of the signal, while filtering the other out.
    This is called **subtractive processing**, since we are removing (filtering) a
    component from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you should use low-pass filtering to isolate large, general, periodic
    trends in time-series data while ignoring faster periodic trends. High-pass filtering,
    on the other hand, should be used when you wish to explore the short-term periodic
    trends while ignoring the long-term trends. One example of this approach is when
    analyzing visitor traffic; you can use high-pass and low-pass filtering to selectively
    ignore monthly trends versus daily trends.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonality analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building on the previous section, we can also use digital signal processing
    to analyze seasonal trends. Seasonal trends are long-term periodic (that is, low-frequency)
    trends that you wish to subtract from your overall data in order to analyze other,
    potentially non-periodic trends in data. Consider the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6c51674-759f-47b5-b62d-43960d3bdacd.png)'
  prefs: []
  type: TYPE_IMG
- en: This data exhibits a combination of linear growth on top of periodic fluctuations
    in activity. Specifically, there are two periodic components (one low-frequency
    and one high-frequency) and one linear component to this data trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to analyze this data, the approach would be to first identify the
    linear trend, either through a large moving-average window or through a linear
    regression. Once the linear trend has been identified, you can subtract it from
    the data to isolate the periodic portions only. This approach is illustrated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ac0f8fa-ab8c-42b4-96dc-220c50a0db30.png)'
  prefs: []
  type: TYPE_IMG
- en: Because signals are additive, you are able to subtract the linear trend from
    the original data in order to isolate the non-linear components of the signal.
    If you've identified multiple trends, either through regressions or other means,
    you can continue to subtract the trends that you've identified from the original
    signal and you'll be left with only the unidentified signal components. Once you've
    identified and subtracted all of the systemic patterns, you'll have only the sensor
    noise remaining.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, once you've identified and subtracted the linear trend from the
    data, you can either perform filtering on the resultant signal in order to isolate
    the low- and high-frequency components, or you can perform a *Fourier analysis*
    on the leftover signal to identify the specific frequencies and amplitudes of
    the remaining components.
  prefs: []
  type: TYPE_NORMAL
- en: Fourier analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fourier analysis is a mathematical technique used to decompose a time-series
    signal into its individual frequency components. Recall that polynomial regressions
    of arbitrary degree can reproduce nearly any signal shape. In a similar manner,
    the sum of a number of sinusoidal oscillators can reproduce nearly any periodic
    signal. If you''ve ever seen an *oscilloscope* or *spectrum analyzer* in action,
    you''ve seen the real-time results of a Fourier transform being applied to a signal.
    In short, a Fourier transformation turns a periodic signal, such as the ones we
    saw in the last section, into a formula similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a[1]sin(f[1]+φ[1]) + a[2]sin(f[2]+φ[2]) + a[3]sin(f[3]+φ[3]) + ... + a[n]sin(f[n]+φ[n])*'
  prefs: []
  type: TYPE_NORMAL
- en: Where *f[n]* represents a frequency, *a[n]* represents its amplitude, and *φ[n]*
    represents a phase offset. By combining an arbitrary number of these sinusoidal
    signals together, one can replicate nearly any periodic signal.
  prefs: []
  type: TYPE_NORMAL
- en: There are many reasons to perform a Fourier analysis. The most intuitive examples
    relate to audio and sound processing. If you take a one-second-long audio sample
    of the note A4 being played on a piano and perform a Fourier transform on it,
    you would see that the frequency of 440 Hz has the largest amplitude. You would
    also see that the harmonics of 440 Hz, like 880 Hz and 1,320 Hz, also have some
    energy. You can use this data to aid in audio fingerprinting, auto-tuning, visualizing,
    and many other applications. The Fourier transformation is a sampling algorithm,
    and so it is susceptible to aliasing and other sampling errors. A Fourier transformation
    can be used to partially recreate an original signal, but much detail would be
    lost in translation. This process would be similar to down-sampling an image and
    then trying to up-sample it again.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other applications for Fourier transforms in nearly every domain.
    The Fourier transform's popularity is due to the fact that, mathematically, many
    types of operations are easier to perform in the frequency domain than the time
    domain. There are many types of problems in mathematics, physics, and engineering
    that are very difficult to solve in the time domain but easy to solve in the frequency
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: A Fourier transformation is a mathematical process that specific algorithms
    perform. The most popular Fourier transform algorithm is called the **Fast Fourier
    Transform** (**FFT**), named s0 because it was much faster than its predecessor,
    the *Discrete Fourier Transform*. The FFT has one significant limitation in that
    the number of samples to be analyzed must be a power of 2, that is, it must be
    128, 256, 512, 1,024, 2,048, and so on, samples long. If you have 1,400 samples
    to analyze, you must either truncate it down to 1,024 samples or pad it up to
    2,048 samples. Most often, you will be *windowing* a larger sample; in the example
    of the piano note recording, we have windowed one second of samples from a live
    or recorded signal. If the audio sample rate is 44,100 Hz, then we would have
    44,100 samples (one second's worth) to give to the Fourier transformation.
  prefs: []
  type: TYPE_NORMAL
- en: When padding, truncating, or windowing samples from a larger signal, you should
    use a *window function*, which is a function that tapers the signal at both ends
    so that it is not sharply cut off by your window. There are many types of window
    functions, each with their own mathematical properties and unique effects on your
    signal processing. Some popular window functions include the rectangular and triangular
    windows, as well as the Gaussian, Lanczos, Hann, Hamming, and Blackman windows,
    which each have desirable properties in different types of analyses.
  prefs: []
  type: TYPE_NORMAL
- en: The output of a Fourier transform algorithm, like the FFT algorithm, is a *frequency
    domain spectrum*. More concretely, the output of the FFT algorithm will be an
    array or a hash table where the keys are frequency buckets (such as 0-10 Hz, 10-20
    Hz, and so on), and the values are amplitude and phase. These may be represented
    as complex numbers, multidimensional arrays, or some other structure specific
    to the algorithm implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Some limitations apply to all sampling algorithms; these are limitations of
    signal processing itself. For instance, aliasing can occur if your signal contains
    components at frequencies above the *Nyquist frequency*, or half the sampling
    rate. In audio, where a sampling rate of 44,100 Hz is common, any frequencies
    above 22,050 Hz will be aliased, or misrepresented as low-frequency signals. Preprocessing
    signals with a low-pass filter is therefore a common technique. Similarly, the
    FFT algorithm can only resolve frequencies up to the Nyquist frequency. The FFT
    algorithm will return only as many frequency buckets as the sample buffer size,
    so if you give it 1,024 samples you will only get 1,024 frequency buckets. In
    audio, this means that each frequency bucket will have a bandwidth of 44,100 Hz
    / 1,024 = 43 Hz. This means that you would not be able to tell the difference
    between 50 Hz and 55 Hz, but you would easily be able to tell the difference between
    50 Hz and 500 Hz. In order to get a higher resolution, you would need to provide
    more samples, however, this, in turn, will reduce the time resolution of your
    windows.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the FFT to analyze the periodic portion of the time-series data
    we saw in the last section. It would be best to perform the FFT after you have
    subtracted the linear trend from the signal. However, if you have a high enough
    frequency resolution, the linear trend may only be interpreted as a low-frequency
    component of the Fourier transformation, so whether or not you need to subtract
    the linear trend will depend on your specific application.
  prefs: []
  type: TYPE_NORMAL
- en: By adding the FFT to the other tools you have learned about in this chapter,
    you are prepared to tackle most real-world regression or time-series analysis
    tasks. Each problem will be unique, and you will have to carefully consider which
    specific tools you will need for your task.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned a number of techniques used in forecasting, signal
    processing, regression, and time-series data analysis. Because forecasting and
    time-series analysis is a broad category, there is no single algorithm you can
    use that covers every case. Instead, this chapter has given you an initial toolbox
    of important concepts and algorithms that you can start applying to your forecasting
    and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, you learned about the difference between regression and classification.
    While classification assigns labels to data points, regression attempts to predict
    the numerical value of a data point. Not all regression is necessarily forecasting,
    but regression is the single most significant technique used in forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: After learning the basics of regression, we explored a few specific types of
    regression. Namely, we discussed linear, polynomial, and exponential regression.
    We saw how regression deals with noise and how we can use it to predict future
    values.
  prefs: []
  type: TYPE_NORMAL
- en: We then turned to the broader concept of time-series analysis, and discussed
    core concepts, such as extracting trends from signals. We discussed tools used
    in digital signal processing that are applicable to time-series analysis, such
    as low-pass and high-pass filters, seasonality analysis, and Fourier transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to look at more advanced machine learning models.
    Specifically, we're going to learn about the neural network—which, by the way,
    can also perform regressions.
  prefs: []
  type: TYPE_NORMAL
