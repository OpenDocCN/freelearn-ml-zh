<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Learning Object Tracking</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned about video surveillance, background modeling, and morphological image processing. We discussed how we can use different morphological operators to apply cool visual effects to input images. In this chapter, we are going to learn how to track an object in a live video. We will discuss the different characteristics of an object that can be used to track it. We will also learn about different methods and techniques for object tracking. Object tracking is used extensively in robotics, self-driving cars, vehicle tracking, player tracking in sports, and video compression.</p>
<p>By the end of this chapter, you will know the following:</p>
<ul>
<li>How to track objects of a specific color</li>
<li>How to build an interactive object tracker</li>
<li>What a corner detector is</li>
<li>How to detect good features to track</li>
<li>How to build an optical flow-based feature tracker</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter requires familiarity with the basics of the C++ programming language. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09</a>. The code can be executed on any operating system, though it is only tested on Ubuntu.</p>
<p class="mce-root">Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2SidbMc">http://bit.ly/2SidbMc</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tracking objects of a specific color</h1>
                </header>
            
            <article>
                
<p>In order to build a good object tracker, we need to understand what characteristics can be used to make our tracking robust and accurate. So, let's take a baby step in that direction and see whether we can use colorspace information to come up with a good visual tracker. One thing to keep in mind is that color information is sensitive to lighting conditions. In real-world applications, you will have to do some preprocessing to take care of that. But for now, let's assume that somebody else is doing that and we are getting clean color images.</p>
<p>There are many different colorspaces, and picking a good one will depend on the different applications that a user is using. While RGB is the native representation on a computer screen, it's not necessarily ideal for humans. When it comes to humans, we give names to colors more naturally based on their hue, which is why <strong>hue saturation value</strong> (<strong><span>HSV</span></strong>) is probably one of the most informative colorspaces. It closely aligns with how we perceive colors. Hue refers to the color spectrum, saturation refers to the intensity of a particular color, and value refers to the brightness of that pixel. This is actually represented in a cylindrical format. You can find a simple explanation at <a href="http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html"><span class="URLPACKT">http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html</span></a>. We can take the pixels of an image to the HSV colorspace and then use this colorspace to measure distances in this colorspace and threshold in this space thresholding to track a given object.</p>
<p>Consider the following frame in the video:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-528 image-border" src="assets/1faadcdb-ac4b-42d9-a9e6-912196cc317b.png" style="width:39.58em;height:23.17em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you run it through the colorspace filter and track the object, you will see something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-529 image-border" src="assets/37d137d4-7ba2-4630-8f26-36b464dc3c5e.png" style="width:44.08em;height:25.42em;"/></p>
<p>As we can see here, our tracker recognizes a particular object in the video based on the color characteristics. In order to use this tracker, we need to know the color distribution of our target object. Here is the code to track a colored object, which selects only pixels that have a certain given hue. The code is well-commented, so read the explanation about each term to see what's happening:</p>
<pre>int main(int argc, char* argv[]) 
{ 
   // Variable declarations and initializations 
     
    // Iterate until the user presses the Esc key 
    while(true) 
    { 
        // Initialize the output image before each iteration 
        outputImage = Scalar(0,0,0); 
         
        // Capture the current frame 
        cap &gt;&gt; frame; 
     
        // Check if 'frame' is empty 
        if(frame.empty()) 
            break; 
         
        // Resize the frame 
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA); 
     
        // Convert to HSV colorspace 
        cvtColor(frame, hsvImage, COLOR_BGR2HSV); 
         
        // Define the range of "blue" color in HSV colorspace 
        Scalar lowerLimit = Scalar(60,100,100); 
        Scalar upperLimit = Scalar(180,255,255); 
         
        // Threshold the HSV image to get only blue color 
        inRange(hsvImage, lowerLimit, upperLimit, mask); 
         
        // Compute bitwise-AND of input image and mask 
        bitwise_and(frame, frame, outputImage, mask=mask); 
         
        // Run median filter on the output to smoothen it 
        medianBlur(outputImage, outputImage, 5); 
         
        // Display the input and output image 
        imshow("Input", frame); 
        imshow("Output", outputImage); 
         
        // Get the keyboard input and check if it's 'Esc' 
        // 30 -&gt; wait for 30 ms 
        // 27 -&gt; ASCII value of 'ESC' key 
        ch = waitKey(30); 
        if (ch == 27) { 
            break; 
        } 
    } 
     
    return 1; 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an interactive object tracker</h1>
                </header>
            
            <article>
                
<p class="mce-root">A colorspace-based tracker gives us the freedom to track a colored object, but we are also constrained to a predefined color. What if we just want to pick an object at random? How do we build an object tracker that can learn the characteristics of the selected object and just track it automatically? This is where the <strong>c</strong><span><strong>ontinuously-adaptive meanshift</strong> (</span><strong>CAMShift</strong>) algorithm comes into picture. It's basically an improved version of the meanshift algorithm.</p>
<p>The concept of meanshift is actually nice and simple. Let's say we select a region of interest and we want our object tracker to track that object. In this region, we select a bunch of points based on the color histogram and we compute the centroid of spatial points. If the centroid lies at the center of this region, we know that the object hasn't moved. But if the centroid is not at the center of this region, then we know that the object is moving in some direction. The movement of the centroid controls the direction in which the object is moving. So, we move the bounding box of the object to a new location so that the new centroid becomes the center of this bounding box. Hence, this algorithm is called meanshift, because the mean (the centroid) is shifting. This way, we keep ourselves updated with the current location of the object.</p>
<p>But the problem with meanshift is that the size of the bounding box is not allowed to change. When you move the object away from the camera, the object will appear smaller to the human eye, but meanshift will not take that into account. The size of the bounding box will remain the same throughout the tracking session. Hence, we need to use CAMShift. The advantage of CAMShift is that it can adapt the size of the bounding box to the size of the object. Along with that, it can also keep track of the orientation of the object.</p>
<p>Let's consider the following frame, in which the object is highlighted:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-530 image-border" src="assets/2a4365f9-5811-42a5-ba22-35ed50858e29.png" style="width:56.50em;height:33.00em;"/></div>
<p>Now that we have selected the object, the algorithm computes the histogram backprojection and extracts all the information. What is histogram backprojection? It's just a way of identifying how well the image fits into our histogram model. We compute the histogram model of a particular thing and then use this model to find that thing in an image. Let's move the object and see how it's getting tracked:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-531 image-border" src="assets/2d8da352-b1c1-435f-b36a-a9caa1d56c82.png" style="width:52.50em;height:30.92em;"/></div>
<p class="mce-root"/>
<p>It looks like the object is getting tracked fairly well. Let's change the orientation and see whether the tracking is maintained:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-532 image-border" src="assets/a2d6e845-f927-4510-bd49-5784687e6d03.png" style="width:51.42em;height:30.08em;"/></div>
<p>As we can see, the bounding ellipse has changed its location as well as orientation. Let's change the perspective of the object and see whether it's still able to track it:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-533 image-border" src="assets/967cea66-cf87-4a11-bb7c-b2dd25b1d4fa.png" style="width:91.67em;height:54.33em;"/></div>
<p>We're still good! The bounding ellipse has changed the aspect ratio to reflect the fact that the object looks skewed now (because of the perspective transformation). Let's look at the user interface functionality in the code:</p>
<pre>Mat image; 
Point originPoint; 
Rect selectedRect; 
bool selectRegion = false; 
int trackingFlag = 0; 
 
// Function to track the mouse events 
void onMouse(int event, int x, int y, int, void*) 
{ 
    if(selectRegion) 
    { 
        selectedRect.x = MIN(x, originPoint.x); 
        selectedRect.y = MIN(y, originPoint.y); 
        selectedRect.width = std::abs(x - originPoint.x); 
        selectedRect.height = std::abs(y - originPoint.y); 
         
        selectedRect &amp;= Rect(0, 0, image.cols, image.rows); 
    } 
     
    switch(event) 
    { 
        case EVENT_LBUTTONDOWN: 
            originPoint = Point(x,y); 
            selectedRect = Rect(x,y,0,0); 
            selectRegion = true; 
            break; 
             
        case EVENT_LBUTTONUP: 
            selectRegion = false; 
            if( selectedRect.width &gt; 0 &amp;&amp; selectedRect.height &gt; 0 ) 
            { 
                trackingFlag = -1; 
            } 
            break; 
    } 
} </pre>
<p>This function basically captures the coordinates of the rectangle that was selected in the window. The user just needs to click and drag with the mouse. There are a set of built-in functions in OpenCV that help us to detect these different mouse events.</p>
<p>Here is the code for performing object tracking based on CAMShift:</p>
<pre>int main(int argc, char* argv[]) 
{ 
    // Variable declaration and initialization 
    ....
    // Iterate until the user presses the Esc key 
    while(true) 
    { 
        // Capture the current frame 
        cap &gt;&gt; frame; 
     
        // Check if 'frame' is empty 
        if(frame.empty()) 
            break; 
         
        // Resize the frame 
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA); 
     
        // Clone the input frame 
        frame.copyTo(image); 
     
        // Convert to HSV colorspace 
        cvtColor(image, hsvImage, COLOR_BGR2HSV);</pre>
<p>We now have the HSV image waiting to be processed. Let's go ahead and see how we can use our thresholds to process this image:</p>
<pre>        if(trackingFlag) 
        { 
            // Check for all the values in 'hsvimage' that are within the specified range 
            // and put the result in 'mask' 
            inRange(hsvImage, Scalar(0, minSaturation, minValue), Scalar(180, 256, maxValue), mask); 
             
            // Mix the specified channels 
            int channels[] = {0, 0}; 
            hueImage.create(hsvImage.size(), hsvImage.depth()); 
            mixChannels(&amp;hsvImage, 1, &amp;hueImage, 1, channels, 1); 
             
            if(trackingFlag &lt; 0) 
            { 
                // Create images based on selected regions of interest 
                Mat roi(hueImage, selectedRect), maskroi(mask, selectedRect); 
                 
                // Compute the histogram and normalize it 
                calcHist(&amp;roi, 1, 0, maskroi, hist, 1, &amp;histSize, &amp;histRanges); 
                normalize(hist, hist, 0, 255, NORM_MINMAX); 
                 
                trackingRect = selectedRect; 
                trackingFlag = 1; 
            } </pre>
<p>As we can see here, we use the HSV image to compute the histogram of the region. We use our thresholds to locate the required color in the HSV spectrum and then filter out the image based on that. Let's go ahead and see how we can compute the histogram backprojection:</p>
<pre>            // Compute the histogram backprojection 
            calcBackProject(&amp;hueImage, 1, 0, hist, backproj, &amp;histRanges); 
            backproj &amp;= mask; 
            RotatedRect rotatedTrackingRect = CamShift(backproj, trackingRect, TermCriteria(TermCriteria::EPS | TermCriteria::COUNT, 10, 1)); 
             
            // Check if the area of trackingRect is too small 
            if(trackingRect.area() &lt;= 1) 
            { 
                // Use an offset value to make sure the trackingRect has a minimum size 
                int cols = backproj.cols, rows = backproj.rows; 
                int offset = MIN(rows, cols) + 1; 
                trackingRect = Rect(trackingRect.x - offset, trackingRect.y - offset, trackingRect.x + offset, trackingRect.y + offset) &amp; Rect(0, 0, cols, rows); 
            } </pre>
<p>We are now ready to display the results. Using the rotated rectangle, let's draw an ellipse around our region of interest:</p>
<pre>            // Draw the ellipse on top of the image 
            ellipse(image, rotatedTrackingRect, Scalar(0,255,0), 3, LINE_AA); 
        } 
         
        // Apply the 'negative' effect on the selected region of interest 
        if(selectRegion &amp;&amp; selectedRect.width &gt; 0 &amp;&amp; selectedRect.height &gt; 0) 
        { 
            Mat roi(image, selectedRect); 
            bitwise_not(roi, roi); 
        } 
         
        // Display the output image 
        imshow(windowName, image); 
         
        // Get the keyboard input and check if it's 'Esc' 
        // 27 -&gt; ASCII value of 'Esc' key 
        ch = waitKey(30); 
        if (ch == 27) { 
            break; 
        } 
    } 
     
    return 1; 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting points using the Harris corner detector</h1>
                </header>
            
            <article>
                
<p>Corner detection is a technique used to detect points of interest in an image. These interest points are also called feature points, or simply features, in computer vision terminology. A corner is basically an intersection of two edges. An interest point is basically something that can be uniquely detected in an image. A corner is a particular case of an interest point. These interest points help us characterize an image. These points are used extensively in applications such as object tracking, image classification, and visual search. Since we know that the corners are interesting, let's see how can detect them.</p>
<p>In computer vision, there is a popular corner detection technique called the Harris corner detector. We basically construct a 2 x 2 matrix based on partial derivatives of the grayscale image, and then analyze the eigenvalues. What does that even mean? Well, let's dissect it so that we can understand it better. Let's consider a small patch in the image. Our goal is to identify whether this patch has a corner in it. So, we consider all the neighboring patches and compute the intensity difference between our patch and all those neighboring patches. If the difference is high in all directions, then we know that our patch has a corner in it. This is an oversimplification of the actual algorithm, but it covers the gist. If you want to understand the underlying mathematical details, you can check out the original paper by <em>Harris</em> and <em>Stephens</em> at <a href="http://www.bmva.org/bmvc/1988/avc-88-023.pdf"><span class="URLPACKT">http://www.bmva.org/bmvc/1988/avc-88-023.pdf</span></a>. A corner is a point with strong intensity differences along two directions.</p>
<p>If we run the Harris corner detector, it will look like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-534 image-border" src="assets/8825804c-b5d9-4478-938c-c4e8b1a17e37.png" style="width:65.92em;height:39.17em;"/></div>
<p class="mce-root"/>
<p>As we can see, the green circles on the TV remote are the detected corners. This will change based on the parameters you choose for the detector. If you modify the parameters, you can see that more points might get detected. If you make it strict, you might not be able to detect soft corners. Let's look at the code to detect Harris corners:</p>
<pre class="mce-root">int main(int argc, char* argv[])<br/>{<br/>// Variable declaration and initialization<br/><br/>// Iterate until the user presses the Esc key<br/>while(true)<br/>{<br/>    // Capture the current frame<br/>    cap &gt;&gt; frame;<br/><br/>    // Resize the frame<br/>    resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);<br/><br/>    dst = Mat::zeros(frame.size(), CV_32FC1);<br/><br/>    // Convert to grayscale<br/>    cvtColor(frame, frameGray, COLOR_BGR2GRAY );<br/><br/>    // Detecting corners<br/>    cornerHarris(frameGray, dst, blockSize, apertureSize, k, BORDER_DEFAULT);<br/><br/>    // Normalizing<br/>    normalize(dst, dst_norm, 0, 255, NORM_MINMAX, CV_32FC1, Mat());<br/>    convertScaleAbs(dst_norm, dst_norm_scaled);</pre>
<p>We converted the image to grayscale and detected corners using our parameters. You can find the full code in the <kbd>.cpp</kbd> files. These parameters play an important role in the number of points that will be detected. You can check out the OpenCV documentation of <kbd><span class="CodeInTextPACKT">cornerHarris()</span></kbd> at <a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345">https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345</a>.</p>
<p>We now have all the information we need. Let's go ahead and draw circles around our corners to display the results:</p>
<pre>        // Drawing a circle around each corner<br/>        for(int j = 0; j &lt; dst_norm.rows ; j++)<br/>        {<br/>            for(int i = 0; i &lt; dst_norm.cols; i++)<br/>            {<br/>                if((int)dst_norm.at&lt;float&gt;(j,i) &gt; thresh)<br/>                {<br/>                    circle(frame, Point(i, j), 8, Scalar(0,255,0), 2, 8, 0);<br/>                }<br/>            }<br/>        }<br/><br/>        // Showing the result<br/>        imshow(windowName, frame);<br/><br/>        // Get the keyboard input and check if it's 'Esc'<br/>        // 27 -&gt; ASCII value of 'Esc' key<br/>        ch = waitKey(10);<br/>        if (ch == 27) {<br/>            break;<br/>        }<br/>    }<br/><br/>    // Release the video capture object<br/>    cap.release();<br/><br/>    // Close all windows<br/>    destroyAllWindows();<br/><br/>    return 1;<br/>}</pre>
<p>As we can see, this code takes an input argument: <kbd><span class="CodeInTextPACKT">blockSize</span></kbd>. Depending on the size you choose, the performance will vary. Start with a value of four and play around with it to see what happens.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Good features to track</h1>
                </header>
            
            <article>
                
<p>Harris corner detector performs well in many cases, but it can still be improved. Around six years after the original paper by <em>Harris</em> and <em>Stephens</em>, <em>Shi</em> and <em>Tomasi</em> came up with something better and they called it <em>Good Features to Track</em>. You can read the original paper here: <a href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf"><span class="URLPACKT">http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf</span></a>. They used a different scoring function to improve the overall quality. Using this method, we can find the N strongest corners in the given image. This is very useful when we don't want to use every single corner to extract information from the image. As we discussed, a good interest point detector is very useful in applications such as object tracking, object recognition, and image search.</p>
<p>If you apply the Shi-Tomasi corner detector to an image, you will see something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-576 image-border" src="assets/f397c6a1-aa49-41f4-923a-ff22ed465243.png" style="width:162.50em;height:94.83em;"/></div>
<p>As we can see here, all the important points in the frame are captured. Let's look at the code to track these features:</p>
<pre>int main(int argc, char* argv[]) 
{ 
    // Variable declaration and initialization 
     
    // Iterate until the user presses the Esc key 
    while(true) 
    { 
        // Capture the current frame 
        cap &gt;&gt; frame; 
         
        // Resize the frame 
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA); 
         
        // Convert to grayscale 
        cvtColor(frame, frameGray, COLOR_BGR2GRAY ); 
         
        // Initialize the parameters for Shi-Tomasi algorithm 
        vector&lt;Point2f&gt; corners; 
        double qualityThreshold = 0.02; 
        double minDist = 15; 
        int blockSize = 5; 
        bool useHarrisDetector = false; 
        double k = 0.07; 
         
        // Clone the input frame 
        Mat frameCopy; 
        frameCopy = frame.clone(); 
         
        // Apply corner detection 
        goodFeaturesToTrack(frameGray, corners, numCorners, qualityThreshold, minDist, Mat(), blockSize, useHarrisDetector, k); </pre>
<p>As we can see, we extracted the frame and used <kbd><span class="CodeInTextPACKT">goodFeaturesToTrack</span></kbd> to detect the corners. It's important to understand that the number of corners detected will depend on our choice of parameters. You can find a detailed explanation at <a href="http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack"><span class="URLPACKT">http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack</span></a>. Let's go ahead and draw circles on these points to display the output image:</p>
<pre>        // Parameters for the circles to display the corners 
        int radius = 8;      // radius of the circles 
        int thickness = 2;   // thickness of the circles 
        int lineType = 8; 
         
        // Draw the detected corners using circles 
        for(size_t i = 0; i &lt; corners.size(); i++) 
        { 
            Scalar color = Scalar(rng.uniform(0,255), rng.uniform(0,255), rng.uniform(0,255)); 
            circle(frameCopy, corners[i], radius, color, thickness, lineType, 0); 
        } 
         
        /// Show what you got 
        imshow(windowName, frameCopy); 
         
        // Get the keyboard input and check if it's 'Esc' 
        // 27 -&gt; ASCII value of 'Esc' key 
        ch = waitKey(30); 
        if (ch == 27) { 
            break; 
        } 
    } 
     
    // Release the video capture object 
    cap.release(); 
     
    // Close all windows 
    destroyAllWindows(); 
     
    return 1; 
}</pre>
<p>This program takes an input argument: <kbd><span class="CodeInTextPACKT">numCorners</span></kbd>. This value indicates the maximum number of corners you want to track. Start with a value of <kbd>100</kbd> and play around with it to see what happens. If you increase this value, you will see more feature points getting detected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature-based tracking</h1>
                </header>
            
            <article>
                
<p>Feature-based tracking refers to tracking individual feature points across successive frames in the video. The advantage here is that we don't have to detect feature points in every single frame. We can just detect them once and keep tracking them after that. This is more efficient than running the detector on every frame. We use a technique called optical flow to track these features. Optical flow is one of the most popular techniques in computer vision. We choose a bunch of feature points and track them through the video stream. When we detect the feature points, we compute the displacement vectors and show the motion of those keypoints between consecutive frames. These vectors are called motion vectors. A motion vector for a particular point is basically just a directional line indicating where that point has moved, as compared to the previous frame. Different methods are used to detect these motion vectors. The two most popular algorithms are the <strong>Lucas-Kanade</strong> method and the <strong>Farneback</strong> algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lucas-Kanade method</h1>
                </header>
            
            <article>
                
<p>The Lucas-Kanade method is used for sparse optical flow tracking. By sparse, we mean that the number of feature points is relatively low. You can refer to their original paper here: <a href="http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf"><span class="URLPACKT">http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf</span></a>. We start the process by extracting the feature points. For each feature point, we create 3 x 3 patches with the feature point at the center. The assumption here is that all the points within each patch will have a similar motion. We can adjust the size of this window depending on the problem at hand.</p>
<p>For each feature point in the current frame, we take the surrounding 3 x 3 patch as our reference point. For this patch, we look in its neighborhood in the previous frame to get the best match. This neighborhood is usually bigger than 3 x 3 because we want to get the patch that's closest to the patch under consideration. Now, the path from the center pixel of the matched patch in the previous frame to the center pixel of the patch under consideration in the current frame will become the motion vector. We do that for all the feature points and extract all the motion vectors.</p>
<p>Let's consider the following frame:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-535 image-border" src="assets/77fb918e-4022-4492-9af6-58024173479e.png" style="width:91.67em;height:53.83em;"/></div>
<p>We need to add some points that we want to track. Just go ahead and click on a bunch of points on this window with your mouse:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-536 image-border" src="assets/81a8a130-e510-4376-b3fe-3c3ce81a862a.png" style="width:100.00em;height:58.42em;"/></div>
<p>If I move into a different position, you will see that the points are still being tracked correctly within a small margin of error:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-577 image-border" src="assets/0715e6df-ef45-4dea-bf7b-dbda0b2822fb.png" style="width:162.50em;height:95.42em;"/></div>
<p class="mce-root"/>
<p>Let's add a lot of points and see what happens:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-537 image-border" src="assets/b375668e-563f-46b1-956a-e54925b634ae.png" style="width:91.67em;height:53.50em;"/></div>
<p>As we can see, it will keep tracking those points. But, you will notice that some of the points will be dropped because of factors such as prominence or speed of movement. If you want to play around with it, you can just keep adding more points to it. You can also let the user select a region of interest in the input video. You can then extract feature points from this region of interest and track the object by drawing a bounding box. It will be a fun exercise!</p>
<p>Here is the code to do Lucas-Kanade-based tracking:</p>
<pre>int main(int argc, char* argv[]) 
{ 
    // Variable declaration and initialization 
     
    // Iterate until the user hits the Esc key 
    while(true) 
    { 
        // Capture the current frame 
        cap &gt;&gt; frame; 
         
        // Check if the frame is empty 
        if(frame.empty()) 
            break; 
         
        // Resize the frame 
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA); 
         
        // Copy the input frame 
        frame.copyTo(image); 
         
        // Convert the image to grayscale 
        cvtColor(image, curGrayImage, COLOR_BGR2GRAY); 
         
        // Check if there are points to track 
        if(!trackingPoints[0].empty()) 
        { 
            // Status vector to indicate whether the flow for the corresponding features has been found 
            vector&lt;uchar&gt; statusVector; 
             
            // Error vector to indicate the error for the corresponding feature 
            vector&lt;float&gt; errorVector; 
             
            // Check if previous image is empty 
            if(prevGrayImage.empty()) 
            { 
                curGrayImage.copyTo(prevGrayImage); 
            } 
             
            // Calculate the optical flow using Lucas-Kanade algorithm 
            calcOpticalFlowPyrLK(prevGrayImage, curGrayImage, trackingPoints[0], trackingPoints[1], statusVector, errorVector, windowSize, 3, terminationCriteria, 0, 0.001); </pre>
<p>We use the current image and the previous image to compute the optical flow information. Needless to say, the quality of the output will depend on the parameters chosen. You can find more details about the parameters at <a href="http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk">http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk</a>. To increase quality and robustness, we need to filter out the points that are very close to each other because they're not adding new information. Let's go ahead and do that:</p>
<pre> 
            int count = 0; 
             
            // Minimum distance between any two tracking points 
            int minDist = 7; 
             
            for(int i=0; i &lt; trackingPoints[1].size(); i++) 
            { 
                if(pointTrackingFlag) 
                { 
                    // If the new point is within 'minDist' distance from an existing point, it will not be tracked 
                    if(norm(currentPoint - trackingPoints[1][i]) &lt;= minDist) 
                    { 
                        pointTrackingFlag = false; 
                        continue; 
                    } 
                } 
                 
                // Check if the status vector is good 
                if(!statusVector[i]) 
                    continue; 
                 
                trackingPoints[1][count++] = trackingPoints[1][i]; 
 
                // Draw a filled circle for each of the tracking points 
                int radius = 8; 
                int thickness = 2; 
                int lineType = 8; 
                circle(image, trackingPoints[1][i], radius, Scalar(0,255,0), thickness, lineType); 
            } 
             
            trackingPoints[1].resize(count); 
        } </pre>
<p>We now have the tracking points. The next step is to refine the location of those points. What exactly does <strong>refine</strong> mean in this context? To increase the speed of computation, there is some level of quantization involved. In layman's terms, you can think of it as rounding off. Now that we have the approximate region, we can refine the location of the point within that region to get a more accurate outcome. Let's go ahead and do that:</p>
<pre>         
        // Refining the location of the feature points 
        if(pointTrackingFlag &amp;&amp; trackingPoints[1].size() &lt; maxNumPoints) 
        { 
            vector&lt;Point2f&gt; tempPoints; 
            tempPoints.push_back(currentPoint); 
             
            // Function to refine the location of the corners to subpixel accuracy. 
            // Here, 'pixel' refers to the image patch of size 'windowSize' and not the actual image pixel 
            cornerSubPix(curGrayImage, tempPoints, windowSize, Size(-1,-1), terminationCriteria); 
             
            trackingPoints[1].push_back(tempPoints[0]); 
            pointTrackingFlag = false; 
        } 
         
        // Display the image with the tracking points 
        imshow(windowName, image); 
         
        // Check if the user pressed the Esc key 
        char ch = waitKey(10); 
        if(ch == 27) 
            break; 
         
        // Swap the 'points' vectors to update 'previous' to 'current' 
        std::swap(trackingPoints[1], trackingPoints[0]); 
         
        // Swap the images to update previous image to current image 
        cv::swap(prevGrayImage, curGrayImage); 
    } 
     
    return 1; 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Farneback algorithm</h1>
                </header>
            
            <article>
                
<p>Gunnar Farneback proposed this optical flow algorithm and it's used for dense tracking. Dense tracking is used extensively in robotics, augmented reality, and 3D mapping. You can check out the original paper here: <a href="http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf"><span class="URLPACKT">http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf</span></a>. The Lucas-Kanade method is a sparse technique, which means that we only need to process some pixels in the entire image. The Farneback algorithm, on the other hand, is a dense technique that requires us to process all the pixels in the given image. So, obviously, there is a trade-off here. Dense techniques are more accurate, but they are slower. Sparse techniques are less accurate, but they are faster. For real-time applications, people tend to prefer sparse techniques. For applications where time and complexity are not a factor, people tend to prefer dense techniques to extract finer details.</p>
<p>In his paper, Farneback describes a method for dense optical-flow estimation based on polynomial expansion for two frames. Our goal is to estimate the motion between these two frames, which is basically a three-step process. In the first step, each neighborhood in both frames is approximated by polynomials. In this case, we are only interested in quadratic polynomials. The next step is to construct a new signal by global displacement. Now that each neighborhood is approximated by a polynomial, we need to see what happens if this polynomial undergoes an ideal translation. The last step is to compute the global displacement by equating the coefficients in the yields of these quadratic polynomials.</p>
<p>Now, how this is feasible? If you think about it, we are assuming that an entire signal is a single polynomial and there is a global translation relating the two signals. This is not a realistic scenario! So, what are we looking for? Well, our goal is to find out whether these errors are small enough so that we can build a useful algorithm that can track the features.</p>
<p>Let's look at a static image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-538 image-border" src="assets/312e61da-2099-4efb-ae7c-e6224558a3fe.png" style="width:125.00em;height:73.50em;"/></div>
<p>If I move sideways, we can see that the motion vectors are pointing in a horizontal direction. It is simply tracking the movement of my head:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-539 image-border" src="assets/d82a7c3d-8a54-4447-9f85-6d329df90574.png" style="width:125.00em;height:73.25em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If I move away from the webcam, you can see that the motion vectors are pointing in a direction perpendicular to the image plane:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-540 image-border" src="assets/22a60ecd-e148-4c97-b659-bb2075c9664b.png" style="width:125.00em;height:73.00em;"/></div>
<p>Here is the code to do optical-flow-based tracking using the Farneback algorithm:</p>
<pre>int main(int, char** argv) 
{ 
    // Variable declaration and initialization 
     
    // Iterate until the user presses the Esc key 
    while(true) 
    { 
        // Capture the current frame 
        cap &gt;&gt; frame; 
         
        if(frame.empty()) 
            break; 
         
        // Resize the frame 
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA); 
         
        // Convert to grayscale 
        cvtColor(frame, curGray, COLOR_BGR2GRAY); 
         
        // Check if the image is valid 
        if(prevGray.data) 
        { 
            // Initialize parameters for the optical flow algorithm 
            float pyrScale = 0.5; 
            int numLevels = 3; 
            int windowSize = 15; 
            int numIterations = 3; 
            int neighborhoodSize = 5; 
            float stdDeviation = 1.2; 
             
            // Calculate optical flow map using Farneback algorithm 
            calcOpticalFlowFarneback(prevGray, curGray, flowImage, pyrScale, numLevels, windowSize, numIterations, neighborhoodSize, stdDeviation, OPTFLOW_USE_INITIAL_FLOW); </pre>
<p>As we can see, we use the Farneback algorithm to compute the optical flow vectors. The input parameters to <kbd><span class="CodeInTextPACKT">calcOpticalFlowFarneback</span></kbd> are important when it comes to the quality of tracking. You can find details about those parameters at <a href="http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html"><span class="URLPACKT">http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html</span></a>. Let's go ahead and draw those vectors on the output image:</p>
<pre>            // Convert to 3-channel RGB 
            cvtColor(prevGray, flowImageGray, COLOR_GRAY2BGR); 
             
            // Draw the optical flow map 
            drawOpticalFlow(flowImage, flowImageGray); 
             
            // Display the output image 
            imshow(windowName, flowImageGray); 
        } 
         
        // Break out of the loop if the user presses the Esc key 
        ch = waitKey(10); 
        if(ch == 27) 
            break; 
         
        // Swap previous image with the current image 
        std::swap(prevGray, curGray); 
    } 
     
    return 1; 
} </pre>
<p>We used a function called <kbd>drawOpticalFlow</kbd> to draw those optical flow vectors. These vectors indicate the direction of motion. Let's look at the function to see how we draw those vectors:</p>
<pre>// Function to compute the optical flow map 
void drawOpticalFlow(const Mat&amp; flowImage, Mat&amp; flowImageGray) 
{ 
    int stepSize = 16; 
    Scalar color = Scalar(0, 255, 0); 
     
    // Draw the uniform grid of points on the input image along with the motion vectors 
    for(int y = 0; y &lt; flowImageGray.rows; y += stepSize) 
    { 
        for(int x = 0; x &lt; flowImageGray.cols; x += stepSize) 
        { 
            // Circles to indicate the uniform grid of points 
            int radius = 2; 
            int thickness = -1; 
            circle(flowImageGray, Point(x,y), radius, color, thickness); 
             
            // Lines to indicate the motion vectors 
            Point2f pt = flowImage.at&lt;Point2f&gt;(y, x); 
            line(flowImageGray, Point(x,y), Point(cvRound(x+pt.x), cvRound(y+pt.y)), color); 
        } 
    } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about object tracking. We learned how to use HSV colorspace to track objects of a specific color. We discussed clustering techniques for object tracking and how we can build an interactive object tracker using the CAMShift algorithm. We looked at corner detectors and how we can track corners in a live video. We discussed how to track features in a video using optical flow. Finally, we understood the concepts behind the Lucas-Kanade and Farneback algorithms and then implemented them.</p>
<p>In the next chapter, we are going to discuss segmentation algorithms and how we can use them for text recognition.</p>


            </article>

            
        </section>
    </body></html>