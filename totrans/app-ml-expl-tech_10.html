<html><head></head><body>
		<div id="_idContainer148">
			<h1 id="_idParaDest-148"><em class="italic"><a id="_idTextAnchor154"/>Chapter 8</em>: Human-Friendly Explanations with TCAV</h1>
			<p>In the previous few chapters, we have extensively discussed <strong class="bold">LIME</strong> and <strong class="bold">SHAP</strong>. You have also seen the practical aspect of applying the Python frameworks of LIME and SHAP to explain black-box models. One major limitation of both frameworks is that the method of explanation is not extremely consistent and intuitive with how non-technical end users would explain an observation. For example, if you have an image of a glass filled with Coke and use LIME and SHAP to explain a black-box model used to correctly classify the image as Coke, both LIME and SHAP would highlight regions of the image that lead to the correct prediction by the trained model. But if you ask a non-technical user to describe the image, the user would classify the image as Coke due to the presence of a dark-colored carbonated liquid in a glass that resembles a Cola drink. In other words, human beings tend to relate any observation with known <em class="italic">concepts</em> to explain it. </p>
			<p><strong class="bold">Testing with Concept Activation Vector (TCAV)</strong> from <em class="italic">Google AI</em> also follows a similar approach in terms of explaining model predictions with known <em class="italic">human concepts</em>. So, in this chapter, we will cover how TCAV can be used to provide concept-based human-friendly explanations. Unlike LIME and SHAP, TCAV works beyond <em class="italic">feature attribution</em> and refers to concepts such as <em class="italic">color</em>, <em class="italic">gender</em>, <em class="italic">race</em>, <em class="italic">shape</em>, <em class="italic">any known object</em>, or an <em class="italic">abstract idea</em> to explain model predictions. In this chapter, we will discuss the workings of the TCAV algorithm. I will cover some of the advantages and disadvantages of the framework. We will also discuss using this framework for practical problem-solving. In <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, under <em class="italic">Representation-based explanation</em>, you did get some exposure to TCAV, but in this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding TCAV intuitively</li>
				<li>Exploring the practical applications of TCAV</li>
				<li>Advantages and limitations</li>
				<li>Potential applications of concept-based explanations</li>
			</ul>
			<p>It's time to get started now!</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor155"/>Technical requirements </h1>
			<p>This code tutorial and the requisite resources can be downloaded or cloned from the GitHub repository for this chapter at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08</a>. Similar to other chapters, Python and Jupyter notebooks are used to implement the practical application of the theoretical concepts covered in this chapter. However, I would recommend you run the notebooks only after you have gone through this chapter for a better understanding. </p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor156"/>Understanding TCAV intuitively</h1>
			<p>The idea of TCAV was first introduced by <em class="italic">Kim et al.</em> in their work – <em class="italic">Interpretability beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</em> (<a href="https://arxiv.org/pdf/1711.11279.pdf">https://arxiv.org/pdf/1711.11279.pdf</a>). The framework was designed to provide interpretability beyond feature attribution, particularly for deep learning models that rely on low-level transformed features that are not human-interpretable. TCAV aims to explain the opaque internal state of the deep learning model using abstract, high-level, human-friendly concepts. In this section, I will present you with an intuitive understanding of TCAV and explain how it works to provide human-friendly explanations.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor157"/>What is TCAV?</h2>
			<p>So far, we have covered many <a id="_idIndexMarker564"/>methods and frameworks to explain ML models through feature-based approaches. But it might occur to you that since most ML models operate on low-level features, the feature-based explanation approaches might highlight features that are not human-interpretable. For example, for explaining image classifiers, pixel intensity values or pixels coordinates in an image might not be useful for end users without any technical background in data science and ML. So, these features are not user-friendly. Moreover, feature-based explanations are always restricted by the selection of features and the number of features present in the dataset. Out of all the features selected by the feature-based explanation methods, end users might be interested in a particular feature that is not picked by the algorithm. </p>
			<p>So, instead of this approach, concept-based approaches provide a much wider abstraction that is human-friendly and more relevant as interpretability is provided in terms of the importance of high-level concepts. So, <strong class="bold">TCAV</strong> is a model interpretability framework from Google AI that implements the idea of a concept-based explanation method in practice. The algorithm depends on <strong class="bold">Concept Activation Vectors (CAV)</strong>, which provide an interpretation <a id="_idIndexMarker565"/>of the internal state of ML models using human-friendly concepts. In a more technical sense, TCAV uses directional derivatives to quantify the importance of human-friendly, high-level concepts for model predictions. For example, while describing hairstyles, concepts such as <em class="italic">curly hair</em>, <em class="italic">straight hair</em>, or <em class="italic">hair color</em> can be used by TCAV. These<a id="_idIndexMarker566"/> user-defined concepts are not the input features of the dataset that are used by the algorithm during the training process.</p>
			<p>The following figure illustrates the key question addressed by TCAV:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B18216_08_001.jpg" alt="Figure 8.1 – TCAV helps us to address the key question of concept importance of a user-defined concept for image classification by a neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – TCAV helps us to address the key question of concept importance of a user-defined concept for image classification by a neural network</p>
			<p>In the next section, let's try to understand the idea of model explanation using abstract concepts.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor158"/>Explaining with abstract concepts</h2>
			<p>By now, you may have an<a id="_idIndexMarker567"/> intuitive understanding of the method of providing explanations with abstract concepts. But why do you think this is an effective approach? Let's take another example. Suppose you are working on building a deep learning-based image classifier for detecting doctors from images. After applying TCAV, let's say that you have found out that the <em class="italic">concept importance</em> of the concept <em class="italic">white male</em> is maximum, followed by <em class="italic">stethoscope</em> and <em class="italic">white coat</em>. The concept importance of <em class="italic">stethoscope</em> and <em class="italic">white coat</em> is expected, but the high concept importance of <em class="italic">white male</em> indicates a biased dataset. Hence, TCAV can help to evaluate <strong class="bold">fairness</strong> in trained models. </p>
			<p>Essentially, the goal of CAVs is to estimate the importance of a concept (such as color, gender, and race) for the prediction of a trained model, even though the <em class="italic">concepts</em> were not used during the model training process. This is because TCAV learns <em class="italic">concepts</em> from a few example samples. For example, in order to learn a <em class="italic">gender</em> concept, TCAV needs a few data instances that have a <em class="italic">male</em> concept and a few <em class="italic">non-male</em> examples. Hence, TCAV can quantitatively estimate the trained model's sensitivity to a particular <em class="italic">concept</em> for that class. For generating explanations, TCAV perturbs data points toward a <em class="italic">concept</em> that is relatable to humans, and <a id="_idIndexMarker568"/>so it is a type of <strong class="bold">global perturbation method</strong>. Next, let's try to learn the main objectives of TCAV.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor159"/>Goals of TCAV</h2>
			<p>I found the approach of TCAV to be very <a id="_idIndexMarker569"/>unique as compared to other explanation methods. One of the main reasons is because the developers of this framework established clear goals that resonate with my own understanding of human-friendly explanations. The following are the established goals of TCAV:</p>
			<ul>
				<li><strong class="bold">Accessibility</strong>: The developers <a id="_idIndexMarker570"/>of TCAV wanted this approach to be accessible to any end user, irrespective of their knowledge of ML or data science.</li>
				<li><strong class="bold">Customization</strong>: The framework can adapt to any user-defined concept. This is not limited to concepts considered <a id="_idIndexMarker571"/>during the training process.</li>
				<li><strong class="bold">Plug-in readiness</strong>: The developers <a id="_idIndexMarker572"/>wanted this approach to work without the need to retrain or fine-tune trained ML models.</li>
				<li><strong class="bold">Global interpretability</strong>: TCAV can interpret the entire class or multiple samples of the dataset with a single<a id="_idIndexMarker573"/> quantitative measure. It is not restricted to the local explainability of data instances. </li>
			</ul>
			<p>Now that we have an idea of<a id="_idIndexMarker574"/> what can be achieved using TCAV, let's discuss the general approach to how TCAV works.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor160"/>Approach of TCAV</h2>
			<p>In this section, we will cover the workings of TCAV in more depth. The overall workings of this algorithm can be <a id="_idIndexMarker575"/>summarized in the following methods:</p>
			<ul>
				<li>Applying directional derivatives to quantitatively estimate the sensitivity of predictions of trained ML models for various user-defined concepts.</li>
				<li>Computing the final<a id="_idIndexMarker576"/> quantitative explanation, which is termed <strong class="bold">TCAVq measure</strong>, without any model re-training or fine-tuning. This measure is the relative importance of each concept to each model prediction class.</li>
			</ul>
			<p>Now, I will try to further simplify the approach of TCAV without using too many mathematical notions. Let's assume we have a model for identifying zebras from images. To apply TCAV, the following approach can be taken:</p>
			<ol>
				<li><strong class="bold">Defining a concept of interest</strong>: The very first <a id="_idIndexMarker577"/>step is to consider the concepts of interest. For our zebra classifier, either we can have a given set of examples that represent the concept (such as black stripes are important in identifying a zebra) or we can have an independent dataset with the concepts labeled. The major benefit of this step is that it does not limit the algorithm from using features used by the model. Even non-technical users or domain experts can define the concepts based on their existing knowledge.</li>
				<li><strong class="bold">Learning concept activation vectors</strong>: The algorithm tries to learn a vector in the space of activation of the layers by training a linear classifier to differentiate <a id="_idIndexMarker578"/>between activations generated by a concept's instances and instances present in any layer. So, a <strong class="bold">CAV</strong> is defined as the normal projection to a hyperplane that separates instances with a concept and instances without a concept in the model's activation. For our zebra classifier, CAVs help to distinguish representations that denote <em class="italic">black stripes</em> and representations that do not denote <em class="italic">black stripes</em>. </li>
				<li><strong class="bold">Estimating directional derivatives</strong>: Directional derivatives are used to quantify the sensitivity of a model prediction toward a concept. So, for our zebra classifier, directional directives <a id="_idIndexMarker579"/>help us to measure the importance of the <em class="italic">black stripes</em> representation in predicting zebras. Unlike saliency maps, which use per-pixel saliency, directional <a id="_idIndexMarker580"/>derivatives are computed on the entire dataset or a set of inputs but for a specific concept. This helps to give a global perspective for the explanation. </li>
				<li><strong class="bold">Estimating the TCAV score</strong>: To quantify the concept importance of a particular class, the TCAV score (<strong class="bold">TCAVq</strong>) is calculated. This metric helps to measure the positive or <a id="_idIndexMarker581"/>negative influence of a defined concept on a particular activation layer of a model. </li>
				<li><strong class="bold">CAV validation</strong>: CAV can be produced<a id="_idIndexMarker582"/> from randomly selected data. But unfortunately, this might not produce meaningful concepts. So, in order to improve the generated concepts, TCAV runs multiple iterations for finding concepts from different batches of data, instead<a id="_idIndexMarker583"/> of training CAV once, on a single batch of data. Then, a <strong class="bold">statistical significance test</strong> is performed using <em class="italic">two-side t-test</em> for selecting the statistically significant concepts. Necessary corrections, such as the <em class="italic">Bonferroni correction</em>, are also performed to control the false discovery rate. </li>
			</ol>
			<p>Thus, we have covered the intuitive workings of the TCAV algorithm. Next, let's cover how TCAV can actually be implemented in practice. </p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor161"/>Exploring the practical applications of TCAV</h1>
			<p>In this section, we will <a id="_idIndexMarker584"/>explore the practical applications of TCAV for explaining pre-trained image explainers with concept importance. The entire notebook tutorial is available in the code repository of this chapter at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb</a>. This tutorial is presented based on the notebook provided in the original GitHub project repository of TCAV <a href="https://github.com/tensorflow/tcav">https://github.com/tensorflow/tcav</a>. I recommend that you all refer to the main project repository of TCAV since the credit for implementation should go to the developers and contributors of TCAV.</p>
			<p>In this tutorial, we will cover how to apply TCAV to validate the concept importance of the concept of <em class="italic">stripes</em> as compared to the <em class="italic">honeycomb</em> pattern for identifying <em class="italic">tigers</em>. The following images illustrate the flow of the approach used by TCAV to ascertain concept importance using a simple visualization:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B18216_08_002.jpg" alt="Figure 8.2 – Using TCAV to estimate the concept importance of stripes in a tiger image classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Using TCAV to estimate the concept importance of stripes in a tiger image classifier</p>
			<p>Let's begin by setting up<a id="_idIndexMarker585"/> our Jupyter notebook.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor162"/>Getting started</h2>
			<p>Similar to the other tutorial <a id="_idIndexMarker586"/>examples covered in the previous chapters, to install the necessary Python modules required to run the notebook, you can use the <strong class="source-inline">pip</strong> <strong class="source-inline">install</strong> command in a Jupyter notebook:</p>
			<pre class="source-code">!pip install --upgrade pandas numpy matplotlib tensorflow tcav</pre>
			<p>You can import all the modules to validate the successful installation of these frameworks:</p>
			<pre class="source-code">import tensorflow as tf</pre>
			<pre class="source-code">import tcav</pre>
			<p>Next, let's take a look at the data that we'll be working with.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor163"/>About the data</h2>
			<p>I felt that the data preparation process, which is provided in the original project repository of TCAV, is slightly time-consuming. So, I have already prepared the necessary datasets, which you<a id="_idIndexMarker587"/> can refer to from this project repository. Since we will be validating the importance of the concept of <em class="italic">stripes</em> for images of <em class="italic">tigers</em>, we will need an image dataset for tigers. The data is collected from the ImageNet collection and is provided in the project repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger</a>. The images are randomly curated and collected using the <em class="italic">data collection script</em> provided in the TCAV repository: <a href="https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet">https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet</a>.</p>
			<p>In order to run TCAV, you would need to have the necessary <em class="italic">concept images</em>, <em class="italic">target class images,</em> and <em class="italic">random dataset images</em>. For this tutorial, I have prepared the concept images from the <em class="italic">Broden dataset</em> (<a href="http://netdissect.csail.mit.edu/data/broden1_224.zip">http://netdissect.csail.mit.edu/data/broden1_224.zip</a>), as suggested in the main project example. Please go through the research work that led to the creation of this dataset: <a href="https://github.com/CSAILVision/NetDissect">https://github.com/CSAILVision/NetDissect</a>. You can also explore the <em class="italic">Broden dataset texture images</em> provided at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts</a> to learn more. I recommend you to experiment with other concepts or other images and play around with TCAV-based concept importance!</p>
			<p class="callout-heading">Broden dataset</p>
			<p class="callout"><em class="italic">David Bau*, Bolei Zhou*, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network Dissection: Quantifying Interpretability of Deep Visual Representations. Computer Vision and Pattern Recognition (CVPR), 2017.</em></p>
			<p>As TCAV also requires some random datasets to ascertain the statistical significance of the concepts learned from target image examples, I have provided some sample random images in the project repository, thereby simplifying the running of the tutorial notebook! But as always, you should experiment with other random image examples as well. These random images are also collected using the image fetcher script provided in the main project: <a href="https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py">https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py</a>.</p>
			<p>To proceed further, you need to define the variables for the target class and the concepts:</p>
			<pre class="source-code">target = 'tiger'  </pre>
			<pre class="source-code">concepts = ['honeycombed', 'striped'] </pre>
			<p>You can also create the<a id="_idIndexMarker588"/> necessary paths and directories to store the generated activations and CAVs as mentioned in the notebook tutorial. Next, let's discuss the model used in this example.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor164"/>Discussions about the deep learning model used</h2>
			<p>In this example, we will <a id="_idIndexMarker589"/>use a pre-trained deep learning model to highlight the fact that even though TCAV is considered to be a model-specific approach, as it is only applicable to neural networks, it does not make an assumption of the network architecture as such and can work well with most deep neural network models.</p>
			<p>For this example, we will use the pre-trained GoogleNet model, <a href="https://paperswithcode.com/method/googlenet">https://paperswithcode.com/method/googlenet</a>, based on the ImageNet dataset (<a href="https://www.image-net.org/">https://www.image-net.org/</a>). The model files are provided in the code repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h</a>. You can load the trained model using the following lines of code:</p>
			<pre class="source-code">model_to_run = 'GoogleNet'</pre>
			<pre class="source-code">sess = utils.create_session()</pre>
			<pre class="source-code">GRAPH_PATH = "models/inception5h/tensorflow_inception_graph.pb"</pre>
			<pre class="source-code">LABEL_PATH = "models/inception5h/imagenet_comp_graph_label_strings.txt"</pre>
			<pre class="source-code">trained_model = model.GoogleNetWrapper_public(sess,</pre>
			<pre class="source-code">                                              GRAPH_PATH,</pre>
			<pre class="source-code">                                              LABEL_PATH)</pre>
			<p>The model wrapper is actually used to get the internal state and tensors of the trained model. Concept importance is actually computed based on the internal neuron activations and hence, this model wrapper is important. For more details about the workings of the internal API, please refer to the following link: <a href="https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb">https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb</a>.</p>
			<p>Next, we would need to generate the concept activations using the <strong class="source-inline">ImageActivationGenerator</strong> method:</p>
			<pre class="source-code">act_generator = act_gen.ImageActivationGenerator(</pre>
			<pre class="source-code">    trained_model, source_dir, activation_dir, </pre>
			<pre class="source-code">    max_examples=100)</pre>
			<p>Next, we will explore model <a id="_idIndexMarker590"/>explainability using TCAV.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor165"/>Model explainability using TCAV</h2>
			<p>As discussed before, TCAV is <a id="_idIndexMarker591"/>currently used to explain neural networks and the inner layers of a neural network. So, it is not model-agnostic, but rather a model-centric explainability method. This requires us to define the bottleneck layer of the network:</p>
			<pre class="source-code">bottlenecks = [ 'mixed4c']</pre>
			<p>The next step will be to apply the TCAV algorithm to create the concept activation vectors. The process also includes performing statistical significance testing using two side t-test between the concept importance of the target class and the random samples: </p>
			<pre class="source-code">num_random_exp= 15</pre>
			<pre class="source-code"> </pre>
			<pre class="source-code">mytcav = tcav.TCAV(sess,</pre>
			<pre class="source-code">                   target,</pre>
			<pre class="source-code">                   concepts,</pre>
			<pre class="source-code">                   bottlenecks,</pre>
			<pre class="source-code">                   act_generator,</pre>
			<pre class="source-code">                   [0.1],</pre>
			<pre class="source-code">                   cav_dir=cav_dir,</pre>
			<pre class="source-code">                   num_random_exp=num_random_exp)</pre>
			<p>The original experiment mentioned in the TCAV paper, <a href="https://arxiv.org/abs/1711.11279">https://arxiv.org/abs/1711.11279</a>, mentioned using at least 500 random experiments to identify the statistically significant concepts. But for the sake of simplicity, and to achieve faster results, we are <a id="_idIndexMarker592"/>using 15 random experiments. You can experiment with more random experiments as well. </p>
			<p>Finally, we can get the results and visualize the concept importance:</p>
			<pre class="source-code">results = mytcav.run(run_parallel=False)</pre>
			<pre class="source-code">utils_plot.plot_results(results,</pre>
			<pre class="source-code">                        num_random_exp=num_random_exp)</pre>
			<p>This will generate the following plot that helps us to compare concept importance:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B18216_08_003.jpg" alt="Figure 8.3 – TCAV concept importance of the concepts of striped and honeycombed &#13;&#10;for identifying tiger images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – TCAV concept importance of the concepts of striped and honeycombed for identifying tiger images</p>
			<p>As you can observe from <em class="italic">Figure 8.3</em>, the <em class="italic">striped</em> concept has significantly higher concept importance than the <em class="italic">honeycombed</em> concept for identifying <em class="italic">tigers</em>. </p>
			<p>Now that we have covered the practical application part, let me give you a similar challenge as an exercise. Can you now use the ImageNet dataset and ascertain the importance of the concept of <em class="italic">water</em> to <em class="italic">ships</em> and of <em class="italic">clouds</em> or <em class="italic">sky</em> to <em class="italic">airplanes</em>? This will help you understand this <a id="_idIndexMarker593"/>concept in more depth and give you more confidence to apply TCAV. Next, we will discuss some advantages and limitations of TCAV. </p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor166"/>Advantages and limitations</h1>
			<p>In the previous section, we covered the practical aspects of TCAV. TCAV is indeed a very interesting and novel approach to explaining complex deep learning models. Although it has many advantages, unfortunately, I did find some limitations in terms of the current framework that can definitely be improved in the revised version.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor167"/>Advantages</h2>
			<p>Let's discuss the following advantages first:</p>
			<ul>
				<li>As you have previously <a id="_idIndexMarker594"/>seen with the LIME framework in <a href="B18216_04_ePub.xhtml#_idTextAnchor076"><em class="italic">Chapter 4</em></a>,<em class="italic"> LIME for Model Interpretability</em> (which generates explanations using a <strong class="bold">global perturbation method</strong>), there can be contradicting explanations for two data instances for the same class. Even though TCAV is also a type of global perturbation method, unlike LIME, TCAV-generated explanations are not only true for a single data instance but also true for the entire class. This is a major advantage of TCAV over LIME, which increases the user's trust in the explanation method.</li>
				<li>Concept-based explanations are closer to how humans would explain an unknown observation, rather than feature-based explanations as adopted in LIME and SHAP. So, TCAV-generated explanations are indeed more human-friendly.</li>
				<li>Feature-based explanations are limited to the features used in the model. To introduce any new feature for model explainability, we would need to re-train the model, whereas a concept-based explanation is more flexible and is not limited to features used during model training. To introduce a new concept, we do not need to retrain the model. Also, for introducing the concepts, you don't have to know anything about ML. You would just have to make the necessary datasets to generate concepts.</li>
				<li>Model explainability is not the only benefit of TCAV. TCAV can help to detect issues during the training process, such as <strong class="bold">imbalanced datasets</strong> leading to <em class="italic">bias</em> in <em class="italic">the dataset vis-à-vis the majority class</em>. In fact, concept importance can be used as a <em class="italic">metric</em> to compare models. For example, suppose you are using a <em class="italic">VGG19</em> model and a <em class="italic">ResNet50</em> model. Let's say both these models have similar accuracy and model performance, yet concept importance for a user-defined concept is<a id="_idIndexMarker595"/> much higher for the VGG19 model as compared to the ResNet50 model. In such a case, it is better to use the VGG19 model as compared to ResNet50. Hence, TCAV can be used to improve the model training process.</li>
			</ul>
			<p>These are some of the distinct advantages of TCAV, which makes it more human-friendly than LIME and SHAP. Next, let's discuss some known limitations of TCAV.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor168"/>Limitations</h2>
			<p>The following are some <a id="_idIndexMarker596"/>of the known disadvantages of the TCAV approach:</p>
			<ul>
				<li>Currently, the approach of concept-based explanation using TCAV is limited to just neural networks. In order to increase its adoption, TCAV would need an implementation that can work with <em class="italic">classical machine learning algorithms</em> such as <em class="italic">Decision Trees</em>, <em class="italic">Support Vector Machines</em>, and <em class="italic">Ensemble Learning algorithms</em>. Both LIME and SHAP can be applied with classical ML algorithms to solve standard ML problems and that is probably why LIME and SHAP have more adoption. Similarly, with text data, too, TCAV has very limited applications.</li>
			</ul>
			<p>TCAV is highly prone to <em class="italic">data drift</em>, <em class="italic">adversarial effects</em>, and <em class="italic">other data quality issues</em> discussed in <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a><em class="italic">, Data-Centric Approaches</em>. If you are using TCAV, you would need to ensure that training data, inference data, and even concept data have similar statistical properties. Otherwise, the concepts generated can become affected due to noise or data impurity issues:</p>
			<ul>
				<li><em class="italic">Guillaume Alain</em> and <em class="italic">Yoshua Bengio</em>, in their paper <em class="italic">Understanding intermediate layers using linear classifier probes</em> (<a href="https://arxiv.org/abs/1610.01644">https://arxiv.org/abs/1610.01644</a>), have expressed some concern about applying TCAV to shallower neural networks. Many similar research papers have suggested that concepts in deeper layers are more separable as compared to concepts in shallower <a id="_idIndexMarker597"/>networks and, hence, the use of TCAV is limited to mostly deep neural networks. </li>
				<li>Preparing a concept dataset can be a challenging and expensive task. Although you don't need ML knowledge to prepare a concept dataset, still, in practice, you do not expect any common end user to spend time creating an annotated concept dataset for any customized user-defined concept. </li>
				<li>I felt that the TCAV Python framework would require further improvements before being used in any production-level system. In my opinion, at the time of writing this chapter, this framework would need to mature further so that it can be used easily with any production-level ML system.</li>
			</ul>
			<p>I think all these limitations can indeed be solved to make TCAV a much more robust framework that is widely adopted. If you are interested, you can also reach out to authors and developers of the TCAV framework and contribute to the open source community! In the next section, let's discuss some potential applications of concept-based explanations. </p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor169"/>Potential applications of concept-based explanations</h1>
			<p>I do see great potential for <a id="_idIndexMarker598"/>concept-based explanations such as TCAV! In this section, you will get exposure to some potential applications of concept-based explanations that can be important research topics for the entire AI community, which are as follows:</p>
			<ul>
				<li><strong class="bold">Estimation of transparency and fairness in AI</strong>: Most regulatory concerns for black-box AI models are related to concepts such as gender, color, and race. Concept-based explanations can actually help to estimate whether an AI algorithm is fair in terms of these abstract concepts. The detection of bias for AI models can actually improve its transparency and help to address certain regulatory concerns. For example, in terms of doctors using deep learning models, TCAV can be used to detect whether the model is biased toward a specific gender, color, or race as ideally, these concepts are not important as regards the model's decision. High concept importance for these concepts indicates the presence of bias. <em class="italic">Figure 8.4</em> illustrates an example where TCAV is used to detect model bias.</li>
			</ul>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B18216_08_004.jpg" alt="Figure 8.4 – TCAV can be used to detect model bias based on concept importance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – TCAV can be used to detect model bias based on concept importance</p>
			<ul>
				<li><strong class="bold">Detection of adversarial attacks with CAV</strong>: If you go through the appendix of the TCAV research paper (<a href="https://arxiv.org/pdf/1711.11279.pdf">https://arxiv.org/pdf/1711.11279.pdf</a>), the authors have <a id="_idIndexMarker599"/>mentioned that the concept importance of actual samples and adversarial samples are quite different. This means that if an image gets impacted by an adversarial attack, the concept importance would also change. So, CAVs can be a potential method in detecting adversarial attacks, as discussed in <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Data-Centric Approaches</em>. </li>
			</ul>
			<ul>
				<li><strong class="bold">Concept-based image clustering</strong>: Using CAVs to cluster images based on similar concepts can be an interesting application. Deep learning-based image search engines are a common application in which clustering or similarity algorithms are applied to feature vectors to locate similar images. However, these are feature-based methods. Similarly, there is a potential to apply concept-based image clustering using CAVs.</li>
				<li><strong class="bold">Automated concept-based explanations (ACE)</strong>: <em class="italic">Ghorbani, Amirata</em>,<em class="italic"> James Wexler</em>, <em class="italic">James Zou</em>, <em class="italic">and Been Kim</em>, in their research work – <em class="italic">Towards automatic concept-based explanations</em>, mentioned an automated version of TCAV that goes through the training images and automatically discovers prominent concepts. This is an interesting work, as I think it can have an important application in identifying incorrectly labeled training data. In industrial applications, getting a perfectly labeled curated dataset is extremely challenging. This problem can be solved to a great extent using ACE.</li>
				<li><strong class="bold">Concept-based Counterfactual Explanation</strong>: In <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, we discussed <strong class="bold">counterfactual explanation (CFE)</strong> as a mechanism for generating actionable insights by suggesting changes to the input features that<a id="_idIndexMarker600"/> can change the overall outcome. However, CFE is a feature-based explanation method. It would be a really interesting topic of research to have a concept-based counterfactual explanation, which is one step closer to human-friendly explanations. </li>
			</ul>
			<p>For example, if we say that it is going to <em class="italic">rain</em> today although there is a <em class="italic">clear sky</em> now, we usually add a further explanation that suggests that the <em class="italic">clear sky</em> can be covered with <em class="italic">clouds</em>, which increases the probability of rainfall. In other words, a <em class="italic">clear sky</em> is a concept related to a <em class="italic">sunny</em> day, while a <em class="italic">cloudy sky</em> is a concept related to <em class="italic">rainfall</em>. This example suggests that the forecast can be flipped if the concept describing the situation is also flipped. Hence, this is the idea of the concept-based counterfactual. The idea is not very far-fetched as <strong class="bold">concept bottleneck models (CBMs)</strong> presented in the research work by <em class="italic">Koh et al.</em>, in <a href="https://arxiv.org/abs/2007.04612">https://arxiv.org/abs/2007.04612</a>, can implement a similar idea of generated concept-based counterfactuals by manipulating the neuron action of the bottleneck layer.</p>
			<p><em class="italic">Figure 8.5</em> illustrates an example of using a concept-based counterfactual example. There is no existing algorithm or framework that can help us achieve this, yet this can be a useful application of concept-based approaches in computer vision.</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B18216_08_005.jpg" alt="Figure 8.5 – An illustration of the idea of concept-based counterfactual examples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – An illustration of the idea of concept-based counterfactual examples</p>
			<p>I feel this is a wide-open<a id="_idIndexMarker601"/> research field and the potential to come up with game-changing applications using concept-based explanations is immense. I do sincerely hope that more and more researchers and AI developers start working on this area to make significant progress in the coming years! Thus, we have arrived at the end of this chapter. Let me now summarize what has been covered in this chapter in the next section.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor170"/>Summary</h1>
			<p>This chapter covers the concepts of TCAV, a novel approach, and a framework developed by Google AI. You have received a conceptual understanding of TCAV, practical exposure to applying the Python TCAV framework, learned about some key advantages and limitations of TCAV, and finally, I presented some interesting ideas regarding potential research problems that can be solved using concept-based explanations.</p>
			<p>In the next chapter, we will explore other popular XAI frameworks and apply these frameworks to solving practical problems.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor171"/>References</h1>
			<p>Please refer to the following resources to gain additional information:</p>
			<ul>
				<li><em class="italic">Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</em>: <a href="https://arxiv.org/pdf/1711.11279.pdf">https://arxiv.org/pdf/1711.11279.pdf</a></li>
				<li><em class="italic">TCAV Python framework -</em> <a href="https://github.com/tensorflow/tcav">https://github.com/tensorflow/tcav</a></li>
				<li><em class="italic">Koh et al. "Concept Bottleneck Models"</em>: <a href="https://arxiv.org/abs/2007.04612">https://arxiv.org/abs/2007.04612</a></li>
				<li><em class="italic">Guillaume Alain and Yoshua Bengio</em>, "<em class="italic">Understanding intermediate layers using linear classifier probes"</em>: <a href="https://arxiv.org/abs/1610.01644">https://arxiv.org/abs/1610.01644</a></li>
				<li><em class="italic">Ghorbani, Amirata, James Wexler, James Zou and Been Kim, "Towards automatic concept-based explanations"</em>: <a href="https://arxiv.org/abs/1902.03129">https://arxiv.org/abs/1902.03129</a></li>
				<li><em class="italic">Detecting Concepts, Chapter 10.3 Molnar, C. (2022). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (2nd ed.).</em>: <a href="https://christophm.github.io/interpretable-ml-book/detecting-concepts.html">https://christophm.github.io/interpretable-ml-book/detecting-concepts.html</a></li>
			</ul>
		</div>
	</body></html>