<html><head></head><body>
		<div id="_idContainer542">
			<h1 id="_idParaDest-120"><a id="_idTextAnchor129"/>Chapter 8:Graph Analysis for Credit Card Transactions</h1>
			<p>Analysis of financial data is one of the most common and important domains in big data and data analysis. Indeed, due to the increasing number of mobile devices and the introduction of a standard platform for online payment, the amount of transactional data that banks are producing and consuming is increasing exponentially.</p>
			<p>As a consequence, new tools and techniques are needed to exploit as much as we can from this huge amount of information in order to better understand customers' behavior and support data-driven decisions in business processes. Data can also be used to build better mechanisms to improve security in the online payment process. Indeed, as online payment systems are becoming increasingly popular due to e-commerce platforms, at the same time, cases of fraud are also increasing. An example of a fraudulent transaction is a transaction performed with a stolen credit card. Indeed, in this case, the fraudulent transactions will be different from the transactions made by the original owner of the credit card.</p>
			<p>However, building automatic procedures to detect fraudulent transactions could be a complex problem due to the large number of variables involved.</p>
			<p>In this chapter, we will describe how we can represent credit card transaction data as a graph in order to automatically detect fraudulent transactions using machine learning algorithms. We will start processing the dataset by applying some of the techniques and algorithms we described in previous chapters to build a fraud detection algorithm.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Generating a graph from credit card transactions</li>
				<li>Extraction of properties and communities from the graph</li>
				<li>Application of supervised and unsupervised machine learning algorithms to fraud classification</li>
			</ul>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor130"/>Technical requirements</h1>
			<p>We will be using <em class="italic">Jupyter</em> notebooks with <em class="italic">Python</em> 3.8 for all of our exercises. The following is a list of Python libraries that will be installed for this chapter using <strong class="source-inline">pip</strong>. For example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line:</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">scikit-learn==0.24.0</p>
			<p class="source-code">pandas==1.1.3</p>
			<p class="source-code">node2vec==0.3.3</p>
			<p class="source-code">numpy==1.19.2</p>
			<p class="source-code">communities==2.2.0</p>
			<p>In the rest of this book, unless clearly stated to the contrary, we will refer to <strong class="source-inline">nx</strong> as the results of the Python <strong class="source-inline">import networkx as nx</strong> command.</p>
			<p>All code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter08">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter08</a>.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor131"/>Overview of the dataset</h1>
			<p>The dataset <a id="_idIndexMarker812"/>used in this chapter is the <em class="italic">Credit Card Transactions Fraud Detection Dataset</em> available on <em class="italic">Kaggle</em> at the following URL: <a href="https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv">https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv</a>.</p>
			<p>The dataset is made up of simulated credit card transactions containing legitimate and fraudulent transactions for the period January 1, 2019 – December 31, 2020. It includes the credit cards of 1,000 customers performing transactions with a pool of 800 merchants. The dataset was generated using <em class="italic">Sparkov Data Generation</em>. More information about the generation algorithm is available at the following URL: <a href="https://github.com/namebrandon/Sparkov_Data_Generation">https://github.com/namebrandon/Sparkov_Data_Generation</a>.</p>
			<p>For each transaction, the dataset contains 23 different features. In the following table, we will show only the information that will be used in this chapter:</p>
			<div>
				<div id="_idContainer511" class="IMG---Figure">
					<img src="image/B16069_08_01.jpg" alt="Table 8.1 – List of variables used in the dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.1 – List of variables used in the dataset</p>
			<p>For the purposes of our <a id="_idIndexMarker813"/>analysis, we will use the <strong class="source-inline">fraudTrain.csv</strong> file. As already suggested, take a look at the dataset by yourself. It is strongly suggested to explore and become as comfortable as possible with the dataset before starting any machine learning task. We also suggest that you investigate two other datasets that will not be covered in this chapter. The first one is the Czech Bank's Financial Analysis dataset, available at https://github.com/Kusainov/czech-banking-fin-analysis. This dataset came from an actual Czech bank in 1999, for the period covering 1993 – 1998. The data pertaining to clients and their accounts consists of directed relations. Unfortunately, there are no labels on the transactions, making it impossible to train a fraud detection engine using machine learning techniques. The second dataset is the paysim1 dataset, available at <a href="https://www.kaggle.com/ntnu-testimon/paysim1">https://www.kaggle.com/ntnu-testimon/paysim1</a>. This dataset comprises simulated mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, which is the provider of the mobile financial service and is currently running in more than 14 countries across the globe. This dataset also contains labels on fraudulent/genuine transactions.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor132"/>Loading the dataset and graph building using networkx</h2>
			<p>The first step of our analysis <a id="_idIndexMarker814"/>will be to load the dataset and build a graph. Since the dataset represents a simple list of transactions, we need to perform <a id="_idIndexMarker815"/>several operations to build the final credit card transaction graph. The dataset is a simple CSV file; we can use <strong class="source-inline">pandas</strong> to load the data as follows:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">df = df[df["is_fraud"]==0].sample(frac=0.20, random_state=42).append(df[df["is_fraud"] == 1])</p>
			<p>In order to help the <a id="_idIndexMarker816"/>reader deal with the dataset, we selected 20% of the genuine transactions and all of the fraudulent transactions. As a result, from a <a id="_idIndexMarker817"/>total of 1,296,675 transactions, we will only use 265,342 transactions. Moreover, we can also investigate the number of fraudulent and genuine transactions in our dataset as follows:</p>
			<p class="source-code">df["is_fraud"].value_counts()</p>
			<p>By way of a result, we get the following:</p>
			<p class="source-code">0    257834</p>
			<p class="source-code">1      7506</p>
			<p>In other words, from a total of 265,342 transactions, only <strong class="source-inline">7506</strong> (2.83 %) are fraudulent transactions, while the others are genuine.</p>
			<p>The dataset can be represented as a graph using the <strong class="source-inline">networkx</strong> library. Before starting with the technical description, we will start by specifying how the graph is built from the data. We used two different approaches to build the graph, namely, the bipartite and tripartite approaches, as described in the paper <em class="italic">APATE: A Novel Approach for Automated Credit Card Transaction Fraud Detection Using Network-Based Extensions</em>, available at https://www.scinapse.io/papers/614715210.</p>
			<p>For the <strong class="bold">bipartite approach</strong>, we build a <a id="_idIndexMarker818"/>weighted bipartite graph <img src="image/B16069_08_001.png" alt=""/> w<a id="_idTextAnchor133"/>here <img src="image/B16069_08_002.png" alt=""/>, where each node <img src="image/B16069_08_003.png" alt=""/> represents a customer, and each node <img src="image/B16069_08_004.png" alt=""/> represents a merchant. An edge <img src="image/B16069_08_005.png" alt=""/> is created if a transaction exists from the customer, <img src="image/B16069_08_006.png" alt=""/>, to the merchant, <img src="image/B16069_08_007.png" alt=""/>. Finally, to each edge of the graph, we assign an (always positive) weight representing the amount (in US dollars) of the transaction. In our formalization, we allow both directed and undirected graphs.</p>
			<p>Since the dataset represents temporal transactions, multiple interactions can happen between a customer and a merchant. In both our formalizations, we decided to collapse all that information in a <a id="_idIndexMarker819"/>single graph. In other words, if multiple transactions are present between a customer and a merchant, we will build a single <a id="_idIndexMarker820"/>edge between the two nodes with its weight given by the sum of all the <a id="_idIndexMarker821"/>transaction amounts. A graphical representation of the direct bipartite graph is visible in <em class="italic">Figure 8.1</em>:</p>
			<div>
				<div id="_idContainer519" class="IMG---Figure">
					<img src="image/B16069_08_011.jpg" alt="Figure 8.1 – Bipartite graph generated from the input dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Bipartite graph generated from the input dataset</p>
			<p>The bipartite graph we <a id="_idIndexMarker822"/>defined can be built using the following code:</p>
			<p class="source-code">def build_graph_bipartite(df_input, graph_type=nx.Graph()):</p>
			<p class="source-code">    df = df_input.copy()</p>
			<p class="source-code">    mapping = {x:node_id for node_id,x in enumerate(set(df["cc_num"].values.tolist() + df["merchant"].values.tolist()))}</p>
			<p class="source-code">    df["from"] = df["cc_num"].apply(lambda x: mapping[x])</p>
			<p class="source-code">    df["to"] = df["merchant"].apply(lambda x: mapping[x])</p>
			<p class="source-code">    df = df[['from', 'to', "amt", "is_fraud"]].groupby(['from', 'to']).agg({"is_fraud": "sum", "amt": "sum"}).reset_index()</p>
			<p class="source-code">    df["is_fraud"] = df["is_fraud"].apply(lambda x: 1 if x&gt;0 else 0)</p>
			<p class="source-code">    G = nx.from_edgelist(df[["from", "to"]].values, create_using=graph_type)</p>
			<p class="source-code">    nx.set_edge_attributes(G, {(int(x["from"]), int(x["to"])):x["is_fraud"] for idx, x in df[["from","to","is_fraud"]].iterrows()}, "label")</p>
			<p class="source-code">    nx.set_edge_attributes(G,{(int(x["from"]), int(x["to"])):x["amt"] for idx, x in df[["from","to","amt"]].iterrows()}, "weight")</p>
			<p class="source-code">    return G</p>
			<p>The code is quite simple. To build the <a id="_idIndexMarker823"/>bipartite credit card transaction graph, we use different <strong class="source-inline">networkx</strong> functions. To go more in depth, the <a id="_idIndexMarker824"/>operations we <a id="_idIndexMarker825"/>performed in the <a id="_idIndexMarker826"/>code are as follows:</p>
			<ol>
				<li>We built a map to assign a <strong class="source-inline">node_id</strong> to each merchant or customer.</li>
				<li>Multiple transactions are aggregated in a single transaction.</li>
				<li>The <strong class="source-inline">networkx</strong> function, <strong class="source-inline">nx.from_edgelist</strong>, is used to build the networkx graph.</li>
				<li>Two attributes, namely, <strong class="source-inline">weight</strong> and <strong class="source-inline">label</strong>, are assigned to each edge. The former represents the total number of transactions between the two nodes, whereas the latter indicates whether the transaction is genuine or fraudulent. </li>
			</ol>
			<p>As we can also see from the code, we can select whether we want to build a directed or an undirected graph. We can build an undirected graph by calling the following function:</p>
			<p class="source-code">G_bu = build_graph_bipartite(df, nx.Graph(name="Bipartite Undirect"))))</p>
			<p>We can instead build a direct graph by calling the following function:</p>
			<p class="source-code">G_bd = build_graph_bipartite(df, nx.DiGraph(name="Bipartite Direct"))))</p>
			<p>The only difference is given by the second parameter we pass in the constructor.</p>
			<p>The <strong class="bold">tripartite approach</strong> is an <a id="_idIndexMarker827"/>extension of the previous one, also allowing the transactions to be represented as a vertex. If, on the one hand, this approach drastically increases network complexity, on the other hand, it allows extra node embeddings to be built for merchants and cardholders and every transaction. Formally for this approach, we build a weighted tripartite graph, <img src="image/B16069_08_008.png" alt=""/>, where <img src="image/B16069_08_009.png" alt=""/>, where each node <img src="image/B16069_08_010.png" alt=""/> represents a customer, each node <img src="image/B16069_08_011.png" alt=""/> represents a merchant, and each node <img src="image/B16069_08_012.png" alt=""/> is a transaction. Two edges <img src="image/B16069_08_013.png" alt=""/> and <img src="image/B16069_08_014.png" alt=""/> are created for each transaction, <img src="image/B16069_08_015.png" alt=""/>, from customer <img src="image/B16069_08_016.png" alt=""/> to the merchant <img src="image/B16069_08_017.png" alt=""/>. </p>
			<p>Finally, to each edge of the <a id="_idIndexMarker828"/>graph, we assign an (always positive) weight representing the amount (in US dollars) of the transaction. Since, in this case, we create a <a id="_idIndexMarker829"/>node for each transaction, we do not need to aggregate multiple transactions from a customer to a merchant. Moreover, as for the <a id="_idIndexMarker830"/>other approach, in our <a id="_idIndexMarker831"/>formalization, we allow both directed and undirected graphs. A graphical representation of the direct bipartite graph is visible in <em class="italic">Figure 8.2</em>:</p>
			<div>
				<div id="_idContainer530" class="IMG---Figure">
					<img src="image/B16069_08_02.jpg" alt="Figure 8.2 – Tripartite graph generated from the input dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Tripartite graph generated from the input dataset</p>
			<p>The <a id="_idIndexMarker832"/>tripartite <a id="_idIndexMarker833"/>graph we <a id="_idIndexMarker834"/>defined can be <a id="_idIndexMarker835"/>built using the following code:</p>
			<p class="source-code">def build_graph_tripartite(df_input, graph_type=nx.Graph()):</p>
			<p class="source-code">    df = df_input.copy()</p>
			<p class="source-code">    mapping = {x:node_id for node_id,x in enumerate(set(df.index.values.tolist() + df["cc_num"].values.tolist() + df["merchant"].values.tolist()))}</p>
			<p class="source-code">    df["in_node"] = df["cc_num"].apply(lambda x: mapping[x])</p>
			<p class="source-code">    df["out_node"] = df["merchant"].apply(lambda x: mapping[x])</p>
			<p class="source-code">    G = nx.from_edgelist([(x["in_node"], mapping[idx]) for idx, x in df.iterrows()] + [(x["out_node"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)</p>
			<p class="source-code">    nx.set_edge_attributes(G,{(x["in_node"], mapping[idx]):x["is_fraud"] for idx, x in df.iterrows()}, "label")</p>
			<p class="source-code">    nx.set_edge_attributes(G,{(x["out_node"], mapping[idx]):x["is_fraud"] for idx, x in df.iterrows()}, "label")</p>
			<p class="source-code">    nx.set_edge_attributes(G,{(x["in_node"], mapping[idx]):x["amt"] for idx, x in df.iterrows()}, "weight")</p>
			<p class="source-code">    nx.set_edge_attributes(G,{(x["out_node"], mapping[idx]):x["amt"] for idx, x in df.iterrows()}, "weight")</p>
			<p class="source-code">    return G</p>
			<p>The code is <a id="_idIndexMarker836"/>quite simple. To <a id="_idIndexMarker837"/>build the <a id="_idIndexMarker838"/>tripartite <a id="_idIndexMarker839"/>credit card transaction graph, we use different <strong class="source-inline">networkx</strong> functions. To go more in depth, the operations we performed in the code are as follows:</p>
			<ol>
				<li value="1">We built a map to assign a <strong class="source-inline">node_id</strong> to each merchant, customer, and transaction.</li>
				<li>The <strong class="source-inline">networkx</strong> function, <strong class="source-inline">nx.from_edgelist</strong>, is used to build the networkx graph,</li>
				<li>Two attributes, namely, <strong class="source-inline">weight</strong> and <strong class="source-inline">label</strong>, are assigned to each edge. The former represents the total number of transactions between the two nodes, whereas the latter indicates whether the transaction is genuine or fraudulent. </li>
			</ol>
			<p>As we can also see from the code, we can select whether we want to build a directed or an undirected graph. We can build an undirected graph by calling the following function:</p>
			<p class="source-code">G_tu = build_graph_tripartite(df, nx.Graph(name="Tripartite Undirect"))</p>
			<p>We can instead build a direct graph by calling the following function:</p>
			<p class="source-code">G_td = build_graph_tripartite(df, nx.DiGraph(name="Tripartite Direct"))</p>
			<p>The only difference is given by the second parameter we pass in the constructor. </p>
			<p>In the formalized <a id="_idIndexMarker840"/>graph representation that we introduced, the real <a id="_idIndexMarker841"/>transactions are represented as edges. According to this structure for both bipartite and tripartite graphs, the classification of fraudulent/genuine <a id="_idIndexMarker842"/>transactions is described as an edge classification task. In this task, the goal is to <a id="_idIndexMarker843"/>assign to a given edge a label (<strong class="source-inline">0</strong> for genuine, <strong class="source-inline">1</strong> for fraudulent) describing whether the transaction the edge represents is fraudulent or genuine.</p>
			<p>In the rest of this chapter, we use for our analysis both bipartite and tripartite undirected graphs, denoted by the Python variables <strong class="source-inline">G_bu</strong> and <strong class="source-inline">G_tu</strong>, respectively. We will leave it to you, as an exercise, an extension of the analyses proposed in this chapter to direct graphs.</p>
			<p>We begin our analysis with a simple check to validate whether our graph is a real bipartite graph using the following line:</p>
			<p class="source-code">from networkx.algorithms import bipartite</p>
			<p class="source-code">all([bipartite.is_bipartite(G) for G in [G_bu,G_tu]]</p>
			<p>As result, we get <strong class="source-inline">True</strong>. This check gives us the certainty that the two graphs are actually bipartite/tripartite graphs.</p>
			<p>Moreover, using the following command, we can get some basic statistics:</p>
			<p class="source-code">for G in [G_bu, G_tu]:</p>
			<p class="source-code"> print(nx.info(G))</p>
			<p>By way of a result, we get the following:</p>
			<p class="source-code">Name: Bipartite Undirect</p>
			<p class="source-code">Type: Graph</p>
			<p class="source-code">Number of nodes: 1676</p>
			<p class="source-code">Number of edges: 201725</p>
			<p class="source-code">Average degree: 240.7220</p>
			<p class="source-code">Name: Tripartite Undirect</p>
			<p class="source-code">Type: Graph</p>
			<p class="source-code">Number of nodes: 267016</p>
			<p class="source-code">Number of edges: 530680</p>
			<p class="source-code">Average degree:   3.9749</p>
			<p>As we can see, the two <a id="_idIndexMarker844"/>graphs differ in both, the number of nodes and the number of edges. The bipartite undirected graph has 1,676, equal to the number of <a id="_idIndexMarker845"/>customers plus the number of merchants with a high number of edges (201,725). The tripartite undirected graph has 267,016, equal to the <a id="_idIndexMarker846"/>number of customers plus the <a id="_idIndexMarker847"/>number of merchants plus all the transactions.</p>
			<p>In this graph, the number of nodes, as expected, is higher (530,680) compared to the bipartite graph. The interesting difference in this comparison is given by the average degree of the two graphs. Indeed, the average degree of the bipartite graph is higher compared to the tripartite graph, as expected. Indeed, since, in the tripartite graph, the connections are "split" by the presence of the transaction nodes, the average degree is lower.</p>
			<p>In the next section, we will describe how we can now use the transaction graphs generated to perform a more complete statistical analysis.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor134"/>Network topology and community detection</h1>
			<p>In this section, we are going to analyze some graph metrics to have a clear picture of the general structure of the graph. We will be using <strong class="source-inline">networkx</strong> to compute most of the useful metrics we have seen in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>. We will try to interpret the metrics to gain insights into the graph.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor135"/>Network topology</h2>
			<p>A good starting point for our <a id="_idIndexMarker848"/>analysis is the extraction of simple graph metrics to have a general understanding of the main properties of bipartite and tripartite transaction graphs.</p>
			<p>We start by looking at the distribution of the degree for both bipartite and tripartite graphs using the following code:</p>
			<p class="source-code">for G in [G_bu, G_tu]:</p>
			<p class="source-code">  plt.figure(figsize=(10,10))</p>
			<p class="source-code">  degrees = pd.Series({k: v for k, v in nx.degree(G)})</p>
			<p class="source-code">  degrees.plot.hist()</p>
			<p class="source-code">  plt.yscale("log")</p>
			<p>By way of a result, we get the plot in the following diagram:</p>
			<div>
				<div id="_idContainer531" class="IMG---Figure">
					<img src="image/B16069_08_03.jpg" alt="Figure 8.3 – Degree distribution for bipartite (left) and tripartite (right) graphs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Degree distribution for bipartite (left) and tripartite (right) graphs</p>
			<p>From <em class="italic">Figure 8.3</em>, it is possible to <a id="_idIndexMarker849"/>see how the distribution of nodes reflects the average degree we previously saw. In greater detail, the bipartite graph has a more variegate distribution, with a peak of around 300. For the tripartite graph, the distribution has a big peak for degree 2, while the other part of the tripartite degree distribution is similar to the bipartite distribution. These distributions completely reflect the differences in how the two graphs were defined. Indeed, if bipartite graphs are made by connections from the customer to the merchant, in the tripartite graph, all the connections pass through the transaction nodes. Those nodes are the majority in the graph, and they all have a degree of 2 (an edge from a custom and an edge to a merchant). As a consequence, the frequency in the bin representing degree 2 is equal to the number of transaction nodes.</p>
			<p>We will continue our investigation by analyzing the <strong class="source-inline">edges weight</strong> distribution:</p>
			<ol>
				<li value="1">We begin by computing the quantile distribution:<p class="source-code">for G in [G_bu, G_tu]:</p><p class="source-code">  allEdgesWeights = pd.Series({(d[0], d[1]): d[2]["weight"] for d in G.edges(data=True)})</p><p class="source-code">  np.quantile(allEdgesWeights.values,[0.10,0.50,0.70,0.9])</p></li>
				<li>By way of a result, we get the following:<p class="source-code">array([  5.03 ,  58.25 ,  98.44 , 215.656])</p><p class="source-code"> array([  4.21,  48.51,  76.4 , 147.1 ])</p></li>
				<li>Using the same command as before, we <a id="_idIndexMarker850"/>can also plot (in log scale) the distribution of <strong class="source-inline">edges weight</strong>, cut to the 90<span class="superscript">th</span> percentile. The result is visible in the following diagram:<div id="_idContainer532" class="IMG---Figure"><img src="image/B16069_08_04.jpg" alt="Figure 8.4 – Edge weight distribution for bipartite (left) and tripartite (right) graphs&#13;&#10;"/></div><p class="figure-caption">Figure 8.4 – Edge weight distribution for bipartite (left) and tripartite (right) graphs</p><p>We can see how, due to the aggregation of the transaction having the same customer and merchant, the distribution of the bipartite graph is shifted to the right (high values) compared to the tripartite graph, where edge weights were not computed, aggregating multiple transactions.</p></li>
				<li>We will now investigate the <strong class="source-inline">betweenness centrality</strong> metric. It measures how many shortest paths pass through a given node, giving an idea of how <em class="italic">central</em> that node is for the <a id="_idIndexMarker851"/>spreading of information inside the network. We can compute the distribution of node centrality by using the following command:<p class="source-code">for G in [G_bu, G_tu]:</p><p class="source-code">  plt.figure(figsize=(10,10))</p><p class="source-code">  bc_distr = pd.Series(nx.betweenness_centrality(G))</p><p class="source-code">  bc_distr.plot.hist()</p><p class="source-code">  plt.yscale("log")</p></li>
				<li>As result, we get the following distributions:<div id="_idContainer533" class="IMG---Figure"><img src="image/B16069_08_05.jpg" alt="Figure 8.5 – Betweenness centrality distribution for bipartite (left) and tripartite (right) graphs&#13;&#10;"/></div><p class="figure-caption">Figure 8.5 – Betweenness centrality distribution for bipartite (left) and tripartite (right) graphs</p><p>As expected, for both graphs, the betweenness centrality is low. This can be understood due to the large number of non-bridging nodes inside the network. Similar to what we saw for the degree distribution, the distribution of betweenness centrality values is different in the two graphs. Indeed, if the bipartite graph has a more variegate distribution with a mean of 0.00072, in the tripartite graph, the <a id="_idIndexMarker852"/>transaction nodes are the ones that mainly move the distribution values and lower the mean to 1.38e-05. Also, in this case, we can see that the distribution for the tripartite graph has a big peak, representing the transaction nodes, and the rest of the distribution is quite similar to the bipartite distribution.</p></li>
				<li>We can finally compute the assortativity of the two graphs using the following code:<p class="source-code">for G in [G_bu, G_tu]:</p><p class="source-code">   print(nx.degree_pearson_correlation_coefficient(G)) </p></li>
				<li>By way of a result, we get the following:<p class="source-code">-0.1377432041049189</p><p class="source-code">-0.8079472914876812</p></li>
			</ol>
			<p>Here, we can observe how both graphs have a negative assortativity, likely showing that well-connected individuals associate with poor-connected individuals. For the bipartite graph, the value is low (-0.14), since customers who have a low degree are only connected with merchants who have high degrees due to the high number of incoming transactions. The assortativity is even lower (-0.81) for the tripartite graph. This behavior is expected due to the presence of the transaction nodes. Indeed, those nodes always have a degree of 2, and they are linked to customers and merchants represented by highly connected nodes.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor136"/>Community detection</h2>
			<p>Another interesting analysis <a id="_idIndexMarker853"/>we can perform is community detection. This analysis can help to identify specific fraudulent patterns:</p>
			<ol>
				<li value="1">The code to perform community extraction is as follows:<p class="source-code">import community</p><p class="source-code">for G in [G_bu, G_tu]:</p><p class="source-code">   parts = community.best_partition(G, random_state=42, weight='weight')</p><p class="source-code">   communities = pd.Series(parts)   print(communities.value_counts().sort_values(ascending=False))</p><p>In this code, we simply use the <strong class="source-inline">community</strong> library to extract the communities in the input graph. We then print the communities detected by the algorithms, sorted according to the number of nodes contained.</p></li>
				<li>For the bipartite graph, we <a id="_idIndexMarker854"/>obtain the following output:<p class="source-code">5     546</p><p class="source-code">0     335</p><p class="source-code">7     139</p><p class="source-code">2     136</p><p class="source-code">4     123</p><p class="source-code">3     111</p><p class="source-code">8      83</p><p class="source-code">9      59</p><p class="source-code">10     57</p><p class="source-code">6      48</p><p class="source-code">11     26</p><p class="source-code">1      13</p></li>
				<li>For the tripartite graph, we obtain the following output:<p class="source-code">11     4828</p><p class="source-code">3      4493</p><p class="source-code">26     4313</p><p class="source-code">94     4115</p><p class="source-code">8      4036</p><p class="source-code">    ... 47     1160</p><p class="source-code">103    1132</p><p class="source-code">95      954</p><p class="source-code">85      845</p><p class="source-code">102     561</p></li>
				<li>Due to a large number of nodes in the tripartite graph, we found 106 communities (we reported just a subset of them), while, for the bipartite graph, only 12 communities <a id="_idIndexMarker855"/>were found. As consequence, to have a clear picture, for the tripartite graph, it is better to plot the distribution of the nodes contained in the different communities using the following command:<p class="source-code">communities.value_counts().plot.hist(bins=20)</p></li>
				<li>By way of a result, we get the following:<div id="_idContainer534" class="IMG---Figure"><img src="image/B16069_08_06.jpg" alt="Figure 8.6 – Distribution of communities' node size&#13;&#10;"/></div><p class="figure-caption">Figure 8.6 – Distribution of communities' node size</p><p>From the diagram, it is possible to see how the peak is reached around 2,500. This means that more than 30 large communities have more than 2,000 nodes. From the plot, it is also possible to see that a few communities have fewer than 1,000 nodes and more than 3,000 nodes.</p></li>
				<li>For each set of <a id="_idIndexMarker856"/>communities detected by the algorithm, we can compute the percentage of fraudulent transactions. The goal of this analysis is to identify specific sub-graphs where there is a high concentration of fraudulent transactions:<p class="source-code">graphs = []</p><p class="source-code">d = {}</p><p class="source-code">for x in communities.unique():</p><p class="source-code">    tmp = nx.subgraph(G, communities[communities==x].index)</p><p class="source-code">    fraud_edges = sum(nx.get_edge_attributes(tmp, "label").values())</p><p class="source-code">    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100</p><p class="source-code">    d[x] = ratio</p><p class="source-code">    graphs += [tmp]</p><p class="source-code">print(pd.Series(d).sort_values(ascending=False))</p></li>
				<li>The code simply generates a node-induced subgraph by using the nodes contained in a specific community. The graph is used to compute the percentage of fraudulent transactions as a ratio of the number of fraudulent edges over the number of all the edges in the graph. We can also plot a node-induced subgraph detected by the community detection algorithm by using the following code:<p class="source-code">gId = 10</p><p class="source-code">spring_pos = nx.spring_layout(graphs[gId])</p><p class="source-code"> edge_colors = ["r" if x == 1 else "g" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]</p><p class="source-code">nx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, edge_color=edge_colors, with_labels=False, node_size=15)</p><p>Given a particular <a id="_idIndexMarker857"/>community index, <strong class="source-inline">gId</strong>, the code extracts the node-induced subgraph, using the node available in the <strong class="source-inline">gId</strong> community index, and plots the graph obtained.</p></li>
				<li>By running the two algorithms on the bipartite graph, we will obtain the following:<p class="source-code">9     26.905830</p><p class="source-code">10    25.482625</p><p class="source-code">6     22.751323</p><p class="source-code">2     21.993834</p><p class="source-code">11    21.333333</p><p class="source-code">3     20.470263</p><p class="source-code">8     18.072289</p><p class="source-code">4     16.218905</p><p class="source-code">7      6.588580</p><p class="source-code">0      4.963345</p><p class="source-code">5      1.304983</p><p class="source-code">1      0.000000</p></li>
				<li>For each community, we have <a id="_idIndexMarker858"/>the percentage of its fraudulent edges. To have a better description of the subgraph, we can plot community 10 by executing the previous line of code using <strong class="source-inline">gId=10</strong>. As a result, we get the following:<div id="_idContainer535" class="IMG---Figure"><img src="image/B16069_08_07.jpg" alt="Figure 8.7 – Induced subgraph of community 10 for the bipartite graph&#13;&#10;"/></div><p class="figure-caption">Figure 8.7 – Induced subgraph of community 10 for the bipartite graph</p></li>
				<li>The image of the induced subgraph allows us to better understand whether specific patterns are visible in the data. Running the same algorithms on the tripartite graph, we obtain the following output:<p class="source-code">6      6.857728</p><p class="source-code">94     6.551151</p><p class="source-code">8      5.966981</p><p class="source-code">1      5.870918</p><p class="source-code">89     5.760271</p><p class="source-code">      ...   </p><p class="source-code">102    0.889680</p><p class="source-code">72     0.836013</p><p class="source-code">85     0.708383</p><p class="source-code">60     0.503461</p><p class="source-code">46     0.205170</p></li>
				<li>Due to the large number of <a id="_idIndexMarker859"/>communities, we can plot the distribution of the fraudulent over genuine ratio with the following command:<p class="source-code">pd.Series(d).plot.hist(bins=20)</p></li>
				<li>By way of a result, we get the following:<div id="_idContainer536" class="IMG---Figure"><img src="image/B16069_08_08.jpg" alt="Figure 8.8 – Distribution of communities' fraudulent/genuine edge ratio&#13;&#10;"/></div><p class="figure-caption">Figure 8.8 – Distribution of communities' fraudulent/genuine edge ratio</p><p>From the diagram, we can <a id="_idIndexMarker860"/>observe that a large part of the distribution is around communities having a ratio of between 2 and 4. There are a few communities with a low ratio (&lt;1) and with a high ratio (&gt;5).</p></li>
				<li>Also, for the tripartite graph, we can plot community 6 (with a ratio of 6.86), made by 1,935 nodes, by executing the previous line of code using <strong class="source-inline">gId=6</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer537" class="IMG---Figure">
					<img src="image/B16069_08_09.jpg" alt="Figure 8.9 – Induced subgraph of community 6 for the tripartite graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Induced subgraph of community 6 for the tripartite graph</p>
			<p>As for the bipartite use case, in this image, we <a id="_idIndexMarker861"/>can see an interesting pattern that could be used to perform a deeper exploration of some important graph sub-regions.</p>
			<p>In this section, we perform some explorative tasks to better understand the graphs and their properties. We also gave an example describing how a community detection algorithm can be used to spot patterns in the data. In the next section, we will describe how machine learning can be used to automatically detect fraudulent transactions.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor137"/>Embedding for supervised and unsupervised fraud detection</h1>
			<p>In this section, we will <a id="_idIndexMarker862"/>describe how the bipartite and tripartite graphs described previously can be used by graph machine learning algorithms to build <a id="_idIndexMarker863"/>automatic procedures for fraud detection using supervised and unsupervised approaches. As we already discussed at the beginning of this chapter, transactions are represented by edges, and we then want to classify each edge in the correct class: fraudulent or genuine.</p>
			<p>The pipeline we will use to perform the classification task is the following:</p>
			<ul>
				<li>A sampling procedure for the imbalanced task</li>
				<li>The use of an unsupervised embedding algorithm to create a feature vector for each edge</li>
				<li>The application of <a id="_idIndexMarker864"/>supervised and unsupervised <a id="_idIndexMarker865"/>machine learning algorithms to the feature space defined in the previous point</li>
			</ul>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor138"/>Supervised approach to fraudulent transaction identification</h2>
			<p>Since our dataset is <a id="_idIndexMarker866"/>strongly imbalanced, with fraudulent transactions representing 2.83% of total transactions, we need to apply some techniques to deal with unbalanced data. In this use case, we will apply a simple random undersampling strategy. Going into more depth, we will take a <a id="_idIndexMarker867"/>subsample of the majority class (genuine transactions) to match the number of samples of the minority class (fraudulent transactions). This is just one of the many techniques available in literature. It is also possible to use outlier detection algorithms, such as isolation forest, to detect fraudulent transactions as outliers in the data. We leave it to you, as an exercise, to extend the analyses using other techniques to deal with imbalanced data, such as random oversampling or using cost-sensitive classifiers for the classification task. Specific techniques for node and edge sampling that can be directly applied to the graph will be described in <a href="B16069_10_Final_JM_ePub.xhtml#_idTextAnchor150"><em class="italic">Chapter 10</em></a>,<em class="italic"> Novel Trends on Graphs</em>:</p>
			<ol>
				<li value="1">The code we use for random undersampling is as follows:<p class="source-code">from sklearn.utils import resample</p><p class="source-code">df_majority = df[df.is_fraud==0]</p><p class="source-code"> df_minority = df[df.is_fraud==1]</p><p class="source-code">df_maj_dowsampled = resample(df_majority, n_samples=len(df_minority), random_state=42)</p><p class="source-code">df_downsampled = pd.concat([df_minority, df_maj_dowsampled])</p><p class="source-code"> G_down = build_graph_bipartite(df_downsampled, nx.Graph())</p></li>
				<li>The code is straightforward. We applied the <strong class="source-inline">resample</strong> function of the <strong class="source-inline">sklearn</strong> package to filter the <strong class="source-inline">downsample</strong> function of the original data frame. We then build a graph using the function defined at the beginning of the chapter. To create the tripartite graph, the <strong class="source-inline">build_graph_tripartite</strong> function should be used. As the next step, we split the dataset into training and validation with a ratio of 80/20:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">train_edges, val_edges, train_labels, val_labels = train_test_split(list(range(len(G_down.edges))), list(nx.get_edge_attributes(G_down, "label").values()), test_size=0.20, random_state=42)</p><p class="source-code"> edgs = list(G_down.edges)</p><p class="source-code">train_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()</p><p class="source-code">train_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))</p><p>As before, also in <a id="_idIndexMarker868"/>this case, the <a id="_idIndexMarker869"/>code is straightforward since we simply apply the <strong class="source-inline">train_test_split</strong> function of the <strong class="source-inline">sklearn</strong> package.</p></li>
				<li>We can now build the feature space using the <strong class="source-inline">Node2Vec</strong> algorithm as follows:<p class="source-code">from node2vec import Node2Vec</p><p class="source-code">node2vec = Node2Vec(train_graph, weight_key='weight')</p><p class="source-code"> model = node2vec_train.fit(window=10)</p><p>The <strong class="source-inline">node2vec</strong> results are used to build, as described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, the edge embedding that will generate the final feature space used by the classifier.</p></li>
				<li>The code to perform this task is the following:<p class="source-code">from sklearn import metrics</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier </p><p class="source-code">from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder</p><p class="source-code">classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]</p><p class="source-code">for cl in classes:</p><p class="source-code">    embeddings = cl(keyed_vectors=model.wv)</p><p class="source-code">    train_embeddings = [embeddings[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]</p><p class="source-code">    val_embeddings = [embeddings[str(edgs[x][0]), str(edgs[x][1])] for x in val_edges]</p><p class="source-code">    rf = RandomForestClassifier(n_estimators=1000, random_state=42)</p><p class="source-code">    rf.fit(train_embeddings, train_labels)</p><p class="source-code">    y_pred = rf.predict(val_embeddings)</p><p class="source-code">    print(cl)</p><p class="source-code">    print('Precision:', metrics.precision_score(val_labels, y_pred))</p><p class="source-code">    print('Recall:', metrics.recall_score(val_labels, y_pred))</p><p class="source-code">    print('F1-Score:', metrics.f1_score(val_labels, y_pred))</p></li>
			</ol>
			<p>Different steps are <a id="_idIndexMarker870"/>performed <a id="_idIndexMarker871"/>compared to the previous code:</p>
			<ol>
				<li value="1">For each <strong class="source-inline">Edge2Vec</strong> algorithm, the previously computed <strong class="source-inline">Node2Vec</strong> algorithm is used to generate the feature space.</li>
				<li>A <strong class="source-inline">RandomForestClassifier</strong> from the <strong class="source-inline">sklearn</strong> Python library is trained on the feature set generated in the previous step.</li>
				<li>Different performance metrics, namely, precision, recall, and F1-score, are computed on the validation test.</li>
			</ol>
			<p>We can apply the code we <a id="_idIndexMarker872"/>previously described to both bipartite and tripartite graphs to solve the fraud detection task. In the <a id="_idIndexMarker873"/>following table, we report the performances for the bipartite graph:</p>
			<div>
				<div id="_idContainer538" class="IMG---Figure">
					<img src="image/B16069_08_021.jpg" alt="Table 8.2 – Supervised fraud edge classification performances for a bipartite graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.2 – Supervised fraud edge classification performances for a bipartite graph</p>
			<p>In the following table, we report the performances for the tripartite graph:</p>
			<div>
				<div id="_idContainer539" class="IMG---Figure">
					<img src="image/B16069_08_031.jpg" alt="Table 8.3 – Supervised fraud edge classification performances for a tripartite graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.3 – Supervised fraud edge classification performances for a tripartite graph</p>
			<p>In <em class="italic">Table 8.2</em> and <em class="italic">Table 8.3</em>, we reported the classification performances obtained using bipartite and tripartite graphs. As we can see from the results, the two methods, in terms of F1-score, precision, and recall, show significant differences. Since, for both graph types, Hadamard and average edge embedding algorithms give the most interesting results, we are going to focus our attention on those two. Going into more detail, the tripartite graph has a better precision compared to the bipartite graph (0.89 and 0.74 for the tripartite graph versus 0.73 and 0.71 for the bipartite graph).</p>
			<p>In contrast, the <a id="_idIndexMarker874"/>bipartite graph has a better recall compared to the tripartite graph (0.76 and 0.79 for the bipartite graph <a id="_idIndexMarker875"/>versus 0.29 and 0.45 for the tripartite graph). We can therefore conclude that in this specific case, the use of a bipartite graph could be a better choice since it achieves high performances in terms of F1 with a smaller graph (in terms of nodes and edges) compared to the tripartite graph.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor139"/>Unsupervised approach to fraudulent transaction identification</h2>
			<p>The same approach can also be <a id="_idIndexMarker876"/>applied in unsupervised tasks using k-means. The main difference is that the generated <a id="_idIndexMarker877"/>feature space will not undergo a train-validation split. Indeed, in the following code, we will compute the <strong class="source-inline">Node2Vec</strong> algorithm on the entire graph generated following the downsampling procedure:</p>
			<p class="source-code">nod2vec_unsup = Node2Vec(G_down, weight_key='weight')</p>
			<p class="source-code"> unsup_vals = nod2vec_unsup.fit(window=10)</p>
			<p>As previously defined for the <a id="_idIndexMarker878"/>supervised analysis, when <a id="_idIndexMarker879"/>building the node feature vectors, we can use different <strong class="source-inline">Egde2Vec</strong> algorithms to run the k-means algorithm as follows:</p>
			<p class="source-code">from sklearn.cluster import KMeans</p>
			<p class="source-code">classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]</p>
			<p class="source-code"> true_labels = [x for x in nx.get_edge_attributes(G_down, "label").values()]</p>
			<p class="source-code">for cl in classes:</p>
			<p class="source-code">    embedding_edge = cl(keyed_vectors=unsup_vals.wv)</p>
			<p class="source-code">    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]</p>
			<p class="source-code">    kmeans = KMeans(2, random_state=42).fit(embedding)</p>
			<p class="source-code">    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)</p>
			<p class="source-code">    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)</p>
			<p class="source-code">    co = metrics.completeness_score(true_labels, kmeans.labels_</p>
			<p class="source-code">    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)</p>
			<p class="source-code">    print(cl)</p>
			<p class="source-code">    print('NMI:', nmi)</p>
			<p class="source-code">    print('Homogeneity:', ho)</p>
			<p class="source-code">    print('Completeness:', co)</p>
			<p class="source-code">    print('V-Measure:', vmeasure)</p>
			<p>Different steps are <a id="_idIndexMarker880"/>performed in the previous code:</p>
			<ol>
				<li value="1">For each <strong class="source-inline">Edge2Vec</strong> algorithm, the previously computed <strong class="source-inline">Node2Vec</strong> algorithm on train and validation sets is used to generate the feature space.</li>
				<li>A <strong class="source-inline">KMeans</strong> clustering algorithm from the <strong class="source-inline">sklearn</strong> Python library is fitted on the feature set generated in the previous step.</li>
				<li>Different <a id="_idIndexMarker881"/>performance metrics, namely, adjusted <strong class="bold">mutual information</strong> (<strong class="bold">MNI</strong>), homogeneity, completeness, and v-measure scores.</li>
			</ol>
			<p>We can apply the code <a id="_idIndexMarker882"/>described previously to both bipartite and tripartite graphs to solve the fraud detection task using the unsupervised algorithm. In the following table, we report the performances for the bipartite graph:</p>
			<div>
				<div id="_idContainer540" class="IMG---Figure">
					<img src="image/B16069_08_041.jpg" alt="Table 8.4 – Unsupervised fraud edge classification performances for the bipartite graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.4 – Unsupervised fraud edge classification performances for the bipartite graph</p>
			<p>In the following table, we report the performances for the tripartite graph:</p>
			<div>
				<div id="_idContainer541" class="IMG---Figure">
					<img src="image/B16069_08_051.jpg" alt="Table 8.5 – Unsupervised fraud edge classification performances for the tripartite graph&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.5 – Unsupervised fraud edge classification performances for the tripartite graph</p>
			<p>In <em class="italic">Table 8.4</em> and <em class="italic">Table 8.5</em>, we reported the <a id="_idIndexMarker883"/>classification performances obtained using bipartite and tripartite graphs with the application of an unsupervised algorithm. As we can see from the results, the two methods show significant differences. It is also worth noticing that, in this case, the performances obtained with the Hadamard embedding algorithm clearly outperform all other approaches.</p>
			<p>As shown by <em class="italic">Table 8.4</em> and <em class="italic">Table 8.5</em>, also for this task, the performances obtained with the tripartite graph outstrip those obtained with the bipartite graph. In the unsupervised case, we can see how the introduction of the transaction nodes improves overall performance. We <a id="_idIndexMarker884"/>can assert, that, in the unsupervised setting, for this specific use case and using as a reference the results obtained in <em class="italic">Table 8.4</em> and <em class="italic">Table 8.5</em>, use of the tripartite graph could be a better choice since it enables the attainment of superior performances compared with the bipartite graph.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor140"/>Summary</h1>
			<p>In this chapter, we described how a classical fraud detection task can be described as a graph problem and how the techniques described in the previous chapter can be used to tackle the problem. Going into more detail, we introduced the dataset we used and described the procedure to transform the transactional data into two types of graph, namely, bipartite and tripartite undirected graphs. We then computed local (along with their distributions) and global metrics for both graphs, comparing the results.</p>
			<p>Moreover, a community detection algorithm was applied to the graphs in order to spot and plot specific regions of the transaction graph where the density of fraudulent transactions is higher compared to the other communities.</p>
			<p>Finally, we solved the fraud detection problem using supervised and unsupervised algorithms, comparing the performances of the bipartite and tripartite graphs. As the first step, since the problem was unbalanced with a higher presence of genuine transactions, we performed simple downsampling. We then applied different Edge2Vec algorithms in combination with a random forest, for the supervised task, and k-means for an unsupervised task, achieving good classification performances.</p>
			<p>This chapter concludes the series of examples that are used to show how graph machine learning algorithms can be applied to problems belonging to different domains, such as social network analysis, text analytics, and credit card transaction analysis. </p>
			<p>In the next chapter, we will describe some practical uses for graph databases and graph processing engines that are useful for scaling out the analysis to large graphs.</p>
		</div>
	</body></html>