- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantifying and Improving Data Properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Procuring data in machine learning systems is a long process. So far, we have
    focused on data collection from source systems and cleaning noise from data. Noise,
    however, is not the only problem that we can encounter in data. Missing values
    or random attributes are examples of data properties that can cause problems with
    machine learning systems. Even the length of the input data can be problematic
    if it is outside of the expected values.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive deeper into the properties of data and how to
    improve them. In contrast to the previous chapter, we will work on feature vectors
    rather than raw data. Feature vectors are already a transformation of the data
    and therefore, we can change properties such as noise or even change how the data
    is perceived.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll focus on the processing of text, which is an important part of many machine
    learning algorithms nowadays. We’ll start by understanding how to transform data
    into feature vectors using simple algorithms such as bag of words. We will also
    learn about techniques to handle problems in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying data properties for machine learning systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Germinating noise – feature engineering in clean datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling noisy data – machine learning algorithms and noise removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating attribute noise – a guide to dataset refinement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering – the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the process of transforming raw data into vectors of
    numbers that can be used in machine learning algorithms. This process is structured
    and requires us to first select which feature extraction mechanism we need to
    use – which depends on the type of the task – and then configure the chosen feature
    extraction mechanism. When the chosen algorithm is configured, we can use it to
    transform the raw input data into a matrix of features – we call this process
    feature extraction. Sometimes, the data needs to be processed before (or after)
    the feature extraction, for example, by merging fields or removing noise. This
    process is called data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: The number of feature extraction mechanisms is large, and we cannot cover all
    of them. Not that we need to either. What we need to understand, however, is how
    the choice of feature extraction mechanism influences the properties of the data.
    We’ll dive much deeper into the process of feature engineering in the next chapter,
    but in this chapter, we will introduce a basic algorithm for textual data. We
    need to introduce it to understand how it impacts the properties of data and how
    to cope with the most common problems that can arise in the context of feature
    extraction, including dealing with “dirty” data that requires cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this process, let us start with the first example of feature extraction
    from text using an algorithm called bag of words. Bag of words is a method that
    transforms a piece of text into a vector of numbers showing which words are part
    of that text. The words form the set of features – or columns – in the resulting
    dataframe. In the following code, we can see how feature extraction works. We
    have used the standard library from `sklearn` to create a bag-of-words feature
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code fragment, we take two lines of C code – `printf("Hello
    world!");` and `return 1` – and then translate this into a matrix of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The line in bold is a statement that creates an instance of the `CodeVectorizer`
    class, which transforms a given text into a vector of features. This includes
    the extraction of the features identified. This line has one parameter – `max_features
    = 3`. This parameter tells the algorithm that we only want three features. In
    this algorithm, the features are the words that are used in the input text. When
    we input the text to the algorithm, it extracts the tokens (words), and then for
    every line, it counts whether it contains these words. This is done in the statement
    `X = vectorizer.fit_transform([sentence1, sentence2])`. When the features are
    extracted, the resulting dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Hello** | **printf** | **return** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **printf(“Hello world!”);** | **1** | **1** | **0** |'
  prefs: []
  type: TYPE_TB
- en: '| **return 1** | **0** | **0** | **1** |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.1 – Extracted features create this dataset
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the table contains the index – the line that was input to
    the algorithm – and then `1` or `0` to show that the line contains the words in
    the vocabulary. Since we only asked for three features, the table has three columns
    – `Hello`, `printf`, and `return`. If we change the parameter of the `CountVectorizer()`,
    we’ll obtain the full list of tokens in these two lines, that is, `hello`, `printf`,
    `return`, and `world`.
  prefs: []
  type: TYPE_NORMAL
- en: For these two simple lines of C code, we get four features, which illustrates
    that this kind of feature extraction can quickly increase the size of the data.
    This leads us on to my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #30'
  prefs: []
  type: TYPE_NORMAL
- en: Balance the number of features with the number of data points. More features
    is not always better.
  prefs: []
  type: TYPE_NORMAL
- en: When creating feature vectors, it is important to extract meaningful features
    that can effectively distinguish between the data points. However, we should keep
    in mind that having more features will require more memory and can make the training
    slower. It is also prone to problems with missing data points.
  prefs: []
  type: TYPE_NORMAL
- en: Clean data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most problematic aspects of datasets, when it comes to machine learning,
    is the presence of empty data points or empty values of features for data points.
    Let’s illustrate that with the example of the features extracted in the previous
    section. In the following table, I introduced an empty data point – the `NaN`
    value in the middle column. This means that the value does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Hello** | **printf** | **return** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **printf(“Hello world!”);** | **1** | **NaN** | **0** |'
  prefs: []
  type: TYPE_TB
- en: '| **return 1** | **0** | **0** | **1** |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.2 – Extracted features with a NaN value in the table
  prefs: []
  type: TYPE_NORMAL
- en: If we use this data as input to a machine learning algorithm, we’ll get an error
    message that the data contains empty values and that the model cannot be trained.
    That is a very accurate description of this problem – if there is a missing value,
    then the model does not know how to handle it and therefore it cannot be trained.
  prefs: []
  type: TYPE_NORMAL
- en: There are two strategies to cope with empty values in datasets – removing the
    data points or imputing the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first strategy – removing the empty data points. The following
    script reads the data from our code reviews that we will use for further calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding fragment of code reads the file and displays its first 10 rows
    for us to inspect what the data contains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the data in memory, we can check how many of the rows contain
    null values for the column that contains the actual line of code, which is named
    `LOC`. Then, we can also remove the rows/data points that do not contain any data.
    The removal of the data points is handled by the following line – `dfReviews =
    dfReviews.dropna()`. This statement removes the lines that are empty and keeps
    the result in the dataframe itself (the `inplace=True` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After these commands, our dataset is prepared to create the feature vector.
    We can use `CountVectorizer` to extract the features from the dataset, as in the
    following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This fragment creates the bag-of-words model (`CountVectorizer`) with two parameters
    – the minimum frequency of the tokens and the maximum frequency of the tokens.
    This means that the algorithm calculates the statistics of how frequently each
    token appears in the dataset and then chooses the ones that fulfill the criteria.
    In our case, the algorithm chooses the tokens that appear at least twice (`min_df=2`)
    and at most 20 times (`max_df=20`).
  prefs: []
  type: TYPE_NORMAL
- en: The result of this code fragment is a large dataframe with 661 features extracted
    for each line of code in our dataset. We can check this by writing `len(df_bow_sklearn.columns)`
    after executing the preceding code fragment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to check how to work with data imputation, let us open a different
    dataset and check how many missing data points we have per column. Let’s read
    the dataset that is named `gerrit_reviews_nan.csv` and list the number of missing
    values in that dataset using the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of this code fragment, we get a list of columns with the number
    of missing values in them – the tail of this list is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There are many missing values and therefore, we need to adopt another strategy
    than removing them. If we remove all these values, we get exactly 0 data points
    – which means that there is a NaN value in one (or more) of the columns for every(!)
    data point. So, we need to adopt another strategy – imputation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to prepare the data for the imputer, which only works on the
    features. Therefore, we need to remove the index from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create the imputer. In this example, I use one of the modern ones,
    which is based on training a classifier on the existing data and then using it
    to fill the data in the original dataset. The fragment of code that trains the
    imputer is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of the code fragment is the actual training of the imputer. After
    this, we can start making the imputations to the dataset, as in the following
    code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After this fragment, we have a dataset that contains imputer values. Now, we
    need to remember that these values are only estimations, not the real ones. This
    particular dataset illustrates this very well. When we execute the `dfNoNaNs.head()`
    command, we can see that some of the imputed values are negative. Since our dataset
    is the result of `CountVectorizer`, the negative values are not likely. Therefore,
    we could use another kind of imputer – `KNNImputer`. That imputer uses the nearest
    neighbor algorithm to find the most similar data points and fills in the missing
    data based on the values of the similar data points. In this way, we get a set
    of imputed values that have the same properties (e.g., no negative values) as
    the rest of the dataset. However, the pattern of the imputed values is different.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, here is my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #30'
  prefs: []
  type: TYPE_NORMAL
- en: Use KNNImputer for data where the similarity between data points is expected
    to be local.
  prefs: []
  type: TYPE_NORMAL
- en: '`KNNImputer` works well when there is a clear local structure in the data,
    especially when neighboring data points are similar in terms of the feature with
    missing values. It can be sensitive to the choice of the number of nearest neighbors
    (`k`).'
  prefs: []
  type: TYPE_NORMAL
- en: '`IterativeImputer` tends to perform well when there are complex relationships
    and dependencies among features in the dataset. It may be more suitable for datasets
    with missing values that are not easily explained by local patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: However, check whether the imputation method provides logical results for the
    dataset at hand, in order to reduce the risk of bias.
  prefs: []
  type: TYPE_NORMAL
- en: Noise in data management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing data and contradictory annotations are only one type of problem with
    data. In many cases, large datasets, which are generated by feature extraction
    algorithms, can contain too much information. Features can be superfluous and
    not contribute to the end results of the algorithm. Many machine learning models
    can deal with noise in the features, called attribute noise, but too many features
    can be costly in terms of training time, storage, and even data collection itself.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we should also pay attention to the attribute noise, identify it,
    and then remove it.
  prefs: []
  type: TYPE_NORMAL
- en: Attribute noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few methods to reduce attribute noise in large datasets. One of
    these methods is an algorithm named the **Pairwise Attribute Noise Detection Algorithm**
    (**PANDA**). PANDA compares features pairwise and identifies which of them adds
    noise to the dataset. It is a very effective algorithm, but unfortunately very
    computationally heavy. If our dataset had a few hundred features (which is when
    we would really need to use this algorithm), we would need a lot of computational
    power to identify these features that bring in little to the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are machine learning algorithms that provide similar functionality
    with little computational overhead. One of these algorithms is the random forest
    algorithm, which allows you to retrieve the set of feature importance values.
    These values are a way of identifying which features are not used in any of the
    decision trees in this forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us then see how to use that algorithm to extract and visualize the feature’s
    importance. For this example, we will use the data extracted from the Gerrit tool
    in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this dataset, we have two columns that we extract features from. The first
    is the `LOC` column, which we use to extract the features using `CountVectorizer`
    – just like in the previous example. These features will be our `X` values later
    for the training algorithm. The second column of interest is the `message` column.
    The `message` column is used to provide the `decision` class. In order to transform
    the text of the message, we use a sentiment analysis model to identify whether
    the message is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s extract the BOW features using `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To transform the message into a sentiment, we can use an openly available model
    from the Hugging Face Hub. We need to install the relevant libraries using the
    following command: `! pip install -q transformers`. Once we have the libraries,
    we can start the feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code fragment uses the pre-trained model for the sentiment analysis
    and one from the standard pipeline – `sentiment-analysis`. The result is a dataframe
    that contains a positive or negative sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have both the `X` values – features extracted from the lines of code
    – and the predicted `Y` values – the sentiment from the review comment message.
    We can use these to create a dataframe that we can use as an input to the random
    forest algorithm, train the algorithm, and identify which features contributed
    the most to the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the random forest model is trained, we can extract the list of important
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This preceding code fragment selects the features with an importance of more
    than `0` and then lists them. We find that 363 out of 662 features are used in
    the predictions. This means that the remaining 270 are just the attribute noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also visualize these features using the `seaborn` library, as in the
    following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding code fragment results in the following diagram for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Feature importance chart with numerous features](img/B19548_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Feature importance chart with numerous features
  prefs: []
  type: TYPE_NORMAL
- en: Since there are so many features, the diagram gets very cluttered and challenging
    to read, so we can only visualize the top 20 features to understand which ones
    are really the most important.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Top 20 most important features in the dataset](img/B19548_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Top 20 most important features in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code examples show that we can reduce the number of features by
    41%, which is almost half of the features. The algorithm takes just a few seconds
    to find the most important features, which makes it the perfect candidate for
    reducing attribute noise in the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #31'
  prefs: []
  type: TYPE_NORMAL
- en: Use the Random Forest classifier to eliminate unnecessary features, as it offers
    very good performance.
  prefs: []
  type: TYPE_NORMAL
- en: Although we do not really get information on how much noise the removed features
    contain, receiving information that they have no value for the prediction algorithm
    is sufficient. Therefore, I recommend using this kind of feature reduction technique
    in the machine learning pipeline in order to reduce the computational and storage
    needs of our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the process of designing machine learning-based software, another important
    property is to understand the distribution of data, and, subsequently, ensure
    that the data used for training and testing is of a similar distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of the data used for training and validation is important as
    the machine learning models identify patterns and re-create them. This means that
    if the data in the training is not distributed in the same way as the data in
    the test set, our model misclassifies data points. The misclassifications (or
    mispredictions) are caused by the fact that the model learns patterns in the training
    data that are different from the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us understand how splitting algorithms work in theory, and how they work
    in practice. *Figure 5**.5* shows how the splitting works on a theoretical and
    conceptual level:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.5 – Splitting data into train and test sets\uFEFF](img/B19548_05_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Splitting data into train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: Icons represent review comments (and discussions). Every icon symbolizes its
    own discussion thread, and each type of icon reflects different teams. The idea
    behind splitting the dataset is that the two sets are very similar, but not identical.
    Therefore, the distribution of elements in the training and test datasets needs
    to be as similar as possible. However, it is not always possible, as *Figure 5**.5*
    shows – there are three out of four icons of one of the kinds in the training
    set and only one in the test set. When designing machine learning software, we
    need to take this aspect into consideration, even though it only relates to machine
    learning models. Our data processing pipeline should contain checks that provide
    the ability to understand whether the data is correctly distributed and, if not,
    we need to correct it. If we do not correct it, our system starts mispredicting.
    The change in the distribution of data over time, which is natural in machine
    learning-based systems, is called concept drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us use this in practice by calculating the distributions of the data in
    our Gerrit reviews dataset. First, we read the data, and then we use the `sklearn`
    `train_test_split` method to create a random split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this code fragment, we separate the predicted values (`y`) from the predictor
    values (`X`) features. Then we use the `train_test_split` method to split the
    dataset into two – two-thirds of the data in the training set and one-third of
    the data in the test set. This 2:1 ratio is the most common, but we can also encounter
    a 4:1 ratio, depending on the application and the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have two sets of data, we should explore whether the distributions
    are similar. Essentially, we should do that for each feature and the predicted
    variable (`y`), but in our dataset, we have 662 features, which means that we
    would have to do as many comparisons. So, let us, for the sake of the example,
    visualize only one of them – the one that was deemed the most important in our
    previous example – `dataresponse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will do the same for the test set too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'These two fragments result in two histograms with the distribution for that
    variable. They are presented in *Figure 5**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.6 – Distribution of dataresponse feature in train and test set\uFEFF\
    ](img/B19548_05_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Distribution of dataresponse feature in train and test set
  prefs: []
  type: TYPE_NORMAL
- en: 'The train set’s distribution is on the left-hand side and the test set’s distribution
    is on the right-hand side. At first glance, the distributions show that there
    is only a single value - 0 value. Therefore, we need to explore the data manually
    a bit more. We can check the distribution by calculating the number of entities
    per value – 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding calculations, we find that there are 624 values of 0 and
    5 values of 1 in the train set. We also find that there are 309 values of 0 and
    1 value of 1 in the test set. These are not exactly the same ratio, but given
    the scale – the 0s are significantly more than the 1s – this does not have any
    impact on the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features in our dataset should have the same distribution, but so do the
    `Y` values – the predicted variables. We can use the same technique to visualize
    the distribution between classes in the `Y` value. The following code fragment
    does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment generates two diagrams, which show what the difference between
    the two classes is. They are presented in *Figure 5**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Distribution of classes (0 and 1) in the training and test data](img/B19548_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Distribution of classes (0 and 1) in the training and test data
  prefs: []
  type: TYPE_NORMAL
- en: The predicted `Y` variable 0s are the negative sentiment values while 1s are
    the positive ones. Although the scales on the *y* axis are different in both diagrams,
    the distributions are very similar – it is roughly 2:1 in terms of the number
    of negative (0) sentiments and positive (1) sentiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classes are not balanced – the number of 0s is much larger than the number
    of 1s, but the distribution is the same. The fact that the classes are not balanced
    means that the model trained on this data is slightly biased towards the negative
    sentiment rather than the positive sentiment. However, this reflects the empirical
    observations that we make: in code reviews, the reviewers are more likely to comment
    on code that needs to be improved rather than on code that is nicely written.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #32'
  prefs: []
  type: TYPE_NORMAL
- en: As much as possible, retain the original distribution of the data as it reflects
    the empirical observations.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can balance the classes using undersampling, oversampling, or similar
    techniques, we should consider keeping the original distribution as much as we
    can. Changing the distribution makes the model “fairer” in terms of predictions/classifications,
    but it changes the nature of the observed phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: How ML models handle noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reducing noise from datasets is a time-consuming task, and it is also a task
    that cannot be easily automated. We need to understand whether we have noise in
    the data, what kind of noise is in the data, and how to remove it. Luckily, most
    machine learning algorithms are pretty good at handling noise.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the algorithm that we have used quite a lot so far – random forest
    – is quite robust to noise in datasets. Random forest is an ensemble model, which
    means that it is composed of several separate decision trees that internally “vote”
    for the best result. This voting process can therefore filter out noise and coalescence
    toward the pattern contained in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms have similar properties too – by utilizing a number
    of small neurons, these networks are robust to noise in large datasets. They can
    coerce the pattern in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #33'
  prefs: []
  type: TYPE_NORMAL
- en: In large-scale software systems, if possible, rely on machine learning models
    to handle noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: It may sound like I’m proposing an easy way out, but I’m not. Manual cleaning
    of the data is crucial, but it is also slow and costly. Therefore, during operations
    in large-scale systems, it is better to select a model that is robust to noise
    in the data and at the same time uses cleaner data. Since manual noise-handling
    processes require time and effort, relying on them would introduce unnecessary
    costs for our product operations.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it’s better to use algorithms that do that for us and therefore create
    products that are reliable and require minimal maintenance. Instead of costly
    noise-cleaning processes, it’s much more cost-efficient to re-train the algorithm
    to let it do the work for you.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we explore data visualization techniques. These techniques
    help us to understand dependencies in the data and whether it exposes characteristics
    that can be learnt by the machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Scott, S. and S. Matwin. Feature engineering for text classification. in*
    *ICML. 1999.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kulkarni, A., et al., Converting text to features. Natural Language Processing
    Recipes: Unlocking Text Data with Machine Learning and Deep Learning Using Python,
    2021:* *p. 63-106.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Van Hulse, J.D., T.M. Khoshgoftaar, and H. Huang, The pairwise attribute noise
    detection algorithm. Knowledge and Information Systems, 2007\. 11:* *p. 171-190.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Li, X., et al.,* *Exploiting BERT for end-to-end aspect-based sentiment analysis.
    arXiv preprint* *arXiv:1910.00883, 2019.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xu, Y. and R. Goodacre, On splitting training and validation set: a comparative
    study of cross-validation, bootstrap and systematic sampling for estimating the
    generalization performance of supervised learning. Journal of analysis and testing,
    2018\. 2(3):* *p. 249-262.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mosin, V., et al. Comparing Input Prioritization Techniques for Testing Deep
    Learning Algorithms. in 2022 48th Euromicro Conference on Software Engineering
    and Advanced Applications (SEAA).* *2022\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Liu, X.-Y., J. Wu, and Z.-H. Zhou, Exploratory undersampling for class-imbalance
    learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),
    2008\. 39(2):* *p. 539-550.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Atla, A., et al., Sensitivity of different machine learning algorithms to
    noise. Journal of Computing Sciences in Colleges, 2011\. 26(5):* *p. 96-103.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
