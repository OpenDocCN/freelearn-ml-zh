- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Quantifying and Improving Data Properties
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化并改进数据属性
- en: Procuring data in machine learning systems is a long process. So far, we have
    focused on data collection from source systems and cleaning noise from data. Noise,
    however, is not the only problem that we can encounter in data. Missing values
    or random attributes are examples of data properties that can cause problems with
    machine learning systems. Even the length of the input data can be problematic
    if it is outside of the expected values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中获取数据是一个漫长的过程。到目前为止，我们主要关注从源系统收集数据和从数据中清除噪声。然而，噪声并不是我们可能在数据中遇到的所有问题的唯一来源。缺失值或随机属性是可能导致机器学习系统出现问题的数据属性示例。即使输入数据的长度如果超出预期值，也可能成为问题。
- en: In this chapter, we will dive deeper into the properties of data and how to
    improve them. In contrast to the previous chapter, we will work on feature vectors
    rather than raw data. Feature vectors are already a transformation of the data
    and therefore, we can change properties such as noise or even change how the data
    is perceived.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨数据的属性以及如何改进它们。与上一章相比，我们将专注于特征向量而不是原始数据。特征向量已经是数据的一种转换，因此我们可以改变诸如噪声等属性，甚至改变数据被感知的方式。
- en: We’ll focus on the processing of text, which is an important part of many machine
    learning algorithms nowadays. We’ll start by understanding how to transform data
    into feature vectors using simple algorithms such as bag of words. We will also
    learn about techniques to handle problems in data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于文本的处理，这是许多机器学习算法中一个重要的部分。我们将从了解如何使用简单的算法，如词袋模型，将数据转换为特征向量开始。我们还将学习处理数据问题的技术。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Quantifying data properties for machine learning systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习系统量化数据属性
- en: Germinating noise – feature engineering in clean datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 培育噪声——在干净数据集中的特征工程
- en: Handling noisy data – machine learning algorithms and noise removal
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理噪声数据——机器学习算法和噪声消除
- en: Eliminating attribute noise – a guide to dataset refinement
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除属性噪声——数据集精炼指南
- en: Feature engineering – the basics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程——基础
- en: Feature engineering is the process of transforming raw data into vectors of
    numbers that can be used in machine learning algorithms. This process is structured
    and requires us to first select which feature extraction mechanism we need to
    use – which depends on the type of the task – and then configure the chosen feature
    extraction mechanism. When the chosen algorithm is configured, we can use it to
    transform the raw input data into a matrix of features – we call this process
    feature extraction. Sometimes, the data needs to be processed before (or after)
    the feature extraction, for example, by merging fields or removing noise. This
    process is called data wrangling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是将原始数据转换为可用于机器学习算法的数字向量的过程。这个过程是有结构的，需要我们首先选择需要使用的特征提取机制——这取决于任务的类型——然后配置所选的特征提取机制。当所选算法配置完成后，我们可以使用它将原始输入数据转换为特征矩阵——我们称这个过程为特征提取。有时，在特征提取之前（或之后）需要处理数据，例如通过合并字段或去除噪声。这个过程称为数据整理。
- en: The number of feature extraction mechanisms is large, and we cannot cover all
    of them. Not that we need to either. What we need to understand, however, is how
    the choice of feature extraction mechanism influences the properties of the data.
    We’ll dive much deeper into the process of feature engineering in the next chapter,
    but in this chapter, we will introduce a basic algorithm for textual data. We
    need to introduce it to understand how it impacts the properties of data and how
    to cope with the most common problems that can arise in the context of feature
    extraction, including dealing with “dirty” data that requires cleaning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取机制的种类繁多，我们无法涵盖所有内容。我们也不需要这样做。然而，我们需要理解的是，特征提取机制的选择如何影响数据的属性。我们将在下一章深入探讨特征工程的过程，但在这章中，我们将介绍一个用于文本数据的基本算法。我们需要介绍它，以便了解它如何影响数据的属性以及如何应对特征提取过程中可能出现的最常见问题，包括处理需要清理的“脏”数据。
- en: To understand this process, let us start with the first example of feature extraction
    from text using an algorithm called bag of words. Bag of words is a method that
    transforms a piece of text into a vector of numbers showing which words are part
    of that text. The words form the set of features – or columns – in the resulting
    dataframe. In the following code, we can see how feature extraction works. We
    have used the standard library from `sklearn` to create a bag-of-words feature
    vector.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个过程，让我们从使用称为“词袋”算法的特征提取的第一个文本示例开始。词袋是一种将文本转换为表示该文本中哪些单词的数字向量的方法。单词形成结果数据框中的特征集——或列。在下面的代码中，我们可以看到特征提取是如何工作的。我们使用了`sklearn`标准库来创建词袋特征向量。
- en: 'In the following code fragment, we take two lines of C code – `printf("Hello
    world!");` and `return 1` – and then translate this into a matrix of features:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们取了两行C代码——`printf("Hello world!");`和`return 1`——然后将这些代码转换成特征矩阵：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The line in bold is a statement that creates an instance of the `CodeVectorizer`
    class, which transforms a given text into a vector of features. This includes
    the extraction of the features identified. This line has one parameter – `max_features
    = 3`. This parameter tells the algorithm that we only want three features. In
    this algorithm, the features are the words that are used in the input text. When
    we input the text to the algorithm, it extracts the tokens (words), and then for
    every line, it counts whether it contains these words. This is done in the statement
    `X = vectorizer.fit_transform([sentence1, sentence2])`. When the features are
    extracted, the resulting dataset looks as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 粗体的行是创建`CodeVectorizer`类实例的语句，该类将给定的文本转换为特征向量。这包括提取已识别的特征。这一行有一个参数——`max_features
    = 3`。此参数告诉算法我们只想获取三个特征。在这个算法中，特征是输入文本中使用的单词。当我们向算法输入文本时，它提取标记（单词），然后对于每一行，它计算是否包含这些单词。这是在语句`X
    = vectorizer.fit_transform([sentence1, sentence2])`中完成的。当特征被提取后，结果数据集看起来如下：
- en: '|  | **Hello** | **printf** | **return** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | **Hello** | **printf** | **return** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **printf(“Hello world!”);** | **1** | **1** | **0** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **printf(“Hello world!”);** | **1** | **1** | **0** |'
- en: '| **return 1** | **0** | **0** | **1** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **return 1** | **0** | **0** | **1** |'
- en: Figure 5.1 – Extracted features create this dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 提取的特征创建此数据集
- en: The first line in the table contains the index – the line that was input to
    the algorithm – and then `1` or `0` to show that the line contains the words in
    the vocabulary. Since we only asked for three features, the table has three columns
    – `Hello`, `printf`, and `return`. If we change the parameter of the `CountVectorizer()`,
    we’ll obtain the full list of tokens in these two lines, that is, `hello`, `printf`,
    `return`, and `world`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的第一行包含索引——输入算法的行——然后是`1`或`0`以表示该行包含词汇表中的单词。由于我们只要求三个特征，因此表格有三列——`Hello`、`printf`和`return`。如果我们更改`CountVectorizer()`的参数，我们将获得这两行中的完整标记列表，即`hello`、`printf`、`return`和`world`。
- en: For these two simple lines of C code, we get four features, which illustrates
    that this kind of feature extraction can quickly increase the size of the data.
    This leads us on to my next best practice.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两行简单的C代码，我们得到了四个特征，这说明了这种特征提取可以快速增加数据的大小。这使我们转向我的下一个最佳实践。
- en: 'Best practice #30'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#30
- en: Balance the number of features with the number of data points. More features
    is not always better.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡特征数量与数据点数量。特征数量并不总是越多越好。
- en: When creating feature vectors, it is important to extract meaningful features
    that can effectively distinguish between the data points. However, we should keep
    in mind that having more features will require more memory and can make the training
    slower. It is also prone to problems with missing data points.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建特征向量时，重要的是提取有意义的特征，这些特征可以有效地区分数据点。然而，我们应该记住，拥有更多特征将需要更多内存，并且可能会使训练过程变慢。它也容易受到缺失数据点的问题。
- en: Clean data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洁数据
- en: One of the most problematic aspects of datasets, when it comes to machine learning,
    is the presence of empty data points or empty values of features for data points.
    Let’s illustrate that with the example of the features extracted in the previous
    section. In the following table, I introduced an empty data point – the `NaN`
    value in the middle column. This means that the value does not exist.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到机器学习时，数据集的一个最棘手的问题就是存在空数据点或数据点的特征值为空。让我们通过前一个章节中提取的特征的例子来说明这一点。在下面的表格中，我引入了一个空数据点——中间列的`NaN`值。这意味着该值不存在。
- en: '|  | **Hello** | **printf** | **return** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | **Hello** | **printf** | **return** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **printf(“Hello world!”);** | **1** | **NaN** | **0** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **printf(“Hello world!”);** | **1** | **NaN** | **0** |'
- en: '| **return 1** | **0** | **0** | **1** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **return 1** | **0** | **0** | **1** |'
- en: Figure 5.2 – Extracted features with a NaN value in the table
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 表格中包含NaN值的提取特征
- en: If we use this data as input to a machine learning algorithm, we’ll get an error
    message that the data contains empty values and that the model cannot be trained.
    That is a very accurate description of this problem – if there is a missing value,
    then the model does not know how to handle it and therefore it cannot be trained.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些数据作为机器学习算法的输入，我们会得到一个错误消息，指出数据包含空值，并且模型无法训练。这是对这个问题的非常准确的描述——如果存在缺失值，那么模型不知道如何处理它，因此无法进行训练。
- en: There are two strategies to cope with empty values in datasets – removing the
    data points or imputing the values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 应对数据集中的空值有两种策略——移除数据点或插补值。
- en: 'Let’s start with the first strategy – removing the empty data points. The following
    script reads the data from our code reviews that we will use for further calculations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个策略开始——移除空数据点。以下脚本读取我们用于进一步计算的数据，即我们的代码审查数据：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding fragment of code reads the file and displays its first 10 rows
    for us to inspect what the data contains.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段读取文件并显示其前10行，以便我们检查数据内容。
- en: 'Once we have the data in memory, we can check how many of the rows contain
    null values for the column that contains the actual line of code, which is named
    `LOC`. Then, we can also remove the rows/data points that do not contain any data.
    The removal of the data points is handled by the following line – `dfReviews =
    dfReviews.dropna()`. This statement removes the lines that are empty and keeps
    the result in the dataframe itself (the `inplace=True` parameter):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据存储在内存中，我们可以检查包含实际代码行（命名为`LOC`）的列中包含null值的行数有多少。然后，我们还可以删除不包含任何数据的行/数据点。数据点的删除由以下行处理——`dfReviews
    = dfReviews.dropna()`。此语句删除了空行，并将结果保留在数据框本身中（`inplace=True`参数）：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After these commands, our dataset is prepared to create the feature vector.
    We can use `CountVectorizer` to extract the features from the dataset, as in the
    following code fragment:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行这些命令后，我们的数据集已准备好创建特征向量。我们可以使用`CountVectorizer`从数据集中提取特征，如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This fragment creates the bag-of-words model (`CountVectorizer`) with two parameters
    – the minimum frequency of the tokens and the maximum frequency of the tokens.
    This means that the algorithm calculates the statistics of how frequently each
    token appears in the dataset and then chooses the ones that fulfill the criteria.
    In our case, the algorithm chooses the tokens that appear at least twice (`min_df=2`)
    and at most 20 times (`max_df=20`).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段创建了一个包含两个参数的词袋模型（`CountVectorizer`）——标记的最小频率和最大频率。这意味着算法计算每个标记在数据集中出现的频率统计，然后选择满足条件的标记。在我们的情况下，算法选择至少出现两次（`min_df=2`）且最多20次（`max_df=20`）的标记。
- en: The result of this code fragment is a large dataframe with 661 features extracted
    for each line of code in our dataset. We can check this by writing `len(df_bow_sklearn.columns)`
    after executing the preceding code fragment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段的结果是一个包含661个特征的大型数据框，每个特征对应于我们数据集中每行代码。我们可以通过在执行上述代码片段后编写`len(df_bow_sklearn.columns)`来检查这一点。
- en: 'In order to check how to work with data imputation, let us open a different
    dataset and check how many missing data points we have per column. Let’s read
    the dataset that is named `gerrit_reviews_nan.csv` and list the number of missing
    values in that dataset using the following code fragment:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查如何处理数据插补，让我们打开一个不同的数据集并检查每列有多少缺失数据点。让我们读取名为`gerrit_reviews_nan.csv`的数据集，并使用以下代码片段列出该数据集中的缺失值：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As a result of this code fragment, we get a list of columns with the number
    of missing values in them – the tail of this list is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个代码片段，我们得到了一个包含列中缺失值数量的列表——列表的尾部如下：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are many missing values and therefore, we need to adopt another strategy
    than removing them. If we remove all these values, we get exactly 0 data points
    – which means that there is a NaN value in one (or more) of the columns for every(!)
    data point. So, we need to adopt another strategy – imputation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多缺失值，因此，我们需要采用不同于删除它们的策略。如果我们删除所有这些值，我们将得到恰好0个数据点——这意味着每个（或更多）数据列中都有一个NaN值。所以，我们需要采用另一种策略——填充。
- en: 'First, we need to prepare the data for the imputer, which only works on the
    features. Therefore, we need to remove the index from the dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为填充器准备数据，它只对特征有效。因此，我们需要从数据集中删除索引：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we can create the imputer. In this example, I use one of the modern ones,
    which is based on training a classifier on the existing data and then using it
    to fill the data in the original dataset. The fragment of code that trains the
    imputer is presented here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建填充器。在这个例子中，我使用了一种基于在现有数据上训练分类器，然后使用它来填充原始数据集中数据的现代方法。训练填充器的代码片段如下所示：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last line of the code fragment is the actual training of the imputer. After
    this, we can start making the imputations to the dataset, as in the following
    code fragment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段的最后一行是填充器的实际训练。在这之后，我们可以开始对数据集进行填充，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After this fragment, we have a dataset that contains imputer values. Now, we
    need to remember that these values are only estimations, not the real ones. This
    particular dataset illustrates this very well. When we execute the `dfNoNaNs.head()`
    command, we can see that some of the imputed values are negative. Since our dataset
    is the result of `CountVectorizer`, the negative values are not likely. Therefore,
    we could use another kind of imputer – `KNNImputer`. That imputer uses the nearest
    neighbor algorithm to find the most similar data points and fills in the missing
    data based on the values of the similar data points. In this way, we get a set
    of imputed values that have the same properties (e.g., no negative values) as
    the rest of the dataset. However, the pattern of the imputed values is different.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段之后，我们得到了一个包含填充值的数据集。现在，我们需要记住这些值只是估计值，而不是真实值。这个特定的数据集很好地说明了这一点。当我们执行`dfNoNaNs.head()`命令时，我们可以看到一些填充值是负数。由于我们的数据集是`CountVectorizer`的结果，负值不太可能。因此，我们可以使用另一种类型的填充器——`KNNImputer`。这个填充器使用最近邻算法找到最相似的数据点，并根据相似数据点的值填充缺失数据。这样，我们得到一组具有相同属性（例如，没有负值）的填充值，与数据集的其余部分相同。然而，填充值的模式是不同的。
- en: Therefore, here is my next best practice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是我的下一个最佳实践。
- en: 'Best practice #30'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#30
- en: Use KNNImputer for data where the similarity between data points is expected
    to be local.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据点之间相似性预期是局部的情况下使用KNNImputer。
- en: '`KNNImputer` works well when there is a clear local structure in the data,
    especially when neighboring data points are similar in terms of the feature with
    missing values. It can be sensitive to the choice of the number of nearest neighbors
    (`k`).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据中有明显的局部结构时，`KNNImputer`表现良好，尤其是在相邻数据点在缺失值的特征上相似时。它可能对最近邻数（`k`）的选择敏感。
- en: '`IterativeImputer` tends to perform well when there are complex relationships
    and dependencies among features in the dataset. It may be more suitable for datasets
    with missing values that are not easily explained by local patterns.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`IterativeImputer`在数据集中特征之间存在复杂关系和依赖时往往表现良好。它可能更适合那些缺失值不容易由局部模式解释的数据集。'
- en: However, check whether the imputation method provides logical results for the
    dataset at hand, in order to reduce the risk of bias.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，检查填充方法是否为当前数据集提供逻辑结果，以降低偏差风险。
- en: Noise in data management
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管理中的噪声
- en: Missing data and contradictory annotations are only one type of problem with
    data. In many cases, large datasets, which are generated by feature extraction
    algorithms, can contain too much information. Features can be superfluous and
    not contribute to the end results of the algorithm. Many machine learning models
    can deal with noise in the features, called attribute noise, but too many features
    can be costly in terms of training time, storage, and even data collection itself.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据和矛盾的注释只是数据问题的一种类型。在许多情况下，由特征提取算法生成的大型数据集可能包含过多的信息。特征可能是多余的，不会对算法的最终结果做出贡献。许多机器学习模型可以处理特征中的噪声，称为属性噪声，但特征过多可能会在训练时间、存储甚至数据收集本身方面造成成本。
- en: Therefore, we should also pay attention to the attribute noise, identify it,
    and then remove it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们也应该注意属性噪声，识别它，然后将其移除。
- en: Attribute noise
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 属性噪声
- en: There are a few methods to reduce attribute noise in large datasets. One of
    these methods is an algorithm named the **Pairwise Attribute Noise Detection Algorithm**
    (**PANDA**). PANDA compares features pairwise and identifies which of them adds
    noise to the dataset. It is a very effective algorithm, but unfortunately very
    computationally heavy. If our dataset had a few hundred features (which is when
    we would really need to use this algorithm), we would need a lot of computational
    power to identify these features that bring in little to the analysis.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集中减少属性噪声有几种方法。其中一种方法是一个名为**成对属性噪声检测算法**（**PANDA**）的算法。PANDA成对比较特征并识别出哪些特征给数据集带来了噪声。这是一个非常有效的算法，但不幸的是计算量非常大。如果我们的数据集有几百个特征（这是我们真正需要使用这个算法的时候），我们需要大量的计算能力来识别这些对分析贡献甚微的特征。
- en: Fortunately, there are machine learning algorithms that provide similar functionality
    with little computational overhead. One of these algorithms is the random forest
    algorithm, which allows you to retrieve the set of feature importance values.
    These values are a way of identifying which features are not used in any of the
    decision trees in this forest.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些机器学习算法提供了类似的功能，同时计算开销很小。其中之一是随机森林算法，它允许你检索特征重要性值的集合。这些值是一种识别哪些特征在这个森林中的任何决策树中都没有被使用的方法。
- en: 'Let us then see how to use that algorithm to extract and visualize the feature’s
    importance. For this example, we will use the data extracted from the Gerrit tool
    in previous chapters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用该算法提取和可视化特征的重要性。在这个例子中，我们将使用前几章从Gerrit工具中提取的数据：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this dataset, we have two columns that we extract features from. The first
    is the `LOC` column, which we use to extract the features using `CountVectorizer`
    – just like in the previous example. These features will be our `X` values later
    for the training algorithm. The second column of interest is the `message` column.
    The `message` column is used to provide the `decision` class. In order to transform
    the text of the message, we use a sentiment analysis model to identify whether
    the message is positive or negative.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们有两个列是从中提取特征的。第一列是`LOC`列，我们使用`CountVectorizer`来提取特征——就像在之前的例子中一样。这些特征将成为训练算法的`X`值。第二列是感兴趣的列是`message`列。`message`列用于提供`decision`类。为了转换消息文本，我们使用情感分析模型来识别消息是正面还是负面。
- en: 'First, let’s extract the BOW features using `CountVectorizer`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`CountVectorizer`提取BOW特征：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To transform the message into a sentiment, we can use an openly available model
    from the Hugging Face Hub. We need to install the relevant libraries using the
    following command: `! pip install -q transformers`. Once we have the libraries,
    we can start the feature extraction:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要将信息转换为情感，我们可以使用Hugging Face Hub上公开可用的模型。我们需要使用以下命令安装相关库：`! pip install -q transformers`。一旦我们有了这些库，我们就可以开始特征提取：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding code fragment uses the pre-trained model for the sentiment analysis
    and one from the standard pipeline – `sentiment-analysis`. The result is a dataframe
    that contains a positive or negative sentiment.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段使用了预训练的情感分析模型和一个来自标准管道的模型——`sentiment-analysis`。结果是包含正面或负面情感的dataframe。
- en: 'Now, we have both the `X` values – features extracted from the lines of code
    – and the predicted `Y` values – the sentiment from the review comment message.
    We can use these to create a dataframe that we can use as an input to the random
    forest algorithm, train the algorithm, and identify which features contributed
    the most to the result:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了`X`值——从代码行中提取的特征——以及预测的`Y`值——来自评论消息的情感。我们可以使用这些信息创建一个数据框，将其用作随机森林算法的输入，训练算法，并确定哪些特征对结果贡献最大：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When the random forest model is trained, we can extract the list of important
    features:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当随机森林模型训练完成后，我们可以提取重要特征列表：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This preceding code fragment selects the features with an importance of more
    than `0` and then lists them. We find that 363 out of 662 features are used in
    the predictions. This means that the remaining 270 are just the attribute noise.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段选择了重要性大于`0`的特征，并将它们列出。我们发现662个特征中有363个被用于预测。这意味着剩下的270个特征只是属性噪声。
- en: 'We can also visualize these features using the `seaborn` library, as in the
    following code fragment:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`seaborn`库可视化这些特征，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This preceding code fragment results in the following diagram for the dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段为数据集生成了以下图表：
- en: '![Figure 5.3 – Feature importance chart with numerous features](img/B19548_05_1.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 具有许多特征的特性重要性图表](img/B19548_05_1.jpg)'
- en: Figure 5.3 – Feature importance chart with numerous features
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 具有许多特征的特性重要性图表
- en: Since there are so many features, the diagram gets very cluttered and challenging
    to read, so we can only visualize the top 20 features to understand which ones
    are really the most important.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征太多，图表变得非常杂乱且难以阅读，所以我们只能可视化前20个特征，以了解哪些是最重要的。
- en: '![Figure 5.4 – Top 20 most important features in the dataset](img/B19548_05_2.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 数据集中最重要的前20个特征](img/B19548_05_2.jpg)'
- en: Figure 5.4 – Top 20 most important features in the dataset
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 数据集中最重要的前20个特征
- en: The preceding code examples show that we can reduce the number of features by
    41%, which is almost half of the features. The algorithm takes just a few seconds
    to find the most important features, which makes it the perfect candidate for
    reducing attribute noise in the datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码示例表明，我们可以将特征数量减少41%，这几乎是特征数量的一半。算法只需几秒钟就能找到最重要的特征，这使得它成为减少数据集中属性噪声的完美候选。
- en: 'Best practice #31'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#31
- en: Use the Random Forest classifier to eliminate unnecessary features, as it offers
    very good performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林分类器来消除不必要的特征，因为它提供了非常好的性能。
- en: Although we do not really get information on how much noise the removed features
    contain, receiving information that they have no value for the prediction algorithm
    is sufficient. Therefore, I recommend using this kind of feature reduction technique
    in the machine learning pipeline in order to reduce the computational and storage
    needs of our pipeline.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们并没有真正得到关于被移除的特征包含多少噪声的信息，但得到它们对预测算法没有价值的信息就足够了。因此，我建议在机器学习管道中使用这种特征减少技术，以减少我们管道的计算和存储需求。
- en: Splitting data
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分割
- en: For the process of designing machine learning-based software, another important
    property is to understand the distribution of data, and, subsequently, ensure
    that the data used for training and testing is of a similar distribution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计基于机器学习的软件的过程中，另一个重要的属性是理解数据的分布，并且随后确保用于训练和测试的数据具有相似的分布。
- en: The distribution of the data used for training and validation is important as
    the machine learning models identify patterns and re-create them. This means that
    if the data in the training is not distributed in the same way as the data in
    the test set, our model misclassifies data points. The misclassifications (or
    mispredictions) are caused by the fact that the model learns patterns in the training
    data that are different from the test data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练和验证的数据分布很重要，因为机器学习模型识别模式并重新创建它们。这意味着如果训练数据中的数据分布与测试集中的数据分布不同，我们的模型就会错误地分类数据点。错误分类（或错误预测）是由于模型在训练数据中学习到的模式与测试数据不同所导致的。
- en: 'Let us understand how splitting algorithms work in theory, and how they work
    in practice. *Figure 5**.5* shows how the splitting works on a theoretical and
    conceptual level:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解分割算法在理论上的工作原理以及在实际中的应用。**图5.5**展示了在理论和概念层面上分割是如何工作的：
- en: "![Figure 5.5 – Splitting data into train and test sets\uFEFF](img/B19548_05_3.jpg)"
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 将数据分割为训练集和测试集](img/B19548_05_3.jpg)'
- en: Figure 5.5 – Splitting data into train and test sets
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 将数据分割为训练集和测试集
- en: Icons represent review comments (and discussions). Every icon symbolizes its
    own discussion thread, and each type of icon reflects different teams. The idea
    behind splitting the dataset is that the two sets are very similar, but not identical.
    Therefore, the distribution of elements in the training and test datasets needs
    to be as similar as possible. However, it is not always possible, as *Figure 5**.5*
    shows – there are three out of four icons of one of the kinds in the training
    set and only one in the test set. When designing machine learning software, we
    need to take this aspect into consideration, even though it only relates to machine
    learning models. Our data processing pipeline should contain checks that provide
    the ability to understand whether the data is correctly distributed and, if not,
    we need to correct it. If we do not correct it, our system starts mispredicting.
    The change in the distribution of data over time, which is natural in machine
    learning-based systems, is called concept drift.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图标代表审查评论（和讨论）。每个图标代表一个自己的讨论线程，每种类型的图标反映不同的团队。分割数据集背后的想法是，这两个集合非常相似，但并不完全相同。因此，训练集和测试集中元素的分发需要尽可能相似。然而，这并不总是可能的，如图*5**.5*所示
    – 训练集中有一种类型的四个图标中的三个，而测试集中只有一个。在设计机器学习软件时，我们需要考虑这个方面，尽管它只与机器学习模型相关。我们的数据处理管道应该包含检查，提供理解数据是否正确分布的能力，如果不正确，我们需要纠正它。如果我们不纠正它，我们的系统开始做出错误的预测。在基于机器学习的系统中，数据分布随时间的变化，这是自然的，被称为概念漂移。
- en: 'Let us use this in practice by calculating the distributions of the data in
    our Gerrit reviews dataset. First, we read the data, and then we use the `sklearn`
    `train_test_split` method to create a random split:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过计算我们的Gerrit审查数据集中数据的分布来实际应用这个方法。首先，我们读取数据，然后使用`sklearn`的`train_test_split`方法创建一个随机分割：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this code fragment, we separate the predicted values (`y`) from the predictor
    values (`X`) features. Then we use the `train_test_split` method to split the
    dataset into two – two-thirds of the data in the training set and one-third of
    the data in the test set. This 2:1 ratio is the most common, but we can also encounter
    a 4:1 ratio, depending on the application and the dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们将预测值（`y`）与预测值（`X`）特征分开。然后我们使用`train_test_split`方法将数据集分割成两个部分 – 训练集中的三分之二数据和测试集中的一分之一数据。这个2:1的比例是最常见的，但根据应用和数据集的不同，我们也可能遇到4:1的比例。
- en: 'Now that we have two sets of data, we should explore whether the distributions
    are similar. Essentially, we should do that for each feature and the predicted
    variable (`y`), but in our dataset, we have 662 features, which means that we
    would have to do as many comparisons. So, let us, for the sake of the example,
    visualize only one of them – the one that was deemed the most important in our
    previous example – `dataresponse`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两组数据，我们应该探索它们的分布是否相似。本质上，我们应该对每个特征和预测变量（`y`）都这样做，但在我们的数据集中，我们有662个特征，这意味着我们可能需要进行如此多的比较。所以，为了举例，我们只可视化其中一个
    – 在我们之前的例子中被认为是最重要的一个 – `dataresponse`：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will do the same for the test set too:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也将对测试集进行同样的操作：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These two fragments result in two histograms with the distribution for that
    variable. They are presented in *Figure 5**.6*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个片段产生了两个直方图，显示了该变量的分布。它们在*图5**.6*中展示：
- en: "![Figure 5.6 – Distribution of dataresponse feature in train and test set\uFEFF\
    ](img/B19548_05_6.jpg)"
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 训练集和测试集中数据响应特征的分布](img/B19548_05_6.jpg)'
- en: Figure 5.6 – Distribution of dataresponse feature in train and test set
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 训练集和测试集中数据响应特征的分布
- en: 'The train set’s distribution is on the left-hand side and the test set’s distribution
    is on the right-hand side. At first glance, the distributions show that there
    is only a single value - 0 value. Therefore, we need to explore the data manually
    a bit more. We can check the distribution by calculating the number of entities
    per value – 0 and 1:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列车集的分布位于左侧，测试集的分布位于右侧。乍一看，分布显示只有一个值 - 0值。因此，我们需要更深入地手动探索数据。我们可以通过计算每个值（0和1）的实体数量来检查分布：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: From the preceding calculations, we find that there are 624 values of 0 and
    5 values of 1 in the train set. We also find that there are 309 values of 0 and
    1 value of 1 in the test set. These are not exactly the same ratio, but given
    the scale – the 0s are significantly more than the 1s – this does not have any
    impact on the machine learning model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的计算中，我们发现训练集中有624个0值和5个1值，测试集中有309个0值和1个1值。这些比例并不完全相同，但考虑到规模——0值显著多于1值——这不会对机器学习模型产生任何影响。
- en: 'The features in our dataset should have the same distribution, but so do the
    `Y` values – the predicted variables. We can use the same technique to visualize
    the distribution between classes in the `Y` value. The following code fragment
    does just that:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的特征应该具有相同的分布，`Y`值——预测变量也是如此。我们可以使用相同的技巧来可视化`Y`值之间的类别分布。下面的代码片段正是这样做的：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code fragment generates two diagrams, which show what the difference between
    the two classes is. They are presented in *Figure 5**.7*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段生成两个图表，显示了两个类别的差异。它们在*图5.7*中展示：
- en: '![Figure 5.7 – Distribution of classes (0 and 1) in the training and test data](img/B19548_05_7.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 训练和测试数据中类（0和1）的分布](img/B19548_05_7.jpg)'
- en: Figure 5.7 – Distribution of classes (0 and 1) in the training and test data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 训练和测试数据中类（0和1）的分布
- en: The predicted `Y` variable 0s are the negative sentiment values while 1s are
    the positive ones. Although the scales on the *y* axis are different in both diagrams,
    the distributions are very similar – it is roughly 2:1 in terms of the number
    of negative (0) sentiments and positive (1) sentiments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的`Y`变量0值是负面情绪值，而1值是正面情绪值。尽管两个图表中y轴的刻度不同，但分布非常相似——在负面（0）情绪和正面（1）情绪的数量上大约是2:1。
- en: 'The classes are not balanced – the number of 0s is much larger than the number
    of 1s, but the distribution is the same. The fact that the classes are not balanced
    means that the model trained on this data is slightly biased towards the negative
    sentiment rather than the positive sentiment. However, this reflects the empirical
    observations that we make: in code reviews, the reviewers are more likely to comment
    on code that needs to be improved rather than on code that is nicely written.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡——0的数量远大于1的数量，但分布相同。类别不平衡的事实意味着在此数据上训练的模型略微偏向负面情绪而不是正面情绪。然而，这反映了我们的经验观察：在代码审查中，审查员更可能评论需要改进的代码，而不是写得很好的代码。
- en: 'Best practice #32'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#32
- en: As much as possible, retain the original distribution of the data as it reflects
    the empirical observations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能保留数据的原始分布，因为它反映了经验观察。
- en: Although we can balance the classes using undersampling, oversampling, or similar
    techniques, we should consider keeping the original distribution as much as we
    can. Changing the distribution makes the model “fairer” in terms of predictions/classifications,
    but it changes the nature of the observed phenomena.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以使用欠采样、过采样或类似的技术来平衡类别，但我们应尽可能保持原始分布。改变分布使模型在预测/分类方面“更公平”，但它改变了观察到的现象的本质。
- en: How ML models handle noise
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型如何处理噪声
- en: Reducing noise from datasets is a time-consuming task, and it is also a task
    that cannot be easily automated. We need to understand whether we have noise in
    the data, what kind of noise is in the data, and how to remove it. Luckily, most
    machine learning algorithms are pretty good at handling noise.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中减少噪声是一个耗时的工作，而且也是一个难以自动化的任务。我们需要了解数据中是否存在噪声，数据中存在什么类型的噪声，以及如何去除它。幸运的是，大多数机器学习算法在处理噪声方面相当出色。
- en: For example, the algorithm that we have used quite a lot so far – random forest
    – is quite robust to noise in datasets. Random forest is an ensemble model, which
    means that it is composed of several separate decision trees that internally “vote”
    for the best result. This voting process can therefore filter out noise and coalescence
    toward the pattern contained in the data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们迄今为止使用得相当多的算法——随机森林——对数据集中的噪声相当鲁棒。随机森林是一个集成模型，这意味着它由几个独立的决策树组成，这些决策树内部“投票”选择最佳结果。因此，这个过程可以过滤掉噪声，并趋向于数据中包含的模式。
- en: Deep learning algorithms have similar properties too – by utilizing a number
    of small neurons, these networks are robust to noise in large datasets. They can
    coerce the pattern in the data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法也有类似的特性——通过利用大量的小神经元，这些网络对大数据集中的噪声具有鲁棒性。它们可以强制数据中的模式。
- en: 'Best practice #33'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #33'
- en: In large-scale software systems, if possible, rely on machine learning models
    to handle noise in the data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型软件系统中，如果可能的话，依赖机器学习模型来处理数据中的噪声。
- en: It may sound like I’m proposing an easy way out, but I’m not. Manual cleaning
    of the data is crucial, but it is also slow and costly. Therefore, during operations
    in large-scale systems, it is better to select a model that is robust to noise
    in the data and at the same time uses cleaner data. Since manual noise-handling
    processes require time and effort, relying on them would introduce unnecessary
    costs for our product operations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能像是在提出一条简单的出路，但事实并非如此。数据的手动清理至关重要，但它也很慢且成本高昂。因此，在大型系统操作期间，最好选择一个对数据噪声鲁棒且同时使用更干净数据的模型。由于手动噪声处理过程需要时间和精力，依赖它们将为我们的产品运营带来不必要的成本。
- en: Therefore, it’s better to use algorithms that do that for us and therefore create
    products that are reliable and require minimal maintenance. Instead of costly
    noise-cleaning processes, it’s much more cost-efficient to re-train the algorithm
    to let it do the work for you.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用为我们做这件事的算法并因此创建可靠且维护成本最低的产品会更好。与其进行昂贵的噪声清理过程，不如重新训练算法，让它为你做这项工作。
- en: In the next chapter, we explore data visualization techniques. These techniques
    help us to understand dependencies in the data and whether it exposes characteristics
    that can be learnt by the machine learning models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨数据可视化技术。这些技术帮助我们理解数据中的依赖关系，以及它是否揭示了可以被机器学习模型学习到的特征。
- en: References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Scott, S. and S. Matwin. Feature engineering for text classification. in*
    *ICML. 1999.*'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*斯科特，S. 和 S. 马特温. 文本分类的特征工程. 在 *ICML. 1999.*'
- en: '*Kulkarni, A., et al., Converting text to features. Natural Language Processing
    Recipes: Unlocking Text Data with Machine Learning and Deep Learning Using Python,
    2021:* *p. 63-106.*'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*库尔卡尼，A. 等，将文本转换为特征. 自然语言处理食谱：使用Python的机器学习和深度学习解锁文本数据，2021:* *p. 63-106.*'
- en: '*Van Hulse, J.D., T.M. Khoshgoftaar, and H. Huang, The pairwise attribute noise
    detection algorithm. Knowledge and Information Systems, 2007\. 11:* *p. 171-190.*'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范·胡尔斯，J.D.，T.M. 科什戈法塔和黄，成对属性噪声检测算法. 知识与信息系统，2007\. 11:* *p. 171-190.*'
- en: '*Li, X., et al.,* *Exploiting BERT for end-to-end aspect-based sentiment analysis.
    arXiv preprint* *arXiv:1910.00883, 2019.*'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*李，X. 等，利用BERT进行端到端基于方面的情感分析. arXiv预印本* *arXiv:1910.00883, 2019.*'
- en: '*Xu, Y. and R. Goodacre, On splitting training and validation set: a comparative
    study of cross-validation, bootstrap and systematic sampling for estimating the
    generalization performance of supervised learning. Journal of analysis and testing,
    2018\. 2(3):* *p. 249-262.*'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*徐，Y. 和 R. 古德雷克，关于分割训练集和验证集：比较交叉验证、自助法和系统抽样在估计监督学习泛化性能方面的研究. 分析与测试杂志，2018\.
    2(3):* *p. 249-262.*'
- en: '*Mosin, V., et al. Comparing Input Prioritization Techniques for Testing Deep
    Learning Algorithms. in 2022 48th Euromicro Conference on Software Engineering
    and Advanced Applications (SEAA).* *2022\. IEEE.*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*莫辛，V. 等. 比较测试深度学习算法的输入优先级技术. 在2022年48届欧姆尼微软件工程和高级应用会议（SEAA）. 2022\. IEEE.*'
- en: '*Liu, X.-Y., J. Wu, and Z.-H. Zhou, Exploratory undersampling for class-imbalance
    learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),
    2008\. 39(2):* *p. 539-550.*'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*刘，X.-Y.，吴，J. 和 周志华，探索性欠采样用于类别不平衡学习. IEEE系统，人，和网络，第B部分（网络学），2008\. 39(2):*
    *p. 539-550.*'
- en: '*Atla, A., et al., Sensitivity of different machine learning algorithms to
    noise. Journal of Computing Sciences in Colleges, 2011\. 26(5):* *p. 96-103.*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*阿特拉，A. 等，不同机器学习算法对噪声的敏感性. 计算机科学学院杂志，2011\. 26(5):* *p. 96-103.*'
