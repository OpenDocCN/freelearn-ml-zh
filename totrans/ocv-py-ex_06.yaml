- en: Chapter 6. Creating a Panoramic Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to stitch multiple images of the
    same scene together to create a panoramic image.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: How to match keypoint descriptors between multiple images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to find overlapping regions between images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to warp images based on the matching keypoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to stitch multiple images to create a panoramic image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching keypoint descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we learned how to extract keypoints using various methods.
    The reason that we extract keypoints is because we can use them for image matching.
    Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching keypoint descriptors](img/B04554_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, it''s the picture of a school bus. Now, let''s take a look
    at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching keypoint descriptors](img/B04554_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is a part of the school bus image and it''s been rotated
    anticlockwise by 90 degrees. We could easily recognize this because our brain
    is invariant to scaling and rotation. Our goal here is to find the matching points
    between these two images. If you do that, it would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching keypoint descriptors](img/B04554_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How did we match the keypoints?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the preceding code, we used the ORB detector to extract the keypoints. Once
    we extracted the keypoints, we used the Brute Force matcher to match the descriptors.
    Brute Force matching is pretty straightforward! For every descriptor in the first
    image, we match it with every descriptor in the second image and take the closest
    one. To compute the closest descriptor, we use the Hamming distance as the metric,
    as shown in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can read more about the Hamming distance at [https://en.wikipedia.org/wiki/Hamming_distance](https://en.wikipedia.org/wiki/Hamming_distance).
    The second argument in the preceding line is a Boolean variable. If this is true,
    then the matcher returns only those keypoints that are closest to each other in
    both directions. This means that if we get (i, j) as a match, then we can be sure
    that the i-th descriptor in the first image has the j-th descriptor in the second
    image as its closest match and vice versa. This increases the consistency and
    robustness of descriptor matching.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the matcher object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider the following line again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the variable matches is a list of DMatch objects. You can read more about
    it in the OpenCV documentation. We just need to quickly understand what it means
    because it will become increasingly relevant in the upcoming chapters. If we are
    iterating over this list of DMatch objects, then each item will have the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**item.distance**: This attribute gives us the distance between the descriptors.
    A lower distance indicates a better match.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**item.trainIdx**: This attribute gives us the index of the descriptor in the
    list of train descriptors (in our case, it''s the list of descriptors in the full
    image).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**item.queryIdx**: This attribute gives us the index of the descriptor in the
    list of query descriptors (in our case, it''s the list of descriptors in the rotated
    subimage).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**item.imgIdx**: This attribute gives us the index of the train image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing the matching keypoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know how to access different attributes of the matcher object, let's
    see how we can use them to draw the matching keypoints. OpenCV 3.0 provides a
    direct function to draw the matching keypoints, but we will not be using that.
    It's better to take a peek inside to see what's happening underneath.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a big output image that can fit both the images side by side.
    So, we do that in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see here, the number of rows is set to the bigger of the two values
    and the number of columns is simply the sum of both the values. For each item
    in the list of matches, we extract the locations of the matching keypoints, as
    we can see in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once we do that, we just draw circles on those points to indicate their locations
    and then draw a line connecting the two points.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the panoramic image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to match keypoints, let''s go ahead and see how we can
    stitch multiple images together. Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the panoramic image](img/B04554_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say we want to stitch the following image with the preceding image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the panoramic image](img/B04554_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we stitch these images, it will look something like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the panoramic image](img/B04554_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s say we captured another part of this house, as seen in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the panoramic image](img/B04554_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we stitch the preceding image with the stitched image we saw earlier, it
    will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the panoramic image](img/B04554_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can keep stitching images together to create a nice panoramic image. Let''s
    take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finding the overlapping regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal here is to find the matching keypoints so that we can stitch the images
    together. So, the first step is to get these matching keypoints. As discussed
    in the previous section, we use a keypoint detector to extract the keypoints,
    and then use a Flann based matcher to match the keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can learn more about Flann at [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.192.5378&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.192.5378&rep=rep1&type=pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The Flann based matcher is faster than Brute Force matching because it doesn't
    compare each point with every single point on the other list. It only considers
    the neighborhood of the current point to get the matching keypoint, thereby making
    it more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Once we get a list of matching keypoints, we use Lowe's ratio test to keep only
    the strong matches. David Lowe proposed this ratio test in order to increase the
    robustness of SIFT.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can read more about this at [http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Basically, when we match the keypoints, we reject the matches in which the ratio
    of the distances to the nearest neighbor and the second nearest neighbor is greater
    than a certain threshold. This helps us in discarding the points that are not
    distinct enough. So, we use that concept here to keep only the good matches and
    discard the rest. If we don't have sufficient matches, we don't proceed further.
    In our case, the default value is 10\. You can play around with this input parameter
    to see how it affects the output.
  prefs: []
  type: TYPE_NORMAL
- en: If we have a sufficient number of matches, then we extract the list of keypoints
    in both the images and extract the homography matrix. If you remember, we have
    already discussed homography in the first chapter. So if you have forgotten about
    it, you may want to take a quick look. We basically take a bunch of points from
    both the images and extract the transformation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Stitching the images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the transformation, we can go ahead and stitch the images.
    We will use the transformation matrix to transform the second list of points.
    We keep the first image as the frame of reference and create an output image that's
    big enough to hold both the images. We need to extract information about the transformation
    of the second image. We need to move it into this frame of reference to make sure
    it aligns with the first image. So, we have to extract the translation information
    and then warp it. We then add the first image into this and construct the final
    output. It is worth mentioning that this works for images with different aspect
    ratios as well. So, if you get a chance, try it out and see what the output looks
    like.
  prefs: []
  type: TYPE_NORMAL
- en: What if the images are at an angle to each other?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we were looking at images that were on the same plane. Stitching
    those images was straightforward and we didn't have to deal with any artifacts.
    In real life, you cannot capture multiple images on exactly the same plane. When
    you are capturing multiple images of the same scene, you are bound to tilt your
    camera and change the plane. So the question is, will our algorithm work in that
    scenario? As it turns out, it can handle those cases as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if the images are at an angle to each other?](img/B04554_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s consider another image of the same scene. It''s at an angle with
    respect to the first image, and it''s partially overlapping as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if the images are at an angle to each other?](img/B04554_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider the first image as our reference. If we stitch these images
    using our algorithm, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if the images are at an angle to each other?](img/B04554_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we keep the second image as our reference, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What if the images are at an angle to each other?](img/B04554_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Why does it look stretched?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you observe, a portion of the output image corresponding to the query image
    looks stretched. It''s because the query image is transformed and adjusted to
    fit into our frame of reference. The reason it looks stretched is because of the
    following lines in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Since the images are at an angle with respect to each other, the query image
    will have to undergo a perspective transformation in order to fit into the frame
    of reference. So, we transform the query image first, and then stitch it into
    our main image to form the panoramic image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to match keypoints among multiple images. We
    discussed how to stitch multiple images together to create a panoramic image.
    We learned how to deal with images that are not on the same plane.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss how to do content-aware image resizing
    by detecting "interesting" regions in the image.
  prefs: []
  type: TYPE_NORMAL
