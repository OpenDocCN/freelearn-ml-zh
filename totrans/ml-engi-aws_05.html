<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer191">
<h1 class="chapter-number" id="_idParaDest-99"><a id="_idTextAnchor105"/><a id="_idTextAnchor106"/>5</h1>
<h1 id="_idParaDest-100"><a id="_idTextAnchor107"/>Pragmatic Data Processing and Analysis</h1>
<p>Data needs to be analyzed, transformed, and processed first before using it when training <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models. In the past, data scientists and ML practitioners had to write custom code from scratch using a variety of libraries, frameworks, and tools (such as <strong class="bold">pandas</strong> and <strong class="bold">PySpark</strong>) to perform the needed analysis and processing work. The custom code prepared by these professionals often needed tweaking since different variations of the steps programmed in the data processing scripts had to be tested on the data before being used for model training. This takes up a significant portion of an ML practitioner’s time, and since this is a manual process, it is usually error-prone as well.</p>
<p>One of the more practical ways to process and analyze data involves the usage of no-code or low-code tools when loading, cleaning, analyzing, and transforming the raw data from different data sources. Using these types of tools will significantly speed up the process since we won’t need to worry about coding the data processing scripts from scratch. In this chapter, we will use <strong class="bold">AWS Glue DataBrew</strong> and <strong class="bold">Amazon SageMaker Data Wrangler</strong> to load, analyze, and process a sample dataset. After cleaning, processing, and transforming the data, we will download and inspect the results in an <strong class="bold">AWS CloudShell</strong> environment. </p>
<p>That said, we will cover the following topics:</p>
<ul>
<li>Getting started with data processing and analysis</li>
<li>Preparing the essential prerequisites</li>
<li>Automating data preparation and analysis with AWS Glue DataBrew</li>
<li>Preparing ML data with Amazon SageMaker Data Wrangler</li>
</ul>
<p>While working on the hands-on solutions in this chapter, you will notice that there are several similarities when using <strong class="bold">AWS Glue DataBrew</strong> and <strong class="bold">Amazon SageMaker Data Wrangler</strong>, but of course, you will notice several differences as well. Before we dive straight into using and comparing these services, let’s have a short discussion first regarding data processing and analysis.</p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor108"/>Technical requirements</h1>
<p>Before we start, it is important that we have the following ready:</p>
<ul>
<li>A web browser (preferably Chrome or Firefox)</li>
<li>Access to the AWS account used in the first four chapters of the book</li>
</ul>
<p>The Jupyter notebooks, source code, and other files used for each chapter are available in this repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Make sure to sign out and NOT use the IAM user created in <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>. In this chapter, you should use the root account or a new IAM user with a set of permissions to create and manage the <strong class="bold">AWS Glue DataBrew</strong>, <strong class="bold">Amazon S3</strong>, <strong class="bold">AWS CloudShell</strong>, and <strong class="bold">Amazon SageMaker</strong> resources. It is recommended to use an IAM user with limited permissions instead of the root account when running the examples in this book. We will discuss this along with other security best practices in further detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>.</p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor109"/>Getting started with data processing and analysis</h1>
<p>In the<a id="_idIndexMarker453"/> previous chapter, we utilized a data warehouse and a data lake to store, manage, and query our data. Data stored in these data sources generally must undergo a series of data processing and data transformation steps similar to those shown in <em class="italic">Figure 5.1</em> before it can be used as a training dataset for ML experiments:</p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 5.1 – Data processing and analysis " height="462" src="image/B18638_05_001.jpg" width="1065"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Data processing and analysis</p>
<p>In <em class="italic">Figure 5.1</em>, we can<a id="_idIndexMarker454"/> see that these data processing steps may involve merging different datasets, along with cleaning, converting, analyzing, and transforming the data using a variety of options and techniques. In practice, data scientists and ML engineers generally spend a lot of hours cleaning the data and getting it ready for use in ML experiments. Some professionals may be used to writing and running custom Python or R scripts to perform this work. However, it may be more practical to use no-code or low-code solutions such as AWS Glue DataBrew and Amazon SageMaker Data Wrangler when dealing with these types of requirements. For one thing, these solutions are more convenient to use since we won’t need to worry about managing the infrastructure, as well as coding the data processing scripts from scratch. We would also be using an easy-to-use visual interface that will help speed up the work significantly. Monitoring and security management are easier as well since these are integrated with other AWS services such as the following: </p>
<ul>
<li><strong class="bold">AWS Identity and Access Management</strong> (<strong class="bold">IAM</strong>) – for controlling and limiting access to AWS services and resources</li>
<li><strong class="bold">Amazon Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) – for defining and configuring a logically isolated network that dictates how resources are accessed and  how each resource communicates with the others within the network</li>
<li><strong class="bold">Amazon CloudWatch</strong> – for monitoring the performance and managing the logs of the resources used</li>
<li><strong class="bold">AWS CloudTrail</strong> – for monitoring and auditing account activity</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">For more information on how these services are used to secure and manage the resources in the AWS account, feel free to check out <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>.</p>
<p>It is important<a id="_idIndexMarker455"/> to note that there are also other options in AWS that can help us when processing and analyzing our data. These include the following:</p>
<ul>
<li><strong class="bold">Amazon Elastic MapReduce</strong> (<strong class="bold">EMR</strong>) and <strong class="bold">EMR Serverless</strong> – for large-scale distributed data processing workloads using a variety of open source tools such as Apache Spark, Apache Hive, and Presto</li>
<li><strong class="bold">Amazon Kinesis</strong> – for processing and analyzing real-time streaming data</li>
<li><strong class="bold">Amazon QuickSight</strong> – for enabling advanced analytics and self-service business intelligence </li>
<li><strong class="bold">AWS Data Pipeline</strong> – for processing and moving data across a variety of services (for example, <strong class="bold">Amazon S3</strong>, <strong class="bold">Amazon Relational Database Service</strong>, and <strong class="bold">Amazon DynamoDB</strong>) using features that help with the scheduling, dependency tracking, and error handling of custom pipeline resources</li>
<li><strong class="bold">SageMaker Processing</strong> – for running custom data processing and analysis scripts (including bias metrics and feature importance computations) on top of the managed infrastructure of AWS with SageMaker</li>
</ul>
<p>Note that this is not an exhaustive list and there are more services and capabilities that can be used for these types of requirements. <em class="italic">What’s the advantage of using these services?</em> When dealing with relatively small datasets, performing data analysis and transformations on our local machine may do the trick. However, once we need to work with much larger datasets, we may need to use a more dedicated set of resources with more computing power, as well as features that allow us to focus on the work we need to do.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will discuss bias detection and feature importance in more detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>,  <em class="italic">Security, Governance, and Compliance Strategies</em>.</p>
<p>In this chapter, we will focus on AWS Glue DataBrew and Amazon SageMaker Data Wrangler and we will <a id="_idIndexMarker456"/>show a few examples of how to use these when processing and analyzing our data. We will start with a “dirty” dataset (containing a few rows with invalid values) and perform the following types of transformations, analyses, and operations on this dataset:</p>
<ul>
<li>Running a data profiling job that analyzes the dataset</li>
<li>Filtering out rows that contain invalid values</li>
<li>Creating a new column from an existing one</li>
<li>Exporting the results after the transformations have been applied</li>
</ul>
<p>Once the file containing the processed results has been uploaded to the output location, we will verify the results by downloading the file and checking whether the transformations have been applied.</p>
<h1 id="_idParaDest-103"><a id="_idTextAnchor110"/>Preparing the essential prerequisites</h1>
<p>In this<a id="_idIndexMarker457"/> section, we will ensure that the following prerequisites are ready before proceeding with the hands-on solutions of this chapter:</p>
<ul>
<li>The Parquet file to be analyzed and processed</li>
<li>The S3 bucket where the Parquet file will be uploaded</li>
</ul>
<h2 id="_idParaDest-104"><a id="_idTextAnchor111"/>Downloading the Parquet file </h2>
<p>In this chapter, we <a id="_idIndexMarker458"/>will <a id="_idIndexMarker459"/>work with a similar <strong class="source-inline">bookings</strong> dataset as the one used in previous chapters. However, the source data is stored in a Parquet file this time, and we have modified some of the rows so that the dataset will have dirty data. That said, let’s download the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file onto our local machine.</p>
<p>You can find it here: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet</a>. </p>
<p class="callout-heading">Note</p>
<p class="callout">Note that storing data using the Parquet format is preferable to storing data using the CSV format. Once you need to work with much larger datasets, the difference in the file sizes of generated Parquet and CSV files becomes apparent. For example, a 1 GB CSV file may end up as just 300 MB (or even less) as a Parquet file! For more information on this topic, feel free to check the following link: <a href="https://parquet.apache.org/docs/">https://parquet.apache.org/docs/</a>.</p>
<p>Make <a id="_idIndexMarker460"/>sure to <a id="_idIndexMarker461"/>download the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file to your local machine before proceeding.</p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor112"/>Preparing the S3 bucket</h2>
<p>You <a id="_idIndexMarker462"/>can <a id="_idIndexMarker463"/>create a new S3 bucket for the hands-on solutions in this chapter or you can reuse an existing one that was created in previous chapters. This S3 bucket will be used to store both the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> source file and the output destination results after running the data processing and transformation steps using AWS Glue DataBrew and Amazon SageMaker Data Wrangler.</p>
<p>Once both prerequisites are ready, we can proceed with using AWS Glue DataBrew to analyze and process our dataset. </p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor113"/>Automating data preparation and analysis with AWS Glue DataBrew</h1>
<p>AWS Glue DataBrew<a id="_idIndexMarker464"/> is a <a id="_idIndexMarker465"/>no-code data preparation service built to help data scientists and ML engineers clean, prepare, and transform data. Similar to the services we used in <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, Glue DataBrew is <em class="italic">serverless</em> as well. This means that we won’t need to worry about<a id="_idIndexMarker466"/> infrastructure management <a id="_idIndexMarker467"/>when using this service to perform data preparation, transformation, and analysis.</p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="Figure 5.2 – The core concepts in AWS Glue DataBrew " height="539" src="image/B18638_05_002.jpg" width="1026"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – The core concepts in AWS Glue DataBrew</p>
<p>In <em class="italic">Figure 5.2</em>, we can see that there are different concepts and resources involved when using AWS Glue DataBrew. We need to have a good idea of what these are before using the service. Here is a quick overview of the concepts and terms used:</p>
<ul>
<li><strong class="bold">Dataset</strong> – Data <a id="_idIndexMarker468"/>stored<a id="_idIndexMarker469"/> in an existing data<a id="_idIndexMarker470"/> source (for example, <strong class="bold">Amazon S3</strong>, <strong class="bold">Amazon Redshift</strong>, or <strong class="bold">Amazon RDS</strong>) or<a id="_idIndexMarker471"/> uploaded from the local <a id="_idIndexMarker472"/>machine to <a id="_idIndexMarker473"/>an S3 bucket.</li>
<li><strong class="bold">Recipe</strong> – A <a id="_idIndexMarker474"/>set of data transformation or data <a id="_idIndexMarker475"/>preparation steps to be performed on a dataset.</li>
<li><strong class="bold">Job</strong> – The<a id="_idIndexMarker476"/> process of running certain instructions<a id="_idIndexMarker477"/> to profile or transform a dataset. Jobs that are used to evaluate a dataset are<a id="_idIndexMarker478"/> called <strong class="bold">profile jobs</strong>. On the other hand, jobs that are used to run a set of instructions to clean, normalize, and transform data are <a id="_idIndexMarker479"/>called <strong class="bold">recipe jobs</strong>. We can use a view <a id="_idIndexMarker480"/>called a <strong class="bold">data lineage</strong> to keep track of the transformation steps that a dataset has been through, along with the origin and destination configured in a job.</li>
<li><strong class="bold">Data profile</strong> – A <a id="_idIndexMarker481"/>report generated after <a id="_idIndexMarker482"/>running a profile job on a dataset.</li>
<li><strong class="bold">Project</strong> – A <a id="_idIndexMarker483"/>managed collection of data, transformation <a id="_idIndexMarker484"/>steps, and jobs.</li>
</ul>
<p>Now that we have<a id="_idIndexMarker485"/> a good idea of what the concepts and terms are, let’s proceed with creating a new dataset.</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor114"/>Creating a new dataset</h2>
<p>In the <em class="italic">Preparing the essential prerequisites</em> section of this chapter, we downloaded a Parquet file <a id="_idIndexMarker486"/>to our local machine. In the next set of steps, we will create a new dataset by uploading this Parquet file from the local machine to an existing Amazon S3 bucket:</p>
<ol>
<li>Navigate to the <strong class="bold">AWS Glue DataBrew</strong> console using the search bar of the <strong class="bold">AWS Management Console</strong>.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region when using services to manage and create different types of resources. You may use a different region but make sure to perform any adjustments needed if certain resources need to be transferred to the region of choice.</p>
<ol>
<li value="2">Go to the <strong class="bold">DATASETS</strong> page by clicking the sidebar icon highlighted in <em class="italic">Figure 5.3</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="Figure 5.3 – Navigating to the DATASETS page " height="369" src="image/B18638_05_003.jpg" width="1071"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Navigating to the DATASETS page</p>
<ol>
<li value="3">Click <strong class="bold">Connect to new dataset</strong>.</li>
<li>Click <strong class="bold">File upload</strong>, as <a id="_idIndexMarker487"/>highlighted in <em class="italic">Figure 5.4</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="Figure 5.4 – Locating the File upload option " height="551" src="image/B18638_05_004.jpg" width="766"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Locating the File upload option</p>
<p class="list-inset">Note that there are different ways to load data and connect to your dataset. We can connect and load data stored in Amazon Redshift, Amazon RDS, and AWS Glue <a id="_idIndexMarker488"/>using <strong class="bold">AWS Glue Data Catalog</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Feel free to check out <a href="https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml">https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml</a> for more information.</p>
<ol>
<li value="5">Specify <strong class="source-inline">bookings</strong> as the value for the <strong class="bold">Dataset name</strong> field (under <strong class="bold">New dataset details</strong>).</li>
<li>Under <strong class="bold">Select a file to upload</strong>, click <strong class="bold">Choose file</strong>. Select the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file from your local machine.</li>
<li>Next, locate <a id="_idIndexMarker489"/>and click the <strong class="bold">Browse S3</strong> button under <strong class="bold">Enter S3 destination</strong>. Select the S3 bucket that you created in the <em class="italic">Preparing the essential prerequisites</em> section of <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Note that the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file in your local machine will be uploaded to the S3 bucket selected in this step. You may create and use a different S3 bucket when working on the hands-on solutions in this chapter. Feel free to create a new S3 bucket using the AWS Management Console or through AWS CloudShell using the AWS CLI.</p>
<ol>
<li value="8">Under <strong class="bold">Additional configurations</strong>, make sure that the <strong class="bold">Selected file type</strong> field is set to <strong class="bold">PARQUET</strong>.</li>
<li>Click the <strong class="bold">Create dataset</strong> button (located at the lower right of the page).</li>
<li>At this point, the <strong class="source-inline">bookings</strong> dataset has been created and it should appear in the list of datasets as shown in <em class="italic">Figure 5.5</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="Figure 5.5 – Navigating to the Datasets preview page " height="372" src="image/B18638_05_005.jpg" width="774"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Navigating to the Datasets preview page</p>
<p class="list-inset">With<a id="_idIndexMarker490"/> that, let’s click the <strong class="source-inline">bookings</strong> dataset name as highlighted in <em class="italic">Figure 5.5</em>. This will redirect you to the <strong class="bold">Dataset preview</strong> page as shown in <em class="italic">Figure 5.6</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<img alt="Figure 5.6 – The dataset preview " height="833" src="image/B18638_05_006.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – The dataset preview</p>
<p class="list-inset">Feel free to check the <strong class="bold">Schema</strong>, <strong class="bold">Text</strong>, and <strong class="bold">Tree</strong> views by clicking the appropriate button on the upper-right side of the <strong class="bold">Dataset preview</strong> pane.</p>
<p>Now that we have successfully uploaded the Parquet file and created a new dataset, let’s proceed with creating and running a profile job to analyze the data.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor115"/>Creating and running a profile job</h2>
<p>Before <a id="_idIndexMarker491"/>performing any data cleaning and data transformation steps, it would be a good idea to analyze the data first and review the properties and statistics of each column in the dataset. Instead of doing this manually, we can use the capability of AWS Glue DataBrew to automatically generate different analysis reports for us. We can generate these reports automatically by running a profile job.</p>
<p>In the next set of steps, we <a id="_idIndexMarker492"/>will create and run a profile job to generate a data profile of the dataset we uploaded:</p>
<ol>
<li value="1">First, click the <strong class="bold">Data profile overview</strong> tab to navigate to the <strong class="bold">Data profile overview</strong> page.</li>
<li>Next, click the <strong class="bold">Run data profile</strong> button. This will redirect you to the <strong class="bold">Create job</strong> page.</li>
<li>On the <strong class="bold">Create job</strong> page, scroll down and locate the <strong class="bold">Job output</strong> settings section, and then click the <strong class="bold">Browse</strong> button to set the <strong class="bold">S3 location</strong> field value. </li>
<li>In the <strong class="bold">Browse S3</strong> pop-up window, locate and select the S3 bucket where the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file was uploaded in an earlier step. </li>
<li>Under <strong class="bold">Permissions</strong>, open the <strong class="bold">Role name</strong> dropdown and select <strong class="bold">Create new IAM role</strong> from the list of options. Specify <strong class="source-inline">mle</strong> as the value for <strong class="bold">New IAM role suffix</strong>.</li>
<li>Click the <strong class="bold">Create and run job</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step may take 3 to 5 minutes to complete. Feel free to grab a cup of coffee or tea! Note that you may see a <strong class="bold">1 job in progress</strong> loading message while waiting for the results to appear.</p>
<ol>
<li value="7">Once the profile job is complete, scroll down, and view the results:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer166">
<img alt="Figure 5.7 – An overview of the data profile " height="828" src="image/B18638_05_007.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – An overview of the data profile</p>
<p class="list-inset">You<a id="_idIndexMarker493"/> should see a <a id="_idIndexMarker494"/>set of results similar to those shown in <em class="italic">Figure 5.7</em>. Feel free to check the following reports generated by the profile job:</p>
<ul>
<li><strong class="bold">Summary</strong> – shows the total number of rows, total number of columns, missing cells, and duplicate rows</li>
<li><strong class="bold">Correlations</strong> – shows a correlation matrix (displaying how each of the variables is related)</li>
<li><strong class="bold">Compare value distribution</strong> – shows a comparative view of the distributions across columns</li>
<li><strong class="bold">Columns summary</strong> – shows summary statistics for each of the columns</li>
</ul>
<p class="list-inset">Optionally, you can navigate to the <strong class="bold">Column statistics</strong> tab and review the reports in that tab as well.</p>
<p>As you can see, it took us just a couple of clicks to generate a data profile that can be used to analyze our dataset. Feel free to review the different reports and statistics generated by the profile job before proceeding with the next part of this chapter.</p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor116"/>Creating a project and configuring a recipe</h2>
<p>It is time that <a id="_idIndexMarker495"/>we create and use an AWS Glue DataBrew project. Creating a project involves working with a dataset and a recipe to perform the desired data processing <a id="_idIndexMarker496"/>and transformation work. Since we don’t have a recipe yet, a new recipe will be created while we are creating and configuring the project. In this section, we will configure a recipe that does the following:</p>
<ul>
<li>Filters out the rows containing invalid <strong class="source-inline">children</strong> column values</li>
<li>Creates a new column (<strong class="source-inline">has_booking_changes</strong>) based on the value of an existing column (<strong class="source-inline">booking_changes</strong>)</li>
</ul>
<p>In the next set of steps, we will create a project and use the interactive user interface to configure a recipe for cleaning and transforming the data:</p>
<ol>
<li value="1">In the upper-right-hand corner of the page, locate and click the <strong class="bold">Create project with this dataset</strong> button. This should redirect you to the <strong class="bold">Create project</strong> page.</li>
<li>Under <strong class="bold">Project details</strong>, specify <strong class="source-inline">bookings-project</strong> as the value for the <strong class="bold">Project name</strong> field.</li>
<li>Scroll down and locate the <strong class="bold">Role name</strong> drop-down field under <strong class="bold">Permissions</strong>. Select the existing IAM role created in an earlier step.</li>
<li>Click the <strong class="bold">Create project</strong> button afterward:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 5.8 – Waiting for the project to be ready " height="980" src="image/B18638_05_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Waiting for the project to be ready</p>
<p class="list-inset">After <a id="_idIndexMarker497"/>clicking the <strong class="bold">Create project</strong> button, you should be redirected <a id="_idIndexMarker498"/>to a page similar to that shown in <em class="italic">Figure 5.8</em>. After creating a project, we should be able to use a highly interactive workspace where we can test and apply a variety of data transformations.</p>
<p class="callout-heading">Note</p>
<p class="callout">This step may take 2 to 3 minutes to complete</p>
<ol>
<li value="5">Once the project session is ready, we’ll do a quick check of the data to find any erroneous entries and spot issues in the data (so that we can filter these out). In the left pane showing a grid view of the data, locate and scroll (either left or right) to the <strong class="bold">children</strong> column similar to what is shown in <em class="italic">Figure 5.9</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="Figure 5.9 – Filtering out the rows with invalid cell values " height="1021" src="image/B18638_05_009.jpg" width="1630"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Filtering out the rows with invalid cell values</p>
<p class="list-inset">We <a id="_idIndexMarker499"/>should see the <strong class="source-inline">children</strong> column between the <strong class="source-inline">adults</strong> and <strong class="source-inline">babies</strong><strong class="bold"> column</strong>. Here, we can see that the minimum value detected <a id="_idIndexMarker500"/>by Glue DataBrew for the <strong class="bold">children</strong> column is <strong class="source-inline">-1</strong> and there are cells under the <strong class="source-inline">children</strong> column with a value of <strong class="source-inline">-1</strong>. Once you have reviewed the different values under the <strong class="source-inline">children</strong> column, click the <strong class="bold">FILTER</strong> button as highlighted in <em class="italic">Figure 5.9</em>. </p>
<p class="callout-heading">Note</p>
<p class="callout">Note that we have intentionally added a certain number of <strong class="source-inline">-1</strong> values under the <strong class="source-inline">children</strong> column in the Parquet file used in this chapter. Given that it is impossible to have a value less than <strong class="source-inline">0</strong> for the <strong class="source-inline">children</strong> column, we will filter out these rows in the next set of steps. </p>
<ol>
<li value="6">After clicking the <strong class="bold">FILTER</strong> button, a drop-down menu should appear. Locate and select <strong class="bold">Greater than or equal to</strong> from the list of options under <strong class="bold">By condition</strong>. This should update the pane on the right-hand side of the page and show the list of configuration options for the <strong class="bold">Filter values</strong> operation.</li>
<li>In the <strong class="bold">Filter values</strong> pane on the right-hand side of the page, locate and select the <strong class="source-inline">children</strong> column from the list of options for the <strong class="bold">Source column</strong> field. For the <strong class="bold">Filter condition</strong> configuration, specify a filter value of <strong class="source-inline">0</strong> in the field with the placeholder background text of <strong class="bold">Enter a filter value</strong>.</li>
<li>Click <strong class="bold">Preview changes</strong>. This should update the left-hand pane and provide a grid <a id="_idIndexMarker501"/>view of the dataset:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="Figure 5.10 – Previewing the results " height="725" src="image/B18638_05_010.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Previewing the results</p>
<p class="list-inset">We should <a id="_idIndexMarker502"/>see that the rows with a value of <strong class="source-inline">-1</strong> under the <strong class="source-inline">children</strong> column have been filtered out, similar to what is shown in <em class="italic">Figure 5.10</em>.</p>
<ol>
<li value="9">Next, click the <strong class="bold">Apply</strong> button.</li>
<li>Let’s proceed with adding a step for creating a new column (from an existing one). Locate and click the <strong class="bold">Add step</strong> button as highlighted in <em class="italic">Figure 5.11</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="Figure 5.11 – Adding a step " height="231" src="image/B18638_05_011.jpg" width="641"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Adding a step</p>
<p class="list-inset">The <strong class="bold">Add step</strong> button <a id="_idIndexMarker503"/>should be in the same row where the <strong class="bold">Clear all</strong> link is located.</p>
<ol>
<li value="11">Open<a id="_idIndexMarker504"/> the drop-down field with the <strong class="bold">Find steps</strong> placeholder background text and then type <strong class="source-inline">create</strong> in the search field. Select the <strong class="bold">Based on conditions</strong> option from the list of results:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="Figure 5.12 – Locating the Based on conditions option " height="332" src="image/B18638_05_012.jpg" width="513"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Locating the Based on conditions option</p>
<p class="list-inset">If you are looking for the search field, simply refer to the highlighted box (at the top) in <em class="italic">Figure 5.12</em>. </p>
<ol>
<li value="12">In the <strong class="bold">Create column</strong> pane, scroll down and locate the <strong class="bold">Source</strong> configuration options. Specify <a id="_idIndexMarker505"/>the following configuration values:<ul><li><strong class="bold">Column name</strong>: <strong class="source-inline">booking_changes</strong></li>
<li><strong class="bold">Logical condition</strong>: <strong class="source-inline">Greater than</strong></li>
<li><strong class="bold">Enter a value</strong>: <strong class="source-inline">0</strong></li>
<li><strong class="bold">Flag result value as</strong>: <strong class="source-inline">True or False</strong></li>
<li><strong class="bold">Destination column</strong>: <strong class="source-inline">has_booking_changes</strong></li>
</ul></li>
<li>Click <strong class="bold">Preview changes</strong>. This should show us a preview of the data with a new column <a id="_idIndexMarker506"/>called <strong class="source-inline">has_booking_changes</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer172">
<img alt="Figure 5.13 – Previewing the changes " height="773" src="image/B18638_05_013.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Previewing the changes</p>
<p class="list-inset">As we can see in <em class="italic">Figure 5.13</em>, this new column has a value of <strong class="source-inline">true</strong> if the <strong class="source-inline">booking_changes</strong> column has a value greater than <strong class="source-inline">0</strong>, and <strong class="source-inline">false</strong> otherwise.</p>
<ol>
<li value="14">Review the preview results before clicking the <strong class="bold">Apply</strong> button. </li>
<li>At this <a id="_idIndexMarker507"/>point, we should have two applied steps in our recipe. Click <strong class="bold">Publish</strong>, as highlighted in <em class="italic">Figure 5.14</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer173">
<img alt="Figure 5.14 – Locating and clicking the Publish button " height="276" src="image/B18638_05_014.jpg" width="753"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Locating and clicking the Publish button</p>
<p class="list-inset">This should open the <strong class="bold">Publish recipe</strong> window.</p>
<ol>
<li value="16">In<a id="_idIndexMarker508"/> the <strong class="bold">Publish recipe</strong> pop-up window, click <strong class="bold">Publish</strong>. Note that we can specify an optional version description when publishing the current recipe.</li>
</ol>
<p>Now that we have a published recipe, we can proceed with creating a recipe job that will execute the different steps configured in the recipe.</p>
<p class="callout-heading">Note</p>
<p class="callout">After publishing the recipe, we still need to run a recipe job before the changes are applied (resulting in the generation of a new file with the applied data transformations).</p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor117"/>Creating and running a recipe job</h2>
<p>As we can <a id="_idIndexMarker509"/>see in <em class="italic">Figure 5.15</em>, a recipe job needs to be configured with a source and a destination. The job reads the data stored in the source, performs the transformation<a id="_idIndexMarker510"/> steps configured in the associated recipe, and stores the processed files in the destination. </p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="Figure 5.15 – A job needs to be configured with a source and a destination " height="122" src="image/B18638_05_015.jpg" width="890"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – A job needs to be configured with a source and a destination</p>
<p>It is important to note that the source data is not modified since the recipe job only connects in a read-only manner. After the recipe job has finished processing all the steps, the job results are stored in one or more of the configured output destinations. </p>
<p>In the next set of steps, we will create and run a recipe job using the recipe we published in the previous section:</p>
<ol>
<li value="1">Navigate to the <strong class="bold">Recipes</strong> page using the sidebar on the left-hand side.</li>
<li>Select the row named <strong class="source-inline">bookings-project-recipe</strong> (which should toggle the checkbox and highlight the entire row).</li>
<li>Click the <strong class="bold">Create job with this recipe</strong> button. This will redirect you to the <strong class="bold">Create job</strong> page.</li>
<li>On the <strong class="bold">Create job</strong> page, specify the following configuration values:<ul><li><strong class="bold">Job details</strong><ul><li><strong class="bold">Job name</strong>: <strong class="source-inline">bookings-clean-and-add-column</strong></li>
</ul></li>
<li><strong class="bold">Job input</strong><ul><li><strong class="bold">Run on</strong>: <strong class="source-inline">Project</strong></li>
<li><strong class="bold">Select a project</strong>: Click <strong class="bold">Browse projects</strong> and then choose <strong class="source-inline">bookings-project</strong>.</li>
</ul></li>
<li><strong class="bold">Job output settings</strong><ul><li><strong class="bold">S3 location</strong>: Click <strong class="bold">Browse</strong>. Locate and choose the same S3 bucket used in the previous steps in this chapter.</li>
</ul></li>
<li><strong class="bold">Permissions</strong>:<ul><li><strong class="bold">Role name</strong>: Choose the IAM role created in an earlier step.</li>
</ul></li>
</ul></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">We are <a id="_idIndexMarker511"/>not limited to storing the job output results in S3. We can also store the output results<a id="_idIndexMarker512"/> in <strong class="bold">Amazon Redshift</strong>, <strong class="bold">Amazon RDS</strong> tables, and more. For more information, feel free to check the following link: <a href="https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml">https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml</a>.</p>
<ol>
<li value="5">Review <a id="_idIndexMarker513"/>the specified configuration and then click the <strong class="bold">Create and run job</strong> button. If you accidentally clicked on the <strong class="bold">Create job</strong> button (beside the <strong class="bold">Create and run job</strong> button), you may click the <strong class="bold">Run job</strong> button <a id="_idIndexMarker514"/>after the job has been created.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Wait for 3 to 5 minutes for this step to complete. Feel free to grab a cup of coffee or tea while waiting!</p>
<p>Wasn’t that easy? Creating, configuring, and running a recipe job is straightforward. Note that we can configure this recipe job and automate the job runs by associating a schedule. For more information on this topic, you can check the following link: <a href="https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml#jobs.scheduling">https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml</a>. </p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor118"/>Verifying the results</h2>
<p>Now, let’s <a id="_idIndexMarker515"/>proceed with inspecting the recipe job output results in AWS CloudShell, a free browser-based shell we can use to manage our AWS resources using a terminal. In the next set of steps, we will download the recipe job output results into the CloudShell environment and check whether the expected changes are reflected in the downloaded file:</p>
<ol>
<li value="1">Once <strong class="bold">Last job run status</strong> of the job has changed to <strong class="bold">Succeeded</strong>, click the <strong class="bold">1 output</strong> link under the <strong class="bold">Output</strong> column. This should open the <strong class="bold">Job output locations</strong> window. Click the S3 link under the <strong class="bold">Destination</strong> column to open the S3 bucket page in a new tab.</li>
<li>Use the <strong class="bold">Find objects by prefix</strong> search box to locate the <strong class="bold">Folder</strong> item with a name that starts with <strong class="source-inline">bookings-clean-and-add-column</strong>. Make sure to press the <em class="italic">ENTER</em> key to filter the list of objects. Navigate to the <strong class="bold">Folder</strong> item by clicking the right link under the <strong class="bold">Name</strong> column. It should contain a CSV file that starts with <strong class="source-inline">bookings-clean-and-add-column</strong> and ends with <strong class="source-inline">part00000</strong>.</li>
<li>Select the CSV file (which should toggle the checkbox) and then click the <strong class="bold">Copy S3 URI</strong> button.</li>
<li>Navigate to <strong class="bold">AWS CloudShell</strong> by clicking the icon, as highlighted in <em class="italic">Figure 5.16</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 5.16 – Navigating to CloudShell " height="69" src="image/B18638_05_016.jpg" width="239"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Navigating to CloudShell</p>
<p class="list-inset">We can find this button in the upper-right-hand corner of the AWS Management Console. You can also use the search bar to navigate to the CloudShell console.</p>
<ol>
<li value="5">When <a id="_idIndexMarker516"/>you see the <strong class="bold">Welcome to AWS CloudShell</strong> window, click the <strong class="bold">Close</strong> button. Wait for the environment to run (for about 1 to 2 minutes) before proceeding.</li>
<li>Run the following commands in the CloudShell environment (after <strong class="bold">$</strong>). Make sure to replace <strong class="source-inline">&lt;PASTE COPIED S3 URL&gt;</strong> with what was copied to the clipboard in an earlier step:<pre class="source-code"><strong class="bold">TARGET=&lt;PASTE COPIED S3 URL&gt;</strong></pre><pre class="source-code"><strong class="bold">aws s3 cp $TARGET bookings.csv</strong></pre></li>
</ol>
<p class="list-inset">This should download the output CSV file from S3 into the CloudShell environment.</p>
<ol>
<li value="7">Use the <strong class="source-inline">head</strong> command to inspect the first few rows of the <strong class="source-inline">bookings.csv</strong> file:<pre class="source-code"><strong class="bold">head bookings.csv</strong></pre></li>
</ol>
<p class="list-inset">This should return the first row containing the header of the CSV file, along with the first few records of the dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="Figure 5.17 – Verifying the job results " height="235" src="image/B18638_05_017.jpg" width="1155"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Verifying the job results</p>
<p class="list-inset">In <em class="italic">Figure 5.17</em>, we can see that the processed dataset now includes the <strong class="source-inline">has_booking_changes</strong> column containing <strong class="source-inline">true</strong> or <strong class="source-inline">false</strong> values. You can inspect the CSV file further and verify that there are no more <strong class="source-inline">-1</strong> values under the <strong class="source-inline">children</strong> column. We will leave this to you as an exercise.</p>
<p>Now that we’re <a id="_idIndexMarker517"/>done using AWS Glue DataBrew to analyze and process our data, we can proceed with using Amazon SageMaker Data Wrangler to perform a similar set of operations. </p>
<p class="callout-heading">Important Note</p>
<p class="callout">Do not forget to delete all Glue DataBrew resources (such as the recipe job, profile job, recipe, project, and dataset) once you are done working on the hands-on solutions in this chapter.</p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor119"/>Preparing ML data with Amazon SageMaker Data Wrangler</h1>
<p>Amazon SageMaker has a lot <a id="_idIndexMarker518"/>of capabilities and features to assist data scientists and ML engineers with the different<a id="_idIndexMarker519"/> ML requirements. One of <a id="_idIndexMarker520"/>the capabilities of SageMaker focused on accelerating data preparation <a id="_idIndexMarker521"/>and data analysis is SageMaker Data Wrangler: </p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler " height="146" src="image/B18638_05_018.jpg" width="1120"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler</p>
<p>In <em class="italic">Figure 5.18</em>, we can see what we can do with our data when using SageMaker Data Wrangler: </p>
<ol>
<li value="1">First, we can import data from a variety of data sources such as Amazon S3, Amazon Athena, and <a id="_idIndexMarker522"/>Amazon Redshift. </li>
<li>Next, we<a id="_idIndexMarker523"/> can create a data flow and transform the data using a variety of data formatting and data transformation options. We can also analyze and visualize the data using both inbuilt and custom options in just a few clicks. </li>
<li>Finally, we can automate the data preparation workflows by exporting one or more of the transformations configured in the data processing pipeline.</li>
</ol>
<p>SageMaker Data Wrangler<a id="_idIndexMarker524"/> is integrated into SageMaker Studio, which allows us to use this capability to process our data and automate our data processing workflows without having to leave the development environment. Instead of having to code everything from scratch using a variety of tools, libraries, and frameworks such as pandas and PySpark, we can simply use SageMaker Data Wrangler to help us prepare custom data flows using an interface and automatically generate reusable code within minutes!</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Make sure to sign out and <em class="italic">NOT</em> use the IAM user created in <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>. You should use the root account or a new IAM user with a set of permissions to create and manage the AWS Glue DataBrew, Amazon S3, AWS CloudShell, and Amazon SageMaker resources. It is recommended to use an IAM user with limited permissions instead of the root account when running the examples in this book. We will discuss this along with other security best practices in detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>.</p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor120"/>Accessing Data Wrangler</h2>
<p>We need <a id="_idIndexMarker525"/>to open SageMaker Studio to access SageMaker Data Wrangler. </p>
<p class="callout-heading">Note</p>
<p class="callout">Make sure that you have completed the hands-on solutions in the <em class="italic">Getting started with SageMaker and SageMaker Studio</em> section of <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, before proceeding. You can also update SageMaker Studio along with Studio Apps (in case you’re using an old version). For more information about this topic, you can check the following link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml</a>. Note that the steps in this section assume that we’re using JupyterLab 3.0. If you are using a different version, you may experience a few differences in terms of layout and user experience.</p>
<p>In the next <a id="_idIndexMarker526"/>set of steps, we will launch SageMaker Studio and access Data Wrangler from the <strong class="bold">File</strong> menu:</p>
<ol>
<li value="1">Navigate to <strong class="bold">SageMaker Studio</strong> by typing <strong class="source-inline">sagemaker studio</strong> into the search bar of the AWS Management Console and selecting <strong class="bold">SageMaker Studio</strong> from the list of results under <strong class="bold">Features</strong>.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region when using services to manage and create different types of resources. You may use a different region but make sure to perform any adjustments needed in case certain resources need to be transferred to the region of choice.</p>
<ol>
<li value="2">Next, click <strong class="bold">Studio</strong> under <strong class="bold">SageMaker Domain</strong> in the sidebar.</li>
<li>Click <strong class="bold">Launch app</strong>, as highlighted in <em class="italic">Figure 5.19</em>. Select <strong class="bold">Studio</strong> from the list of drop-down options:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 5.19 – Opening SageMaker Studio " height="427" src="image/B18638_05_019.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – Opening SageMaker Studio</p>
<p class="list-inset">This will<a id="_idIndexMarker527"/> redirect you to <strong class="bold">SageMaker Studio</strong>. Wait for a few seconds for the interface to load.</p>
<ol>
<li value="4">Open the <strong class="bold">File</strong> menu. From the list of options under the <strong class="bold">New</strong> submenu, locate and click <strong class="bold">Data Wrangler Flow</strong>. You should see the <strong class="bold">Data Wrangler is loading</strong> message for about 3 to 5 minutes while an <strong class="source-inline">ml.m5.4xlarge</strong> instance is being provisioned to run Data Wrangler. Once ready, you’ll see the <strong class="bold">Import data</strong> page.</li>
</ol>
<p>With that, let’s proceed with importing our data in the following section.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Once you are done with the hands-on solutions in this chapter, the <strong class="source-inline">ml.m5.4xlarge</strong> instance used to run Data Wrangler needs to be turned off immediately to avoid any additional charges. Click and locate the circle icon on the left-hand sidebar to show the list of running instances, apps, kernel sessions, and terminal sessions. Make sure to shut down all running instances under <strong class="bold">RUNNING INSTANCES</strong> whenever you are done using SageMaker Studio.</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor121"/>Importing data</h2>
<p>There<a id="_idIndexMarker528"/> are several options when importing data for use in Data Wrangler. We can import and load data from a<a id="_idIndexMarker529"/> variety of<a id="_idIndexMarker530"/> sources including Amazon S3, Amazon Athena, Amazon Redshift, Databricks (JDBC), and Snowflake.</p>
<p>In the next set of steps, we will focus on importing the data stored in the Parquet file uploaded in an S3 bucket in our account:</p>
<ol>
<li value="1">On the <strong class="bold">Import data</strong> page (under the <strong class="bold">Import</strong> tab), click <strong class="bold">Amazon S3</strong>.</li>
<li>On the <strong class="bold">Import a dataset from S3</strong> page, locate and select the <strong class="source-inline">synthetic.bookings.dirty.parquet</strong> file uploaded in one of the S3 buckets in your AWS account.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">In case you skipped the <em class="italic">Automating data preparation and analysis with AWS Glue DataBrew</em> section of this chapter, you need to upload the Parquet file downloaded in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter to a new or existing Amazon S3 bucket.</p>
<ol>
<li value="3">If you see a <strong class="bold">Preview Error</strong> notification similar to what is shown in <em class="italic">Figure 5.20</em>, you may remove this by opening the <strong class="bold">File type</strong> dropdown and choosing <strong class="bold">parquet</strong> from the list of options.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 5.20 – Setting the file type to Parquet " height="556" src="image/B18638_05_020.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – Setting the file type to Parquet</p>
<p class="list-inset">The <strong class="bold">Preview Error</strong> message should disappear after the <strong class="bold">parquet</strong> option is selected from the <strong class="bold">File type</strong> drop-down menu.</p>
<p class="list-inset">If you’re<a id="_idIndexMarker531"/> using JupyterLab version 3.0, the <strong class="bold">parquet</strong> option is already preselected.</p>
<ol>
<li value="4">If you’re using JupyterLab version 1.0, the <strong class="bold">csv</strong> option may be preselected instead of the <strong class="bold">parquet</strong> option. That said, regardless of the version, we should set the <strong class="bold">File type</strong> drop-down value to <strong class="bold">parquet</strong>. Click the <strong class="bold">Import</strong> button located in the upper-right corner of the page. This will redirect you back to the <strong class="bold">Data flow</strong> page.</li>
</ol>
<p>Note that we are not<a id="_idIndexMarker532"/> limited to importing from Amazon S3. We can also import data from Amazon Athena, Amazon Redshift, and other data sources. You may check <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml</a> for more information.</p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor122"/>Transforming the data</h2>
<p>There <a id="_idIndexMarker533"/>are many built-in options in SageMaker Data Wrangler when processing and transforming our data. In this chapter, we will show a quick demo of how to use a custom PySpark script to transform data. </p>
<p class="callout-heading">Note</p>
<p class="callout">For more information on the <a id="_idIndexMarker534"/>numerous data transforms available, feel free to check the following link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml</a>.</p>
<p>In the next set <a id="_idIndexMarker535"/>of steps, we will add and configure a custom PySpark transform to clean and process our data:</p>
<ol>
<li value="1">If you can see the <strong class="bold">Data types · Transform: synthetic.bookings.dirty.parquet</strong> page, navigate back to the <strong class="bold">Data flow</strong> page by clicking the <strong class="bold">&lt; Data flow</strong> button located in the top-left-hand corner of the page. We’ll go back to this page in a bit, after having a quick look at the current configuration of the data flow in the next step.</li>
<li>On the <strong class="bold">Data flow</strong> page, click the <strong class="bold">+</strong> button, as highlighted in <em class="italic">Figure 5.21</em>. Select <strong class="bold">Add transform</strong> from the list of options:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 5.21 – Adding a transform " height="500" src="image/B18638_05_021.jpg" width="973"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Adding a transform</p>
<p class="list-inset">In this chapter, we will only work with a single dataset. However, it is important to note that we can work on and merge two datasets using the <strong class="bold">Join</strong> option as we have in <em class="italic">Figure 5.21</em>.</p>
<ol>
<li value="3">In <a id="_idIndexMarker536"/>the <strong class="bold">ALL STEPS</strong> pane on the left side of the page, click the <strong class="bold">Add step</strong> button. This should show a list of options for transforming the dataset.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you’re using <strong class="bold">JupyterLab 1.0</strong>, you should see the left pane labeled as <strong class="bold">TRANSFORMS</strong> instead of <strong class="bold">ALL STEPS</strong>.</p>
<ol>
<li value="4">Select <strong class="bold">Custom transform</strong> from the list of options available.</li>
<li>In <strong class="bold">CUSTOM TRANSFORM</strong>, enter the following code into the code editor:<pre class="source-code">df = df.filter(df.<strong class="bold">children</strong> &gt;= 0)</pre><pre class="source-code">expression = df.<strong class="bold">booking_changes</strong> &gt; 0</pre><pre class="source-code">df = df.withColumn('<strong class="bold">has_booking_changes</strong>', expression)</pre></li>
</ol>
<p class="list-inset">What this code block does is select and retain all rows where the value of the <strong class="source-inline">children</strong> column is <strong class="source-inline">0</strong> or higher, and creates a new column, <strong class="source-inline">has_booking_changes,</strong> that has a value of <strong class="source-inline">true</strong> if the value in the <strong class="source-inline">booking_changes</strong> column is greater than <strong class="source-inline">0</strong> and <strong class="source-inline">false</strong> otherwise.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you’re using <strong class="bold">JupyterLab 1.0</strong>, you should see the left-hand pane labeled <strong class="bold">CUSTOM PYSPARK</strong> instead of <strong class="bold">CUSTOM TRANSFORM</strong>.</p>
<ol>
<li value="6">Click the <strong class="bold">Preview</strong> button. This should show a preview of the data with the additional <strong class="source-inline">has_booking_changes</strong> column, similar to what we have in <em class="italic">Figure 5.22</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 5.22 – Previewing the changes " height="606" src="image/B18638_05_022.jpg" width="1013"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – Previewing the changes</p>
<p class="list-inset">You <a id="_idIndexMarker537"/>should find the <strong class="source-inline">has_booking_changes</strong> column beside the <strong class="source-inline">total_of_special_requests</strong> column (which is the leftmost column in the preview).</p>
<ol>
<li value="7">Once you have finished reviewing the preview of the data, you can provide an optional <strong class="bold">Name</strong> value before clicking the <strong class="bold">Add</strong> button.</li>
<li>After clicking the <strong class="bold">Add</strong> button in the previous step, locate and click the <strong class="bold">&lt; Data flow</strong> link (or the <strong class="bold">Back to data flow</strong> link) located on the upper-right-hand side of the page.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">It is important to note that the steps are not executed yet, as we’re just defining what will run in a later step.</p>
<p>Note that we are just scratching the surface of what we can do with SageMaker Data Wrangler here. Here <a id="_idIndexMarker538"/>are a few other examples of the other transforms available as well:</p>
<ul>
<li>Balancing the data (for example, random oversampling, random undersampling, and SMOTE)</li>
<li>Encoding categorical data (for example, one-hot encoding, similarity encoding)</li>
<li>Handling missing time series data</li>
<li>Extracting features from time series data</li>
</ul>
<p>For a more complete list of transforms, feel free to check the following link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml</a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Feel free to check the following blog post in case you’re interested in diving a bit deeper into how to balance data using a variety of techniques (such as random oversampling, random undersampling, and SMOTE): <a href="https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/">https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/</a>.</p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor123"/>Analyzing the data</h2>
<p>It is <a id="_idIndexMarker539"/>critical that we analyze the data we will use in later steps to train ML models. We need to have a good idea of the properties that could inadvertently affect the behavior and performance of the ML models trained using this data. There are a variety of ways to analyze a dataset and the great thing about SageMaker Data Wrangler is that it allows us to choose from a list of pre-built analysis options and visualizations, including the ones in the following list:</p>
<ul>
<li><strong class="bold">Histograms</strong> – can <a id="_idIndexMarker540"/>be used to show the “shape” of the distribution of the data</li>
<li><strong class="bold">Scatter plots</strong> – can<a id="_idIndexMarker541"/> be used to show the relationship between two numeric variables (using dots representing each data point from the dataset)</li>
<li><strong class="bold">Table summary</strong> – can be <a id="_idIndexMarker542"/>used to show the summary statistics of the dataset (for example, the number of records or the minimum and maximum values in each column) </li>
<li><strong class="bold">Feature importance scores</strong> (using Quick Model) – used to analyze the impact of <a id="_idIndexMarker543"/>each feature when predicting a target variable</li>
<li><strong class="bold">Target leakage analysis</strong> – can <a id="_idIndexMarker544"/>be used to detect columns in the dataset that are strongly correlated with the column we want to predict</li>
<li><strong class="bold">Anomaly detection for time series data</strong> – can be used to detect outliers in time <a id="_idIndexMarker545"/>series data</li>
<li><strong class="bold">Bias reports</strong> – can <a id="_idIndexMarker546"/>be<a id="_idIndexMarker547"/> used to detect potential biases in the dataset (by calculating different bias metrics)</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Note that this is not an exhaustive list and you may see other options when you are working on the hands-on portion of this section.</p>
<p>In the next set of steps, we will create an analysis and generate a bias report:</p>
<ol>
<li value="1">Click the <strong class="bold">+</strong> button and select <strong class="bold">Add analysis</strong> from the list of options, similar to that in <em class="italic">Figure 5.23</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 5.23 – Adding an analysis " height="536" src="image/B18638_05_023.jpg" width="954"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – Adding an analysis</p>
<p class="list-inset">You should see the <strong class="bold">Create analysis</strong> pane located on the left-hand side of the page.</p>
<ol>
<li value="2">Specify the<a id="_idIndexMarker548"/> following configuration options in the <strong class="bold">Create analysis</strong> pane:  <ul><li><strong class="bold">Analysis type</strong>: <strong class="source-inline">Bias report</strong></li>
<li><strong class="bold">Analysis name</strong>: <strong class="source-inline">Sample analysis</strong></li>
<li><strong class="bold">Select the column your model predicts (target)</strong>: <strong class="source-inline">is_cancelled</strong></li>
<li><strong class="bold">Predicted value(s)</strong>: <strong class="source-inline">1</strong></li>
<li><strong class="bold">Select the column to analyze for bias</strong>: <strong class="source-inline">babies</strong></li>
<li><strong class="bold">Is your column a value or threshold?</strong>: <strong class="source-inline">Threshold</strong></li>
<li><strong class="bold">Column threshold to analyze for bias</strong>: <strong class="source-inline">1</strong></li>
</ul></li>
<li>Scroll down to the bottom of the page. Locate and click the <strong class="bold">Check for bias</strong> button (beside the <strong class="bold">Save</strong> button).</li>
<li>Scroll up and locate the bias report, similar to what is shown in <em class="italic">Figure 5.24</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 5.24 – The bias report " height="683" src="image/B18638_05_024.jpg" width="986"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – The bias report</p>
<p class="list-inset">Here, we<a id="_idIndexMarker549"/> can see that the <strong class="bold">Class Imbalance</strong> metric value is <strong class="source-inline">0.92</strong>. This means that the dataset is highly imbalanced and the advantaged group (<strong class="source-inline">is_cancelled = 1</strong>) is represented at a much higher rate compared to the disadvantaged group (<strong class="source-inline">is_cancelled = 0</strong>).</p>
<p class="callout-heading">Note</p>
<p class="callout">We will dive deeper into the details of how bias metrics are computed and interpreted in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>. </p>
<ol>
<li value="5">Scroll down and click the <strong class="bold">Save</strong> button (beside the <strong class="bold">Check for bias</strong> button)</li>
<li>Locate and click the <strong class="bold">&lt; Data flow</strong> link (or the <strong class="bold">Back to data flow</strong> link) to return to the <strong class="bold">Data flow</strong> page.</li>
</ol>
<p>In addition to bias reports, we can generate data visualizations such as histograms and scatter plots to help us analyze our data. We can even generate a quick model using the provided dataset <a id="_idIndexMarker550"/>and generate a feature importance report (with scores showing the impact of each feature when predicting a target variable). For more information, feel free to check out the following link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml</a>.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor124"/>Exporting the data flow</h2>
<p>With<a id="_idIndexMarker551"/> everything ready, let’s proceed with exporting the data flow we prepared in the previous sections. There are different options when performing the export operation. This includes exporting the data to an Amazon S3 bucket. We can also choose to export one or more steps from the data flow<a id="_idIndexMarker552"/> to <strong class="bold">SageMaker Pipelines</strong> using a Jupyter notebook that includes the relevant blocks of code. Similarly, we also have the option<a id="_idIndexMarker553"/> to export the features we have prepared to <strong class="bold">SageMaker Feature Store</strong>. There’s an option to export the data flow steps directly to Python code as well. </p>
<p class="callout-heading">Note</p>
<p class="callout">Once the data flow steps have been exported and converted to code, the generated code and the Jupyter notebooks can be run to execute the different steps configured in the data flow. Finally, experienced ML practitioners may opt to modify the generated notebooks and code if needed.</p>
<p>In the next set of steps, we will perform the export operation and generate a Jupyter notebook that will<a id="_idIndexMarker554"/> utilize a <strong class="bold">SageMaker processing</strong> job to process the data and save the results to an S3 bucket:</p>
<ol>
<li value="1">Click the <strong class="bold">+</strong> button after the third box, <strong class="bold">Python (PySpark)</strong> (or using the custom name that you specified in an earlier step), as highlighted in <em class="italic">Figure 5.25</em>, and then open the list of options under <strong class="bold">Export to</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 5.25 – Exporting the step " height="438" src="image/B18638_05_025.jpg" width="1021"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – Exporting the step</p>
<p class="list-inset">This <a id="_idIndexMarker555"/>should give us a list of options that includes the following: </p>
<ul>
<li><strong class="bold">Amazon S3 (via Jupyter Notebook)</strong></li>
<li><strong class="bold">SageMaker Pipelines (via Jupyter Notebook)</strong></li>
<li><strong class="bold">Python Code</strong></li>
<li><strong class="bold">SageMaker Feature Store (via Jupyter Notebook)</strong></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">If you’re using JupyterLab 1.0, you will need to navigate to the <strong class="bold">Export data flow</strong> page first by clicking the <strong class="bold">Export</strong> tab beside the <strong class="bold">Data Flow</strong> tab. After that, you’ll need to click the third box (under <strong class="bold">Custom PySpark</strong>) and then click the <strong class="bold">Export Step</strong> button (which will open the drop-down list of options).</p>
<ol>
<li value="2">Select <strong class="bold">Amazon S3 (via Jupyter Notebook)</strong> from the list of options. This should generate and open the <strong class="bold">Save to S3 with a SageMaker Processing Job</strong> Jupyter notebook. Note that at this point, the configured data transformations have not been applied yet and we would need to run the cells in the generated notebook file to apply the transformations.</li>
<li>Locate and click the first runnable cell. Run it using the <strong class="bold">Run the selected cells and advance</strong> button, as highlighted in <em class="italic">Figure 5.26</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 5.26 – Running the first cell " height="526" src="image/B18638_05_026.jpg" width="1003"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.26 – Running the first cell</p>
<p class="list-inset">As we<a id="_idIndexMarker556"/> have in <em class="italic">Figure 5.26</em>, we can find the first runnable cell under <strong class="bold">Inputs and Outputs</strong>. You may see a “<strong class="bold">Note: The kernel is still starting. Please execute this cell again after the kernel is started.</strong>” message while waiting for the kernel to start.</p>
<p class="callout-heading">Note</p>
<p class="callout">Wait for the kernel to start. This step may take around 3 to 5 minutes while an ML instance is being provisioned to run the Jupyter notebook cells. Once you are done with the hands-on solutions in this chapter, the ML instance used to run the Jupyter Notebook cells needs to be turned off immediately to avoid any additional charges. Click and locate the circle icon on the left-hand sidebar to show the list of running instances, apps, kernel sessions, and terminal sessions. Make sure to shut down all running instances under <strong class="bold">RUNNING INSTANCES</strong> whenever you are done using SageMaker Studio.</p>
<ol>
<li value="4">Once the kernel is ready, click on the cell containing the first code block under <strong class="bold">Inputs and Outputs</strong> (the same cell we tried running in the previous step). Open the <strong class="bold">Run</strong> menu and then select <strong class="bold">Run All Cells</strong> from the list of options. Scroll down to the first runnable cell under <strong class="bold">Job Status &amp; S3 Output Location</strong>. Wait<a id="_idIndexMarker557"/> for all the cells to finish running before proceeding with the next step. Note that the cells under <strong class="bold">(Optional) Next Steps</strong> would raise an error since the <strong class="source-inline">run_optional_steps</strong> variable is set to <strong class="source-inline">False</strong>.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are wondering what a SageMaker processing job is, it is a job that utilizes the managed infrastructure of AWS to run a script. This script is coded to perform a set of operations defined by the user (or creator of the script). You can check <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml</a> for more information on this topic.</p>
<p class="list-inset">It may take about 10 to 20 minutes to run all the cells in the <strong class="bold">Save to S3 with a SageMaker Processing Job</strong> Jupyter notebook. While waiting, let’s quickly check the different sections in the notebook:</p>
<ul>
<li><strong class="bold">Inputs and Outputs</strong> – where we specify the input and output configuration for the flow export</li>
<li><strong class="bold">Run Processing Job</strong> – where we configure and run a SageMaker processing job</li>
<li><strong class="bold">(Optional)Next Steps</strong> – where we can optionally load the processed data into pandas for further inspection and train a model with SageMaker</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">If you encounter an error with a message similar to <strong class="bold">ClientError: A error occurred (ValidationException) when calling the CreateProcessingJob operation: Input and Output names must be globally unique: [synthetic.bookings.dirty.parquet]</strong>, make sure that there are no duplicates in the <strong class="source-inline">input_name</strong> values of the <strong class="source-inline">ProcessingInput</strong> objects stored in the <strong class="source-inline">data_sources</strong> list (and we should only have a single <strong class="source-inline">ProcessingInput</strong> object in the list). If you encounter other unexpected errors, feel free to troubleshoot the Python code as needed.</p>
<ol>
<li value="5">Once <strong class="source-inline">SystemExit</strong> has been raised under <strong class="bold">(Optional)Next Steps</strong>, locate and scroll<a id="_idIndexMarker558"/> to the cell under <strong class="bold">Job Status &amp; S3 Output Location</strong> and copy the S3 path highlighted in <em class="italic">Figure 5.27</em> to a text editor (for example, VS Code) on your local machine:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 5.27 – Copying the S3 path where the job results are stored " height="677" src="image/B18638_05_027.jpg" width="978"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.27 – Copying the S3 path where the job results are stored</p>
<p class="list-inset">You should find the S3 path right after <strong class="bold">Job results are saved to S3 path:</strong>. Make sure to wait for <strong class="source-inline">SystemExit</strong> to be raised before proceeding.</p>
<p>Now that we have finished running the cells from the generated Jupyter notebook, you might be wondering, <em class="italic">what is the point of generating a Jupyter notebook in the first place</em>? Why not run the data flow steps directly without having to generate a script or a notebook? The answer to this is simple: these generated Jupyter notebooks are meant to serve as initial <a id="_idIndexMarker559"/>templates that can be customized to fit the requirements of the work that needs to be done. </p>
<p class="callout-heading">Note</p>
<p class="callout">Wait! Where’s the processed version of the dataset? In the next section, we will quickly turn off the instances automatically launched by SageMaker Studio to manage costs. After turning off the resources, we will proceed with downloading and checking the output CSV file saved in S3 in the <em class="italic">Verifying the results</em> section near the end of the chapter. </p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor125"/>Turning off the resources</h2>
<p>It is<a id="_idIndexMarker560"/> important to note that SageMaker Studio automatically launches an <strong class="source-inline">ml.m5.4xlarge</strong> instance (at the time of writing) whenever we use and access SageMaker Data Wrangler. In addition to this, another ML instance is provisioned when running one or more cells inside a Jupyter notebook. If we were to create and run ML experiments on a Jupyter notebook using an AWS Deep Learning Container similar to that in <em class="italic">Figure 5.28</em>, then an <strong class="source-inline">ml.g4dn.xlarge</strong> instance may be provisioned as well. These instances and resources need to be manually turned off and removed since these are not turned off automatically even during periods of inactivity:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 5.28 – A high-level view of how SageMaker Studio operates " height="439" src="image/B18638_05_028.jpg" width="1133"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.28 – A high-level view of how SageMaker Studio operates</p>
<p>Turning off these resources is crucial since we do not want to pay for the time when these resources are not being used. In the next set of steps, we will locate and turn off the running instances in SageMaker Studio:</p>
<ol>
<li value="1">Click the circle icon in the sidebar, as highlighted in <em class="italic">Figure 5.29</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 5.29 – Turning off the running instances " height="565" src="image/B18638_05_029.jpg" width="1276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.29 – Turning off the running instances</p>
<p class="list-inset">Clicking the circle icon should open and show the running instances, apps, and terminals in SageMaker Studio. </p>
<ol>
<li value="2">Turn off all <a id="_idIndexMarker561"/>running instances under <strong class="bold">RUNNING INSTANCES</strong> by clicking the <strong class="bold">Shut down</strong> button for each of the instances as highlighted in <em class="italic">Figure 5.29</em>. Clicking the <strong class="bold">Shut down</strong> button will open a pop-up window verifying the instance shutdown operation. Click the <strong class="bold">Shut down all</strong> button to proceed.</li>
</ol>
<p>Moving forward, you may want to install and use a JupyterLab extension that automatically turns off certain resources during periods of inactivity similar to the <strong class="bold">SageMaker Studio auto-shutdown extension</strong>. You can<a id="_idIndexMarker562"/> find the extension here: <a href="https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension">https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Even after installing the extension, it is still recommended to manually check and turn off the resources after using SageMaker Studio. Make sure to perform regular inspections and cleanup of resources as well.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor126"/>Verifying the results</h2>
<p>At this point, the processed <a id="_idIndexMarker563"/>version of the dataset should be stored in the destination S3 path you copied to a text editor in your local machine. In the next set of steps, we will download this into the AWS CloudShell environment and check whether the expected changes are reflected in the downloaded file:</p>
<ol>
<li value="1">In SageMaker Studio, open the <strong class="bold">File</strong> menu and select <strong class="bold">Log out</strong> from the list of options. This will redirect you back to the <strong class="bold">SageMaker Domain</strong> page.</li>
<li>Navigate to <strong class="bold">CloudShell</strong> by clicking the icon highlighted in <em class="italic">Figure 5.30</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 5.30 – Navigating to CloudShell " height="84" src="image/B18638_05_030.jpg" width="288"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.30 – Navigating to CloudShell</p>
<p class="list-inset">We can find this button in the upper-right-hand corner of the AWS Management Console. You may also use the search bar to navigate to the CloudShell console.</p>
<ol>
<li value="3">Once the terminal is ready, move all files in the CloudShell environment to the <strong class="source-inline">/tmp</strong> directory by running the following (after <strong class="bold">$</strong>):<pre class="source-code"><strong class="bold">mv * /tmp 2&gt;/dev/null</strong></pre></li>
<li>Use the <strong class="source-inline">aws s3 cp</strong> command to copy the generated CSV file stored in S3 to the CloudShell environment. Make sure to replace <strong class="source-inline">&lt;PASTE S3 URL&gt;</strong> with the S3 URL copied from the <strong class="bold">Save to S3 with a SageMaker Processing Job</strong> notebook to a text editor on your local machine:<pre class="source-code"><strong class="bold">S3_PATH=&lt;PASTE S3 URL&gt;</strong></pre><pre class="source-code"><strong class="bold">aws s3 cp $S3_PATH/ . --recursive</strong></pre></li>
<li>Recursively<a id="_idIndexMarker564"/> list the files and directories using the following command:<pre class="source-code"><strong class="bold">ls -R</strong></pre></li>
</ol>
<p class="list-inset">You should see a CSV file stored inside <strong class="source-inline">&lt;UUID&gt;/default</strong>. </p>
<ol>
<li value="6">Finally, use the <strong class="source-inline">head</strong> command to inspect the CSV file:<pre class="source-code"><strong class="bold">head */default/*.csv</strong></pre></li>
</ol>
<p class="list-inset">This should give us the first few lines of the CSV file, similar to what we have in <em class="italic">Figure 5.31</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 5.31 – Verifying the changes " height="214" src="image/B18638_05_031.jpg" width="912"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.31 – Verifying the changes</p>
<p class="list-inset">Here, we can see that the dataset has the new <strong class="source-inline">has_booking_changes</strong> column containing <strong class="source-inline">true</strong> and <strong class="source-inline">false</strong> values. You may inspect the CSV file further and verify that there are no more <strong class="source-inline">-1</strong> values under the <strong class="source-inline">children</strong> column. We will leave this to you as an exercise (that is, verifying that there are no more <strong class="source-inline">-1</strong> values under the <strong class="source-inline">children</strong> column of the CSV file).</p>
<p>Now that we have finished using both Amazon SageMaker Data Wrangler and AWS Glue DataBrew to process and analyze a sample dataset, you might be wondering when to use one of these tools over the other. Here are some general recommendations when deciding:</p>
<ul>
<li>If you are planning to use custom transforms using PySpark similar to those we performed in this chapter, then you may want to use Amazon SageMaker Data Wrangler.</li>
<li>If the source, connection, or file type format is not supported in SageMaker Data Wrangler (for example, Microsoft Excel workbook format or <strong class="source-inline">.xlsx</strong> files), then you may want to use AWS Glue Data Brew.</li>
<li>If you want to export the data processing workflow and automatically generate a Jupyter notebook, then you may want to use Amazon SageMaker Data Wrangler.</li>
<li>If the primary users of the tool have minimal coding experience and would prefer processing and analyzing the data without reading, customizing, or writing a single line of code, then AWS Glue Data Brew may be used instead of Amazon SageMaker Data Wrangler.</li>
</ul>
<p>Of course, these <a id="_idIndexMarker565"/>are just some of the guidelines you can use but the decision on which tool to use will ultimately depend on the context of the work that needs to be done, along with the limitations of the tools at the time that the decision needs to be made. Features and limitations change over time, so make sure to review as many angles as possible when deciding.</p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor127"/>Summary</h1>
<p>Data needs to be cleaned, analyzed, and prepared before it is used to train ML models. Since it takes time and effort to work on these types of requirements, it is recommended to use no-code or low-code solutions such as AWS Glue DataBrew and Amazon SageMaker Data Wrangler when analyzing and processing our data. In this chapter, we were able to use these two services to analyze and process our sample dataset. Starting with a sample “dirty” dataset, we performed a variety of transformations and operations, which included (1) profiling and analyzing the data, (2) filtering out rows containing invalid data, (3) creating a new column from an existing one, (4) exporting the results into an output location, and (5) verifying whether the transformations have been applied to the output file.</p>
<p>In the next chapter, we will take a closer look at Amazon SageMaker and we will dive deeper into how we can use this managed service when performing machine learning experiments.</p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor128"/>Further reading</h1>
<p>For more information on the topics covered in this chapter, feel free to check out the following resources:</p>
<ul>
<li><em class="italic">AWS Glue DataBrew product and service integrations</em> (<a href="https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml">https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml</a>)</li>
<li><em class="italic">Security in AWS Glue DataBrew</em> (<a href="https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml">https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml</a>)</li>
<li><em class="italic">Create and Use a Data Wrangler Flow</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml</a>)</li>
<li><em class="italic">Data Wrangler – Transform</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml</a>)</li>
<li><em class="italic">Data Wrangler – Troubleshooting</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml</a>)</li>
</ul>
</div>
<div>
<div id="_idContainer192">
</div>
</div>
</div>


<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer193">
</div>
</div>
<div class="Content" id="_idContainer194">
<h1 id="_idParaDest-122"><a id="_idTextAnchor129"/>Part 3: <a id="_idTextAnchor130"/>Diving Deeper with Relevant Model Training and Deployment Solutions</h1>
<p>In this section, readers will learn the relevant model training and deployment solutions using the different capabilities and features of Amazon SageMaker, along with other AWS services.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em></li>
<li><a href="B18638_07.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a>, <em class="italic">SageMaker Deployment Solutions</em></li>
</ul>
</div>
<div>
<div id="_idContainer195">
</div>
</div>
</div>
</body></html>