<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Transforming Images with Morphological Operations</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Eroding and dilating images using morphological filters</li><li class="listitem">Opening and closing images using morphological filters</li><li class="listitem">Detecting edges and corners using morphological filters</li><li class="listitem">Segmenting images using watersheds</li><li class="listitem">Extracting distinctive regions using MSER</li><li class="listitem">Extracting foreground objects with the GrabCut algorithm</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec34" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">
<strong class="calibre2">Mathematical morphology</strong><a id="id354" class="calibre1"/> is a theory that was developed in the 1960s for the analysis and processing of discrete images. It defines a series of operators that transform an image by probing it with a predefined shape element. The way this shape element intersects the neighborhood of a pixel determines the result of the operation. This chapter presents the most important morphological operators. It also explores the problems of image segmentation and feature detection using algorithms based on morphological operators.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec35" class="calibre1"/>Eroding and dilating images using morphological filters</h1></div></div></div><p class="calibre8">Erosion <a id="id355" class="calibre1"/>and<a id="id356" class="calibre1"/> dilation are the most fundamental <a id="id357" class="calibre1"/>morphological operators. Therefore, we<a id="id358" class="calibre1"/> will present these in the first recipe. The fundamental component in mathematical morphology is the<a id="id359" class="calibre1"/> <strong class="calibre2">structuring element</strong>. A structuring element can be simply defined as a configuration of pixels (the square shape in the following figure) on which an origin is defined (also called an <strong class="calibre2">anchor point</strong>). Applying a morphological filter consists of probing each pixel <a id="id360" class="calibre1"/>of the <a id="id361" class="calibre1"/>image using this structuring element. When the origin of the structuring element is aligned <a id="id362" class="calibre1"/>with a given pixel, its<a id="id363" class="calibre1"/> intersection with the image defines a set of pixels on which a particular morphological operation is applied (the nine shaded pixels in the following figure). In principle, the<a id="id364" class="calibre1"/> structuring element can be of any shape, but most often, a simple shape such as a square, circle, or diamond with the origin at the center is used (mainly for efficiency reasons), as shown in the following figure:</p><div><img src="img/00050.jpeg" alt="Eroding and dilating images using morphological filters" class="calibre10"/></div><p class="calibre11"> </p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec98" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">As morphological filters often work on binary images, we will use the binary image that was created through thresholding in the first recipe of the previous chapter. However, since the convention is to have the foreground objects represented by high (white) pixel values and the background objects by low (black) pixel values in morphology, we have negated the image.</p><p class="calibre8">In morphological terms, the following image is said to be the complement of the image that was created in the previous chapter:</p><div><img src="img/00051.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec99" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Erosion <a id="id365" class="calibre1"/>and dilation are <a id="id366" class="calibre1"/>implemented in <a id="id367" class="calibre1"/>OpenCV as simple<a id="id368" class="calibre1"/> functions, which are <code class="email">cv::erode</code> and <code class="email">cv::dilate</code>. Their <a id="id369" class="calibre1"/>usage is straightforward:</p><div><pre class="programlisting">   // Read input image
   cv::Mat image= cv::imread("binary.bmp");

   // Erode the image
   cv::Mat eroded;  // the destination image
   cv::erode(image,eroded,cv::Mat());


   // Dilate the image
   cv::Mat dilated;  // the destination image
   cv::dilate(image,dilated,cv::Mat());</pre></div><p class="calibre8">The two images <a id="id370" class="calibre1"/>produced by these function calls are seen in the following screenshots. The first screenshot shows erosion:</p><div><img src="img/00052.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The second screenshot shows the dilation result:</p><div><img src="img/00053.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec100" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">As with all the <a id="id371" class="calibre1"/>other morphological filters, the two filters of this recipe operate on the set of pixels (or the neighborhood) around<a id="id372" class="calibre1"/> each pixel as defined <a id="id373" class="calibre1"/>by the structuring element. Recall that when applied to a given pixel, the anchor point of the structuring element is aligned with this pixel location, and all the pixels that intersect the structuring element are included in the <a id="id374" class="calibre1"/>current set. <strong class="calibre2">Erosion</strong><a id="id375" class="calibre1"/> replaces the current pixel with the minimum pixel value found in the defined pixel set. <strong class="calibre2">Dilation</strong> <a id="id376" class="calibre1"/>is the complementary operator, and it replaces the current pixel with the maximum pixel value found in the defined pixel set. Since the input binary image contains only black (<code class="email">0</code>) and white (<code class="email">255</code>) pixels, each pixel is replaced by either a white or black pixel.</p><p class="calibre8">A good way to picturize the effect of these two operators is to think in terms of background (black) and foreground (white) objects. With erosion, if the structuring element when placed at a given pixel location touches the background (that is, one of the pixels in the intersecting set is black), then this pixel will be sent to the background. In the case of dilation, if the structuring element on a background pixel touches a foreground object, then this pixel will be assigned a white value. This explains why the size of the objects has been reduced (the shape has been eroded) in the eroded image. Note how some of <a id="id377" class="calibre1"/>the small objects (which can be considered as "noisy" background pixels) have also <a id="id378" class="calibre1"/>been completely eliminated. Similarly, the dilated objects are now larger, and some of the "holes" inside them have been filled. By default, OpenCV uses a 3 x 3 square structuring element. This default structuring element is obtained <a id="id379" class="calibre1"/>when an empty matrix (that is, <code class="email">cv::Mat()</code>) is specified as the third argument in the function call, as it was done in the preceding example. You <a id="id380" class="calibre1"/>can also specify a structuring element of the size (and shape) you want by providing a matrix in which the nonzero element defines the structuring element. In the following example, a 7 x 7 structuring element is applied:</p><div><pre class="programlisting">   cv::Mat element(7,7,CV_8U,cv::Scalar(1));
   cv::erode(image,eroded,element);</pre></div><p class="calibre8">The effect is much more destructive in this case, as shown in the following screenshot:</p><div><img src="img/00054.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Another way to obtain the same result is to repetitively apply the same structuring element on an image. The two functions have an optional parameter to specify the number of repetitions:</p><div><pre class="programlisting">   // Erode the image 3 times.
   cv::erode(image,eroded,cv::Mat(),cv::Point(-1,-1),3);</pre></div><p class="calibre8">The argument <code class="email">cv::Point(-1,-1)</code> means that the origin is at the center of the matrix (default); it can be defined anywhere on the structuring element. The image that is obtained will <a id="id381" class="calibre1"/>be identical to the image we obtained with <a id="id382" class="calibre1"/>the 7 x 7 structuring element. Indeed, eroding an image twice is similar to eroding an image with a structuring element dilated with itself. This also applies to dilation.</p><p class="calibre8">Finally, since<a id="id383" class="calibre1"/> the notion of background/foreground is arbitrary, we can make the following observation (which is a fundamental property <a id="id384" class="calibre1"/>of the erosion/dilation operators). Eroding the foreground objects with a structuring element can be seen as a dilation of the background part of the image. In other words, we can make the following observations:</p><div><ul class="itemizedlist"><li class="listitem">The erosion of an image is equivalent to the complement of the dilation of the complement image</li><li class="listitem">The dilation of an image is equivalent to the complement of the erosion of the complement image</li></ul></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec101" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Note that even though we applied our morphological filters on binary images here, these filters can be applied on gray-level or even color images with the same definitions.</p><p class="calibre8">Also, note that the OpenCV morphological functions support in-place processing. This means that you can use the input image as the destination image, as follows:</p><div><pre class="programlisting">   cv::erode(image,image,cv::Mat());</pre></div><p class="calibre8">OpenCV will create the required temporary image for you for this to work properly.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch05lvl2sec102" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Opening and closing images using morphological filters</em> recipe applies the erosion and dilation filters in cascade to produce new operators</li><li class="listitem">The <em class="calibre9">Detecting edges and corners using morphological filters</em> recipe applies morphological filters on gray-level images</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec36" class="calibre1"/>Opening and closing images using morphological filters</h1></div></div></div><p class="calibre8">The previous <a id="id385" class="calibre1"/>recipe introduced you to the <a id="id386" class="calibre1"/>two fundamental morphological operators: dilation and erosion. From these, other operators can be <a id="id387" class="calibre1"/>defined. The<a id="id388" class="calibre1"/> next two recipes will present some of them. The opening and closing operators are presented in this recipe.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec103" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">In order to apply higher-level morphological filters, you need to use the <code class="email">cv::morphologyEx</code> function<a id="id389" class="calibre1"/> with the appropriate function code. For example, the following call will apply the closing operator:</p><div><pre class="programlisting">   cv::Mat element5(5,5,CV_8U,cv::Scalar(1));
   cv::Mat closed;
   cv::morphologyEx(image,closed,cv::MORPH_CLOSE,element5);</pre></div><p class="calibre8">Note that we used a 5 x 5 structuring element to make the effect of the filter more apparent. If we use the binary image of the preceding recipe as input, we will obtain an image similar to what's shown in the following screenshot:</p><div><img src="img/00055.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Similarly, applying the morphological opening operator will result in the following screenshot:</p><div><img src="img/00056.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The preceding image is obtained from the following code:</p><div><pre class="programlisting">   cv::Mat opened;
   cv::morphologyEx(image,opened,cv::MORPH_OPEN,element5);</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec104" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The <a id="id390" class="calibre1"/>opening and closing filters are simply defined in <a id="id391" class="calibre1"/>terms of the basic erosion <a id="id392" class="calibre1"/>and dilation operations. <strong class="calibre2">Closing</strong><a id="id393" class="calibre1"/> is defined as the <a id="id394" class="calibre1"/>erosion of the dilation of an image. <strong class="calibre2">Opening</strong><a id="id395" class="calibre1"/> is defined as the dilation of the erosion of an image.</p><p class="calibre8">Consequently, one can compute the closing of an image using the following calls:</p><div><pre class="programlisting">   // dilate original image
   cv::dilate(image,result,cv::Mat());
   // in-place erosion of the dilated image
   cv::erode(result,result,cv::Mat()); </pre></div><p class="calibre8">The opening filter can be obtained by reverting these two function calls. While examining the result of the closing filter, it can be seen that the small holes of the white foreground objects have been filled. The <a id="id396" class="calibre1"/>filter also connects several adjacent objects together. Basically, any holes or gaps that are too small to completely contain the structuring element will be eliminated by the filter.</p><p class="calibre8">Reciprocally, the <a id="id397" class="calibre1"/>opening filter eliminated several small objects from the scene. All the objects that were too small to contain the structuring element have been removed.</p><p class="calibre8">These filters are <a id="id398" class="calibre1"/>often used in object detection. The closing filter connects the objects erroneously fragmented into smaller pieces together, while the <a id="id399" class="calibre1"/>opening filter removes the small blobs introduced by the image noise. Therefore, it is advantageous to use them in a sequence. If our test binary image is successively closed and opened, we obtain an image that shows only the main objects in the scene, as shown in the following screenshot. You can also apply the opening filter before the closing filter if you wish to prioritize noise filtering, but this will be at the price of eliminating some fragmented objects.</p><div><img src="img/00057.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that applying the same opening (and similarly the closing) operator on an image several times has no effect. Indeed, as the holes have been filled by the first opening filter an additional application of the same filter will not produce any other changes to the image. In mathematical terms, these operators are said to be idempotent.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec105" class="calibre1"/>See also</h2></div></div></div><p class="calibre8">The opening and closing operators are often used to clean up an image before extracting its connected components as explained in the <em class="calibre9">Extracting the components' contours</em> recipe of <a class="calibre1" title="Chapter 7. Extracting Lines, Contours, and Components" href="part0052_split_000.html#page">Chapter 7</a>, <em class="calibre9">Extracting Lines, Contours, and Components</em>.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec37" class="calibre1"/>Detecting edges and corners using morphological filters</h1></div></div></div><p class="calibre8">Morphological filters<a id="id400" class="calibre1"/> can also be used to <a id="id401" class="calibre1"/>detect specific <a id="id402" class="calibre1"/>features in an image. In this recipe, we<a id="id403" class="calibre1"/> will learn how to detect contours and corners in a gray-level image.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec106" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">In this recipe, the following image will be used:</p><div><img src="img/00058.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec107" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The edges <a id="id404" class="calibre1"/>of an <a id="id405" class="calibre1"/>image <a id="id406" class="calibre1"/>can be <a id="id407" class="calibre1"/>detected by using the appropriate filter of the <code class="email">cv::morphologyEx</code> function. Refer to the following code:</p><div><pre class="programlisting">// Get the gradient image using a 3x3 structuring element
cv::Mat result;
cv::morphologyEx(image,result,
                         cv::MORPH_GRADIENT,cv::Mat());

// Apply threshold to obtain a binary image
int threshold= 40;
cv::threshold(result, result, 
                    threshold, 255, cv::THRESH_BINARY);</pre></div><p class="calibre8">The following image is obtained as the result:</p><div><img src="img/00059.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In order to detect corners using morphology, we now define a class named <code class="email">MorphoFeatures</code> as follows:</p><div><pre class="programlisting">class MorphoFeatures {

  private:

     // threshold to produce binary image
    int threshold;
    // structuring elements used in corner detection
    cv::Mat_&lt;uchar&gt; cross;
    cv::Mat_&lt;uchar&gt; diamond;
    cv::Mat_&lt;uchar&gt; square;
    cv::Mat_&lt;uchar&gt; x;</pre></div><p class="calibre8">The detection <a id="id408" class="calibre1"/>of corners using morphological corners is a bit complex since it requires the successive application of several <a id="id409" class="calibre1"/>different <a id="id410" class="calibre1"/>morphological filters. This is a good example of <a id="id411" class="calibre1"/>the use of nonsquare structuring elements. Indeed, this requires four different structuring elements shaped as a square, diamond, cross, and X-shape to be defined in the constructor (all these structuring elements have a fixed 5 x 5 dimension for simplicity):</p><div><pre class="programlisting">MorphoFeatures() : threshold(-1), 
      cross(5, 5), diamond(5, 5), square(5, 5), x(5, 5) {
  
      // Creating the cross-shaped structuring element
      cross &lt;&lt;
        0, 0, 1, 0, 0,
        0, 0, 1, 0, 0,
        1, 1, 1, 1, 1,
        0, 0, 1, 0, 0,
        0, 0, 1, 0, 0;
      // Similarly creating the other elements</pre></div><p class="calibre8">In the detection of corner features, all these structuring elements are applied in a cascade to obtain the resulting corner map:</p><div><pre class="programlisting">cv::Mat getCorners(const cv::Mat &amp;image) {

   cv::Mat result;

   // Dilate with a cross   
   cv::dilate(image,result,cross);

   // Erode with a diamond
   cv::erode(result,result,diamond);

   cv::Mat result2;
   // Dilate with a X   
   cv::dilate(image,result2,x);

   // Erode with a square
   cv::erode(result2,result2,square);
   // Corners are obtained by differencing
   // the two closed images
   cv::absdiff(result2,result,result);

   // Apply threshold to obtain a binary image
   applyThreshold(result);

   return result;
}</pre></div><p class="calibre8">The <a id="id412" class="calibre1"/>corners<a id="id413" class="calibre1"/> are then detected on an image by using the following code:</p><div><pre class="programlisting">// Get the corners
cv::Mat corners;
corners= morpho.getCorners(image);

// Display the corner on the image
morpho.drawOnImage(corners,image);
cv::namedWindow("Corners on Image");
cv::imshow("Corners on Image",image);</pre></div><p class="calibre8">In the<a id="id414" class="calibre1"/> image, the <a id="id415" class="calibre1"/>detected corners are displayed as circles, as shown in the following screenshot:</p><div><img src="img/00060.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec108" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">A good way to <a id="id416" class="calibre1"/>understand the effect <a id="id417" class="calibre1"/>of morphological operators on a gray-level image is to consider an image as a topological relief in which<a id="id418" class="calibre1"/> the gray levels correspond to elevation (or altitude). Under this perspective, the bright regions correspond to <a id="id419" class="calibre1"/>mountains, while the dark areas correspond to the valleys of the terrain. Also, since edges correspond to a rapid transition between the dark and bright pixels, these can be pictured as abrupt cliffs. If an erosion operator is applied on such a terrain, the net result will be to replace each pixel by the lowest value in a certain neighborhood, thus reducing its height. As a result, cliffs will be "eroded" as the valleys expand. Dilation has the exact opposite effect; that is, cliffs will gain terrain over the valleys. However, in both cases, the plateaux (that is, the area of constant intensity) will remain relatively unchanged.</p><p class="calibre8">These observations lead to a simple way to detect the edges (or cliffs) of an image. This can be done by computing the difference between the dilated and eroded images. Since these two transformed images differ mostly at the edge locations, the image edges will be emphasized by the subtraction. This is exactly what the <code class="email">cv::morphologyEx</code> function does when the <code class="email">cv::MORPH_GRADIENT</code> argument is inputted. Obviously, the larger the structuring element is, the thicker the detected edges will be. This edge detection operator is also called the <a id="id420" class="calibre1"/>
<strong class="calibre2">Beucher</strong> gradient (the next chapter will discuss the concept of an image gradient in more detail). Note that similar results can also be obtained by simply subtracting the original image from the dilated one or the eroded image from the original. The resulting edges would be thinner.</p><p class="calibre8">Corner detection<a id="id421" class="calibre1"/> is a bit more complex since <a id="id422" class="calibre1"/>it uses four different structuring elements. This operator is not implemented in OpenCV, but we present it here to demonstrate how the structuring elements of various shapes can be defined and combined. The idea is to close the image by dilating and eroding it with two different structuring elements. These elements are chosen such that they leave straight edges unchanged, but because of their respective effects, the edges at corner points will be affected. Let's use the simple following image made of a single white square to better understand the effect of this asymmetrical closing operation:</p><div><img src="img/00061.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first square is the original image. When dilated with a cross-shaped structuring element, the square edges expand, except at the corner points where the cross shape does not hit the square. This<a id="id423" class="calibre1"/> is the result illustrated by the square in the middle. This dilated image is then eroded by a structuring element that <a id="id424" class="calibre1"/>has a diamond shape. This erosion brings back most edges to their original position but pushes the corners <a id="id425" class="calibre1"/>even further since they were <a id="id426" class="calibre1"/>not dilated. The rightmost square is then obtained, which (as it can be seen) has lost its sharp corners. The same procedure is repeated with the X-shaped <a id="id427" class="calibre1"/>and square-shaped <a id="id428" class="calibre1"/>structuring elements. These two elements are the rotated versions of the previous elements and will consequently capture the corners at a 45-degree orientation. Finally, differencing the two results will extract the corner features.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec109" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Applying directional filters to detect edges</em> recipe in <a class="calibre1" title="Chapter 6. Filtering the Images" href="part0047_split_000.html#page">Chapter 6</a>, <em class="calibre9">Filtering the Images</em> describes the other filters that perform edge detection</li><li class="listitem"><a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points</em>, presents different operators that perform corner detection</li><li class="listitem">The article, <em class="calibre9">The Morphological gradients</em>, <em class="calibre9">J.-F. Rivest, P. Soille, and S. Beucher, ISET's symposium on electronic imaging science and technology, SPIE, Feb. 1992</em>, discusses the concept of morphological gradients in more detail</li><li class="listitem">The article, <em class="calibre9">A modified regulated morphological corner detector</em>, <em class="calibre9">F.Y. Shih, C.-F. Chuang, and V. Gaddipati, Pattern Recognition Letters, volume 26, issue 7, May 2005</em>, gives more information on morphological corner detection</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec38" class="calibre1"/>Segmenting images using watersheds</h1></div></div></div><p class="calibre8">The watershed transformation<a id="id429" class="calibre1"/> is a popular image processing algorithm that is used to quickly segment an image into homogenous regions. It relies <a id="id430" class="calibre1"/>on the idea that when the image is seen as a topological relief, the homogeneous regions correspond to relatively flat basins delimited by steep edges. As a result of its simplicity, the original version of this algorithm tends to over-segment the image, which produces multiple small regions. This is why OpenCV proposes a variant of this algorithm that uses a set of predefined markers that guide the definition of the image segments.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec110" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The watershed segmentation is obtained through the use of the <code class="email">cv::watershed</code> function. The input for this function is a 32-bit signed integer-marker image in which each nonzero pixel represents a label. The idea is to mark some pixels of the image that are known to belong to a given region. From this initial labeling, the watershed algorithm will determine the regions to which the other pixels belong. In this recipe, we will first create the marker image as a <a id="id431" class="calibre1"/>gray-level image and then convert it into <a id="id432" class="calibre1"/>an image of integers. We have conveniently encapsulated this step into a <code class="email">WatershedSegmenter</code> class. Refer to the following code:</p><div><pre class="programlisting">class WatershedSegmenter {

  private:

     cv::Mat markers;

  public:

     void setMarkers(const cv::Mat&amp; markerImage) {

      // Convert to image of ints
      markerImage.convertTo(markers,CV_32S);
     }

     cv::Mat process(const cv::Mat &amp;image) {

      // Apply watershed
      cv::watershed(image,markers);

      return markers;
     }</pre></div><p class="calibre8">The way these markers are obtained depends on the application. For example, some preprocessing steps might have resulted in the identification of some pixels that belong to an object of interest. The watershed would then be used to delimitate the complete object from that initial detection. In this recipe, we will simply use the binary image used throughout this chapter in order to identify the animals of the corresponding original image (this is the image shown at the beginning of <a class="calibre1" title="Chapter 4. Counting the Pixels with Histograms" href="part0032_split_000.html#page">Chapter 4</a>, <em class="calibre9">Counting the Pixels with Histograms</em>). Therefore, from our binary image, we need to identify the pixels that belong to the foreground (the animals) and the pixels that belong to the background (mainly the grass). Here, we will mark the foreground pixels with the label <code class="email">255</code> and the background pixels with the label <code class="email">128</code> (this choice is totally arbitrary; any label number other than <code class="email">255</code> will work). The other pixels, that is, the ones for which the labeling is unknown are assigned the value <code class="email">0</code>.</p><p class="calibre8">As of now, the binary image includes too many white pixels that belong to the various parts of the image. We will then severely erode this image in order to retain only the pixels that belong to the important objects:</p><div><pre class="programlisting">   // Eliminate noise and smaller objects
   cv::Mat fg;
   cv::erode(binary,fg,cv::Mat(),cv::Point(-1,-1),4);</pre></div><p class="calibre8">The result is the following image:</p><div><img src="img/00062.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that a <a id="id433" class="calibre1"/>few pixels that belong to the background (forest) are still present. Let's keep them. Therefore, they will be considered to correspond to an <a id="id434" class="calibre1"/>object of interest. Similarly, we also select a few pixels of the background by a large dilation of the original binary image:</p><div><pre class="programlisting">   // Identify image pixels without objects
   cv::Mat bg;
   cv::dilate(binary,bg,cv::Mat(),cv::Point(-1,-1),4);
   cv::threshold(bg,bg,1,128,cv::THRESH_BINARY_INV);</pre></div><p class="calibre8">The resulting black pixels correspond to the background pixels. This is why the thresholding operation assigns the value <code class="email">128</code> to these pixels immediately after the dilation. The following image is obtained:</p><div><img src="img/00063.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">These images <a id="id435" class="calibre1"/>are combined to form the marker image as follows:</p><div><pre class="programlisting">   // Create markers image
   cv::Mat markers(binary.size(),CV_8U,cv::Scalar(0));
   markers= fg+bg;</pre></div><p class="calibre8">Note how we <a id="id436" class="calibre1"/>used the overloaded operator <code class="email">+</code> here in order to combine the images. The following image will be used as the input to the watershed algorithm:</p><div><img src="img/00064.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In this input image, the <a id="id437" class="calibre1"/>white areas belong, for sure, to the <a id="id438" class="calibre1"/>foreground objects, the gray areas are a part of the background, and the black areas have an unknown label. The segmentation is then obtained as follows:</p><div><pre class="programlisting">   // Create watershed segmentation object
   WatershedSegmenter segmenter;

   // Set markers and process
   segmenter.setMarkers(markers);
   segmenter.process(image);</pre></div><p class="calibre8">The marker image is then updated such that each zero pixel is assigned one of the input labels, while the pixels that belong to the found boundaries have a value <code class="email">-1</code>. The resulting image of the labels is as follows:</p><div><img src="img/00065.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The boundary image will be similar to the following screenshot:</p><div><img src="img/00066.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec111" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">As we did in the <a id="id439" class="calibre1"/>preceding recipes, we will use the topological map analogy in the description of the watershed algorithm. In order to create a watershed segmentation, the idea is to progressively flood the image starting at level 0. As the level of "water" progressively increases (to levels 1, 2, 3, and so on), catchment basins are formed. The size of these basins also gradually increases and, consequently, the water of two different basins will eventually merge. When this happens, a watershed is created in order to keep the two basins separate. Once the level of water has reached its maximal level, the sets of these created basins and watersheds form the watershed segmentation.</p><p class="calibre8">As expected, the <a id="id440" class="calibre1"/>flooding process initially creates many small individual basins. When all of these are merged, many watershed lines are created, which results in an over-segmented image. To overcome this problem, a modification to this algorithm has been proposed in which the flooding process starts from a predefined set of marked pixels. The basins created from these markers are labeled in accordance with the values assigned to the initial marks. When two basins having the same label merge, no watersheds are created, thus preventing over-segmentation. This is what happens when the <a id="id441" class="calibre1"/>
<code class="email">cv::watershed</code> function is called. The input marker image is updated to produce the final watershed segmentation. Users can input a marker image with any number of labels and pixels of unknown labeling left to value <code class="email">0</code>. The <a id="id442" class="calibre1"/>marker image is chosen to be an image of a 32-bit signed integer in order to be able to define more than <code class="email">255</code> labels. It also allows the <a id="id443" class="calibre1"/>special value, <code class="email">-1</code>, to be assigned to the pixels associated with a watershed. This is returned by the <code class="email">cv::watershed</code> function.</p><p class="calibre8">To facilitate the display of the result, we have introduced two special methods. The first method returns an image of the labels (with watersheds at value <code class="email">0</code>). This is easily done through thresholding, as follows:</p><div><pre class="programlisting">     // Return result in the form of an image
     cv::Mat getSegmentation() {
        
      cv::Mat tmp;
      // all segment with label higher than 255
      // will be assigned value 255
      markers.convertTo(tmp,CV_8U);

      return tmp;
     }</pre></div><p class="calibre8">Similarly, the second method returns an image in which the watershed lines are assigned the value <code class="email">0</code>, and the rest of the image is at <code class="email">255</code>. This time the <code class="email">cv::convertTo</code> method is used to achieve this result, as follows:</p><div><pre class="programlisting">     // Return watershed in the form of an image
     cv::Mat getWatersheds() {
   
      cv::Mat tmp;
      // Each pixel p is transformed into
      // 255p+255 before conversion
      markers.convertTo(tmp,CV_8U,255,255);

      return tmp;
     }</pre></div><p class="calibre8">The linear transformation that is applied before the conversion allows the <code class="email">-1</code> pixels to be converted into <code class="email">0</code> (since <em class="calibre9">-1*255+255=0</em>).</p><p class="calibre8">Pixels with a value greater than <code class="email">255</code> are assigned the value <code class="email">255</code>. This is due to the saturation operation that is applied when signed integers are converted into unsigned characters.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec112" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Obviously, the marker image can be obtained in many different ways. For example, users can be interactively<a id="id444" class="calibre1"/> asked to paint areas on the objects and the <a id="id445" class="calibre1"/>background of a scene. Alternatively, in an attempt to identify an object located at the center of an image, one can also simply input an image with the central area marked with a certain label and the border of the image (where the background is assumed to be present) marked with another label. This marker image can be created as follows:</p><div><pre class="programlisting">// Identify background pixels
cv::Mat imageMask(image.size(),CV_8U,cv::Scalar(0));
cv::rectangle(imageMask,cv::Point(5,5),
     cv::Point(image.cols-5,
               image.rows-5),cv::Scalar(255),3);
// Identify foreground pixels 
// (in the middle of the image)
cv::rectangle(imageMask,
     cv::Point(image.cols/2-10,image.rows/2-10),
     cv::Point(image.cols/2+10,image.rows/2+10),
     cv::Scalar(1),10);</pre></div><p class="calibre8">If we superimpose this marker image on a test image, we will obtain the following image:</p><div><img src="img/00067.jpeg" alt="There's more..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The following is the resulting watershed image:</p><div><img src="img/00068.jpeg" alt="There's more..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec113" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article, <em class="calibre9">The viscous watershed transform</em>, <em class="calibre9">C. Vachier and F. Meyer, Journal of Mathematical Imaging and Vision, volume 22, issue 2-3, May 2005</em>, gives more information on the watershed transform</li><li class="listitem">The last recipe of this chapter, <em class="calibre9">Extracting foreground objects with the GrabCut algorithm</em>, presents another image segmentation algorithm that can also segment an image into background and foreground objects</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec39" class="calibre1"/>Extracting distinctive regions using MSER</h1></div></div></div><p class="calibre8">In the <a id="id446" class="calibre1"/>previous recipe, you learned how an image <a id="id447" class="calibre1"/>can be segmented into regions by gradually flooding it and creating watersheds. The <strong class="calibre2">maximally stable extremal regions</strong> (<strong class="calibre2">MSER</strong>) algorithm uses the same immersion analogy in order to extract meaningful regions in an image. These regions will also be created by flooding the image level by level, but this time, we will be interested in the basins that remain relatively stable for a period of time during the immersion process. It will be observed that these regions correspond to some distinctive parts of the scene objects pictured in the image.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec114" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The basic <a id="id448" class="calibre1"/>class to compute the MSER of an image is <a id="id449" class="calibre1"/>
<code class="email">cv::MSER</code>. An <a id="id450" class="calibre1"/>instance of this class can be created by using the default empty constructor. In our case, we chose to initialize it by specifying a minimum and maximum size for the detected regions in order to limit their number. Then, our call will be as follows:</p><div><pre class="programlisting">// basic MSER detector
cv::MSER mser(5, // delta value for extremal region detection
            200, // min acceptable area 
          1500); // max acceptable area</pre></div><p class="calibre8">Now, the MSER can be obtained by a call to a functor, specifying the input image and an appropriate output data structure, as follows:</p><div><pre class="programlisting">  // vector of point sets
  std::vector&lt;std::vector&lt;cv::Point&gt;&gt; points;
  // detect MSER features
  mser(image, points);</pre></div><p class="calibre8">The result is a vector of regions represented by the pixel points that compose each of them. In order to visualize the results, we create a blank image on which we will display the detected regions in different colors (which are randomly chosen). This is done as follows:</p><div><pre class="programlisting">  // create white image
  cv::Mat output(image.size(),CV_8UC3);
  output= cv::Scalar(255,255,255);
  
  // random number generator
  cv::RNG rng;

  // for each detected feature
  for (std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::
            iterator it= points.begin();
         it!= points.end(); ++it) {

    // generate a random color
    cv::Vec3b c(rng.uniform(0,255),
               rng.uniform(0,255),
               rng.uniform(0,255));

    // for each point in MSER set
    for (std::vector&lt;cv::Point&gt;::
                 iterator itPts= it-&gt;begin();
              itPts!= it-&gt;end(); ++itPts) {

      //do not overwrite MSER pixels
      if (output.at&lt;cv::Vec3b&gt;(*itPts)[0]==255) {

        output.at&lt;cv::Vec3b&gt;(*itPts)= c;
      }
    }
  }</pre></div><p class="calibre8">Note that<a id="id451" class="calibre1"/> the MSER form a hierarchy of regions. Therefore, to <a id="id452" class="calibre1"/>make all of these visible, we have chosen to not overwrite the small regions when they are included in larger ones. If the MSER are detected on the following image:</p><div><img src="img/00069.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Then, the resulting image will be (refer to the book's graphics PDF to view this image in color) as follows:</p><div><img src="img/00070.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">These are the<a id="id453" class="calibre1"/> raw results of the detection. Nevertheless, it<a id="id454" class="calibre1"/> can be observed how this operator has been able to extract some meaningful regions (for example, the building's windows) from this image.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec115" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">MSER uses the same mechanism as the watershed algorithm; that is, it proceeds by gradually flooding the image from level <code class="email">0</code> to level <code class="email">255</code>. As the level of water increases, you can observe that the sharply delimitated darker areas form the basins that have a relatively stable shape for a period of time (recall that under the immersion analogy, the water levels correspond to the intensity levels). These stable basins are the MSER. These are detected by considering the connected regions at each level and measuring their stability. This is done by comparing the current area of a region with the area it previously had when the level was down by a value of delta. When this relative variation reaches a local minimum, the region is identified as a MSER. The delta value that is used to measure the relative stability is the first parameter in the constructor of the <code class="email">cv::MSER</code> class; its default value is <code class="email">5</code>. In addition, to be considered, the size of a region must be within a certain predefined range. The acceptable minimum and maximum region sizes are the next two parameters of the constructor. We must also ensure that the MSER is stable (the fourth parameter), that is, the relative variation of its shape is small enough. The stable regions can be included in the larger regions (called parent regions).</p><p class="calibre8">To be valid, a parent MSER must be sufficiently different from its child; this is the diversity criterion, and it is specified by the fifth parameter of the <code class="email">cv::MSER</code> constructor. In the example used in the previous section, the default value for these last two parameters were used. (The default values are <code class="email">0.25</code> for the maximum allowable variation of a MSER and <code class="email">0.2</code> for the minimum diversity of a parent MSER.)</p><p class="calibre8">The output <a id="id455" class="calibre1"/>of the MSER detector is a vector of point sets. Since we are generally more interested in a region as a whole rather than its individual <a id="id456" class="calibre1"/>pixel locations, it is common to represent a MSER by a simple geometrical shape that describes the MSER location and size. A bounding ellipse is a commonly used representation. In order to obtain these ellipses, we will make use of two convenient OpenCV functions. The first is the <code class="email">cv::minAreaRect</code> function<a id="id457" class="calibre1"/> that finds the rectangle of minimum area that binds all the points in a set. This rectangle is described by a <code class="email">cv::RotatedRect</code> instance. Once this bounding rectangle is found, it is possible to draw the inscribed ellipse on the image by using the<a id="id458" class="calibre1"/> <code class="email">cv::ellipse</code> function. Let's encapsulate this complete process in one class. The constructor of this class basically repeats the one of the <code class="email">cv::MSER</code> class. Refer to the following code:</p><div><pre class="programlisting">class MSERFeatures {

  private:

    cv::MSER mser;        // mser detector
    double minAreaRatio;  // extra rejection parameter

  public:

    MSERFeatures(
           // aceptable size range
           int minArea=60, int maxArea=14400,
           // min value for MSER area/bounding-rect area 
           double minAreaRatio=0.5,
           // delta value used for stability measure 
           int delta=5, 
           // max allowed area variation
           double maxVariation=0.25, 
           // min size increase between child and parent
           double minDiversity=0.2):
           mser(delta,minArea,maxArea,
           maxVariation,minDiversity), 
           minAreaRatio(minAreaRatio) {}</pre></div><p class="calibre8">One extra parameter (<code class="email">minAreaRatio</code>) has been added to eliminate the MSER for which the bounding rectangle has an area that differs greatly from the one of the MSER it represents. This is to remove the less interesting elongated shapes.</p><p class="calibre8">The list of representative bounding rectangles is computed by the following method:</p><div><pre class="programlisting">// get the rotated bounding rectangles 
// corresponding to each MSER feature
// if (mser area / bounding rect area) &lt; areaRatio, 
// the feature is rejected
void getBoundingRects(const cv::Mat &amp;image, 
                      std::vector&lt;cv::RotatedRect&gt; &amp;rects) {

  // detect MSER features
  std::vector&lt;std::vector&lt;cv::Point&gt;&gt; points;
  mser(image, points);

  // for each detected feature
  for (std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::
            iterator it= points.begin();
         it!= points.end(); ++it) {
           
        // Extract bouding rectangles
        cv::RotatedRect rr= cv::minAreaRect(*it);
    
        // check area ratio
        if (it-&gt;size() &gt; minAreaRatio*rr.size.area())
        rects.push_back(rr);
  }
}</pre></div><p class="calibre8">The<a id="id459" class="calibre1"/> corresponding <a id="id460" class="calibre1"/>ellipses are drawn on the image using the following method:</p><div><pre class="programlisting">// draw the rotated ellipses corresponding to each MSER
cv::Mat getImageOfEllipses(const cv::Mat &amp;image,
            std::vector&lt;cv::RotatedRect&gt; &amp;rects, 
            cv::Scalar color=255) {

  // image on which to draw
  cv::Mat output= image.clone();

  // get the MSER features
  getBoundingRects(image, rects);

  // for each detected feature
  for (std::vector&lt;cv::RotatedRect&gt;::
            iterator it= rects.begin();
         it!= rects.end(); ++it) {
        cv::ellipse(output,*it,color);
  }

  return output;
}</pre></div><p class="calibre8">The <a id="id461" class="calibre1"/>detection <a id="id462" class="calibre1"/>of the MSER is then obtained as follows:</p><div><pre class="programlisting">  // create MSER feature detector instance
  MSERFeatures mserF(200, // min area 
                    1500, // max area
                    0.5); // ratio area threshold
                          // default delta is used

  // the vector of bounding rotated rectangles
  std::vector&lt;cv::RotatedRect&gt; rects;

  // detect and get the image
  cv::Mat result= mserF.getImageOfEllipses(image,rects);</pre></div><p class="calibre8">By applying this function to the previously used image, we will get the following image:</p><div><img src="img/00071.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Comparing this <a id="id463" class="calibre1"/>result with the previous result should convince you that this later representation is easier to interpret. Note how the child <a id="id464" class="calibre1"/>and parent MSER are often represented by very similar ellipses. In some cases, it would then be interesting to apply a minimum variation criterion on these ellipses in order to eliminate these repeated representations.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec116" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Computing components' shape descriptors</em> recipe in <a class="calibre1" title="Chapter 7. Extracting Lines, Contours, and Components" href="part0052_split_000.html#page">Chapter 7</a>, <em class="calibre9">Extracting Lines, Contours, and Components</em> will show you how to compute other properties of connected point sets</li><li class="listitem"><a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points</em>, will explain how to use MSER as an interest point detector</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec40" class="calibre1"/>Extracting foreground objects with the GrabCut algorithm</h1></div></div></div><p class="calibre8">OpenCV proposes the<a id="id465" class="calibre1"/> implementation <a id="id466" class="calibre1"/>of another popular algorithm for image segmentation: the <a id="id467" class="calibre1"/>
<strong class="calibre2">GrabCut</strong> algorithm. This algorithm is not based on mathematical morphology, but we have presented it here since it shows some similarities in its use with the watershed segmentation <a id="id468" class="calibre1"/>algorithm <a id="id469" class="calibre1"/>presented earlier in this chapter. GrabCut is computationally more expensive than watershed, but it generally produces more accurate results. It is the best algorithm to use when you want to extract a foreground object in a still image (for example, to cut and paste an object from one picture to another).</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec117" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The <code class="email">cv::grabCut</code> function<a id="id470" class="calibre1"/> is easy to use. You just need to input an image, and label some of its pixels as belonging to the background or to the foreground. Based on this partial labeling, the algorithm will then determine a foreground/background segmentation for the complete image.</p><p class="calibre8">One way to specify a partial foreground/background labeling for an input image is by defining a rectangle inside which the foreground object is included:</p><div><pre class="programlisting">   // define bounding rectangle
   // the pixels outside this rectangle
   // will be labeled as background 
   cv::Rect rectangle(5,70,260,120);</pre></div><div><img src="img/00072.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">All the pixels <a id="id471" class="calibre1"/>outside this rectangle will then be marked as the background. In addition to the input image and its<a id="id472" class="calibre1"/> segmentation image, calling the <code class="email">cv::grabCut</code> function requires the definition of two matrices, which will contain the models built by the algorithm as follows:</p><div><pre class="programlisting">   cv::Mat result; // segmentation (4 possible values)
   cv::Mat bgModel,fgModel; // the models (internally used)
   // GrabCut segmentation
   cv::grabCut(image,    // input image
            result,      // segmentation result
            rectangle,   // rectangle containing foreground 
            bgModel,fgModel, // models
            5,           // number of iterations
            cv::GC_INIT_WITH_RECT); // use rectangle</pre></div><p class="calibre8">Note how we specified that we are using the bounding rectangle mode using the <code class="email">cv::GC_INIT_WITH_RECT</code> flag as the last argument of the function (the next section will discuss the other available mode). The input/output segmentation image can have one of the following four values:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">cv::GC_BGD</code>: This<a id="id473" class="calibre1"/> is the value for the pixels that certainly belong to the background (for example, pixels outside the rectangle in our example)</li><li class="listitem"><code class="email">cv::GC_FGD</code>: This <a id="id474" class="calibre1"/>is the value for the pixels that certainly belong to the foreground (there are none in our example)</li><li class="listitem"><code class="email">cv::GC_PR_BGD</code>: This<a id="id475" class="calibre1"/> is the value for the pixels that probably belong to the background</li><li class="listitem"><code class="email">cv::GC_PR_FGD</code>: This <a id="id476" class="calibre1"/>is the value for the pixels that probably belong to the foreground (that is, the initial value for the pixels inside the rectangle in our example)</li></ul></div><p class="calibre8">We get a <a id="id477" class="calibre1"/>binary image of the segmentation by extracting the pixels that have a value equal to <code class="email">cv::GC_PR_FGD</code>. Refer to the following code:</p><div><pre class="programlisting">   // Get the pixels marked as likely foreground
   cv::compare(result,cv::GC_PR_FGD,result,cv::CMP_EQ);
   // Generate output image
   cv::Mat foreground(image.size(),CV_8UC3,
                      cv::Scalar(255,255,255));
   image.copyTo(foreground,// bg pixels are not copied
                result);</pre></div><p class="calibre8">To extract all the foreground pixels, that is, with values equal to <code class="email">cv::GC_PR_FGD</code> or <code class="email">cv::GC_FGD</code>, it is possible to check the value of the first bit, as follows:</p><div><pre class="programlisting">   // checking first bit with bitwise-and
   result= result&amp;1; // will be 1 if FG </pre></div><p class="calibre8">This is possible because these constants are defined as values 1 and 3, while the other two (<code class="email">cv::GC_BGD</code> and <code class="email">cv::GC_PR_BGD</code>) are defined as 0 and 2. In our example, the same result is obtained because the segmentation image does not contain the <code class="email">cv::GC_FGD</code> pixels (only the <code class="email">cv::GC_BGD</code> pixels have been inputted).</p><p class="calibre8">Finally, we <a id="id478" class="calibre1"/>obtain an image of the foreground objects (over a white background) by the following copy operation with a mask:</p><div><pre class="programlisting">   // Generate output image
   cv::Mat foreground(image.size(),CV_8UC3,
             cv::Scalar(255,255,255)); // all white image
   image.copyTo(foreground,result); // bg pixels not copied</pre></div><p class="calibre8">The following image is obtained as the result:</p><div><img src="img/00073.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec118" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">In the preceding example, the GrabCut algorithm was able to extract the foreground objects by simply specifying a rectangle inside which these objects (the four animals) were contained. Alternatively, one <a id="id479" class="calibre1"/>could also assign the values <code class="email">cv::GC_BGD</code> and <code class="email">cv::GC_FGD</code> to some specific pixels of the segmentation image, which are provided as the second argument of the <code class="email">cv::grabCut</code> function. You would then specify <code class="email">GC_INIT_WITH_MASK</code> as the input mode flag. These input labels could be obtained, for example, by asking a user to interactively mark a few elements of the image. It is also possible to combine these two input modes.</p><p class="calibre8">Using this<a id="id480" class="calibre1"/> input information, the GrabCut algorithm creates the background/foreground segmentation by proceeding as follows. Initially, a foreground label (<code class="email">cv::GC_PR_FGD</code>) is tentatively assigned to all the unmarked pixels. Based on the current classification, the algorithm groups the pixels into clusters of similar colors (that is, <code class="email">K</code> clusters for the background and <code class="email">K</code> clusters for the foreground). The next step is to determine a background/foreground segmentation by introducing boundaries between the foreground and background pixels. This is done through an optimization process that tries to connect pixels with similar labels, and that imposes a penalty for placing a boundary in the regions of relatively uniform intensity. This optimization problem can be efficiently solved using the <a id="id481" class="calibre1"/>
<strong class="calibre2">Graph Cuts</strong> algorithm, a method that can find the optimal solution of a problem by representing it as a connected graph <a id="id482" class="calibre1"/>on which cuts are applied in order to compose an optimal configuration. The obtained segmentation <a id="id483" class="calibre1"/>produces new labels for the pixels. The clustering process can then be repeated, and a new optimal segmentation is found again, and so on. Therefore, the GrabCut algorithm is an iterative procedure that gradually improves the segmentation result. Depending on the complexity of the scene, a good solution can be found in more or less number of iterations (in easy cases, one iteration would be enough).</p><p class="calibre8">This explains the argument of the function where the user can specify the number of iterations to be applied. The two internal models maintained by the algorithm are passed as an argument of the function (and returned). Therefore, it is possible to call the function with the models of the last run again if one wishes to improve the segmentation result by performing additional iterations.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec119" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article, <em class="calibre9">GrabCut: Interactive Foreground Extraction using Iterated Graph Cuts in ACM Transactions on Graphics (SIGGRAPH) volume 23, issue 3, August 2004,</em> <em class="calibre9">C. Rother, V. Kolmogorov, and A. Blake</em> describes the GrabCut algorithm in detail.</li></ul></div></div></div></body></html>