- en: Chapter 5. Tracking Objects in Videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Object tracking is one of the most important applications of computer vision.
    It can be used for many applications, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human–computer interaction: We might want to track the position of a person''s
    finger and use its motion to control the cursor on our machines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Surveillance: Street cameras can capture pedestrians'' motions that can be
    tracked to detect suspicious activities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video stabilization and compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Statistics in sports: By tracking a player''s movement in a game of football,
    we can provide statistics such as distance travelled, heat maps, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Optical flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image Pyramids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global Motion Estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KLT tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optical flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optical flow is an algorithm that detects the pattern of the motion of objects,
    or edges, between consecutive frames in a video. This motion may be caused by
    the motion of the object or the motion of the camera. Optical flow is a vector
    that depicts the motion of a point from the first frame to the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optical flow algorithm works under two basic assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: The pixel intensities are almost constant between consecutive frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neighboring pixels have the same motion as the anchor pixel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can represent the intensity of a pixel in any frame by *f(x,y,t)*. Here,
    the parameter *t* represents the frame in a video. Let''s assume that, in the
    next *dt* time, the pixel moves by *(dx,dy)*. Since we have assumed that the intensity
    doesn''t change in consecutive frames, we can say:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x,y,t) = f(x + dx,y + dy,t + dt)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we take the Taylor series expansion of the RHS in the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow](img/B02052_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Cancelling the common term, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow](img/B02052_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![Optical flow](img/B02052_05_12.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dividing both sides of the equation by *dt* we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow](img/B02052_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation is called the optical flow equation. Rearranging the equation
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow](img/B02052_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that this represents the equation of a line in the *(u,v)* plane.
    However, with only one equation available and two unknowns, this problem is under
    constraint at the moment. Two of the most widely used methods to calculate the
    optical flow are explained in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: The Horn and Schunck method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By taking into account our assumptions, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Horn and Schunck method](img/B02052_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can say that the first term will be small due to our assumption that the
    brightness is constant between consecutive frames. So, the square of this term
    will be even smaller. The second term corresponds to the assumption that the neighboring
    pixels have similar motion to the anchor pixel. We need to minimize the preceding
    equation. For this, we differentiate the preceding equation with respect to *u*
    and *v*. We get the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Horn and Schunck method](img/B02052_05_16.jpg)![The Horn and Schunck
    method](img/B02052_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![The Horn and Schunck method](img/B02052_05_18.jpg) and ![The Horn and
    Schunck method](img/B02052_05_19.jpg) are the Laplacians of *u* and *v* respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The Lucas and Kanade method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start off with the optical flow equation that we derived earlier and noticed
    that it is under constrained as it has one equation and two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To overcome this problem, we make use of the assumption that pixels in a 3x3
    neighborhood have the same optical flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite these equations in the form of matrices, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be rewritten in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, *A* is a 9x2 matrix, *U* is a 2x1 matrix, and *b* is a 9x1 matrix.
    Ideally, to solve for *U*, we just need to multiply by ![The Lucas and Kanade
    method](img/B02052_05_24.jpg) on both sides of the equation. However, this is
    not possible, as we can only take the inverse of square matrices. Thus, we try
    to transform *A* into a square matrix by first multiplying the equation by ![The
    Lucas and Kanade method](img/B02052_05_25.jpg) on both sides of the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now ![The Lucas and Kanade method](img/B02052_05_27.jpg) is a square matrix
    of dimension 2x2\. Hence, we can take its inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On solving this equation, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This method of multiplying the transpose and then taking an inverse is called
    **pseudo-inverse**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation can also be obtained by finding the minimum of the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'According to the optical flow equation and our assumptions, this value should
    be equal to zero. Since the neighborhood pixels do not have exactly the same values
    as the anchor pixel, this value is very small. This method is called **Least Square
    Error**. To solve for the minimum, we differentiate this equation with respect
    to *u* and *v*, and equate it to zero. We get the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_31.jpg)![The Lucas and Kanade
    method](img/B02052_05_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have two equations and two variables, so this system of equations can
    be solved. We rewrite the preceding equations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_33.jpg)![The Lucas and Kanade
    method](img/B02052_05_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, by arranging these equations in the form of a matrix, we get the same equation
    as obtained earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since, the matrix *A* is now a 2x2 matrix, it is possible to take an inverse.
    On taking the inverse, the equation obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be simplified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Solving for *u* and *v*, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas and Kanade method](img/B02052_05_38.jpg)![The Lucas and Kanade
    method](img/B02052_05_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we have the values for all the ![The Lucas and Kanade method](img/B02052_05_40.jpg),
    ![The Lucas and Kanade method](img/B02052_05_41.jpg), and ![The Lucas and Kanade
    method](img/B02052_05_42.jpg). Thus, we can find the values of *u* and *v* for
    each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: When we implement this algorithm, it is observed that the optical flow is not
    very smooth near the edges of the objects. This is due to the brightness constraint
    not being satisfied. To overcome this situation, we use **image pyramids** (explained
    in detail in the following sections).
  prefs: []
  type: TYPE_NORMAL
- en: Checking out the optical flow on Android
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see the optical flow in action on Android, we will create a grid of points
    over a video feed from the camera, and then the lines will be drawn for each point
    that will depict the motion of the point on the video, which is superimposed by
    the point on the grid.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin, we will set up our project to use OpenCV and obtain the feed
    from the camera. We will process the frames to calculate the optical flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a new project in Android Studio, in the same way as we did in
    the previous chapters. We will set the activity name to `MainActivity.java` and
    the XML resource file as `activity_main.xml`. Second, we will give the app the
    permissions to access the camera. In the `AndroidManifest.xml` file, add the following
    lines to the manifest tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that your activity tag for `MainActivity` contains the following
    line as an attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `activity_main.xml` file will contain a simple `JavaCameraView`. This is
    a custom OpenCV defined layout that enables us to access the camera frames and
    processes them as normal `Mat` objects. The XML code has been shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s work on some Java code. First, we''ll define some global variables
    that we will use later in the code or for other sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to create a callback function for OpenCV, like we did earlier.
    In addition to the code we used earlier, we will also enable `CameraView` to capture
    frames for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now check whether the OpenCV manager is installed on the phone, which
    contains the required libraries. In the `onResume` function, add the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `onCreate()` function, add the following line before calling `setContentView`
    to prevent the screen from turning off, while using the app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now initialize our `JavaCameraView` object. Add the following lines
    after `setContentView` has been called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we called `setCvCameraViewListener` with the `this` parameter.
    For this, we need to make our activity implement the `CvCameraViewListener2` interface.
    So, your class definition for the `MainActivity` class should look like the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add a menu to this activity to toggle between different examples in
    this chapter. Add the following lines to the `onCreateOptionsMenu` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now add some actions to the menu items. In the `onOptionsItemSelected`
    function, add the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a `resetVars` function to reset all the `Mat` objects. It has been
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also add the code to make sure that the camera is released for use
    by other applications, whenever our application is suspended or killed. So, add
    the following snippet of code to the `onPause` and `onDestroy` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After the OpenCV camera has been started, the `onCameraViewStarted` function
    is called, which is where we will add all our object initializations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the `onCameraViewStopped` function is called when we stop capturing
    frames. Here we will release all the objects we created when the view was started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will add the implementation to process each frame of the feed that we
    captured from the camera. OpenCV calls the `onCameraFrame` method for each frame,
    with the frame as a parameter. We will use this to process each frame. We will
    use the `viewMode` variable to distinguish between the optical flow and the KLT
    tracker, and have different case constructs for the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `gray()`function to obtain the Mat object that contains the
    captured frame in a grayscale format. OpenCV also provides a similar function
    called `rgba()` to obtain a colored frame. Then we will check whether this is
    the first run. If this is the first run, we will create and fill up a `features`
    array that stores the position of all the points in a grid, where we will compute
    the optical flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mPrevGray` object refers to the previous frame in a grayscale format.
    We copied the points to a `prevFeatures` object that we will use to calculate
    the optical flow and store the corresponding points in the next frame in `nextFeatures`.
    All of the computation is carried out in the `calcOpticalFlowPyrLK` OpenCV defined
    function. This function takes in the grayscale version of the previous frame,
    the current grayscale frame, an object that contains the feature points whose
    optical flow needs to be calculated, and an object that will store the position
    of the corresponding points in the current frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the position of the grid of points and their position in the next
    frame as well. So, we will now draw a line that depicts the motion of each point
    on the grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Before the loop ends, we have to copy the current frame to `mPrevGray` so that
    we can calculate the optical flow in the subsequent frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After we end the switch case construct, we will return a Mat object. This is
    the image that will be displayed as an output to the user of the application.
    Here, since all our operations and processing were performed on the grayscale
    image, we will return this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this is all about optical flow. The result can be seen in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking out the optical flow on Android](img/B02052_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Optical flow at various points in the camera feed
  prefs: []
  type: TYPE_NORMAL
- en: Image pyramids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pyramids are multiple copies of the same images that differ in their sizes.
    They are represented as layers, as shown in the following figure. Each level in
    the pyramid is obtained by reducing the rows and columns by half. Thus, effectively,
    we make the image''s size one quarter of its original size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image pyramids](img/B02052_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Relative sizes of pyramids
  prefs: []
  type: TYPE_NORMAL
- en: Pyramids intrinsically define **reduce** and **expand** as their two operations.
    Reduce refers to a reduction in the image's size, whereas expand refers to an
    increase in its size.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use a convention that lower levels in a pyramid mean downsized images
    and higher levels mean upsized images.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian pyramids
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the reduce operation, the equation that we use to successively find levels
    in pyramids, while using a 5x5 sliding window, has been written as follows. Notice
    that the size of the image reduces to a quarter of its original size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian pyramids](img/B02052_05_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The elements of the weight kernel, *w*, should add up to 1\. We use a 5x5 Gaussian
    kernel for this task. This operation is similar to convolution with the exception
    that the resulting image doesn''t have the same size as the original image. The
    following image shows you the reduce operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian pyramids](img/B02052_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The reduce operation
  prefs: []
  type: TYPE_NORMAL
- en: 'The expand operation is the reverse process of reduce. We try to generate images
    of a higher size from images that belong to lower layers. Thus, the resulting
    image is blurred and is of a lower resolution. The equation we use to perform
    expansion is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian pyramids](img/B02052_05_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight kernel in this case, *w*, is the same as the one used to perform
    the reduce operation. The following image shows you the expand operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gaussian pyramids](img/B02052_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The expand operation
  prefs: []
  type: TYPE_NORMAL
- en: The weights are calculated using the Gaussian function that we used in [Chapter
    1](ch01.html "Chapter 1. Applying Effects to Images"), *Applying Effects to Images*,
    to perform Gaussian blur.
  prefs: []
  type: TYPE_NORMAL
- en: Laplacian pyramids
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Laplacian pyramids are images that generally represent the edges. They are
    obtained from Gaussian pyramids. They are calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplacian pyramids](img/B02052_05_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*g[i] and Expand* (*g*[*i*+1]) are not the same once we downsize an image;
    we lose information, which cannot be recovered.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Laplacian pyramids](img/B02052_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian and Laplacian pyramids in OpenCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To see how pyramids are created in OpenCV, we will create two new activities
    called `PyramidActivity` and `HomeActivity`. The `PyramidActivity` class will
    load an image from the gallery, and then, based on the user''s options, perform
    the required actions. `HomeActivity` is used to call either `PyramidActivity`
    or `MainActivity` based on options provided by the user. So first, we make the
    resource for the `HomeActivity` class and call it `activity_home.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In our Java code, we will add listeners to these buttons to call the respective
    activities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we move on to the implementation of `PyramidActivity`. First, we will take
    a look at `activity_pyramid.xml`. We will add buttons to perform various actions
    as per the user''s options. The possible options are Gaussian pyramid up, Gaussian
    pyramid down, and Laplacian pyramid calculation. The following code is inserted
    into `LinearLayout` inside `ScrollView`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also have a menu file for this activity that will be used to load images
    from the gallery. We will have a similar method to load images from the gallery
    that we did in the earlier chapters. We will have the following lines in `PyramidActivity.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will define some global variables that we will need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We also need to specify the OpenCV callback function and initialize it in `onResume`,
    as we did earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `onCreate` function, after we initialize all our buttons, we will first
    disable them until an image has been loaded from the gallery. So, add the following
    lines after initializing all the buttons in this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In our `onActivityResult`, we will check whether the image has been loaded
    successfully, and if it has been we activate the buttons. We also load the image
    to a Mat and store it for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will add the listeners for each of the buttons. In your `onCreate`,
    add the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will implement the `executeTask` function that will perform the required
    computations in `AsyncTask`, and after they are completed, they will be loaded
    into `ImageView` that we have in our layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have called `pyrUp` and `pyrDown` with just two arguments; however,
    you can specify a custom size for the results by calling the function as `Imgproc.pyrUp(srcMat,
    dstMat, resultSize)`.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV doesn't provide a separate function to calculate the Laplacian pyramid,
    but we can use the Gaussian pyramids to generate our Laplacian pyramids.
  prefs: []
  type: TYPE_NORMAL
- en: Basic 2D transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An object in 3D space can cast a projection in 2D space that is different from
    the original projection. Such transformations are called 2D transformations. They
    are shown in the following image. We will use some of these transformations to
    explain concepts discussed later in the chapter and also in other chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic 2D transformations](img/B02052_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We write these transformations in the mathematical form, along with their matrix
    representations, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation**: The mathematical representation of a translation transformation
    is given by:![Basic 2D transformations](img/B02052_05_47.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Affine**: The mathematical representation of an affine transformation is
    given by:![Basic 2D transformations](img/B02052_05_65.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rigid**: The mathematical representation of a rigid transformation is given
    by:![Basic 2D transformations](img/B02052_05_48.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projective**: The mathematical representation of a projective transformation
    is given by:![Basic 2D transformations](img/B02052_05_49.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global motion estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Global motion estimation, as the name suggests, is the detection of motion
    using all pixels in a frame in its calculation. Some of the applications of global
    motion estimation include:'
  prefs: []
  type: TYPE_NORMAL
- en: Video stabilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video encoding/decoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This method was proposed by Bergen et.al. (1992). In this method, when the
    distance between the camera and the background scenes is large, we can approximate
    the motion of objects as affine transformations. The equations we saw earlier
    were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite these equations in the matrix form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This can be written as ![Global motion estimation](img/B02052_05_52.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the optical flow equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We try to estimate the motion in the image such that all the pixels satisfy
    it. Thus, we sum up the optical flow equation for all the pixels and try to generate
    an estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Global motion estimation](img/B02052_05_55.jpg) should ideally be zero but
    practically, it is a small value. Thus, the squared error will be small. Hence,
    we need to minimize it for the best results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_56.jpg)![Global motion estimation](img/B02052_05_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation can be minimized with respect to ![Global motion estimation](img/B02052_05_61.jpg)
    to the following linear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This linear equation can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This algorithm is now divided into four subparts: pyramid construction, motion
    estimation, image warping, and coarse-to-fine refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: For the pyramid construction, we first take a Gaussian pyramid of the images
    at time *t* and *t-1*, and compute the global flows iteratively, starting from
    the smallest layers going toward the bigger layers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, for each layer, to find the motion estimation, we use the linear equation
    derived earlier to compute A and B for the frames at time *t* and *t-1*, and use
    this information to compute an estimate for *a* ![Global motion estimation](img/B02052_05_60.jpg).
    We then warp the image at time *t-1* to an image, which tries to generate the
    object motion from the original image. This new image is compared to the image
    captured at time *t*. We then iteratively warp the image frame obtained at *t-1*
    to compute the value of ![Global motion estimation](img/B02052_05_61.jpg). With
    this value of ![Global motion estimation](img/B02052_05_61.jpg), we generate another
    warped image, which is then compared to the image at time *t*. We use this value
    of ![Global motion estimation](img/B02052_05_61.jpg) to update the value of *a*.
    This process is performed multiple times until we have a good enough estimate
    of the motion of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image warping is the process of performing any transformation on an image to
    produce another image. For this method, we perform affine transformations because
    of our earlier assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global motion estimation](img/B02052_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the final step, coarse-to-fine refinement, we make use of image pyramids
    to extend our model to include dense images (for example, representing a depth
    map).
  prefs: []
  type: TYPE_NORMAL
- en: The Kanade-Lucas-Tomasi tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having seen local and global motion estimation, we will now take a look at
    object tracking. Tracking objects is one of the most important applications of
    computer vision. The **Kanade-Lucas-Tomasi** (**KLT**) tracker implements an optical
    flow to track objects in videos. The steps to implement the algorithm are explained
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect Harris corners in the first frame of the video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each detected Harris corner, compute the motion between consecutive frames
    using the optical flow (translator) and local affine transformation (affine).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now link these motion vectors from frame-to-frame to track the corners.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate new Harris corners after a specific number of frames (say, 10 to 20)
    to compensate for new points entering the scene or to discard the ones going out
    of the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Track the new and old Harris points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checking out the KLT tracker on OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have seen earlier, the KLT tracker is one of the best algorithms available
    to track objects in videos. For this example, we will take a feed from the camera,
    detect some good trackable features, and update these to the new locations, as
    obtained by the `calcOpticalFlowPyrLK` function. We will just add a new case construct
    to the code that we wrote for the optical flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `goodFeaturesToTrack` function uses the Shi-Tomasi method to calculate good
    trackable features in an image. This could be replaced by any reliable feature
    calculation technique. It takes the frame in a grayscale format as the input and
    returns the list of features. It also takes the parameter of the maximum number
    of features, to track, the quality of features, and the minimum distance between
    features respectively. For the purpose of this sample, we will only calculate
    features in the first frame and track these features in the subsequent frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will obtain the optical flow for the feature points obtained previously.
    Note that `nextFeatures` contains the locations of the corresponding points in
    the previous frame in `prevFeatures`. We will mark the location of the feature
    points with circles. Note that we are drawing the circles at the new locations
    of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to set the current frame as the previous frame, and the current
    feature point locations as the locations of the features in the previous frame
    so as to enable tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of Shi-Tomasi tracker and the KLT tracker can be seen in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking out the KLT tracker on OpenCV](img/B02052_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The white circles in the following image represent the features that we are
    tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking out the KLT tracker on OpenCV](img/B02052_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As it is visible, a small number of points are not tracked properly. For example,
    consider the feature point at the *L* key. As you can see, in one frame, it is
    at the *L* key, while in the other frame, it shifts to the key with the semicolon.
    If you consider the feature points at the *Y* and *J* keys, they remain in their
    positions. This is because at the keys *Y* and *J*, there are well-defined corners;
    hence, the feature points are better there.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to detect a local and global motion in a video,
    and how we can track objects. We have also learned about Gaussian and Laplacian
    pyramids, and how they can be used to improve the performance of some computer
    vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to align multiple images and how to stitch
    them together to form a panoramic image.
  prefs: []
  type: TYPE_NORMAL
