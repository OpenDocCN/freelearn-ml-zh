- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Labeling Is a Collaborative Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the field of **artificial intelligence** (**AI**) continues to evolve, publicly
    available tools such as ChatGPT, **Large Language Model Meta AI** (**LLaMA**),
    Bard, Midjourney, and others have set a new benchmark for what's possible to achieve
    with structured and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: These models obviously rely on advanced algorithms and massive amounts of data,
    but many people are unaware that human labeling remains a critical component in
    their ongoing refinement and advancement. As an example, ChatGPT’s model infrastructure
    relies on individuals reviewing and annotating data samples that are then fed
    back into the model to improve its understanding of natural language and context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we explore how to get the most out of data collection and
    annotation tasks involving human labelers. We will cover these general topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why we need human annotators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding common challenges arising from human labeling tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a framework for achieving high-quality labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best-practice approaches for motivating human annotators, avoiding bias, and
    dealing with ambiguity in labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firstly, let’s understand why human input is a cornerstone of data-centric **machine**
    **learning** (**ML**).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the benefits of diverse human labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incorporating a diverse range of individuals and perspectives in the human labeling
    process offers several advantages. Humans bring a level of precision and accuracy
    to data annotation that is difficult for machines to match. While automated systems
    may struggle with ambiguity or complexity, human annotators can leverage their
    understanding and reasoning capabilities to make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Data can change over time, and new scenarios can arise that were not present
    in the original training data. Human annotators can adapt to these changes, providing
    updated annotations that reflect the new realities. This ensures that ML models
    remain relevant and effective as the data evolves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key strengths of human labelers over programmatic labeling include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain expertise**: Labelers with subject-matter expertise can provide valuable
    insights and annotations that help the model better comprehend specific topics
    and domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active learning**: This approach involves prioritizing data samples that
    the model finds ambiguous or challenging, enabling labelers to focus on areas
    where their input can have the greatest impact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity of perspectives**: A diverse group of labelers can help mitigate
    potential biases in the training data, leading to a fairer and more inclusive
    AI model. By involving labelers from diverse backgrounds and with varied experiences,
    a model can be exposed to a broader range of linguistic nuances, cultural contexts,
    and perspectives, improving its overall performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced contextual understanding**: By drawing on the experiences and knowledge
    of labelers from different backgrounds, the model can develop a deeper understanding
    of language nuances, idioms, and cultural references. Exposure to a wide variety
    of perspectives and inputs can make the model more resilient and versatile, enabling
    it to handle a broader range of tasks and scenarios effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control (QC)**: Regular audits and evaluations of labeler output
    can help ensure consistent annotation quality and adherence to guidelines, which
    is essential for effective model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adherence to ethics**: There may be data or scenarios that shouldn’t end
    up as model input based on ethical considerations. In these cases, human labelers
    play a crucial role in helping models meet ethical standards. As an example, OpenAI,
    the company behind ChatGPT, uses human annotators to review, label, and filter
    out toxic “not safe for work” data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although human annotation is a key part of data-centric model development, humans
    also add to any ML project new behaviors, biases, and risks that must be managed.
    We will now discuss these typical challenges before presenting our framework for
    managing them.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding common challenges arising from human labelers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into the best practices of labeling accuracy and consistency,
    we will define common challenges we must tackle through our labeling framework.
    Labeling inaccuracy and ambiguity are generally triggered by one or more of the
    following seven causes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Poor instructions**: Labeling inconsistencies will arise from unclear or
    insufficient instructions for the data annotation task. If annotators are not
    given clear guidelines, they may make assumptions or guesses that lead to inconsistent
    or inaccurate annotations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human bias**: Bias can introduce ambiguity when the data is skewed toward
    a particular result or outcome, leading to inaccurate interpretations. A common
    solution is to assign multiple annotators to label the same data, choosing the
    most frequently occurring label as the correct one. However, this aggregation
    or voting method can sometimes exacerbate bias rather than rectify it. For instance,
    if the majority of annotators have a particular bias, their consensus may reflect
    this bias rather than the true data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human error**: Annotators are humans, and humans make mistakes. Even the
    most well-trained, engaged, and focused annotators are one typo or mouse click
    away from applying an incorrect label leading to random noise in a dataset. These
    mistakes do occur but are unlikely to happen in a systematic way. Nevertheless,
    we need to have a way of identifying and correcting these mistakes so that they
    don’t introduce unnecessary random noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Objective versus subjective tasks**: Every task lies on a spectrum from purely
    objective (with a single correct answer) to highly subjective (with many potentially
    correct *interpretations*). The more subjective a task is, the more ambiguity
    it tends to introduce into data annotation as different annotators may have different
    interpretations. As you will learn in this chapter, even tasks that seem relatively
    straightforward can contain hidden layers of subjectivity at the boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty**: Tasks that are inherently complex or hard to comprehend can
    lead to ambiguity in data annotation. If a task is too difficult, annotators may
    struggle to understand or complete it correctly, leading to inconsistent or inaccurate
    annotations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ambiguity**: Some tasks or datasets are naturally ambiguous, meaning there’s
    room for multiple valid interpretations. This ambiguity can lead to inconsistencies
    in data annotation, as different annotators may interpret the same data in different
    ways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static versus variable labeling**: In static labeling, each data point is
    assigned a single, unchanging label. In contrast, variable labeling allows labels
    to change based on context or additional information. Variable labeling can introduce
    ambiguity as the same data point may be labeled differently in different contexts.
    This form of labeling inconsistency may also arise as annotators become more familiar
    with a task, which causes them to alter their perception of the definition of
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now introduce our framework for achieving accurate and consistent labels.
    It is specifically designed to identify or prevent issues stemming from these
    seven common labeling challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a framework for high-quality labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Annotations and reviews done by humans can be labor-intensive and susceptible
    to human errors and inconsistency. As such, the goal is to build datasets that
    are both accurate and consistent, requiring labels to meet accuracy standards
    as well as ensuring results from different annotators are within the same range.
  prefs: []
  type: TYPE_NORMAL
- en: These goals may seem obvious at first, but in reality, it can be very tricky
    to get human labelers to conform to the same opinion. On top of that, we also
    need to verify that a consensus opinion is not biased somehow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our framework for achieving high-quality human annotations consists of six
    dimensions. We will briefly summarize these dimensions before delving into a detailed
    explanation of how to achieve them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clear instructions**: To ensure high-quality labels, the instructions for
    the annotation task must be explicit and unambiguous. The annotators should have
    a clear understanding of what is expected of them, including details about the
    task, the criteria for labeling, and examples of correctly labeled data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aligned motivations**: The annotators’ motivations should align with the
    goal of obtaining high-quality labels. This could involve rewarding accuracy,
    providing feedback, and creating an environment that encourages meticulous work.
    When annotators feel that their work is valuable and recognized, they are more
    likely to produce high-quality labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subject-matter experts (SMEs)**: Utilizing annotators who are experts in
    the subject matter can significantly improve the quality of labels. These individuals
    possess deep knowledge and understanding of the context behind the data, enabling
    them to recognize subtleties and nuances that others may miss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative collaboration**: High-quality labels can be achieved through a
    process of iterative collaboration. Annotators should be encouraged to communicate
    and collaborate, revisiting and refining their labels based on collective feedback
    and discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity of thinking**: A diverse group of annotators brings different perspectives
    and interpretations to the task, which can lead to more comprehensive and robust
    labels. Diversity of thinking can help uncover blind spots and reduce bias in
    the labeling process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dealing with ambiguity**: As ambiguity is inherent in any data annotation
    task, training annotators on how to handle ambiguous cases is essential for achieving
    high-quality labels. This could include strategies such as seeking additional
    information, consulting with peers or supervisors, or following predefined rules
    for ambiguous cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s delve into these six dimensions in detail to understand how to establish
    an exceptional labeling process. It all begins by ensuring our instructions to
    annotators are crystal clear.
  prefs: []
  type: TYPE_NORMAL
- en: Designing clear instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may seem blatantly obvious that a labeling task should come with clear instructions,
    but as you will learn throughout this chapter, that is not necessarily an easy
    task. Assignments should be clear not just to the people creating them but also,
    more importantly, to the annotators who will execute them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This challenge has three components to it: firstly, instructions should contain
    specific details for tasks to be carried out reliably, regardless of who is performing
    them. Secondly, the instruction design should include ways of picking up instruction
    issues early and throughout the assignment. Thirdly, we must make sure our annotators
    are adequately qualified and motivated for the task.'
  prefs: []
  type: TYPE_NORMAL
- en: McInnis et al. (2016)1 studied the issues that can arise if we don’t manage
    one or more of these components. The researchers looked at the impact of unclear
    instructions and misaligned motivations between requestors and annotators using
    **Amazon Mechanical** **Turk** (**AMT**).
  prefs: []
  type: TYPE_NORMAL
- en: They found that seasoned annotators use various tools and techniques to assess
    the quality of an assignment and the reliability of the requesters who posted
    it, before taking on new projects. They do this to pick clearly defined assignments
    that can be performed with no hiccups while avoiding tasks that will pay them
    little money or impact their reputation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, good annotators will circumvent unclear assignments to avoid iterative
    tasks or unfair rejection of completed work (rejection of an annotator’s work
    results in non-payment for the work performed). The authors found that annotators
    commonly look for the following risk factors in task instructions when deciding
    whether to take on an assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: Flaws in the task or interface design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unclear evaluation criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unresponsive, arbitrary resolution of rejections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of information on requesters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inexperienced and unfamiliar requesters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks with poor return
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritizing efficiency over quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As data professionals, it is our job to provide task instructions that mitigate
    these seven factors, whether we are using crowdsourced annotators or in-house
    SMEs. However, sometimes you won’t know whether your instructions are clear until
    you use them in a live setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the question is: how can we best align the understanding of a task
    between requestors and annotators while also ensuring that we have the right people
    on the job?'
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. (2016)2 developed a best-practice method for this purpose called
    **Gated Instruction**. This technique is used for training annotators, aligning
    the understanding of tasks between requestors and annotators, and identifying
    underperforming workers.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Instruction is based on the idea that humans learn better when they are
    given feedback on their performance. This is done by providing an interactive
    teaching environment where users can receive feedback on their annotations and
    adjust their approach accordingly. The goal is to create a system that can provide
    accurate annotations with minimal effort from the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gated Instruction Crowdsourcing Protocol is a simple and generalizable
    three-phase process designed to ensure quality data annotation. It includes an
    interactive tutorial, screening questions, and batches of questions with continued
    screening. The authors describe the protocol as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Phase I – Interactive tutorial
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This phase involves a comprehensive tutorial that explains the task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: Workers are given clear definitions of each relation and tagging criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Workers practice by annotating sentences that illustrate each relation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Immediate feedback is provided after each practice sentence to guide the workers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phase II – Screening questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This phase is designed to evaluate the worker’s understanding of the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Workers are asked to annotate a representative set of five gold standard questions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feedback is given on each question to help the workers understand their errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Workers who fail a majority of these questions are excluded from the remaining
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phase III – Batches of questions (with continued screening)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This phase focuses on maintaining high-quality work during the task execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Gold standard questions are included in the task without providing feedback.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sets of five gold standard questions are included in each batch of 20 questions,
    with their frequency decreasing exponentially.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Workers who score less than 80% accuracy on the last 10 gold standard questions
    are eliminated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These principles ensure the integrity and efficiency of the process:'
  prefs: []
  type: TYPE_NORMAL
- en: Only workers with an AMT reputation above a certain threshold are accepted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A link to the definitions of relations is provided throughout the task for quick
    reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Workers must correct any mistakes highlighted in the feedback before proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After each batch, feedback is provided on earnings so far and performance on
    gold standard questions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Workers are reminded of a bonus upon completion of all 10 batches, encouraging
    consistent high-quality work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gated Instruction provides more accurate results than conventional instruction
    and screening methods since users are able to receive feedback on their annotations
    and adjust their approach accordingly. Using this approach, the authors improved
    precision from 0.50 to 0.77 and recall from 0.70 to 0.78 on the same dataset.
    This was in comparison to results from workers who were instructed using more
    traditional methods of instruction.
  prefs: []
  type: TYPE_NORMAL
- en: These two studies have used crowdsourced annotators to conduct their research,
    but the same frameworks are still highly relevant when your labelers are not crowdsourced
    workers, but SMEs with whom you have a closer working relationship.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you’re selecting annotators from a pool of in-house colleagues,
    you might base your selection on someone’s particular expertise, years of experience,
    and interest in contributing to the project. As you will learn in the next section,
    annotators’ motivation to perform a task can have a big influence on how data
    is collected and, ultimately, how your models perform on that data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build on these principles as we discuss how to motivate annotators and
    use SMEs for more complex labeling assignments.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning motivations and using SMEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While technology has made leaps and bounds in automating data collection, human
    data collectors still hold an essential place in various fields. They bring a
    level of understanding, empathy, and judgment that machines cannot replicate.
    For instance, human data collectors and annotators can interpret nuances in language,
    context, and emotions. Similarly, they can engage with respondents, build rapport,
    and encourage more open and honest responses.
  prefs: []
  type: TYPE_NORMAL
- en: A significant challenge with human data collectors is maintaining their motivation
    and engagement in alignment with secondary uses of this data. Let’s discuss four
    factors that commonly contribute to this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '#1 – Lack of purpose'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If data collectors do not understand the significance of their work, they may
    feel disconnected and unmotivated. A data collector might ask themselves, “Why
    am I collecting this information if no one is going to use it?”
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: Communicate the big picture and show how every data point becomes
    a valuable building block of ML solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: We start any data collection and labeling exercise with the assumption that
    workers are intelligent and able to understand the importance of collecting rich,
    unbiased information. They understand that any compromise in data quality can
    have negative implications for its future use. Our role then becomes to articulate
    the importance of good data collection and explain its intended use. This articulation
    will significantly mitigate issues related to data quality.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of years ago, Manmohan and Jonas ran a number of ML projects that required
    front-line staff members to collect a bunch of information through customer interviews.
    These interviews had been running for years so we already had lots of data collected,
    but the information was captured as text in a conversational format and therefore
    difficult to interpret statistically.
  prefs: []
  type: TYPE_NORMAL
- en: We wanted to make this data easier to use for ML purposes, so we gathered all
    front-line workers for a presentation and workshop on the importance of data quality.
    After showing these data collectors how we used the data they collected to build
    specific ML solutions, they were surprised to learn how impactful their jobs could
    be.
  prefs: []
  type: TYPE_NORMAL
- en: As one team member said, “If only I had known that the information I collect
    from one person could be used to help thousands of other customers, I would have
    put way more effort into the details.”
  prefs: []
  type: TYPE_NORMAL
- en: In the workshop, we made data collectors take ownership of the situation by
    creating a series of templated questions that would improve the accuracy and signal
    of collected data. Our front-line colleagues took great pride in improving the
    way they could collect data for the greater good of the company and its customers.
    They had a transformed sense of ownership for data quality and the results that
    came from “their data.”
  prefs: []
  type: TYPE_NORMAL
- en: We were happy too as the uplift in data quality ended up boosting our model
    accuracy by up to 40% in some instances.
  prefs: []
  type: TYPE_NORMAL
- en: '#2 – Tedious nature of data tasks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data collection and labeling can be repetitive and monotonous, leading to boredom
    and disengagement. This creates an underlying incentive to complete tasks with
    minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: We use the following four strategies to reduce the boring parts
    of data collection and labeling: refining our questions, simplifying the process,
    eliminating unnecessary data, and automating wherever possible.'
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes mindboggling how much impact you can have by teaching data collectors
    to ask better questions. More specific and targeted questions will typically yield
    more meaningful and engaging responses. This improves data quality while making
    the data collection process more interesting and less repetitive.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the process could mean streamlining workflows, using more user-friendly
    software, or providing clear instructions and training to those involved in data
    collection. By making the process more straightforward, we can reduce the cognitive
    load on individuals and make the task less tiresome.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re like most data professionals, the notion of discarding or limiting
    data collection may induce a degree of anxiety. However, it’s vital to appreciate
    that data collection shouldn’t mean hoarding every bit of data you come across,
    especially if it gets in the way of maintaining a great user experience. In fact,
    collecting unnecessary or irrelevant data can add to the monotony of the task
    and create clutter that hinders data analysis. Therefore, it’s crucial to identify
    and focus only on the data that’s truly relevant to your research question or
    business objective.
  prefs: []
  type: TYPE_NORMAL
- en: Our last simplification strategy, automation, is often a great solution to the
    tedium of data collection, but we prefer to only introduce automation once we
    have exhausted the other three strategies. There is no point in automating something
    that should be done differently or not at all.
  prefs: []
  type: TYPE_NORMAL
- en: For example, scraping tools can automatically extract large volumes of data
    from websites or documents, and rules-based logic or ML algorithms can label and
    organize data. You will learn some of these automation techniques in the coding
    chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: A little while back, we worked with a large business to build real-time predictions
    of a caller’s propensity to buy investment products. The business wanted their
    call center staff to have this prediction served up to them during the call so
    that they could match callers with the right experts for their investment needs.
  prefs: []
  type: TYPE_NORMAL
- en: These screening calls would typically take 15 to 20 minutes to complete as call
    center staff painstakingly moved through pages and pages of questions relating
    to the caller’s personal details, demographics, investment experience, and risk
    appetite.
  prefs: []
  type: TYPE_NORMAL
- en: Although lots of data were being collected, it was not the right kind of information
    to build our prediction on. We identified a small handful of supplementary data
    points required to reliably determine someone’s product needs. However, adding
    these to the data collection process as additional questions was unfeasible because
    the screening calls were already long and tedious. To get the information we needed,
    we had to create a win-win scenario for callers, call center staff, and data scientists
    alike.
  prefs: []
  type: TYPE_NORMAL
- en: As we workshopped this challenge with the call center team, we discovered that
    about 20% of the existing questions could be enhanced to make the conversation
    more concise, while another 15% of questions could be removed entirely because
    they captured irrelevant or duplicate information. Lastly, 10% of the data collected
    could be prefilled or derived based on callers’ answers to other questions.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing these changes, the average call-handling time decreased by
    about 30% even after we added five new questions to the process. This reduction
    in call duration enabled the processing of more calls per day, leading to an increase
    in conversions into paying customers – a win-win for everyone involved.
  prefs: []
  type: TYPE_NORMAL
- en: '#3 – Lack of incentives'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Without appropriate rewards or recognition, data collectors may lack the motivation
    to perform at their best. For instance, if a person is incentivized by the *quantity*
    of data points collected instead of the *quality* of individual responses, it
    may result in more superficial completion of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: We have seen the best results when we focus on motivating data
    collectors across four different areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Show what good looks like**: Firstly, it’s essential to set clear expectations
    for what constitutes high-quality work. Most people take pride in their work and
    strive to perform well when they understand the value and impact of their efforts.
    By defining the purpose and significance of the data collection or labeling task
    and demonstrating examples of excellent work, you provide a tangible target for
    your team to aim for. This clarity helps ensure the collected data is robust,
    relevant, and rich in valuable insights while minimizing noise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create mutual benefits**: Secondly, fostering a sense of shared success between
    data collectors and data users is crucial. When data collectors understand how
    their efforts contribute to the bigger picture – perhaps driving key business
    decisions or fueling innovative projects – they are more likely to feel invested
    in their work. This sense of ownership and contribution can significantly enhance
    the quality of data collection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Provide non-monetary rewards**: The third area of focus is non-monetary rewards.
    Recognition and appreciation are powerful motivators. Gamification can be a highly
    effective tool in this regard, transforming the process of data collection into
    an engaging competition. Implementing features such as publicly displayed leaderboards,
    badges, or points can foster a sense of achievement and encourage healthy competition
    among team members. The longstanding concept of “Employee of the Month” not only
    rewards exceptional performance but also sets a standard for others to aspire
    to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Provide monetary rewards**: Finally, monetary rewards can be a potent incentive.
    Tying compensation to the quality of work can drive individuals to meet or exceed
    set standards. However, this approach requires a clear framework outlining performance
    expectations and objective measures of performance. At the same time, you also
    need to have the ability to influence someone’s compensation, which can be difficult
    if your annotators and data collectors are in-house staff. While not always feasible,
    monetary incentives can be a powerful motivator when available. If they are not
    an option, doubling down on the other three areas of motivation can still yield
    impressive results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In conclusion, motivating data collectors and annotators to do a great job is
    multifaceted. It requires a mix of clear communication, mutual benefits, recognition,
    and appropriate compensation.
  prefs: []
  type: TYPE_NORMAL
- en: '#4 – Work environment'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A stressful or unsupportive work environment can also affect motivation levels.
    If a data collector or annotator has many competing priorities, they may adopt
    a “close enough is good enough” attitude toward data collection.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: Creating a suitable working environment for annotators and data
    collectors to focus on the main task at hand basically requires you to remove
    any unnecessary tasks or friction getting in the way of someone performing their
    job.'
  prefs: []
  type: TYPE_NORMAL
- en: We regularly refuse to work on data science projects unless everyone needed
    for the project is motivated to participate and makes themselves available for
    the duration of the project. This often requires us to negotiate for a dedicated
    time in the calendar with SMEs, data collectors, and their managers.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to provide a feedback mechanism for data collectors. That
    way, they can contribute to the process and call out stuff that doesn’t work or
    could be done better, such as questions that could be framed differently, changing
    the order of questions or tasks, confusing labeling requirements, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Give annotators and data collectors the necessary tools and resources to do
    their job swiftly and effectively. Examples include digitizing data collection,
    automating data population wherever possible, conditionally formatting drop-down
    options, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous and timely feedback between project participants is also vital. Surprising
    workers with negative feedback late in the process can be demotivating and counterproductive.
    Instead, collaborate and provide feedback throughout the process to ensure alignment
    from the outset. Assume that there may be hiccups along the way, and be ready
    to coach your team through them proactively. Conversely, if you assume that everyone
    has understood every instruction as intended, then you’re setting yourself up
    for failure.
  prefs: []
  type: TYPE_NORMAL
- en: Some people don’t consider this kind of work part of the data science domain.
    In our opinion, this is where a lot of data professionals fall short. It’s important
    to recognize that human behavior and bias can be a seriously limiting factor for
    data science projects, and you want to reduce this impact as much as possible.
    If you don’t lean into these non-technical hurdles, you will end up with poorer
    data as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the next dimension in our framework: iterative collaboration.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborating iteratively
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Iterative collaboration should be a central strategy for human labeling tasks.
    Basically, it entails creating an ongoing process of feedback and fine-tuning
    the data labeling process. Here are three guiding principles for implementing
    a collaborative approach to data labeling, validated by data labeling platform
    SUPA’s best-practice approach3.
  prefs: []
  type: TYPE_NORMAL
- en: Start small and iron out any issues early
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Initiating the data labeling process with smaller datasets is a practical approach.
    Instead of starting with thousands of observations, begin with a calibration batch
    of, say, 50 observations. This manageable dataset allows you to review the labels,
    spot potential problems, enhance instructions, and provide feedback to the labelers.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat this process until you are confident that you have gone through a representative
    sample of the full dataset, that you have identified most edge cases, and that
    your labelers can perform the task consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling inconsistencies across the same observations indicates a need for rule
    revision. If annotators are interpreting guidelines differently, then it’s worth
    understanding whether the cause is unclear labeling rules, discrepancies in the
    understanding and experience of individual labelers, or something entirely different.
    In other words, your labeling instructions are never set-and-forget. They should
    be under constant supervision and evolve to meet the needs of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize what good looks like
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Annotation rules can occasionally be vague, leading to subjective interpretations
    and inconsistencies. For instance, in an image labeling exercise, a rule such
    as “Only label an item when most of it is visible” might be interpreted differently
    by different labelers. Therefore, it’s important to visually show what good looks
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Visual illustrations are invaluable in the data labeling process, as they offer
    labelers explicit guidelines on how labeled objects should appear. We recommend
    showing examples of how to perform a labeling task correctly, but also examples
    of the opposite. By offering visual illustrations of good and bad labeling conventions,
    labelers gain a deeper understanding of the task, thereby enhancing their productivity
    and precision.
  prefs: []
  type: TYPE_NORMAL
- en: Be very specific about edge cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Edge cases are situations that deviate from the ordinary and can result in inconsistent
    labels due to subjective opinions. For instance, should a toy car be labeled as
    a car? Is a tandem bicycle one or two bicycles, or is it in its own category?
  prefs: []
  type: TYPE_NORMAL
- en: To manage edge cases effectively, you need to have a mechanism in place for
    annotators to flag these items for further consideration. If you don’t have a
    feedback mechanism in place, it’s likely that annotators will make up their own
    judgment on the spot to complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: Pradhan et al. (2022)4 propose a so-called *FIND-RESOLVE-LABEL* workflow for
    crowdsourced annotation, aimed at addressing these three iteration steps. The
    FIND-RESOLVE-LABEL workflow is a guided labeling process designed to reveal ambiguities
    for a specific labeling task and associated instructions.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a labeling task might be to identify images with a woman in it.
    On the surface, this task appears rather simple, but in reality, annotators can
    quickly face ambiguity. For example, when is someone a woman, and when is someone
    a girl? Does it even matter? What about a statue of a woman or the *Mona Lisa*
    painting – does that count? What if the woman is only partially visible?
  prefs: []
  type: TYPE_NORMAL
- en: In reality, it can be very difficult to provide high-quality instructions for
    a given labeling task because there can be so many dimensions to consider.
  prefs: []
  type: TYPE_NORMAL
- en: 'The FIND-RESOLVE-LABEL workflow aims to discover and remove these ambiguities
    at the beginning of the labeling exercise. It consists of three key components
    that work together to streamline the data labeling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find**: In this initial stage, annotators are provided with labeling instructions
    and asked to identify examples that are ambiguous based on these instructions.
    For each identified example, labelers are also asked to provide a concept tag
    that provides an explanation for why a certain label was chosen. This allows for
    the collection of the rationale and conceptual thinking behind labeling decisions,
    which can then be fed back into improved labeling instructions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Resolve**: Once the data points are identified, the next step involves resolving
    any ambiguities or conflicts in the data. This may require domain expertise to
    make informed decisions on how to address inconsistencies or missing information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Label**: Finally, after resolving any issues, the data points are labeled
    appropriately, ensuring high-quality annotations that can be used for training
    ML models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pradhan and his team discovered that focusing on the most unclear data points
    and clarifying them during labeling greatly improved data quality. They noticed
    that in some ambiguous scenarios, many annotators agreed on answers that were
    different from what the requester regarded as correct. This means that even when
    smart answer aggregation methods are used, there’s a risk of getting incorrect
    labels for these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the study also found that workers could correctly label observations
    closely related to the main concept. This suggests we might not need to explain
    every possible ambiguity during the task because a well-chosen set of examples
    could help the team correctly label other unclear examples.
  prefs: []
  type: TYPE_NORMAL
- en: With these findings in mind, let’s explore how to deal with ambiguity among
    annotators in a data-centric fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with ambiguity and reflecting diversity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the help of human annotators, we can produce datasets that are incredibly
    rich in information, but this sometimes requires us to tackle ambiguity in innovative
    ways. At the same time, ambiguity can be hard to spot. What one person finds obvious,
    another person may find entirely confusing.
  prefs: []
  type: TYPE_NORMAL
- en: Companies and researchers use internal staff, volunteers, or crowdsourcing platforms
    such as AMT to gain access to human annotators at affordable rates. These labelers
    come from diverse backgrounds and carry different biases, all of which can impact
    the quality of labeling – especially when there is some element of judgment involved.
  prefs: []
  type: TYPE_NORMAL
- en: This challenge only grows as AI and ML are used to classify and generate new
    content from datasets that can be interpreted differently depending on context
    and who is doing the interpretation. This is demonstrated in a research paper,
    *The Risk of Racial Bias in Hate Speech Detection*, by Sap et al. (2019)5, which
    investigates how annotators’ insensitivity to differences in dialect can lead
    to racial bias in automated hate speech detection.
  prefs: []
  type: TYPE_NORMAL
- en: The paper states that even datasets designed specifically for detecting hate
    speech contain an inherent bias toward specific groups or minority languages.
    This is because the underlying parameters are created based on the preferences
    of annotators who may be unaware of subtle nuances between different ethnicities
    or languages.
  prefs: []
  type: TYPE_NORMAL
- en: For example, some ethnicities or social groups may use colloquial language that
    seems rude or offensive to people from other demographics. For instance, the researchers
    discovered that labelers tended to mark phrases in African American English as
    being more toxic than those using General American English.
  prefs: []
  type: TYPE_NORMAL
- en: These inherent biases are difficult to avoid because annotators are rarely complete
    SMEs but people following general instructions. At the same time, labelers are
    unlikely to be a representative sample of the general public. For example, the
    majority of AMT participants have historically been comprised of younger individuals
    who are unmarried and without children. The vast majority of Turkers hail from
    only two countries, the US and India, while less than 2% come from the Global
    South6.
  prefs: []
  type: TYPE_NORMAL
- en: This problem doesn’t just pertain to the subject of hate speech. As such, certain
    dialects, lifestyles, cultural backgrounds, and worldviews may be overrepresented,
    while others remain underrepresented when determining any kind of label that requires
    subjective interpretation. The resulting poor data collection can lead to an increased
    gap between the labels used and the real-world scenarios they are meant to represent.
  prefs: []
  type: TYPE_NORMAL
- en: The imbalance that exists in data labeling is evident in some of the world’s
    most widely used public training datasets. Research has found that two of the
    most common databases, *ImageNet* and *Open Images*, are biased toward the US
    and Europe, evident by the fact that models created from these datasets have poorer
    performance on images sourced from the Global South7.
  prefs: []
  type: TYPE_NORMAL
- en: Images of grooms, for instance, receive a lower accuracy rating when they come
    from Ethiopia or Pakistan compared to similar images from the US. This particular
    discrepancy is due to the way objects such as “wedding” and “spices” are interpreted
    depending on their cultural context, with publicly available recognition systems
    struggling to correctly classify them when sourced from countries outside of America
    or Europe.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding approaches for dealing with ambiguity in labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Labels are often absolute, but opinions are not. At the same time, humans will
    tend to disagree on what more abstract labels should be. As data-centric practitioners,
    we should anticipate ambiguity and disagreement among annotators and have a plan
    in place for managing it.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that we actually *want* ambiguity to show up so that we can
    deal with it in the right way. Ambiguous scenarios can arise because labeling
    instructions are unclear, but they can also uncover new labels that must be included
    in our dataset. Therefore, we should try to design our labeling teams to maximize
    the likelihood that we will tease out disagreements if they are there.
  prefs: []
  type: TYPE_NORMAL
- en: A great way to do this is to involve a more diverse pool of annotators who are
    better attuned to differences in elements such as language and opinion, but in
    doing this, we also want to elevate the opinions of minority groups. Stanford
    University researchers Gordon et al. (2022)8 propose an approach for this purpose,
    called *Jury Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In previous research by Gordon et al. (2021)9, it was found that when accounting
    for labels from non-majority groups in a comment toxicity labeling task, the classifier’s
    performance decreased from 0.95 ROC AUC to 0.73 ROC AUC. This means that the classifier
    is not as effective when applied to comments from people who are not part of the
    majority group. In other words, it is impossible to make everyone agree, so we
    should consider who we listen to – not necessarily the simple majority.
  prefs: []
  type: TYPE_NORMAL
- en: Jury Learning stands in contrast to more straightforward aggregation or majority
    voting methods. Rather than basing labels on a majority rule or a probability,
    Jury Learning actively uses varying opinions to pick out underlying biases and
    suppress minority opinions. It is proposed as a way to integrate dissenting voices
    into ML systems in order to prevent them from becoming over-reliant on a single
    opinion or view. Here is how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '**Jury Learning** is a **supervised ML** (**SML**) approach that resolves disagreements
    explicitly through the metaphor of a jury. This approach allows practitioners
    to specify whose voices their classifiers reflect, and in what proportion. The
    goal of Jury Learning is to define which people or groups determine a system’s
    prediction and in what proportion, allowing developers to analyze – and potentially
    mitigate – any potential biases that may be present in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to use Jury Learning effectively, practitioners must first identify
    the jurors they wish to include in their model. This can be done by selecting
    individuals who represent different perspectives on the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say the task is to label whether a movie is good. Practitioners
    could select jurors (labelers) from different demographics such as age, gender,
    ethnicity, location, political orientation, or socio-economic status. Once the
    jurors have been selected, they must then provide input into the model’s predictions
    by answering questions about whether a movie is good and why.
  prefs: []
  type: TYPE_NORMAL
- en: Once all of the jurors have provided their input into the model’s predictions,
    practitioners can then use this data to create an aggregate prediction for each
    individual case. By considering multiple perspectives on each case, practitioners
    can ensure that their models are more accurate and less biased than if they had
    relied solely on one perspective when making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, practitioners can also use Jury Learning to analyze any potential
    biases present in their models by comparing the aggregate predictions made by
    different juries, composed of individuals from different backgrounds or perspectives.
    This comparison effectively provides us with a prediction range rather than binary
    labels. The analysis can help identify any areas where bias may be present and
    allow practitioners to adjust their models accordingly in order to reduce any
    potential bias and improve overall accuracy. This process is illustrated in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – An overview of the Jury Learning process taken from Gordon et
    al. (2022)](img/B19297_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – An overview of the Jury Learning process taken from Gordon et al.
    (2022)
  prefs: []
  type: TYPE_NORMAL
- en: By now, you may have noticed that Jury Learning is not simple to set up. As
    with many data-centric approaches, Jury Learning requires extensive planning and
    coordination to ensure that all participants have a clear understanding of the
    expectations and guidelines of the process. This can be a time-consuming and resource-intensive
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Jury Learning also requires a labeler pool of a certain size to ensure the necessary
    diversity of opinions and views. This can be a challenge, particularly for projects
    with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge that comes with implementing Jury Learning is the need for
    meta-information on labelers. To ensure that the results are both accurate and
    reliable, Jury Learning requires labelers with diverse skill sets and backgrounds.
    Gathering this information and developing a pool of labelers that meets the required
    standards can be a difficult task that again requires upfront planning.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these challenges, Jury Learning provides an innovative and effective
    way for practitioners to incorporate dissenting voices into ML models while also
    helping them identify and mitigate any potential biases present in their models.
    By considering multiple perspectives when making predictions and analyzing potential
    biases present in their models, practitioners can ensure that their ML models
    are both accurate and unbiased when making decisions about important tasks that
    are prone to subjectivity.
  prefs: []
  type: TYPE_NORMAL
- en: To round off this chapter, let’s explore how we can measure ambiguity or disagreement
    among annotators statistically.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring labeling consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed a range of tools and techniques for creating consistent
    and high-quality annotations. While these elements create the foundation for good
    datasets, we also want to be able to measure whether our annotators are performing
    consistently.
  prefs: []
  type: TYPE_NORMAL
- en: To gauge annotator consistency, we recommend using two measures of labeling
    consistency called intra- and interobserver variability, respectively. These are
    standard terms in clinical research and refer to the degree of agreement among
    different measurements or evaluations made by the same observer (intra-) or by
    different observers (inter-). To simplify the explanation, consider “observer”
    to be interchangeable with “labeler,” “annotator, “rater,” “data collector,” and
    any other similar term we have used throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: While both intra- and interobserver variability relate to measurement consistency,
    they address different aspects. Intra-observer variability refers to the consistency
    of a single observer over time, while inter-observer variability refers to the
    consistency between different observers. Factors such as training, experience,
    and standardization of protocols can significantly influence both.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking observer variability is crucial as it directly impacts the quality
    and reliability of your input dataset and, therefore, your model. If the same
    object is interpreted differently by various observers (inter-observer variability)
    or even by the same observer at different times (intra-observer variability),
    it could lead to inconsistencies in labeling, thereby affecting the overall quality
    of ML outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Several factors contribute to observer variability, including lack of standardization
    in measurement techniques, observer fatigue, and subjective interpretations. As
    an example, someone’s judgment might be influenced by their level of experience,
    personal bias, or even their state of mind at the time of observation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that labeling discrepancies between two or more observers
    do not necessarily mean that one observer is correct and others are incorrect.
    Labeling disagreements may simply mean that the labeled object or situation is
    ambiguous or transitory and therefore difficult to give a hard label.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, two doctors might disagree on the relative progression of an illness
    based on medical imaging or symptom descriptions. This could be because the diagnosis
    is uncertain rather than one doctor being more correct than the other.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique for measuring variability is the **intraclass correlation
    coefficient** (**ICC**). ICC is a statistical tool used to assess the consistency
    or conformity of annotations made by one or more observers measuring the same
    entity. Unlike the commonly used Pearson correlation coefficient, which measures
    linear relationships between variables, ICC assesses the reliability of ratings
    within the same group of data. It’s particularly useful when we want to know how
    strongly units in the same group resemble each other.
  prefs: []
  type: TYPE_NORMAL
- en: A high ICC value close to 1 indicates a high similarity between values from
    the same group. Conversely, a low ICC suggests less agreement among the ratings.
  prefs: []
  type: TYPE_NORMAL
- en: There are different forms of ICC, each applicable in specific circumstances.
    For instance, some forms are more suitable when we have a single measurement from
    each subject, while others are better suited for an average of several measurements.
    The choice of form depends on the nature of your study and the type of data you
    have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows six common definitions outlined by Shrout and
    Fleiss (1979)10:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Six common ICCs](img/B19297_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Six common ICCs
  prefs: []
  type: TYPE_NORMAL
- en: To build our intuition around the use of ICC scores, let’s work through a practical
    example using the **Pingouin** Python package. Pingouin is an open source package
    with a large number of useful statistical features. It primarily utilizes pandas
    and NumPy, so make sure you have these installed as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example scenario, suppose we have four wine-tasting judges rating the
    quality of eight different wines by giving them a score of 0 to 9\. We would like
    to know whether these judges are rating the wines consistently. The judges’ ratings
    are displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Wine-tasting scores from four different judges](img/B19297_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Wine-tasting scores from four different judges
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, three types of variability can occur:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variability due to differences in the objects being assessed (suppose two different
    samples of the same wine had slightly different tastes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variability caused by the assessment of observers, for example, the difference
    between judges B and C’s rating of wine *#3*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variability in the use of labels; for example, everyone finds wine *#1* the
    worst, but three different ratings have been used to rate it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ICC calculation will take all of these into account as it is based on **analysis
    of variance** (**ANOVA**) analysis. Before we get to calculating ICC, we must
    first determine which type of measure we’re after. We can use the following screenshot
    as a guide to selecting the correct form of ICC for our situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – ICC model selection guide](img/B19297_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – ICC model selection guide
  prefs: []
  type: TYPE_NORMAL
- en: 'In our scenario, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: Each judge has rated each wine once, so we can determine that all subjects have
    been evaluated by the same group of observers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we will assume that the four judges have been chosen randomly
    from a larger pool of potential judges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are interested in the reliability of individual observers rather than the
    average reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Therefore, we’re looking to use the ICC2 calculation to determine our* *reliability
    score*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then use the following script to run the ICC function over our wine scores.
    The Pingouin ICC operator, `intraclass_corr`, will calculate and present all six
    common ICC measures, but we are only interested in ICC2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output. Our ICC score is 0.728, which means our
    judges are in moderate agreement on ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Output table](img/B19297_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Output table
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to understand that there are no strict thresholds for what constitutes
    an “acceptable” ICC score. Though there are no rigid benchmarks, some general
    guidelines can help interpret ICC scores. These ranges are not absolute and should
    be interpreted in the context of your specific study or analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ICC less than 0.5**: This range is generally considered to indicate poor
    reliability. For instance, if you have an ICC of 0.3 for a set of ratings, it
    would suggest a low level of agreement among raters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICC between 0.5 and 0.75**: Scores in this range are typically considered
    to show moderate reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICC between 0.75 and 0.9**: These scores suggest good reliability. If you
    achieve an ICC of 0.8, for example, it indicates a high degree of agreement among
    your raters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICC greater than 0.90**: This range represents excellent reliability. An
    ICC of 0.95, for example, would suggest almost perfect agreement among raters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When interpreting ICC scores, it’s also important to consider several factors
    that can influence their reliability. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample size**: As with many statistical measures, the ICC is sensitive to
    sample size. Larger sample sizes tend to provide more reliable estimates of the
    ICC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data outcome range**: ICC scores can also be impacted by the potential range
    of outcomes being assessed and the difficulty of determining annotations. For
    instance, if our wine judges could only label wines as “good” or “bad” (1, 0)
    rather than a range (0–9), then that would likely alter the final ICC score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subject variability**: The ICC is also influenced by the variability among
    subjects. High subject variability can lead to lower ICC values, even when raters
    are consistent in their ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, interpreting ICC scores requires understanding the context of your
    study, the nature of your data, and the specific form of ICC used. Always consider
    these factors when interpreting and communicating your results.
  prefs: []
  type: TYPE_NORMAL
- en: Remember – ICC scores are just one piece of the puzzle. They should be used
    in conjunction with other statistical measures and insights to provide a comprehensive
    understanding of your data. Let’s summarize what we’ve covered so far in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve examined the critical role that humans play in
    ensuring data quality, particularly in the initial stages of data labeling. We’ve
    recognized that while human labelers are indispensable, they also present certain
    challenges, including biases and inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: To address these issues, we’ve explored various strategies to train labelers
    effectively for high-quality dataset development. The key takeaway here is that
    well-trained labelers, armed with clear instructions, can significantly increase
    the overall quality of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Improving task instructions emerged as a recurring theme, underscoring their
    importance in facilitating the labeling process. Iterative collaboration was also
    highlighted as an essential practice, promoting continuous improvement through
    feedback and refinement.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have gained a comprehensive understanding
    of why human involvement is crucial in data-centric model building, the challenges
    posed by human labelers, and practical ways to overcome them. More importantly,
    you’ll have learned how to use specific frameworks to achieve quality labeling,
    setting a solid foundation for successful ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on these skills and delve deeper into the
    technical aspects of data cleaning and augmentation before we explore programmatic
    labeling techniques in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques
    for Programmatic Labeling in Machine Learning*. It’s time to get deep into code!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*McInnis B.*, *Cosley D.*, *Nam C.*, *Leshed G.*, *Taking a HIT: Designing
    around Rejection, Mistrust, Risk, and Workers’ Experiences in Amazon Mechanical
    Turk*, *Information Science & Law School*, *Cornell* *University.* [https://dl.acm.org/doi/epdf/10.1145/2858036.2858539](https://dl.acm.org/doi/epdf/10.1145/2858036.2858539)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Liu A.*, *Soderland S.*, *Bragg J.*, *Lin C. H.*, *Ling X.*, *Weld D. S.*,
    *Effective Crowd Annotation for Relation Extraction*, *Turing Center*, *Department
    of Computer Science and Engineering*, *University of* *Washington.* [https://aclanthology.org/N16-1104.pdf](https://aclanthology.org/N16-1104.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.supa.so/post/iteration-a-key-data-labeling-process-often-overlooked](https://www.supa.so/post/iteration-a-key-data-labeling-process-often-overlooked),
    viewed July 30, 2023.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Pradhan V. K.*, *Schaekerman M.*, *Lease M.*, *2022*, *In Search of Ambiguity:
    A Three-Stage Workflow Design to Clarify Annotation Guidelines for Crowd Workers*,
    *Front. Artif. Intell.*, *18 May 2022*, *Sec. Machine Learning and Artificial
    Intelligence Volume 5 -* *2022.* [https://doi.org/10.3389/frai.2022.828187](https://doi.org/10.3389/frai.2022.828187)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Sap, M.*, *Card, D.*, *Gabriel, S.*, *Choi, Y.*, *Smith, N. A.*, *The Risk
    of Racial Bias in Hate Speech Detection*, *Paul G. Allen School of Computer Science
    & Engineering*, *University of Washington*, *Seattle*, *USA*, *Machine Learning
    Department*, *Carnegie Mellon University*, *Pittsburgh*, *USA*, *Allen Institute
    for Artificial Intelligence*, *Seattle*, *USA*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://venturebeat.com/business/the-ai-industry-is-built-on-geographic-and-social-inequality-research-shows/](https://venturebeat.com/business/the-ai-industry-is-built-on-geographic-and-social-inequality-research-shows/),
    viewed April 22, 2023.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://venturebeat.com/ai/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/](https://venturebeat.com/ai/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/),
    viewed April 22, 2023.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Gordon M. L.*, *Lam M. S.*, *Park J. S.*, *Patel K.*, *Hancock J.*, *Hashimoto
    T.*, *Bernstein M. S.*, *2022*. *Jury Learning: Integrating Dissenting Voices
    into Machine Learning Models*. In *CHI Conference on Human Factors in Computing
    Systems (CHI ’22)*, *April 29-May 5, 2022*, *New Orleans*, *LA*, *USA*. *ACM*,
    *New York*, *NY*, *USA*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Gordon M. L.*, *Zhou K.*, *Patel K.*, *Hashimoto T.*, *Bernstein M.S.*, *2021*.
    *The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics
    In Line With Reality*. In *CHI Conference on Human Factors in Computing Systems
    (CHI ’21)*, *May 8-13, 2021*, *Yokohama*, *Japan*. *ACM*, *New York*, *NY*, *USA*,
    14 pages. [https://doi.org/10.1145/3411764.3445423](https://doi.org/10.1145/3411764.3445423)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Shrout, P. E.* & *Fleiss, J. L.* (*1979*), *Intraclass correlations: uses
    in assessing rater reliability*, *Psychological Bulletin*, *86(2)*, *420.* [https://psycnet.apa.org/doi/10.1037/0033-2909.86.2.420](https://psycnet.apa.org/doi/10.1037/0033-2909.86.2.420),
    viewed July 30, 2023.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Part 3: Technical Approaches to Better Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we explore technical approaches to enhance data quality and management
    in machine learning. We cover topics ranging from data cleaning, programmatic
    labeling, and synthetic data usage, to addressing bias and handling rare events.
    Each chapter gives you essential skills and knowledge to work efficiently with
    data in machine learning, highlighting how important good quality data is in building
    robust ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19297_05.xhtml#_idTextAnchor070)*, Techniques for Data Cleaning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for Programmatic
    Labeling in Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19297_07.xhtml#_idTextAnchor111)*, Using Synthetic Data in Data-Centric
    Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19297_08.xhtml#_idTextAnchor125)*, Techniques for Identifying
    and Removing Bias*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19297_09.xhtml#_idTextAnchor141)*, Dealing with Edge Cases and
    Rare Events in Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
