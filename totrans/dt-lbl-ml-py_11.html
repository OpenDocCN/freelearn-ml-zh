<html><head></head><body>
<div id="_idContainer143">
<h1 class="chapter-number" id="_idParaDest-243"><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-244"><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.2.1">Labeling Audio Data</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will embark on this transformative journey through the realms of real-time audio capture, cutting-edge transcription with the Whisper model, and audio classification using a </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">CNN</span></strong><span class="koboSpan" id="kobo.7.1">), with a focus on spectrograms. </span><span class="koboSpan" id="kobo.7.2">Additionally, we’ll </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.8.1">explore innovative audio augmentation techniques. </span><span class="koboSpan" id="kobo.8.2">This chapter not only equips you with the tools and techniques essential for comprehensive audio data labeling but also unveils the boundless possibilities that lie at the intersection of AI and audio processing, redefining the landscape of audio </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">data labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">Welcome to a journey through the intricate world of audio data labeling! </span><span class="koboSpan" id="kobo.10.2">In this chapter, we embark on an exploration of cutting-edge techniques and technologies that empower us to unravel the richness of audio content. </span><span class="koboSpan" id="kobo.10.3">Our adventure unfolds through a diverse set of topics, each designed to enhance your understanding of audio processing </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">and labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">Our journey begins with the dynamic realm of real-time audio capture using microphones. </span><span class="koboSpan" id="kobo.12.2">We delve into the art of voice classification, using the random forest classifier to discern and categorize distinct voices in the </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">captured audio.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">Then, we introduce the groundbreaking Whisper model, a powerful tool for transcribing uploaded audio data. </span><span class="koboSpan" id="kobo.14.2">Witness the seamless integration of the Whisper model with OpenAI for accurate transcriptions, followed by a meticulous labeling process. </span><span class="koboSpan" id="kobo.14.3">As we unfold the capabilities of the Whisper model, we draw insightful comparisons with other open source models dedicated to audio </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">data analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.16.1">Our journey takes a visual turn as we explore the creation of spectrograms, visually capturing the intricate details of sound. </span><span class="koboSpan" id="kobo.16.2">The transformative CNNs come into play, elevating audio classification through visual representations. </span><span class="koboSpan" id="kobo.16.3">Learn the art of labeling spectrograms, unraveling a new dimension in </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">audio processing.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">Prepare to expand your horizons as we venture into the realm of augmented data for audio labeling. </span><span class="koboSpan" id="kobo.18.2">Discover the transformative impact of noise augmentation, time-stretching, and pitch-shifting on audio data. </span><span class="koboSpan" id="kobo.18.3">Uncover the techniques to enhance the robustness of your labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">audio datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">Our exploration culminates in the innovative domain of Azure Cognitive Services. </span><span class="koboSpan" id="kobo.20.2">Immerse yourself in the capabilities of Azure to transform speech to text and achieve speech translation. </span><span class="koboSpan" id="kobo.20.3">Witness the seamless integration of Azure Cognitive Services, revolutionizing the landscape of </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">audio processing.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">We are going to cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.24.1">Capturing real-time voice using a microphone and classifying voices using the random </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">forest classifier</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Uploading audio data and transcribing an audio file using OpenAI’s Whisper model and then labeling </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">the transcription.</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">A comparison of the Whisper model with other open source models for audio </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">data analysis</span></span></li>
<li><span class="koboSpan" id="kobo.30.1">Creating a spectrogram for audio data and then labeling the spectrogram, using CNN for </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">audio classification</span></span></li>
<li><span class="koboSpan" id="kobo.32.1">Augmenting audio data such as noise augmentation, time-stretching, </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">and pitch-shifting</span></span></li>
<li><span class="koboSpan" id="kobo.34.1">Azure Cognitive Services for speech-to-text and </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">speech translation</span></span></li>
</ul>
<h1 id="_idParaDest-245"><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.36.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.37.1">We are going to install the following </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">Python libraries.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.39.1">openai-whisper</span></strong><span class="koboSpan" id="kobo.40.1"> is the </span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.41.1">Python library provided by OpenAI, offering access to the powerful </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.42.1">Whisper </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">Automatic Speech Recognition</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">ASR</span></strong><span class="koboSpan" id="kobo.46.1">) model. </span><span class="koboSpan" id="kobo.46.2">It allows you to transcribe audio data with </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">state-of-the-art accuracy:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.48.1">
%pip install openai-whisper</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.49.1">librosa</span></strong><span class="koboSpan" id="kobo.50.1"> is a </span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.51.1">Python package for music and audio analysis. </span><span class="koboSpan" id="kobo.51.2">It provides tools for various tasks, such as loading audio files, extracting features, and performing transformations, making it a valuable library for audio </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">data processing:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.53.1">
%pip install librosa</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.54.1">pytube</span></strong><span class="koboSpan" id="kobo.55.1"> is</span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.56.1"> a lightweight, dependency-free Python library for downloading YouTube videos. </span><span class="koboSpan" id="kobo.56.2">It simplifies the process of fetching video content from YouTube, making it suitable for various applications involving </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">YouTube data:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.58.1">
%pip install pytube</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.59.1">transformers</span></strong><span class="koboSpan" id="kobo.60.1"> is a </span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.61.1">popular Python library developed by Hugging Face. </span><span class="koboSpan" id="kobo.61.2">It provides</span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.62.1"> pre-trained models and various utilities for </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">natural language processing</span></strong><span class="koboSpan" id="kobo.64.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.65.1">NLP</span></strong><span class="koboSpan" id="kobo.66.1">) tasks. </span><span class="koboSpan" id="kobo.66.2">This includes transformer-based models such as BERT  </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">and GPT:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.68.1">
%pip install transformers</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.69.1">joblib</span></strong><span class="koboSpan" id="kobo.70.1"> is a </span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.71.1">Python library for lightweight pipelining in Python. </span><span class="koboSpan" id="kobo.71.2">It is particularly useful for parallelizing and caching computations, making it efficient for tasks involving parallel processing and </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">job scheduling:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.73.1">
%pip install joblib</span></pre> <h2 id="_idParaDest-246"><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.74.1">Downloading FFmpeg</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.75.1">FFmpeg</span></strong><span class="koboSpan" id="kobo.76.1"> is a </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.77.1">versatile and open source multimedia framework that facilitates the handling, conversion, and</span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.78.1"> manipulation of audio and video </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">files (</span></span><a href="https://ffmpeg.org/download.html"><span class="No-Break"><span class="koboSpan" id="kobo.80.1">https://ffmpeg.org/download.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.81.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.82.1">To download</span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.83.1"> FFmpeg for macOS, select </span><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">static FFmpeg binaries for macOS 64-bit</span></strong><span class="koboSpan" id="kobo.85.1"> from </span><a href="https://evermeet.cx/ffmpeg/"><span class="koboSpan" id="kobo.86.1">https://evermeet.cx/ffmpeg/</span></a><span class="koboSpan" id="kobo.87.1">. </span><span class="koboSpan" id="kobo.87.2">Download </span><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">ffmpeg-6.1.1.7z</span></strong><span class="koboSpan" id="kobo.89.1"> and extract and copy it to your </span><strong class="source-inline"><span class="koboSpan" id="kobo.90.1">&lt;home directory&gt;/&lt;new folder&gt;/bin</span></strong><span class="koboSpan" id="kobo.91.1">. </span><span class="koboSpan" id="kobo.91.2">Change </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">System preferences</span></strong><span class="koboSpan" id="kobo.93.1"> | </span><strong class="bold"><span class="koboSpan" id="kobo.94.1">Security and privacy</span></strong><span class="koboSpan" id="kobo.95.1">| </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">General</span></strong><span class="koboSpan" id="kobo.97.1">, and then select </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">Open anyway</span></strong><span class="koboSpan" id="kobo.99.1">. </span><span class="koboSpan" id="kobo.99.2">Then, double-click the </span><strong class="source-inline"><span class="koboSpan" id="kobo.100.1">ffmpeg</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.101.1">executable file.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">To download FFmpeg for a Windows OS, select Windows builds by BtbN: </span><a href="https://github.com/BtbN/FFmpeg-Builds/releases"><span class="koboSpan" id="kobo.103.1">https://github.com/BtbN/FFmpeg-Builds/releases</span></a><span class="koboSpan" id="kobo.104.1">. </span><span class="koboSpan" id="kobo.104.2">Download </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">ffmpeg-master-latest-win64-gpl.zip</span></strong><span class="koboSpan" id="kobo.106.1">. </span><span class="koboSpan" id="kobo.106.2">Extract and set the </span><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">path</span></strong><span class="koboSpan" id="kobo.108.1"> environment variable of the extracted </span><strong class="source-inline"><span class="koboSpan" id="kobo.109.1">ffmpeg</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.110.1">bin folder.</span></span></p>
<p><span class="koboSpan" id="kobo.111.1">The code for this chapter is available at GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">here: </span></span><a href="https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11"><span class="No-Break"><span class="koboSpan" id="kobo.113.1">https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.114.1">.</span></span></p>
<h2 id="_idParaDest-247"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.115.1">Azure Machine Learning</span></h2>
<p><span class="koboSpan" id="kobo.116.1">If you want to explore the Whisper </span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.117.1">model along with other machine learning models available in the Azure Machine Learning model catalog, you can create a free Azure account at </span><a href="https://azure.microsoft.com/en-us/free"><span class="koboSpan" id="kobo.118.1">https://azure.microsoft.com/en-us/free</span></a><span class="koboSpan" id="kobo.119.1">. </span><span class="koboSpan" id="kobo.119.2">Then, you can try Azure Machine</span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.120.1"> Learning for free </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">at </span></span><a href="https://azure.microsoft.com/en-us/products/machine-learning/"><span class="No-Break"><span class="koboSpan" id="kobo.122.1">https://azure.microsoft.com/en-us/products/machine-learning/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.123.1">.</span></span></p>
<h1 id="_idParaDest-248"><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.124.1">Real-time voice classification with Random Forest</span></h1>
<p><span class="koboSpan" id="kobo.125.1">In an era </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.126.1">marked by the integration of advanced technologies into our daily lives, real-time voice classification systems have emerged as pivotal tools across various domains. </span><span class="koboSpan" id="kobo.126.2">The Python script in this section, showcasing the implementation of a real-time voice classification system using the Random Forest classifier from scikit-learn, is a testament to the versatility and significance of </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">such applications.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">The primary objective of this script is to harness the power of machine learning to differentiate between positive audio samples, indicative of human speech (voice), and negative samples, representing background noise or non-vocal elements. </span><span class="koboSpan" id="kobo.128.2">By employing the Random Forest classifier, a robust and widely used algorithm from the scikit-learn library, the script endeavors to create an efficient model capable of accurately classifying real-time </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">audio input.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">The real-world applications of this voice classification system are extensive, ranging from enhancing user experiences in voice-controlled smart devices to enabling automated voice commands in robotics. </span><span class="koboSpan" id="kobo.130.2">Industries such as telecommunications, customer service, and security can leverage real-time voice classification to enhance communication systems, automate processes, and bolster </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">security protocols.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">Whether it involves voice-activated virtual assistants, hands-free communication in automobiles, or voice-based authentication systems, the ability to classify and understand spoken language in real time is pivotal. </span><span class="koboSpan" id="kobo.132.2">This script provides a foundational understanding of the implementation process, laying the groundwork for developers and enthusiasts to integrate similar voice classification mechanisms into their projects and contribute to the evolution of voice-centric applications in the </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">real world.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">Let’s see the Python script that demonstrates a real-time voice classification system, using the Random Forest classifier from scikit-learn. </span><span class="koboSpan" id="kobo.134.2">The goal is to capture audio samples, distinguish between positive samples (voice) and negative samples (background noise or non-voice), and train a model for </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">voice classification.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Let’s us see the </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.137.1">Python code that provides a simple framework to build a real-time voice classification system, allowing you to collect your own voice samples to train and test </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">the model:</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.139.1">Import the Python libraries</span></strong><span class="koboSpan" id="kobo.140.1">: First, let’s import the requisite libraries using the following </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">code snippet:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.142.1">
import numpy as np
import sounddevice as sd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.143.1">Capture audio samples</span></strong><span class="koboSpan" id="kobo.144.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">capture_audio</span></strong><span class="koboSpan" id="kobo.146.1"> function uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">sounddevice</span></strong><span class="koboSpan" id="kobo.148.1"> library to record real-time audio. </span><span class="koboSpan" id="kobo.148.2">The user is prompted to speak, and the function captures audio for a specified duration (the default is </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">five seconds):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.150.1">
# Function to capture real-time audio
def capture_audio(duration=5, sampling_rate=44100):
    print("Recording...")
    audio_data = sd.rec(int(sampling_rate * duration), \
        samplerate=sampling_rate, channels=1, dtype='int16')
    sd.wait()
    return audio_data.flatten()</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.151.1">Collect training data</span></strong><span class="koboSpan" id="kobo.152.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">collect_training_data</span></strong><span class="koboSpan" id="kobo.154.1"> function gathers training data for voice and non-voice samples. </span><span class="koboSpan" id="kobo.154.2">For positive samples (voice), the user is prompted to speak, and audio data is recorded using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">capture_audio</span></strong><span class="koboSpan" id="kobo.156.1"> function. </span><span class="koboSpan" id="kobo.156.2">For negative samples (background noise or non-voice), the user is prompted to create ambient noise </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">without speaking:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.158.1">
# Function to collect training data
def collect_training_data(num_samples=10, label=0):
    X = []
    y = []
    for _ in range(num_samples):
        input("Press Enter and speak for a few seconds...")
    audio_sample = capture_audio()
    X.append(audio_sample)
    y.append(label)
    return np.vstack(X), np.array(y)
# Main program
class VoiceClassifier:
    def __init__(self):
        self.model = RandomForestClassifier()
    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)
    def predict(self, X_test):
        return self.model.predict(X_test)
# Collect positive samples (voice)
positive_X, positive_y = collect_training_data(num_samples=10, label=1)
# Collect negative samples (background noise or non-voice)
negative_X, negative_y = collect_training_data(num_samples=10, label=0)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.159.1">Combine and shuffle data</span></strong><span class="koboSpan" id="kobo.160.1">: Positive</span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.161.1"> and negative samples are combined into feature vectors (</span><strong class="source-inline"><span class="koboSpan" id="kobo.162.1">X</span></strong><span class="koboSpan" id="kobo.163.1">) and corresponding labels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">y</span></strong><span class="koboSpan" id="kobo.165.1">). </span><span class="koboSpan" id="kobo.165.2">The data is shuffled to ensure a balanced distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.167.1">
# Combine and shuffle the data
X = np.vstack([positive_X, negative_X])
y = np.concatenate([positive_y, negative_y])</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.168.1">Split data into training and testing sets</span></strong><span class="koboSpan" id="kobo.169.1">: The data is split into training and testing sets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">train_test_split</span></strong><span class="koboSpan" id="kobo.171.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">from scikit-learn:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.173.1">
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, \
    test_size=0.2, random_state=42)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.174.1">Train the voice classifier</span></strong><span class="koboSpan" id="kobo.175.1">: A </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">VoiceClassifier</span></strong><span class="koboSpan" id="kobo.177.1"> class is defined, encapsulating the random forest model. </span><span class="koboSpan" id="kobo.177.2">An instance of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">VoiceClassifier</span></strong><span class="koboSpan" id="kobo.179.1"> is created, and the model is trained using the positive and negative </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.181.1">
# Train the voice classifier model
voice_classifier = VoiceClassifier()
voice_classifier.train(X_train, y_train)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.182.1">Make predictions</span></strong><span class="koboSpan" id="kobo.183.1">: The trained model predicts labels for the </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">test set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.185.1">
# Make predictions on the test set
predictions = voice_classifier.predict(X_test)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.186.1">Evaluate model accuracy</span></strong><span class="koboSpan" id="kobo.187.1">: The </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.188.1">accuracy of the model is evaluated using scikit-learn’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">accuracy_score</span></strong><span class="koboSpan" id="kobo.190.1"> function, comparing predicted labels with </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">actual labels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.192.1">
# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {accuracy * 100:.2f}%")</span></pre> <p><span class="koboSpan" id="kobo.193.1">When you run this code, it prompts with the following pop-up window to enter </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">and speak.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.195.1"><img alt="" role="presentation" src="image/B18944_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.196.1">Figure 11.1 – Prompt to start speaking</span></p>
<p><span class="koboSpan" id="kobo.197.1">Then, you speak a few sentences that will </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">be recorded:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<span class="koboSpan" id="kobo.199.1"><img alt="" role="presentation" src="image/B18944_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.200.1">Figure 11.2 – Trained model accuracy</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.201.1">Inference</span></strong><span class="koboSpan" id="kobo.202.1">: Let’s see the practical application of a pre-trained voice classification model for real-time voice inference. </span><span class="koboSpan" id="kobo.202.2">Leveraging the scikit-learn library’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">RandomForestClassifier</span></strong><span class="koboSpan" id="kobo.204.1">, the model was previously trained to discern between positive samples (voice) and negative samples (non-voice or </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">background noise).</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">The </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.207.1">primary objective of this script is to demonstrate the seamless integration of the pre-trained voice classification model into a real-time voice inference system. </span><span class="koboSpan" id="kobo.207.2">You are prompted to provide audio input by pressing </span><em class="italic"><span class="koboSpan" id="kobo.208.1">Enter</span></em><span class="koboSpan" id="kobo.209.1"> and speaking for a few seconds, after which the model predicts whether the input contains human speech or </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">non-voice elements:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.211.1">
import joblib
# Save the trained model during training
joblib.dump(voice_classifier, "voice_classifier_model.pkl")
import numpy as np
import sounddevice as sd
from sklearn.ensemble import RandomForestClassifier
#from sklearn.externals import joblib # For model persistence
# Load the pre-trained model
voice_classifier = joblib.load("voice_classifier_model.pkl")
# Function to capture real-time audio
def capture_audio(duration=5, sampling_rate=44100):
    print("Recording...")
    audio_data = sd.rec(int(sampling_rate * duration), \
        samplerate=sampling_rate, channels=1, dtype='int16')
    sd.wait()
    return audio_data.flatten()
# Function to predict voice using the trained model
def predict_voice(audio_sample):
    prediction = voice_classifier.predict([audio_sample])
    return prediction[0]
# Main program for real-time voice classification
def real_time_voice_classification():
    while True:
        input("Press Enter and speak for a few seconds...")
    # Capture new audio
        new_audio_sample = capture_audio()
    # Predict if it's voice or non-voice
        result = predict_voice(new_audio_sample)
    if result == 1:
        print("Voice detected!")
    else:
        print("Non-voice detected.")
if __name__ == "__main__":
    real_time_voice_classification()</span></pre> <p><span class="koboSpan" id="kobo.212.1">The </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.213.1">output is </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<span class="koboSpan" id="kobo.215.1"><img alt="" role="presentation" src="image/B18944_11_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.216.1">Figure 11.3 – Inference output</span></p>
<p><span class="koboSpan" id="kobo.217.1">In a similar manner, we can use this model to label a voice as male or female to analyze customer calls and understand </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">target customers.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">We have seen the real-time voice classification inference that holds significant relevance in numerous scenarios, including voice-activated applications, security systems, and communication devices. </span><span class="koboSpan" id="kobo.219.2">By loading a pre-trained model, users can experience the instantaneous and accurate classification of voice in </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">real-world situations.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">Whether applied to enhance accessibility features, automate voice commands, or implement voice-based security protocols, this script serves as a practical example of deploying machine learning models for voice classification in real-time scenarios. </span><span class="koboSpan" id="kobo.221.2">As technology continues to advance, the seamless integration of voice inference models contributes</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.222.1"> to the evolution of user-friendly and responsive applications across </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">various domains.</span></span></p>
<p><span class="koboSpan" id="kobo.224.1">Now, let’s see how to transcribe audio using the OpenAI </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">Whisper model.</span></span></p>
<h1 id="_idParaDest-249"><a id="_idTextAnchor254"/><span class="koboSpan" id="kobo.226.1">Transcribing audio using the OpenAI Whisper model</span></h1>
<p><span class="koboSpan" id="kobo.227.1">In this section, we are going to see how to transcribe audio file to text using the </span><strong class="bold"><span class="koboSpan" id="kobo.228.1">OpenAI Whisper</span></strong><span class="koboSpan" id="kobo.229.1"> model</span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.230.1"> and then label the audio transcription using the </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.231.1">OpenAI </span><strong class="bold"><span class="koboSpan" id="kobo.232.1">large language </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.233.1">model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.234.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.235.1">LLM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.237.1">Whisper</span></strong><span class="koboSpan" id="kobo.238.1"> is an open</span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.239.1"> source ASR model developed by OpenAI. </span><span class="koboSpan" id="kobo.239.2">It is trained on nearly 700,000 hours of multilingual speech data and is capable of transcribing audio to text in almost 100 different languages. </span><span class="koboSpan" id="kobo.239.3">According to OpenAI, Whisper “approaches human level robustness and accuracy on English </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">speech recognition.”</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">In a recent </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.242.1">benchmark study, Whisper was compared to other open source ASR models, such as wav2vec 2.0 and Kaldi. </span><span class="koboSpan" id="kobo.242.2">The study found that Whisper performed better than wav2vec 2.0 in terms of accuracy and speed across five different use cases, including conversational AI, phone calls, meetings, videos, and </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">earnings calls.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">Whisper is also known for its affordability, accuracy, and features. </span><span class="koboSpan" id="kobo.244.2">It is best suited for audio-to-text use cases and is not well-suited for text-to-audio or speech </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">synthesis tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.246.1">The</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.247.1"> Whisper model can be imported as a Python library. </span><span class="koboSpan" id="kobo.247.2">The other option is to use the Whisper model available in the model catalog</span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.248.1"> at </span><strong class="bold"><span class="koboSpan" id="kobo.249.1">Azure Machine </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.250.1">Learning studio</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">Let’s see the process of transcribing audio using the OpenAI Whisper ASR using the Python library now. </span><span class="koboSpan" id="kobo.252.2">It’s crucial to ensure the existence and accessibility of the specified audio file for successful transcription. </span><span class="koboSpan" id="kobo.252.3">The transcribed text is likely stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">text['text']</span></strong><span class="koboSpan" id="kobo.254.1">, as indicated by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">print</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.256.1"> statement.</span></span></p>
<p><span class="koboSpan" id="kobo.257.1">First, we need to install the whisper model, as mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.258.1">technical requirements</span></em><span class="koboSpan" id="kobo.259.1"> section. </span><span class="koboSpan" id="kobo.259.2">Then, we import the OpenAI </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">Whisper model.</span></span></p>
<h2 id="_idParaDest-250"><a id="_idTextAnchor255"/><span class="koboSpan" id="kobo.261.1">Step 1 – importing the Whisper model</span></h2>
<p><span class="koboSpan" id="kobo.262.1">Let us import the </span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.263.1">required </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">Python libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.265.1">
import whisper
Import pytube</span></pre> <p><span class="koboSpan" id="kobo.266.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">whisper</span></strong><span class="koboSpan" id="kobo.268.1"> library is imported, which is the library providing access to the OpenAI Whisper ASR model. </span><span class="koboSpan" id="kobo.268.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">pytube</span></strong><span class="koboSpan" id="kobo.270.1"> library is imported to download </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">YouTube videos.</span></span></p>
<h2 id="_idParaDest-251"><a id="_idTextAnchor256"/><span class="koboSpan" id="kobo.272.1">Step 2 – loading the base Whisper model</span></h2>
<p><span class="koboSpan" id="kobo.273.1">Let us load the</span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.274.1"> base </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">Whisper model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.276.1">
model = whisper.load_model("base")</span></pre> <p><span class="koboSpan" id="kobo.277.1">The Whisper model is loaded using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">whisper.load_model</span></strong><span class="koboSpan" id="kobo.279.1"> function with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">"base"</span></strong><span class="koboSpan" id="kobo.281.1"> argument. </span><span class="koboSpan" id="kobo.281.2">This loads the base version of the Whisper </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">ASR model.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">Let us </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.284.1">download the audio stream from a YouTube video. </span><span class="koboSpan" id="kobo.284.2">Even though we are using a video file, we are only focusing on the audio of  the YouTube video and downloading an audio stream from it. </span><span class="koboSpan" id="kobo.284.3">Alternatively, you can directly use any </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">audio file:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.286.1">
#we are importing Pytube library
import pytube
#we are downloading YouTube video from YouTube link
video = "https://youtu.be/g8Q452PEXwY"
data = pytube.YouTube(video)</span></pre> <p><span class="koboSpan" id="kobo.287.1">The YouTube video URL is specified. </span><span class="koboSpan" id="kobo.287.2">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">pytube.YouTube</span></strong><span class="koboSpan" id="kobo.289.1"> class, the video data </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">is fetched:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.291.1">
# Converting and downloading as 'MP4' file
audio = data.streams.get_audio_only()
audio.download()</span></pre> <p><span class="koboSpan" id="kobo.292.1">This code utilizes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">pytube</span></strong><span class="koboSpan" id="kobo.294.1"> library to download the audio stream from a video hosted on a platform such as YouTube. </span><span class="koboSpan" id="kobo.294.2">Let’s examine the preceding </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">code snippet:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">audio = data.streams.get_audio_only()</span></strong><span class="koboSpan" id="kobo.297.1">: This line fetches the audio stream of the video. </span><span class="koboSpan" id="kobo.297.2">It uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">get_audio_only() </span></strong><span class="koboSpan" id="kobo.299.1">method to obtain a stream containing only the </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">audio content.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">audio.download()</span></strong><span class="koboSpan" id="kobo.302.1">: Once the audio stream is obtained, this line downloads the audio content. </span><span class="koboSpan" id="kobo.302.2">The download is performed in the default format, which is typically an MP4 file containing only the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">audio data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.304.1">In summary, the </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.305.1">code extracts the audio stream from a video and downloads it as an MP4 file, preserving only the </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">audio content.</span></span></p>
<h2 id="_idParaDest-252"><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.307.1">Step 3 – setting up FFmpeg</span></h2>
<p><span class="koboSpan" id="kobo.308.1">Whisper is designed to</span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.309.1"> transcribe audio, but it requires a specific format for processing. </span><span class="koboSpan" id="kobo.309.2">The format required by Whisper for processing audio is WAV format. </span><span class="koboSpan" id="kobo.309.3">Whisper is designed to transcribe audio in WAV format, and it may not directly support other formats. </span><span class="koboSpan" id="kobo.309.4">Therefore, audio data that needs to be processed by Whisper should be provided in the WAV format. </span><span class="koboSpan" id="kobo.309.5">FFmpeg acts as a bridge by converting various audio formats (such as MP3, WAV, or AAC) into a format that Whisper </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">can handle.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">For example, if the input is in the MP3 format, FFmpeg can convert it to a format suitable for Whisper. </span><span class="koboSpan" id="kobo.311.2">Whisper typically requires audio data in WAV format, so FFmpeg can convert the input MP3 file to WAV during the process. </span><span class="koboSpan" id="kobo.311.3">This conversion allows the audio data to be in a format compatible with the requirements of the </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">Whisper model.</span></span></p>
<p><span class="koboSpan" id="kobo.313.1">Without this conversion, Whisper wouldn’t be able to process the </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">audio effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.315.1">In scenarios</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.316.1"> where real-time transcription is needed (such as streaming a </span><strong class="bold"><span class="koboSpan" id="kobo.317.1">real-time messaging protocol</span></strong><span class="koboSpan" id="kobo.318.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.319.1">RTMP</span></strong><span class="koboSpan" id="kobo.320.1">) feed), FFmpeg </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.321.1">helps segment the audio stream. </span><span class="koboSpan" id="kobo.321.2">It splits the continuous audio into smaller chunks (e.g., 30-second segments) that can be processed individually. </span><span class="koboSpan" id="kobo.321.3">Each segment is then passed to Whisper </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">for transcription:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.323.1">
# Set the FFMPEG environment variable to the path of your ffmpeg executable
os.environ['PATH'] = '/&lt;your_path&gt;/audio-orchestrator-ffmpeg/bin:' + os.environ['PATH']</span></pre> <p><span class="koboSpan" id="kobo.324.1">The code sets the FFmpeg environment variable to the path of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">ffmpeg</span></strong><span class="koboSpan" id="kobo.326.1"> executable. </span><span class="koboSpan" id="kobo.326.2">This is necessary for handling audio and </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">video files.</span></span></p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.328.1">Step 4 – transcribing the YouTube audio using the Whisper model</span></h2>
<p><span class="koboSpan" id="kobo.329.1">Now, let’s </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.330.1">transcribe the YouTube audio using the </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">Whisper model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.332.1">
model = whisper.load_model('base')
text = model.transcribe('Mel Spectrograms with Python and Librosa Audio Feature Extraction.mp4')
#printing the transcribe
text['text']</span></pre> <p><span class="koboSpan" id="kobo.333.1">Here's </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<span class="koboSpan" id="kobo.335.1"><img alt="" role="presentation" src="image/B18944_11_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 11.4 – A snippet of the code output</span></p>
<p><span class="koboSpan" id="kobo.337.1">The </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.338.1">Whisper model is loaded again to ensure that it uses the base model. </span><span class="koboSpan" id="kobo.338.2">The transcribe function is called on the model with the filename of the audio file as an argument. </span><span class="koboSpan" id="kobo.338.3">The resulting transcribed text is printed </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">text['text']</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.342.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.343.1">The provided filename in </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">model.transcribe</span></strong><span class="koboSpan" id="kobo.345.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">Mel Spectrograms with Python and Librosa Audio Feature Extraction.mp4</span></strong><span class="koboSpan" id="kobo.347.1">. </span><span class="koboSpan" id="kobo.347.2">Make sure this file exists and is accessible for the code to transcribe successfully. </span></p>
<p><span class="koboSpan" id="kobo.348.1">Now, let’s see another code example on how to transcribe an audio file </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">to text:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.350.1">
model = whisper.load_model('base')
text = model.transcribe('/Users/&lt;username&gt;/PacktPublishing/DataLabeling/Ch11/customer_call_audio.m4a')
#printing the transcribe
text['text']</span></pre> <p><span class="koboSpan" id="kobo.351.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">the output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.353.1">
' Hello, I have not received the product yet. </span><span class="koboSpan" id="kobo.353.2">I am very disappointed. </span><span class="koboSpan" id="kobo.353.3">Are you going to replace if my product is damaged or missed? </span><span class="koboSpan" id="kobo.353.4">I will be happy if you replace with your product in case I miss the product due to incorrect shipping address.'</span></pre> <p><span class="koboSpan" id="kobo.354.1">Now, let’s perform sentiment analysis to label this text transcribed from a </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">customer call.</span></span></p>
<h2 id="_idParaDest-254"><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.356.1">Classifying a transcription using Hugging Face transformers</span></h2>
<p><span class="koboSpan" id="kobo.357.1">Now, let’s use Hugging</span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.358.1"> Face transformers </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.359.1">to classify the output text from the previous customer call audio transcription and perform sentiment analysis to </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">label it.</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">The following code snippet utilizes Hugging Face’s transformers library to perform sentiment analysis on a given text. </span><span class="koboSpan" id="kobo.361.2">It begins by importing the necessary module, and then it loads a pre-trained sentiment analysis pipeline from Hugging Face’s transformers. </span><span class="koboSpan" id="kobo.361.3">The code defines an example text that expresses dissatisfaction with a product not yet received. </span><span class="koboSpan" id="kobo.361.4">Subsequently, the sentiment analysis pipeline is applied to classify the sentiment of the text, and the result is displayed by printing it to the console. </span><span class="koboSpan" id="kobo.361.5">The sentiment analysis model outputs a label indicating the sentiment, such as positive or negative, along with a </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">confidence score.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">Let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">Python library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.365.1">
from transformers import pipeline
# Load the sentiment analysis pipeline
sentiment_classifier = pipeline('sentiment-analysis')
# text for sentiment analysis
text="Hello, I have not received the product yet. </span><span class="koboSpan" id="kobo.365.2">I am very disappointed.are you going to replace if my product is damaged or missed.I will be happy if you replace with new product incase i missed the product die to incorrect shipping address"
# Perform sentiment analysis
result = sentiment_classifier(text)
# Display the result
print(result)</span></pre> <p><span class="koboSpan" id="kobo.366.1">Here </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.367.1">is </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">the </span></span><span class="No-Break"><a id="_idIndexMarker1071"/></span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.370.1">
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). </span><span class="koboSpan" id="kobo.370.2">Using a pipeline without specifying a model name and revision in production is not recommended.
</span><span class="koboSpan" id="kobo.370.3">[{'label': 'NEGATIVE', 'score': 0.9992625117301941}]</span></pre> <h1 id="_idParaDest-255"><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.371.1">Hands-on – labeling audio data using a CNN</span></h1>
<p><span class="koboSpan" id="kobo.372.1">In this section, we will </span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.373.1">see how to </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.374.1">train the CNN network on audio data and use it to label the </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">audio data.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">The following code demonstrates the process of labeling audio data using a CNN. </span><span class="koboSpan" id="kobo.376.2">The code outlines how to employ a CNN to label audio data, specifically training the model on a dataset of cat and dog audio samples. </span><span class="koboSpan" id="kobo.376.3">The goal is to classify new, unseen audio data as either a cat or a dog. </span><span class="koboSpan" id="kobo.376.4">Let’s take the cat and dog sample audio data and train the CNN model. </span><span class="koboSpan" id="kobo.376.5">Then, we will send new unseen data to the model to predict whether it is a cat or </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">a dog:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.378.1">Load and pre-process the data</span></strong><span class="koboSpan" id="kobo.379.1">: The audio data for cats and dogs is loaded from the specified folder structure using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">load_and_preprocess_data</span></strong><span class="koboSpan" id="kobo.381.1"> function. </span><span class="koboSpan" id="kobo.381.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">load_and_preprocess_data</span></strong><span class="koboSpan" id="kobo.383.1"> function processes the audio data, converting it into mel spectrograms and resizing them for </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">model compatibility.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Split data into training and testing sets</span></strong><span class="koboSpan" id="kobo.386.1">: The loaded and pre-processed data is split into training and testing sets using </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">train_test_split</span></strong><span class="koboSpan" id="kobo.388.1">, and labels are converted to </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">one-hot encoding.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.390.1">Create a neural network model</span></strong><span class="koboSpan" id="kobo.391.1">: A CNN model is created using TensorFlow and Keras, comprising convolutional layers, pooling layers, and fully </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">connected layers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.393.1">Compile the model</span></strong><span class="koboSpan" id="kobo.394.1">: The model is compiled with an Adam optimizer, categorical cross-entropy loss, and accuracy as the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">evaluation metric.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.396.1">Train the model</span></strong><span class="koboSpan" id="kobo.397.1">: The CNN model is trained on the training data for a specified number </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">of epochs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.399.1">Evaluate the accuracy of the model</span></strong><span class="koboSpan" id="kobo.400.1">: The accuracy of the trained model is evaluated on the </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">testing set.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.402.1">Save the trained model</span></strong><span class="koboSpan" id="kobo.403.1">: The trained model is saved for </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">future use.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.405.1">Test the new audio file</span></strong><span class="koboSpan" id="kobo.406.1">: Finally, the saved model is loaded, and a new audio file (in this case, a cat meow) is processed and classified, with class probabilities and accuracy displayed for </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">each class.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.408.1">In summary, this</span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.409.1"> code provides</span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.410.1"> a comprehensive guide on using a CNN to label audio data, from data loading and preprocessing to model training, evaluation, and prediction on new </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">audio samples.</span></span></p>
<p><span class="koboSpan" id="kobo.412.1">Let’s import all the required Python modules as the </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">first step:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.414.1">
import os
import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.image import resize
from tensorflow.keras.models import load_model</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.415.1">Step 1: Load and preprocess data</span></strong><span class="koboSpan" id="kobo.416.1">: Now, let’s load and pre-process the data for cats and dogs. </span><span class="koboSpan" id="kobo.416.2">The</span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.417.1"> source of this </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.418.1">dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">is </span></span><a href="https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs"><span class="No-Break"><span class="koboSpan" id="kobo.420.1">https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.421.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.422.1">
# Define your folder structure
data_dir = '../cats_dogs/data/'
classes = ['cat', 'dog']</span></pre> <p><span class="koboSpan" id="kobo.423.1">This code establishes the folder structure for a dataset containing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">'cat'</span></strong><span class="koboSpan" id="kobo.425.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">'dog'</span></strong><span class="koboSpan" id="kobo.427.1">, categories, with the data located at the specified directory, </span><strong class="source-inline"><span class="koboSpan" id="kobo.428.1">'../cats_dogs/data/'</span></strong><span class="koboSpan" id="kobo.429.1">. </span><span class="koboSpan" id="kobo.429.2">Next, let’s pre-process </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.431.1">
# define the function for Load and preprocess audio data
def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):
data = []
labels = []
for i, class_name in enumerate(classes):
    class_dir = os.path.join(data_dir, class_name)
    for filename in os.listdir(class_dir):
        if filename.endswith('.wav'):
            file_path = os.path.join(class_dir, filename)
            audio_data, sample_rate = librosa.load(file_path, sr=None)
            # Perform preprocessing (e.g., convert to Mel spectrogram and resize)
            mel_spectrogram = \
                librosa.feature.melspectrogram( \
                    y=audio_data, sr=sample_rate)
            mel_spectrogram = resize( \
                np.expand_dims(mel_spectrogram, axis=-1), \
                target_shape)
            print(mel_spectrogram)
            data.append(mel_spectrogram)
        labels.append(i)
return np.array(data), np.array(labels)</span></pre> <p><span class="koboSpan" id="kobo.432.1">This code </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.433.1">defines a function </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.434.1">named </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">load_and_preprocess_data</span></strong><span class="koboSpan" id="kobo.436.1"> that loads and preprocesses audio data from a specified directory. </span><span class="koboSpan" id="kobo.436.2">It iterates through each class of audio, reads </span><strong class="source-inline"><span class="koboSpan" id="kobo.437.1">.wav</span></strong><span class="koboSpan" id="kobo.438.1"> files, and uses the Librosa library to convert the audio data into a mel spectrogram. </span><span class="koboSpan" id="kobo.438.2">We learned about the mel spectrogram in </span><a href="B18944_10.xhtml#_idTextAnchor221"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.440.1"> in the </span><em class="italic"><span class="koboSpan" id="kobo.441.1">Visualizing audio data with Matplotlib and Librosa – spectrogram </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.442.1">visualization</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.443.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">The mel spectrogram is then resized to a target shape (128x128) before being appended to the data list, along with the corresponding class labels. </span><span class="koboSpan" id="kobo.444.2">The function returns the preprocessed data and labels as </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">NumPy arrays.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.446.1">Step 2: Split data into training and testing sets</span></strong><span class="koboSpan" id="kobo.447.1">: This code segment divides the preprocessed audio data and corresponding labels into training and testing sets. </span><span class="koboSpan" id="kobo.447.2">It utilizes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.448.1">load_and_preprocess_data</span></strong><span class="koboSpan" id="kobo.449.1"> function to load and preprocess the data. </span><span class="koboSpan" id="kobo.449.2">The labels are then converted into one-hot encoding using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">to_categorical</span></strong><span class="koboSpan" id="kobo.451.1"> function. </span><span class="koboSpan" id="kobo.451.2">Finally, the data is split into training and testing sets with an 80–20 ratio using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">train_test_split</span></strong><span class="koboSpan" id="kobo.453.1"> function, ensuring reproducibility with a specified </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">random seed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.455.1">
# Split data into training and testing sets
data, labels = load_and_preprocess_data(data_dir, classes)
labels = to_categorical(labels, num_classes=len(classes)) # Convert labels to one-hot encoding
X_train, X_test, y_train, y_test = train_test_split(data, \
    labels, test_size=0.2, random_state=42)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.456.1">Step 3: Create a neural network model</span></strong><span class="koboSpan" id="kobo.457.1">: This code defines a neural network model for audio classification. </span><span class="koboSpan" id="kobo.457.2">The model architecture includes convolutional layers with max pooling for feature extraction, followed by a flattening layer. </span><span class="koboSpan" id="kobo.457.3">Subsequently, there is a dense layer with ReLU activation for further feature processing. </span><span class="koboSpan" id="kobo.457.4">The final output layer utilizes softmax activation to produce class probabilities. </span><span class="koboSpan" id="kobo.457.5">The model is constructed using the Keras </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.458.1">functional API, specifying the</span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.459.1"> input and output layers, and is ready to be trained on the </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">provided data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.461.1">
# Create a neural network model
input_shape = X_train[0].shape
input_layer = Input(shape=input_shape)
x = Conv2D(32, (3, 3), activation='relu')(input_layer)
x = MaxPooling2D((2, 2))(x)
x = Conv2D(64, (3, 3), activation='relu')(x)
x = MaxPooling2D((2, 2))(x)
x = Flatten()(x)
x = Dense(64, activation='relu')(x)
output_layer = Dense(len(classes), activation='softmax')(x)
model = Model(input_layer, output_layer)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.462.1">Step 4: Compile the model</span></strong><span class="koboSpan" id="kobo.463.1">: This code compiles the previously defined neural network model using the Adam optimizer, with a learning rate of </span><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">0.001</span></strong><span class="koboSpan" id="kobo.465.1">, categorical cross-entropy as the loss function (suitable for multi-class classification), and accuracy as the evaluation metric. </span><span class="koboSpan" id="kobo.465.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">model.summary()</span></strong><span class="koboSpan" id="kobo.467.1"> command provides a concise overview of the model’s architecture, including the number of parameters and the structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">each layer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), \
    loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()</span></pre> <p><span class="koboSpan" id="kobo.470.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">the</span></span><span class="No-Break"><a id="_idIndexMarker1082"/></span><span class="No-Break"><span class="koboSpan" id="kobo.472.1"> output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<span class="koboSpan" id="kobo.473.1"><img alt="" role="presentation" src="image/B18944_11_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.474.1">Figure 11.5 – Model summary</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.475.1">Step 5: Train the model</span></strong><span class="koboSpan" id="kobo.476.1">: This code initiates the training of the neural network model using the</span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.477.1"> training data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">X_train</span></strong><span class="koboSpan" id="kobo.479.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">y_train</span></strong><span class="koboSpan" id="kobo.481.1">) for </span><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">20</span></strong><span class="koboSpan" id="kobo.483.1"> epochs, with a batch size of </span><strong class="source-inline"><span class="koboSpan" id="kobo.484.1">32</span></strong><span class="koboSpan" id="kobo.485.1">. </span><span class="koboSpan" id="kobo.485.2">The validation data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.486.1">X_test</span></strong><span class="koboSpan" id="kobo.487.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.488.1">y_test</span></strong><span class="koboSpan" id="kobo.489.1">) is used to evaluate the model’s performance </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.491.1">
# Train the model
model.fit(X_train, y_train, epochs=20, batch_size=32, \
    validation_data=(X_test, y_test))</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.492.1">Step 6: Test the accuracy of the model</span></strong><span class="koboSpan" id="kobo.493.1">: After training completion, it calculates the test accuracy of the model on the separate test set and prints the accuracy score, providing </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.494.1">insights into the</span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.495.1"> model’s effectiveness in classifying </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">audio data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.497.1">
#Test  the accuracy of model
test_accuracy=model.evaluate(X_test,y_test,verbose=0)
print(test_accuracy[1])</span></pre> <p><span class="koboSpan" id="kobo.498.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.500.1"><img alt="" role="presentation" src="image/B18944_11_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.501.1">Figure 11.6 –  Accuracy of the model</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.502.1">Step 7: Save the </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.503.1">trained model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.505.1">
# Save the model
model.save('audio_classification_model.h5')</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.506.1">Step 8: Test the new audio file</span></strong><span class="koboSpan" id="kobo.507.1">: Let’s classify the new audio file and label it using this </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">saved model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.509.1">
# Load the saved model
model = load_model('audio_classification_model.h5')
# Define the target shape for input spectrograms
target_shape = (128, 128)
# Define your class labels
classes = ['cat', 'dog']
# Function to preprocess and classify an audio file
def test_audio(file_path, model):
# Load and preprocess the audio file
audio_data, sample_rate = librosa.load(file_path, sr=None)
mel_spectrogram = librosa.feature.melspectrogram( \
    y=audio_data, sr=sample_rate)
mel_spectrogram = resize(np.expand_dims(mel_spectrogram, \
    axis=-1), target_shape)
mel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))</span></pre> <p><span class="koboSpan" id="kobo.510.1">This code </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.511.1">defines a function, </span><strong class="source-inline"><span class="koboSpan" id="kobo.512.1">test_audio</span></strong><span class="koboSpan" id="kobo.513.1">, to preprocess and classify an audio file. </span><span class="koboSpan" id="kobo.513.2">It loads and preprocesses the audio data from the specified file path using Librosa, generating a mel spectrogram. </span><span class="koboSpan" id="kobo.513.3">The spectrogram is then resized and reshaped to match the input dimensions expected by the model. </span><span class="koboSpan" id="kobo.513.4">This function is designed to prepare audio data for classification using </span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.514.1">a neural network model, providing a streamlined way to apply the trained model to new audio files </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">for prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.516.1">Now, let’s make </span><strong class="bold"><span class="koboSpan" id="kobo.517.1">predictions</span></strong><span class="koboSpan" id="kobo.518.1">. </span><span class="koboSpan" id="kobo.518.2">In this </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.519.1">code segment, predictions are made using the trained neural network model on a specific audio file (</span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">./cat-meow-14536.mp3</span></strong><span class="koboSpan" id="kobo.521.1">). </span><span class="koboSpan" id="kobo.521.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">test_audio</span></strong><span class="koboSpan" id="kobo.523.1"> function is employed to preprocess the audio file and obtain class probabilities and the predicted class index. </span><span class="koboSpan" id="kobo.523.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">model.predict</span></strong><span class="koboSpan" id="kobo.525.1"> method generates predictions, and the class probabilities are extracted from the result. </span><span class="koboSpan" id="kobo.525.2">The predicted class index is determined by identifying the class with the highest probability. </span><span class="koboSpan" id="kobo.525.3">This process demonstrates how the trained model can be utilized to classify new audio data, providing insights into the content of the tested </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">audio file:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.527.1">
# Make predictions
predictions = model.predict(mel_spectrogram)
# Get the class probabilities
class_probabilities = predictions[0]
# Get the predicted class index
predicted_class_index = np.argmax(class_probabilities)
return class_probabilities, predicted_class_index
# Test an audio filetest_audio_file = '../Ch10/cat-meow-14536.mp3'
class_probabilities, predicted_class_index = test_audio( \
    test_audio_file, model)</span></pre> <p><span class="koboSpan" id="kobo.528.1">The</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.529.1"> following code </span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.530.1">snippet iterates through all the classes in the model and prints the predicted probabilities for each class, based on the audio file classification. </span><span class="koboSpan" id="kobo.530.2">For each class, it displays the class label and its corresponding probability, providing a comprehensive view of the model’s confidence in assigning the audio file to each </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">specific category:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.532.1">
# Display results for all classes
for i, class_label in enumerate(classes):
    probability = class_probabilities[i]
    print(f'Class: {class_label}, Probability: {probability:.4f}')</span></pre> <p><span class="koboSpan" id="kobo.533.1">The following code calculates and reveals the predicted class and accuracy of the audio file classification. </span><span class="koboSpan" id="kobo.533.2">It identifies the predicted class using the index with the highest probability, retrieves the corresponding class label and accuracy from the results, and then prints the predicted class along with its associated accuracy. </span><span class="koboSpan" id="kobo.533.3">This provides a concise summary of the model’s prediction for the given audio file and the confidence level associated with the classification. </span><span class="koboSpan" id="kobo.533.4">Calculate and display the predicted class </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">and accuracy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.535.1">
predicted_class = classes[predicted_class_index]
accuracy = class_probabilities[predicted_class_index]
print(f'The audio is labeled Spectrogram Visualization
as: {predicted_class}')
print(f'Accuracy: {accuracy:.4f}')</span></pre> <p><span class="koboSpan" id="kobo.536.1">This is the output </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">we get:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.538.1"><img alt="" role="presentation" src="image/B18944_11_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.539.1">Figure 11.7 – Output showing probabilities and accuracy of the model</span></p>
<p><span class="koboSpan" id="kobo.540.1">We have </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.541.1">seen how to</span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.542.1"> transcribe audio data using machine learning. </span><span class="koboSpan" id="kobo.542.2">Now, let’s see how to do audio data augmentation and train a model with augmented data. </span><span class="koboSpan" id="kobo.542.3">Finally, we will see and compare the accuracy with and without </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">augmented data.</span></span></p>
<h1 id="_idParaDest-256"><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.544.1">Exploring audio data augmentation</span></h1>
<p><span class="koboSpan" id="kobo.545.1">Let’s see how to </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.546.1">manipulate audio data by adding noise, </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">using NumPy.</span></span></p>
<p><span class="koboSpan" id="kobo.548.1">Adding noise to audio data during training helps the model become more robust in real-world scenarios, where there might be background noise or interference. </span><span class="koboSpan" id="kobo.548.2">By exposing a model to a variety of noisy conditions, it learns to </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">generalize better.</span></span></p>
<p><span class="koboSpan" id="kobo.550.1">Augmenting audio data with noise prevents a model from memorizing specific patterns in the training data. </span><span class="koboSpan" id="kobo.550.2">This encourages the model to focus on more general features, which can lead to better generalization on </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">unseen data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.552.1">
import numpy as np
def add_noise(data, noise_factor):
    noise = np.random.randn(len(data))
    augmented_data = data + noise_factor * noise
    # Cast back to same data type
    augmented_data = augmented_data.astype(type(data[0]))
    return augmented_data</span></pre> <p><span class="koboSpan" id="kobo.553.1">This code defines a function named </span><strong class="source-inline"><span class="koboSpan" id="kobo.554.1">add_noise</span></strong><span class="koboSpan" id="kobo.555.1"> that adds random noise to an input data array. </span><span class="koboSpan" id="kobo.555.2">The level of noise is controlled by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">noise_factor</span></strong><span class="koboSpan" id="kobo.557.1"> parameter. </span><span class="koboSpan" id="kobo.557.2">The function generates random noise using NumPy, adds it to the original data, and then returns the augmented data. </span><span class="koboSpan" id="kobo.557.3">To ensure the data type consistency, the augmented data is cast back to the same type as the elements in the original data array. </span><span class="koboSpan" id="kobo.557.4">This function can be used for data augmentation, a technique commonly employed in machine learning to enhance model robustness by introducing variations in the </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">training data.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Let’s test this function </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.560.1">using sample audio data, </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.562.1">
# Sample audio data
sample_data = np.array([0.1, 0.2, 0.3, 0.4, 0.5])
# Sample noise factor
sample_noise_factor = 0.05
# Apply augmentation
augmented_data = add_noise(sample_data, sample_noise_factor)
# Print the original and augmented data
print("Original Data:", sample_data)
print("Augmented Data:", augmented_data)</span></pre> <p><span class="koboSpan" id="kobo.563.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.565.1"><img alt="" role="presentation" src="image/B18944_11_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.566.1">Figure 11.8 – Representation of the augmented data</span></p>
<p><span class="koboSpan" id="kobo.567.1">Now, let’s retrain </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.568.1">our CNN model using data augmentation for the classification of dogs and cats sounds that we saw in the </span><em class="italic"><span class="koboSpan" id="kobo.569.1">Hands-on – labeling audio data using a </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.570.1">CNN</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.571.1"> section:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.572.1">
# Load and preprocess audio data
def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):
    data = []
    labels = []
    noise_factor = 0.05
    for i, class_name in enumerate(classes):
    class_dir = os.path.join(data_dir, class_name)
    for filename in os.listdir(class_dir):
    if filename.endswith('.wav'):
    file_path = os.path.join(class_dir, filename)
    audio_data, sample_rate = librosa.load(file_path, sr=None)
    # Apply noise manipulation
    noise = np.random.randn(len(audio_data))
    augmented_data = audio_data + noise_factor * noise
    augmented_data = augmented_data.astype(type(audio_data[0]))
    # Perform preprocessing (e.g., convert to Mel spectrogram and resize)
    mel_spectrogram = librosa.feature.melspectrogram( \
        y=augmented_data, sr=sample_rate)
    mel_spectrogram = resize( \
        np.expand_dims(mel_spectrogram, axis=-1), target_shape)
    print(mel_spectrogram)
    data.append(mel_spectrogram)
    labels.append(i)
    return np.array(data), np.array(labels)</span></pre> <p><span class="koboSpan" id="kobo.573.1">In this code, we introduced data augmentation by adding random noise </span><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">(noise_factor * noise)</span></strong><span class="koboSpan" id="kobo.575.1"> to the audio data before spectrogram conversion. </span><span class="koboSpan" id="kobo.575.2">This helps improve the </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.576.1">model’s robustness by exposing it to varied instances of the same class </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.578.1">
test_accuracy=model.evaluate(X_test,y_test,verbose=0)
print(test_accuracy[1])</span></pre> <p><span class="koboSpan" id="kobo.579.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.581.1"><img alt="" role="presentation" src="image/B18944_11_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.582.1">Figure 11.9 – Accuracy of the model</span></p>
<p><span class="koboSpan" id="kobo.583.1">By using this </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.584.1">noise-augmented audio data, the model accuracy increased from 0.946 to 0.964. </span><span class="koboSpan" id="kobo.584.2">Depending on the data, we can apply data augmentation and test the accuracy to decide whether data augmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">is required.</span></span></p>
<p><span class="koboSpan" id="kobo.586.1">Let’s see three more data augmentation techniques applied to an original audio file – time-stretching, pitch-shifting, and dynamic </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">range compression.</span></span></p>
<p><span class="koboSpan" id="kobo.588.1">The following Python script employs the librosa library for audio processing, loading an initial audio file that serves as the baseline for augmentation. </span><span class="koboSpan" id="kobo.588.2">Subsequently, functions are defined to apply each augmentation technique independently. </span><span class="koboSpan" id="kobo.588.3">Time-stretching alters the temporal duration of the audio, pitch-shifting modifies the pitch without affecting speed, and dynamic range compression adjusts the </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">volume dynamics.</span></span></p>
<p><span class="koboSpan" id="kobo.590.1">The augmented waveforms are visually presented side by side with the original waveform using Matplotlib. </span><span class="koboSpan" id="kobo.590.2">This visualization aids in understanding the transformative impact of each augmentation technique on the audio data. </span><span class="koboSpan" id="kobo.590.3">Through this script, you will gain insights into the practical implementation of audio augmentation, a valuable practice for creating diverse and robust datasets for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.592.1">As audio data </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.593.1">labeling becomes increasingly integral to various applications, mastering the art of augmentation ensures the generation of comprehensive datasets, thereby enhancing the effectiveness of machine learning models. </span><span class="koboSpan" id="kobo.593.2">Whether applied to speech recognition, sound classification, or voice-enabled applications, audio augmentation is a powerful technique for refining and enriching </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">audio datasets:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.595.1">
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
from scipy.io.wavfile import write
# Load the audio file
audio_file_path = "../ch10/cats_dogs/cat_1.wav"
y, sr = librosa.load(audio_file_path)
# Function for time stretching augmentation
def time_stretching(y, rate):
    return librosa.effects.time_stretch(y, rate=rate)
# Function for pitch shifting augmentation
def pitch_shifting(y, sr, pitch_factor):
    return librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_factor)
# Function for dynamic range compression augmentation
def dynamic_range_compression(y, compression_factor):
    return y * compression_factor
# Apply dynamic range compression augmentation
compression_factor = 0.5 # Adjust as needed
y_compressed = dynamic_range_compression(y, compression_factor)
# Apply time stretching augmentation
y_stretched = time_stretching(y, rate=1.5)
# Apply pitch shifting augmentation
y_pitch_shifted = pitch_shifting(y, sr=sr, pitch_factor=3)
# Display the original and augmented waveforms
plt.figure(figsize=(12, 8))
plt.subplot(4, 1, 1)
librosa.display.waveshow(y, sr=sr)
plt.title('Original Waveform')
plt.subplot(4, 1, 2)
librosa.display.waveshow(y_stretched, sr=sr)
plt.title('Time Stretched Waveform')
plt.subplot(4, 1, 3)
librosa.display.waveshow(y_pitch_shifted, sr=sr)
plt.title('Pitch Shifted Waveform')
plt.subplot(4, 1, 4)
librosa.display.waveshow(y_compressed, sr=sr)
plt.title('Dynamic Range Compressed Waveform')
plt.tight_layout()
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.596.1">Here</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.597.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<span class="koboSpan" id="kobo.599.1"><img alt="" role="presentation" src="image/B18944_11_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.600.1">Figure 11.10 – Data augmentation techniques – time stretching, pitch shifting, and dynamic range compression</span></p>
<p><span class="koboSpan" id="kobo.601.1">Now, let’s move on to another interesting topic in labeling audio data in the </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">next section.</span></span></p>
<h1 id="_idParaDest-257"><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.603.1">Introducing Azure Cognitive Services – the speech service</span></h1>
<p><span class="koboSpan" id="kobo.604.1">Azure Cognitive Services offers a comprehensive set of speech-related services that empower developers to integrate powerful speech capabilities into their applications. </span><span class="koboSpan" id="kobo.604.2">Some key speech services available in Azure AI include </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">Speech-to-text (speech recognition)</span></strong><span class="koboSpan" id="kobo.607.1">: This </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.608.1">converts spoken language into written text, enabling applications to transcribe audio content such as voice commands, interviews, </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">or conversations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.610.1">Speech Translation</span></strong><span class="koboSpan" id="kobo.611.1">: This </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.612.1">translates spoken language into another language in real time, facilitating multilingual communication. </span><span class="koboSpan" id="kobo.612.2">This service is valuable for applications requiring language translation for </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">global audiences.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.614.1">These Azure Cognitive Services speech capabilities cater to a diverse range of applications, from accessibility features and voice-enabled applications to multilingual communication and personalized user experiences. </span><span class="koboSpan" id="kobo.614.2">Developers can leverage these services to enhance the functionality and accessibility of their applications through seamless integration of </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">speech-related features.</span></span></p>
<h2 id="_idParaDest-258"><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.616.1">Creating an Azure Speech service</span></h2>
<p><span class="koboSpan" id="kobo.617.1">Let’s create a speech </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.618.1">service using the Azure portal, </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">as follows.</span></span></p>
<p><span class="koboSpan" id="kobo.620.1">Go to the Azure portal at </span><a href="https://portal.azure.com"><span class="koboSpan" id="kobo.621.1">https://portal.azure.com</span></a><span class="koboSpan" id="kobo.622.1">,  search for </span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">speech service</span></strong><span class="koboSpan" id="kobo.624.1">, and then create a </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">new service.</span></span></p>
<p><span class="koboSpan" id="kobo.626.1">As shown in the following screenshot, enter the project details such as your resource group speech service name and region details. </span><span class="koboSpan" id="kobo.626.2">Then, click on the </span><strong class="bold"><span class="koboSpan" id="kobo.627.1">Review + create</span></strong><span class="koboSpan" id="kobo.628.1"> button to create a speech service in an </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">Azure environment.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<span class="koboSpan" id="kobo.630.1"><img alt="" role="presentation" src="image/B18944_11_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.631.1">Figure 11.11 – Create Speech Services</span></p>
<p><span class="koboSpan" id="kobo.632.1">Now, your Azure speech service is deployed, and you can go to that speech service resource by clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.633.1">Go to resource</span></strong><span class="koboSpan" id="kobo.634.1"> button on the deployment screen. </span><span class="koboSpan" id="kobo.634.2">Then, on the speech service resource screen, click on </span><strong class="bold"><span class="koboSpan" id="kobo.635.1">Go to speech studio</span></strong><span class="koboSpan" id="kobo.636.1">. </span><span class="koboSpan" id="kobo.636.2">In </span><strong class="bold"><span class="koboSpan" id="kobo.637.1">Speech Studio</span></strong><span class="koboSpan" id="kobo.638.1">, you can see various services for captioning with speech to text, post call transcription and analytics, and a live chat avatar, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">following screenshot.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<span class="koboSpan" id="kobo.640.1"><img alt="" role="presentation" src="image/B18944_11_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.641.1">Figure 11.12 –  Speech Studio</span></p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor264"/><span class="koboSpan" id="kobo.642.1">Speech to text</span></h2>
<p><span class="koboSpan" id="kobo.643.1">Now, let’s try to use</span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.644.1"> the speech to text service. </span><span class="koboSpan" id="kobo.644.2">As shown in the following screenshot, you can drag and drop an audio file or upload it, and record audio with microphone. </span><span class="koboSpan" id="kobo.644.3">You can see the corresponding </span><strong class="bold"><span class="koboSpan" id="kobo.645.1">Text</span></strong><span class="koboSpan" id="kobo.646.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.647.1">JSON</span></strong><span class="koboSpan" id="kobo.648.1"> tabs on the right-side window for the uploaded audio file, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">following screenshot.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<span class="koboSpan" id="kobo.650.1"><img alt="" role="presentation" src="image/B18944_11_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.651.1">Figure 11.13 – Real-time speech to text</span></p>
<h2 id="_idParaDest-260"><a id="_idTextAnchor265"/><span class="koboSpan" id="kobo.652.1">Speech translation</span></h2>
<p><span class="koboSpan" id="kobo.653.1">Now, let’s see how to translate </span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.654.1">the speech. </span><span class="koboSpan" id="kobo.654.2">On the following screen, we are translating from English to French. </span><span class="koboSpan" id="kobo.654.3">Let’s choose a spoken language and a </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">target language.</span></span></p>
<p><span class="koboSpan" id="kobo.656.1">Then, speak in English and record the audio with a microphone. </span><span class="koboSpan" id="kobo.656.2">The translated text in French is shown on the right side of the window, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">following screenshot.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<span class="koboSpan" id="kobo.658.1"><img alt="" role="presentation" src="image/B18944_11_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.659.1">Figure 11.14 – Translated text test results</span></p>
<p><span class="koboSpan" id="kobo.660.1">We can also see the</span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.661.1"> original text on the </span><strong class="bold"><span class="koboSpan" id="kobo.662.1">Original text</span></strong><span class="koboSpan" id="kobo.663.1"> tab, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">following screenshot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<span class="koboSpan" id="kobo.665.1"><img alt="" role="presentation" src="image/B18944_11_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.666.1">Figure 11.15 –  Original text in test results</span></p>
<p><span class="koboSpan" id="kobo.667.1">We have seen how to transcribe from speech to text and translate from English to French using Azure speech services. </span><span class="koboSpan" id="kobo.667.2">Aside from this, there are many other Azure speech services in Azure Speech Studio that you can apply, based on </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">your requirements.</span></span></p>
<h1 id="_idParaDest-261"><a id="_idTextAnchor266"/><span class="koboSpan" id="kobo.669.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.670.1">In this chapter, we explored three key sections that delve into the comprehensive process of handling audio data. </span><span class="koboSpan" id="kobo.670.2">The journey began with the upload of audio data, leveraging the Whisper model for transcription, and subsequently labeling the transcriptions using OpenAI. </span><span class="koboSpan" id="kobo.670.3">Following this, we ventured into the creation of spectrograms and employed CNNs to label these visual representations, unraveling the intricate details of sound through advanced neural network architectures. </span><span class="koboSpan" id="kobo.670.4">The chapter then delved into audio labeling with augmented data, thereby enhancing the dataset for improved model training. </span><span class="koboSpan" id="kobo.670.5">Finally, we saw the Azure Speech service for speech to text and speech translation. </span><span class="koboSpan" id="kobo.670.6">This multifaceted approach equips you with a holistic understanding of audio data processing, from transcription to visual representation analysis and augmented labeling, fostering a comprehensive skill set in audio data </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">labeling techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.672.1">In the next and final chapter, we will explore different hands-on tools for </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">data labeling.</span></span></p>
</div>
</body></html>