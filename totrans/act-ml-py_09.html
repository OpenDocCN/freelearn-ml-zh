<html><head></head><body>
<div id="book-content" class="calibre2">
<div id="sbo-rt-content" class="calibre3"><div id="_idContainer093" class="calibre4">
			<h1 id="_idParaDest-76" class="calibre8"><a id="_idTextAnchor078" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>6</h1>
			<h1 id="_idParaDest-77" class="calibre8"><a id="_idTextAnchor079" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Evaluating and Enhancing Efficiency</h1>
			<p class="calibre6">In this chapter, we will explore the important aspects of rigorously evaluating the performance of active machine learning systems. We will cover various topics such as automation, testing, monitoring, and determining the stopping criteria. In this chapter we will use a paid cloud service, such as AWS, to demonstrate how an automatic, efficient active learning pipeline can be implemented in the <span>real world.</span></p>
			<p class="calibre6">By thoroughly understanding these concepts and techniques, we can ensure a comprehensive active ML process that yields accurate and reliable results. Through this exploration, we will gain insights into the effectiveness and efficiency of active ML systems, enabling us to make informed decisions <span>and improvements.</span></p>
			<p class="calibre6">By the end of this chapter, we will have covered <span>the following:</span></p>
			<ul class="calibre16">
				<li class="calibre20">Creating efficient active <span>ML pipelines</span></li>
				<li class="calibre20">Monitoring active <span>ML pipelines</span></li>
				<li class="calibre20">Determining when to stop active <span>ML runs</span></li>
				<li class="calibre20">Enhancing production model monitoring with <span>active ML</span></li>
			</ul>
			<h1 id="_idParaDest-78" class="calibre8"><a id="_idTextAnchor080" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Technical requirements</h1>
			<p class="calibre6">For this chapter, you will need <span>the following:</span></p>
			<ul class="calibre16">
				<li class="calibre20">A MongoDB <span>account: (</span><a href="https://www.mongodb.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://www.mongodb.com/</span></a><span>)</span></li>
				<li class="calibre20">A ClearML <span>account: (</span><a href="https://app.clear.ml/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://app.clear.ml/</span></a><span>)</span></li>
				<li class="calibre20">GPU: You may check out the specific hardware requirements from the web page of the tool you will <span>be using</span></li>
				<li class="calibre20">An EC2 instance, factoring in <span>cost considerations</span></li>
			</ul>
			<p class="calibre6">In this chapter, you will need to install <span>these packages:</span></p>
			<pre class="source-code">
pip install clearml
pip install pymongo</pre>			<p class="calibre6">You will need the <span>following imports:</span></p>
			<pre class="source-code">
import os
from clearml import Task, TaskTypes
import pymongo
import datetime</pre>			<h1 id="_idParaDest-79" class="calibre8"><a id="_idTextAnchor081" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Creating efficient active ML pipelines</h1>
			<p class="calibre6">As we have seen in the previous chapter, efficient active ML pipelines consist of end-to-end pipelines. This means that the active ML algorithm needs to be able to access the unlabeled <a id="_idIndexMarker299" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>data, select the most informative frames, and then seamlessly send them to the labeling platform. All these steps need to happen one after the other in an automatic manner in order to reduce <span>manual intervention.</span></p>
			<p class="calibre6">Moreover, it is essential to test this pipeline to ensure that each step works properly. An example of <a id="_idIndexMarker300" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>a cloud-hosted active ML pipeline would be <span>as follows:</span></p>
			<ol class="calibre16">
				<li class="calibre17">Unlabeled data is stored in an AWS <span>S3 bucket.</span></li>
				<li class="calibre17">An active ML algorithm runs on an EC2 instance that can access the <span>S3 bucket.</span></li>
				<li class="calibre17">The results of the active ML run are saved in a dedicated S3 bucket specifically for this purpose and are linked to the labeling platform used for <span>the project.</span></li>
				<li class="calibre17">The final step of the active ML run is to link the selected frames to the labeling platform and create the annotation project, ready for the labelers to work <span>on it.</span></li>
			</ol>
			<p class="calibre6">When selecting and configuring the EC2 instance for running the active ML code, it is essential to consider efficiency. It is highly likely that a GPU will be required to perform the inference and compute the active ML embeddings. For example, if you are using Lightly, you can refer to their hardware recommendations on this page (<a href="https://docs.lightly.ai/docs/hardware-recommendations" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://docs.lightly.ai/docs/hardware-recommendations</a>). Additionally, it is important to take into account the cost of the chosen EC2 instance and determine if it aligns with <a id="_idIndexMarker301" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>your budget. You can find the AWS EC2 on-demand pricing here (<a href="https://aws.amazon.com/ec2/pricing/on-demand/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://aws.amazon.com/ec2/pricing/on-demand/</a>). When you are not running any active ML process, stopping the instance is a good practice to <span>save money.</span></p>
			<p class="calibre6">Other good <a id="_idIndexMarker302" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>practices include having a requirements.txt file that lists all the required versions of the packages for the run. For example, for the packages used in <a href="B21789_05.xhtml#_idTextAnchor069" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 5</em></span></a>, <em class="italic">Leveraging Active Learning for Big Data</em>, the <strong class="source-inline">requirements.txt</strong> file would look something <span>like this.</span></p>
			<pre class="source-code">
awscli==1.31.6
ultralytics==8.0.145
lightly==1.4.23
docker==6.1.3
encord==0.1.85</pre>			<p class="calibre6">You can replace any version with the desired version; ideally, using the latest versions of the packages would <span>be better.</span></p>
			<p class="calibre6">Additionally, making the pipeline configurable through parameters enables easier scaling. For example, specifying options such as sampling strategy, model selection, and data source via a YAML configuration file. This allows for changing pipeline behavior without code changes, simplifying integration into workflows. As a reminder, we have explored different sampling strategies in <a href="B21789_02.xhtml#_idTextAnchor027" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 2</em></span></a>, <em class="italic">Designing Query Strategy Frameworks</em>, and we have explored model selection for computer vision tasks in <a href="B21789_04.xhtml#_idTextAnchor056" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 4</em></span></a>, <em class="italic">Applying Active Learning to </em><span><em class="italic">Computer Vision</em></span><span>.</span></p>
			<p class="calibre6">A simple example of configuring a YAML file using our Lightly example from <a href="B21789_05.xhtml#_idTextAnchor069" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 5</em></span></a>, <em class="italic">Leveraging Active Learning for Big Data</em>, might look <span>like this:</span></p>
			<pre class="source-code">
model_path_in_s3: 's3://my-models-library/my-best-object-detection-model.pt'
inference_confidence_threshold: 0.3
proportionSamples: 0.20  # 20% of the samples
isSSLenabled: true
maxSSLepochs: 20
important_classes: {"person": 0, " sports ball": 32}
balance: true
balance_strategy:{ 'person': 0.50, 'sports ball': 0.50}
videos_folder_in_s3: "test"
s3_bucket_output: 'labeling-queue'</pre>			<p class="calibre6">Then, those <a id="_idIndexMarker303" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>parameters can be accessed using <span>this function:</span></p>
			<pre class="source-code">
def get_config_yaml(path_to_config_yaml="config.yaml"):
    # Open the YAML file
    with open(path_to_config_yaml, "r") as file:
        # Load the YAML content
        config = yaml.safe_load(file)
    file.close()
    return config</pre>			<p class="calibre6">Followed <span>by this:</span></p>
			<pre class="source-code">
config = get_config_yaml(path_to_config_yaml="config.yaml")
model_path_in_s3 = config["model_path_in_s3"]
inference_confidence_threshold = config["inference_confidence_threshold"]
proportionSamples = config["proportionSamples"]
isSSLenabled = config["isSSLenabled"]
if isSSLenabled:
  maxSSLepochs = config['maxSSLepochs']
important_classes = config["important_classes"]
s3_bucket_output = config["s3_bucket_output"]
balance_strategy = config['balance_strategy']
if balance_strategy:
  balance = config['balance']</pre>			<p class="calibre6">Those variables can then be used in the code in <a href="B21789_05.xhtml#_idTextAnchor069" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 5</em></span></a>, <em class="italic">Leveraging Active Learning for Big Data</em>, and will make it scalable to all workflows <span>and applications.</span></p>
			<p class="calibre6">Thus, for your <a id="_idIndexMarker304" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>projects, you only need to change the YAML file and can then use the scripts for all your projects without modifying the <span>scripts themselves.</span></p>
			<p class="calibre6">Next, we will explore solutions to monitor our active ML runs and make sure that we are able to have a full overview of the <span>whole process.</span></p>
			<h1 id="_idParaDest-80" class="calibre8"><a id="_idTextAnchor082" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Monitoring active ML pipelines</h1>
			<p class="calibre6">The <strong class="bold">proactive monitoring</strong> of active ML pipelines is critical to ensure their optimal performance in <a id="_idIndexMarker305" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>production environments. Achieving this requires a focused approach on several key areas for effective observation, utilizing a variety of <a id="_idIndexMarker306" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>specialized tools specifically designed for these tasks. A central aspect of this monitoring process is <strong class="bold">comprehensive logging</strong>. It is essential <a id="_idIndexMarker307" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>for every phase of the active ML pipeline to implement detailed logging practices, capturing a broad spectrum of data, such as useful insights, errors, warnings, and other pertinent metadata. This diligent approach to log monitoring is key in quickly identifying and diagnosing issues, enabling prompt and efficient resolutions. Furthermore, these logs offer invaluable insights into the pipeline’s performance and behavior, aiding in the continuous enhancement of the active ML systems. Simple logging can be done in the scripts themselves with libraries such as <strong class="source-inline">logging</strong>, which is a versatile and <span>built-in library.</span></p>
			<p class="calibre6">Incorporating an MLOps <a id="_idIndexMarker308" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>platform such as <strong class="bold">ClearML</strong> (<a href="https://clear.ml/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://clear.ml/</a>) can significantly streamline the monitoring of pipeline runs. ClearML provides real-time statistics, graphical data visualizations, extensive logging, model artifacts, and the ability to compare pipeline runs in a side-by-side format. While traditionally used for improving the observability of ML training and deployment pipelines, ClearML is also highly effective for active ML pipelines, enhancing their management <span>and oversight.</span></p>
			<p class="calibre6">The following <a id="_idIndexMarker309" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>sample code snippet can be seamlessly integrated into the active ML codebase. This snippet is designed to set up a ClearML project named <strong class="source-inline">active_learning_runs</strong> and initialize a task within it, labeled <strong class="source-inline">testing-AL</strong>, which corresponds to a specific run. Once implemented, this configuration enables the automatic tracking of the run, as illustrated in <span><em class="italic">Figure 6</em></span><em class="italic">.1</em>, under the console tab of the ClearML project configured via this <span>code snippet:</span></p>
			<div class="calibre18">
				<div id="_idContainer090" class="img---figure">
					<img src="image/B21789_06_01.jpg" alt="Figure 6.1 – Console log automatically saved after initializing the ClearML run" class="calibre95"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 6.1 – Console log automatically saved after initializing the ClearML run</p>
			<p class="calibre6">This integration not only streamlines the process of monitoring and managing ML runs but also ensures that all relevant data and metrics are systematically captured and made accessible for analysis within the <span>ClearML platform:</span></p>
			<pre class="source-code">
Task.set_credentials(api_host='https://api.community.clear.ml',
                    web_host='https://app.community.clear.ml',
                    key='your_clearml_access_key',
                    secret='your_clearml_secret_key')
task = Task.init(project_name='active_learning_runs',
                task_name='testing-AL',
                task_type=TaskTypes.inference)</pre>			<p class="calibre6">We opt for the ClearML task type <strong class="source-inline">TaskTypes.inference</strong>, as it is specifically tailored to facilitate the comprehensive logging and meticulous monitoring of the inference processes in machine learning. This task type is adept at meticulously tracking the input data, the predictions outputted by the model, and a range of relevant metrics or performance indicators, making it particularly suited for active ML runs. Indeed, as we have seen <a id="_idIndexMarker310" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>in previous chapters, active ML runs consist of conducting inference to identify the most advantageous frames to add to the labeling queue. Therefore, <strong class="source-inline">TaskTypes.inference</strong> is the ideal choice here. This task type is instrumental in enabling the systematic collection and thorough analysis of key performance metrics unique to the inference stage, such as latency. Furthermore, utilizing <strong class="source-inline">TaskTypes.inference</strong> empowers teams to accrue critical insights regarding the model’s behavior when interfacing with real-time data—a fundamental aspect for the success of active ML systems. This detailed understanding of a run’s real-time performance is invaluable for optimizing active ML strategies and enhancing overall <span>model efficacy.</span></p>
			<p class="calibre6">Another valuable <a id="_idIndexMarker311" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>tool for monitoring active ML pipelines is <strong class="bold">MongoDB</strong> (<a href="https://www.mongodb.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://www.mongodb.com/</a>), which is a widely used database known for its user-friendly nature. Its flexibility makes it an excellent choice for ML pipelines, which often evolve over time. In the context of active ML pipelines, MongoDB can be employed to generate a labeling queue automatically, for instance. This application of MongoDB not only streamlines the data handling process but also contributes to the overall efficiency and adaptability of the <span>ML pipeline.</span></p>
			<p class="calibre6">Let’s take a look at an example code snippet for this. First, we need to log in <span>to </span><span><strong class="source-inline">mongodb</strong></span><span>:</span></p>
			<pre class="source-code">
username = 'your_mongodb_username'
password = 'your_mongodb_pwd'
cluster = 'your_mongodb_cluster'
uri = 'mongodb+srv://' + username + ':' + password + '@' + cluster
client = pymongo.MongoClient(uri)</pre>			<p class="calibre6">Next, we create a MongoDB collection specifically for this project, which we will name <strong class="source-inline">ml_demo_project</strong>. Within this collection, we will create a table titled <strong class="source-inline">ml_labeling_queue_demo</strong>. This organizational structure in MongoDB will facilitate the efficient management and retrieval of data pertinent to <span>our project:</span></p>
			<pre class="source-code">
db = client['ml_demo_project']
collection = db['ml_labeling_queue_demo']</pre>			<p class="calibre6">The final step involves populating our labeling queue table with data derived from our active ML run. This is a general example and should be tailored to fit the specific requirements <a id="_idIndexMarker312" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>of individual projects. By integrating this step, we ensure that the information from the ML run is accurately and efficiently transferred to the labeling queue, setting the stage for subsequent processing and analysis tailored to the unique needs of <span>each project:</span></p>
			<pre class="source-code">
document = {'Name dataset': 'demo',
            'Labeler': 'TBD',
            'Reviewer': 'TBD',
            'Labeling status': 'In queue',
            'Reviewing status': 'None',
            'date': datetime.datetime.now()}
collection.insert_one(document)</pre>			<p class="calibre6">This MongoDB collection streamlines the process of monitoring progress in the labeling queue, facilitating efficient communication with labelers about upcoming items. This setup eliminates the need for the manual entry of each new annotation project, significantly enhancing workflow efficiency. By automating the tracking and updating of the queue, it ensures a more seamless and coordinated approach to managing <span>labeling tasks.</span></p>
			<p class="calibre6">Actively monitoring via logs, MLOps platforms, databases, and other tools is essential for maintaining visibility and quickly catching any issues in production-active ML pipelines. This helps minimize risks and improve <span>system reliability.</span></p>
			<p class="calibre6">To ensure that <a id="_idIndexMarker313" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>this monitoring is effectively utilized in decision-making processes, it’s crucial to establish clear criteria for critical actions, such as determining when to stop active ML runs, which we will <span>cover next.</span></p>
			<h1 id="_idParaDest-81" class="calibre8"><a id="_idTextAnchor083" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Determining when to stop active ML runs</h1>
			<p class="calibre6">Active ML runs are dynamic and iterative processes that require careful monitoring, as we have already seen. But they also require strategic decision-making to determine the optimal point <a id="_idIndexMarker314" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>for cessation. The decision to stop an active ML run is critical as it impacts both the performance and efficiency of the learning model. This section focuses on the key considerations and strategies to effectively determine when to stop active machine <span>learning runs.</span></p>
			<p class="calibre6">In active ML, establishing clear performance goals specific to the project is crucial. For instance, consider a project aimed at developing a facial recognition system. Here, accuracy and precision might be the chosen performance metrics. A diverse test set, mirroring real-world conditions and varied facial features, is crucial for evaluating <span>the model.</span></p>
			<p class="calibre6">Let’s say the pre-defined threshold on the established test set for accuracy is set at 95% and for precision, at 90%. The active ML process should continue until the model consistently achieves or surpasses these metrics on the test set. If the model shows an accuracy of 95% or more and a precision of 90% or more on the test set, it suggests the model has learned to generalize well across different faces and scenarios. This consistent performance on a diverse test set indicates the model is ready for real-world application, having been effectively tailored through the active <span>learning process.</span></p>
			<p class="calibre6">Additional considerations play a vital role in determining when to stop the active ML process. In the preceding facial recognition example, there are several simple but important additional factors to consider when deciding to stop the active ML process. Here are some steps you could take before deciding to stop the active <span>ML process:</span></p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="bold">Watch out for overfitting</strong>: This happens when the model does much better on training data than on the test set. If we see this, it’s time to stop and adjust the model to <span>avoid overfitting.</span></li>
				<li class="calibre20"><strong class="bold">Think about our resources</strong>: Resources such as time, computing power, and money are scarce. Even if our model hasn’t hit the 95% accuracy or 90% precision we want, we might have to stop if we’re running low on <span>these resources.</span></li>
				<li class="calibre20"><strong class="bold">Be aware of diminishing returns</strong>: This means if training more isn’t really improving our model, it might have learned as much as it can. Continuing to train it in this case <span>won’t help.</span></li>
				<li class="calibre20"><strong class="bold">Keep reassessing the model with feedback loops</strong>: As the world and data change, our model’s goals might need to change, too. Regularly checking that our model still meets our current needs helps keep it relevant <span>and effective.</span></li>
			</ul>
			<p class="calibre6">The decision to stop an active ML run should be based on a combination of reaching predefined <a id="_idIndexMarker315" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>performance metrics, maintaining stability on a diverse test set, monitoring resource constraints, and being vigilant about overfitting and diminishing returns. By carefully considering these factors, we can ensure that active ML models are both effective and efficient, aligning with the overarching goals of <span>the project.</span></p>
			<p class="calibre6">Let’s now discuss how we can use active learning in a <span>production environment.</span></p>
			<h1 id="_idParaDest-82" class="calibre8"><a id="_idTextAnchor084" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Enhancing production model monitoring with active ML</h1>
			<p class="calibre6">Having already established a comprehensive understanding of active ML, this section shifts <a id="_idIndexMarker316" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>focus to its practical <a id="_idIndexMarker317" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>application in monitoring machine learning models in production environments. The dynamic nature of user data and market conditions presents a unique challenge for maintaining the accuracy and relevance of deployed models. Active ML emerges as a pivotal tool in this context, offering a proactive approach to identify and adapt to changes in real time. This section will explore the methodologies and strategies through which active ML can be harnessed to continuously improve and adjust models based on evolving user data, ensuring that these models remain robust, efficient, and aligned with current trends and <span>user behaviors.</span></p>
			<h2 id="_idParaDest-83" class="calibre9"><a id="_idTextAnchor085" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Challenges in monitoring production models</h2>
			<p class="calibre6">There are <a id="_idIndexMarker318" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>several challenges when it comes to monitoring production models. First, we have data drift and <span>model decay.</span></p>
			<p class="calibre6"><strong class="bold">Data drift</strong> refers to the <a id="_idIndexMarker319" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>change in the input data fed into a machine learning model over time. This change can occur due to various reasons, such as evolving user behaviors, seasonal effects, economic shifts, or changes in the broader environment in which the model operates. The key characteristic of data drift is that the statistical properties of the current input data differ from those of the original training data, as shown in <span><em class="italic">Figure 6</em></span><em class="italic">.2</em>. Data drift can significantly impact the performance of a model since the assumptions the model was originally trained on no longer hold true. It can lead to a decrease in accuracy and reliability, making the model less effective at making predictions <span>or classifications:</span></p>
			<div class="calibre18">
				<div id="_idContainer091" class="img---figure">
					<img src="image/B21789_06_02.jpg" alt="Figure 6.2 – Illustration of data drift" class="calibre96"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 6.2 – Illustration of data drift</p>
			<p class="calibre6"><strong class="bold">Model decay</strong>, also known <a id="_idIndexMarker320" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>as model degradation or performance decay, refers to the decline in the performance of a machine learning model over time. This phenomenon is closely related to data drift, as shown in <span><em class="italic">Figure 6</em></span><em class="italic">.3</em>, as one of the primary causes of model decay is the changing nature of the data the model encounters in a <span>live environment:</span></p>
			<div class="calibre18">
				<div id="_idContainer092" class="img---figure">
					<img src="image/B21789_06_03.jpg" alt="Figure 6.3 – Model decay over time" class="calibre97"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 6.3 – Model decay over time</p>
			<p class="calibre6">However, model decay <a id="_idIndexMarker321" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>can also occur due to other factors, such as <span>the following:</span></p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="bold">Changes in relationships</strong>: Over time, the relationships between variables might change, making the model’s learned <span>patterns outdated</span></li>
				<li class="calibre20"><strong class="bold">Feedback loops</strong>: If a model’s predictions are used as part of a decision-making process that influences future data, it can create feedback loops that gradually degrade the <span>model’s performance</span></li>
				<li class="calibre20"><strong class="bold">External factors</strong>: Unforeseen external factors such as policy changes, natural disasters, or global events can also lead to <span>model decay</span></li>
			</ul>
			<p class="calibre6">Both data drift and model decay highlight the need for continuous monitoring and updating machine learning models in production. Identifying when and how these changes occur is crucial for maintaining the effectiveness and accuracy of <span>the models.</span></p>
			<h2 id="_idParaDest-84" class="calibre9"><a id="_idTextAnchor086" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Active ML to monitor models in production</h2>
			<p class="calibre6">Active ML is particularly well suited to combat data drift and model decay due to its dynamic and <a id="_idIndexMarker322" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>responsive nature. Indeed, active ML stands out for its targeted data acquisition, as previously discussed in our chapters, where it excels in identifying and acquiring the most informative data points. This approach is particularly advantageous in addressing data drift and model decay, as the algorithm <a id="_idIndexMarker323" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>actively queries for new data that accurately represents the current environment or user behavior. This method is not only more efficient than passively collecting large datasets but also ensures that the data is relevant and not redundant. Active ML systems are inherently adaptable, quickly adjusting their understanding and predictions in response to new data, a feature crucial for maintaining effectiveness amidst changing data distributions. This adaptability is augmented by the system’s capacity for continuous learning and improvement. As active ML systems receive new data points and feedback, they are constantly updating and refining their models, thereby mitigating the effects of model decay and ensuring that the models evolve in line with changes in the data <span>and environment.</span></p>
			<p class="calibre6">Active ML addresses the cost and time-intensive nature of data labeling by selecting the most informative samples, a process that proves especially beneficial in adapting to data drift. The efficient use of resources in labeling ensures maximum benefit to the model. Additionally, active ML algorithms are designed for the <strong class="bold">early detection of shifts in data patterns</strong> or <strong class="bold">performance drops</strong>, acting as an early warning system for data drift and model decay. This early detection capability allows for prompt interventions, such as model adjustments or retraining, to prevent significant <span>performance degradation.</span></p>
			<p class="calibre6">In <a href="B21789_02.xhtml#_idTextAnchor027" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 2</em></span></a>, <em class="italic">Designing Query Strategy Frameworks</em>, we also discussed how active ML provides customizable query strategies, including uncertainty sampling and query by committee, which can be tailored to the specific needs of an application. This flexibility enables more effective responses to the unique challenges of data drift and model decay in various scenarios, underlining the comprehensive adaptability of active ML in dynamic <span>data environments.</span></p>
			<p class="calibre6">The early detection of drift and decay is crucial to sustain the performance of machine learning models once they are deployed. Active ML assumes an indispensable role here, functioning effectively as an early warning system. This capability is crucial for the pre-emptive identification and mitigation of potential issues before they escalate, thus preserving the model’s integrity <span>and accuracy.</span></p>
			<p class="calibre6">We will now <a id="_idIndexMarker324" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>explore the mechanisms and strategies through which active ML <a id="_idIndexMarker325" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>can be used to accomplish this, highlighting its significance in the proactive management and maintenance of ML models in <span>dynamic environments.</span></p>
			<p class="calibre6">Here are <a id="_idIndexMarker326" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>some mechanisms for early detection using active ML methods that we have seen in <span>previous chapters:</span></p>
			<ol class="calibre16">
				<li class="calibre17"><strong class="bold">Uncertainty sampling</strong>: Active ML algorithms often employ uncertainty sampling, where the model identifies data points for which it has the lowest confidence in its predictions. A sudden increase in the number of such points can signal a change in the underlying data distribution, indicating potential <span>data drift.</span></li>
				<li class="calibre17"><strong class="bold">Anomaly detection</strong>: Active ML systems can be equipped with anomaly detection capabilities to spot unusual patterns in incoming data. These anomalies might be indicative of changes in the data landscape that could lead to model decay if <span>not addressed.</span></li>
				<li class="calibre17"><strong class="bold">Query by committee</strong>: This approach involves maintaining multiple models or versions of a model (the committee) and using their disagreement to identify challenging data points. A growing disparity in the predictions of committee members can indicate emerging data drift or model decay, as it suggests that the models are becoming increasingly uncertain about the <span>current data.</span></li>
				<li class="calibre17"><strong class="bold">Feedback loops</strong>: In scenarios where user feedback or real-world outcomes are available, active ML can use this feedback to assess model performance. Rapid changes in user feedback patterns can provide early indications of shifts in data trends or declining <span>model effectiveness.</span></li>
			</ol>
			<p class="calibre6">Active ML’s capability for early issue detection is essential, allowing for prompt and timely interventions. This proactive strategy is markedly more effective than conventional reactive methods, where issues are only addressed following notable performance declines. By identifying problems early, active ML ensures that resources allocated for model retraining or adjustments are utilized efficiently and judiciously. This aspect is <a id="_idIndexMarker327" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>particularly crucial in environments where computational resources or labeled data are scarce. Moreover, in end-user applications, the consistency <a id="_idIndexMarker328" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>of model performance is essential for maintaining user trust. Through the early detection and timely correction of data drift or model decay, active ML contributes significantly to a reliable and consistent user experience, underlining its value in sustaining the credibility and effectiveness of machine learning models in various <span>real-world applications.</span></p>
			<h2 id="_idParaDest-85" class="calibre9"><a id="_idTextAnchor087" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Early detection for data drift and model decay</h2>
			<p class="calibre6">The effective <a id="_idIndexMarker329" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>implementation of early <a id="_idIndexMarker330" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>detection for data drift and <a id="_idIndexMarker331" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>model decay in active ML necessitates several <span>key </span><span><a id="_idIndexMarker332" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/></span><span>considerations:</span></p>
			<ul class="calibre16">
				<li class="calibre20">Choosing the right metrics for monitoring is the key. The metrics should align closely with the model’s objectives and the unique characteristics of the data <span>and application.</span></li>
				<li class="calibre20">Setting realistic thresholds for alerts is also crucial, striking a balance between sensitivity and practicality to avoid frequent false alarms or missing <span>critical changes.</span></li>
				<li class="calibre20">Integrating active ML systems with existing data pipelines is crucial for real-time monitoring and quick responses to detected issues, thereby enhancing system efficiency and responsiveness. For practical implementation, this means linking the active ML algorithm directly to the storage of user data, enabling the system to engage and analyze new data automatically as soon as it’s uploaded. This integration ensures continuous, up-to-date monitoring, which is vital for the timely detection and handling of potential data drift or <span>model decay.</span></li>
			</ul>
			<p class="calibre6">Let’s consider an example from a retail use case, specifically in the context of a computer vision system used for inventory management. Imagine a retail store using an active ML system equipped with computer vision for inventory management. This system is designed to monitor the store’s shelves using cameras to track stock levels, detect when items are running low, and identify when restocking is needed. The computer vision model is initially trained on a dataset of images depicting various states of shelf stock, from fully stocked to nearly empty. It learns to recognize different products, their locations, and their quantities. Over time, the store introduces new products and changes the layout of some items. The active ML system, during its routine monitoring, starts detecting anomalies in the images; the model encounters unfamiliar images, leading to uncertainty in its predictions. This uncertainty, often reflected in lower confidence scores, indicates potential anomalies. The system also employs statistical <a id="_idIndexMarker333" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>methods to identify outliers—data points that significantly <a id="_idIndexMarker334" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>deviate from established patterns of product arrangements. Additionally, it analyzes changes over time, comparing current images against historical data to spot deviations in product types or arrangements. When the system flags <a id="_idIndexMarker335" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>an anomaly, it alerts store <a id="_idIndexMarker336" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>managers for further inspection. If the change is intentional, such as with the introduction of new products, this information is used to update and retrain the model, ensuring it adapts to the new store layout and inventory. If the change is unintentional, such as a misplaced product, it can be corrected to maintain inventory accuracy. This adaptive process ensures the ML model remains effective in real-time inventory management, adjusting to both gradual and sudden changes in the <span>retail environment.</span></p>
			<p class="calibre6">In this example, the active ML system’s anomaly detection capability is crucial for maintaining the effectiveness of the inventory management system. It ensures that the computer vision model remains accurate and reliable in tracking inventory despite changes in the store’s product range and layout, thus preventing model decay and ensuring <span>operational efficiency.</span></p>
			<p class="calibre6">To sum up, by focusing on the most informative data points and integrating user feedback, active ML provides a dynamic approach to maintaining the relevance and accuracy of models amidst evolving user data and market conditions. The adaptability and efficiency delivered by this approach are vital for the long-term success of machine learning applications in <span>various sectors.</span></p>
			<h1 id="_idParaDest-86" class="calibre8"><a id="_idTextAnchor088" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Summary</h1>
			<p class="calibre6">In this chapter, we have delved deeply into the crucial aspects of rigorously evaluating the performance of active ML systems. We began by understanding the significance of automating processes to enhance efficiency and accuracy. The chapter then guided us through various testing methodologies, emphasizing their role in ensuring robust and reliable active <span>ML pipelines.</span></p>
			<p class="calibre6">A significant portion of our discussion focused on the criticality of the continuous monitoring of active ML pipelines. This monitoring is not just about observing the performance but also involves understanding and interpreting the results to make <span>data-driven decisions.</span></p>
			<p class="calibre6">One of the most pivotal topics we covered was determining the appropriate stopping criteria for active ML runs. We explored how setting pre-defined performance metrics, such as accuracy and precision, is crucial in guiding these decisions. We also emphasized the importance of a diverse and representative test set to ensure the model’s applicability in <span>real-world scenarios.</span></p>
			<p class="calibre6">Additionally, we discussed the need to be mindful of overfitting, resource limitations, diminishing returns, and the importance of implementing feedback loops. These considerations play a key role in not only determining when to stop the ML run but also in ensuring the overall success and relevance of the model in a constantly <span>evolving environment.</span></p>
			<p class="calibre6">Finally, we have established that active ML is exceptionally adept at monitoring models in production environments. Its application extends to the early detection of data drift and model decay, particularly when seamlessly integrated into user data pipelines. This integration enables the active ML system to monitor data, ensuring that any deviations or anomalies are promptly detected continuously. Moreover, the system can be configured to trigger alerts when these irregularities occur, allowing for immediate attention and action. This capability not only enhances the model’s reliability and accuracy but also ensures its adaptability and resilience in the face of evolving data landscapes, making active ML a powerful tool in production <span>model monitoring.</span></p>
			<p class="calibre6">In the next chapter, <em class="italic">Utilizing Tools and Packages for Active ML</em>, we will turn our attention to the various Python libraries, frameworks, and tools commonly used in active ML. We will provide an overview of these resources, highlighting their value in implementing various active ML techniques. This will equip you with the necessary knowledge and skills to elevate your active ML projects and define what tools are best suited for them. This chapter promises to be a comprehensive guide to the current active <span>ML tools.</span></p>
		</div>
	</div>
</div>
</body></html>