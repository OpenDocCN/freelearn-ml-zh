<html><head></head><body>
		<div id="_idContainer269">
			<h1 id="_idParaDest-43"><em class="italic"><a id="_idTextAnchor047"/>Chapter 5</em>: Exploring Heuristic Search</h1>
			<p><strong class="bold">Heuristic search</strong> is the third out of four groups of hyperparameter tuning methods. The key difference between <a id="_idIndexMarker175"/>this group and the other groups is that all the methods that belong to this group work by performing <em class="italic">trial and error</em> to achieve the optimal solution. Similar to the acquisition function in Bayesian optimization (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>), all methods in this group also employ the concept of <em class="italic">exploration versus exploitation</em>. <strong class="bold">Exploration</strong> means performing a <a id="_idIndexMarker176"/>search in the unexplored space to lower the probability of being stuck in the local optima, while <strong class="bold">exploitation</strong> means performing a search in the local space that is <a id="_idIndexMarker177"/>known to have a good chance of containing the optimal solution. </p>
			<p>In this chapter, we will discuss several methods that belong to the heuristic search group, including <strong class="bold">simulated annealing</strong> (<strong class="bold">SA</strong>), <strong class="bold">genetic algorithms</strong> (<strong class="bold">GAs</strong>), <strong class="bold">particle swarm optimization</strong> (<strong class="bold">PSO</strong>), and <strong class="bold">Population-Based Training</strong> (<strong class="bold">PBT</strong>). Similar to <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>, we will discuss the definition of each method, what the differences are between them, how they work, and the pros and cons of each method.</p>
			<p>By the end of this chapter, you will understand the concept of the aforementioned hyperparameter tuning methods that belong to the heuristic search group. You will be able to explain these methods with confidence when someone asks you, at both a high-level and detailed fashion, along with the pros and cons. Once you are confident enough to explain them to other people, this means you have understood the ins and outs of each method. Thus, in practice, you can understand what’s happening if there are errors or you don’t get the expected results; you will also know how to configure the method so that it matches your specific problem.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding simulated annealing</li>
				<li>Understanding genetic algorithms</li>
				<li>Understanding particle swarm optimization</li>
				<li>Understanding population based training</li>
			</ul>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor048"/>Understanding simulated annealing</h1>
			<p><strong class="bold">SA</strong> is the heuristic search method that is inspired by the process of <strong class="bold">metal annealing</strong> in metallurgy. This method is <a id="_idIndexMarker178"/>similar to the random search <a id="_idIndexMarker179"/>hyperparameter tuning method (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring Exhaustive Search</em>), except for the existence of a criterion that guides how the hyperparameter tuning process works. In other words, SA is like a <em class="italic">smoothed version of random search</em>. Just like random search, it is suggested to use SA when each trial doesn’t take too much time and you have enough computational resources. </p>
			<p>In the metal annealing process, the metal is heated to a very high temperature for a certain time and slowly cooled to increase its strength, reducing its hardness and making it easier to work with. The goal of giving a very high heat is to excite the metal’s atoms so that they can move around freely and randomly. During this random movement, atoms usually tend to form a better configuration. Then, the slow cooling process is performed so that we can have a crystalline form of the material.</p>
			<p>Just like in the metal annealing process, SA works by randomly choosing the set of hyperparameters to be tested. At each trial, the method will consider some of the “neighbors” of the current set, randomly. If the acceptance criterion is met, then the method will change its focus to that “neighbor” set. The acceptance criterion is not a deterministic function, it is a stochastic function, which means probability comes into play during the process. This probabilistic way of deciding is similar to the cooling phase in the metal annealing process, where we accept a smaller number of bad hyperparameter sets as more parts of the search space are explored.</p>
			<p>SA is a modified version of one of the most popular heuristic optimization methods, known as <strong class="bold">stochastic hill climbing</strong> (<strong class="bold">SHC</strong>). SHC is very simple to understand and implement, which <a id="_idIndexMarker180"/>means that SA is as well. <em class="italic">In general</em>, SHC works by initializing the random point within a pre-defined bound (the <em class="italic">hyperparameter space, in our case</em>) and treating it as the current best solution. Then, it randomly searches for the next candidate within the surrounding of the selected point. Then, we need to compare the selected candidate with the current best solution. If the candidate is better than or equal to the current best solution, SHC will treat the candidate as <a id="_idIndexMarker181"/>the new best solution. This process is repeated until the stopping criterion is met. </p>
			<p>The following steps <a id="_idIndexMarker182"/>show how SHC optimization works in general:</p>
			<ol>
				<li>Define the bound of the space, <em class="italic">B</em>, and the step size, <em class="italic">S</em>.</li>
				<li>Define the stopping criterion. Usually, it is defined as the number of iterations, but other stopping criteria definitions also work.</li>
				<li>Initialize the random point within the bound, <em class="italic">B</em>. </li>
				<li>Set the selected point from <em class="italic">Step 3</em> as the current point, <em class="italic">current_point</em>, as well as the best point, <em class="italic">best_point</em>.</li>
				<li>Randomly sample the next candidate within the <em class="italic">S</em> distance from <em class="italic">best_point</em> and within the bound, <em class="italic">B</em>, then store it as <em class="italic">candidate_point</em>.</li>
				<li>If <em class="italic">candidate_point</em> is better than or equal to <em class="italic">best_point</em>, then replace <em class="italic">best_point</em> with <em class="italic">candidate_point</em>.</li>
				<li>Replace <em class="italic">current_point</em> with <em class="italic">candidate_point</em>.</li>
				<li>Repeat <em class="italic">Steps 5</em> to <em class="italic">7</em> until the stopping criterion is met. </li>
			</ol>
			<p>The main difference between SA and SHC is located in <em class="italic">Steps 5</em> and <em class="italic">6</em>. In SHC, we always sample the next <a id="_idIndexMarker183"/>candidate from the surrounding of the <em class="italic">best_point</em>, while in SA, we sample from the surrounding of <em class="italic">current_point</em>. In SHC, we only accept a candidate that is better than or equal to the current best solution, while in SA, we <em class="italic">may also accept a worse candidate</em> with a certain probability that is guided by the acceptance criterion, <em class="italic">AC</em>, which is defined as follows:</p>
			<p><img src="image/Formula_B18753_05_001.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_05_002.png" alt=""/>    , <img src="image/Formula_B18753_05_003.png" alt=""/>is the objective function and <img src="image/Formula_B18753_05_004.png" alt=""/> is <em class="italic">temperature</em> with a positive value. See <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a> if you are not familiar with the objective function term.</p>
			<p>The <img src="image/Formula_B18753_05_005.png" alt=""/>formula results in a value between 0 and 1, where it always results in a value of 1 when the <em class="italic">candidate_point</em> is better than or equal to the <em class="italic">current_point</em>. In other words, we always <a id="_idIndexMarker184"/>accept the <em class="italic">candidate_point</em> when it is better than or equal to the <em class="italic">current_point</em>. It is worth noting that <em class="italic">better</em> does not necessarily mean has a greater value. If you are working with a maximization problem, then better <a id="_idIndexMarker185"/>means greater. However, if you are working with a minimization problem, then it is the other way around. For example, if the cross-validation score you are measuring is the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>), where a lower score corresponds to better performance, then the <em class="italic">candidate_point</em> is considered better than the <em class="italic">current_point</em> if the <img src="image/Formula_B18753_05_006.png" alt=""/> value is less than <img src="image/Formula_B18753_05_007.png" alt=""/>.</p>
			<p>Although <img src="image/Formula_B18753_05_008.png" alt=""/> is impacted by both <img src="image/Formula_B18753_05_009.png" alt=""/> and <img src="image/Formula_B18753_05_010.png" alt=""/>, we can only control the value of <img src="image/Formula_B18753_05_011.png" alt=""/>. In practice, the <em class="italic">initial value</em> of <img src="image/Formula_B18753_05_012.png" alt=""/> <em class="italic">is treated as a hyperparameter</em> and is usually set to a high value. Over the number of trials, the value of <img src="image/Formula_B18753_05_013.png" alt=""/> is decreased following the so-called <strong class="bold">annealing schedule</strong> or cooling <a id="_idIndexMarker186"/>schedule scheme. There are several annealing schedule schemes that we can follow. The three most popular schemes are as follows:</p>
			<ul>
				<li><strong class="bold">Geometric cooling</strong>: This annealing schedule works by decreasing the temperature via <a id="_idIndexMarker187"/>a cooling factor of <img src="image/Formula_B18753_05_014.png" alt=""/>. In geometric <a id="_idIndexMarker188"/>cooling, the initial temperature, <img src="image/Formula_B18753_05_015.png" alt=""/>, is multiplied by the cooling factor <img src="image/Formula_B18753_05_016.png" alt=""/> number of times, where <img src="image/Formula_B18753_05_017.png" alt=""/> is the current number of iterations:</li>
			</ul>
			<p><img src="image/Formula_B18753_05_018.png" alt=""/></p>
			<p>This can be seen in the following graph:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B18753_05_001.jpg" alt="Figure 5.1 – Effect of the initial temperature in geometric cooling on the acceptable criterion&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Effect of the initial temperature in geometric cooling on the acceptable criterion</p>
			<ul>
				<li><strong class="bold">Linear cooling</strong>: This annealing schedule works by decreasing the temperature linearly via a <a id="_idIndexMarker189"/>cooling factor, <img src="image/Formula_B18753_05_019.png" alt=""/>. The value of <img src="image/Formula_B18753_05_020.png" alt=""/>is chosen in such a <a id="_idIndexMarker190"/>way that <img src="image/Formula_B18753_05_021.png" alt=""/>will still have a positive value after <img src="image/Formula_B18753_05_022.png" alt=""/>iterations. For example, <img src="image/Formula_B18753_05_023.png" alt=""/>, where <img src="image/Formula_B18753_05_024.png" alt=""/> is the expected final temperature after <img src="image/Formula_B18753_05_025.png" alt=""/> iterations:</li>
			</ul>
			<p><img src="image/Formula_B18753_05_026.png" alt=""/></p>
			<p>The following <a id="_idIndexMarker191"/>graph shows this annealing schedule:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B18753_05_002.jpg" alt="Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable criterion&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable criterion</p>
			<ul>
				<li><strong class="bold">Fast SA</strong>: This annealing <a id="_idIndexMarker192"/>schedule works by decreasing the temperature proportional to the current number of iterations, <img src="image/Formula_B18753_05_027.png" alt=""/>:</li>
			</ul>
			<p><img src="image/Formula_B18753_05_028.png" alt=""/></p>
			<p>This annealing schedule can be seen in the following graph:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B18753_05_003.jpg" alt="Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable criterion&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable criterion</p>
			<p>Based on <em class="italic">Figures 5.1</em> to <em class="italic">5.3</em>, we can see that no matter what annealing schedule scheme we use and what the initial temperature is, we will always have a lower <img src="image/Formula_B18753_05_029.png" alt=""/> value as the number of iterations increases, which means we will <em class="italic">accept fewer bad candidates as the number of iterations increases</em>. However, why do we need to accept bad candidates in the first place? The main purpose of SA not directly rejecting worse candidates, as <a id="_idIndexMarker193"/>in the SHC method, is to <em class="italic">balance the exploration and exploitation trade-off</em>. The high initial value of temperature allows SA to explore most of the parts of the <a id="_idIndexMarker194"/>hyperparameter space, and slowly focus on specific parts of the space as the number of iterations increases, just like how the metal annealing process works.</p>
			<p>Remember that <img src="image/Formula_B18753_05_030.png" alt=""/>only takes <img src="image/Formula_B18753_05_031.png" alt=""/>into account when the <em class="italic">candidate_point</em> is worse than the <em class="italic">current_point.</em> This means that, based on <em class="italic">Figure 5.4</em>, we can say that the worse the suggested candidate is (the higher <img src="image/Formula_B18753_05_032.png" alt=""/> is), the lower the value of <img src="image/Formula_B18753_05_033.png" alt=""/>will be, and thus, the lower the probability of accepting the suggested bad candidate. This is the other way around for <img src="image/Formula_B18753_05_034.png" alt=""/> in that the higher the value of <img src="image/Formula_B18753_05_035.png" alt=""/>is, the higher the value of <img src="image/Formula_B18753_05_036.png" alt=""/>will be, and thus, the higher the probability of accepting the suggested bad candidate (see <em class="italic">Figures 5.1</em> to <em class="italic">5.3</em>):</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B18753_05_004.jpg" alt="Figure 5.4 – Effect of Δf on the acceptable criterion&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Effect of Δf on the acceptable criterion</p>
			<p>To summarize, the <a id="_idIndexMarker195"/>following steps show how <em class="italic">SA works as a hyperparameter tuning method</em>:</p>
			<ol>
				<li value="1">Split the original full data into train and test sets (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>).</li>
				<li>Define the hyperparameter space, <em class="italic">H</em>, with the accompanied distributions.</li>
				<li>Define the initial temperature, <em class="italic">T0</em>.</li>
				<li>Define the objective function, <em class="italic">f</em>, based on the train set (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>).</li>
				<li>Define the stopping criterion. Usually, the number of trials is used. However, it is also possible to use the time taken or convergence as the stopping criterion.</li>
				<li>Set the current temperature, <em class="italic">T</em>, using the value from <em class="italic">T0</em>.</li>
				<li>Initialize a random set of hyperparameters that have been sampled from the hyperparameter space, <em class="italic">H.</em></li>
				<li>Set the selected set from <em class="italic">Step 7</em> as the current set, <em class="italic">current_set</em>, as well as the best set, <em class="italic">best_set</em>.</li>
				<li>Randomly sample the next candidate set, <em class="italic">candidate_set</em>, from the “neighbor” of the <em class="italic">current_set</em> within <a id="_idIndexMarker196"/>the hyperparameter space, <em class="italic">H</em>. The definition of the “neighbor” may differ across different types of hyperparameter distributions.</li>
				<li>Generate a random number between 0 and 1 from the uniform distribution and store it as <em class="italic">rnd</em>.</li>
				<li>Decide whether to accept the <em class="italic">candidate_set</em> or not:<ol><li value="1">Calculate the value of <img src="image/Formula_B18753_05_038.png" alt=""/> using the value of <em class="italic">T</em>, <em class="italic">f(candidate_set)</em>, and <em class="italic">f(current_set)</em>.</li><li>If the value of <em class="italic">rnd</em> is smaller than <img src="image/Formula_B18753_05_039.png" alt=""/>, then replace <em class="italic">current_set</em> with <em class="italic">candidate_set</em>.</li><li>If <em class="italic">candidate_set</em> is better than or equal to <em class="italic">current_set</em>, then replace <em class="italic">best_set</em> with <em class="italic">candidate_set</em>.</li></ol></li>
				<li>Apply the annealing schedule to the temperature, <em class="italic">T</em>.</li>
				<li>Repeat <em class="italic">Steps 9</em> to <em class="italic">12</em> until the stopping criterion is met.</li>
				<li>Train on the <a id="_idIndexMarker197"/>full training set using the <em class="italic">best_set</em> hyperparameters.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>The following <a id="_idIndexMarker198"/>table lists the list of pros and cons of SA as a hyperparameter <a id="_idIndexMarker199"/>tuning method:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B18753_05_005.jpg" alt="Figure 5.5 – Pros and cons of SA&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Pros and cons of SA</p>
			<p>In this section, we learned about SA, starting from what it is, how it works, what makes it different from SHC and random search, and its pros and cons. We will discuss another interesting heuristic search method that is inspired by the natural selection theory in the next section.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor049"/>Understanding genetic algorithms</h1>
			<p><strong class="bold">GAs</strong> are popular heuristic search methods that are inspired by Charles Darwin’s <em class="italic">theory of natural selection</em>. Unlike SA, which is classified as a <strong class="bold">single-point-based</strong> heuristic search method, GAs <a id="_idIndexMarker200"/>are categorized as <strong class="bold">population-based</strong> methods since <a id="_idIndexMarker201"/>they maintain a group of possible candidate <a id="_idIndexMarker202"/>solutions instead of just a single candidate solution at each trial. As a hyperparameter tuning method, you are recommended to utilize a GA when each trial doesn’t take too much time and you have enough computational resources, such as parallel computing resources.</p>
			<p>To have a better understanding of GAs, let’s start with a simple example. Let’s say we have a task to generate a pre-defined target word based on <em class="italic">only</em> a collection of words that are built from 26 alphabet letters in lowercase. For instance, the target word is “big,” and we have a collection that consists of the words “sea,” “pig,” “dog,” “bus,” and “tie.”</p>
			<p>Based on the given collection of words, what should we do to generate the word “big?” It is no doubt a very easy and straightforward task. We just have to pick the letter “b” from the word “bus,” “i” from the word “pig” or “tie,” and “g” from the word “dog.” Voila! We get the word “big.” You may be wondering how this example is related to the GA method or even the natural selection theory. This example is a very simple task and there is no need to utilize a GA to solve the problem. However, we need this kind of example so that you have a better understanding of how GAs work since you already know the correct answer in the first place.</p>
			<p>To solve this task using GA, you must know the three key items in GA related to the evolution theory. The first key item is <strong class="bold">variation</strong>. Imagine if the given collection of words consists of <a id="_idIndexMarker203"/>only the word “sea.” There’s no way we can generate the word “big” based on only the word “sea.” This is why variation is needed in the <strong class="bold">initial population</strong> (<em class="italic">the collection of words, in our example</em>). Without enough variation, we may not be able to achieve the optimal solution (<em class="italic">to generate the word “big,” in our example</em>) since there is no <strong class="bold">individual</strong> (<em class="italic">each word in the collection of words, in our example</em>) within the population that can evolve to the target word. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Population is <em class="italic">not the hyperparameter space</em>. In GAs or other population-based heuristic <a id="_idIndexMarker204"/>search methods, population refers to the candidates of the optimal hyperparameter set.</p>
			<p>The second key item is <strong class="bold">selection</strong>. You can think of this item as being similar to the idea of natural selection <a id="_idIndexMarker205"/>that happens in the real world. It’s about selecting individuals that are more suitable for the surrounding environment (<em class="italic">words that are similar to the word “big,” in our example</em>) and thus can survive in the world. In GAs, we need quantitative guidance for us to perform the selection, which is usually called the <strong class="bold">fitness function</strong>. This function helps us judge how good an individual is concerning the objective we <a id="_idIndexMarker206"/>want to achieve. In our example, we can create a fitness function that measures the proportion of indexes of the word that has the same letters as the target word in the corresponding indexes. For example, the word “tie” has a fitness score of <img src="image/Formula_B18753_05_041.png" alt=""/> since only one out of three indexes contains the same letters as the target word, which is the index one that has the letter “i.”</p>
			<p>Using this fitness function, we can evaluate the fitness score for each individual in the population, and then select which individuals should be added to the <strong class="bold">mating pool</strong> as <strong class="bold">parents</strong>. The mating pool is a collection of individuals that are considered high-quality individuals and thus called parents.</p>
			<p>The third key item is <strong class="bold">heredity</strong>. This item refers to the concept of <strong class="bold">reproduction</strong> or passing parents’ <strong class="bold">genes</strong> (<em class="italic">each letter in the word, in our example</em>) to their children or <strong class="bold">offspring</strong>. How is reproduction done in GAs? Taking the same spirit of natural selection, in Gas, we only <a id="_idIndexMarker207"/>perform the reproduction step from parents in the mating pool, meaning we only want to mate high-quality individuals with the hope to get only high-quality offspring in the next <strong class="bold">generation</strong> (a <em class="italic">new population is created in the next iteration</em>). There are two steps in the reproduction phase, namely the <strong class="bold">crossover</strong> and <strong class="bold">mutation</strong> steps. The crossover step is when we randomly <a id="_idIndexMarker208"/>mix or permute parents’ genes to generate offspring’s genes, while <a id="_idIndexMarker209"/>the mutation step is when we randomly change the value of offspring’s genes to add variation to the genes (see <em class="italic">Figure 5.6</em>). An individual that is mutated is called a <strong class="bold">mutant</strong>. The random value that is used in the mutation step <a id="_idIndexMarker210"/>should be drawn from the same gene’s distribution, meaning we can only use lower-case letters as the random values in our example, not floating points or integers:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B18753_05_006.jpg" alt="Figure 5.6 – The crossover and mutation steps in a GA&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – The crossover and mutation steps in a GA</p>
			<p>Now that you are <a id="_idIndexMarker211"/>aware of the three key items in a GA, we can start solving the task <a id="_idIndexMarker212"/>from the previous example using a GA. Let’s assume we haven’t been given the collection of words so that we can learn the complete procedures of the GA. The target word is still “big.”</p>
			<p>First, we must initialize a population with the <em class="italic">NPOP</em> number of individuals. The initialization process is usually done randomly to ensure we have enough variation in the population. By random, this means that the genes of each individual in the population are <a id="_idIndexMarker213"/>generated randomly. Let’s say we want to generate the initial population, which consists of seven individuals, where the generated results are “bee,” “tea,” “pie,” “bit,” “dog,” “cat,” and “dig.”</p>
			<p>Now, we can evaluate the fitness score of each individual in the population. Let’s say we use the fitness function that was defined previously. So, we got the following scores for each individual; “bee:” <img src="image/Formula_B18753_05_042.png" alt=""/>, “tea:” <img src="image/Formula_B18753_05_043.png" alt=""/>, “pie:” <img src="image/Formula_B18753_05_044.png" alt=""/>, “bit:” <img src="image/Formula_B18753_05_045.png" alt=""/>, ”dog:” <img src="image/Formula_B18753_05_046.png" alt=""/>, “cat:” <img src="image/Formula_B18753_05_047.png" alt=""/>, and “dig:” <img src="image/Formula_B18753_05_048.png" alt=""/>. </p>
			<p>Based on the fitness score of each individual, we can select which individual should be added to the mating pool as a parent. There are many strategies that we can adopt to select the best individuals from the population, but in this case, let’s just get the top three individuals based on the fitness score and randomly select individuals that have the same fitness score. Let’s say that, after running the selection strategy, we get a mating pool that consists of “bit,” “dig,” and “bee” as parents.</p>
			<p>The next step is to perform the crossover and mutation steps. Before that, however, we need to specify the crossover probability, <em class="italic">CXPB</em>, and the mutation probability, <em class="italic">MUTPB</em>, which defines the probability of crossing two parents in the mating pool and mutating an offspring, respectively. This means we are neither performing crossover on all parent pairs nor mutating <a id="_idIndexMarker214"/>all offspring – we will only perform those steps based on the predefined probability. Let’s say that only “dig” and “bee” have chosen to be crossed, and the resulting offspring of the crossover is “deg” and “bie.” So, the mating pool currently consists of “bit,” “deg,” and “bie.” Now, we need to perform mutation on “deg” and “bie.” Let’s say that after mutating them, we got “den” and “tie.” This means that the mating pool is currently consisting of “bit,” “den,” and “tie.” </p>
			<p>After performing the crossover and mutation steps, we need to generate a new population for the next generation. The new population will consist of all crossed parents, mutated offspring, as well as other individuals from the current population. So, the next population consists of “bit,” “den,” “tie,” “tea,” “pie,” “dog,” and “cat.” </p>
			<p>Based on the new population, we have to repeat the selection, crossover, and mutation process. This procedure needs to be done <em class="italic">NGEN</em> times, where NGEN refers to the number of generations, and it is predefined by the developer. </p>
			<p>The following <a id="_idIndexMarker215"/>steps define <em class="italic">how GA works in general</em>, as an optimization m<a id="_idTextAnchor050"/>ethod:</p>
			<ol>
				<li value="1">Define the population size, <em class="italic">NPOP</em>, the crossover probability, <em class="italic">CXPB</em>, the mutation probability, <em class="italic">MUTPB</em>, and the number of generations or number of trials, <em class="italic">NGEN</em>.</li>
				<li>Define the fitness function, <em class="italic">f</em>.</li>
				<li>Initialize a population with <em class="italic">NPOP</em> individuals, where each individual’s genes are initialized randomly.</li>
				<li>Evaluate all individuals in the population based on the fitness function, <em class="italic">f</em>.</li>
				<li>Select the best individuals based on <em class="italic">Step 4</em> and store them in a mating pool.</li>
				<li>Perform the crossover process on the parents in the mating pool with a probability of <em class="italic">CXPB</em>.</li>
				<li>Perform the mutation process on the offspring results from <em class="italic">Step 8</em> with a probability of <em class="italic">MUTPB</em>.</li>
				<li>Generate a new population consisting of all the individuals from <em class="italic">Step 6</em>, <em class="italic">Step 7</em>, and the rest of the individuals from the current population.</li>
				<li>Replace the <a id="_idIndexMarker216"/>current population with the new population.</li>
				<li>Repeat <em class="italic">Steps 6</em> to <em class="italic">9</em> <em class="italic">NGEN</em> times.</li>
			</ol>
			<p>Now, let’s look at a more concrete example of how a GA works in general. We will use the same objective function that we used in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Evaluating Machine Learning Models</em> and treat this as a minimization problem. The objective function is defined as follows:</p>
			<p><img src="image/Formula_B18753_05_049.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_05_050.png" alt=""/> is the noise that follows the standard normal distribution. We are only going to perform a search within the <img src="image/Formula_B18753_05_051.png" alt=""/> range. It is worth noting that in this example, we assume that we <a id="_idIndexMarker217"/>know what the true objective function is. However, in practice, this function is unknown. In this case, each individual will only have one gene, which is the value of <img src="image/Formula_B18753_05_052.png" alt=""/> itself.</p>
			<p>Let’s say we define the hyperparameters for the GA method as <em class="italic">NPOP = 25</em>, <em class="italic">CXPB = 0.5</em>, <em class="italic">MUTPB = 0.15</em>, and <em class="italic">NGEN = 6</em>. As for the strategy of each <strong class="bold">genetic operator</strong>, we are using the <strong class="bold">Tournament</strong>, <strong class="bold">Blend</strong>, and <strong class="bold">PolynomialBounded</strong> strategies for selection, crossover, and mutation operators, respectively. The <em class="italic">Tournament</em> selection strategy works by <a id="_idIndexMarker218"/>selecting the best individuals among <em class="italic">tournsize</em> and the randomly chosen individual’s <em class="italic">NPOP</em> times, where <em class="italic">tournsize</em> is the number of individuals participating in the tournament. The <em class="italic">Blend</em> crossover strategy works by performing <a id="_idIndexMarker219"/>a linear combination between two continuous individual genes, where the weight of the linear combination is governed by the <em class="italic">alpha</em> hyperparameter. The <em class="italic">PolynomialBounded</em> mutation strategy works by passing continuous <a id="_idIndexMarker220"/>individual genes to a predefined polynomial mapping. </p>
			<p>There are many strategies available that you can follow based on your hyperparameter space specification. We will talk more about different strategies and how to implement the GA method using the <strong class="bold">DEAP</strong> package in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>. For now, let’s see the results of applying a GA on <a id="_idIndexMarker221"/>the dummy objective function, <em class="italic">f</em>. Note that the points in each plot correspond to each individual in the population:</p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B18753_05_007.jpg" alt="Figure 5.7 – GA process&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – GA process</p>
			<p>Based on the preceding figure, we can see that in the first generation, individuals are scattered all around the place since it is initialized randomly. In the second generation, several individuals that are initialized around point –1.0 moved to other places that have lower fitness scores. However, in the third generation, there are new individuals around point –1.0 again. This may <a id="_idIndexMarker222"/>be due to the random mutation operator that’s been applied to them. There are also several individuals stuck in the local optima, which is around point –0.5. In the fourth generation, most of the individuals have moved to places with lower fitness scores, although some of them are still stuck in the local optima. In the fifth generation, individuals are starting to converge in several places. </p>
			<p>Finally, in the sixth generation, all of them converged to the near-global optima, which is around point 1.5. Note that we still have <em class="italic">NPOP=25</em> individuals in the sixth generation, but all of them are located in the same place, which is why you can only see one <a id="_idIndexMarker223"/>dot in the plot. This also applies to other generations if you see that there are fewer than 25 individuals in the plot. The convergence trend across <a id="_idIndexMarker224"/>generations can be seen in the following graph:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B18753_05_008.jpg" alt="Figure 5.8 – Convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Convergence plot</p>
			<p>The trend that’s shown in the preceding graph matches our previous analysis. However, we can get additional information from this plot. At first, many of the individuals are located in places with high fitness scores, but some individuals already get the best fitness score. Across generations, most of the individuals started to converge, and finally, in the last generation, all individuals had the best fitness score. It is worth noting that, in practice, it is not guaranteed that a GA will achieve the global optimal solution. </p>
			<p>At this point, you may be wondering, how can a GA be adopted as a hyperparameter tuning method? What is the corresponding definition of all terms in the GA within the context of hyperparameter tuning? What does an individual mean when performing hyperparameter tuning with a GA? </p>
			<p><em class="italic">As a hyperparameter tuning method</em>, the GA method treats a set of hyperparameters as an individual where the hyperparameter values are the genes. To have better clarity on what each important term in the GA method means, in the context of hyperparameter tuning, please <a id="_idIndexMarker225"/>refer to the following table:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/B18753_05_009.jpg" alt="Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context</p>
			<p>Now that you are aware <a id="_idIndexMarker226"/>of the corresponding definition of each important <a id="_idIndexMarker227"/>term in the GA method, we can define the formal procedure to utilize <em class="italic">the GA method as a hyperparameter tuning method</em>:</p>
			<ol>
				<li value="1">Split the original full data into train and test sets.</li>
				<li>Define the hyperparameter space, <em class="italic">H</em>, with the accompanied distributions.</li>
				<li>Define the population size, <em class="italic">NPOP</em>.</li>
				<li>Define the crossover probability, <em class="italic">CXPB</em>, and mutation probability, <em class="italic">MUTPB</em>.</li>
				<li>Define the number of trials, <em class="italic">NGEN</em>, as the stopping criterion.</li>
				<li>Define the objective function, <em class="italic">f</em>, based on the train set.</li>
				<li><em class="italic">Initialize</em> a population with <em class="italic">NPOP</em> sets of hyperparameters, where each set is drawn randomly from the hyperparameter space, <em class="italic">H</em>.</li>
				<li>Evaluate all hyperparameter sets in the population based on the objective function, <em class="italic">f</em>.</li>
				<li><em class="italic">Select</em> several best candidate sets based on <em class="italic">Step 8</em>.</li>
				<li>Perform <em class="italic">crossover</em> on candidate sets from <em class="italic">Step 9</em> with a probability of <em class="italic">CXPB</em>.</li>
				<li>Perform <em class="italic">mutation</em> on the crossed candidate sets from <em class="italic">Step 10</em> with a probability of <em class="italic">MUTPB</em>.</li>
				<li>Generate a new population consisting of all sets of hyperparameters from <em class="italic">Step 10</em>, <em class="italic">Step 11</em>, and the rest of the sets from the current population. The new population will also consist of <em class="italic">NPOP</em> sets of hyperparameters.</li>
				<li>Repeat <em class="italic">Steps 8</em> to <em class="italic">12</em> <em class="italic">NGEN</em> times.</li>
				<li>Train on the full <a id="_idIndexMarker228"/>training set using the final hyperparameter values.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>It is worth noting that when utilizing a GA as a hyperparameter tuning method, the GA itself has four hyperparameters, namely <em class="italic">NPOP</em>, <em class="italic">CXPB</em>, <em class="italic">MUTPB</em>, and <em class="italic">NGEN</em>, that control the performance of the hyperparameter tuning results, as well as the <em class="italic">exploration versus exploitation trade-off</em>. To be more precise, <em class="italic">CXPB</em> and <em class="italic">MUTPB</em>, or the crossover and mutation probability, respectively, are responsible for controlling the <em class="italic">exploration</em> rate, while the <em class="italic">selection</em> step, along with its strategy, controls the <em class="italic">exploitation</em> rate.</p>
			<p>The following table <a id="_idIndexMarker229"/>lists the pros and cons of using a GA as a <a id="_idIndexMarker230"/>hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B18753_05_010.jpg" alt="Figure 5.10 – Pros and Cons of the GA method&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Pros and Cons of the GA method</p>
			<p>The need to evaluate all individuals in each generation means we multiplied the original time complexity that our objective has by <em class="italic">NPOP * NGEN</em>. It’s very costly! That’s why the GA method is not suitable for you if you have an expensive objective function and/or low computational resources. However, if you do have time to wait for the experiment to be done, and you have massively parallel computing resources, then the GA method is suitable <a id="_idIndexMarker231"/>for you. From a theoretical perspective, the GA method can also work with various types of hyperparameters – we just need to choose the appropriate crossover and mutation strategies for the corresponding hyperparameters. The GA method is better than SA in terms of having a population to guide which part of the subspace needs to be exploited more. However, it is worth noting that the GA method can still be stuck in local optima. </p>
			<p>In this section, we discussed the GA method, starting with what it is, how it works both in terms of its general setup and the hyperparameter tuning context, and its pros and cons. We will discuss another interesting population-based heuristic search method in the next section.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor051"/>Understanding particle swarm optimization</h1>
			<p><strong class="bold">PSO</strong> is also a population-based heuristic search method, similar to the GA method. PSO is inspired by the <a id="_idIndexMarker232"/>schools of fish and flocks of birds’ social interaction in nature. As a hyperparameter tuning method, PSO is suggested to be utilized if your search space contains many non-categorical hyperparameters, each trial doesn’t take much time, and you have enough computational resources – especially parallel computing resources.</p>
			<p>PSO is one of the most popular methods within the bigger <strong class="bold">swarm intelligence</strong> (<strong class="bold">SI</strong>) group of methods. There <a id="_idIndexMarker233"/>are various methods in SI that are inspired by the social interaction of animals in nature, such as herds of land animals, colonies of ants, flocks of birds, schools of fish, and many more. The common characteristics of SI methods <a id="_idIndexMarker234"/>are <em class="italic">population-based</em>, individuals within the population are relatively <em class="italic">similar to each other</em>, and the ability of the population to move in a specific direction systemically <em class="italic">without a single coordinator</em> inside or outside the population. In other words, the population can organize themselves based on the <em class="italic">local interactions</em> of individuals interacting with each other and/or the surrounding environment.</p>
			<p>When a flock of birds is looking for food, it is believed that each bird can contribute to the group by sharing information about their sights, so that the group can move in the right direction. PSO is a method that simulates the movement of a flock of birds to optimize the objective <a id="_idIndexMarker235"/>function. In PSO, the flock of birds is called a <strong class="bold">swarm</strong> and each <a id="_idIndexMarker236"/>bird is called a <strong class="bold">particle</strong>. </p>
			<p>Each particle is defined by its <strong class="bold">position</strong> vector and <strong class="bold">velocity</strong> vector. The movement of each particle consists <a id="_idIndexMarker237"/>of both stochastic and deterministic components. In <a id="_idIndexMarker238"/>other words, the movement of each particle is not only based on a predefined rule but is also influenced by random components. Each particle also remembers its own <strong class="bold">best position</strong>, which gives the best objective function value along the trajectory it has passed. Then, along with the <strong class="bold">global best position</strong>, it is used to update the <a id="_idIndexMarker239"/>velocity and position of each particle at a particular time. The global best position is just the position of the best particle from the previous step. </p>
			<p>Let’s say that <img src="image/Formula_B18753_05_053.png" alt=""/> is the position vector in a <em class="italic">d</em>-dimensional space of the <img src="image/Formula_B18753_05_054.png" alt=""/> particle out of <em class="italic">m</em> particles <a id="_idIndexMarker240"/>in the swarm, and that <img src="image/Formula_B18753_05_055.png" alt=""/> is the velocity vector of the same size for the <img src="image/Formula_B18753_05_056.png" alt=""/> particle, as shown here:</p>
			<p><img src="image/Formula_B18753_05_057.png" alt=""/></p>
			<p><img src="image/Formula_B18753_05_058.png" alt=""/></p>
			<p>Let’s also define the best position for each particle and the global best position vectors, respectively:</p>
			<p><img src="image/Formula_B18753_05_059.png" alt=""/></p>
			<p><img src="image/Formula_B18753_05_060.png" alt=""/></p>
			<p>The following formulas define how each particle’s position and velocity vectors are updated in each iteration:</p>
			<p><img src="image/Formula_B18753_05_061.png" alt=""/></p>
			<p><img src="image/Formula_B18753_05_062.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_05_063.png" alt=""/>, <img src="image/Formula_B18753_05_064.png" alt=""/>, and <img src="image/Formula_B18753_05_065.png" alt=""/> are the hyperparameters that control the <em class="italic">exploration versus exploitation trade-off</em>. <img src="image/Formula_B18753_05_066.png" alt=""/> has a value <a id="_idIndexMarker241"/>between zero and one is usually <a id="_idIndexMarker242"/>called the <strong class="bold">inertia weight coefficient</strong>, while <img src="image/Formula_B18753_05_067.png" alt=""/> and <img src="image/Formula_B18753_05_068.png" alt=""/> are <a id="_idIndexMarker243"/>called the <strong class="bold">cognitive</strong> and <strong class="bold">social coefficients</strong>, respectively. <img src="image/Formula_B18753_05_069.png" alt=""/> and <img src="image/Formula_B18753_05_070.png" alt=""/> are the random values between zero and one and act as the stochastic components of the particle movement. Note that the <em class="italic">d</em>-dimensions of the position and <a id="_idIndexMarker244"/>velocity vectors refer to the number of hyperparameters we have in the search space, while the <em class="italic">m</em> particles refer to the number of candidate hyperparameters that are sampled from the hyperparameter space.</p>
			<p>Updating the velocity vector may seem intimidating the first time, but actually, you can understand it more easily by treating the formula as three separate parts. The first part, or the left-most side of the formula, aims to update the next velocity proportional to the current velocity. The second part, or the middle part of the formula, aims to update the velocity toward the direction of the best position that the <img src="image/Formula_B18753_05_071.png" alt=""/> particle has, while also adding a stochastic component to it. The third part, or the right-most side of the formula, aims to bring the <img src="image/Formula_B18753_05_072.png" alt=""/> particle closer to the global best position, with additional random behavior applied to it. The following diagram helps illustrate this: </p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B18753_05_011.jpg" alt="Figure 5.11 – Updating the particle’s position and velocity&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Updating the particle’s position and velocity</p>
			<p>The preceding diagram isn’t the same as the stated formula since the random components and the hyperparameters are missing from the picture. However, this diagram can help <a id="_idIndexMarker245"/>us understand the high-level concept of how each particle’s position and velocity vectors are updated in each iteration. We can see that the final updated velocity (<em class="italic">see the orange line</em>) is calculated based on three vectors, namely the current velocity (<em class="italic">see the brown line</em>), the particle best position (<em class="italic">see the green line</em>), and the global best position (<em class="italic">see the purple line</em>). Based on the final updated velocity, we can get the updated position of the <img src="image/Formula_B18753_05_073.png" alt=""/> particle – that is, <img src="image/Formula_B18753_05_074.png" alt=""/>.</p>
			<p>Now, let’s discuss how the hyperparameters affect the formula. The inertia weight coefficient, <img src="image/Formula_B18753_05_075.png" alt=""/>,controls how <a id="_idIndexMarker246"/>much we want to put our focus on the current velocity when updating the velocity vector. On the other hand, the cognitive coefficient, <img src="image/Formula_B18753_05_076.png" alt=""/>, and the social coefficient, <img src="image/Formula_B18753_05_077.png" alt=""/>, control how much we should focus on the particle’s past trajectory history and swarm’s search result, respectively. When we set <img src="image/Formula_B18753_05_078.png" alt=""/>, we don’t take into account the influence of the best position of the <img src="image/Formula_B18753_05_079.png" alt=""/> particle, which may lead us to be <em class="italic">trapped in the local optima</em>. When we set <img src="image/Formula_B18753_05_080.png" alt=""/>, we ignore the influence of the global best position, which may lead us to a <em class="italic">slower convergence</em> speed.</p>
			<p>Now that you are aware of the position and velocity components of each particle in the swarm, take a look at the following steps, which define <em class="italic">how PSO works in general</em> as an <a id="_idIndexMarker247"/>optimization method:</p>
			<ol>
				<li value="1">Define the swarm size, <em class="italic">N</em>, the inertia weight coefficient, <em class="italic">w</em>, the cognitive coefficient, <em class="italic">c1</em>, the social coefficient, <em class="italic">c2</em>, and the maximum number of trials.</li>
				<li>Define the fitness function, <em class="italic">f</em>.</li>
				<li>Initialize a swarm with <em class="italic">N</em> particles, where each particle’s position and velocity vectors are initialized randomly.</li>
				<li>Set each particle’s current position vector as their best position vector, <em class="italic">pbi</em>.</li>
				<li>Set the current global best position, <em class="italic">gb</em>, by selecting a position vector from all <em class="italic">N</em> particles that have the most optimal fitness score.</li>
				<li>Update each particle’s position and velocity vector based on the updating formula.</li>
				<li>Evaluate all the particles in the swarm based on the fitness function, <em class="italic">f</em>.</li>
				<li>Update each particle’s best position vectors, <em class="italic">pbi</em>:<ol><li value="1">Compare each particle’s current fitness score from <em class="italic">Step 7</em> with its <em class="italic">pbi</em> fitness score.</li><li>If the current fitness score is better than the <em class="italic">pbi</em> fitness score, update <em class="italic">pbi</em> with the current position vector.</li></ol></li>
				<li>Update the global best position vector, <em class="italic">gb</em>:<ol><li value="1">Compare each particle’s current fitness score from <em class="italic">Step 7</em> with the previous <em class="italic">gb</em> fitness score.</li><li>If the current fitness score is better than the <em class="italic">gb</em> fitness score, update <em class="italic">gb</em> with the current position vector.</li></ol></li>
				<li>Update each <a id="_idIndexMarker248"/>particle’s position and velocity vector based on the updating formula.</li>
				<li>Repeat <em class="italic">Steps 7</em> to <em class="italic">10</em> until the maximum number of trials is reached.</li>
				<li>Return the final global best position, <em class="italic">gb</em>.</li>
			</ol>
			<p>It is worth noting that the definition of the optimal fitness score (or a better fitness score in the previously stated procedure) will depend on what type of optimization problem you are trying <a id="_idIndexMarker249"/>to solve. If it is a minimization problem, then a smaller fitness score is better. If it is a maximization problem, then it is the other way around.</p>
			<p>To have even a better understanding of how PSO works, let’s go through an example. Let’s define the fitness function as follows:</p>
			<p><img src="image/Formula_B18753_05_081.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_05_082.png" alt=""/> and <img src="image/Formula_B18753_05_083.png" alt=""/> are only defined within the <img src="image/Formula_B18753_05_084.png" alt=""/> range. The following <em class="italic">contour plot</em> shows what our objective function looks like. We will learn more about how to implement PSO using the <strong class="bold">DEAP</strong> package in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="image/B18753_05_012.jpg" alt="Figure 5.12 – A contour plot showing the objective function and its global minimum&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – A contour plot showing the objective function and its global minimum</p>
			<p>Here, you can see that the global minimum (<em class="italic">see the red cross marker</em>) is located at (0.497, 0.295) with <a id="_idIndexMarker250"/>an objective function value of –0.649. Let’s try to utilize PSO to see how well it estimates the minimum value of the objective function compared to the true global minimum. Let’s say we define the hyperparameter for PSO as <em class="italic">N=20</em>, <em class="italic">w=0.5</em>, <em class="italic">c1=0.3</em>, and <em class="italic">c2=0.5</em>, and set the maximum number of trials to 16. </p>
			<p>You can see the initial swarm illustration in the following contour plot. The blue dots refer to each <a id="_idIndexMarker251"/>of the particles, the blue arrow on each particle refers to the particle’s velocity vector, the black dots refer to each particle’s best position vectors, and the red star marker refers to the current global best position vector at a particular iteration:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B18753_05_013.jpg" alt="Figure 5.13 – A PSO initial swarm&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – A PSO initial swarm</p>
			<p>Since the initial particles at the swarm are initialized randomly, the direction of the velocity vectors is all over the place (see <em class="italic">Figure 5.13</em>). You can see how each particle’s position and velocity vectors are updated in each iteration, along with the global best position vector, as shown here: </p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B18753_05_014.jpg" alt="Figure 5.14 – PSO process&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – PSO process</p>
			<p>Even at the first iteration, each particle’s velocity vector is pointing toward the global minimum, which <a id="_idIndexMarker252"/>is located in the bottom left of the plot. In each iteration, the position and velocity vectors are updated and move closer to the global minimum. At the end of the iteration loop, most of the particles are located around the global minimum position, where the final global best position vector is located at (0.496, 0.290) with a fitness score of around –0.648. This estimation is very close to the true global minimum of the objective function! </p>
			<p>It is worth noting that the velocity vector of each particle contains two components: magnitude and direction. The magnitude will impact the length of the velocity vector in <em class="italic">Figure 5.14</em>. While <a id="_idIndexMarker253"/>you may not see the difference in length between each particle’s velocity vector, they are different from each other!</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout"><em class="italic">As a hyperparameter tuning method</em>, in the PSO method, <em class="italic">particle</em> and <em class="italic">swarm</em> refer to the candidate set of hyperparameters that are sampled from the hyperparameter space and the collection of hyperparameter set candidates, respectively. The position vector of each particle refers to the values of each hyperparameter in a particle. Finally, the velocity vector refers to the <em class="italic">delta of hyperparameter values</em> that will be utilized to update the <a id="_idIndexMarker254"/>values of each hyperparameter in a particle.</p>
			<p>The following <a id="_idIndexMarker255"/>steps define <em class="italic">how PSO works as a hyperparameter tuning method</em>:</p>
			<ol>
				<li value="1">Split the original full data into train and test sets.</li>
				<li>Define the hyperparameter space, <em class="italic">H</em>, with the accompanied distributions.</li>
				<li>Define the collection size, <em class="italic">N</em>, the inertia weight coefficient, <em class="italic">w</em>, the cognitive coefficient, <em class="italic">c1</em>, the social coefficient, <em class="italic">c2</em>, and the maximum number of trials.</li>
				<li>Define the objective function, <em class="italic">f</em>, based on the train set.</li>
				<li>Initialize a collection of <em class="italic">N</em> sets of hyperparameters, where each set is drawn randomly from the hyperparameter space, <em class="italic">H</em>. </li>
				<li>Randomly initialize the velocity vector for each set of hyperparameters in the collection.</li>
				<li>Set each set’s current hyperparameter values as their best values, <em class="italic">pbi</em>.</li>
				<li>Set the current global best set of hyperparameters, <em class="italic">gb</em>, by selecting a set from all <em class="italic">N</em> sets of hyperparameters that have the most optimal objective function score.</li>
				<li>Update each set’s hyperparameter values and velocity vector based on the updating formula.</li>
				<li>Evaluate all sets of hyperparameters in the collection based on the objective function, <em class="italic">f</em>.</li>
				<li>Update each set’s best hyperparameter values, <em class="italic">pbi</em>:<ol><li value="1">Compare each set’s current score from <em class="italic">Step 10</em> with its <em class="italic">pbi</em> score.</li><li>If the current score is better than the <em class="italic">pbi</em> score, update <em class="italic">pbi</em> with the current hyperparameter values.</li></ol></li>
				<li>Update the global best set of hyperparameters, <em class="italic">gb</em>:<ol><li value="1">Compare each set’s current score from <em class="italic">Step 10</em> with the previous <em class="italic">gb</em> score.</li><li>If the current score is better than the <em class="italic">gb</em> score, update <em class="italic">gb</em> with the current set of hyperparameters.</li></ol></li>
				<li>Update each set’s hyperparameter values and velocity vector based on the updating formula.</li>
				<li>Repeat <em class="italic">Steps 10</em> to <em class="italic">13</em> until the maximum number of trials is reached.</li>
				<li>Train on the <a id="_idIndexMarker256"/>full training set using the global best set of hyperparameters.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>One issue with the updating formula in the PSO method is that it only works on numerical variables, especially <a id="_idIndexMarker257"/>continuous variables, meaning we can’t directly utilize the original PSO as a hyperparameter tuning method if our hyperparameter space contains discrete hyperparameters. Motivated by this issue, there are several variants of PSO that are designed to be able to work in discrete spaces as well. The first variant is designed to work specifically for <a id="_idIndexMarker258"/>binary variables and is called <strong class="bold">binary PSO</strong>. In this variant, the updating formula for the velocity vector is the same, meaning we still treat the velocity vector in <a id="_idIndexMarker259"/>a continuous space, but the updating formula for the position vector is modified, like so:</p>
			<p><img src="image/Formula_B18753_05_085.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_05_086.png" alt=""/> is a random number drawn from a uniform distribution within the <img src="image/Formula_B18753_05_087.png" alt=""/>, <img src="image/Formula_B18753_05_088.png" alt=""/> interval, and the<em class="italic"> j</em> subscript refers to each component in the <em class="italic">i</em><span class="superscript">th</span><em class="italic"> </em>particle. As you can see, in the binary PSO variant, we can work within the discrete space, but we are restricted to only having binary variables. </p>
			<p>What about when we have a combination of discrete and continuous numerical hyperparameters? For example, our hyperparameter space for a neural network model contains the learning rate, dropout rate, and the number of layers. We can’t utilize the original PSO method directly since the number of layers hyperparameter expects an integer input, not a continuous or floating-point input. We also can’t utilize the binary PSO variant since the learning rate and dropout rate are continuous, and the number of layers hyperparameter is also not binary.</p>
			<p>One simple thing we can do is <em class="italic">round the updated velocity</em> vector component values, but only for components that correspond to the discrete position component, before passing it to the position vector updating formula. This way, we can ensure that our discrete hyperparameters will still always be within the discrete space. However, this workaround still has an issue. The rounding operation may make the updating procedure of the velocity vector suboptimal. Why? Because of the possibility that no matter the updated values of the velocity vector, so long as they are still within a similar range of one integer point, then the position vector will not be updated anymore. This will contribute to a lot of redundant computational costs.</p>
			<p>There is another workaround to make PSO operate well both in continuous and discrete spaces. On top of rounding the updated velocity vector component values, we can also <em class="italic">update the inertia weight coefficient dynamically</em>. The motivation is to help a particle focus on its past velocity values so that it is not stuck in the local or global optimum, which is influenced by <img src="image/Formula_B18753_05_089.png" alt=""/> or <img src="image/Formula_B18753_05_090.png" alt=""/>. The dynamic inertia weight updating procedure can be done based on several factors, such as the relative distance between its current position vector and its best position vector, the difference between the current number of trials and the maximum number <a id="_idIndexMarker260"/>of trials, and many more.</p>
			<p>There are many variants of how we can dynamically update the inertia weight coefficient during trials; we will leave it to you to choose what works well for your specific case.</p>
			<p>Although we can modify the updating formula in PSO to make it work not only for continuous but also discrete variables, we are still faced with several issues, as stated previously. Thus, to utilize the maximum power of PSO within the continuous space, there’s another variant of PSO that tries to synergize PSO with the Bayesian optimization method, called <strong class="bold">PSO-BO</strong>. The goal of PSO-BO is to utilize PSO as a replacement for Bayesian <a id="_idIndexMarker261"/>optimization’s acquisition function optimizer (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>). So, rather than using a second-order optimization method to optimize the acquisition function, we can utilize PSO as the optimizer to help decide which set of hyperparameters to be tested in the next trial of the Bayesian optimization hyperparameter tuning procedure.  </p>
			<p>The following <a id="_idIndexMarker262"/>table summarizes <a id="_idIndexMarker263"/>the pros and cons of utilizing PSO as a hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/B18753_05_015.jpg" alt="Figure 5.15 – Pros and cons of PSO&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Pros and cons of PSO</p>
			<p>Now that you <a id="_idIndexMarker264"/>are aware of what PSO is, how <a id="_idIndexMarker265"/>it works, its several variants, and its pros and cons, let’s discuss another interesting population-based heuristic search method.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor052"/>Understanding Population-Based Training</h1>
			<p><strong class="bold">PBT</strong> is a population-based heuristic search method, just like the GA method and PSO. However, PBT is not a <a id="_idIndexMarker266"/>nature-inspired algorithm like GA or PSO. Instead, inspired by the GA method itself. PBT is suggested for when you are working with a neural-network-based type of model and <em class="italic">just need the final trained model</em> without knowing the specifically chosen hyperparameter configurations. </p>
			<p>PBT is specifically designed to <em class="italic">work only with a neural network-based</em> type of models, such as a multilayer perceptron, deep reinforcement learning, transformers, GAN, and any other neural network-based models. It can be said that PBT does both hyperparameter tuning and <em class="italic">model training</em> since the weights of the neural network model are inherited during the process. So, PBT is not only for choosing the most optimal hyperparameter configurations but also for transferring the weights or parameters of the model to other individuals within the population. That’s why the output of PBT is not a hyperparameter configuration but a model.</p>
			<p>PBT is a <em class="italic">hybrid</em> method of the <em class="italic">random search</em> and <em class="italic">sequential search</em> methods, such as manual search and Bayesian search (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring Exhaustive Search </em>and <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em> for more details). Random search is a very <a id="_idIndexMarker267"/>good method for finding a good subspace for sensitive hyperparameters. Sequential search methods tend to give better performance than <a id="_idIndexMarker268"/>random search if we have enough computational resources and time to execute the optimization process. However, the fact that those methods need to be executed sequentially makes the experiment take a very long time to run. PBT comes with a solution to combine the best of both worlds into a <em class="italic">single training optimization process</em>, meaning the model training and hyperparameter tuning process are merged into a single process.</p>
			<p>The term <em class="italic">Population-Based</em> in PBT comes from the fact that it is inspired by the GA method in terms of utilizing knowledge of the whole population to produce a better-performing individual. Note that the <strong class="bold">individual</strong> part of PBT refers to each of the <em class="italic">N</em> models with different parameters and hyperparameters in the <strong class="bold">population</strong> or a collection of all those <em class="italic">N</em> models.</p>
			<p>The search process in PBT starts by <em class="italic">initializing a population</em>, <em class="italic">P</em>, that contains <em class="italic">N</em> models, <img src="image/Formula_B18753_05_091.png" alt=""/>, with their own randomly sampled parameters, <img src="image/Formula_B18753_05_092.png" alt=""/>, and randomly sampled hyperparameters, <img src="image/Formula_B18753_05_093.png" alt=""/>. Within each iteration of the search process, the <em class="italic">training step</em> is triggered for each of the <em class="italic">N</em> models. The training step consists of both forward and backward propagation procedures that utilize gradient-based optimization methods, just like the usual training <a id="_idIndexMarker269"/>procedure for a neural network-based model. Once the training step is done, the next step is to perform an <em class="italic">evaluation step</em>. The purpose of the evaluation step is to evaluate the current model’s <em class="italic">Mi</em> performance on the unseen validation data. </p>
			<p>Once the model, <em class="italic">Mi</em>, is considered <em class="italic">ready</em>, PBT will trigger the <em class="italic">exploit</em> and <em class="italic">explore</em> steps. The definition of a model being ready may vary, but we can define “ready” as passing a predefined number of steps or passing a predefined performance threshold. Both the exploit and explore steps have the same goal, which is to update the model’s parameters and hyperparameters. The difference is determined by how they do the update process. </p>
			<p>The <strong class="bold">exploit</strong> step will decide, based on the evaluation results from the whole population, whether <a id="_idIndexMarker270"/>to keep utilizing the current set of parameters and hyperparameters or to focus on a more promising set. For example, the exploit step can be done by replacing a model that is considered as part of the bottom X% models in the whole population with a randomly sampled model from the top X% models in the population. Note that a model consists of all the parameters and hyperparameters. On the other hand, the <strong class="bold">explore</strong> step updates the <a id="_idIndexMarker271"/>model’s set of hyperparameters, <em class="italic">not parameters</em>, by proposing a new set. You can propose a new set by randomly perturbing the current set of hyperparameters with a predefined probability or by resampling the set of hyperparameters from the top X% models in the population. Note that this exploration step is only done on the chosen model from the exploitation step. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The exploration step in PBT is inspired by random search. This step can identify which subspace of <a id="_idIndexMarker272"/>hyperparameters needs to be explored more using <em class="italic">partially trained models</em> chosen from the exploitation step. The evaluation <a id="_idIndexMarker273"/>step that is done within the search process also enables us to remove the drawback of the sequential optimization process.</p>
			<p>The exploitation and exploration procedure in the PBT method allows us to update a model’s set of hyperparameters in an <em class="italic">online fashion</em>, while also putting more focus on the promising <a id="_idIndexMarker274"/>hyperparameter and weight space. The iterative process of train-eval-exploit-explore is performed <em class="italic">asynchronously in parallel</em> for each of the <em class="italic">N</em> individuals in the population until the stopping criterion is met.  </p>
			<p>The following <a id="_idIndexMarker275"/>steps summarize <em class="italic">how PBT works as a single training optimization process</em>:</p>
			<ol>
				<li value="1">Split the original full data into train, validation, and test sets (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>). </li>
				<li>Define the hyperparameter space, <em class="italic">H</em>, with the accompanied distributions.</li>
				<li>Define the population size, <em class="italic">N</em>, the exploration perturbation factor, <em class="italic">perturb_fact</em>, the exploration resampling probability, <em class="italic">resample_prob</em>, and the exploitation fraction, <em class="italic">frac</em>.</li>
				<li>Define the model’s <em class="italic">readiness criterion</em>. Usually, the number of SGD optimization steps is used. However, it is also possible to use the model’s performance threshold as the criterion.</li>
				<li>Define the <em class="italic">checkpoint directory</em> that is used to store the model’s weights and hyperparameters.</li>
				<li>Define the evaluation function, <em class="italic">f</em>.</li>
				<li>Initialize a population, <em class="italic">P</em>, that contains <em class="italic">N</em> models, <img src="image/Formula_B18753_05_094.png" alt=""/>, with their own randomly sampled parameters, <img src="image/Formula_B18753_05_095.png" alt=""/>, and randomly sampled hyperparameters, <img src="image/Formula_B18753_05_096.png" alt=""/>, from the hyperparameter space, <em class="italic">H</em>.</li>
				<li>For each model in the population, <em class="italic">P</em>, run the following steps <em class="italic">in parallel</em>:<ol><li value="1">Run one <a id="_idIndexMarker276"/>step of the training process for the model, <em class="italic">M</em><span class="subscript">i</span>, with the <img src="image/Formula_B18753_05_097.png" alt=""/> parameter and a set of hyperparameters, <img src="image/Formula_B18753_05_098.png" alt=""/>.</li><li>If the <em class="italic">readiness criterion</em> has been met, do the following. If not, go back to <em class="italic">Step I</em>:<ul><li>Perform the <em class="italic">evaluation</em> step based on <em class="italic">f</em> on the validation set.</li><li>Perform the <em class="italic">exploitation</em> step on the model, <em class="italic">M</em><span class="subscript">i</span>, based on the predefined exploitation fraction, <em class="italic">frac</em>. This step will result in a new set of parameters and hyperparameters.</li><li>Perform the <em class="italic">exploration</em> step on the set of hyperparameters from the exploitation step based on the predefined <em class="italic">perturb_fact</em> and <em class="italic">resample_prob</em>.</li><li>Perform the <em class="italic">evaluation</em> step on the new set of parameters and hyperparameters based on <em class="italic">f</em> on the validation set.</li><li><em class="italic">Update</em> the model, <em class="italic">M</em><span class="subscript">i</span>, with the new set of parameters and hyperparameters.</li></ul></li><li>Repeat Steps I and II until the end of the training loop. Usually, it is defined by the number of epochs.</li></ol></li>
				<li>Return the <a id="_idIndexMarker277"/>model with the best evaluation score in the population, <em class="italic">P</em>.</li>
				<li>Evaluate the final model on the test set.</li>
			</ol>
			<p>It is worth noting that, in practice, such as in the implementation of the <strong class="bold">NNI</strong> package (see <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>), the readiness criterion defined in <em class="italic">Step 4</em> is an epoch. In other words, the second step within <em class="italic">Step 8</em> will only be run after each training epoch, not in the middle of an epoch. It is also worth noting that the checkpoint directory defined in <em class="italic">Step 5</em> is needed because, in PBT, we need to copy weights from another model in the population, while that’s not the case for the other hyperparameter tuning methods we’ve learned about so far.</p>
			<p>While the original PBT algorithm states that we can run <em class="italic">Step 8</em> asynchronously in parallel, this is not the <a id="_idIndexMarker278"/>case in the implementation of the <strong class="bold">NNI</strong> package, which will be used in this book to implement PBT. In the NNI package implementation, the process is run <em class="italic">synchronously</em>, meaning that we can continue to the next epoch once all of the individuals or models in the population have finished the previous epoch.</p>
			<p>The following <a id="_idIndexMarker279"/>table lists the pros and cons of the <a id="_idIndexMarker280"/>PBT method:</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B18753_05_016.jpg" alt="Figure 5.16 – Pros and cons of PBT&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Pros and cons of PBT</p>
			<p>In this section, you learned all you need to know about PBT, including what it is, how it works, what makes it different from other heuristic search methods, and its pros and cons.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor053"/>Summary</h1>
			<p>In this chapter, we discussed the third out of four groups of hyperparameter tuning methods, called the heuristic search group. We discussed what the heuristic search method is in general and several variants of heuristic search methods, including SA, the GA method, PSO, and PBT. We saw what makes each of the variants differ from each other, along with the pros and cons of each. At this point, you should be able to explain heuristic search in confidence when someone asks you. You should also be able to debug and set up the most suitable configuration of the chosen method that suits your specific problem definition.</p>
			<p>In the next chapter, we will start discussing multi-fidelity optimization, the last group of hyperparameter tuning methods. The goal of the next chapter is similar to this one’s: to provide a better understanding of the methods that belong to the multi-fidelity optimization group so that you can explain those methods in confidence when someone asks you. By doing this, you will be able to configure each of the methods for your specific problem!</p>
		</div>
	</body></html>