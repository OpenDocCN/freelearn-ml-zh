<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Principal Components Analysis</h1>
            </header>

            <article>
                
<div class="packt_quote">"Some people skate to the puck. I skate to where the puck is going to be."<br/>
                                                                                                         - Wayne Gretzky</div>
<p>This chapter is the second one where we will focus on unsupervised learning techniques. In the previous chapter, we covered cluster analysis, which provides us with the groupings of similar observations. In this chapter, we will see how to reduce the dimensionality and improve the understanding of our data by grouping the correlated variables with <strong>Principal Components Analysis</strong> (<strong>PCA</strong>). Then, we will use the principal components in supervised learning.</p>
<p>In many datasets, particularly in the social sciences, you will see many variables highly correlated with each other. They may additionally suffer from high dimensionality or, as it is better known, the <strong>curse of dimensionality</strong>. This is a problem because the number of samples needed to estimate a function grows exponentially with the number of input features. In such datasets, there may be the case that some variables are redundant as they end up measuring the same constructs, for example, income and poverty or depression and anxiety. The goal then is to use PCA in order to create a smaller set of variables that capture most of the information from the original set of variables, thus simplifying the dataset and often leading to hidden insights. These new variables (principal components) are highly uncorrelated with each other. In addition to supervised learning, it is also very common to use these components to perform data visualization.</p>
<p>From over a decade of either doing or supporting analytics using PCA, it has been my experience that it is widely used but poorly understood, especially among people who don't do the analysis but consume the results. It is intuitive to understand that you are creating a new variable from the other correlated variables. However, the technique itself is shrouded in potentially misunderstood terminology and mathematical concepts that often bewilder the layperson. The intention here is to provide a good foundation on what it is and how to use it by covering the following:</p>
<ul>
<li>Preparing a datset for PCA</li>
<li>Conducting PCA</li>
<li>Selecting our principal components</li>
<li>Building a predictive model using principal components</li>
<li>Making out of sample predictions using the predictive model</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of the principal components</h1>
            </header>

            <article>
                
<p>PCA is the process of finding the principal components. What exactly are these?</p>
<p>We can consider that a component is a normalized linear combination of the features (James, 2012). The first principal component in a dataset is the linear combination that captures the maximum variance in the data. A second component is created by selecting another linear combination that maximizes the variance with the constraint that its direction is perpendicular to the first component. The subsequent components (equal to the number of variables) would follow this same rule.</p>
<p>A couple of things here. This definition describes the <strong>linear combination</strong>, which is one of the key assumptions in PCA. If you ever try and apply PCA to a dataset of variables having a low correlation, you will likely end up with a meaningless analysis. Another key assumption is that the mean and variance for a variable are sufficient statistics. What this tells us is that the data should fit a normal distribution so that the covariance matrix fully describes our dataset, that is, <strong>multivariate normality</strong>. PCA is fairly robust to non-normally distributed data and is even used in conjunction with binary variables, so the results are still interpretable.</p>
<p>Now, what is this direction described here and how is the linear combination determined? The best way to grasp this subject is with a visualization. Let's take a small dataset with two variables and plot it. PCA is sensitive to scale, so the data has been scaled with a mean of zero and standard deviation of one. You can see in the following figure that this data happens to form the shape of an oval with the diamonds representing each observation:</p>
<div class="CDPAlignCenter CDPAlign"><img height="190" width="270" class=" image-border" src="assets/image_09_001.jpg"/></div>
<p>Looking at the plot, the data has the most variance along the <em>x</em> axis, so we can draw a dashed horizontal line to represent our <strong>first principal component</strong> as shown in the following image. This component is the linear combination of our two variables or <em>PC1 = α<sub>11</sub>X<sub>1</sub> + α<sub>12</sub>X<sub>2</sub></em>, where the coefficient weights are the variable loadings on the principal component. They form the basis of the direction along which the data varies the most. This equation is constrained by <em>1</em> in order to prevent the selection of arbitrarily high values. Another way to look at this is that the dashed line minimizes the distance between itself and the data points. This distance is shown for a couple of points as arrows, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="194" width="276" class=" image-border" src="assets/image_09_002.jpg"/></div>
<p>The <strong>second principal component</strong> is then calculated in the same way, but it is uncorrelated with the first, that is, its direction is at a right angle or orthogonal to the first principal component. The following plot shows the second principal component added as a dotted line:</p>
<div class="CDPAlignCenter CDPAlign"><img height="189" width="268" class=" image-border" src="assets/image_09_003.jpg"/></div>
<p>With the principal component loadings calculated for each variable, the algorithm will then provide us with the principal component scores. The scores are calculated for each principal component for each observation. For <strong>PC1</strong> and the first observation, this would equate to the formula: <em>Z<sub>11</sub> = α<sub>11</sub> * (X<sub>11</sub> - average of X<sub>1</sub>) + α<sub>12</sub> * (X<sub>12</sub> - average of X<sub>2</sub>)</em>. For <strong>PC2</strong> and the first observation, the equation would be <em>Z<sub>12</sub> = α<sub>21</sub> * (X<sub>11</sub> - average of X<sub>2</sub>) + α<sub>22</sub> * (X<sub>12</sub> - average of X<sub>2</sub>)</em>. These principal component scores are now the new feature space to be used in whatever analysis you will undertake.</p>
<p>Recall that the algorithm will create as many principal components as there are variables, accounting for 100 percent of the possible variance. So, how do we narrow down the components to achieve the original objective in the first place? There are some heuristics that one can use, and in the upcoming modeling process, we will look at the specifics; but a common method to select a principal component is if its <strong>eigenvalue</strong> is greater than one. While the algebra behind the estimation of eigenvalues and <strong>eigenvectors</strong> is outside the scope of this book, it is important to discuss what they are and how they are used in PCA.</p>
<div class="packt_infobox">The optimized linear weights are determined using linear algebra in order to create what is referred to as an eigenvector. They are optimal because no other possible combination of weights could explain variation better than they do. The eigenvalue for a principal component then is the total amount of variation that it explains in the entire dataset.</div>
<p>Recall that the equation for the first principal component is <em>PC1 = α<sub>11</sub>X<sub>1</sub> + α<sub>12</sub>X<sub>2</sub></em>.</p>
<p>As the first principal component accounts for the largest amount of variation, it will have the largest eigenvalue. The second component will have the second highest eigenvalue and so forth. So, an eigenvalue greater than one indicates that the principal component accounts for more variance than any of the original variables does by itself. If you standardize the sum of all the eigenvalues to one, you will have the percentage of the total variance that each component explains. This will also aid you in determining a proper cut-off point.</p>
<p>The eigenvalue criterion is certainly not a hard-and-fast rule and must be balanced with your knowledge of the data and business problem at hand. Once you have selected the number of principal components, you can rotate them in order to simplify their interpretation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Rotation</h1>
            </header>

            <article>
                
<p>Should you rotate or not? As stated previously, rotation helps in the interpretation of the principal components by modifying the loadings of each variable. The overall variation explained by the rotated number of components will not change, but the contributions to the total variance explained by each component will change. What you will find by rotation is that the loading values will either move farther or closer to zero, theoretically aiding in identifying those variables that are important to each principal component. This is an attempt to associate a variable to only one principal component. Remember that this is unsupervised learning, so you are trying to understand your data, not test some hypothesis. In short, rotation aids you in this endeavor.</p>
<p>The most common form of principal component rotation is known as <strong>varimax</strong>. There are other forms such as <strong>quartimax</strong> and <strong>equimax</strong>, but we will focus on varimax rotation. In my experience, I've never seen the other methods provide better solutions. Trial and error on your part may be the best way to decide the issue.</p>
<div class="packt_infobox">With varimax, we are maximizing the sum of the variances of the squared loadings. The varimax procedure rotates the axis of the feature space and their coordinates without changing the locations of the data points.</div>
<p>Perhaps, the best way to demonstrate this is via another simple illustration. Let's assume that we have a dataset of variables <strong><span class="packt_screen">A</span></strong> through <strong><span class="packt_screen">G</span></strong> and we have two principal components. Plotting this data, we will end up with the following illustration:</p>
<div class="CDPAlignCenter CDPAlign"><img height="240" width="258" class=" image-border" src="assets/image_09_004.jpg"/></div>
<p>For the sake of argument, let's say that variable A's loadings are -0.4 on <strong>PC1</strong> and 0.1 on <strong>PC2.</strong> Now, let's say that variable D's loadings are 0.4 on <span class="packt_screen">PC1</span> and -0.3 on <strong>PC2</strong>. For point E, the loadings are -0.05 and -0.7, respectively. Note that the loadings will follow the direction of the principal component. After running a varimax procedure, the rotated components will look as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="225" width="222" class=" image-border" src="assets/image_09_005.jpg"/></div>
<p>The following are the new loadings on <strong><span class="packt_screen">PC1</span></strong> and <strong><span class="packt_screen">PC2</span></strong> after rotation:</p>
<ul>
<li>Variable <strong><span class="packt_screen">A</span></strong>: -0.5 and 0.02</li>
<li>Variable <strong><span class="packt_screen">D</span></strong>: 0.5 and -0.3</li>
<li>Variable <strong><span class="packt_screen">E</span></strong>: 0.15 and -0.75</li>
</ul>
<p>The loadings have changed but the data points have not. With this simple illustration, we can't say that we have simplified the interpretation, but this should help you understand what is happening during the rotation of the principal components.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>For this example, we will delve into the world of sports; in particular, the <strong>National Hockey League</strong> (<strong>NHL</strong>). Much work has been done on baseball (think of the book and movie, <em>Moneyball</em>) and football; both are American and games that people around the world play with their feet. For my money, there is no better spectator sport than hockey. Perhaps that is an artifact of growing up on the frozen prairie of North Dakota. Nonetheless, we can consider this analysis as our effort to start a MoneyPuck movement.</p>
<p>In this analysis, we will look at the statistics for 30 NHL teams in a data set I've compiled from <a href="http://www.nhl.com">www.nhl.com</a> and <a href="http://www.puckalytics.com">www.puckalytics.com</a>. The goal is to build a model that predicts the total points for a team from an input feature space developed using PCA in order to provide us with some insight on what it takes to be a top professional team. We will learn a model from the 2015-16 season, which saw the Pittsburgh Penguins crowned as champions, and then test its performance on the current season's results as of February 15, 2017. The files are <kbd>nhlTrain.csv</kbd> and <kbd>nhlTest.csv</kbd> on <a href="https://github.com/datameister66/data/">https://github.com/datameister66/data/</a>.</p>
<p>NHL standings are based on a points system, so our outcome will be team points per game. It is important to understand how the NHL awards points to the teams. Unlike football or baseball where only wins and losses count, professional hockey uses the following point system for each game:</p>
<ul>
<li>The winner gets two points whether that is in regulation, overtime, or as a result of the post-overtime shootout</li>
<li>A regulation loser receives no points</li>
<li>An overtime or shootout loser receives one point; the so-called <strong>loser point</strong></li>
</ul>
<p>The NHL started this point system in 2005 and it is not without controversy, but it hasn't detracted from the game's elegant and graceful violence.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>To begin with, we will load the necessary packages in order to download the data and conduct the analysis. Please ensure that you have these packages installed prior to loading:</p>
<pre>
<strong>    &gt; library(ggplot2) #support scatterplot<br/><br/>    &gt; library(psych) #PCA package<br/></strong>
</pre>
<p>Let's also assume you've put the two <kbd>.csv</kbd> files into your working directory, so read the training data using the <kbd>read.csv()</kbd> function:</p>
<pre>
<strong>    &gt; train &lt;- read.csv("NHLtrain.csv")<br/></strong>
</pre>
<p>Examine the data using the structure function, <kbd>str()</kbd>. For brevity, I've included only the first few lines of the output of the command:</p>
<pre>
    <strong>&gt; str(train)<br/>    'data.frame': 30 obs. of 15 variables:<br/>    $ Team : Factor w/ 30 levels "Anaheim","Arizona",..: 1 2 3 4 5 6 7 <br/>      8 9 10 ...<br/>    $ ppg : num 1.26 0.95 1.13 0.99 0.94 1.05 1.26 1 0.93 1.33 ...<br/>    $ Goals_For : num 2.62 2.54 2.88 2.43 2.79 2.39 2.85 2.59 2.6 3.23 <br/>      ...<br/>    $ Goals_Against: num 2.29 2.98 2.78 2.62 3.13 2.7 2.52 2.93 3.02 <br/>      2.78 ...</strong>
</pre>
<p>The next thing that we will need to do is look at the variable names. </p>
<pre>
    <strong>&gt; names(train)</strong><br/>    <strong>[1] "Team" "ppg" "Goals_For" "Goals_Against" "Shots_For" </strong><br/><strong>    [6] "Shots_Against" "PP_perc" "PK_perc" "CF60_pp" "CA60_sh" </strong><br/><strong>    [11] "OZFOperc_pp" "Give" "Take" "hits" "blks"</strong>
</pre>
<p>Let's go over what they mean:</p>
<ul>
<li><kbd>Team</kbd>: This is the team's city</li>
<li><kbd>ppg</kbd>: The average points per game per the point calculation discussed earlier</li>
<li><kbd>Goals_For</kbd>: The average goals the team scores per game</li>
<li><kbd>Goals_Against</kbd>: The goals allowed per game</li>
<li><kbd>Shots_For</kbd>: Shots on goal per game</li>
<li><kbd>Shots_Against</kbd>: Opponent shots on goal per game</li>
<li><kbd>PP_perc</kbd>: Percent of power play opportunities the team scores a goal</li>
</ul>
<ul>
<li><kbd>PK_perc</kbd>: Percent of time the team does not allow a goal when their opponent is on the power play</li>
<li><kbd>CF60_pp</kbd>: The team's Corsi Score per 60 minutes of power play time; Corsi Score is the sum of shots for (Shots_For), shot attempts that miss the net and shots blocked by the opponent</li>
<li><kbd>CA60_sh</kbd>: The opponents Corsi Score per 60 minutes of opponent power play time i.e. the team is shorthanded</li>
<li><kbd>OZFOperc_pp</kbd>: The p<span>ercentage of face offs that took place in the offensive zone while the team was on the power play</span></li>
<li><kbd>Give</kbd>: The average number per game that the team gives away the puck</li>
<li><kbd>Take</kbd>: The average number per game that the team gains control of the puck </li>
<li><kbd>hits</kbd>: The average number of the team's bodychecks per game</li>
<li><kbd>blks</kbd>: The average number per game of the team's blocking an opponent's shot on goal</li>
</ul>
<p>We'll need to have the data standardized with mean 0 and standard deviation of 1. Once we do that we can create and plot the correlations of the input features using the <kbd>cor.plot()</kbd> function available in the psych package:</p>
<pre>
    <strong>&gt; train.scale &lt;- scale(train[, -1:-2])<br/><br/>    &gt; nhl.cor &lt;- cor(train.scale)<br/><br/>    &gt; cor.plot(nhl.cor)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="229" width="395" class="image-border" src="assets/image_plotPCA_01.png"/></div>
<p>A couple of things are of interest. Notice that <kbd>Shots_For</kbd> is correlated with <kbd>Goals_For</kbd> and conversely, <kbd>Shots_Against</kbd> with <kbd>Goals_Against</kbd>. There also is some negative correlation with <kbd>PP_perc</kbd> and <kbd>PK_perc</kbd> with <kbd>Goals_Against</kbd>.</p>
<p>As such, this should be an adequate dataset to extract several principal components. </p>
<p>Please note that these are features/variables that I've selected based on my interest. There are a bunch of different statistics you can gather on your own and see if you can improve the predictive power.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>For the modeling process, we will follow the following steps:</p>
<ol>
<li>Extract the components and determine the number to retain.</li>
<li>Rotate the retained components.</li>
<li>Interpret the rotated solution.</li>
<li>Create the factor scores.</li>
<li>Use the scores as input variables for regression analysis and evaluate the performance on the test data.</li>
</ol>
<p>There are many different ways and packages to conduct PCA in R, including what seems to be the most commonly used <kbd>prcomp()</kbd> and <kbd>princomp()</kbd> functions in base R. However, for my money, it seems that the <kbd>psych</kbd> package is the most flexible with the best options.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Component extraction</h1>
            </header>

            <article>
                
<p>To extract the components with the <kbd>psych</kbd> package, you will use the <kbd>principal()</kbd> function. The syntax will include the data and whether or not we want to rotate the components at this time:</p>
<pre>
    <strong>&gt; pca &lt;- principal(train.scale, rotate="none")</strong>
</pre>
<p>You can examine the components by calling the <kbd>pca</kbd> object that we created. However, my primary intent is to determine what should be the number of components to retain. For that, a scree plot will suffice. <span>A scree plot can aid you in assessing the components that explain the most variance in the data. It shows the</span> <kbd>Component</kbd> <span>number on the</span> <em>x</em>-<span>axis and their associated</span> <kbd>Eigenvalues</kbd> <span>on the</span> <em>y</em><span>-axis:</span></p>
<pre>
    <strong>&gt; plot(pca$values, type="b", ylab="Eigenvalues", xlab="Component")</strong>
</pre>
<p><span>The following is the output of the preceding command:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="300" width="517" class="image-border" src="assets/image_pca_02.png"/></div>
<p>What you are looking for is a point in the scree plot where the rate of change decreases. This will be what is commonly called an elbow or bend in the plot. That elbow point in the plot captures<span> the fact that additional variance explained by a component does not differ greatly from one component to the next. In other words, it is the break point where the plot flattens out. In this plot, five components look pretty compelling.</span></p>
<p><span>Another rule I've learned over the years is that you should capture about 70% of the total variance, which means that the cumulative variance explained by each of the selected components accounts for 70 percent of the variance explained by all the components.</span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Orthogonal rotation and interpretation</h1>
            </header>

            <article>
                
<p>As we discussed previously, the point behind rotation is to maximize the loadings of the variables on a specific component, which helps in simplifying the interpretation by reducing/eliminating the correlation among these components. The method to conduct orthogonal rotation is known as <kbd>"varimax"</kbd>. There are other non-orthogonal rotation methods that allow correlation across factors/components. The choice of the rotation methodology that you will use in your profession should be based on the pertinent literature, which exceeds the scope of this chapter. Feel free to experiment with this dataset. I think that when in doubt, the starting point for any PCA should be orthogonal rotation.</p>
<p>For this process, we will simply turn back to the <kbd>principal()</kbd> function, slightly changing the syntax to account for <span><span>5</span></span> components and orthogonal rotation, as follows:</p>
<pre>
<strong>    &gt; pca.rotate &lt;- principal(train.scale, nfactors = 5, rotate = <br/>      "varimax")</strong>    <br/><br/><strong>    &gt; pca.rotate<br/>    Principal Components Analysis<br/>    Call: principal(r = train.scale, nfactors = 5, rotate = "varimax")<br/>    Standardized loadings (pattern matrix) based upon correlation <br/>      matrix<br/>                    RC1   RC2    RC5   RC3    RC4    h2    u2  com<br/>    Goals_For     -0.21  0.82   0.21  0.05  -0.11  0.78  0.22  1.3<br/>    Goals_Against  0.88 -0.02  -0.05  0.21   0.00  0.82  0.18  1.1<br/>    Shots_For     -0.22  0.43   0.76 -0.02  -0.10  0.81  0.19  1.8<br/>    Shots_Against  0.73 -0.02  -0.20 -0.29   0.20  0.70  0.30  1.7<br/>    PP_perc       -0.73  0.46  -0.04 -0.15   0.04  0.77  0.23  1.8<br/>    PK_perc       -0.73 -0.21   0.22 -0.03   0.10  0.64  0.36  1.4<br/>    CF60_pp       -0.20  0.12   0.71  0.24   0.29  0.69  0.31  1.9<br/>    CA60_sh        0.35  0.66  -0.25 -0.48  -0.03  0.85  0.15  2.8<br/>    OZFOperc_pp   -0.02 -0.18   0.70 -0.01   0.11  0.53  0.47  1.2<br/>    Give          -0.02  0.58   0.17  0.52   0.10  0.65  0.35  2.2<br/>    Take           0.16  0.02   0.01  0.90  -0.05  0.83  0.17  1.1<br/>    hits          -0.02 -0.01   0.27 -0.06   0.87  0.83  0.17  1.2<br/>    blks           0.19  0.63  -0.18  0.14   0.47  0.70  0.30  2.4<br/><br/>                       RC1  RC2  RC5  RC3  RC4<br/>SS loadings           2.69 2.33 1.89 1.55 1.16<br/>Proportion Var        0.21 0.18 0.15 0.12 0.09<br/>Cumulative Var        0.21 0.39 0.53 0.65 0.74<br/>Proportion Explained  0.28 0.24 0.20 0.16 0.12<br/>Cumulative Proportion 0.28 0.52 0.72 0.88 1.00</strong>
</pre>
<p>There are two important things to digest here in the output. The first is the variable loadings for each of the five components that are labeled <kbd>RC1</kbd> through <kbd>RC5</kbd>. We see with component one that <kbd>Goals_Against</kbd> and <kbd>Shots_Against</kbd> have high positive loadings, while <kbd>PP_perc</kbd> and <kbd>PK_perc</kbd> have high negative loadings. The high loading for component two is <kbd>Goals_For</kbd>. Component five has high loadings with <kbd>Shots_For</kbd>, <kbd>ff</kbd>, and <kbd>OZFOperc_pp</kbd>. Component three seems to be only about the variables take while component four is about hits. Next, we will move on to the second part for examination: the table starting with the sum of square, <kbd>SS loadings</kbd>. Here, the numbers are the eigenvalues for each component. When they are normalized, you will end up with the <kbd>Proportion Explained</kbd> row, which as you may have guessed, stands for the proportion of the variance explained by each component. You can see that component one explains 28 percent of all the variance explained by the five rotated components. Remember above I mentioned the heuristic rule that your selected components should account for a minimum of about 70 of the total variation. Well, if you look at the <kbd>Cumulative Var</kbd> row, you see that these five rotated components account for 74% of the total and we can feel confident we have the right number to go forward with our modeling.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating factor scores from the components</h1>
            </header>

            <article>
                
<p>We will now need to capture the rotated component loadings as the factor scores for each individual team. These scores indicate how each observation (in our case, the NHL team) relates to a rotated component. Let's do this and capture the scores in a data frame as we will need to use it for our regression analysis:</p>
<pre>
<strong>   &gt; pca.scores &lt;- data.frame(pca.rotate$scores)</strong><br/><br/><strong>   &gt; head(pca.scores)</strong><br/><strong>             RC1          RC2        RC5         RC3        RC4</strong><br/><strong>   1 -2.21526408  0.002821488  0.3161588  -0.1572320  1.5278033</strong><br/><strong>   2  0.88147630 -0.569239044 -1.2361419  -0.2703150 -0.0113224</strong><br/><strong>   3  0.10321189  0.481754024  1.8135052  -0.1606672  0.7346531</strong><br/><strong>   4 -0.06630166 -0.630676083 -0.2121434  -1.3086231  0.1541255</strong><br/><strong>   5  1.49662977  1.156905747 -0.3222194   0.9647145 -0.6564827</strong><br/><strong>   6 -0.48902169 -2.119952370  1.0456190   2.7375097 -1.3735777</strong>
</pre>
<p>We now have the scores for each component for each team. These are simply the variables for each observation multiplied by the loadings on each component and then summed. We now can bring in the response (<kbd>ppg</kbd>) as a column in the data.</p>
<pre>
<strong>   &gt; pca.scores$ppg &lt;- train$ppg<br/></strong>
</pre>
<p>With this done, we will now move on to the predictive model.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regression analysis</h1>
            </header>

            <article>
                
<p>To do this part of the process, we will repeat the steps and code from <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em>. If you haven't done so, please look at <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em> for some insight on how to interpret the following output.</p>
<p>We will use the following <kbd>lm()</kbd> function to create our linear model with all the factors as inputs and then summarize the results:</p>
<pre>
<strong>    &gt; nhl.lm &lt;- lm(ppg ~ ., data = pca.scores)</strong><br/><strong><br/>    &gt; summary(nhl.lm)</strong><br/><br/><strong>    Call:</strong><br/><strong>    lm(formula = ppg ~ ., data = pca.scores)</strong><br/><br/><strong>    Residuals:</strong><br/><strong>          Min        1Q   Median       3Q      Max </strong><br/><strong>    -0.163274 -0.048189 0.003718 0.038723 0.165905 </strong><br/><br/><strong>    Coefficients:</strong><br/><strong>          Estimate       Std. Error t value Pr(&gt;|t|) </strong><br/><strong>    (Intercept) 1.111333   0.015752  70.551  &lt; 2e-16  ***</strong><br/><strong>    RC1        -0.112201   0.016022  -7.003  3.06e-07 ***</strong><br/><strong>    RC2         0.070991   0.016022   4.431  0.000177 ***</strong><br/><strong>    RC5         0.022945   0.016022   1.432  0.164996 </strong><br/><strong>    RC3        -0.017782   0.016022  -1.110  0.278044 </strong><br/><strong>    RC4        -0.005314   0.016022  -0.332  0.743003 </strong><br/><strong>    ---</strong><br/><strong>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong><br/><br/><strong>Residual standard error: 0.08628 on 24 degrees of freedom</strong><br/><strong>Multiple R-squared: 0.7502, Adjusted R-squared: 0.6981 </strong><br/><strong>F-statistic: 14.41 on 5 and 24 DF, p-value: 1.446e-06<br/></strong>
</pre>
<p>The good news is that our overall model is highly significant statistically, with <kbd>p-value</kbd> of <kbd>1.446e-06</kbd> and <kbd>Adjusted R-squared</kbd> is almost 70 percent. The bad news is that three components are not significant. We could simply choose to keep them in our model, but let's see what happens if we exclude them, just keeping <strong>RC1</strong> and <strong>RC2</strong>:</p>
<pre>
<strong>    &gt; nhl.lm2 &lt;- lm(ppg ~ RC1 + RC2, data = pca.scores)</strong><br/><strong><br/>    &gt; summary(nhl.lm2)</strong><br/><br/><strong>    Call:</strong><br/><strong>    lm(formula = ppg ~ RC1 + RC2, data = pca.scores)</strong><br/><br/><strong>    Residuals:</strong><br/><strong>         Min       1Q  Median      3Q     Max </strong><br/><strong>    -0.18914 -0.04430 0.01438 0.05645 0.16469 </strong><br/><br/><strong>    Coefficients:</strong><br/><strong>               Estimate Std. Error t value  Pr(&gt;|t|) </strong><br/><strong>    (Intercept) 1.11133    0.01587  70.043   &lt; 2e-16  ***</strong><br/><strong>    RC1        -0.11220    0.01614  -6.953   1.8e-07  ***</strong><br/><strong>    RC2         0.07099    0.01614   4.399   0.000153 ***</strong><br/><strong>    ---</strong><br/><strong>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong><br/><br/><strong>Residual standard error: 0.0869 on 27 degrees of freedom</strong><br/><strong>Multiple R-squared: 0.7149, Adjusted R-squared: 0.6937 </strong><br/><strong>F-statistic: 33.85 on 2 and 27 DF, p-value: 4.397e-08<br/></strong>
</pre>
<p>This model still achieves roughly the same <kbd>Adjusted R-squared</kbd> value (93.07 percent) with statistically significant factor coefficients. I will spare you the details of running the diagnostic tests. Instead, let's look at some plots in order to examine our analysis better. We can do a scatterplot of the predicted and actual values with the base R graphics, as follows:</p>
<pre>
<strong>    &gt; plot(nhl.lm2$fitted.values, train$ppg,</strong><br/><strong>      main="Predicted versus Actual",</strong><br/><strong>      xlab="Predicted",ylab="Actual")<br/></strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="247" width="421" class="image-border" src="assets/image_pca_03.png"/></div>
<p>This confirms that our model does a good job of using two components to predict the team's success and also highlights the strong linear relationship between the principal components and team points per game. Let's kick it up a notch by doing a scatterplot using the <kbd>ggplot2</kbd> package and include the team names in it. The only problem is that it is a very powerful function with many options. There are numerous online resources to help you navigate the <kbd>ggplot()</kbd> maze, but this code should help you on your way. Let's first create our baseline plot and assign it to an object called <kbd><span><span>p</span></span></kbd> then add various plot functionality.</p>
<pre>
<strong>   &gt; train$pred &lt;- round(nhl.lm2$fitted.values, digits = 2)</strong><br/><br/><strong>   &gt; p &lt;- ggplot(train, aes(x = pred,</strong><br/><strong>     y = ppg,</strong><br/><strong>     label = Team))</strong><br/><br/><strong>   &gt; p + geom_point() +</strong><br/><strong>     geom_text(size = 3.5, hjust = 0.1, vjust = -0.5, angle = 0) +</strong><br/><strong>     xlim(0.8, 1.4) + ylim(0.8, 1.5) +</strong><br/><strong>     stat_smooth(method = "lm", se = FALSE)<br/></strong>
</pre>
<p><span>The following is the output of the preceding commands:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="403" width="686" class="image-border" src="assets/image_pca_04.png"/></div>
<p>The syntax to create <kbd>p</kbd> is very simple. We just specified the data frame and put in <kbd>aes()</kbd> what we want our <kbd>x</kbd> and <kbd>y</kbd> to be along with the variable that we want to use as labels. We then just add layers of neat stuff such as data points. Add whatever you want to the plot by including <kbd>+</kbd> in the syntax, as follows:</p>
<pre>
    <strong>&gt; p + geom_point() +</strong>
</pre>
<p>We specified how we wanted our <kbd>team</kbd> labels to appear. It takes quite a bit of trial and error to get the font size and position in order:</p>
<pre>
    <strong>geom_text() +</strong>
</pre>
<p>Now, specify the <em>x</em> and <em>y</em> axis limits, otherwise the plot will cut out any observations that fall outside them, as follows:</p>
<pre>
    <strong>xlim() + ylim() +</strong>
</pre>
<p>Finally, we added a best fit line with no standard error shading:</p>
<pre>
    <strong>stat_smooth(method = "lm", se = FALSE)</strong>
</pre>
<p>I guess one way to think about this plot is that the teams below the line underachieved, while those above it overachieved. </p>
<p>Another bit of analysis will be to plot the teams in relationship to their factor scores, what is referred to as a <strong>biplot</strong>. Once again, <kbd>ggplot()</kbd> facilitates this analysis. Using the preceding code as our guide, let's update it and see what the result is:</p>
<pre>
<strong>    &gt; pca.scores$Team &lt;- train$Team</strong><br/><br/><strong>    &gt; p2 &lt;- ggplot(pca.scores, aes(x = RC1, y = RC2, label = Team))</strong><br/><br/><strong>    &gt; p2 + geom_point() +</strong><br/><strong>      geom_text(size = 2.75, hjust = .2, vjust = -0.75, angle = 0) +</strong><br/><strong>      xlim(-2.5, 2.5) + ylim(-3.0, 2.5)<br/></strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_pca_05.png"/></div>
<p>As you can see, the <em>x</em> axis are the team scores on <span class="packt_screen">RC1</span> and the <em>y</em> axis are the scores on <span class="packt_screen">RC2</span>. Look at the <span class="packt_screen">Anaheim</span> ducks with the lowest score on <span class="packt_screen">RC1</span> and an average score for <span class="packt_screen">RC2</span>. Now think about the impact of this. With the negative loadings on <span class="packt_screen">RC1</span> for the power play and penalty kill, along with the positive loading of <kbd>Goals_Against</kbd>, it would indicate that the team performed well defensively, and was effective shorthanded. By the way, <span class="packt_screen">Pittsburgh</span> was the eventual winner of the Stanley Cup. Their scores are solid, but nothing noteworthy. Keep in mind that the team had a horrible start to the season and fired the coach they started the season with. It would be interesting to compare how they did on this analysis in the first half of the season versus the latter half.<br/>
You can evaluate the model error as well, like we did previously. Let's look at <strong>Root Means Squared Error</strong> (<strong>RMSE</strong>):</p>
<pre>
<strong>    &gt; sqrt(mean(nhl.lm2$residuals^2))</strong><br/><strong>    [1] 0.08244449</strong>
</pre>
<p>With that done, we need to see how it performs out of sample. We are going to load the test data, predict the team scores on the components, then make our predictions based on the linear model. The <kbd>predict</kbd> function from the psych package will automatically scale the test data:</p>
<pre>
<strong>    &gt; test &lt;- read.csv("NHLtest.csv")</strong><br/><strong><br/>    &gt; test.scores &lt;- data.frame(predict(pca.rotate, test[, c(-1:-2)]))</strong><br/><strong><br/>    &gt; test.scores$pred &lt;- predict(nhl.lm2, test.scores)</strong>
</pre>
<p>I think we should plot the results as we did above, showing team names. Let's get this all in a data frame:</p>
<pre>
<strong>    &gt; test.scores$ppg &lt;- test$ppg</strong><br/><br/><strong>    &gt; test.scores$Team &lt;- test$Team</strong>
</pre>
<p>Then, utilize the power of <kbd>ggplot()</kbd>:</p>
<pre>
<strong>    &gt; p &lt;- ggplot(test.scores, aes(x = pred,</strong><br/><strong>                                  y = ppg,</strong><br/><strong>                                  label = Team)) </strong><br/><br/><strong>    &gt; p + geom_point() + </strong><br/><strong>      geom_text(size=3.5, hjust=0.4, vjust = -0.9, angle = 35) + </strong><br/><strong>      xlim(0.75, 1.5) + ylim(0.5, 1.6) +</strong><br/><strong>      stat_smooth(method="lm", se=FALSE)</strong>
</pre>
<p><span>The output of the preceding command is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="218" width="383" class="image-border" src="assets/image_pca_06.png"/></div>
<p>I abbreviated the team names in the test data to make it easier to understand. Our points per game leader is the Washington Capitals and the worst team is the Colorado Avalanche. In fact, when I pulled this data, Colorado had lost five straight games. They did break that losing streak as I watched them beat Carolina in overtime.</p>
<p>Finally, let's check the RMSE.</p>
<pre>
<strong>    &gt; resid &lt;- test.scores$ppg - test.scores$pred</strong><br/><br/><strong>    &gt; sqrt(mean(resid^2))</strong><br/><strong>    [1] 0.1011561</strong>
</pre>
<p>That is not bad with an output of sample error of 0.1 versus in sample of 0.08. I think we can declare this a valid model. However, there are still a ton of team statistics we could add here to improve predictive power and reduce error. I'll keep working on it, and I hope you do as well.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we took a second stab at unsupervised learning techniques by exploring PCA, examining what it is, and applying it in a practical fashion. We explored how it can be used to reduce the dimensionality and improve the understanding of the dataset when confronted with numerous highly correlated variables. Then, we applied it to real data from the National Hockey League, using the resulting principal components in a regression analysis to predict total team points. Additionally, we explored ways to visualize the data and the principal components.</p>
<p>As an unsupervised learning technique, it requires some judgment along with trial and error to arrive at an optimal solution that is acceptable to business partners. Nevertheless, it is a powerful tool to extract latent insights and to support supervised learning.</p>
<p>We will next look at using unsupervised learning to develop market basket analyses and recommendation engines in which PCA can play an important role.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>