<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 3. Customer Analytics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Customer Analytics</h1></div></div></div><p>Customer analytics is a process in which we use the data of customer behavior to derive the most important business decisions using market segmentation and predictive analytics. Market segmentation is the process of dividing the user base into subgroups based on their behavior and other types of shared characteristics. This will help companies in providing customized products for each user segment. The result of this kind of analysis will lead the company to grow their business in an effective manner. Companies also make more profit. There are a lot of advantages. I know this is only a brief discussion about market segmentation, but just bear with me for a while. I will give you all the necessary information in the upcoming sections.</p><p>Companies can use the result generated by market segmentation and predictive models for direct marketing, site selection, customer acquisition, and customer relationship management. In short, with the help of customer analytics, the company can decide the most optimal and effective marketing strategy as well as growth strategy. The company can achieve great results with a limited amount of marking expenditure. Customer analytics include various methods. You can refer to the names of these methods in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_03_01.jpg" alt="Customer Analytics" width="748" height="382"/><div class="caption"><p>Figure 3.1: Variety of methods for customer analytics</p></div></div><p>In this chapter, we won't be covering all the methods given in the previous figure, but we will cover the methods that are most widely used in the industry. We will build a customer segmentation application. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing customer segmentation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li></ul></div></li><li class="listitem" style="list-style-type: disc">Understanding the datasets</li><li class="listitem" style="list-style-type: disc">Building the baseline approach for customer segmentation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the baseline approach</li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix</li><li class="listitem" style="list-style-type: disc">Testing the result of the baseline approach</li><li class="listitem" style="list-style-type: disc">Problems with the baseline approach</li><li class="listitem" style="list-style-type: disc">Optimizing the baseline approach  </li></ul></div></li><li class="listitem" style="list-style-type: disc">Building the revised approach for customer segmentation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the revised approach</li><li class="listitem" style="list-style-type: disc">Testing the revised approach</li><li class="listitem" style="list-style-type: disc">Problems with the revised approach</li><li class="listitem" style="list-style-type: disc">Understanding how to improve the revised approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach for customer segmentation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Testing the best approach</li><li class="listitem" style="list-style-type: disc">Customer segmentation for various domains</li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><p>We will start with customer segmentation.</p><div class="section" title="Introducing customer segmentation"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec36"/>Introducing customer segmentation</h1></div></div></div><p>In this section, we will cover customer segmentation in detail. Initially, I provided just a brief introduction <a id="id283" class="indexterm"/>of customer segmentation so that you could understand the term a bit. Here, we will understand a lot more about customer segmentation, which will help us further when we build the customer segmentation analysis.</p><p>As mentioned earlier, customer segmentation is a process where we divide the consumer base of the company into subgroups. We need to generate the subgroups by using some specific characteristics so that the company sells more products with less marketing expenditure. Before moving forward, we need to understand the basics, for example, what do I mean by customer base? What do I mean by segment? How do we generate the consumer subgroup? What are the characteristics that we consider while we are segmenting the consumers? Let's answers these questions one by one.</p><p>Basically, the consumer <a id="id284" class="indexterm"/>base of any company consists of two types of consumers: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Existing consumers</li><li class="listitem">Potential consumers</li></ol></div><p>Generally, we need to categorize our consumer base into subgroups. These subgroups are called segments. We need to create the groups in such a way that each subgroup of customers has some shared characteristics. In order to explain how to generate the subgroup, let me give you an example.</p><p>Suppose a <a id="id285" class="indexterm"/>company is selling baby products. Then, it needs to come up with a consumer segment (consumer subgroup) that includes the consumers who want to buy the baby products. We can build the first segment (subgroup) with the help of a simple criterion. We will include consumers who have one baby in their family and bought a baby product in the last month. Now, the company launches a baby product that is too costly or premium. In that case, we can further divide the first subgroup into monthly income and socio-economic status. Based on these new criteria, we can generate the second subgroup of consumers. The company will target the consumers of the second subgroup for the costly and premium products, and for general products, the company will target consumers who are part of the first subgroup.</p><p>When we have different segments, we can design a customized marketing strategy as well as customized products that suit the customer of the particular segment. This segment-wise marketing will help the company sell more products with lower marketing expenses. Thus, the company will make more profit. This is the main reason why companies use customer segmentation analysis nowadays. Customer segmentation is used among other domain such as the retail domain, finance domain, and in customer relationship management (CRM)-based products. I have provided a list of the basic features that can be considered during the segmentation. You can refer to them in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_02.jpg" alt="Introducing customer segmentation" width="1000" height="272"/><div class="caption"><p>Figure 3.2: List of basic features used in customer segmentation</p></div></div><p>You may wonder how companies are making marketing strategies based on the customer segmentation analysis. The answer is companies are using the STP approach to make the <a id="id286" class="indexterm"/>marketing strategy firm. What is the STP approach? First of all, STP stands for Segmentation-Targeting-Positioning. In this approach, there are three stages. The points that we handle in each stage are explained as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Segmentation</strong></span>: In this stage, we create segments of our customer base using their profile characteristics as well as consider features provided in the preceding figure. Once the segmentation is firm, we move on to the next stage.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Targeting</strong></span>: In this stage, marketing teams evaluate segments and try to understand which kind of product is suited to which particular segment(s). The team performs this exercise for each segment, and finally, the team designs customized products that will attract the customers of one or many segments. They will also select which product should be offered to which segment.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Positioning</strong></span>: This is the last stage of the STP process. In this stage, companies study the market opportunity and what their product is offering to the customer. The marketing team should come up with a unique selling proposition. Here, the team also tries to understand how a particular segment perceives the products, brand, or service. This is a way for companies to determine how to best position their offering. The marketing and product teams of companies create a value proposition that clearly explains how their offering is better than any other competitors. Lastly, the companies start their campaign representing this value proposition in such a way that the consumer base will be happy about what they are getting.</li></ul></div><p>I have summarized <a id="id287" class="indexterm"/>all the preceding points in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_03_03.jpg" alt="Introducing customer segmentation" width="772" height="691"/><div class="caption"><p>Figure 3.3: Summarization of the STP approach</p></div></div><p>We have covered most of the basic parts of customer segmentation. Now it's time to move on to <a id="id288" class="indexterm"/>the problem statement.</p><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec47"/>Introducing the problem statement</h2></div></div></div><p>As you know, customer segmentation helps companies retain existing customers as well as acquire <a id="id289" class="indexterm"/>new potential customers. Based on the segmentation, companies can create customized products for a particular customer segment, but so far, we don't know how to generate the segments. This is the point that we will focus on in this chapter. You need to learn how to create customer segmentation. There are many domains for which we can build customer segmentation, such as e-commerce, travel, finance, telecom, and so on.  Here, we will focus only on the e-commerce domain.</p><p>Here is a detailed explanation of the problem statement, input, and output for the e-commerce customer segmentation application that we will be building:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Problem statement</strong></span>: The goal of our customer segmentation application is to come up with a solution for the given questions:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Can we categorize the customers in a particular segment based on their buying patterns? </li><li class="listitem" style="list-style-type: disc">Can we predict which kind of items they will buy in future based on their segmentation?</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Input</strong></span>: We will be using e-commerce data that contains the list of purchases in 1 year for 4,000 customers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Output</strong></span>: The first goal is that we need to categorize our consumer base into appropriate customer segments. The second goal is we need to predict the purchases for the current year and the next year based on the customers' first purchase.</li></ul></div><p>You may wonder how we can achieve a prediction about the upcoming purchases using segmentation. Well, let me tell you how segmentation helps us! So, we don't know the purchase pattern of the new customer, but we know the customer profile. We also know which product the customer has bought. So, we can put the customer into one of the segments where all other customers have purchased similar items and share similar kinds of profile.</p><p>Let me give <a id="id290" class="indexterm"/>you an example. Say, a person has bought a Harry Potter book and that person lives in the UK. The age group of the customer is from 13-22. If we have already generated a customer segment that satisfies these characteristics, then we will put this new customer in that particular subgroup. We will derive the list of items that the customer may buy in future. We will also offer similar services that other customers in the subgroup have.</p><p>The approach that we will be using in order to develop customer segmentation for the e-commerce domain can also be used in other domains, but data points (features) will differ for each domain. Later on in the chapter, we will discuss what kind of data points you may consider for other domains, such as travelling, finance, and so on. I will provide the list of data points for other domains that will help you build the customer segmentation application from scratch.</p><p>Now it is time to understand the dataset for building customer segmentation for the e-commerce domain.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec37"/>Understanding the datasets</h1></div></div></div><p>Finding out <a id="id291" class="indexterm"/>an appropriate dataset is a challenging task in data science. Sometimes, you find a dataset but it is not in the appropriate format. Our problem statement will decide what type of dataset and data format we need. These kinds of activities are a part of data wrangling.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>Data wrangling is defined as the process of transforming and mapping data from one data form into another. With transformation and mapping, our intention should be to create an appropriate and valuable dataset that can be useful in order to develop analytics products. Data wrangling is also referred to as data munging and is a crucial part of any data science application.</p></div></div><p>Generally, e-commerce datasets are proprietary datasets, and it's rare that you get transactions <a id="id292" class="indexterm"/>of real users. Fortunately, <span class="emphasis"><em>The UCI Machine Learning Repository</em></span> hosts a dataset named <span class="emphasis"><em>Online Retail</em></span>. This dataset contains actual transactions from UK retailers.</p><div class="section" title="Description of the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec48"/>Description of the dataset</h2></div></div></div><p>This Online <a id="id293" class="indexterm"/>Retail dataset contains the actual transactions between December 1, 2010  and December 9, 2011.  All the transactions are taken from the registered non-store online retail platform. These online retail platforms are mostly based in the  UK. The online retail platforms are selling unique all-occasion gifts. Many consumers of these online retail platforms are wholesalers. There are 532610 records in this dataset.</p></div><div class="section" title="Downloading the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec49"/>Downloading the dataset</h2></div></div></div><p>You can <a id="id294" class="indexterm"/>download this dataset by using either of the following links:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/online+retail">http://archive.ics.uci.edu/ml/datasets/online+retail</a></li><li class="listitem"><a class="ulink" href="https://www.kaggle.com/fabiendaniel/customer-segmentation/data">https://www.kaggle.com/fabiendaniel/customer-segmentation/data</a></li></ol></div><p>
</p></div><div class="section" title="Attributes of the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec50"/>Attributes of the dataset</h2></div></div></div><p>These are <a id="id295" class="indexterm"/>the attributes in this dataset. We will take a look at a short description for each of them:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">InvoiceNo: This data attribute indicates the invoice numbers. It is a six-digit integer number. The records are uniquely assigned for each transaction. If the invoice number starts with the letter 'c', then it indicates a cancellation.</li><li class="listitem">StockCode: This data attribute indicates the product (item) code. It is a five-digit integer number. All the item codes are uniquely assigned to each distinct product.</li><li class="listitem">Description: This data attribute contains the description about the item.</li><li class="listitem">Quantity: This data attribute contains the quantities for each product per transaction. The data is in a numeric format.</li><li class="listitem">InvoiceDate: The data attribute contains the invoice date and time. It indicates the day and time when each transaction was generated.</li><li class="listitem">UnitPrice: The price indicates the product price per unit in sterling.</li><li class="listitem">CustomerID: This column has the customer identification number. It is a five-digit integer number uniquely assigned to each customer.</li><li class="listitem">Country: This column contains the geographic information about the customer. It records the country name for the customers.</li></ol></div><p>You can <a id="id296" class="indexterm"/>refer to the sample of the dataset given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_04.jpg" alt="Attributes of the dataset" width="938" height="219"/><div class="caption"><p>Figure 3.4: Sample recodes from the dataset</p></div></div><p>Now we will start building the customer segmentation application.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the baseline approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec38"/>Building the baseline approach</h1></div></div></div><p>In this section, we will start implementing the basic model for the customer segmentation <a id="id297" class="indexterm"/>application. Furthermore, we will improve this baseline approach. While implementing, we will cover the necessary concepts, technical aspects, and significance of performing that particular step. You can find the <a id="id298" class="indexterm"/>code for the customer-segmentation application at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Customer_segmentation">https://github.com/jalajthanaki/Customer_segmentation</a>
</p><p>
The code related to this chapter is given in a single iPython notebook. You can access the notebook using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb">https://github.com/jalajthanaki/Customer_segmentation/blob/master/Cust_segmentation_online_retail.ipynb</a>.</p><p>Refer to the code given on GitHub because it will help you understand things better. Now let's begin the implementation!</p><div class="section" title="Implementing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec51"/>Implementing the baseline approach</h2></div></div></div><p>In order to <a id="id299" class="indexterm"/>implement the customer segmentation model, our implementation will have the following steps: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Data preparation</li><li class="listitem">Exploratory data analysis (EDA) </li><li class="listitem">Generating customer categories</li><li class="listitem">Classifying customers</li></ol></div><p> Let's begin with data preparation!</p><div class="section" title="Data preparation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec23"/>Data preparation</h3></div></div></div><p>This is <a id="id300" class="indexterm"/>a basic step when you try to build any analytics <a id="id301" class="indexterm"/>application. First, we need to be sure that the format of the data is in an appropriate form. If it is not, then we need to prepare our dataset in such a way that we can build our application easily. In this step, we will find out whether we have a good quality dataset or not. We can also find out some basic facts about the dataset.</p><p>Luckily, we don't need to change the format of our e-commerce dataset, but we will be exploring the dataset in such a way that we can find out the quality of the dataset. If format of the dataset is not proper then you need to decide the format of the dataset in such a way <a id="id302" class="indexterm"/>that any kind of analysis can be performed using the dataset. You can convert the data records either in CSV format or in JSON <a id="id303" class="indexterm"/>format or in XML format. In addition, we can derive general facts about the dataset, such as whether our dataset is biased or not, whether the dataset contains any null values, the mapping of the customers with <code class="literal">Customer_ID</code> is proper or not, whether their purchases are properly recorded in dataset or not, and so on.</p><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec28"/>Loading the dataset</h4></div></div></div><p>In order to <a id="id304" class="indexterm"/>load the dataset, we will use the pandas <code class="literal">read_csv</code> API. You can find the code snippet given in the following screenshot: </p><div class="mediaobject"><img src="Images/B08394_03_05.jpg" alt="Loading the dataset" width="878" height="414"/><div class="caption"><p>Figure 3.5: Code snippet for loading the dataset</p></div></div><p>As you can see, the dimensions of the dataset are (541909, 8). This means that there are 541,909 records in the dataset and eight data attributes. We have already covered these eight data attributes.</p><p>Now we <a id="id305" class="indexterm"/>need to perform exploratory data analysis (EDA), which can help us preprocess our dataset.</p></div></div><div class="section" title="Exploratory data analysis (EDA)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec24"/>Exploratory data analysis (EDA) </h3></div></div></div><p>In this <a id="id306" class="indexterm"/>section, we need to check the <a id="id307" class="indexterm"/>statistical properties of the dataset and perform some preprocessing steps: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Removing null data entries</li><li class="listitem">Removing duplicate data entries </li><li class="listitem">EDA for various data attributes</li></ol></div><div class="section" title="Removing null data entries"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec29"/>Removing null data entries</h4></div></div></div><p>First, we need <a id="id308" class="indexterm"/>to check the data type of each of the attributes as well as find out which column has a null value. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_06.jpg" alt="Removing null data entries" width="838" height="401"/><div class="caption"><p>Figure 3.6: Code snippet for exploring the dataset</p></div></div><p>As you can see in the code, we have generated the total number of null values for each data attribute. We have also generated the percentage of null values for each data attribute. We <a id="id309" class="indexterm"/>can observe that for the <code class="literal">CustomerID</code> column, there are ~25% data entries that are null. That means there is no <code class="literal">CustomerID</code> value available for ~25% of the dataset. This indicates that there are many entries that do not belong to any customer. These are abended data entries. We cannot map them to the existing CustomerIDs. As a result, we need to delete them. You can find the code snippet for deleting null data entries from the dataset in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_07.jpg" alt="Removing null data entries" width="851" height="316"/><div class="caption"><p>Figure 3.7: Deleting null data entries</p></div></div></div><div class="section" title="Removing duplicate data entries"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec30"/>Removing duplicate data entries</h4></div></div></div><p>After this step, we will check whether there are any duplicate data entries present in the dataset. In order <a id="id310" class="indexterm"/>to answer this question, we will use the pandas <code class="literal">duplicate()</code> function. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_08.jpg" alt="Removing duplicate data entries" width="606" height="96"/><div class="caption"><p>Figure 3.8: Removing duplicate data entries</p></div></div><p>As you can <a id="id311" class="indexterm"/>see, we found 5,225 duplicate data entries. Therefore, we have removed them.</p><p>Now let's analyze each data attribute in detail.</p></div><div class="section" title="EDA for various data attributes"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec31"/>EDA for various data attributes</h4></div></div></div><p>EDA for <a id="id312" class="indexterm"/>
<a id="id313" class="indexterm"/>each data attribute will help us get more insight into the dataset. Later on, we will use these facts to build an accurate customer segmentation application.</p><p> We will start exploring data attributes in the following order:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Country</li><li class="listitem">Customer and products</li><li class="listitem">Product categories</li><li class="listitem">Defining product categories</li></ol></div><div class="section" title="Country"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec01"/><span class="strong"><strong>Country</strong></span></h5></div></div></div><p>We need <a id="id314" class="indexterm"/>to find out facts such as how many countries there are in our dataset. In order to answer this question, we need to execute the code shown in the following screenshot: </p><div class="mediaobject"><img src="Images/B08394_03_09.jpg" alt="Country" width="541" height="188"/><div class="caption"><p>Figure 3.9: Code snippet for generating the number of counties present in the dataset</p></div></div><p>We also need to find the country from which we receive the maximum number of orders. We can find that out by using the pandas <code class="literal">groupby()</code> and <code class="literal">count()</code> functions. We sort <a id="id315" class="indexterm"/>the number of orders in descending order.  You can refer to the code snippet in the following screenshot: </p><div class="mediaobject"><img src="Images/B08394_03_10.jpg" alt="Country" width="789" height="709"/><div class="caption"><p> Figure 3.10: Code snippet for generating country-wise number of orders</p></div></div><p>As you can see in the preceding snippet, there are a majority of orders from UK-based customers. Now we <a id="id316" class="indexterm"/>need to explore the customer and products variables.</p></div><div class="section" title="Customer and products"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec02"/><span class="strong"><strong>Customer and products</strong></span></h5></div></div></div><p>Here, we have <a id="id317" class="indexterm"/>approximately 400,000 data items. We need to know the number of users and products that are present in these data entries. We will be using the <code class="literal">value_counts()</code> function from the <code class="literal">pandas </code>library. Take a look at the code snippet in the following screenshot: </p><div class="mediaobject"><img src="Images/B08394_03_11.jpg" alt="Customer and products" width="669" height="209"/><div class="caption"><p>Figure 3.11: Code for exploring customer and products</p></div></div><p>As you can see in the above screen shot that this dataset contains the records of 4372 users who bought 3684 different items</p><p>We have derived some interesting facts. In the given dataset, there are 4,372 customers who have bought 3,684 different products. The total number of transactions is 22,190.</p><p>We should also find out how many products have been purchased for each transaction. For that, we will use the <code class="literal">InvoiceNo </code>and <code class="literal">InvoiceDate </code>data attributes, and we will calculate the number of products purchased for every transaction. You can refer to the code snippet shown in the following screenshot: </p><div class="mediaobject"><img src="Images/B08394_03_12.jpg" alt="Customer and products" width="776" height="410"/><div class="caption"><p>Figure 3.12: Code snippet for exploring the number of products per transaction</p></div></div><p>As shown <a id="id318" class="indexterm"/>in the preceding code snippet, we can make the following observations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are some users who have made a purchase only once on the e-commerce platform and bought one item. An example of this kind of user is <code class="literal">customerID 12346</code>.</li><li class="listitem" style="list-style-type: disc">There are some users who frequently buy a large number of items per order. An example of this kind of user is <code class="literal">customerID 12347</code>.</li><li class="listitem" style="list-style-type: disc">If you look at the InvoiceNo data attribute, then you can see that there is the prefix <code class="literal">C</code> for one invoice. This <code class="literal">'C'</code> indicates that the particular transaction has been canceled.</li></ul></div><p>As we know, there can be a couple of canceled orders present in our dataset, and we need to count the number of transactions corresponding to the canceled orders. We have used a simple check condition using the lambda expression. Now we will calculate the percentage of canceled orders. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_13.jpg" alt="Customer and products" width="694" height="475"/><div class="caption"><p>Figure 3.13: Code snippet for generating the percentage of canceled orders</p></div></div><p>Let's list <a id="id319" class="indexterm"/>down some of the canceled order entries so that we can find out how to handle them. Take a look at the following screenshot:
</p><div class="mediaobject"><img src="Images/B08394_03_14.jpg" alt="Customer and products" width="767" height="288"/><div class="caption"><p>Figure 3.14: List of canceled orders</p></div></div><p>Basically, in order to handle the canceled orders, we will need to take the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">As you can observe, if the order is canceled, then there is another transaction that will mostly have an identical transaction except for the quantity and invoice date.  First, we need to check whether this is true for all entries.</li><li class="listitem" style="list-style-type: disc">We can perform this checking operation by using simple logic. Mostly, the canceled order has a negative quantity, so we will check whether there is an order indicating the same quantity (but positive), with the same description values.</li><li class="listitem" style="list-style-type: disc">There are some discount entries as well, and we need to handle them. We will discard the discount entries.</li></ul></div><p>You can <a id="id320" class="indexterm"/>refer to the code for this, as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_15.jpg" alt="Customer and products" width="764" height="401"/><div class="caption"><p>Figure 3.15: Code for handelling cancel orders</p></div></div><p>When we run the preceding code, we find out that there are no similar entries present in our dataset for all canceled transactions. In order to overcome this situation, we will create a <a id="id321" class="indexterm"/>new variable in our dataframe, which indicates whether the transaction has been canceled or not. There are three possibilities for canceled orders:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are some transactions that were canceled without counterparts. A few of them are probably due to the fact that the buy orders were performed before December 2010. We have the dataset from December 2010 to December 2011.</li><li class="listitem" style="list-style-type: disc">There are some orders that were canceled with exactly one counterpart. We will consider them as well.</li><li class="listitem" style="list-style-type: disc">There are some entries that are doubtful. We will check whether there is at least one counterpart with the exact same quantity available. If available, then we can mark those entries as doubtful.</li></ul></div><p>You can refer to the code shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_16.jpg" alt="Customer and products" width="764" height="626"/><div class="caption"><p>Figure 3.16: Code snippet for generating flags for canceled orders</p></div></div><p>As we can see in the preceding code snippet, there are 7,521 entries that show the canceled <a id="id322" class="indexterm"/>orders with their counterpart. There are 1,226 entries that show canceled orders without their counterpart. For the sake of simplicity, we are going to delete all the entries related to the canceled orders. The code for deleting these records is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_17.jpg" alt="Customer and products" width="766" height="457"/><div class="caption"><p>Figure 3.17: Code snippet for deleting canceled orders</p></div></div><p>Now let's analyze <a id="id323" class="indexterm"/>the entries based on the stock code because we know that during the identification of the canceled order, we discover discount items based on the <span class="emphasis"><em>stock code D</em></span>. So first of all, we will be listing down all the stock codes and their meaning. You can refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_18.jpg" alt="Customer and products" width="762" height="439"/><div class="caption"><p>Figure 3.18: Code snippet for stock code</p></div></div><p>Now let's focus <a id="id324" class="indexterm"/>on the pricing of the individual order. In the given dataset, the order from a single customer has been split into several lines. What do I mean by several lines? In order to understand that, refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_19.jpg" alt="Customer and products" width="968" height="430"/><div class="caption"><p>Figure 3.19: Understanding data entries for orders</p></div></div><p>Each entry in our dataset indicates prizes for a single kind of product. If the order including different products is placed by a single customer, then there are multiple entries for that particular order. The number of data entries depends on how many different <a id="id325" class="indexterm"/>products that order has. As you can see in the preceding figure, there were three different products included in one order. We need to obtain the total price for each order. In order to achieve that, we will add a column named <span class="emphasis"><em>TotalPrice</em></span>, which gives us the total value of the order or the basket price for a single order. The main logic for deriving <span class="emphasis"><em>TotalPrice</em></span> is that we are multiplying <span class="emphasis"><em>UnitPrice</em></span> with the net quantity. We obtain the net quantity by deducting the canceled quantity from the total quantity.  Take a look at the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_20.jpg" alt="Customer and products" width="1000" height="341"/><div class="caption"><p>Figure 3.20: Code for obtaining TotalPrice</p></div></div><p>Once we obtain the total price, we will generate the sum for individual orders and then group our entries based on the invoice data. We will list only those data entries that have a basket price greater than 0. The code to achieve this is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_21.jpg" alt="Customer and products" width="816" height="495"/><div class="caption"><p>Figure 3.21: Code for generating the basket price based on the invoice date</p></div></div><p>Now it's time to get an idea about the distribution of the orders' amounts for the given dataset. What do I mean by distribution of the orders' amounts? Well, we should be aware about the prices for all the orders present in the dataset, and we need to put in the ranges <a id="id326" class="indexterm"/>based on the amount of all the orders. This will help us derive the number of orders in the dataset that are above £200. It will also help us identify the number of orders that are below £100. This kind of information helps us know the data distribution based on the number of orders. This will give us a basic picture about sales on the e-commerce platform. The code snippet for generating data distribution based on the orders' amounts is displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_22.jpg" alt="Customer and products" width="797" height="409"/><div class="caption"><p>Figure 3.22: Code snippet for generating data distribution based on orders' amounts</p></div></div><p>You can <a id="id327" class="indexterm"/>see the pictorial representation of this data distribution as follows:</p><div class="mediaobject"><img src="Images/B08394_03_23.jpg" alt="Customer and products" width="482" height="394"/><div class="caption"><p>Figure 3.23: Pictorial representation of the data distribution</p></div></div><p>As we can <a id="id328" class="indexterm"/>see, approximately 65% of the orders are above £200. We have explored orders in great detail. Now let's begin with the analysis of product categories.</p></div><div class="section" title="Product categories"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec03"/><span class="strong"><strong>Product categories</strong></span></h5></div></div></div><p>In this <a id="id329" class="indexterm"/>section, we will be doing an EDA of the product-related data attribute. We will include the following kinds of analysis in this section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Analyzing the product description</li><li class="listitem" style="list-style-type: disc">Defining the product categories</li><li class="listitem" style="list-style-type: disc">Characterizing the content of clusters</li></ul></div><div class="section" title="Analyzing the product description"><div class="titlepage"><div><div><h6 class="title"><a id="ch03lvl6sec01"/>
<span class="strong"><strong>Analyzing the product description</strong></span>
</h6></div></div></div><p>In this section, we <a id="id330" class="indexterm"/>will be using two data attributes. We will use the <code class="literal">StockCode </code>data attribute, which contains a unique ID for each product. We will also use the <code class="literal">Description </code>data attribute in order to group the products in different categories. Let's start with the product description.</p><p>First, we will define the function that will take the dataframe as input, and then we will perform the the following operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We will extract names (nouns) from the product description.</li><li class="listitem" style="list-style-type: disc">Then, we will generate the root form of the extracted names. We will store the root of the name as the key and all associated names as its value. We will use a stemmer from the NLTK library for this step. A stemmer basically generates the root form of the words by removing suffixes and prefixes.</li><li class="listitem" style="list-style-type: disc">We will count the frequency of the roots of the names, which means we will count how many times the root form of each name appears.</li><li class="listitem" style="list-style-type: disc">If various names have the same root, then we consider the root form as the keyword tag.</li></ul></div><p>You can see the code for this function in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_24.jpg" alt="Analyzing the product description" width="728" height="658"/><div class="caption"><p>Figure 3.24: Code snippet of the function for generating keywords from the product description</p></div></div><p>Now we need <a id="id331" class="indexterm"/>to call this function and feed the input dataframe. You can take a look at the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_25.jpg" alt="Analyzing the product description" width="809" height="115"/><div class="caption"><p> Figure 3.25: Code snippet that actually generates keywords</p></div></div><p>Here, we are returning three variables:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Keyword:</code> This is the list of extracted names</li><li class="listitem" style="list-style-type: disc"><code class="literal">Keywords_roots: </code>This is a dictionary where the keys are the root of the name and values are the list of names associated with root name.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Count_keywords:</code> This is a dictionary that keeps track of the frequency of each name. The count indicates the number of times a particular name appeared in the description. Later on, we will convert the dictionary into a list.</li></ul></div><p>Now let's plot <a id="id332" class="indexterm"/>the keywords versus their frequency graphs. The code is given in the following screenshot: 
</p><div class="mediaobject"><img src="Images/B08394_03_26.jpg" alt="Analyzing the product description" width="701" height="519"/><div class="caption"><p>Figure 3.26: Code snippet for generating the frequency graph</p></div></div><p>As you can see in the preceding figure, the word (meaning the noun or the name) heart has appeared <a id="id333" class="indexterm"/>the maximum number of times in the product description. You might wonder what the significance of generating this word frequency is. Well, we are using this to categorize products. Now it's time to look into how to come up with product categories.</p></div><div class="section" title="Defining product categories"><div class="titlepage"><div><div><h6 class="title"><a id="ch03lvl6sec02"/><span class="strong"><strong>Defining product categories</strong></span></h6></div></div></div><p>Here we <a id="id334" class="indexterm"/>will obtain the product categories. We have obtained more than 1,400 keywords, and the most frequent names have appeared in more than 200 products. Now we need to remove words that are less important. We can observe some useless words, such as names of colors and discard them. So, we will consider words that appear in the dataset more than 13 times. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_27.jpg" alt="Defining product categories" width="555" height="214"/><div class="caption"><p>Figure 3.27: Code snippet for preserving important words</p></div></div><p>Now we need to encode the data. Here, we have textual data and we need to convert it into a numerical format. For this, we will use one-hot encoding. One-hot encoding is a simple concept. In order to understand it, refer to the given matrix x. Take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_28.jpg" alt="Defining product categories" width="536" height="236"/><div class="caption"><p>Figure 3.28: Table for understanding one-hot data encoding</p></div></div><p>If a particular <a id="id335" class="indexterm"/>word is present in the product description, then the value of the coefficient is 1, and if the word is not present in the product description, then the value of the coefficient is 0. You can refer to the the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_29.jpg" alt="Defining product categories" width="742" height="296"/><div class="caption"><p>Figure 3.29: Intuitive example for one-hot data encoding</p></div></div><p>As you can see, this data encoding is a binary kind of vectorization because we are placing either zero or one. We will get a sparse vector for each word after encoding. In layman's terms, we can say that this kind of vectorization indicates the presence of the word in the product description.</p><p>Now let's create <a id="id336" class="indexterm"/>the groups or cluster for the product based on the price range. For that, we will be using the keyword list that we have generated, check whether the product description has the words that are present in the keywords, and take the mean value of <code class="literal">UnitPrice</code>. You can refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_30.jpg" alt="Defining product categories" width="635" height="704"/><div class="caption"><p>Figure 3.30: Code snippet for generating the product group based on the price range</p></div></div><p>Now we will <a id="id337" class="indexterm"/>create clusters of the products. We will be using the k-means clustering algorithm. We will also be using the scikit-learn library to implement the K-means clustering algorithm. The algorithm from scikit-learn uses Euclidean distance. In our case, this is not the best choice. We should use Hamming distance. The most suitable library for that is <code class="literal">Kmods</code>, but this library is not available for all operating systems, so we have to use the scikit-learn library. We need to define the number of clusters that can represent the data perfectly. We will come up with the ideal number of clusters, and then we will use the silhouette score.</p><p>You can take a look at how the k-means clustering algorithm works by using the link of this book: <a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing">https://www.packtpub.com/big-data-and-business-intelligence/python-natural-language-processing</a>, Refer section K-means clustering form <a class="link" href="ch08.xhtml" title="Chapter 8. Developing Chatbots">Chapter 8</a>,  <span class="emphasis"><em>Machine Learning for NLP problems</em></span>.</p><p>Let's take <a id="id338" class="indexterm"/>a step back and understand the silhouette score first. The silhouette coefficient is calculated using two things. The first is the mean intra-cluster distance (a) and the second is the mean nearest-cluster distance (b) for each sample in our dataset. So, the equation is as follows:</p><p>
<span class="emphasis"><em>(b-a) / max (a, b)</em></span>
</p><p>The <span class="emphasis"><em>b</em></span> indicates the distance between a sample and the nearest cluster that the sample is not a part of. This score works if the number of labels is <span class="emphasis"><em>2&lt;= n_labels &lt;= n_samples –1</em></span>. The best possible value for this score is 1, and worst value is –1. Value 0 shows that we have overlapping clusters. Negative values indicate that the sample has been assigned to the wrong cluster. Refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_31.jpg" alt="Defining product categories" width="751" height="276"/><div class="caption"><p>Figure 3.31: Code snippet for choosing the ideal number of clusters using silhouette score</p></div></div><p>Here, we have implemented the code using the scikit-learn API. As we can see, beyond five clusters, a cluster may contain very few elements, so we choose to categorize the products into five clusters. We will try to increase the value of the silhouette score. For that, we will iterate through the dataset. You can refer to the code shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_32.jpg" alt="Defining product categories" width="774" height="259"/><div class="caption"><p>Figure 3.32: Code snippet to improvise the silhouette score</p></div></div><p>Now let's <a id="id339" class="indexterm"/>move on to characterizing the content of the clusters section, which can help us understand how well the products have been classified into particular clusters.</p></div></div><div class="section" title="Characterizing the content of clusters"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec04"/><span class="strong"><strong>Characterizing the content of clusters</strong></span></h5></div></div></div><p>In this section, we will <a id="id340" class="indexterm"/>analyze the properties of the product cluster. There will be three subsections here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Silhouette intra-cluster score analysis</li><li class="listitem" style="list-style-type: disc">Analysis using a word cloud</li><li class="listitem" style="list-style-type: disc">Principal component analysis (PCA)</li></ul></div><p>Before we jump into this analysis, we need to check the number of products in each cluster. For that, we will be using the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_33.jpg" alt="Characterizing the content of clusters" width="283" height="162"/><div class="caption"><p>Figure 3.33: Code snippet for counting the number of products for each cluster</p></div></div><p>As you can see in the output, there are 1,009 products that belong to cluster number 3, whereas <a id="id341" class="indexterm"/>there are only 470 products that belong to cluster number 4. We will start an in-depth analysis of these five clusters and their elements. First, we will start with the silhouette intra-cluster score analysis.</p><div class="section" title="Silhouette intra-cluster score analysis"><div class="titlepage"><div><div><h6 class="title"><a id="ch03lvl6sec03"/><span class="strong"><strong>Silhouette intra-cluster score analysis</strong></span>
</h6></div></div></div><p>Basically, in this <a id="id342" class="indexterm"/>section, we will be checking the intra-cluster score for each element. We will sort the silhouette intra-cluster score. After sorting, we will draw a graph where the <span class="emphasis"><em>x</em></span> <span class="emphasis"><em>axis</em></span> represents the silhouette coefficient value and the <span class="emphasis"><em>y</em></span> <span class="emphasis"><em>axis</em></span> represents the cluster label. We generate the silhouette intra-cluster score for all the samples. We are building this graph because we want to choose an optimal value for <code class="literal">n_clusters</code> based on the silhouette intra-cluster score.</p><p>As we have generated the silhouette intra-cluster score earlier, we know <code class="literal">n_clusters = 5</code> is the ideal choice for us, so we will represent the clusters in a pictorial manner. You can refer to the function that generates graphs in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_34.jpg" alt="Silhouette intra-cluster score analysis" width="801" height="464"/><div class="caption"><p>Figure 3.34: Code snippet of the function for silhouette intra-cluster score analysis</p></div></div><p> After <a id="id343" class="indexterm"/>executing and calling this function, we can obtain the graph displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_35.jpg" alt="Silhouette intra-cluster score analysis" width="781" height="626"/><div class="caption"><p>Figure 3.35: Code snippet and graph for silhouette intra-cluster analysis</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>Note that here, we obtain the graph for the optimal <code class="literal">n_cluster</code> value. This value is 5 in our case.</p></div></div></div></div><div class="section" title="Analysis using a word cloud"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec05"/><span class="strong"><strong>Analysis using a word cloud</strong></span></h5></div></div></div><p>In this <a id="id344" class="indexterm"/>section, we will analyze the clusters based on the keywords. We will check what words each cluster has. For this analysis, we will be using the word cloud library. You must be wondering why we are using this type of analysis. In our clusters, we are expecting similar kinds of products to belong to one cluster. We, as humans, know the language. When we see the words for the entire cluster, we can easily conclude whether our clusters have similar kinds of products or not. We will generate graphs that are intuitive enough for us to judge the accuracy of clustering.</p><p>You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_36.jpg" alt="Analysis using a word cloud" width="684" height="183"/><div class="caption"><p>Figure 3.36: Code snippet for generating a word cloud</p></div></div><p>You can <a id="id345" class="indexterm"/>refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_37.jpg" alt="Analysis using a word cloud" width="626" height="594"/><div class="caption"><p>Figure 3.37: Code snippet for generating word cloud graphs</p></div></div><p>You can <a id="id346" class="indexterm"/>refer to the graphs given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_38.jpg" alt="Analysis using a word cloud" width="803" height="555"/><div class="caption"><p>Figure 3.38: Word cloud graphs for all five clusters</p></div></div><p>From the <a id="id347" class="indexterm"/>preceding graphs, we can conclude the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cluster number 2 contains all the words related to gifts, such as Christmas, packaging, gift, cards, and so on.</li><li class="listitem" style="list-style-type: disc">Cluster number 4 contains all the words related to luxury items and jewelry. So, keywords such as necklace, silver, lace, and so on are present in this cluster.</li><li class="listitem" style="list-style-type: disc">There are some words that are present in every cluster, so it is difficult to clearly distinguish them.</li></ul></div><p>Now let's jump to the next section, where we will perform principal component analysis.</p></div><div class="section" title="Principal component analysis (PCA)"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec06"/>Principal component analysis (PCA)</h5></div></div></div><p>In order <a id="id348" class="indexterm"/>to check whether all the clusters have truly distinct values, we need to focus on their composition. As we know, the one-hot encoded matrix of the keywords has a large number of dimensions or a large number of variables. There may be a situation where because of the large number of variables, our clustering algorithm may over-fit the dataset. First of all, we need to reduce the number of variables, but we cannot reduce them randomly. We need to choose the most important variables that can represent most of the characteristics of the dataset. The procedure for reducing the number of variables logically is called dimensionality reduction.</p><p> In order to achieve this, we will be using PCA, which is a statistical technique in which we will perform orthogonal transformation in order to convert a highly correlated set of data samples into a set of values that are linearly uncorrelated variables, and these variables are referred to as principal components. So basically, we will be using PCA because we want to reduce the number of variables that we have considered so far. PCA is a famous technique for dimensionality reduction. By using PCA, we can avoid the over-fitting issue.</p><p>Now, you might want to know situations in which you can use PCA, and they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If we want to reduce the number of variables (the number of features or the number of dimensions) but we cannot identify which variables can be considered and which can't</li><li class="listitem" style="list-style-type: disc">If we want to ensure that our variables are independent of each other</li><li class="listitem" style="list-style-type: disc">If we are comfortable making our independent variables less interpretable</li></ul></div><p>In our case, we need to reduce the number of variables. For that, we are going to implement the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_39.jpg" alt="Principal component analysis (PCA)" width="844" height="675"/><div class="caption"><p>Figure 3.39: Code snippet for implementing PCA</p></div></div><p>As you <a id="id349" class="indexterm"/>can see in the preceding code, we are checking the amount of variance explained by each component. We need to consider more than 100 components to explain 90% of the variance of our dataset.</p><p>Here, I will consider a limited number of components because this decomposition is performed only to visualize the data. You can refer to the code shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_40.jpg" alt="Principal component analysis (PCA)" width="677" height="728"/><div class="caption"><p>Figure 3.40: Code snippet for generating PCA decomposition graphs</p></div></div><p>As you <a id="id350" class="indexterm"/>can see, we have used PCA components using <code class="literal">PCA(n_components=50)</code>, and we have stored the values in dataframe  <code class="literal">mat</code>, which we can use in future.</p><p>The output of the preceding code is in the form of graphs. So, you can refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_41.jpg" alt="Principal component analysis (PCA)" width="848" height="229"/><div class="caption"><p>Figure 3.41: Graphs for the PCA for each cluster</p></div></div><p>Here, we have used <code class="literal">tight_layout</code>, which is the reason why the graphs shrank a bit.  </p><p>So far, we have <a id="id351" class="indexterm"/>performed enough EDA to help us generate a basic insight into the dataset. Now we will move on to the next section, where we will start building customer categories or customer segmentation. We will take into account all the findings that we have implemented so far.</p></div></div></div><div class="section" title="Generating customer categories"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec25"/>Generating customer categories</h3></div></div></div><p>As you know, our first goal is to develop customer segmentation. From this section onward, we will <a id="id352" class="indexterm"/>focus mainly on how <a id="id353" class="indexterm"/>we can come up with customer segmentation. So far, we have done an analysis of orders, products, prices, and so on. Here, our main focus is on generating customer categories based on the insights that we got during EDA.</p><p>These are the steps that we are going to follow in order to develop the customer categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Formatting data:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"> Grouping products </li><li class="listitem" style="list-style-type: disc">Splitting the dataset</li><li class="listitem" style="list-style-type: disc">Grouping orders </li></ul></div></li><li class="listitem" style="list-style-type: disc">Creating customer categories:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"> Data encoding</li><li class="listitem" style="list-style-type: disc">Generating customer categories</li></ul></div></li></ul></div><p>Now let's see <a id="id354" class="indexterm"/>what we are going to <a id="id355" class="indexterm"/>do in each of these steps.</p><div class="section" title="Formatting data"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec32"/>Formatting data</h4></div></div></div><p>As mentioned <a id="id356" class="indexterm"/>earlier, we will be using the findings that we generated during EDA. In the previous section, we generated five clusters <a id="id357" class="indexterm"/>for products. In order to perform the rest of the analysis, we will use this already generated list of keywords, matrices, and clusters. By using them, we will be generating a new categorical variable, <code class="literal">categ_product</code>. This variable indicates the cluster of each product. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_42.jpg" alt="Formatting data" width="603" height="446"/><div class="caption"><p> Figure 3.42: Code snippet for generating new categorical variable categ_product</p></div></div><p>As you can <a id="id358" class="indexterm"/>see, the new variable indicates the cluster <a id="id359" class="indexterm"/>number for each data entry.  Now let's group the products.</p><div class="section" title="Grouping products"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec07"/>Grouping products </h5></div></div></div><p>You might <a id="id360" class="indexterm"/>wonder that if we have already developed the categories of the product, then why are we performing the grouping step here. Well, here, we will perform grouping in such a way that we can know what amount has been spent in each product category. For this, we will add five new variables, for example, categ_0, categ_1, categ_2, categ_3, and categ_4. You can refer to the code snippet displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_43.jpg" alt="Grouping products" width="759" height="508"/><div class="caption"><p>Figure 3.43: Code snippet for generating the amount spent in each product category</p></div></div><p>Orders are <a id="id361" class="indexterm"/>split into multiple entries, so we need to use the basket price. This time, we will merge the basket price as well as the way it is distributed over five product categories. We will put all this information into the new dataframe. Refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_44.jpg" alt="Grouping products" width="794" height="547"/><div class="caption"><p>Figure 3.44: Code snippet for obtaining the distribution of basket prices for five clusters</p></div></div><p>Finally, we have <a id="id362" class="indexterm"/>the basket price for each order, and we also know the price distribution over five clusters. The new dataframe is <code class="literal">basket_price</code>. Now let's move on to the next section.</p></div><div class="section" title="Splitting the dataset"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec08"/>Splitting the dataset</h5></div></div></div><p>In this section, we will be using the dataframe <code class="literal">basket_price</code>, which contains data entries for <a id="id363" class="indexterm"/>the past 12 months. The second goal of this application is to predict the customer purchase behavior based on their first site visit or purchase. So, in order to achieve that goal right now, we will split the dataset. We will use 10 months' dataset for training and 2 months' dataset for testing. I'm including this step here because later on, we can use these training and testing datasets and you can easily get to use the new dataframe. You can refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_45.jpg" alt="Splitting the dataset" width="717" height="137"/><div class="caption"><p>Figure 3.45: Code snippet for splitting the dataset using time</p></div></div><p>Now we <a id="id364" class="indexterm"/>will group the customers and their orders along with the basket price distribution.</p></div><div class="section" title="Grouping orders"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec09"/>Grouping orders </h5></div></div></div><p>Here, we will <a id="id365" class="indexterm"/>merge the customers and their orders so that we can learn which customer placed how many orders. We will also generate the minimum order amount, the maximum order amount, and the mean order amount. Refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_46.jpg" alt="Grouping orders" width="895" height="388"/><div class="caption"><p>Figure 3.46: Code snippet for generating order-wise stats for each customer</p></div></div><p>We will also generate two variables that indicate the number of days elapsed since the last purchase and the first purchase. The names of these variables are <code class="literal">FirstPurchase </code>and <code class="literal">LastPurchase</code>. Refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_47.jpg" alt="Grouping orders" width="931" height="391"/><div class="caption"><p>Figure 3.47: Code snippet for generating elapsed days for the last and first purchase</p></div></div><p>The customer <a id="id366" class="indexterm"/>categories in which we are interested are the ones that make only one order. One of our main objectives is to target these customers in such a way that we can retain them. We need to obtain the data for the number of customers that belong to this category. For that, refer to the code given in the following screenshot:</p><p> </p><div class="mediaobject"><img src="Images/B08394_03_48.jpg" alt="Grouping orders" width="762" height="102"/><div class="caption"><p> Figure 3.48: Code snippet for generating the number of customers with one purchase</p></div></div><p>
</p><p>From the preceding code, we can find out that 40% of the customer base has placed only one order, and we need to retain them.</p><p>Now let's build the customer categories.</p></div></div><div class="section" title="Creating customer categories"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec33"/>Creating customer categories</h4></div></div></div><p>Basically, we will <a id="id367" class="indexterm"/>be generating the customer segmentation here. So, we will work on achieving the first goal of the chapter in this section. We will build customer segmentation based on the customers' purchase pattern. There <a id="id368" class="indexterm"/>are two steps in this section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data encoding </li><li class="listitem" style="list-style-type: disc">Generating customer categories or customer segmentation</li></ul></div><p>We will start with data encoding.</p><div class="section" title="Data encoding"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec10"/>Data encoding</h5></div></div></div><p>We will be <a id="id369" class="indexterm"/>generating the dataframe that contains the summary of all operations we have performed so far. Each record of this dataframe is associated with a single client. We can use this information to characterize various types of customers.</p><p>The dataframe that we have generated has different variables. All these variables have different ranges and variations. So, we need to generate a matrix where these data entries are standardized. You can refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_49.jpg" alt="Data encoding" width="900" height="317"/><div class="caption"><p>Figure 3.49: Code snippet for generating summary data entries for each client</p></div></div><p>Before creating customer segmentation, we need to create the base. This base should include important variables. We need to include a small number of important variables. In order to select the important variables, we will be using principal component analysis. So, that we can describe the segmentation accurately. We will use PCA for this task. The code snippet is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_50.jpg" alt="Data encoding" width="899" height="668"/><div class="caption"><p>Figure 3.50: Code snippet for PCA in order to generate the customer segmentation</p></div></div><p>Here, we can <a id="id370" class="indexterm"/>see that there are eight principal components. Now let's move on to the next section, where we will generate the customer segmentation.</p></div><div class="section" title="Generating customer categories"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec11"/>Generating customer categories</h5></div></div></div><p>We will be <a id="id371" class="indexterm"/>using the k-means clustering algorithm to generate segmentation. The number of clusters will be derived by using the silhouette score. We have used the silhouette score earlier, and by using the same method, we can derive the number of clusters. Here, we obtain 11 clusters based on the silhouette score. You can refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_51.jpg" alt="Generating customer categories" width="746" height="292"/><div class="caption"><p>Figure 3.51: Code snippet for generating customer segmentations</p></div></div><p>As you <a id="id372" class="indexterm"/>can see, there is a large difference in the size of the segmentation, so we need to analyze the components of the clusters using PCA.</p></div><div class="section" title="PCA analysis"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec12"/>PCA analysis</h5></div></div></div><p>We will use <a id="id373" class="indexterm"/>six components here. The code snippet and graphical representation of PCA for 11 clusters are given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_52.jpg" alt="PCA analysis" width="816" height="802"/><div class="caption"><p>Figure 3.52: Code snippet for implementing PCA and generating graphs</p></div></div><p>As an <a id="id374" class="indexterm"/>output, the following graphs have been generated:</p><div class="mediaobject"><img src="Images/B08394_03_53.jpg" alt="PCA analysis" width="874" height="318"/><div class="caption"><p>Figure 3.53: Graphs of PCA for customer segmentation</p></div></div><p>I have displayed only three graphs here. In the code, there are nine graphs. When you run the code, you can see them all. Note that the first component separates the tiniest cluster <a id="id375" class="indexterm"/>from the rest. For this dataset, we can say that there will always be a representation in which two segments will appear to be distinct. Now let's obtain silhouette scores.</p></div><div class="section" title="Analyzing the cluster using silhouette scores"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec13"/>Analyzing the cluster using silhouette scores</h5></div></div></div><p>In this <a id="id376" class="indexterm"/>section, we will generate the silhouette score for each cluster. This will indicate the quality of the separation of data samples. You can refer to the code snippet and graph shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_54.jpg" alt="Analyzing the cluster using silhouette scores" width="788" height="684"/><div class="caption"><p>Figure 3.54: Code snippet for generating graphs for silhouette scores</p></div></div><p>From the <a id="id377" class="indexterm"/>preceding graphs, we can ensure that all the clusters are disjointed. Now we need to learn more about the habits of the customers of each cluster. To do that, we will add variables that define the cluster to which each customer belongs.  </p><p>For this, we will be generating a new dataframe, <code class="literal">selected_customers</code>. After generating the new dataframe, we will average the content of the dataframe. This will provide us with the average basket price, total visits, and so on. You can refer to the code shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_55.jpg" alt="Analyzing the cluster using silhouette scores" width="708" height="304"/><div class="caption"><p>Figure 3.55: Code snippet for storing the habits of the customers</p></div></div><p>Now we <a id="id378" class="indexterm"/>need to reorganize the content of the dataframe. We will be considering two points here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We need to reorganize the data based on the amount spent in each product category</li><li class="listitem">After that, we will reorganize the content based on the total amount spent</li></ol></div><p>You can take a look at the implementation shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_56.jpg" alt="Analyzing the cluster using silhouette scores" width="898" height="547"/><div class="caption"><p>Figure 3.56: Code snippet for reorganizing the dataset</p></div></div><p>As you <a id="id379" class="indexterm"/>can see, we have obtained the behavior of the customer for each segment. Now we can recommend the items based on these characteristics. We can design the marketing campaign based on the generated facts.</p><p>The particular marketing strategy can be applied to customers who belong to cluster 4 and cluster 8. We should recommend the premium products to the cluster 1 clients.</p><p>So far, we have achieved our first goal. Now it's time to aim for the second goal. So let's begin!</p></div></div></div><div class="section" title="Classifying customers"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec26"/>Classifying customers</h3></div></div></div><p>Before we <a id="id380" class="indexterm"/>begin, let's have a refresher on what our goal is. This helps you understand things in a clearer manner. The objective is <a id="id381" class="indexterm"/>that we are going to build a classifier that will classify the customers into different customer segments that were established in the previous section. We also need one more feature. Our classifier should generate this classification result when the customer visits the platform for the first time. In order to implement this kind of functionality, we will be using various supervised machine learning algorithms. We will use the scikit-learn API.</p><p>In order <a id="id382" class="indexterm"/>to develop the baseline classifier, we need <a id="id383" class="indexterm"/>to perform the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Defining the helper functions</li><li class="listitem" style="list-style-type: disc">Splitting the data into training and testing </li><li class="listitem" style="list-style-type: disc">Implementing the Machine Learning (ML) algorithm</li></ul></div><div class="section" title="Defining helper functions"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec34"/>Defining helper functions</h4></div></div></div><p>Basically, we <a id="id384" class="indexterm"/>define a class named <code class="literal">class_fit</code> and then we define various functions that can help us when we train the ML model. These are the helper functions that we will be using:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The <code class="literal">train</code> function helps us train the model</li><li class="listitem">The  <code class="literal">predict</code> function helps us predict the result for the test dataset or the new data sample</li><li class="listitem">The <code class="literal">grid_search</code> function helps us find out appropriate hyperparameters and the value of cross-validation(CV) folds</li><li class="listitem">The <code class="literal">grid_fit</code> function helps us train the model using cross-validation and generate the optimal hyperparameters .</li><li class="listitem">The <code class="literal">grid_predict</code> function helps us generate prediction as well as the accuracy score.</li></ol></div><p>You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_57.jpg" alt="Defining helper functions" width="744" height="392"/><div class="caption"><p>Figure 3.57: Code snippet for the helper function</p></div></div><p>Now let's move on to the next section.</p></div><div class="section" title="Splitting the data into training and testing"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec35"/>Splitting the data into training and testing </h4></div></div></div><p>We will be <a id="id385" class="indexterm"/>using the data that we <a id="id386" class="indexterm"/>have stored in the <code class="literal">selected_customers</code> dataframe. You can see some entries of the dataset on which we will apply the ML algorithm. Take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_58.jpg" alt="Splitting the data into training and testing" width="983" height="292"/><div class="caption"><p>Figure 3.58: Sample entries in the dataset</p></div></div><p>As you can see, we will predict the cluster number for the new customer, so we have stored that <a id="id387" class="indexterm"/>value as <code class="literal">Y</code>, and columns such as <code class="literal">mean, categ_0 to categ_4</code> are used as input features for the ML model, so we have stored them in the <code class="literal">X</code> variable. Now we need to split this data into training <a id="id388" class="indexterm"/>and testing. For that, we use the sklearn  API <code class="literal">train_test_split()</code>. We are using 80% of the data for training and 20% of data for testing. Take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_59.jpg" alt="Splitting the data into training and testing" width="725" height="29"/><div class="caption"><p>Figure 3.59: Code snippet for splitting the dataset into training and testing</p></div></div><p>We have the training and testing datasets with us. Now, we need to start implementing the ML algorithm.</p></div><div class="section" title="Implementing the Machine Learning (ML) algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec36"/>Implementing the Machine Learning (ML) algorithm</h4></div></div></div><p>For the <a id="id389" class="indexterm"/>baseline approach, we will be implementing the Support Vector machine (SVM) classifier. We will be using helper functions that we have previously defined. Here, I will create an instance of the class and call the methods that we have declared previously. Take a look at the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_60.jpg" alt="Implementing the Machine Learning (ML) algorithm" width="559" height="136"/><div class="caption"><p>Figure 3.60: Code snippet for training the model using the SVM classifier</p></div></div><p>As you can see in the code snippet, <code class="literal">svc</code> is the class instance. We are using linear SVM. We have used <code class="literal">grid_search</code> to search optimal hyperparameters as well as obtain the number <a id="id390" class="indexterm"/>of CV folds. After that, we have called the <code class="literal">grid_fit</code> method, which is used to train the ML model using our training dataset.</p><p>This is the way we have implemented our baseline approach. Now let's test the result.</p></div></div></div><div class="section" title="Understanding the testing matrix"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec52"/>Understanding the testing matrix</h2></div></div></div><p>We will <a id="id391" class="indexterm"/>be using the confusion matrix and the learning <a id="id392" class="indexterm"/>curve to evaluate the ML models. So before starting with the testing, we need to understand what the confusion matrix and the learning curve are. We will cover these concepts one by one.</p><div class="section" title="Confusion matrix"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec27"/>Confusion matrix</h3></div></div></div><p>When we <a id="id393" class="indexterm"/>are implementing a multi-class classifier, naturally, we have multiple classes and the number of data entries belonging to all the <a id="id394" class="indexterm"/>classes is different, so during testing, we need to know whether the classifier performs equally well for all the classes or whether it is biased toward some classes. This analysis can be done using the confusion matrix. It will have a count of how many data entries are correctly classified and how many are misclassified.</p><p>Let's take an example. Say, there is a total of 10 data entries that belong to a class, and the label for that class is 1. Now when we generate the prediction from our ML model, we will check how many data entries out of the 10 entries get the predicted class label 1. Suppose six data entries are correctly classified and get the class label 1. In this case, for six entries, the <span class="emphasis"><em>predicted label</em></span> and <span class="emphasis"><em>True label</em></span> is the same, so the accuracy is 60%, whereas for the remaining data entries, the ML model misclassifies them. The ML model predicts class labels other than 1.</p><p>From the preceding example, you can see that the confusion matrix gives us an idea about how many data entries are classified correctly and how many are misclassified. We can <a id="id395" class="indexterm"/>explore the class-wise accuracy of the classifier. Take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_61.jpg" alt="Confusion matrix" width="706" height="700"/><div class="caption"><p>Figure 3.61: Example of confusion matrix</p></div></div><p>Now let's take <a id="id396" class="indexterm"/>a look at the learning curve.</p></div><div class="section" title="Learning curve"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec28"/>Learning curve</h3></div></div></div><p>We are <a id="id397" class="indexterm"/>plotting two lines here. One line indicates the training score, and the other line indicates the testing score. Here, the training and testing scores <a id="id398" class="indexterm"/>determine cross-validated training and testing scores for different training dataset sizes. By using this learning curve, we can monitor whether the ML model is converging properly or not. Both the CV score and the training score will help us determine whether training is going in the right direction or the ML model is suffering from over-fitting or under-fitting. With the increased size of dataset, if the CV score and training scores achieve a low score, then it means that the training was not performed in a proper manner. However, if the CV score and training score increase with the increased size of dataset, then it means that the training is moving in the right direction. Refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_62.jpg" alt="Learning curve" width="553" height="985"/><div class="caption"><p>Figure 3.62: Bad and good examples for the learning curve</p></div></div><p>Now that <a id="id399" class="indexterm"/>we have understood the basic intuition behind the testing <a id="id400" class="indexterm"/>matrix, we can start testing our baseline approach.</p></div></div><div class="section" title="Testing the result of the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec53"/>Testing the result of the baseline approach</h2></div></div></div><p>In this <a id="id401" class="indexterm"/>section, we will test the baseline model using the following approaches:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generating the accuracy score for the classifier</li><li class="listitem" style="list-style-type: disc">Generating the confusion matrix for the classifier</li><li class="listitem" style="list-style-type: disc">Generating the learning curve for the classifier</li></ul></div><div class="section" title="Generating the accuracy score for classifier"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec29"/>Generating the accuracy score for classifier</h3></div></div></div><p>First, we <a id="id402" class="indexterm"/>will use <code class="literal">grid_predict</code> to generate the accuracy score for testing the dataset. We will check the accuracy of the SVM algorithm. For that, the code snippet is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_63.jpg" alt="Generating the accuracy score for classifier" width="283" height="65"/><div class="caption"><p>Figure 3.63: Code snippet for generating the accuracy score</p></div></div><p>We got a 79.50% precision for the baseline approach. Now let's look at the quality of the prediction using the confusion matrix.</p></div><div class="section" title="Generating the confusion matrix for the classifier"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec30"/>Generating the confusion matrix for the classifier</h3></div></div></div><p>Now we <a id="id403" class="indexterm"/>will generate the confusion matrix, which will give us a fair idea about which class is classified correctly and which classes have misclassified the data most of the time. To generate the confusion matrix, refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_64.jpg" alt="Generating the confusion matrix for the classifier" width="875" height="580"/><div class="caption"><p>Figure 3.64: Code snippet for generating the confusion matrix</p></div></div><p>We have used the <code class="literal">confusion_matrix</code> API for sklearn. To draw the plot, we will define a method with the name <code class="literal">plot_confusion_matrix</code>. With the help of the preceding code, we have generated the confusion matrix given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_65.jpg" alt="Generating the confusion matrix for the classifier" width="564" height="589"/><div class="caption"><p>Figure 3.65: Confusion matrix for the baseline approach</p></div></div><p>As you <a id="id404" class="indexterm"/>can see, the classifier was able to classify the data into class labels 0, 2, 4, 6, and 10 accurately, whereas for class labels 1, 5, 7, and 8, the classifier is not performing so well.</p><p>Let's draw the learning curve for the baseline approach.</p></div><div class="section" title="Generating the learning curve for the classifier"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec31"/>Generating the learning curve for the classifier</h3></div></div></div><p>A learning <a id="id405" class="indexterm"/>curve indicates whether the classifier is facing the over-fitting or under-fitting issue. The <code class="literal">plot_learning_curve</code> method is used to draw the learning curve for the classifier. You can refer to the code snippet in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_66.jpg" alt="Generating the learning curve for the classifier" width="785" height="593"/><div class="caption"><p>Figure 3.66: Code snippet for generating the learning curve for the baseline approach</p></div></div><p>The learning <a id="id406" class="indexterm"/>curve is displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_67.jpg" alt="Generating the learning curve for the classifier" width="456" height="320"/><div class="caption"><p>Figure 3.67: Learning curve for the baseline approach</p></div></div><p>As you can see, the CV curve converges at the same limit when we increase the sample size. This means <a id="id407" class="indexterm"/>that we have low variance and we are not suffering from over-fitting. Variance is the value that indicates how much our target function will change if we will provide different training dataset. Ideally the value of the target function is derived from the training dataset by Machine Learning algorithm however the value of estimated function should not change too much if we use another training dataset. Minor change (minor variance) in the estimated function is expected. Here, the accuracy score has a low bias, which means the model is not facing the under-fitting issue as well.</p></div></div><div class="section" title="Problems with the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec54"/>Problems with the baseline approach</h2></div></div></div><p>In this section, we will be discussing the problems we are facing with the baseline approach <a id="id408" class="indexterm"/>so that we can optimize the current approach. The problems are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The precision score is low. There is scope for improvement.</li><li class="listitem" style="list-style-type: disc">We need to try other ML algorithms so that we can compare the results. Later on, if there is a need, then we can build the voting mechanism.</li></ul></div><p>Basically, in the <a id="id409" class="indexterm"/>revised approach, we need to try out various ML algorithms so that we will be sure which algorithm we can use and which ones we should not use.</p></div><div class="section" title="Optimizing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec55"/>Optimizing the baseline approach  </h2></div></div></div><p>In this section, we will take all the problems into consideration and discuss the approach through <a id="id410" class="indexterm"/>which we will increase the accuracy of our classifier. As discussed in the previous section, we need to implement other ML algorithms. These are the six algorithms that we are going to implement with the revised approach: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Logistic regression</li><li class="listitem" style="list-style-type: disc">K-nearest neighbor</li><li class="listitem" style="list-style-type: disc">Decision tree</li><li class="listitem" style="list-style-type: disc">Random forest </li><li class="listitem" style="list-style-type: disc">Adaboost classifier </li><li class="listitem" style="list-style-type: disc">Gradient boosting classifier</li></ul></div><p>Based on the precision score of all the preceding algorithms, we will decide which algorithm can be used and which can't be used.</p><p>Without wasting time, let's start implementing the revised approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec39"/>Building the revised approach</h1></div></div></div><p> In this <a id="id411" class="indexterm"/>section, we will implement the various ML algorithms, check their precision score, and monitor their learning curve. There is a total of six ML algorithms that will be used to identify which one is the best suited for our application.</p><div class="section" title="Implementing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec56"/>Implementing the revised approach</h2></div></div></div><p>In this section, we will <a id="id412" class="indexterm"/>be implementing logistic regression, K-nearest neighbor, decision tree, random forest, Adaboost, and gradient descent. In order to implement this, we will be using the helper class that we built earlier. You can take a look at the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_68.jpg" alt="Implementing the revised approach" width="813" height="772"/><div class="caption"><p>Figure 3.68: Code snippet for performing training using various ML classifiers</p></div></div><p>We have already generated a precision score for all the classifiers. We can see random forest and <a id="id413" class="indexterm"/>gradient-boosting classifiers with great precision. However, we have still not checked their learning curve. First, we will check their learning curve and then conclude whether any classifier has been facing the over-fitting or under-fitting issue.</p></div><div class="section" title="Testing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec57"/>Testing the revised approach</h2></div></div></div><p>In this <a id="id414" class="indexterm"/>section, we will be checking the learning curves for all the classifiers. You can refer to the learning curves in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_69.jpg" alt="Testing the revised approach" width="887" height="933"/><div class="caption"><p>Figure 3.69: Learning curve for various ML classifiers</p></div></div><p>You can see <a id="id415" class="indexterm"/>that all the classifiers are trained appropriately. There is no under-fitting or over-fitting issue. With the increase data size, the scores are improving as well.</p></div><div class="section" title="Problems with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec58"/>Problems with the revised approach</h2></div></div></div><p>The major <a id="id416" class="indexterm"/>problem with this approach is that we need to decide which algorithm we need to use and which one we should stop using. We will discard the Adaboost classifier as its precision score is too low.</p><p>There is another catch that I need to highlight here. There is no single classifier that works well for all class labels. There may be a classifier that works well for class label 0, whereas another may work well for class label 8.  I believe, we should not discard any other classifier. We need to come up with a voting mechanism. In more technical terms, we need to develop an ensemble model so that the quality of our prediction is great and accurate.</p><p>Now we will take a look at what our approach will be in order to build a voting classifier that can give us the best possible accuracy.</p><div class="section" title="Understanding how to improve the revised approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec32"/>Understanding how to improve the revised approach</h3></div></div></div><p>As discussed, in order <a id="id417" class="indexterm"/>to improve the revised approach, we will be using a voting mechanism. For that, we will be using scikit-learn voting classifier APIs. First of all, we will use grid searching in order to generate appropriate hyperparameters for each classifier. After that, we will use voting-classifier APIs of scikit-learn and train the model. The approach is simple, so let's start implementing it.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec40"/>The best approach </h1></div></div></div><p>The classifier <a id="id418" class="indexterm"/>model that we will be generating in this approach should give us the best possible accuracy. We have already discussed this approach. If you are new to ensemble ML models, then let me give you a basic intuitive idea behind it. In layman's terms, ensemble ML models basically use a combination of various ML algorithms. What is the benefit of combining various ML models together? Well, we know there is no single classifier that can perfectly classify all the samples, so if we combine more than one classifier, then we can get more accuracy because the problem with one classifier can be overcome by another classifier. Due to this reason, we will use a voting classifier that is a type of ensemble classifier.</p><div class="section" title="Implementing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec59"/>Implementing the best approach</h2></div></div></div><p>As you know, we use <a id="id419" class="indexterm"/>grid search and voting classifier APIs to implement the best approach. As discussed, first, we will use grid search to obtain the best possible hyperparameters and then use the voting classifier API. The step-by-step implementation is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_70.jpg" alt="Implementing the best approach" width="694" height="413"/><div class="caption"><p>Figure 3.70: Code snippet for the best approach</p></div></div><p>As you can see, we get 90% precision for this approach. This time, we need to test the approach <a id="id420" class="indexterm"/>on our hold out corpus of two months so that we can find out how the voting classifier is performing on the unseen dataset.</p><p>In the next section, we will be testing this approach.</p></div><div class="section" title="Testing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec60"/>Testing the best approach</h2></div></div></div><p>We test <a id="id421" class="indexterm"/>our ML model on 20% of the dataset, which we put aside before even starting the training. This dataset is kind of a dev dataset for us. For training, we have considered 10 months' dataset. Now it is time to test the model on the hold out corpus. Here, our hold-out corpus consists of 2 months' data entries. These are the steps <a id="id422" class="indexterm"/>that we need to implement:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Transforming the hold-out corpus in the form of the training dataset</li><li class="listitem" style="list-style-type: disc">Converting the transformed dataset into a matrix form</li><li class="listitem" style="list-style-type: disc">Generating the predictions</li></ul></div><p>So let's start with the first step.</p><div class="section" title="Transforming the hold-out corpus in the form of the training dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec33"/>Transforming the hold-out corpus in the form of the training dataset</h3></div></div></div><p>First of all, we <a id="id423" class="indexterm"/>need to convert the data that resides in the <code class="literal">set_test </code>dataframe in the form of the training dataset. For that, we will store the copy in the new dataframe with the name <code class="literal">basket_price</code>.</p><p>Now we will generate the user characteristic data with the help of the same operation that we perform for the baseline approach. Don't worry. When you see the code, you will remember the steps that we performed earlier. After transforming the dataset, we will store it in the dataframe, <code class="literal">transactions_per_user</code>. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_71.jpg" alt="Transforming the hold-out corpus in the form of the training dataset" width="774" height="526"/><div class="caption"><p>Figure 3.71: Code snippet for transforming the test dataset into the same form of training dataset</p></div></div><p>Now let's <a id="id424" class="indexterm"/>convert the dataset into a matrix form.</p></div><div class="section" title="Converting the transformed dataset into a matrix form"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec34"/>Converting the transformed dataset into a matrix form</h3></div></div></div><p>Our classifiers <a id="id425" class="indexterm"/>take the matrix as an input, so we need to convert the transformed dataset into the matrix format. For that, we will use the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_72.jpg" alt="Converting the transformed dataset into a matrix form" width="724" height="90"/><div class="caption"><p>Figure 3.72: Code snippet for converting the test dataset into the matrix format</p></div></div><p>We are using a basic type conversion here.</p></div><div class="section" title="Generating the predictions"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec35"/>Generating the predictions</h3></div></div></div><p>In this section, we will be generating the precision score using voting classifiers. So, in order to <a id="id426" class="indexterm"/>generate the prediction for the test dataset, we need to use the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_03_73.jpg" alt="Generating the predictions" width="663" height="77"/><div class="caption"><p>Figure 3.73: Code snippet for generating the precision score for the test dataset</p></div></div><p>As you can see, we will achieve 76% of accuracy on our hold-out corpus. This is nice because we just use 10 months of data to build this model. By using 10 months' dataset, we achieve the best possible accuracy for this domain. If we consider more number of  datarecords, then we can still improve the results. This can be an exercise for you guys to consider more datasets and improvise the result.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Customer segmentation for various domains"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec41"/>Customer segmentation for various domains</h1></div></div></div><p>Note that <a id="id427" class="indexterm"/>we are considering e-commerce data here, but you can consider other datasets of various domains. You can build customer segmentation for a company providing travel services, financial services, and so on. The data points will vary from domain to domain.</p><p>For travel services, you could consider how frequently a user is booking flights or rooms using the traveling platform. Demographic and professional information helps a great deal, say, how many times a user uses promotional offers. The data for user activity is important as well.</p><p>If you are building a segmentation application for the financial domain, then you can consider the data points such as:  the transaction history of the account holder, for example, the frequency of using a debit card or a credit card, per-month income, per-month expenditure, the average balance the customer is maintaining in their bank account(s), the type of account user have, professional information of the customer, and so on. There <a id="id428" class="indexterm"/>are other common data points that you can consider for both the domains, such as the time spent on the website or the mobile app.</p><p>Right now, I will limit myself to these two domains, but you can perform customer segmentation for the telecom domain, the marketing domain, the educational domain, the entertainment domain, and so on.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec42"/>Summary</h1></div></div></div><p>All the given analytics models we have developed so far are critical for running a successful business. In this chapter, we developed customer segmentation based on the behavior of the customers. In order to do that, we used various algorithms, such as SVM, linear regression, decision tree, random forest, gradient boosting, voting-based models, and so on. By using the voting-based model, we achieved the best possible accuracy. Customer segmentation analysis is important for small and midsized organizations because these analysis help them optimize their marketing strategy as well as significantly improve the customer acquisition cost. I developed the code for the customer churn analysis, available at: <a class="ulink" href="https://github.com/jalajthanaki/Customer_churn_analysis">https://github.com/jalajthanaki/Customer_churn_analysis</a>, and for customer life-time value analysis at: <a class="ulink" href="https://github.com/jalajthanaki/Customer_lifetime_value_analysis">https://github.com/jalajthanaki/Customer_lifetime_value_analysis</a> . You can refer to them to learn more about customer analytics. You can read about customer analytics at: <a class="ulink" href="https://github.com/Acrotrend/Awesome-Customer-Analytics">https://github.com/Acrotrend/Awesome-Customer-Analytics</a>.</p><p>In the upcoming chapter, we will build a recommendation system that is specific to e-commerce products. We will build a recommendation application that will recommend books to users based on their browsing and purchasing activities on the platform. We will implement various techniques to build the best possible recommendation engine. So keep reading!</p></div></div>



  </body></html>