<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer412">
    <h1 class="chapterNumber">8</h1>
    <h1 class="chapterTitle" id="_idParaDest-196">Finding Patterns – Market Basket Analysis Using Association Rules</h1>
    <p class="normal">Think back to your last impulse purchase. Maybe in the grocery store checkout lane, you bought a pack of chewing gum or a candy bar. Perhaps on a late-night trip for diapers and formula, you picked up a caffeinated beverage or a six-pack of beer. You might have even bought this book on a bookseller’s recommendation. These impulse buys are no coincidence, as retailers use sophisticated data analysis techniques to identify useful patterns for marketing promotions and driving upselling via product placement.</p>
    <p class="normal">In years past, such recommendations were based on the subjective intuition of marketing professionals and inventory managers. Now, barcode scanners, inventory databases, and online shopping carts all generate transactional data that machine learning can use to learn purchasing patterns. The practice is commonly known as market basket analysis because it has been so frequently applied to supermarket data.</p>
    <p class="normal">Although the technique originated with shopping data, it is also useful in other contexts. By the time you finish this chapter, you will know how to apply market basket analysis techniques to your own tasks, whatever they may be. Generally, the work involves:</p>
    <ul>
      <li class="bulletList">Understanding the peculiarities of transactional data</li>
      <li class="bulletList">Using simple performance measures to find associations in large databases</li>
      <li class="bulletList">Knowing how to identify useful and actionable patterns</li>
    </ul>
    <p class="normal">Because market basket analysis is able to discover nuggets of insight in many types of large datasets, as we apply the technique, you are likely to identify applications to your work even if you have no affiliation with the retail sector.</p>
    <h1 class="heading-1" id="_idParaDest-197">Understanding association rules</h1>
    <p class="normal">The building blocks of a market basket analysis are the items that may appear in any given transaction. Groups of one or more items are surrounded by brackets to indicate that they form <a id="_idIndexMarker951"/>a set, or more specifically, an <strong class="keyWord">itemset</strong> that appears in the data with some regularity. Transactions are specified in terms of itemsets, such as the following transaction that might be found in a typical grocery store:</p>
    <p class="center"><code class="inlineCode">{bread, peanut butter, jelly}</code></p>
    <p class="normal">The result of a market basket analysis is a collection of <strong class="keyWord">association rules</strong> that specify patterns found in the relationships among items in the itemsets. Association rules are always composed from subsets of itemsets and are denoted by relating one itemset on the <strong class="keyWord">left-hand side</strong> (<strong class="keyWord">LHS</strong>) of the <a id="_idIndexMarker952"/>rule to another itemset on the <strong class="keyWord">right-hand side</strong> (<strong class="keyWord">RHS</strong>) of the <a id="_idIndexMarker953"/>rule. The LHS is the condition that needs to be met in order to trigger the rule, and the RHS is the expected result of meeting that condition. A rule identified from the preceding example transaction might be expressed in the form:</p>
    <p class="center"><code class="inlineCode">{peanut butter, jelly} → {bread}</code></p>
    <p class="normal">In plain language, this association rule states that if peanut butter and jelly are purchased together, then bread is also likely to be purchased. In other words, “peanut butter and jelly imply bread.”</p>
    <p class="normal">Developed in the context of retail transaction databases, association rules are not used for prediction, but rather for unsupervised knowledge discovery in large databases. This is unlike the classification and numeric prediction algorithms presented in previous chapters. Even so, you will find that the result of association rule learning is closely related to and shares many features with the result of classification rule learning as presented in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>.</p>
    <p class="normal">Because association rule learners are unsupervised, there is no need for the algorithm to be trained and the data does not need to be labeled ahead of time. The program is simply unleashed on a dataset in the hope that interesting associations are found. The downside, of course, is that there isn’t an easy way to objectively measure the performance of a rule learner, aside from evaluating it for qualitative usefulness—typically, an eyeball test of some sort.</p>
    <p class="normal">Although association rules are most often used for market basket analysis, they are helpful for finding patterns in many different types of data. Other potential applications include:</p>
    <ul>
      <li class="bulletList">Searching for interesting and frequently occurring patterns of DNA and protein sequences in cancer data</li>
      <li class="bulletList">Finding patterns of purchases or medical claims that occur in combination with fraudulent credit card or insurance use</li>
      <li class="bulletList">Identifying combinations of behavior that precede customers dropping their cellular phone service or upgrading their cable television package</li>
    </ul>
    <p class="normal">Association rule <a id="_idIndexMarker954"/>analysis is used to search for interesting connections among a very large number of elements. Human beings are capable of such insight quite intuitively, but it often takes expert-level knowledge or a great deal of experience to do what a rule learning algorithm can do in minutes or even seconds. Additionally, some datasets are simply too large and complex for a human being to find the needle in the haystack.</p>
    <h2 class="heading-2" id="_idParaDest-198">The Apriori algorithm for association rule learning</h2>
    <p class="normal">Just as large transactional datasets create challenges for humans, these datasets also present challenges for machines. Transactional datasets can be large in both the number of <a id="_idIndexMarker955"/>transactions as well as the number of items or features that are recorded. The fundamental problem of searching for interesting <a id="_idIndexMarker956"/>itemsets is that the number of potential itemsets grows exponentially with the number of items. Given <em class="italic">k</em> items that can appear or not appear in a set, there are <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">k</sup> possible itemsets that could be potential rules. A retailer that sells only 100 different items could have on the order of <em class="italic">2^100 = 1.27e+30</em> itemsets that an algorithm must evaluate—a seemingly impossible task.</p>
    <p class="normal">Rather than evaluating each of these itemsets one by one, a smarter rule learning algorithm takes advantage of the fact that many of the potential combinations of items are rarely, if ever, found in practice. For instance, even if a store sells both automotive items and food products, a set of <em class="italic">{motor oil, bananas}</em> is likely to be extraordinarily uncommon. By ignoring these rare (and perhaps less important) combinations, it is possible to limit the scope of the search for rules to a more manageable size.</p>
    <p class="normal">Much work has been done to identify heuristic algorithms for reducing the number of itemsets to search. Perhaps the most widely-known approach for efficiently searching large databases for <a id="_idIndexMarker957"/>rules is known as <strong class="keyWord">Apriori</strong>. Introduced in 1994 by Rakesh Agrawal and Ramakrishnan Srikant, the Apriori algorithm has since become somewhat synonymous with association rule learning, despite the invention of newer and <a id="_idIndexMarker958"/>faster algorithms. The name is derived from <a id="_idIndexMarker959"/>the fact that the algorithm utilizes a simple prior (that is, <em class="italic">a priori</em>) belief about the properties of frequent itemsets.</p>
    <p class="normal">Before we discuss that in more depth, it’s worth noting that this algorithm, like all learning algorithms, is not without its strengths and weaknesses. Some of these are listed as follows:</p>
    <table class="table-container" id="table001-6">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Capable of working with large amounts of transactional data</li>
              <li class="bulletList">Results in rules that are easy to understand</li>
              <li class="bulletList">Useful for data mining and discovering unexpected knowledge in databases</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Not very helpful for relatively small datasets</li>
              <li class="bulletList">Takes effort to separate true insight from common sense</li>
              <li class="bulletList">Easy to draw spurious conclusions from random patterns</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">As noted earlier, the Apriori algorithm employs a simple <em class="italic">a priori</em> belief as a guideline for reducing the association rule search space: all subsets of a frequent itemset must also be frequent. This heuristic is <a id="_idIndexMarker960"/>known as the <strong class="keyWord">Apriori property</strong>. Using this astute observation, it is possible to dramatically limit the number of rules to search. For example, the set <em class="italic">{motor oil, bananas}</em> can only be frequent if both <em class="italic">{motor oil}</em> and <em class="italic">{bananas}</em> occur frequently as well. Consequently, if either <em class="italic">{motor oil}</em> or <em class="italic">{bananas}</em> are infrequent, then any set containing these items can be excluded from the search.</p>
    <div class="note">
      <p class="normal">For additional details on the Apriori algorithm, refer to <em class="italic">Fast Algorithms for Mining Association Rules, Agrawal, R., Srikant, R., Proceedings of the 20th International Conference on Very Large Databases, 1994, pp. 487-499</em>.</p>
    </div>
    <p class="normal">To see how this principle can be applied in a more realistic setting, let’s consider a simple transaction database. The following table shows five completed transactions at an imaginary hospital’s gift shop:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_08_01.png"/></figure>
    <p class="packt_figref">Figure 8.1: Itemsets representing five transactions in a hypothetical hospital’s gift shop</p>
    <p class="normal">By looking <a id="_idIndexMarker961"/>at the sets of purchases, one can infer that there are a couple of typical buying patterns. A person visiting a sick friend or family member <a id="_idIndexMarker962"/>tends to buy a get-well card and flowers, while visitors of new mothers tend to buy plush toy bears and balloons. Such patterns are notable because they appear frequently enough to catch our interest; we simply apply a bit of logic and subject matter experience to explain the rule.</p>
    <p class="normal">In a similar fashion, the Apriori algorithm uses statistical measures of an itemset’s “interestingness” to locate association rules in much larger transaction databases. In the sections that follow, we will discover how Apriori computes such measures of interest, and how they are combined with the Apriori property to reduce the number of rules to be learned.</p>
    <h2 class="heading-2" id="_idParaDest-199">Measuring rule interest – support and confidence</h2>
    <p class="normal">Whether or not an association rule is deemed interesting is determined by two statistical <a id="_idIndexMarker963"/>measures: support and confidence. By providing minimum thresholds for each of these metrics and applying the Apriori principle, it is easy to drastically limit the number of rules reported. If this limit is too strict, it may cause only the most obvious or common-sense rules to be identified. For this reason, it is important to carefully understand the types of rules that are excluded under these criteria so that the right balance can be obtained.</p>
    <p class="normal">The <strong class="keyWord">support</strong> of an itemset or rule measures how frequently it occurs in the data. For instance, the itemset <em class="italic">{get well card, flowers}</em> has the support of <em class="italic">3 / 5 = 0.6</em> in the hospital gift shop data. Similarly, the support for <em class="italic">{get well card} <img alt="" src="../Images/B17290_08_004.png"/> {flowers}</em> is also <em class="italic">0.6</em>. Support can be calculated for any itemset or even a single item; for instance, the support for <em class="italic">{candy bar}</em> is <em class="italic">2/5 = 0.4</em>, since candy bars appear in 40 percent of purchases. A function defining support for itemset <em class="italic">X</em> could be defined as:</p>
    <p class="center"><img alt="" src="../Images/B17290_08_001.png"/></p>
    <p class="normal">Here, <em class="italic">N</em> is the <a id="_idIndexMarker964"/>number of transactions in the database, and <em class="italic">count(X)</em> is the number of transactions containing itemset <em class="italic">X</em>.</p>
    <p class="normal">A rule’s <strong class="keyWord">confidence</strong> is a measurement of its predictive power or accuracy. It is defined as the support of the itemset containing both <em class="italic">X</em> and <em class="italic">Y</em> divided by the support of the itemset containing only <em class="italic">X</em>:</p>
    <p class="center"><img alt="" src="../Images/B17290_08_002.png"/></p>
    <p class="normal">Essentially, the confidence tells us the proportion of transactions where the presence of item or itemset <em class="italic">X</em> results in the presence of item or itemset <em class="italic">Y</em>. Keep in mind that the confidence that <em class="italic">X</em> leads to <em class="italic">Y</em> is not the same as the confidence that <em class="italic">Y</em> leads to <em class="italic">X</em>. </p>
    <p class="normal">For example, the confidence of <em class="italic">{flowers} <img alt="" src="../Images/B17290_08_004.png"/> {get-well card}</em> is <em class="italic">0.6 / 0.8 = 0.75</em>. In comparison, the confidence of <em class="italic">{get-well card} <img alt="" src="../Images/B17290_08_004.png"/> {flowers}</em> is <em class="italic">0.6 / 0.6 = 1.0</em>. This means that a purchase of flowers also includes the purchase of a get-well card 75 percent of the time, while a purchase of a get-well card also includes flowers 100 percent of the time. This information could be quite useful to the gift shop management.</p>
    <div class="packt_tip">
      <p class="normal">You may have noticed similarities between support, confidence, and the Bayesian probability rules covered in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>. In fact, <em class="italic">support(A, B)</em> is the same as <em class="italic">P(A <img alt="" src="../Images/B17290_08_005.png"/> B)</em> and <em class="italic">confidence(A </em><em class="italic"><img alt="" src="../Images/B17290_08_004.png"/></em><em class="italic"> B)</em> is the same as <em class="italic">P(B | A)</em>. It is just the context that differs.</p>
    </div>
    <p class="normal">Rules like <em class="italic">{get-well card} <img alt="" src="../Images/B17290_08_004.png"/> {flowers}</em> are known as <strong class="keyWord">strong rules</strong> because they have both high support and confidence. One way to find more strong rules would be to examine every possible combination of items in the gift shop, measure the support and confidence, and report back only those rules that meet certain levels of interest. However, as noted before, this strategy is generally not feasible for anything but the smallest of datasets.</p>
    <p class="normal">In the <a id="_idIndexMarker965"/>next section, you will see how the Apriori algorithm uses minimum levels of support and confidence with the Apriori principle to find strong rules quickly by reducing the number of rules to a more manageable level.</p>
    <h2 class="heading-2" id="_idParaDest-200">Building a set of rules with the Apriori principle</h2>
    <p class="normal">Recall that the Apriori principle states that all subsets of a frequent itemset must also be frequent. In other words, if <em class="italic">{A, B}</em> is frequent, then <em class="italic">{A}</em> and <em class="italic">{B}</em> must both be frequent. Recall <a id="_idIndexMarker966"/>also that, by definition, the <a id="_idIndexMarker967"/>support metric indicates how frequently an itemset appears in the data. Therefore, if we know that <em class="italic">{A}</em> does not meet a desired support threshold, there is no reason to consider <em class="italic">{A, B}</em> or any other itemset containing <em class="italic">{A}</em>; these cannot possibly be frequent.</p>
    <p class="normal">The Apriori algorithm uses this logic to exclude potential association rules prior to evaluating them. The process of creating rules then occurs in two phases:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Identifying all the itemsets that meet a minimum support threshold</li>
      <li class="numberedList">Creating rules from these itemsets using those meeting a minimum confidence threshold</li>
    </ol>
    <p class="normal">The first phase occurs in multiple iterations. Each successive iteration involves evaluating the support of a set of increasingly large itemsets. For instance, iteration one involves evaluating the set of 1-item itemsets (1-itemsets), iteration two evaluates the 2-itemsets, and so on. The result of each iteration <em class="italic">i</em> is a set of all the <em class="italic">i</em>-itemsets that meet the minimum support threshold.</p>
    <p class="normal">All the itemsets from iteration <em class="italic">i</em> are combined to generate candidate itemsets for evaluation in iteration <em class="italic">i + 1</em>. But the Apriori principle can eliminate some of them even before the next round begins. If <em class="italic">{A}</em>, <em class="italic">{B}</em>, and <em class="italic">{C}</em> are frequent in iteration one, while <em class="italic">{D}</em> is not frequent, then iteration two will consider only <em class="italic">{A, B}</em>, <em class="italic">{A, C}</em>, and <em class="italic">{B, C}</em>. Thus, the algorithm needs to evaluate only three itemsets rather than the six 2-item itemsets that would have needed to be evaluated if the sets containing <em class="italic">D</em> had not been eliminated <em class="italic">a priori</em>.</p>
    <p class="normal">Continuing this thought, suppose during iteration two it is discovered that <em class="italic">{A, B}</em> and <em class="italic">{B, C}</em> are frequent, but <em class="italic">{A, C}</em> is not. Although iteration three would normally begin by evaluating the support for the 3-item itemset <em class="italic">{A, B, C}</em>, this step is not necessary. Why not? The Apriori principle states that <em class="italic">{A, B, C}</em> cannot possibly be frequent, since the subset <em class="italic">{A, C}</em> is not. Therefore, having generated no new itemsets in iteration three, the algorithm may stop.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_08_02.png"/></figure>
    <p class="packt_figref">Figure 8.2: In this example, the Apriori algorithm only evaluated 7 of the 15 potential itemsets that can occur in transactional data for four items (the 0-item itemset is not shown)</p>
    <p class="normal">At this <a id="_idIndexMarker968"/>point, the second phase of the Apriori <a id="_idIndexMarker969"/>algorithm may begin. Given the set of frequent itemsets, association rules are generated from all possible subsets. For instance, <em class="italic">{A, B}</em> would result in candidate rules for <em class="italic">{A} <img alt="" src="../Images/B17290_08_004.png"/> {B}</em> and <em class="italic">{B} <img alt="" src="../Images/B17290_08_004.png"/> {A}</em>. These are evaluated against a minimum confidence threshold, and any rule that does not meet the desired confidence level is eliminated.</p>
    <h1 class="heading-1" id="_idParaDest-201">Example – identifying frequently purchased groceries with association rules</h1>
    <p class="normal">As noted in this chapter’s introduction, market basket analysis is used behind the scenes for <a id="_idIndexMarker970"/>the recommendation systems used in many brick-and-mortar and online retailers. The learned <a id="_idIndexMarker971"/>association rules indicate the items that are often purchased together. Knowledge of these patterns provides insight into new ways a grocery chain might optimize the inventory, advertise promotions, or organize the physical layout of the store. For instance, if shoppers frequently purchase coffee or orange juice with a breakfast pastry, it may be possible to increase profit by relocating pastries closer to coffee and juice. </p>
    <p class="normal">Similarly, online retailers can use the information for dynamic recommendation engines that suggest items related to those you’ve already viewed or to follow up after a website <a id="_idIndexMarker972"/>visit or online purchase with an email that suggests add-on items in a practice called <strong class="keyWord">active after-marketing</strong>.</p>
    <p class="normal">In <a id="_idIndexMarker973"/>this tutorial, we will <a id="_idIndexMarker974"/>perform a market basket analysis of transactional data from a grocery store. In doing so, we will see how the Apriori algorithm is able to efficiently evaluate a potentially massive set of association rules. The same techniques could also be applied to many other business tasks, from movie recommendations to dating sites to finding dangerous interactions among medications.</p>
    <h2 class="heading-2" id="_idParaDest-202">Step 1 – collecting data</h2>
    <p class="normal">Our market basket analysis will utilize purchase data from one month of operation at a real-world <a id="_idIndexMarker975"/>grocery store. The data contains 9,835 transactions, or about 327 transactions per day (roughly 30 transactions per hour in a 12-hour business day), suggesting that the retailer is not particularly large, nor is it particularly small.</p>
    <div class="note">
      <p class="normal">The dataset used here was adapted from the <code class="inlineCode">Groceries</code> dataset in the <code class="inlineCode">arules</code> R package. For more information, see <em class="italic">Implications of Probabilistic Data Modeling for Mining Association Rules, Hahsler, M., Hornik, K., Reutterer, T., 2005</em>. In <em class="italic">From Data and Information Analysis to Knowledge Engineering, Gaul, W., Vichi, M., Weihs, C., Studies in Classification, Data Analysis, and Knowledge Organization, 2006, pp. 598–605</em>.</p>
    </div>
    <p class="normal">A typical grocery store offers a huge variety of items. There might be five brands of milk, a dozen types of laundry detergent, and three brands of coffee. Given the moderate size of the retailer in this example, we will assume that it is not terribly concerned with finding rules that apply only to a specific brand of milk or detergent, and thus all brand names are removed from the purchases. This reduces the number of grocery items to a more manageable 169 types, using broad categories such as chicken, frozen meals, margarine, and soda.</p>
    <div class="packt_tip">
      <p class="normal">If you hope to identify highly specific association rules—such as whether customers prefer grape or strawberry jelly with their peanut butter— you will need a tremendous amount of transactional data. Large chain retailers use databases of many millions of transactions in order to find associations among particular brands, colors, or flavors of items.</p>
    </div>
    <p class="normal">Do you <a id="_idIndexMarker976"/>have any guesses about which types of items might be purchased together? Will wine and cheese be a common pairing? Bread and butter? Tea and honey? Let’s dig into this data and see if these guesses can be confirmed.</p>
    <h2 class="heading-2" id="_idParaDest-203">Step 2 – exploring and preparing the data</h2>
    <p class="normal">Transactional data is stored in a slightly different format than we have used previously. Most of <a id="_idIndexMarker977"/>our prior analyses utilized data <a id="_idIndexMarker978"/>in a matrix format where rows indicated example instances and columns indicated features. As described in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>, in matrix format, all examples must have exactly the same set of features.</p>
    <p class="normal">In comparison, transactional data is more freeform. As usual, each row in the data specifies a single example—in this case, a transaction. However, rather than having a set number of features, each record comprises a comma-separated list of any number of items, from one to many. In essence, the features may differ from example to example.</p>
    <div class="packt_tip">
      <p class="normal">To follow along with this analysis, download the <code class="inlineCode">groceries.csv</code> file from the Packt Publishing GitHub repository for this chapter and save it in your R working directory.</p>
    </div>
    <p class="normal">The first five rows of the raw <code class="inlineCode">groceries.csv</code> file are as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">   citrus fruit,semi-finished bread,margarine,ready soups
   tropical fruit,yogurt,coffee
   whole milk
   pip fruit,yogurt,cream cheese,meat spreads
   other vegetables,whole milk,condensed milk,long life bakery product
</code></pre>
    <p class="normal">These lines indicate five separate grocery store transactions. The first transaction included four items: <code class="inlineCode">citrus fruit</code>, <code class="inlineCode">semi-finished bread</code>, <code class="inlineCode">margarine</code>, and <code class="inlineCode">ready soups</code>. In comparison, the third transaction included only one item: <code class="inlineCode">whole milk</code>.</p>
    <p class="normal">Suppose we <a id="_idIndexMarker979"/>tried to load the data using the <code class="inlineCode">read.csv()</code> function as we did in prior analyses. R would happily comply and read the <a id="_idIndexMarker980"/>data into a data frame in matrix format as follows:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_08_03.png"/></figure>
    <p class="packt_figref">Figure 8.3: Reading transactional data into R as a data frame will cause problems later on </p>
    <p class="normal">You will notice that R created four columns to store the items in the transactional data: <code class="inlineCode">V1</code>, <code class="inlineCode">V2</code>, <code class="inlineCode">V3</code>, and <code class="inlineCode">V4</code>. Although this may seem reasonable, if we use the data in this form, we will encounter problems later. R chose to create four variables because the first line had exactly four comma-separated values. However, we know that grocery purchases can contain more than four items; in the four-column design, such transactions will be broken across multiple rows in the matrix. We could try to remedy this by putting the transaction with the largest number of items at the top of the file, but this ignores another more problematic issue.</p>
    <p class="normal">By structuring the data this way, R has constructed a set of features that record not just the items in the transactions but also the order they appear. If we imagine our learning algorithm as an attempt to find a relationship among <code class="inlineCode">V1</code>, <code class="inlineCode">V2</code>, <code class="inlineCode">V3</code>, and <code class="inlineCode">V4</code>, then the <code class="inlineCode">whole milk</code> item in <code class="inlineCode">V1</code> might be treated differently than the <code class="inlineCode">whole milk</code> item appearing in <code class="inlineCode">V2</code>. Instead, we need a dataset that does not treat a transaction as a set of positions to be filled (or not filled) with specific items, but rather as a market basket that either contains or does not contain each item.</p>
    <h3 class="heading-3" id="_idParaDest-204">Data preparation – creating a sparse matrix for transaction data</h3>
    <p class="normal">The solution <a id="_idIndexMarker981"/>to this problem utilizes a data <a id="_idIndexMarker982"/>structure called a sparse matrix. You may recall that we used a sparse matrix to process text data in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>. Just as with the preceding dataset, each row in the sparse matrix indicates a transaction. However, the sparse matrix has a column (that is, a feature) for every item that could possibly appear in someone’s shopping bag. Since there are 169 different items in our grocery store data, our sparse matrix will contain 169 columns.</p>
    <p class="normal">Why not just store this as a data frame as we did in most of our prior analyses? The reason is that as additional transactions and items are added, a conventional data structure quickly becomes too large to fit in the available memory. Even with the relatively small transactional dataset used here, the matrix contains nearly 1.7 million cells, most of which contain zeros (hence the name “sparse” matrix—there are very few non-zero values).</p>
    <p class="normal">Since there <a id="_idIndexMarker983"/>is no benefit to storing all these zeros, a sparse matrix does not actually store the full matrix in memory; it only stores the cells that are occupied by an item. This allows the structure to be more memory efficient than an equivalently sized matrix or data frame.</p>
    <p class="normal">In order to create the sparse matrix data structure from transactional data, we can use the functionality provided by the <code class="inlineCode">arules</code> (association rules) package. Install and load the package using the <code class="inlineCode">install.packages("arules")</code> and <code class="inlineCode">library(arules)</code> commands.</p>
    <div class="note">
      <p class="normal">For more information on the <code class="inlineCode">arules</code> package, refer to <em class="italic">arules - A Computational Environment for Mining Association Rules and Frequent Item Sets, Hahsler, M., Gruen, B., Hornik, K., Journal of Statistical Software, 2005, Vol. 14</em>.</p>
    </div>
    <p class="normal">Because we’re loading transactional data, we cannot simply use the <code class="inlineCode">read.csv()</code> function used previously. Instead, <code class="inlineCode">arules</code> provides a <code class="inlineCode">read.transactions()</code> function that is similar to <code class="inlineCode">read.csv()</code> with the exception that it results in a sparse matrix suitable for transactional data. The parameter <code class="inlineCode">sep = ","</code> specifies that items in the input file are separated by a comma. To read the <code class="inlineCode">groceries.csv</code> data into a sparse matrix named <code class="inlineCode">groceries</code>, type the following line:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceries <span class="hljs-operator">&lt;-</span> read.transactions<span class="hljs-punctuation">(</span><span class="hljs-string">"groceries.csv"</span><span class="hljs-punctuation">,</span> sep <span class="hljs-operator">=</span> <span class="hljs-string">","</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">To see some basic information about the <code class="inlineCode">groceries</code> matrix we just created, use the <code class="inlineCode">summary()</code> function on the object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">transactions as itemMatrix in sparse format with
 9835 rows (elements/itemsets/transactions) and
 169 columns (items) and a density of 0.02609146
</code></pre>
    <p class="normal">The first block of information in the output provides a summary of the sparse matrix we created. The output <code class="inlineCode">9835 rows</code> refers to the number of transactions, and <code class="inlineCode">169 columns</code> indicates each of the 169 different items that might appear in someone’s grocery basket. Each cell in the matrix is a <code class="inlineCode">1</code> if the item was purchased for the corresponding transaction, or <code class="inlineCode">0</code> otherwise.</p>
    <p class="normal">The density value of <code class="inlineCode">0.02609146</code> (2.6 percent) refers to the proportion of non-zero matrix cells. Since there are <em class="italic">9,835 * 169 = 1,662,115</em> positions in the matrix, we can calculate that a total of <em class="italic">1,662,115 * 0.02609146 = 43,367</em> items were purchased during the store’s 30 days of operation (ignoring the fact that duplicates of the same items might have been purchased). With an additional step, we can determine that the average transaction contained <em class="italic">43,367 / 9,835 = 4.409</em> distinct grocery items. Of course, if we look <a id="_idIndexMarker984"/>a little further down the output, we’ll see that the mean number of items per transaction has already been provided.</p>
    <p class="normal">The next block of <code class="inlineCode">summary()</code> output lists the items that were most commonly found in the transactional data. Since <em class="italic">2,513 / 9,835 = 0.2555</em>, we can determine that <code class="inlineCode">whole milk</code> appeared in 25.6 percent of transactions. </p>
    <p class="normal"><code class="inlineCode">Other vegetables</code>, <code class="inlineCode">rolls/buns</code>, <code class="inlineCode">soda</code>, and <code class="inlineCode">yogurt</code> round out the list of other common items, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">most frequent items:
      whole milk   other vegetables       rolls/buns
            2513               1903             1809
            soda             yogurt          (Other)
            1715               1372            34055
</code></pre>
    <p class="normal">We are also presented with a set of statistics about the size of the transactions. A total of 2,159 transactions contained only a single item, while one transaction had 32 items. The first quartile and median purchase size are two and three items respectively, implying that 25 percent of the transactions contained two or fewer items and about half contained three items or fewer. The mean of 4.409 items per transaction matches the value we calculated by hand:</p>
    <pre class="programlisting con"><code class="hljs-con">element (itemset/transaction) length distribution:
sizes
   1    2    3    4    5    6    7    8    9   10   11   12 
2159 1643 1299 1005  855  645  545  438  350  246  182  117 
  13   14   15   16   17   18   19   20   21   22   23   24 
  78   77   55   46   29   14   14    9   11    4    6    1 
  26   27   28   29   32 
   1    1    1    3    1
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   2.000   3.000   4.409   6.000  32.000
</code></pre>
    <p class="normal">Finally, the bottom of the output includes additional information about any metadata that may be associated with the item matrix, such as item hierarchies or labels. We have not used <a id="_idIndexMarker985"/>these advanced features, but the output still indicates that the data has labels. The <code class="inlineCode">read.transactions()</code> function added these automatically upon load using the item names in the original CSV file, and the first three labels (in alphabetical order) are shown:</p>
    <pre class="programlisting con"><code class="hljs-con">includes extended item information – examples:
            labels
1 abrasive cleaner
2 artif. Sweetener
3   baby cosmetics
</code></pre>
    <p class="normal">Note that the <code class="inlineCode">arules</code> package represents items internally using numeric item ID numbers with no connection to the item in the real world. By default, most <code class="inlineCode">arules</code> functions will decode these numbers using the item labels. However, to illustrate the numeric IDs, we can examine the first two transactions without decoding using the so-called “long” format. In long-format transactional data, each row is a single item from a single transaction rather than each row representing a single transaction with multiple items. For instance, because the first and second transactions had four and three items, respectively, the long format represents these transactions in seven rows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>toLongFormat<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> decode <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">),</span> n <span class="hljs-operator">=</span> <span class="hljs-number">7</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  TID item
1   1   30
2   1   89
3   1  119
4   1  133
5   2   34
6   2  158
7   2  168
</code></pre>
    <p class="normal">In this representation of the transactional data, the column named <code class="inlineCode">TID</code> refers to the transaction ID—that is, the first or second market basket—and the column named <code class="inlineCode">item</code> refers to the internal ID number assigned to the item. As the first transaction contained <em class="italic">{citrus fruit, margarine, ready soups, and semi-finished bread}</em>, we can assume that the item ID of 30 refers to <em class="italic">citrus fruit</em> and 89 refers to <em class="italic">margarine</em>.</p>
    <p class="normal">The <code class="inlineCode">arules</code> package, of course, includes features for examining transaction data in more intuitive formats. To look at the contents of the sparse matrix, use the <code class="inlineCode">inspect()</code> function in <a id="_idIndexMarker986"/>combination with R’s vector operators. The first five transactions can be viewed as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    items                      
[1] {citrus fruit,             
     margarine,                
     ready soups,              
     semi-finished bread}      
[2] {coffee,                   
     tropical fruit,           
     yogurt}                   
[3] {whole milk}               
[4] {cream cheese,             
     meat spreads,             
     pip fruit,                
     yogurt}                   
[5] {condensed milk,           
     long life bakery product, 
     other vegetables,         
     whole milk}               
</code></pre>
    <p class="normal">When formatted using the <code class="inlineCode">inspect()</code> function, the data does not look very different from what we had seen in the original CSV file. </p>
    <p class="normal">Because the <code class="inlineCode">groceries</code> object is stored as a sparse item matrix, the <code class="inlineCode">[row, column]</code> notation can be used to examine desired items as well as desired transactions. Using this with the <code class="inlineCode">itemFrequency()</code> function allows us to see the proportion of all transactions that contain the specified item. For instance, to view the support level for the first three items across all rows in the grocery data, use the following command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> itemFrequency<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">[,</span> <span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">3</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">abrasive cleaner artif. sweetener   baby cosmetics
    0.0035587189     0.0032536858     0.0006100661
</code></pre>
    <p class="normal">Notice that the items in the sparse matrix are arranged in columns in alphabetical order. Abrasive cleaners and artificial sweeteners are found in about 0.3 percent of the transactions, while baby cosmetics are found in about 0.06 percent of the transactions.</p>
    <h3 class="heading-3" id="_idParaDest-205">Visualizing item support – item frequency plots</h3>
    <p class="normal">To present these statistics visually, use the <code class="inlineCode">itemFrequencyPlot()</code> function. This creates a bar chart <a id="_idIndexMarker987"/>depicting the proportion of transactions containing specified items. Since transactional data contains a very large number of items, you will often need to limit those appearing in the plot in order to produce a legible chart.</p>
    <p class="normal">If you would like to display items that appear in a minimum proportion of transactions, use <code class="inlineCode">itemFrequencyPlot()</code> with the <code class="inlineCode">support</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> itemFrequencyPlot<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> support <span class="hljs-operator">=</span> <span class="hljs-number">0.1</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">As shown in the following plot, this results in a histogram showing the eight items in the <code class="inlineCode">groceries</code> data with at least 10 percent support:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_08_04.png"/></figure>
    <p class="packt_figref">Figure 8.4: Support levels for all grocery items in at least 10 percent of transactions</p>
    <p class="normal">If you would rather limit the plot to a specific number of items, use the function with the <code class="inlineCode">topN</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> itemFrequencyPlot<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> topN <span class="hljs-operator">=</span> <span class="hljs-number">20</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The histogram is then sorted by decreasing support, as shown in the following diagram for the <a id="_idIndexMarker988"/>top 20 items in the <code class="inlineCode">groceries</code> data:</p>
    <figure class="mediaobject"><img alt="A picture containing chart  Description automatically generated" src="../Images/B17290_08_05.png"/></figure>
    <p class="packt_figref">Figure 8.5: Support levels for the top 20 grocery items</p>
    <h3 class="heading-3" id="_idParaDest-206">Visualizing the transaction data – plotting the sparse matrix</h3>
    <p class="normal">In addition to looking at specific items, it’s also possible to obtain a bird’s-eye view of the entire <a id="_idIndexMarker989"/>sparse matrix using the <code class="inlineCode">image()</code> function. Of course, because the matrix itself is very large, it is usually best to request a subset of the entire matrix. The command to display the sparse matrix for the first five transactions is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> image<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">])</span>
</code></pre>
    <p class="normal">The resulting diagram depicts a matrix with 5 rows and 169 columns, indicating the 5 transactions and 169 possible items we requested. Cells in the matrix are filled with black for transactions (rows) where the item (column) was purchased.</p>
    <figure class="mediaobject"><img alt="A picture containing chart  Description automatically generated" src="../Images/B17290_08_06.png"/></figure>
    <p class="packt_figref">Figure 8.6: A visualization of the sparse matrix for the first five transactions</p>
    <p class="normal">Although <em class="italic">Figure 8.6</em> is small and may be slightly hard to read, you can see that the first, fourth, and fifth transactions contained four items each, since their rows have four cells filled in. On the right side of the diagram, you can also see that rows three and five, and rows two and four, share an item in common.</p>
    <p class="normal">This visualization can be a useful tool for exploring transactional data. For one, it may help with the identification of potential data issues. Columns that are filled all the way down could indicate items that are purchased in every transaction—a problem that could arise, perhaps, if a retailer’s name or identification number was inadvertently included in the transaction dataset.</p>
    <p class="normal">Additionally, patterns in the diagram may help reveal interesting segments of transactions and items, particularly if the data is sorted in interesting ways. For example, if the transactions are sorted by date, patterns in the black dots could reveal seasonal effects in the <a id="_idIndexMarker990"/>number or types of items purchased. Perhaps around Christmas or Hanukkah, toys are more common; around Halloween, perhaps candies become popular. This type of visualization could be especially powerful if the items were also sorted into categories. In most cases, however, the plot will look fairly random, like static on a television screen.</p>
    <p class="normal">Keep in mind that this visualization will not be as useful for extremely large transaction databases because the cells will be too small to discern. Still, by combining it with the <code class="inlineCode">sample()</code> function, you can view the sparse matrix for a randomly sampled set of transactions. The command to create a random selection of 100 transactions is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> image<span class="hljs-punctuation">(</span>sample<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> <span class="hljs-number">100</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">This creates a matrix diagram with 100 rows and 169 columns:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_08_07.png"/></figure>
    <p class="packt_figref">Figure 8.7: A visualization of the sparse matrix for 100 randomly selected transactions</p>
    <p class="normal">A few columns <a id="_idIndexMarker991"/>seem fairly heavily populated, indicating some very popular items at the store. However, the distribution of dots seems fairly random overall. Given nothing else of note, let’s continue with our analysis.</p>
    <h2 class="heading-2" id="_idParaDest-207">Step 3 – training a model on the data</h2>
    <p class="normal">With data preparation complete, we can now work at finding associations among shopping <a id="_idIndexMarker992"/>cart items. We will use an implementation of the Apriori algorithm in the <code class="inlineCode">arules</code> package we’ve been using for exploring and preparing the <code class="inlineCode">groceries</code> data. You’ll need to install and load this package if you have not done so already. </p>
    <p class="normal">The following table shows the syntax for creating sets of rules with the <code class="inlineCode">apriori()</code> function:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_08_08.png"/></figure>
    <p class="packt_figref">Figure 8.8: Apriori association rule learning syntax</p>
    <p class="normal">Although running the <code class="inlineCode">apriori()</code> function is straightforward, there can sometimes be a fair amount <a id="_idIndexMarker993"/>of trial and error needed to find the <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code> parameters that produce a reasonable number of association rules. If you set these levels too high, then you might find no rules, or might find rules that are too generic to be very useful. On the other hand, a threshold too low might result in an unwieldy number of rules. Worse, the operation might take a very long time or run out of memory during the learning phase.</p>
    <p class="normal">On the <code class="inlineCode">groceries</code> data, using the default settings of <code class="inlineCode">support = 0.1</code> and <code class="inlineCode">confidence = 0.8</code> leads to a disappointing outcome. Although the full output has been omitted for brevity, the end result is a set of zero rules:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> apriori<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">...
set of 0 rules
</code></pre>
    <p class="normal">Obviously, we need to widen the search a bit.</p>
    <div class="packt_tip">
      <p class="normal">If you think about it, this outcome should not have been terribly surprising. Because <code class="inlineCode">support = 0.1</code> by default, in order to generate a rule, an item must have appeared in at least <em class="italic">0.1 * 9,385 = 938.5</em> transactions. Since only eight items appeared this frequently in our data, it’s no wonder we didn’t find any rules.</p>
    </div>
    <p class="normal">One way to approach the problem of setting a minimum support is to think about the smallest <a id="_idIndexMarker994"/>number of transactions needed before a stakeholder would consider a pattern interesting. For instance, one could argue that if an item is purchased twice a day (about 60 times in a month of data) then it may be important. From there, it is possible to calculate the support level needed to find only rules matching at least that many transactions. Since 60 out of 9,835 equals approximately 0.006, we’ll try setting the support there first.</p>
    <p class="normal">Setting the minimum confidence involves a delicate balance. On the one hand, if the confidence is too low, then we might be overwhelmed with many unreliable rules—such as dozens of rules indicating items purchased together frequently by chance alone, like bread and batteries. How would we know where to target our advertising budget then? On the other hand, if we set the confidence too high, then we will be limited to rules that are obvious or inevitable—like the fact that a smoke detector is always purchased in combination with batteries. In this case, moving the smoke detectors closer to the batteries is unlikely to generate additional revenue, since the two items were already almost always purchased together.</p>
    <div class="packt_tip">
      <p class="normal">The appropriate minimum confidence level depends a great deal on the goals of your analysis. If you start with a conservative value, you can always reduce it to broaden the search if you aren’t finding actionable intelligence.</p>
    </div>
    <p class="normal">We’ll start with a confidence threshold of 0.25, which means that in order to be included in the results, the rule must be correct at least 25 percent of the time. This will eliminate the most unreliable rules while allowing some room for us to modify behavior with targeted promotions.</p>
    <p class="normal">Now we’re ready to generate some rules. In addition to the minimum <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code> parameters, it is helpful to set <code class="inlineCode">minlen = 2</code> to eliminate rules that contain fewer than two items. This prevents uninteresting rules from being created simply because the item is purchased frequently, for instance, <code class="inlineCode">{} =&gt; whole milk</code>. This rule meets the <a id="_idIndexMarker995"/>minimum support and confidence because whole milk is purchased in over 25 percent of transactions, but it isn’t a very actionable insight.</p>
    <p class="normal">The full command for finding a set of association rules using the Apriori algorithm is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryrules <span class="hljs-operator">&lt;-</span> apriori<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> parameter <span class="hljs-operator">=</span> <span class="hljs-built_in">list</span><span class="hljs-punctuation">(</span>support <span class="hljs-operator">=</span>
                            <span class="hljs-number">0.006</span><span class="hljs-punctuation">,</span> confidence <span class="hljs-operator">=</span> <span class="hljs-number">0.25</span><span class="hljs-punctuation">,</span> minlen <span class="hljs-operator">=</span> <span class="hljs-number">2</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">The first few lines of output describe the parameter settings we specified, as well as several others that remained set to their defaults; for definitions of these, use the <code class="inlineCode">?APparameter</code> help command. The second set of lines shows behind-the-scenes algorithmic control parameters that may be helpful for much larger datasets, as they control computing tradeoffs like optimizing for speed or memory use. For information on these parameters, use the <code class="inlineCode">?APcontrol</code> help command:</p>
    <pre class="programlisting con"><code class="hljs-con">Apriori
Parameter specification:
 confidence minval smax arem  aval originalSupport maxtime support
       0.25    0.1    1 none FALSE            TRUE       5   0.006
 minlen maxlen target  ext
      2     10  rules TRUE
Algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 TRUE TRUE  FALSE TRUE    2    TRUE
</code></pre>
    <p class="normal">Next, the output includes information about the steps in the Apriori algorithm execution:</p>
    <pre class="programlisting con"><code class="hljs-con">Absolute minimum support count: 59 
set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
sorting and recoding items ... [109 item(s)] done [0.00s].
creating transaction tree ... done [0.00s].
checking subsets of size 1 2 3 4 done [0.00s].
writing ... [463 rule(s)] done [0.00s].
creating S4 object  ... done [0.00s].
</code></pre>
    <p class="normal">Given the small size of the transactional dataset, most of the rows show steps that took virtually no time to run—denoted as <code class="inlineCode">[0.00s]</code> in the output here, but your output may vary slightly depending on computer capability. </p>
    <p class="normal">The <code class="inlineCode">Absolute minimum support count</code> refers to the smallest count of transactions that would meet the support threshold of 0.006 we specified. Since <em class="italic">0.006 * 9,835 = 59.01</em>, the algorithm <a id="_idIndexMarker996"/>requires items to appear in a minimum of 59 transactions. The <code class="inlineCode">checking subsets of size 1 2 3 4</code> output suggests that the algorithm tested <em class="italic">i</em>-itemsets of 1, 2, 3, and 4 items before stopping the iteration process and writing the final set of 463 rules.</p>
    <p class="normal">The end result of the <code class="inlineCode">apriori()</code> function is a rules object, which we can peek into by typing its name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryrules
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">set of 463 rules
</code></pre>
    <p class="normal">Our <code class="inlineCode">groceryrules</code> object contains quite a large set of association rules! To determine whether any of them are useful, we’ll have to dig deeper.</p>
    <h2 class="heading-2" id="_idParaDest-208">Step 4 – evaluating model performance</h2>
    <p class="normal">To obtain a high-level overview of the association rules, we can use <code class="inlineCode">summary()</code> as follows. The rule length distribution tells us how many rules have each count of items. In our <a id="_idIndexMarker997"/>rule set, 150 rules have only two items, while 297 have three, and 16 have four. The summary statistics associated with this distribution are also provided in the first few lines of output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">set of 463 rules
rule length distribution (lhs + rhs):sizes
  2   3   4 
150 297  16 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.000   2.000   3.000   2.711   3.000   4.000
</code></pre>
    <div class="packt_tip">
      <p class="normal">As noted in the previous output, the size of the rule is calculated as the total of both the left-hand side (<code class="inlineCode">lhs</code>) and right-hand side (<code class="inlineCode">rhs</code>) of the rule. This means that a rule like <code class="inlineCode">{bread} =&gt; {butter}</code> is two items and <code class="inlineCode">{peanut butter, jelly} =&gt; {bread}</code> is three.</p>
    </div>
    <p class="normal">Next, we <a id="_idIndexMarker998"/>see the summary statistics of the rule quality measures, including <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code>, as well as <code class="inlineCode">coverage</code>, <code class="inlineCode">lift</code>, and <code class="inlineCode">count</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">summary of quality measures:
    support           confidence        coverage       
 Min.   :0.006101   Min.   :0.2500   Min.   :0.009964  
 1st Qu.:0.007117   1st Qu.:0.2971   1st Qu.:0.018709  
 Median :0.008744   Median :0.3554   Median :0.024809  
 Mean   :0.011539   Mean   :0.3786   Mean   :0.032608  
 3rd Qu.:0.012303   3rd Qu.:0.4495   3rd Qu.:0.035892  
 Max.   :0.074835   Max.   :0.6600   Max.   :0.255516  
      lift            count      
 Min.   :0.9932   Min.   : 60.0  
 1st Qu.:1.6229   1st Qu.: 70.0  
 Median :1.9332   Median : 86.0  
 Mean   :2.0351   Mean   :113.5  
 3rd Qu.:2.3565   3rd Qu.:121.0  
 Max.   :3.9565   Max.   :736.0
</code></pre>
    <p class="normal">The <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code> measures should not be very surprising, since we used these as selection criteria for the rules. We might be alarmed if most or all of the rules had <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code> very near the minimum thresholds, as this would mean that we may have set the bar too high. This is not the case here, as there are many rules with much higher values of each.</p>
    <p class="normal">The <code class="inlineCode">count</code> and <code class="inlineCode">coverage</code> measures are closely related to <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code>. As defined here, <strong class="keyWord">count</strong> is simply the numerator of the support metric or the number (rather than proportion) of transactions that contained the item. Because the absolute minimum support count was 59, it is unsurprising that the minimum observed count of 60 is close to the parameter setting. The maximum count of 736 suggests that an item appeared in 736 out of 9,835 transactions; this relates to the maximum observed support as <em class="italic">736 / 9,835 = 0.074835</em>.</p>
    <p class="normal">The <strong class="keyWord">coverage</strong> of an association rule is simply the support of the left-hand side of the rule, but it has a useful real-world interpretation: it can be understood as the chance that a rule applies to any given transaction in the dataset, selected at random. Thus, the minimum <code class="inlineCode">coverage</code> of 0.009964 suggests that the least applicable rule covers only about one percent of transactions; the maximum <code class="inlineCode">coverage</code> of 0.255516 suggests that at least one rule covers more than 25 percent of transactions. Surely, this rule involves <code class="inlineCode">whole milk</code>, as it is the only item that appears in so many transactions.</p>
    <p class="normal">The final <a id="_idIndexMarker999"/>column is a metric we have not considered yet. The <strong class="keyWord">lift</strong> of a rule measures how much more likely one item or itemset is to be purchased relative to its typical rate of purchase, given that you know another item or itemset has been purchased. This is defined by the following equation:</p>
    <p class="center"><img alt="" src="../Images/B17290_08_003.png"/></p>
    <div class="packt_tip">
      <p class="normal">Unlike confidence, where the item order matters, <em class="italic">lift(X </em><em class="italic"><img alt="" src="../Images/B17290_08_004.png"/></em><em class="italic"> Y)</em> is the same as <em class="italic">lift(Y </em><em class="italic"><img alt="" src="../Images/B17290_08_004.png"/></em><em class="italic"> X)</em>.</p>
    </div>
    <p class="normal">For example, suppose at a grocery store, most people purchase milk and bread. By chance alone, we would expect to find many transactions with both milk and bread. However, if <em class="italic">lift(milk </em><em class="italic"><img alt="" src="../Images/B17290_08_004.png"/></em><em class="italic"> bread)</em> is greater than 1, this implies that the two items are found together more often than expected by chance alone. In other words, someone who purchases one of the items is more likely to purchase the other. A large lift value is therefore a strong indicator that a rule is important and reflects a true connection between the items, and that the rule will be useful for business purposes. Keep in mind, however, that this is only the case for sufficiently large transactional datasets; lift values can be exaggerated for items with low support.</p>
    <div class="note">
      <p class="normal">A pair of <a id="_idIndexMarker1000"/>authors from the <code class="inlineCode">apriori</code> package have proposed new <a id="_idIndexMarker1001"/>metrics called <strong class="keyWord">hyper-lift</strong> and <strong class="keyWord">hyper-confidence</strong> to address the shortcomings of these measures for data with rare items. For more information, see <em class="italic">M. Hahsler and K. Hornik, New Probabilistic Interest Measures for Association Rules (2018). </em><a href="https://arxiv.org/pdf/0803.0966.pdf"><span class="url">https://arxiv.org/pdf/0803.0966.pdf</span></a>.</p>
    </div>
    <p class="normal">In the final section of the <code class="inlineCode">summary()</code> output, we receive mining information, telling us about how the rules were chosen. Here, we see that the <code class="inlineCode">groceries</code> data, which contained 9,835 transactions, was used to construct rules with a minimum support of 0.006 and minimum confidence of 0.25:</p>
    <pre class="programlisting con"><code class="hljs-con">mining info:
      data  transactions support confidence
 groceries          9835   0.006       0.25
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1002"/>take a look at specific rules using the <code class="inlineCode">inspect()</code> function. For instance, the first three rules in the <code class="inlineCode">groceryrules</code> object can be viewed as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">3</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    lhs                rhs               support    
[1] {potted plants} =&gt; {whole milk}      0.006914082
[2] {pasta}         =&gt; {whole milk}      0.006100661
[3] {herbs}         =&gt; {root vegetables} 0.007015760
    confidence coverage   lift     count
[1] 0.4000000  0.01728521 1.565460 68   
[2] 0.4054054  0.01504830 1.586614 60   
[3] 0.4312500  0.01626843 3.956477 69   
</code></pre>
    <p class="normal">The first rule can be read in plain language as “if a customer buys potted plants, they will also buy whole milk.” With a support of about 0.007 and a confidence of 0.400, we can determine that this rule covers about 0.7 percent of transactions and is correct in 40 percent of purchases involving potted plants. The lift value tells us how much more likely a customer is to buy whole milk relative to the average customer, given that they bought a potted plant. Since we know that about 25.6 percent of customers bought whole milk (<code class="inlineCode">support</code>), while 40 percent of customers buying a potted plant bought whole milk (<code class="inlineCode">confidence</code>), we can compute the lift as <em class="italic">0.40 / 0.256 = 1.56</em>, which matches the value shown.</p>
    <div class="packt_tip">
      <p class="normal">Note that the column labeled <code class="inlineCode">support</code> indicates the support for the rule, not the support for the <code class="inlineCode">lhs</code> or <code class="inlineCode">rhs</code> alone. The column labeled <code class="inlineCode">coverage</code> is the support for the left-hand side.</p>
    </div>
    <p class="normal">Although the confidence and lift are high, does <em class="italic">{potted plants} <img alt="" src="../Images/B17290_08_004.png"/> {whole milk}</em> seem like a very useful rule? Probably not, as there doesn’t seem to be a logical reason why someone would be more likely to buy milk with a potted plant. Yet our data suggests otherwise. How can we make sense of this fact?</p>
    <p class="normal">A common approach is to take the association rules and divide them into the following three categories:</p>
    <ul>
      <li class="bulletList">Actionable</li>
      <li class="bulletList">Trivial</li>
      <li class="bulletList">Inexplicable</li>
    </ul>
    <p class="normal">Obviously, the goal of a market <a id="_idIndexMarker1003"/>basket analysis is to find <strong class="keyWord">actionable</strong> rules that provide a clear and interesting insight. Some rules are clear and others <a id="_idIndexMarker1004"/>are interesting; it is less common to find a combination of both of these factors.</p>
    <p class="normal">So-called <strong class="keyWord">trivial</strong> rules include <a id="_idIndexMarker1005"/>any rules that are so obvious that they are not worth mentioning—they are clear, but not interesting. Suppose you are a marketing consultant being paid large sums of money to identify new opportunities for cross-promoting items. If you report the finding that <em class="italic">{diapers} <img alt="" src="../Images/B17290_08_004.png"/> {formula}</em>, you probably won’t be invited back for another consulting job.</p>
    <div class="packt_tip">
      <p class="normal">Trivial rules can also sneak in disguised as more interesting results. For instance, say you found an association between a particular brand of children’s cereal and a popular animated movie. This finding is not very insightful if the movie’s main character is on the front of the cereal box.</p>
    </div>
    <p class="normal">Rules are <strong class="keyWord">inexplicable</strong> if the connection between the items is so unclear that figuring out how to use the information is impossible or nearly impossible. The rule may simply be a random pattern in the data, for instance, a rule stating that <em class="italic">{pickles} <img alt="" src="../Images/B17290_08_004.png"/> {chocolate ice cream}</em> may be due to a single customer whose pregnant wife had regular cravings for strange combinations of foods.</p>
    <p class="normal">The best rules are the hidden gems—the undiscovered insights that only seem obvious once discovered. Given enough time, one could evaluate each and every rule to find the gems. However, the data scientists working on the analysis may not be the best judge of whether a rule is actionable, trivial, or inexplicable. Consequently, better rules are likely to arise <a id="_idIndexMarker1006"/>via collaboration with the domain experts responsible for managing the retail chain, who can help interpret the findings. In the next section, we’ll facilitate such sharing by employing methods for sorting and exporting the learned rules so that the most interesting results float to the top.</p>
    <h2 class="heading-2" id="_idParaDest-209">Step 5 – improving model performance</h2>
    <p class="normal">Subject matter experts may be able to identify useful rules very quickly, but it would be a poor use <a id="_idIndexMarker1007"/>of their time to ask them to evaluate hundreds or thousands of rules. Therefore, it’s useful to be able to sort the rules according to different criteria and get them out of R in a form that can be shared with marketing teams and examined in more depth. In this way, we can improve the performance of our rules by making the results more actionable.</p>
    <p class="normal">If you are running into memory limitations or if Apriori is taking too long to run, it may also be possible to improve the computational performance of the association rule mining process itself by using a more recent algorithm.</p>
    <h3 class="heading-3" id="_idParaDest-210">Sorting the set of association rules</h3>
    <p class="normal">Depending upon the objectives of the market basket analysis, the most useful rules might be <a id="_idIndexMarker1008"/>those with the highest support, confidence, or lift. The <code class="inlineCode">arules</code> package works with the R <code class="inlineCode">sort()</code> function to allow reordering of the list of rules so that those with the highest or lowest values of the quality measure come first.</p>
    <p class="normal">To reorder the <code class="inlineCode">groceryrules</code> object, we can <code class="inlineCode">sort()</code> while specifying a value of <code class="inlineCode">"support"</code>, <code class="inlineCode">"confidence"</code>, or <code class="inlineCode">"lift"</code> to the <code class="inlineCode">by</code> parameter. By combining the sort with vector operators, we can obtain a specific number of interesting rules. For instance, the best five rules according to the <code class="inlineCode">lift</code> statistic can be examined using the following command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>sort<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">,</span> by <span class="hljs-operator">=</span> <span class="hljs-string">"lift"</span><span class="hljs-punctuation">)[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">])</span>
</code></pre>
    <p class="normal">The output is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">    lhs                    rhs                      support
[1] {herbs}             =&gt; {root vegetables}    0.007015760
[2] {berries}           =&gt; {whipped/sour cream} 0.009049314
[3] {other vegetables,                                     
     tropical fruit,                                       
     whole milk}        =&gt; {root vegetables}    0.007015760
[4] {beef,                                                 
     other vegetables}  =&gt; {root vegetables}    0.007930859
[5] {other vegetables,                                     
     tropical fruit}    =&gt; {pip fruit}          0.009456024
    confidence coverage   lift     count
[1] 0.4312500  0.01626843 3.956477 69
[2] 0.2721713  0.03324860 3.796886 89
[3] 0.4107143  0.01708185 3.768074 69
[4] 0.4020619  0.01972547 3.688692 78
[5] 0.2634561  0.03589222 3.482649 93
</code></pre>
    <p class="normal">These rules appear to be more interesting than the ones we looked at previously. The first rule, with a <code class="inlineCode">lift</code> of about 3.96, implies that people who buy herbs are nearly four times <a id="_idIndexMarker1009"/>more likely to buy root vegetables than the typical customer— perhaps for a stew of some sort. Rule two is also interesting. Whipped cream is over three times more likely to be found in a shopping cart with berries versus other carts, suggesting perhaps a dessert pairing.</p>
    <div class="packt_tip">
      <p class="normal">By default, the sort order is decreasing, meaning the largest values come first. To reverse this order, add an additional parameter, <code class="inlineCode">decreasing = FALSE</code>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-211">Taking subsets of association rules</h3>
    <p class="normal">Suppose that, given the preceding rule, the marketing team is excited about the possibility <a id="_idIndexMarker1010"/>of creating an advertisement to promote berries, which are now in season. Before finalizing the campaign, however, they ask you to investigate whether berries are often purchased with other items. To answer this question, we’ll need to find all the rules that include berries in some form.</p>
    <p class="normal">The <code class="inlineCode">subset()</code> function provides a method for searching for subsets of transactions, items, or rules. To use it to find any rules with berries appearing in the rule, use the following command. This will store the rules in a new object named <code class="inlineCode">berryrules</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> berryrules <span class="hljs-operator">&lt;-</span> subset<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">,</span> items <span class="hljs-operator">%in%</span> <span class="hljs-string">"berries"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">We can then inspect the rules as we had done with the larger set:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>berryrules<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The result is the following set of rules:</p>
    <pre class="programlisting con"><code class="hljs-con">    lhs          rhs                  support    
[1] {berries} =&gt; {whipped/sour cream} 0.009049314
[2] {berries} =&gt; {yogurt}             0.010574479
[3] {berries} =&gt; {other vegetables}   0.010269446
[4] {berries} =&gt; {whole milk}         0.011794611
    confidence coverage  lift     count
[1] 0.2721713  0.0332486 3.796886  89  
[2] 0.3180428  0.0332486 2.279848 104  
[3] 0.3088685  0.0332486 1.596280 101  
[4] 0.3547401  0.0332486 1.388328 116  
</code></pre>
    <p class="normal">There are <a id="_idIndexMarker1011"/>four rules involving berries, two of which seem to be interesting enough to be called actionable. In addition to whipped cream, berries are also purchased frequently with yogurt—a pairing that could serve well for breakfast or lunch, as well as dessert.</p>
    <p class="normal">The <code class="inlineCode">subset()</code> function is <a id="_idIndexMarker1012"/>very powerful. The criteria for choosing <a id="_idIndexMarker1013"/>the subset can be defined with several keywords and operators:</p>
    <ul>
      <li class="bulletList">The keyword <code class="inlineCode">items</code>, explained previously, matches an item appearing anywhere in the rule. To limit the subset to where the match occurs only on the left-hand side or right-hand side, use <code class="inlineCode">lhs</code> or <code class="inlineCode">rhs</code> instead.</li>
      <li class="bulletList">The operator <code class="inlineCode">%in%</code> means that at least one of the items must be found in the list you defined. If you wanted any rules matching either <code class="inlineCode">berries</code> or <code class="inlineCode">yogurt</code>, you could write <code class="inlineCode">items %in% c("berries", "yogurt")</code>.</li>
      <li class="bulletList">Additional operators are available for partial matching (<code class="inlineCode">%pin%</code>) and complete matching (<code class="inlineCode">%ain%</code>). Partial matching allows you to find both citrus fruit and tropical fruit using one search: <code class="inlineCode">items %pin% "fruit"</code>. Complete matching requires that all listed items are present. For instance, <code class="inlineCode">items %ain% c("berries", "yogurt")</code> finds only rules with both <code class="inlineCode">berries</code> and <code class="inlineCode">yogurt</code>.</li>
      <li class="bulletList">Subsets can also be limited by <code class="inlineCode">support</code>, <code class="inlineCode">confidence</code>, or <code class="inlineCode">lift</code>. For instance, <code class="inlineCode">confidence &gt; 0.50</code> would limit the rules to those with confidence greater than 50 percent.</li>
      <li class="bulletList">Matching criteria can be combined with standard R logical operators such as AND (<code class="inlineCode">&amp;</code>), OR (<code class="inlineCode">|</code>), and NOT (<code class="inlineCode">!</code>).</li>
    </ul>
    <p class="normal">Using these <a id="_idIndexMarker1014"/>options, you can limit the selection of rules to be as specific or general as you would like.</p>
    <h3 class="heading-3" id="_idParaDest-212">Saving association rules to a file or data frame</h3>
    <p class="normal">To share <a id="_idIndexMarker1015"/>the results of your market basket analysis, you can save the rules to a CSV file with the <code class="inlineCode">write()</code> function. This will produce a CSV file that can be used in most spreadsheet programs, including Microsoft Excel:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> write<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">,</span> file <span class="hljs-operator">=</span> <span class="hljs-string">"groceryrules.csv"</span><span class="hljs-punctuation">,</span>
          sep <span class="hljs-operator">=</span> <span class="hljs-string">","</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">quote</span> <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">,</span> row.names <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Sometimes it is also convenient to convert the rules into an R data frame. This can be accomplished using the <code class="inlineCode">as()</code> function, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryrules_df <span class="hljs-operator">&lt;-</span> as<span class="hljs-punctuation">(</span>groceryrules<span class="hljs-punctuation">,</span> <span class="hljs-string">"data.frame"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This creates a data frame with the rules in character format, and numeric vectors for support, confidence, coverage, lift, and count:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>groceryrules_df<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':	463 obs. of  6 variables:
 $ rules     : chr  "{potted plants} =&gt; {whole milk}"   "{pasta} =&gt; {whole milk}" "{herbs} =&gt; {root vegetables}"   "{herbs} =&gt; {other vegetables}" ...
 $ support   : num  0.00691 0.0061 0.00702 0.00773 0.00773 ...
 $ confidence: num  0.4 0.405 0.431 0.475 0.475 ...
 $ coverage  : num  0.0173 0.015 0.0163 0.0163 0.0163 ...
 $ lift      : num  1.57 1.59 3.96 2.45 1.86 ...
 $ count     : int  68 60 69 76 76 69 70 67 63 88 ...
</code></pre>
    <p class="normal">Saving the rules to a data frame may be useful if you want to perform additional processing on the rules or need to export them to another database.</p>
    <h3 class="heading-3" id="_idParaDest-213">Using the Eclat algorithm for greater efficiency</h3>
    <p class="normal">The <strong class="keyWord">Eclat algorithm</strong>, which is <a id="_idIndexMarker1016"/>named for its use of “equivalence class <a id="_idIndexMarker1017"/>itemset clustering and bottom-up lattice traversal” methods, is a slightly more modern and substantially faster association rule learning algorithm. While the implementation details are outside the scope of this book, it can be understood as a close relative of Apriori; it too assumes all subsets of frequent itemsets are also frequent. However, Eclat is able to search even fewer subsets by utilizing clever tricks that provide shortcuts to identify the potentially maximal frequent itemsets and search only these itemsets’ subsets. Where Apriori is a form of a breadth-first algorithm because it searches wide before it searches deep, Eclat is considered a depth-first algorithm in that it dives to the final endpoint and searches only as wide as needed. For some use cases, this can lead to a performance gain of an order of magnitude and less memory consumed.</p>
    <div class="note">
      <p class="normal">For more information on Eclat, refer to <em class="italic">New Algorithms for Fast Discovery of Association Rules, Zaki, M. J., Parthasarathy, S., Ogihara, M., Li, W., KDD-97 Proceedings, 1997</em>.</p>
    </div>
    <p class="normal">A key tradeoff with Eclat’s fast searching is that it skips the phase in Apriori in which confidence is calculated. It assumes that once the itemsets with high support are obtained, the most useful associations can be identified later—whether manually via a subjective eyeball test, or via another round of processing to compute metrics like confidence and lift. This being said, the <code class="inlineCode">arules</code> package makes it just as easy to apply Eclat as Apriori, despite the additional step in the process.</p>
    <p class="normal">We begin with the <code class="inlineCode">eclat()</code> function and setting the <code class="inlineCode">support</code> parameter to 0.006 as before; however, note that the confidence is not set at this stage:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryitemsets_eclat <span class="hljs-operator">&lt;-</span> eclat<span class="hljs-punctuation">(</span>groceries<span class="hljs-punctuation">,</span> support <span class="hljs-operator">=</span> <span class="hljs-number">0.006</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Some of the output is omitted here, but the last few lines are similar to what we obtained from the <code class="inlineCode">apriori()</code> function, with the key exception that 747 itemsets were written rather than 463 rules:</p>
    <pre class="programlisting con"><code class="hljs-con">Absolute minimum support count: 59 
create itemset … 
set transactions …[169 item(s), 9835 transaction(s)] done [0.00s].
sorting and recoding items … [109 item(s)] done [0.00s].
creating sparse bit matrix … [109 row(s), 9835 column(s)] done [0.00s].
writing  … [747 set(s)] done [0.02s].
Creating S4 object  … done [0.00s].
</code></pre>
    <p class="normal">The resulting Eclat itemset object can be used with the <code class="inlineCode">inspect()</code> function as we did with <a id="_idIndexMarker1018"/>the Apriori rules object. The following command shows the first five itemsets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>groceryitemsets_eclat<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    items                       support     count
[1] {potted plants, whole milk} 0.006914082 68   
[2] {pasta, whole milk}         0.006100661 60   
[3] {herbs, whole milk}         0.007727504 76   
[4] {herbs, other vegetables}   0.007727504 76   
[5] {herbs, root vegetables}    0.007015760 69   
</code></pre>
    <p class="normal">To produce rules from the itemsets, use the <code class="inlineCode">ruleInduction()</code> function with the desired <code class="inlineCode">confidence</code> parameter value as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryrules_eclat <span class="hljs-operator">&lt;-</span> ruleInduction<span class="hljs-punctuation">(</span>groceryitemsets_eclat<span class="hljs-punctuation">,</span>
    confidence <span class="hljs-operator">=</span> <span class="hljs-number">0.25</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">With <code class="inlineCode">support</code> and <code class="inlineCode">confidence</code> set to the earlier values of 0.006 and 0.25, respectively, it is no surprise that the Eclat algorithm produced the same set of 463 rules as Apriori:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> groceryrules_eclat
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">set of 463 rules
</code></pre>
    <p class="normal">The resulting rules object can be inspected just as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> inspect<span class="hljs-punctuation">(</span>groceryrules_eclat<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    lhs                rhs                support    
[1] {potted plants} =&gt; {whole milk}       0.006914082
[2] {pasta}         =&gt; {whole milk}       0.006100661
[3] {herbs}         =&gt; {whole milk}       0.007727504
[4] {herbs}         =&gt; {other vegetables} 0.007727504
[5] {herbs}         =&gt; {root vegetables}  0.007015760
    confidence lift    
[1] 0.4000000  1.565460
[2] 0.4054054  1.586614
[3] 0.4750000  1.858983
[4] 0.4750000  2.454874
[5] 0.4312500  3.956477
</code></pre>
    <p class="normal">Given <a id="_idIndexMarker1019"/>the ease of use with either method, if you have a very large transactional dataset, it may be worth testing Eclat and Apriori on smaller random samples of transactions to see if one outperforms the other.</p>
    <h1 class="heading-1" id="_idParaDest-214">Summary</h1>
    <p class="normal">Association rules are used to find insight into the massive transaction databases of large retailers. As an unsupervised learning process, association rule learners are capable of extracting knowledge from large databases without any prior knowledge of what patterns to seek. The catch is that it takes some effort to reduce the wealth of information into a smaller and more manageable set of results. The Apriori algorithm, which we studied in this chapter, does so by setting minimum thresholds of interestingness and reporting only the associations meeting these criteria.</p>
    <p class="normal">We put the Apriori algorithm to work while performing a market basket analysis for a month’s worth of transactions at a modestly sized supermarket. Even in this small example, a wealth of associations was identified. Among these, we noted several patterns that may be useful for future marketing campaigns. The same methods we applied are used at much larger retailers on databases many times this size, and can also be applied to projects outside of a retail setting.</p>
    <p class="normal">In the next chapter, we will examine another unsupervised learning algorithm. Just like association rules, it is intended to find patterns within data. But unlike association rules that seek groups of related items or features, the methods in the next chapter are concerned with finding connections and relationships among the examples.</p>
    <h1 class="heading-1" id="_idParaDest-215">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>