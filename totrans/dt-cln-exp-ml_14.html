<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer193">
<h1 id="_idParaDest-127"><em class="italic"><a id="_idTextAnchor126"/>Chapter 10</em>: Logistic Regression</h1>
<p>In this and the next few chapters, we will explore models for classification. These involve targets with two or several class values, such as whether a student will pass a class or not or whether a customer will choose chicken, beef, or tofu at a restaurant with only these three choices. There are several machine learning algorithms for these kinds of classification problems. We will take a look at some of the most popular ones in this chapter.</p>
<p>Logistic regression has been used to build models with binary targets for decades. Traditionally, it has been used to generate estimates of the impact of an independent variable or variables on the odds of a dichotomous outcome. Since our focus is on prediction, rather than the effect of each feature, we will also explore regularization techniques, such as lasso regression. These techniques can improve the accuracy of our classification predictions. We will also examine strategies for predicting a multiclass target (when there are more than two possible target values).</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Key concepts of logistic regression</li>
<li>Binary classification with logistic regression</li>
<li>Regularization with logistic regression</li>
<li>Multinomial logistic regression</li>
</ul>
<h1 id="_idParaDest-128"><a id="_idTextAnchor127"/>Technical requirements</h1>
<p>In this chapter, we will stick to the libraries that are available in most scientific distributions of Python: pandas, NumPy, and scikit-learn. All the code in this chapter will run fine with scikit-learn versions 0.24.2 and 1.0.2.</p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor128"/>Key concepts of logistic regression</h1>
<p>If you are familiar with linear regression, or read <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>, of this book, you have probably anticipated some of the issues we will discuss in this chapter – regularization, linearity among regressors, and normally distributed residuals. If you have built supervised machine learning models in the past or worked through the last few chapters of this book, then you have also likely anticipated that we will spend some time discussing the bias-variance tradeoff and how that influences our choice of model.</p>
<p>I remember being introduced to logistic regression 35 years ago in a college course. It is often presented in undergraduate texts almost as a special case of linear regression; that is, linear regression with a binary dependent variable coupled with some transformation to keep predictions between 0 and 1.</p>
<p>It does share many similarities with linear regression of a numeric target variable. Logistic regression is relatively easy to train and interpret. Optimization techniques for both linear and logistic regression are efficient and can generate low bias predictors.</p>
<p>Also like linear regression, logistic regression predicts a target based on weights assigned to each feature. But to constrain the predicted probability to between 0 and 1, we use the sigmoid function. This function takes any value and maps it to a value between 0 and 1:</p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="" height="148" src="image/B17978_10_001.jpg" width="462"/>
</div>
</div>
<p>As <em class="italic">x</em> approaches infinity, <img alt="" height="48" src="image/B17978_10_002.png" width="90"/> gets closer to 1. As <em class="italic">x</em> approaches negative infinity, <img alt="" height="48" src="image/B17978_10_003.png" width="90"/> gets closer to 0.</p>
<p>The following plot illustrates a sigmoid function:</p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 10.1 – Sigmoid function " height="657" src="image/B17978_10_0011.jpg" width="1289"/>
</div>
</div>
<p class="figure-caption">Figure 10.1 – Sigmoid function</p>
<p>We can plug the familiar equation for linear regression, <img alt="" height="48" src="image/B17978_10_004.png" width="420"/>, into the sigmoid function to predict the probability of class membership:</p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="" height="128" src="image/B17978_10_005.jpg" width="839"/>
</div>
</div>
<p>Here <img alt="" height="48" src="image/B17978_10_006.png" width="180"/> is the predicted probability of class membership in the binary case. The coefficients (the betas) can be converted into odds ratios for interpretation, as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="" height="66" src="image/B17978_10_007.jpg" width="201"/>
</div>
</div>
<p>Here, <em class="italic">r</em> is the odds ratio and β is the coefficient. A 1-unit increase in the value of a feature multiplies the odds of class membership by <img alt="" height="47" src="image/B17978_10_008.png" width="50"/>. Similarly, for a binary feature, a true value has <img alt="" height="48" src="image/B17978_10_009.png" width="50"/> times the odds of class membership as does a false value for that feature, all else being equal.</p>
<p>Logistic regression has several advantages as an algorithm for classification problems. Features can be dichotomous, categorical, or numeric, and do not need to be normally distributed. The target variable can have more than two possible values, as we will discuss later, and it can be nominal or ordinal. Another key advantage is that the relationship between features and the target is not assumed to be linear.</p>
<p>The nomenclature here is a tad confusing. Why are we using a regression algorithm for a classification problem? Well, logistic regression predicts the probability of class membership. We apply a decision rule to those probabilities to predict membership. The default threshold is often 0.5 with binary targets. Instances with predicted probabilities greater than or equal to 0.5 get a positive class or 1 or True; those less than 0.5 are assigned 0 or False.</p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Logistic regression extensions</h2>
<p>We will consider two key extensions of logistic regression in this chapter. We will explore multiclass models – that is, those where the target has more than two values. We will also examine the regularization of logistic models to improve (lessen) variance.</p>
<p>A popular choice when constructing multiclass models is <strong class="bold">multinomial logistic regression</strong> (<strong class="bold">MLR</strong>). With MLR, the prediction probability distribution is a multinomial probability distribution. We can replace the equation we used for the binary classifier with a softmax function:</p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="" height="264" src="image/B17978_10_010.jpg" width="839"/>
</div>
</div>
<p>Here, <img alt="" height="47" src="image/B17978_10_011.png" width="513"/>. This calculates a probability for each class label, <em class="italic">j</em>, where <em class="italic">k</em> is the number of classes.</p>
<p>An alternative to multinomial logistic regression when we have more than two classes is <strong class="bold">one-versus-rest</strong> (<strong class="bold">OVR</strong>) logistic regression. This<a id="_idIndexMarker795"/> extension to logistic regression turns the multiclass problem into a binary problem, estimating the probability of class membership versus membership in all of the other classes. The key assumption here is that membership in each class is independent. We will use MLR in an example in this chapter. One advantage it has over OVR is that the predicted probabilities are more reliable.</p>
<p>As mentioned <a id="_idIndexMarker796"/>previously, logistic regression has some of the same challenges as linear regression, including that the low bias of our predictions comes with high variance. This is more likely to be a problem when several features are highly correlated. Fortunately, we can deal with this with regularization, just as we saw in <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>.</p>
<p>Regularization adds a penalty to the loss function. We still seek to minimize the error, but also constrain the size of our parameters. <strong class="bold">L1</strong> regularization, also <a id="_idIndexMarker797"/>referred to as<a id="_idIndexMarker798"/> lasso regression, penalizes the absolute value of the weights (or coefficients): </p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="" height="236" src="image/B17978_10_012.jpg" width="243"/>
</div>
</div>
<p>Here, <em class="italic">p</em> is the number of features and λ determines the strength of the regularization. <strong class="bold">L2</strong> regularization, also<a id="_idIndexMarker799"/> referred to <a id="_idIndexMarker800"/>as ridge regression, penalizes the squared values of the weights:</p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="" height="258" src="image/B17978_10_013.jpg" width="236"/>
</div>
</div>
<p>Both L1 and L2 regularization push the weights toward 0, though L1 regularization is more likely to lead to sparse models. In scikit-learn, we use the <em class="italic">C</em> parameter to adjust the value of λ, where <em class="italic">C</em> is just the inverse of λ:</p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="" height="160" src="image/B17978_10_014.jpg" width="198"/>
</div>
</div>
<p>We can get a <a id="_idIndexMarker801"/>balance between L1 and L2 with elastic net regression. With elastic net regression, we adjust the L1 ratio. A value of 0.5 uses L1 and L2 equally. We can use hyperparameter tuning to choose the best value for the L1 ratio.</p>
<p>Regularization can result in a model with lower variance, which is a good tradeoff when we are less concerned about our coefficients than we are with our predictions.</p>
<p>Before building a model with regularization, we will construct a fairly straightforward logistic model with a binary target. We will also spend a good amount of time evaluating that model. This will be the first classification model we will build in this book and model evaluation looks very different for those models than it does for regression models. </p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Binary classification with logistic regression</h1>
<p>Logistic regression is <a id="_idIndexMarker802"/>often used to model health outcomes<a id="_idIndexMarker803"/> when the target is binary, such as whether the person gets a disease or not. We will go through an example of that in this section. We will build a model to predict if an individual will have heart disease based on personal characteristics such as smoking and alcohol drinking habits; health features, including BMI, asthma, diabetes, and skin cancer; and age.</p>
<p class="callout-heading">Note</p>
<p class="callout">In this chapter, we will work exclusively with data on heart disease that’s available for public download at <a href="https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease">https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease</a>. This dataset is derived from the United States Center for Disease Control data on more than 400,000 individuals from 2020. Data columns include whether respondents ever had heart disease, body mass index, ever smoked, heavy alcohol drinking, age, diabetes, and kidney disease. We will work with a 30,000 individual sample in this section to speed up the processing, but the full dataset is available in the same folder in this book’s GitHub repository.</p>
<p>We will <a id="_idIndexMarker804"/>also do <a id="_idIndexMarker805"/>a little more preprocessing in this chapter than we have in previous chapters. We will integrate much of this work with our pipeline. This will make it easier to reuse this code in the future and lessens the likelihood of data leakage. Follow these steps:</p>
<ol>
<li>We will start by importing the same libraries we have worked with in the last few chapters. We will also import the <strong class="source-inline">LogisticRegression</strong> and <strong class="source-inline">metrics</strong> modules. We will use the <strong class="source-inline">metrics</strong> module from scikit-learn to evaluate each of our classification models in this part of this book. In addition to <strong class="source-inline">matplotlib</strong> for visualizations, we will also use <strong class="source-inline">seaborn</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import StratifiedKFold</p><p class="source-code">from sklearn.feature_selection import RFECV</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p></li>
<li>We are also going to need several custom classes to handle the preprocessing. We have already seen the <strong class="source-inline">OutlierTrans</strong> class. Here, we have added a couple of new classes – <strong class="source-inline">MakeOrdinal</strong> and <strong class="source-inline">ReplaceVals</strong>:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans,\</p><p class="source-code">  MakeOrdinal, ReplaceVals</p></li>
</ol>
<p>The <strong class="source-inline">MakeOrdinal</strong> class <a id="_idIndexMarker806"/>takes a character feature and assigns numeric values based on an alphanumeric sort. For example, a feature that has three possible values – not well, okay, and well – would be transformed into an ordinal feature with values of 0, 1, and 2, respectively. </p>
<p>Recall that<a id="_idIndexMarker807"/> scikit-learn pipeline transformers must have <strong class="source-inline">fit</strong> and <strong class="source-inline">transform</strong> methods, and must inherit from <strong class="source-inline">BaseEstimator</strong>. They often also inherit from <strong class="source-inline">TransformerMixin</strong>, though there are other options.</p>
<p>All the action in the <strong class="source-inline">MakeOrdinal</strong> class happens in the <strong class="source-inline">transform</strong> method. We loop over all of the columns that are passed to it by the column transformer. For each column, we find all the unique values and sort them alphanumerically, storing the unique values in a NumPy array that we name <strong class="source-inline">cats</strong>. Then, we use a lambda function and NumPy’s <strong class="source-inline">where</strong> method to find the index of <strong class="source-inline">cats</strong> associated with each feature value:</p>
<p class="source-code">class MakeOrdinal(BaseEstimator,TransformerMixin):</p>
<p class="source-code">  def fit(self,X,y=None):</p>
<p class="source-code">    return self</p>
<p class="source-code">    </p>
<p class="source-code">  def transform(self,X,y=None):</p>
<p class="source-code">    Xnew = X.copy()</p>
<p class="source-code">    for col in Xnew.columns:</p>
<p class="source-code">      cats = np.sort(Xnew[col].unique())</p>
<p class="source-code">      Xnew[col] = Xnew.\</p>
<p class="source-code">        apply(lambda x: int(np.where(cats==\</p>
<p class="source-code">        x[col])[0]), axis=1)</p>
<p class="source-code">    return Xnew.values</p>
<p><strong class="source-inline">MakeOrdinal</strong> will work fine when the alphanumeric order matches a meaningful order, as with <a id="_idIndexMarker808"/>the previous example. When that is not true, we can use <strong class="source-inline">ReplaceVals</strong> to assign appropriate ordinal values. This class replaces values in any feature with alternative values based on a dictionary passed to it.</p>
<p>We could <a id="_idIndexMarker809"/>have just used the pandas <strong class="source-inline">replace</strong> method without putting it in a pipeline, but this way, it is easier to integrate our recoding with other pipeline steps, such as feature scaling:</p>
<p class="source-code">class ReplaceVals(BaseEstimator,TransformerMixin):</p>
<p class="source-code">  def __init__(self,repdict):</p>
<p class="source-code">    self.repdict = repdict</p>
<p class="source-code">  def fit(self,X,y=None):</p>
<p class="source-code">    return self</p>
<p class="source-code">    </p>
<p class="source-code">  def transform(self,X,y=None):</p>
<p class="source-code">    Xnew = X.copy().replace(self.repdict)</p>
<p class="source-code">return Xnew.values</p>
<p>Do not worry if you do not fully understand how we will use these classes yet. It will be clearer when we add them to our column transformations.</p>
<ol>
<li value="3">Next, we will <a id="_idIndexMarker810"/>load the heart disease data and take a look at a few rows. Several string features are conceptually binary, such as <strong class="source-inline">alcoholdrinkingheavy</strong>, which is <strong class="source-inline">Yes</strong> when the person is a heavy drinker and <strong class="source-inline">No</strong> otherwise. We will need to encode these features before running a model.</li>
</ol>
<p>The <strong class="source-inline">agecategory</strong> feature is character data that represents the age interval. We will need to <a id="_idIndexMarker811"/>convert that feature into numeric:</p>
<p class="source-code">healthinfo = pd.read_csv("data/healthinfo.csv")</p>
<p class="source-code">healthinfo.set_index("personid", inplace=True)</p>
<p class="source-code">healthinfo.head(2).T</p>
<p class="source-code"><strong class="bold">personid                    299391       252786</strong></p>
<p class="source-code"><strong class="bold">heartdisease                Yes          No</strong></p>
<p class="source-code"><strong class="bold">bmi                         28.48        25.24</strong></p>
<p class="source-code"><strong class="bold">smoking                     Yes          Yes</strong></p>
<p class="source-code"><strong class="bold">alcoholdrinkingheavy        No           No</strong></p>
<p class="source-code"><strong class="bold">stroke                      No           No</strong></p>
<p class="source-code"><strong class="bold">physicalhealthbaddays       7            0</strong></p>
<p class="source-code"><strong class="bold">mentalhealthbaddays         0            2</strong></p>
<p class="source-code"><strong class="bold">walkingdifficult            No           No</strong></p>
<p class="source-code"><strong class="bold">gender                      Male         Female</strong></p>
<p class="source-code"><strong class="bold">agecategory                 70-74        65-69</strong></p>
<p class="source-code"><strong class="bold">ethnicity                   White        White</strong></p>
<p class="source-code"><strong class="bold">diabetic    No, borderline diabetes      No</strong></p>
<p class="source-code"><strong class="bold">physicalactivity            Yes          Yes</strong></p>
<p class="source-code"><strong class="bold">genhealth                   Good         Very good</strong></p>
<p class="source-code"><strong class="bold">sleeptimenightly            8            8</strong></p>
<p class="source-code"><strong class="bold">asthma                      No           No</strong></p>
<p class="source-code"><strong class="bold">kidneydisease               No           No</strong></p>
<p class="source-code"><strong class="bold">skincancer                  No           Yes</strong></p>
<ol>
<li value="4">Let’s look at<a id="_idIndexMarker812"/> the size of the DataFrame <a id="_idIndexMarker813"/>and how many missing values we have. There are 30,000 instances, but there are no missings for any of the 18 data columns. That’s great. We won’t have to worry about that when we construct our pipeline:<p class="source-code">healthinfo.shape</p><p class="source-code"><strong class="bold">(30000, 18)</strong></p><p class="source-code">healthinfo.isnull().sum()</p><p class="source-code"><strong class="bold">heartdisease                0</strong></p><p class="source-code"><strong class="bold">bmi                         0</strong></p><p class="source-code"><strong class="bold">smoking                     0</strong></p><p class="source-code"><strong class="bold">alcoholdrinkingheavy        0</strong></p><p class="source-code"><strong class="bold">stroke                      0</strong></p><p class="source-code"><strong class="bold">physicalhealthbaddays       0</strong></p><p class="source-code"><strong class="bold">mentalhealthbaddays         0</strong></p><p class="source-code"><strong class="bold">walkingdifficult            0</strong></p><p class="source-code"><strong class="bold">gender                      0</strong></p><p class="source-code"><strong class="bold">agecategory                 0</strong></p><p class="source-code"><strong class="bold">ethnicity                   0</strong></p><p class="source-code"><strong class="bold">diabetic                    0</strong></p><p class="source-code"><strong class="bold">physicalactivity            0</strong></p><p class="source-code"><strong class="bold">genhealth                   0</strong></p><p class="source-code"><strong class="bold">sleeptimenightly            0</strong></p><p class="source-code"><strong class="bold">asthma                      0</strong></p><p class="source-code"><strong class="bold">kidneydisease               0</strong></p><p class="source-code"><strong class="bold">skincancer                  0</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>Let’s change the <strong class="source-inline">heartdisease</strong> variable, which will be our target, into a <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> variable. This <a id="_idIndexMarker814"/>will give us one less thing<a id="_idIndexMarker815"/> to worry about later. One thing to notice right away is that the target’s values are quite imbalanced. Less than 10% of our observations have heart disease. That, of course, is good news, but it presents some challenges for modeling that we will need to handle: <p class="source-code">healthinfo.heartdisease.value_counts()</p><p class="source-code"><strong class="bold">No        27467</strong></p><p class="source-code"><strong class="bold">Yes       2533</strong></p><p class="source-code"><strong class="bold">Name: heartdisease, dtype: int64</strong></p><p class="source-code">healthinfo['heartdisease'] = \</p><p class="source-code">  np.where(healthinfo.heartdisease=='No',0,1).\</p><p class="source-code">     astype('int')</p><p class="source-code">healthinfo.heartdisease.value_counts()</p><p class="source-code"><strong class="bold">0        27467</strong></p><p class="source-code"><strong class="bold">1        2533</strong></p><p class="source-code"><strong class="bold">Name: heartdisease, dtype: int64</strong></p></li>
<li>We should organize our features by the preprocessing we will be doing with them. We will be scaling the numeric features and doing one-hot encoding with the categorical features. We want to make the <strong class="source-inline">agecategory</strong> and <strong class="source-inline">genhealth</strong> features, which are currently strings, into ordinal features.</li>
</ol>
<p>We need to do a specific cleanup of the <strong class="source-inline">diabetic</strong> feature. Some individuals indicate no, but that they were borderline. For our purposes, we will consider them a <em class="italic">no</em>. Some<a id="_idIndexMarker816"/> individuals had diabetes during their pregnancies only. We will consider them a <em class="italic">yes</em>. For both <strong class="source-inline">genhealth</strong> and <strong class="source-inline">diabetic</strong>, we will set up a dictionary<a id="_idIndexMarker817"/> that will indicate how feature values should be replaced. We will use that dictionary in the <strong class="source-inline">ReplaceVals</strong> transformer of our pipeline:</p>
<p class="source-code">num_cols = ['bmi','physicalhealthbaddays',</p>
<p class="source-code">   'mentalhealthbaddays','sleeptimenightly']</p>
<p class="source-code">binary_cols = ['smoking','alcoholdrinkingheavy',</p>
<p class="source-code">  'stroke','walkingdifficult','physicalactivity',</p>
<p class="source-code">  'asthma','kidneydisease','skincancer']</p>
<p class="source-code">cat_cols = ['gender','ethnicity']</p>
<p class="source-code">spec_cols1 = ['agecategory']</p>
<p class="source-code">spec_cols2 = ['genhealth']</p>
<p class="source-code">spec_cols3 = ['diabetic']</p>
<p class="source-code">rep_dict = {</p>
<p class="source-code">  'genhealth': {'Poor':0,'Fair':1,'Good':2,</p>
<p class="source-code">    'Very good':3,'Excellent':4},</p>
<p class="source-code">  'diabetic': {'No':0,</p>
<p class="source-code">    'No, borderline diabetes':0,'Yes':1,</p>
<p class="source-code">    'Yes (during pregnancy)':1}           </p>
<p class="source-code">}</p>
<ol>
<li value="7">We should<a id="_idIndexMarker818"/> take a look at some frequencies for the binary features, as well as other categorical features. A large percentage of the individuals (42%) report that they have been smokers. 14% report that they have difficulty<a id="_idIndexMarker819"/> walking:<p class="source-code">healthinfo[binary_cols].\</p><p class="source-code">  apply(pd.value_counts, normalize=True).T</p><p class="source-code"><strong class="bold">                        No       Yes</strong></p><p class="source-code"><strong class="bold">smoking                 0.58     0.42</strong></p><p class="source-code"><strong class="bold">alcoholdrinkingheavy    0.93     0.07</strong></p><p class="source-code"><strong class="bold">stroke                  0.96     0.04</strong></p><p class="source-code"><strong class="bold">walkingdifficult        0.86     0.14</strong></p><p class="source-code"><strong class="bold">physicalactivity        0.23     0.77</strong></p><p class="source-code"><strong class="bold">asthma                  0.87     0.13</strong></p><p class="source-code"><strong class="bold">kidneydisease           0.96     0.04</strong></p><p class="source-code"><strong class="bold">skincancer              0.91     0.09</strong></p></li>
<li>Let’s also look at frequencies for the other categorical features. There are nearly equal numbers of men and women. Most people report excellent or very good health:<p class="source-code">for col in healthinfo[cat_cols + </p><p class="source-code">['genhealth','diabetic']].columns:</p><p class="source-code">  print(col, "----------------------",</p><p class="source-code">  healthinfo[col].value_counts(normalize=True).\</p><p class="source-code">      sort_index(), sep="\n", end="\n\n")</p></li>
</ol>
<p>This<a id="_idIndexMarker820"/> produces <a id="_idIndexMarker821"/>the following output:</p>
<p class="source-code"><strong class="bold">gender</strong></p>
<p class="source-code"><strong class="bold">----------------------</strong></p>
<p class="source-code"><strong class="bold">Female   0.52</strong></p>
<p class="source-code"><strong class="bold">Male     0.48</strong></p>
<p class="source-code"><strong class="bold">Name: gender, dtype: float64</strong></p>
<p class="source-code"><strong class="bold">ethnicity</strong></p>
<p class="source-code"><strong class="bold">----------------------</strong></p>
<p class="source-code"><strong class="bold">American Indian/Alaskan Native   0.02</strong></p>
<p class="source-code"><strong class="bold">Asian                            0.03</strong></p>
<p class="source-code"><strong class="bold">Black                            0.07</strong></p>
<p class="source-code"><strong class="bold">Hispanic                         0.09</strong></p>
<p class="source-code"><strong class="bold">Other                            0.03</strong></p>
<p class="source-code"><strong class="bold">White                            0.77</strong></p>
<p class="source-code"><strong class="bold">Name: ethnicity, dtype: float64</strong></p>
<p class="source-code"><strong class="bold">genhealth</strong></p>
<p class="source-code"><strong class="bold">----------------------</strong></p>
<p class="source-code"><strong class="bold">Excellent   0.21</strong></p>
<p class="source-code"><strong class="bold">Fair        0.11</strong></p>
<p class="source-code"><strong class="bold">Good        0.29</strong></p>
<p class="source-code"><strong class="bold">Poor        0.04</strong></p>
<p class="source-code"><strong class="bold">Very good   0.36</strong></p>
<p class="source-code"><strong class="bold">Name: genhealth, dtype: float64</strong></p>
<p class="source-code"><strong class="bold">diabetic</strong></p>
<p class="source-code"><strong class="bold">----------------------</strong></p>
<p class="source-code"><strong class="bold">No                        0.84</strong></p>
<p class="source-code"><strong class="bold">No, borderline diabetes   0.02</strong></p>
<p class="source-code"><strong class="bold">Yes                       0.13</strong></p>
<p class="source-code"><strong class="bold">Yes (during pregnancy)    0.01</strong></p>
<p class="source-code"><strong class="bold">Name: diabetic, dtype: float64</strong></p>
<ol>
<li value="9">We <a id="_idIndexMarker822"/>should <a id="_idIndexMarker823"/>also look at some descriptive statistics for the numerical features. The median value for both bad physical health and mental health days is 0; that is, at least half of the observations report no bad physical health days, and at least half report no bad mental health days over the previous month:<p class="source-code">healthinfo[num_cols].\</p><p class="source-code">  agg(['count','min','median','max']).T</p><p class="source-code">                       count    min    median  max</p><p class="source-code"><strong class="bold">bmi                    30,000   12     27      92</strong></p><p class="source-code"><strong class="bold">physicalhealthbaddays  30,000   0      0       30</strong></p><p class="source-code"><strong class="bold">mentalhealthbaddays    30,000   0      0       30</strong></p><p class="source-code"><strong class="bold">sleeptimenightly       30,000   1      7       24</strong></p></li>
</ol>
<p>We will need to do some scaling. We will also need to do some encoding of the categorical features. There are also some extreme values for the numerical features. A <strong class="source-inline">sleeptimenightly</strong> value of 24 seems unlikely! It is probably a good idea to deal with them.</p>
<ol>
<li value="10">Now, we are <a id="_idIndexMarker824"/>ready to build our pipeline. Let’s create the training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(healthinfo[num_cols + </p><p class="source-code">    binary_cols + cat_cols + spec_cols1 +</p><p class="source-code">    spec_cols2 + spec_cols3],\</p><p class="source-code">  healthinfo[['heartdisease']], test_size=0.2,</p><p class="source-code">    random_state=0)</p></li>
<li> Next, we<a id="_idIndexMarker825"/> will set up the column transformations. We will create a one-hot encoder instance that we will use for all of the categorical features. For the numeric columns, we will remove extreme values using the <strong class="source-inline">OutlierTrans</strong> object and then impute the median.</li>
</ol>
<p>We will convert the <strong class="source-inline">agecategory</strong> feature into an ordinal one using the <strong class="source-inline">MakeOrdinal</strong> transformer and code the <strong class="source-inline">genhealth</strong> and <strong class="source-inline">diabetic</strong> features using the <strong class="source-inline">ReplaceVals</strong> transformer.</p>
<p>We will add the column transformation to our pipeline in the next step:</p>
<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p>
<p class="source-code">standtrans = make_pipeline(OutlierTrans(3),</p>
<p class="source-code">  SimpleImputer(strategy="median"),</p>
<p class="source-code">  StandardScaler())</p>
<p class="source-code">spectrans1 = make_pipeline(MakeOrdinal(),</p>
<p class="source-code">  StandardScaler())</p>
<p class="source-code">spectrans2 = make_pipeline(ReplaceVals(rep_dict),</p>
<p class="source-code">  StandardScaler())</p>
<p class="source-code">spectrans3 = make_pipeline(ReplaceVals(rep_dict))</p>
<p class="source-code">bintrans = make_pipeline(ohe)</p>
<p class="source-code">cattrans = make_pipeline(ohe)</p>
<p class="source-code">coltrans = ColumnTransformer(</p>
<p class="source-code">  transformers=[</p>
<p class="source-code">    ("stand", standtrans, num_cols),</p>
<p class="source-code">    ("spec1", spectrans1, spec_cols1),</p>
<p class="source-code">    ("spec2", spectrans2, spec_cols2),</p>
<p class="source-code">    ("spec3", spectrans3, spec_cols3),</p>
<p class="source-code">    ("bin", bintrans, binary_cols),</p>
<p class="source-code">    ("cat", cattrans, cat_cols),</p>
<p class="source-code">  ]</p>
<p class="source-code">)</p>
<ol>
<li value="12">Now, we <a id="_idIndexMarker826"/>are <a id="_idIndexMarker827"/>ready to set up and fit our pipeline. First, we will instantiate logistic regression and stratified k-fold objects, which we will use with recursive feature elimination. Recall that recursive feature elimination needs an estimator. We use stratified k-fold to get approximately the same target value distribution in each fold.</li>
</ol>
<p>Now, we must create another logistic regression instance for our model. We will set the <strong class="source-inline">class_weight</strong> parameter to <strong class="source-inline">balanced</strong>. This should improve the model’s ability to deal with the class imbalance. Then, we will add the column transformation, recursive feature elimination, and logistic regression instance to our pipeline, and then fit it:</p>
<p class="source-code">lrsel = LogisticRegression(random_state=1, </p>
<p class="source-code">  max_iter=1000)</p>
<p class="source-code">kf = StratifiedKFold(n_splits=5, shuffle=True)</p>
<p class="source-code">rfecv = RFECV(estimator=lrsel, cv=kf)</p>
<p class="source-code">lr = LogisticRegression(random_state=1,</p>
<p class="source-code">  class_weight='balanced', max_iter=1000)</p>
<p class="source-code">pipe1 = make_pipeline(coltrans, rfecv, lr)</p>
<p class="source-code">pipe1.fit(X_train, y_train.values.ravel())</p>
<ol>
<li value="13">We need<a id="_idIndexMarker828"/> to do a little work to recover<a id="_idIndexMarker829"/> the column names from the pipeline after the fit. We can use the <strong class="source-inline">get_feature_names</strong> method of the one-hot encoder for the <strong class="source-inline">bin</strong> transformer and the <strong class="source-inline">cat</strong> transformer for this. This gives us the column names for the binary and categorical features after the encoding. The names of the numerical features remain unchanged. We will use the feature names later:<p class="source-code">new_binary_cols = \</p><p class="source-code">  pipe1.named_steps['columntransformer'].\</p><p class="source-code">  named_transformers_['bin'].\</p><p class="source-code">  named_steps['onehotencoder'].\</p><p class="source-code">  get_feature_names(binary_cols)</p><p class="source-code">new_cat_cols = \</p><p class="source-code">  pipe1.named_steps['columntransformer'].\</p><p class="source-code">  named_transformers_['cat'].\</p><p class="source-code">  named_steps['onehotencoder'].\</p><p class="source-code">  get_feature_names(cat_cols)</p><p class="source-code">new_cols = np.concatenate((np.array(num_cols +</p><p class="source-code">  spec_cols1 + spec_cols2 + spec_cols3),</p><p class="source-code">  new_binary_cols, new_cat_cols))</p><p class="source-code">new_cols</p><p class="source-code"><strong class="bold">array(['bmi', 'physicalhealthbaddays',</strong></p><p class="source-code"><strong class="bold">       'mentalhealthbaddays', 'sleeptimenightly',</strong></p><p class="source-code"><strong class="bold">       'agecategory', 'genhealth', 'diabetic',</strong></p><p class="source-code"><strong class="bold">       'smoking_Yes', 'alcoholdrinkingheavy_Yes',</strong></p><p class="source-code"><strong class="bold">       'stroke_Yes', 'walkingdifficult_Yes',</strong></p><p class="source-code"><strong class="bold">       'physicalactivity_Yes', 'asthma_Yes',</strong></p><p class="source-code"><strong class="bold">       'kidneydisease_Yes', 'skincancer_Yes',</strong></p><p class="source-code"><strong class="bold">       'gender_Male', 'ethnicity_Asian',</strong></p><p class="source-code"><strong class="bold">       'ethnicity_Black', 'ethnicity_Hispanic',</strong></p><p class="source-code"><strong class="bold">       'ethnicity_Other', 'ethnicity_White'],</strong></p><p class="source-code"><strong class="bold">      dtype=object)</strong></p></li>
<li>Now, let’s look at<a id="_idIndexMarker830"/> the results from the<a id="_idIndexMarker831"/> recursive feature elimination. We can use the <strong class="source-inline">ranking_</strong> attribute of the <strong class="source-inline">rfecv</strong> object to get the ranking of each feature. Those with a <em class="italic">1</em> for ranking will be selected for our model.</li>
</ol>
<p>If we use the <strong class="source-inline">get_support</strong> method or the <strong class="source-inline">support_</strong> attribute of the <strong class="source-inline">rfecv</strong> object instead of the <strong class="source-inline">ranking_</strong> attribute, we get just those features that will be used in our model – that is, those with a ranking of 1. We will do that in <a id="_idIndexMarker832"/>the <a id="_idIndexMarker833"/>next step:</p>
<p class="source-code">rankinglabs = \</p>
<p class="source-code"> np.column_stack((pipe1.named_steps['rfecv'].ranking_,</p>
<p class="source-code"> new_cols))</p>
<p class="source-code">pd.DataFrame(rankinglabs,</p>
<p class="source-code"> columns=['rank','feature']).\</p>
<p class="source-code"> sort_values(['rank','feature']).\</p>
<p class="source-code"> set_index("rank")</p>
<p class="source-code"><strong class="bold">                       feature</strong></p>
<p class="source-code"><strong class="bold">rank                          </strong></p>
<p class="source-code"><strong class="bold">1                  agecategory</strong></p>
<p class="source-code"><strong class="bold">1     alcoholdrinkingheavy_Yes</strong></p>
<p class="source-code"><strong class="bold">1                   asthma_Yes</strong></p>
<p class="source-code"><strong class="bold">1                     diabetic</strong></p>
<p class="source-code"><strong class="bold">1              ethnicity_Asian</strong></p>
<p class="source-code"><strong class="bold">1              ethnicity_Other</strong></p>
<p class="source-code"><strong class="bold">1              ethnicity_White</strong></p>
<p class="source-code"><strong class="bold">1                  gender_Male</strong></p>
<p class="source-code"><strong class="bold">1                    genhealth</strong></p>
<p class="source-code"><strong class="bold">1            kidneydisease_Yes</strong></p>
<p class="source-code"><strong class="bold">1                  smoking_Yes</strong></p>
<p class="source-code"><strong class="bold">1                   stroke_Yes</strong></p>
<p class="source-code"><strong class="bold">1         walkingdifficult_Yes</strong></p>
<p class="source-code"><strong class="bold">2           ethnicity_Hispanic</strong></p>
<p class="source-code"><strong class="bold">3               skincancer_Yes</strong></p>
<p class="source-code"><strong class="bold">4                          bmi</strong></p>
<p class="source-code"><strong class="bold">5        physicalhealthbaddays</strong></p>
<p class="source-code"><strong class="bold">6             sleeptimenightly</strong></p>
<p class="source-code"><strong class="bold">7          mentalhealthbaddays</strong></p>
<p class="source-code"><strong class="bold">8         physicalactivity_Yes</strong></p>
<p class="source-code"><strong class="bold">9              ethnicity_Black</strong></p>
<ol>
<li value="15">We can get <a id="_idIndexMarker834"/>the odds ratios from the coefficients from the logistic regression. Recall that the odds ratio is the exponentiated coefficient. There are 13 coefficients, which makes sense because we learned in the previous step that 13 features got a ranking of 1.</li>
</ol>
<p>We will use the <strong class="source-inline">get_support</strong> method of the <strong class="source-inline">rfecv</strong> step to get the names of the selected features and create a NumPy array with those names and the odds ratios, <strong class="source-inline">oddswithlabs</strong>. We then create a pandas DataFrame and sort by the odds ratio in descending order.</p>
<p>Not surprisingly, those<a id="_idIndexMarker835"/> who had a stroke and older individuals are substantially more likely to have heart disease. If the individual had a stroke, they had three times the odds of having heart disease, controlling for everything else. The odds of having heart disease increase by 2.88 times for each increase in age category. On the other hand, the odds of having heart disease decline by about half (57%) for every increase in general health; from, say, fair to good. Surprisingly, heavy alcohol drinking is associated with lower odds of heart disease, controlling for everything else:</p>
<p class="source-code">oddsratios = np.exp(pipe1.\</p>
<p class="source-code">  named_steps['logisticregression'].coef_)</p>
<p class="source-code">oddsratios.shape</p>
<p class="source-code"><strong class="bold">(1, 13)</strong></p>
<p class="source-code">selcols = new_cols[pipe1.\</p>
<p class="source-code">  named_steps['rfecv'].get_support()]</p>
<p class="source-code">oddswithlabs = np.column_stack((oddsratios.\</p>
<p class="source-code">  ravel(), selcols))</p>
<p class="source-code">pd.DataFrame(oddswithlabs, </p>
<p class="source-code">  columns=['odds','feature']).\</p>
<p class="source-code">  sort_values(['odds'], ascending=False).\</p>
<p class="source-code">  set_index('odds')</p>
<p class="source-code"><strong class="bold">                        feature</strong></p>
<p class="source-code"><strong class="bold">odds                          </strong></p>
<p class="source-code"><strong class="bold">3.01                stroke_Yes</strong></p>
<p class="source-code"><strong class="bold">2.88               agecategory</strong></p>
<p class="source-code"><strong class="bold">2.12               gender_Male</strong></p>
<p class="source-code"><strong class="bold">1.97         kidneydisease_Yes</strong></p>
<p class="source-code"><strong class="bold">1.75                  diabetic</strong></p>
<p class="source-code"><strong class="bold">1.55               smoking_Yes</strong></p>
<p class="source-code"><strong class="bold">1.52                asthma_Yes</strong></p>
<p class="source-code"><strong class="bold">1.30      walkingdifficult_Yes</strong></p>
<p class="source-code"><strong class="bold">1.27           ethnicity_Other</strong></p>
<p class="source-code"><strong class="bold">1.22           ethnicity_White</strong></p>
<p class="source-code"><strong class="bold">0.72           ethnicity_Asian</strong></p>
<p class="source-code"><strong class="bold">0.61  alcoholdrinkingheavy_Yes</strong></p>
<p class="source-code"><strong class="bold">0.57                 genhealth</strong></p>
<p>Now that <a id="_idIndexMarker836"/>we have fit our logistic regression model, we <a id="_idIndexMarker837"/>are ready to evaluate it. In the next section, we will spend some time looking into various performance measures, including accuracy and sensitivity. We will use many of the concepts that we introduced in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>.</p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/>Evaluating a logistic regression model</h2>
<p>The <a id="_idIndexMarker838"/>most intuitive measure of a classification model’s performance is its accuracy – that is, how often our predictions are correct. In some cases, however, we might be at least as concerned about sensitivity – the percent of positive cases that we predict correctly – as accuracy; we may even be willing to lose a little accuracy to improve sensitivity. Predictive models of diseases often fall into that category. But whenever there is a class imbalance, measures such as accuracy and sensitivity can give us very different estimates of the performance of our model. </p>
<p>In addition to being concerned about accuracy or sensitivity, we might be worried about our model’s <strong class="bold">specificity</strong> or <strong class="bold">precision</strong>. We may want a model that can identify negative cases with high reliability, even if that means it does not do as good a job of identifying positives. Specificity is a <a id="_idIndexMarker839"/>measure of the percentage of all negatives identified by the model.</p>
<p>Precision, which<a id="_idIndexMarker840"/> is the percentage of predicted positives that are positives, is another important measure. For some applications, it is important to limit false positives, even if we have to tolerate lower sensitivity. An apple grower, using image recognition to identify bad apples, may prefer a high-precision model to a more sensitive one, not wanting to discard apples unnecessarily.</p>
<p>This can be made more clear by looking at a confusion matrix:</p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 10.2 – Confusion matrix of actual by predicted values for a binary target " height="435" src="image/B17978_10_002.jpg" width="1650"/>
</div>
</div>
<p class="figure-caption">Figure 10.2 – Confusion matrix of actual by predicted values for a binary target</p>
<p>The confusion matrix helps us conceptualize accuracy, sensitivity, specificity, and precision. Accuracy is the percentage of observations for which our prediction was correct. This can be stated more precisely as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="" height="122" src="image/B17978_10_015.jpg" width="822"/>
</div>
</div>
<p>Sensitivity is<a id="_idIndexMarker841"/> the number of times we predicted positives correctly divided by the number of positives. It might be helpful to glance at the confusion matrix again and confirm that actual positive values can either be predicted positives (TP) or predicted negatives (FN). Sensitivity is also referred to as <strong class="bold">recall</strong> or<a id="_idIndexMarker842"/> the <strong class="bold">true positive rate</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="" height="122" src="image/B17978_10_016.jpg" width="586"/>
</div>
</div>
<p>Specificity is the number of times we correctly predicted a negative value (TN) divided by the number of actual negative values (TN + FP). Specificity is also<a id="_idIndexMarker843"/> known as the <strong class="bold">true negative rate</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="" height="123" src="image/B17978_10_017.jpg" width="598"/>
</div>
</div>
<p>Precision is the number of times we correctly predicted a positive value (TP) divided by the number of positive values predicted:</p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="" height="124" src="image/B17978_10_018.jpg" width="563"/>
</div>
</div>
<p>We went over these concepts in more detail in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for</em> <em class="italic">Model Evaluation</em>. In this section, we will examine the accuracy, sensitivity, specificity, and precision of our logistic regression model <a id="_idIndexMarker844"/>of heart disease:</p>
<ol>
<li value="1">We can use the <strong class="source-inline">predict</strong> method of the pipeline we fitted in the previous section to generate predictions from our logistic regression. Then, we can generate a confusion matrix:<p class="source-code">pred = pipe1.predict(X_test)</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Heart Disease Prediction Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 10.3 – A confusion matrix for heart disease prediction " height="441" src="image/B17978_10_003.jpg" width="567"/>
</div>
</div>
<p class="figure-caption">Figure 10.3 – A confusion matrix for heart disease prediction</p>
<p>The first thing <a id="_idIndexMarker845"/>to notice here is that most of the action is in the top-left quadrant, where we correctly predict actual negative values in the testing data. That is going to help our accuracy a fair bit. Nonetheless, we have a fair number of false positives. We predict heart disease 1,430 times (out of 5,506 negative instances) when there is no heart disease. We do seem to do an okay job of identifying positive heart disease instances, correctly classifying 392 instances (out of 494) that were positive.</p>
<ol>
<li value="2">Let’s calculate the accuracy, sensitivity, specificity, and precision. The overall accuracy is not great, at 74%. Sensitivity is pretty decent though, at 79%. (Of course, how <em class="italic">decent</em> the sensitivity is depends on the domain and judgment. For something such as heart disease, we likely want it to be higher.) This can be seen in the following code:<p class="source-code">tn, fp, fn, tp = skmet.confusion_matrix(y_test.values.ravel(), pred).ravel()</p><p class="source-code">tn, fp, fn, tp</p><p class="source-code"><strong class="bold">(4076, 1430, 102, 392)</strong></p><p class="source-code">accuracy = (tp + tn) / pred.shape[0]</p><p class="source-code">accuracy</p><p class="source-code"><strong class="bold">0.7446666666666667</strong></p><p class="source-code">sensitivity = tp / (tp + fn)</p><p class="source-code">sensitivity</p><p class="source-code"><strong class="bold">0.7935222672064778</strong></p><p class="source-code">specificity = tn / (tn+fp)</p><p class="source-code">specificity</p><p class="source-code"><strong class="bold">0.7402833272793317</strong></p><p class="source-code">precision = tp / (tp + fp)</p><p class="source-code">precision</p><p class="source-code"><strong class="bold">0.21514818880351264</strong></p></li>
<li>We can do these <a id="_idIndexMarker846"/>calculations in a more straightforward way using the <strong class="source-inline">metrics</strong> module (I chose a more roundabout approach in the previous step to illustrate how the calculations are done):<p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred,</p><p class="source-code">  pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.74, sensitivity: 0.79, specificity: 0.74, precision: 0.22</strong></p></li>
</ol>
<p>The biggest <a id="_idIndexMarker847"/>problem with our model is the very low level of precision – that is, 22%. This is due to the large number of false positives. The majority of the time that our model predicts positive, it is wrong.</p>
<p>In addition to the four measures that we have already calculated, it can also be helpful to get the false positive rate. The false positive rate is the propensity of our model to predict positive when the actual value is negative:</p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="" height="108" src="image/B17978_10_019.jpg" width="674"/>
</div>
</div>
<ol>
<li value="4">Let’s calculate the false positive rate:<p class="source-code">falsepositiverate = fp / (tn + fp)</p><p class="source-code">falsepositiverate</p><p class="source-code"><strong class="bold">0.25971667272066834</strong></p></li>
</ol>
<p>So, 26% of the time that a person does not have heart disease, we predicted that they do. While we certainly want to limit the number of false positives, this often means sacrificing some sensitivity. We will demonstrate why this is true later in this section.</p>
<ol>
<li value="5">We should take a closer look at the prediction probabilities generated by our model. Here, the threshold for a positive class prediction is 0.5, which is often the default with logistic regression. (Recall that logistic regression predicts a probability <a id="_idIndexMarker848"/>of class membership. We need an accompanying decision rule, such as the 0.5 threshold, to predict the class.) This can be seen in the following code:<p class="source-code">pred_probs = pipe1.predict_proba(X_test)[:, 1]</p><p class="source-code">probdf = \</p><p class="source-code">  pd.DataFrame(zip(pred_probs, pred,</p><p class="source-code">  y_test.values.ravel()),</p><p class="source-code">  columns=(['prob','pred','actual']))</p><p class="source-code">probdf.groupby(['pred'])['prob'].\</p><p class="source-code">  agg(['min','max','count'])</p><p class="source-code"><strong class="bold">        min        max        count</strong></p><p class="source-code"><strong class="bold">pred                 </strong></p><p class="source-code"><strong class="bold">0       0.01       0.50       4178</strong></p><p class="source-code"><strong class="bold">1       0.50       0.99       1822</strong></p></li>
<li>We can<a id="_idIndexMarker849"/> use a <strong class="bold">kernel density estimate</strong> (<strong class="bold">KDE</strong>) plot to visualize these probabilities. We can also see how a different decision rule may impact our predictions. For example, we could move the threshold from 0.5 to 0.25. At a glance, that has some advantages. The area between the two possible thresholds has somewhat more heart disease cases than no heart disease cases. We would be getting the brown area between the dashed lines right, predicting heart disease correctly where we would not have with the 0.5 threshold. That is a larger area than the green area between the lines, where we turn some of the true negative predictions at<a id="_idIndexMarker850"/> the 0.5 threshold into false positives at the 0.25 threshold:<p class="source-code">sns.kdeplot(probdf.loc[probdf.actual==1].prob,</p><p class="source-code">  shade=True, color='red',label="Heart Disease")</p><p class="source-code">sns.kdeplot(probdf.loc[probdf.actual==0].prob,</p><p class="source-code">  shade=True,color='green',label="No Heart Disease")</p><p class="source-code">plt.axvline(0.25, color='black', linestyle='dashed',</p><p class="source-code">  linewidth=1)</p><p class="source-code">plt.axvline(0.5, color='black', linestyle='dashed',</p><p class="source-code">  linewidth=1)</p><p class="source-code">plt.title("Predicted Probability Distribution")</p><p class="source-code">plt.legend(loc="upper left")</p></li>
</ol>
<p>This generates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 10.4 – Heart disease predicted probability distribution " height="445" src="image/B17978_10_004.jpg" width="561"/>
</div>
</div>
<p class="figure-caption">Figure 10.4 – Heart disease predicted probability distribution</p>
<p>Let’s consider the<a id="_idIndexMarker851"/> tradeoff between precision and sensitivity a little more carefully than we have so far. Remember that precision is the rate at which we are right when we predict a positive class value. Sensitivity, also referred to as recall or the true positive rate, is the rate at which we identify an actual positive as positive.</p>
<ol>
<li value="7">We can plot precision and sensitivity curves as follows:<p class="source-code">prec, sens, ths = skmet.precision_recall_curve(y_test, pred_probs)</p><p class="source-code">sens = sens[1:-20]</p><p class="source-code">prec = prec[1:-20]</p><p class="source-code">ths  = ths[:-20]</p><p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(ths, prec, label='Precision')</p><p class="source-code">ax.plot(ths, sens, label='Sensitivity')</p><p class="source-code">ax.set_title('Precision and Sensitivity by Threshold')</p><p class="source-code">ax.set_xlabel('Threshold')</p><p class="source-code">ax.set_ylabel('Precision and Sensitivity')</p><p class="source-code">ax.legend()</p></li>
</ol>
<p>This <a id="_idIndexMarker852"/>generates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 10.5 – Precision and sensitivity at threshold values " height="447" src="image/B17978_10_0051.jpg" width="570"/>
</div>
</div>
<p class="figure-caption">Figure 10.5 – Precision and sensitivity at threshold values</p>
<p>As the<a id="_idIndexMarker853"/> threshold increases beyond 0.2, there is a sharper decrease in sensitivity than there is an increase in precision.</p>
<ol>
<li value="8">It is often also helpful to look at the false positive rate with the sensitivity rate. The false positive rate is the propensity of our model to predict positive when the actual value is negative. One way to see that relationship is with a ROC curve:<p class="source-code">fpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)</p><p class="source-code">ths = ths[1:]</p><p class="source-code">fpr = fpr[1:]</p><p class="source-code">tpr = tpr[1:]</p><p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(fpr, tpr, linewidth=4, color="black")</p><p class="source-code">ax.set_title('ROC curve')</p><p class="source-code">ax.set_xlabel('False Positive Rate')</p><p class="source-code">ax.set_ylabel('Sensitivity')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 10.6 – ROC curve " height="444" src="image/B17978_10_006.jpg" width="560"/>
</div>
</div>
<p class="figure-caption">Figure 10.6 – ROC curve</p>
<p>Here, we can <a id="_idIndexMarker854"/>see that increasing the false positive rate buys us less increase in sensitivity the higher the false positive rate is. Beyond a false positive rate of 0.5, there is not much payoff at all.</p>
<ol>
<li value="9">It may also be helpful to just plot the false positive rate and sensitivity by threshold:<p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(ths, fpr, label="False Positive Rate")</p><p class="source-code">ax.plot(ths, tpr, label="Sensitivity")</p><p class="source-code">ax.set_title('False Positive Rate and Sensitivity by Threshold')</p><p class="source-code">ax.set_xlabel('Threshold')</p><p class="source-code">ax.set_ylabel('False Positive Rate and Sensitivity')</p><p class="source-code">ax.legend()</p></li>
</ol>
<p>This <a id="_idIndexMarker855"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 10.7 – Sensitivity and false positive rate " height="445" src="image/B17978_10_0071.jpg" width="565"/>
</div>
</div>
<p class="figure-caption">Figure 10.7 – Sensitivity and false positive rate</p>
<p>Here, we can see that as we lower the threshold below 0.25, the false positive rate increases more rapidly than sensitivity.</p>
<p>These last two visualizations hint at the possibility of finding an optimal threshold value – that is, one with the best tradeoff between sensitivity and the false positive rate; at least mathematically, ignoring domain knowledge.</p>
<ol>
<li value="10"> We <a id="_idIndexMarker856"/>will calculate the <strong class="bold">Youden J</strong> statistic to <a id="_idIndexMarker857"/>find this threshold value. We get this by passing a vector, which is the difference between the true positive and false positive rates at each threshold, to NumPy’s <strong class="source-inline">argmax</strong> function. We want the value of the threshold at that index. The optimal threshold according to this calculation is 0.46, which isn’t very different from the default:<p class="source-code">jthresh = ths[np.argmax(tpr – fpr)]</p><p class="source-code">jthresh</p><p class="source-code"><strong class="bold">0.45946882675453804</strong></p></li>
<li> We can redo the confusion matrix based on this alternative threshold:<p class="source-code">pred2 = np.where(pred_probs&gt;=jthresh,1,0)</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred2)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(</p><p class="source-code">  confusion_matrix=cm, </p><p class="source-code">  display_labels=['Negative', 'Positive'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(</p><p class="source-code">  title='Heart Disease Prediction Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 10.8 – Confusion matrix of heart disease prediction  " height="442" src="image/B17978_10_008.jpg" width="567"/>
</div>
</div>
<p class="figure-caption">Figure 10.8 – Confusion matrix of heart disease prediction </p>
<ol>
<li value="12">This gives us a <a id="_idIndexMarker858"/>small improvement in sensitivity:<p class="source-code">skmet.recall_score(y_test.values.ravel(), pred)</p><p class="source-code"><strong class="bold">0.7935222672064778</strong></p><p class="source-code">skmet.recall_score(y_test.values.ravel(), pred2)</p><p class="source-code"><strong class="bold">0.8380566801619433</strong></p></li>
</ol>
<p>The point here is not that we should change thresholds willy-nilly. This is often a bad idea. But we should keep two points in mind. First, when we have a highly imbalanced class, a 0.5 threshold may not make sense. Second, this is an important place to lean on domain knowledge. For some classification problems, a false positive is substantially less important than a false negative.</p>
<p>In this section, we focused on sensitivity, precision, and false positive rate as measures of model performance. That is partly because of space limitations, but also because of the issues with this particular target – imbalance classes and the likely preference for sensitivity. We <a id="_idIndexMarker859"/>will be emphasizing other measures, such as accuracy and specificity, in other models that we will be building in the next few chapters. In the rest of this chapter, we will look at a couple of extensions of logistic regression, regularization and multinomial logistic regression.</p>
<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Regularization with logistic regression</h1>
<p>If you have<a id="_idIndexMarker860"/> already worked your way through <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear Regression Models</em>, and read the first section of this chapter, you already have a good idea of how regularization works. We add a penalty to the estimator<a id="_idIndexMarker861"/> that minimizes our parameter estimates. The size of that penalty is typically tuned based on a measure of model performance. We will work through that in this section. Follow these steps:</p>
<ol>
<li value="1">We will load the same modules that we worked with in the previous section, plus the modules we will need for the necessary hyperparameter tuning. We will use <strong class="source-inline">RandomizedSearchCV</strong> and <strong class="source-inline">uniform</strong> to find the best value for our penalty strength:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import RepeatedStratifiedKFold</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans,\</p><p class="source-code">  MakeOrdinal, ReplaceVals</p></li>
<li>Next, we <a id="_idIndexMarker862"/>will load the heart disease <a id="_idIndexMarker863"/>data and do a little processing:<p class="source-code">healthinfo = pd.read_csv("data/healthinfosample.csv")</p><p class="source-code">healthinfo.set_index("personid", inplace=True)</p><p class="source-code">healthinfo['heartdisease'] = \</p><p class="source-code">  np.where(healthinfo.heartdisease=='No',0,1).\</p><p class="source-code">  astype('int')</p></li>
<li>Next, we will organize our features to facilitate the column transformation we will do in a couple of steps:<p class="source-code">num_cols = ['bmi','physicalhealthbaddays',</p><p class="source-code">   'mentalhealthbaddays','sleeptimenightly']</p><p class="source-code">binary_cols = ['smoking','alcoholdrinkingheavy',</p><p class="source-code">  'stroke','walkingdifficult','physicalactivity',</p><p class="source-code">  'asthma','kidneydisease','skincancer']</p><p class="source-code">cat_cols = ['gender','ethnicity']</p><p class="source-code">spec_cols1 = ['agecategory']</p><p class="source-code">spec_cols2 = ['genhealth']</p><p class="source-code">spec_cols3 = ['diabetic']</p><p class="source-code">rep_dict = {</p><p class="source-code">  'genhealth': {'Poor':0,'Fair':1,'Good':2,</p><p class="source-code">    'Very good':3,'Excellent':4},</p><p class="source-code">  'diabetic': {'No':0,</p><p class="source-code">    'No, borderline diabetes':0,'Yes':1,</p><p class="source-code">    'Yes (during pregnancy)':1}           </p><p class="source-code">}</p></li>
<li>Now, we <a id="_idIndexMarker864"/>must create testing and<a id="_idIndexMarker865"/> training DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(healthinfo[num_cols + </p><p class="source-code">    binary_cols + cat_cols + spec_cols1 +</p><p class="source-code">    spec_cols2 + spec_cols3],\</p><p class="source-code">  healthinfo[['heartdisease']], test_size=0.2,</p><p class="source-code">    random_state=0)</p></li>
<li>Then, we must set up the column transformations:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(3),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  StandardScaler())</p><p class="source-code">spectrans1 = make_pipeline(MakeOrdinal(),</p><p class="source-code">  StandardScaler())</p><p class="source-code">spectrans2 = make_pipeline(ReplaceVals(rep_dict),</p><p class="source-code">  StandardScaler())</p><p class="source-code">spectrans3 = make_pipeline(ReplaceVals(rep_dict))</p><p class="source-code">bintrans = make_pipeline(ohe)</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">    ("spec1", spectrans1, spec_cols1),</p><p class="source-code">    ("spec2", spectrans2, spec_cols2),</p><p class="source-code">    ("spec3", spectrans3, spec_cols3),</p><p class="source-code">    ("bin", bintrans, binary_cols),</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Now, we <a id="_idIndexMarker866"/>are ready to run our model. We <a id="_idIndexMarker867"/>will instantiate logistic regression and repeated stratified k-fold objects. Then, we will create a pipeline with our column transformation from the previous step and the logistic regression. </li>
</ol>
<p>After that, we will create a list of dictionaries for our hyperparameters, rather than just one dictionary, as we have done previously in this book. This is because not all hyperparameters work together. For example, we cannot use an L1 penalty with a <strong class="source-inline">newton-cg</strong> solver. The <strong class="source-inline">logisticregression__</strong> (note the double underscore) prefix to the dictionary key names indicates that we want the values to be passed to the logistic regression step of our pipeline.</p>
<p>We will set the <strong class="source-inline">n_iter</strong> parameter to <strong class="source-inline">20</strong> for our randomized grid search to get it to sample<a id="_idIndexMarker868"/> hyperparameters 20 times. Each <a id="_idIndexMarker869"/>of those times, the grid search will select from the hyperparameters listed in one of the dictionaries. We will indicate that we want the grid search scoring to be based on the area under the ROC curve:</p>
<p class="source-code">lr = LogisticRegression(random_state=1, class_weight='balanced', max_iter=1000)</p>
<p class="source-code">kf = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=0)</p>
<p class="source-code">pipe1 = make_pipeline(coltrans, lr)</p>
<p class="source-code">reg_params = [</p>
<p class="source-code">  {</p>
<p class="source-code">    'logisticregression__solver': ['liblinear'],</p>
<p class="source-code">    'logisticregression__penalty': ['l1','l2'],</p>
<p class="source-code">    'logisticregression__C': uniform(loc=0, scale=10)</p>
<p class="source-code">  },</p>
<p class="source-code">  {</p>
<p class="source-code">    'logisticregression__solver': ['newton-cg'],</p>
<p class="source-code">    'logisticregression__penalty': ['l2'],</p>
<p class="source-code">    'logisticregression__C': uniform(loc=0, scale=10)</p>
<p class="source-code">  },</p>
<p class="source-code">  {</p>
<p class="source-code">    'logisticregression__solver': ['saga'],</p>
<p class="source-code">    'logisticregression__penalty': ['elasticnet'],</p>
<p class="source-code">    'logisticregression__l1_ratio': uniform(loc=0, scale=1),   </p>
<p class="source-code">    'logisticregression__C': uniform(loc=0, scale=10)</p>
<p class="source-code">  }</p>
<p class="source-code">]</p>
<p class="source-code">rs = RandomizedSearchCV(pipe1, reg_params, cv=kf, </p>
<p class="source-code">  n_iter=20, scoring='roc_auc')</p>
<p class="source-code">rs.fit(X_train, y_train.values.ravel())</p>
<ol>
<li value="7">After <a id="_idIndexMarker870"/>fitting the <a id="_idIndexMarker871"/>search, the <strong class="source-inline">best_params</strong> attribute gives us the parameters associated with the highest score. Elastic net regression, with an L1 ratio closer to L1 than to L2, performs the best:<p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'logisticregression__C': 0.6918282397356423,</strong></p><p class="source-code"><strong class="bold"> 'logisticregression__l1_ratio': 0.758705704020254,</strong></p><p class="source-code"><strong class="bold"> 'logisticregression__penalty': 'elasticnet',</strong></p><p class="source-code"><strong class="bold"> 'logisticregression__solver': 'saga'}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.8410275986723489</strong></p></li>
<li>Let’s look at some of the other top scores from the grid search. The best three models have pretty much the same score. One uses elastic net regression, another L1, and another L2.</li>
</ol>
<p>The <strong class="source-inline">cv_results_</strong> dictionary of the grid search provides us with lots of information about <a id="_idIndexMarker872"/>the 20 models that were tried. The <strong class="source-inline">params</strong> list in that dictionary <a id="_idIndexMarker873"/>has a somewhat complicated structure because some keys are not present for some iterations, such as <strong class="source-inline">L1_ratio</strong>. We can use <strong class="source-inline">json_normalize</strong> to flatten the structure:</p>
<p class="source-code">results = \</p>
<p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p>
<p class="source-code">    columns=['meanscore']).\</p>
<p class="source-code">  join(pd.json_normalize(rs.cv_results_['params'])).\</p>
<p class="source-code">  sort_values(['meanscore'], ascending=False)</p>
<p class="source-code">results.head(3).T</p>
<p class="source-code"><strong class="bold">                              15          4      12</strong></p>
<p class="source-code"><strong class="bold">meanscore                     0.841       0.841  0.841</strong></p>
<p class="source-code"><strong class="bold">logisticregression__C         0.692       1.235  0.914</strong></p>
<p class="source-code"><strong class="bold">logisticregression__l1_ratio  0.759       NaN    NaN</strong></p>
<p class="source-code"><strong class="bold">logisticregression__penalty   elasticnet  l1     l2</strong></p>
<p class="source-code"><strong class="bold">logisticregression__solver  saga  liblinear  liblinear</strong></p>
<ol>
<li value="9">Let’s take a look at the confusion matrix:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = \</p><p class="source-code">  skmet.ConfusionMatrixDisplay(confusion_matrix=cm,</p><p class="source-code">  display_labels=['Negative', 'Positive'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.\</p><p class="source-code">  set(title='Heart Disease Prediction Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This <a id="_idIndexMarker874"/>generates <a id="_idIndexMarker875"/>the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="Figure 10.9 – Confusion matrix of heart disease prediction " height="446" src="image/B17978_10_009.jpg" width="562"/>
</div>
</div>
<p class="figure-caption">Figure 10.9 – Confusion matrix of heart disease prediction</p>
<ol>
<li value="10">Let’s also look at some metrics. Our scores are largely unchanged from our model without regularization:<p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred,</p><p class="source-code">    pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.74, sensitivity: 0.79, specificity: 0.74, precision: 0.21</strong></p></li>
</ol>
<p>Even though <a id="_idIndexMarker876"/>regularization provided no obvious improvement in the performance of our model, there are many times when it does. It is also not as necessary to worry about feature selection when using L1 regularization, as the weights for less important features will be 0.</p>
<p>We still haven’t dealt with how to handle models where the target has more than two possible values, though almost all the discussion in the last two sections applies to multiclass models as well. In the next section, we will learn how to use multinomial logistic regression to model multiclass targets.</p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor133"/>Multinomial logistic regression</h1>
<p>Logistic<a id="_idIndexMarker877"/> regression would not be as useful if it only worked for binary classification problems. Fortunately, we can use multinomial logistic regression <a id="_idIndexMarker878"/>when our target has more than two values.</p>
<p>In this section, we will work with data on machine failures as a function of air and process temperature, torque, and rotational speed.</p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on machine failure is available for public use at <a href="https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification">https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification</a>. There are 10,000 observations, 12 features, and two possible targets. One is binary – that is, the machine failed or didn’t. The other has types of failure. The instances in this dataset are synthetic, generated by a process designed to mimic machine failure rates and causes.</p>
<p>Let’s learn how<a id="_idIndexMarker879"/> to use multinomial logistic regression<a id="_idIndexMarker880"/> to model machine failure:</p>
<ol>
<li value="1">First, we will import the now-familiar libraries. We will also import <strong class="source-inline">cross_validate</strong>, which we first used in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for</em> <em class="italic">Model Evaluation</em>, to help us evaluate our model:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import RepeatedStratifiedKFold</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.model_selection import cross_validate</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We will <a id="_idIndexMarker881"/>load the machine failure data and take a look at its structure. We do not have any<a id="_idIndexMarker882"/> missing data. That’s great news:<p class="source-code">machinefailuretype = pd.read_csv("data/machinefailuretype.csv")</p><p class="source-code">machinefailuretype.info()</p><p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p><p class="source-code">RangeIndex: 10000 entries, 0 to 9999</p><p class="source-code">Data columns (total 10 columns):</p><p class="source-code"><strong class="bold"> #   Column              Non-Null Count      Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------             --------------       ----  </strong></p><p class="source-code"><strong class="bold"> 0   udi                 10000 non-null      int64  </strong></p><p class="source-code"><strong class="bold"> 1   product             10000 non-null      object </strong></p><p class="source-code"><strong class="bold"> 2   machinetype         10000 non-null      object </strong></p><p class="source-code"><strong class="bold"> 3   airtemp             10000 non-null      float64</strong></p><p class="source-code"><strong class="bold"> 4   processtemperature  10000 non-null      float64</strong></p><p class="source-code"><strong class="bold"> 5   rotationalspeed     10000 non-null      int64  </strong></p><p class="source-code"><strong class="bold"> 6   torque              10000 non-null      float64</strong></p><p class="source-code"><strong class="bold"> 7   toolwear            10000 non-null      int64  </strong></p><p class="source-code"><strong class="bold"> 8   fail                10000 non-null      int64  </strong></p><p class="source-code"><strong class="bold"> 9   failtype            10000 non-null      object </strong></p><p class="source-code"><strong class="bold">dtypes: float64(3), int64(4), object(3)</strong></p><p class="source-code"><strong class="bold">memory usage: 781.4+ KB</strong></p></li>
<li>Let’s look at a few rows. <strong class="source-inline">machinetype</strong> has values of <strong class="source-inline">L</strong>, <strong class="source-inline">M</strong>, and <strong class="source-inline">H</strong>. These values are proxies for machines of low, medium, and high quality, respectively:<p class="source-code">machinefailuretype.head()</p><p class="source-code"><strong class="bold">   udi product machinetype airtemp processtemperature\</strong></p><p class="source-code"><strong class="bold">0  1   M14860  M           298     309 </strong></p><p class="source-code"><strong class="bold">1  2   L47181  L           298     309 </strong></p><p class="source-code"><strong class="bold">2  3   L47182  L           298     308 </strong></p><p class="source-code"><strong class="bold">3  4   L47183  L           298     309 </strong></p><p class="source-code"><strong class="bold">4  5   L47184  L           298     309 </strong></p><p class="source-code"><strong class="bold">   Rotationalspeed  torque  toolwear  fail  failtype  </strong></p><p class="source-code"><strong class="bold">0   1551             43       0        0    No Failure</strong></p><p class="source-code"><strong class="bold">1   1408             46       3        0    No Failure</strong></p><p class="source-code"><strong class="bold">2   1498             49       5        0    No Failure</strong></p><p class="source-code"><strong class="bold">3   1433             40       7        0    No Failure</strong></p><p class="source-code"><strong class="bold">4   1408             40       9        0    No Failure</strong></p></li>
<li>We<a id="_idIndexMarker883"/> should also generate some <a id="_idIndexMarker884"/>frequencies:<p class="source-code">machinefailuretype.failtype.value_counts(dropna=False).sort_index()</p><p class="source-code"><strong class="bold">Heat Dissipation Failure    112</strong></p><p class="source-code"><strong class="bold">No Failure                  9652</strong></p><p class="source-code"><strong class="bold">Overstrain Failure          78</strong></p><p class="source-code"><strong class="bold">Power Failure               95</strong></p><p class="source-code"><strong class="bold">Random Failures             18</strong></p><p class="source-code"><strong class="bold">Tool Wear Failure           45</strong></p><p class="source-code"><strong class="bold">Name: failtype, dtype: int64</strong></p><p class="source-code">machinefailuretype.machinetype.\</p><p class="source-code">  value_counts(dropna=False).sort_index()</p><p class="source-code"><strong class="bold">H    1003</strong></p><p class="source-code"><strong class="bold">L    6000</strong></p><p class="source-code"><strong class="bold">M    2997</strong></p><p class="source-code"><strong class="bold">Name: machinetype, dtype: int64</strong></p></li>
<li>Let’s collapse the <strong class="source-inline">failtype</strong> values and create numeric code for them. We will combine<a id="_idIndexMarker885"/> random failures and<a id="_idIndexMarker886"/> tool wear failures since the counts are so low for random failures:<p class="source-code">def setcode(typetext):</p><p class="source-code">  if (typetext=="No Failure"):</p><p class="source-code">    typecode = 1</p><p class="source-code">  elif (typetext=="Heat Dissipation Failure"):</p><p class="source-code">    typecode = 2</p><p class="source-code">  elif (typetext=="Power Failure"):</p><p class="source-code">    typecode = 3</p><p class="source-code">  elif (typetext=="Overstrain Failure"):</p><p class="source-code">    typecode = 4</p><p class="source-code">  else:</p><p class="source-code">    typecode = 5</p><p class="source-code">  return typecode</p><p class="source-code">machinefailuretype["failtypecode"] = \</p><p class="source-code">  machinefailuretype.apply(lambda x: setcode(x.failtype), axis=1)</p></li>
<li>We should confirm that <strong class="source-inline">failtypecode</strong> does what we intended:<p class="source-code">machinefailuretype.groupby(['failtypecode','failtype']).size().\</p><p class="source-code">  reset_index()</p><p class="source-code"><strong class="bold">  failtypecode   failtype                     0</strong></p><p class="source-code"><strong class="bold">0     1          No Failure                   9652</strong></p><p class="source-code"><strong class="bold">1     2          Heat Dissipation Failure     112</strong></p><p class="source-code"><strong class="bold">2     3          Power Failure                95</strong></p><p class="source-code"><strong class="bold">3     4          Overstrain Failure           78</strong></p><p class="source-code"><strong class="bold">4     5          Random Failures              18</strong></p><p class="source-code"><strong class="bold">5     5          Tool Wear Failure            45</strong></p></li>
<li>Let’s <a id="_idIndexMarker887"/>also get some descriptive<a id="_idIndexMarker888"/> statistics:<p class="source-code">num_cols = ['airtemp','processtemperature','rotationalspeed',</p><p class="source-code">  'torque','toolwear']</p><p class="source-code">cat_cols = ['machinetype']</p><p class="source-code">machinefailuretype[num_cols].agg(['min','median','max']).T</p><p class="source-code"><strong class="bold">                      min      median    max</strong></p><p class="source-code"><strong class="bold">airtemp               295      300       304</strong></p><p class="source-code"><strong class="bold">processtemperature    306      310       314</strong></p><p class="source-code"><strong class="bold">rotationalspeed       1,168    1,503     2,886</strong></p><p class="source-code"><strong class="bold">torque                4        40        77</strong></p><p class="source-code"><strong class="bold">toolwear              0        108       253</strong></p></li>
<li>Now, let’s create the testing and training DataFrames. We will also set up the column transformations:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(machinefailuretype[num_cols +</p><p class="source-code">  cat_cols], machinefailuretype[['failtypecode']],</p><p class="source-code">  test_size=0.2, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(3),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  StandardScaler())</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Now, let’s <a id="_idIndexMarker889"/>set <a id="_idIndexMarker890"/>up a pipeline with our column transformations and our multinomial logistic regression model We just need to set the <strong class="source-inline">multi_class</strong> attribute to multinomial when we instantiate the logistic regression:<p class="source-code">lr = LogisticRegression(random_state=0, </p><p class="source-code">  multi_class='multinomial', solver='lbfgs',</p><p class="source-code">  max_iter=1000)</p><p class="source-code">kf = RepeatedStratifiedKFold(n_splits=10,</p><p class="source-code">  n_repeats=5, random_state=0)</p><p class="source-code">pipe1 = make_pipeline(coltrans, lr)</p></li>
<li> Now, we<a id="_idIndexMarker891"/> can generate a confusion<a id="_idIndexMarker892"/> matrix:<p class="source-code">cm = skmet.confusion_matrix(y_test, </p><p class="source-code">   pipe1.fit(X_train, y_train.values.ravel()).\</p><p class="source-code">   predict(X_test))</p><p class="source-code">cmplot = \</p><p class="source-code">   skmet.ConfusionMatrixDisplay(confusion_matrix=cm,</p><p class="source-code">   display_labels=['None', 'Heat','Power','Overstrain','Other'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.\</p><p class="source-code">  set(title='Machine Failure Type Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 10.10 – Confusion matrix of predicted machine failure types " height="442" src="image/B17978_10_0101.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 10.10 – Confusion matrix of predicted machine failure types</p>
<p>The <a id="_idIndexMarker893"/>confusion matrix shows that <a id="_idIndexMarker894"/>our model does not do a good job of predicting the failure type when there is a failure, particularly with power failures or other failures.</p>
<ol>
<li value="11"> We can use <strong class="source-inline">cross_validate</strong> to evaluate this model. We mainly get excellent scores for accuracy, precision, and sensitivity (recall). However, this is misleading. The weighted scores when the classes are so imbalanced (almost all of the instances have <strong class="source-inline">no failure</strong>) are very heavily influenced by the class that contains almost all of the values. Our model gets <strong class="source-inline">no failure</strong> correct reliably.</li>
</ol>
<p>If we look at the <strong class="source-inline">f1_macro</strong> score (recall from <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for</em> <em class="italic">Model Evaluation</em>, that <strong class="source-inline">f1</strong> is the harmonic mean of precision and sensitivity), we will see that our model does not do very well for classes other than the <strong class="source-inline">no failure</strong> class. (The <strong class="source-inline">macro</strong> score is just a simple average.)</p>
<p>We could have just used a classification report here, as we did in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for</em> <em class="italic">Model Evaluation</em>, but I<a id="_idIndexMarker895"/> sometimes find it <a id="_idIndexMarker896"/>helpful to generate the stats I need:</p>
<p class="source-code">scores = cross_validate(</p>
<p class="source-code">  pipe1, X_train, y_train.values.ravel(), \</p>
<p class="source-code">  scoring=['accuracy', 'precision_weighted',</p>
<p class="source-code">           'recall_weighted', 'f1_macro',</p>
<p class="source-code">           'f1_weighted'], </p>
<p class="source-code">  cv=kf, n_jobs=-1)</p>
<p class="source-code">accuracy, precision, sensitivity, f1_macro, f1_weighted = \</p>
<p class="source-code">  np.mean(scores['test_accuracy']),\</p>
<p class="source-code">  np.mean(scores['test_precision_weighted']),\</p>
<p class="source-code">  np.mean(scores['test_recall_weighted']),\</p>
<p class="source-code">  np.mean(scores['test_f1_macro']),\</p>
<p class="source-code">  np.mean(scores['test_f1_weighted'])</p>
<p class="source-code">accuracy, precision, sensitivity, f1_macro, f1_weighted</p>
<p class="source-code"><strong class="bold">(0.9716499999999999,</strong></p>
<p class="source-code"><strong class="bold"> 0.9541025493784612,</strong></p>
<p class="source-code"><strong class="bold"> 0.9716499999999999,</strong></p>
<p class="source-code"><strong class="bold"> 0.3820938909478524,</strong></p>
<p class="source-code"><strong class="bold"> 0.9611411229222823)</strong></p>
<p>In this section, we explored how to construct a multinomial logistic regression model. This approach works regardless of whether the target is nominal or ordinal. In this case, it was nominal. We<a id="_idIndexMarker897"/> also saw how we can extend the model evaluation approaches we used for logistic regression <a id="_idIndexMarker898"/>with a binary target. We reviewed how to interpret the confusion matrix and scoring metrics when we have more than two classes.</p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Summary</h1>
<p>Logistic regression has been a go-to tool for me for many, many years when I have needed to predict a categorical target. It is an efficient algorithm with low bias. Some of its disadvantages, such as high variance and difficulty handling highly correlated predictors, can be addressed with regularization and feature selection. We went over examples of doing that in this chapter. We also examined how to handle imbalanced classes in terms of what such targets mean for modeling and interpretation of results.</p>
<p>In the next chapter, we will look at a very popular alternative to logistic regression for classification – decision trees. We will see that decision trees have many advantages that make them a particularly good option if we need to model complexity, without having to worry as much about how our features are specified as we do with logistic regression.</p>
</div>
</div>
</body></html>