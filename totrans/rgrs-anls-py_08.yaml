- en: Chapter 8. Advanced Regression Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce some advanced regression methods. Since
    many of them are very complex, we will skip most of the mathematical formulations,
    providing the readers instead with the ideas underneath the techniques and some
    practical advice, such as explaining when and when not to use the technique. We
    will illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: Least Angle Regression (LARS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGD classification with hinge loss (note that this is not a regressor, it's
    a classifier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble of regressors (bagging and boosting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boosting Regressor with Least Angle Deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least Angle Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although very similar to Lasso (seen in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*), Least Angle
    Regression, or simply LARS, is a regression algorithm that, in a fast and smart
    way, selects the best features to use in the model, even though they're very closely
    correlated to each other. LARS is an evolution of the Forward Selection (also
    called Forward Stepwise Regression) algorithm and of the Forward Stagewise Regression
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how the Forward Selection algorithm works, based on the hypothesis
    that all the variables, including the target one, have been previously normalized:'
  prefs: []
  type: TYPE_NORMAL
- en: Of all the possible predictors for a problem, the one with the largest absolute
    correlation with the target variable *y* is selected (that is, the one with the
    most explanatory capability). Let's call it *p[1]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the other predictors are now projected onto *p[1]* Least Angle Regression,
    and the projection is removed, creating a vector of residuals orthogonal to *p[1]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 1 is repeated on the residual vectors, and the most correlated predictor
    is again selected. Let's name it *p²* Apply subscript.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 2 is repeated, using *p[2]*, creating a vector of residuals orthogonal
    to *p[2]* (and also *p[1]*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process continues until the prediction is satisfying, or when the largest
    absolute correlation falls below a set threshold. After each iteration, a new
    predictor is added to the list of predictors, and the residual is orthogonal to
    all of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This method is not very popular because it has a serious limitation due to
    its extremely greedy approach; however, it''s fairly quick. Let''s now consider
    that we have a regression problem with two highly correlated variables. Forward
    Selection, on this dataset, will select the predictor on the basis of the first
    or the second variable, and then, since the residual will be very low, will reconsider
    the other variable in a far later step (eventually, never). This fact will lead
    to overfitting problems on the model. Wouldn''t it be better if the two highly
    correlated variables were selected together, balancing the new predictor? That''s
    practically the core idea of the Forward Stagewise Regression algorithm, where,
    in each step, the best predictor is partially added to the model. Let''s provide
    the details here:'
  prefs: []
  type: TYPE_NORMAL
- en: In the model, every feature has an associate weight of zero—that is, *w[i] =
    0* for each feature *i*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of all the possible predictors for a problem, the one with the largest (absolute)
    correlation with the target variable *y* is partially added to the model—that
    is, in the model, the weight of *w[i]* is increased by *ε*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2, until the exploratory power is below a predefined threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This method represents a great improvement on the Forward Selected because,
    in the case of correlated features, both of them will be in the final model with
    a similar weight. The result is very good, but the enormous number of iterations
    needed to create the model is the really big problem with this algorithm. Again,
    the method becomes impractical because of its running time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LARS algorithm instead operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the model, every feature has an associate weight of zero—that is, *w[i] =
    0* for each feature *i*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of the possible predictors for a problem, the one with the largest (absolute)
    correlation with the target variable *y* is partially added to the model—that
    is, in the model, the weight of *w[i]* is increased by *ε*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep increasing *w[i]* till any other predictor (let's say *j*) has as much
    correlation with the residual vector as the current predictor has.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase *w[i]* and *w[j]* simultaneously until another predictor has as much
    correlation with the residual vector as the current predictors have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep adding predictors and weights until all the predictors are in the model
    or it meets another termination criterion, such as the number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This solution is able to compose the best pieces of Forward Selection and Stagewise
    Regression, creating a solution that is stable, not so prone to overfitting, and
    fast. Before getting to the examples, you may wonder why it is named Least Angle
    Regression. The answer is very simple: if the features and output are represented
    as vectors in the Cartesian space, at every iteration LARS includes in the model
    the variable most correlated with the residual vector, which is the one that generates
    the least angle with the residual. Actually, the whole process can be expressed
    visually.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual showcase of LARS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Visual showcase of LARS](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the visual situation: two predictors (**x1** and **x2**), not necessarily
    orthogonal, and the target (**y**). Note that, at the beginning, the residual
    corresponds to the target. Our model starts at **u0** (where all the weights are
    *0*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, since **x2** makes a smaller angle with the residual compared to **x1**,
    we start *walking* in the direction of **x2**, while we keep computing the residual
    vector. Now, a question: where should we stop?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visual showcase of LARS](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We should stop at **u1**, where the angle between the residual and **x1** is
    the same as the angle between the residual and **x2**. We then walk in the direction
    of the composition **x1** and **x2**, reaching **y**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visual showcase of LARS](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A code example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now see LARS in action in Python on the Diabetic dataset, which consists
    of 10 numerical variables (age, sex, weight, blood pressure, and so on) measured
    on 442 patients, and an indication of disease progression after one year. First,
    we want to visualize the path of the weights of the coefficients. To do so, the
    `lars_path()` class comes to our help (especially if its training is verbose):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![A code example](img/00123.jpeg)![A code example](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the output table, you can see that the first feature inserted in the model
    is the number 2, followed by the number 8 and so on. In the image, instead, you
    can simultaneously see the values of the coefficients (colored lines) and the
    steps (dotted lines). Remember that, at every step, one coefficient becomes non-zero,
    and all the coefficients in the model are updated linearly. On the right side
    of the image, you can find the final values of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the graphical way to see the LARS coefficients; if we only need a regressor
    (exactly as we''ve seen in the previous chapters), we can just use the `Lars`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may expect, the regressor object can be fitted with the method `.fit`,
    and its weights (coefficients) are exactly the ones shown in the previous screenshot.
    To get the quality of the model, in a similar fashion to the other regressors,
    you can use the method score. In respect of the training data, here''s the scoring
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: LARS wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: The smart way in which coefficients are updated produces low overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is intuitive and easily interpretable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training is as fast as Forward Selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is great when the number of features is comparable with, or greater than,
    the number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: It might not work very well when the number of features is very large—that is,
    where the number of features is far greater than the number of observations, since
    in such an occurrence it's very probable you'll find spurious correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It won't work with very noisy features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bayesian regression is similar to linear regression, as seen in [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*,
    but, instead of predicting a value, it predicts its probability distribution.
    Let''s start with an example: given `X`, the training observation matrix, and
    `y`, the target vector, linear regression creates a model (that is a series of
    coefficients) that fits the line that has the minimal error with the training
    points. Then, when a new observation arrives, the model is applied to that point,
    and a predicted value is outputted. That''s the only output from linear regression,
    and no conclusions can be made as to whether the prediction, for that specific
    point, is accurate or not. Let''s take a very simple example in code: the observed
    phenomenon has only one feature, and the number of observations is just `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s fit a *classic* linear regression model, and let''s try to predict
    the regression value for a point outside the training support (in this simple
    example, we predict the value for a point whose `x` value is double the max of
    the training values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now plot the training points, the fitted line, and the predicted test
    point (on the extreme right of the image):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Bayesian regression](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To have a probability density function of the predicted value, we should start
    from the beginning and change a hypothesis and some steps in the linear regressor.
    Since this is an advanced algorithm, the math involved is very heavy and we prefer
    to communicate the idea underlying the methods, instead of exposing pages and
    pages of math formulation.
  prefs: []
  type: TYPE_NORMAL
- en: First, we are only able to infer a distribution on the predicted value if every
    variable is modeled as a distribution. In fact, weights in this model are treated
    as random variables with a normal distribution, centered in zero (that is, a spherical
    Gaussian) and having an unknown variance (learnt from the data). The regularization
    imposed by this algorithm is very similar to the one set by Ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of a prediction is a value (exactly as in linear regression) and
    a variance value. Using the value as the mean, and the variance as an actual variance,
    we can then represent the probability distribution of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Bayesian regression](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian regression wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: Robustness to Gaussian noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great if the number of features is comparable to the number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: Time-consuming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hypotheses imposed on the variables are often far from real
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGD classification with hinge loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 4. Logistic Regression"), *Logistic Regression* we explored a classifier
    based on a regressor, logistic regression. Its goal was to fit the best probabilistic
    function associated with the probability of one point to be classified with a
    label. Now, the core function of the algorithm considers all the training points
    of the dataset: what if it''s only built on the boundary ones? That''s exactly
    the case with the linear **Support Vector Machine** (SVM) classifier, where a
    linear decision plane is drawn by only considering the points close to the separation
    boundary itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond working on the support vectors (the closest points to the boundary),
    SVM uses a new decision loss, called **hinge**. Here''s its formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SGD classification with hinge loss](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where t is the intended label of the point x and w the set of weights in the
    classifier. The hinge loss is also sometimes called **softmax**, because it's
    actually a clipped max. In this formula, just the boundary points (that is, the
    support vectors) are used.
  prefs: []
  type: TYPE_NORMAL
- en: In the first instance, this function, although convex, is non differentiable,
    so approaches based on stochastic gradient descent (SGD) are theoretically invalid.
    In practical terms, since it's a continuous function, it has a piecewise derivative.
    This leads to the fact that SGD can be actively used in this technique to derive
    a quick and approximate solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example in Python: let''s use the `SGDClassifier` class (as seen
    in [Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 4. Logistic Regression"), *Logistic Regression*) with the `hinge` loss,
    applied on a dataset of `100` points drawn from `2` classes. With this piece of
    code, we''re interested in seeing the decision boundary and the support vectors
    chosen by the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![SGD classification with hinge loss](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The image presents the points belonging to the two classes' points (the dots
    on the right and left) and the decision boundary (the solid line between the classes).
    In addition, it contains two dotted lines, which connect the support vectors for
    each class (that is, points on these lines are support vectors). The decision
    boundary is, simply, the line at the same distance between them.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logistic regression learner is intended to make use of all the input points
    of the training set, and emit a probability as output. SGD with hinge loss, instead,
    directly produces a label, and only uses the points on the boundary to improve
    the model. How are their performances? Let''s make a test with an artificial dataset
    with 20 features (of them, 5 are informative, 5 redundant, and 10 random) and
    10,000 observations. Then, we split the data into 70/30 as training set and test
    set and we train two SGD classifiers: one with the hinge loss function and the
    second with the logistic loss function. Finally, we compare the accuracy of their
    predictions on their test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As a rule of thumb, SVM is generically more accurate than logistic regression,
    but its performance is not extraordinary. SVM, though, is slower during the training
    process; in fact, with regard to training times, logistic regression is more than
    30% faster than SVM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: SVR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As for linear regressor/logistic regression, even SVM has a regression counterpart,
    called **Support Vector Regressor** (SVR). Its math formulation is very long and
    beyond the scope of this book. However, since it''s very effective, we believe
    it is important to depict how it works in practice, as applied to the Boston dataset
    and compared with a linear regressor model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: SVM wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pros are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Can use SGD to speed up the processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output is usually more accurate than logistic regression (since only boundary
    points are in the formula)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It works very well if the points of the two classes are linearly separable,
    although an extension for non-linearly separable classes is available. In this
    case, though complexity is very high, results are still usually great.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for logistic regression, it can be used for two-class problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression trees (CART)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very common learner, recently used very much due to its speed, is the regression
    tree. It's a non-linear learner, can work with both categorical and numerical
    features, and can be used alternately for classification or regression; that's
    why it's often called **Classification and Regression Tree** (CART). Here, in
    this section, we will see how regression trees work.
  prefs: []
  type: TYPE_NORMAL
- en: A tree is composed of a series of nodes that split the branch into two children.
    Each branch, then, can go in another node, or remain a leaf with the predicted
    value (or class).
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the root (that is, the whole dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best feature with which to split the dataset, *F1*, is identified as well
    as the best splitting value. If the feature is numerical, the splitting value
    is a threshold *T1*: in this case, the left child branch will be the set of observations
    where *F1* is below *T1*, and the right one is the set of observations where *F1*
    is greater than, or equal to, *T1*. If the feature is categorical, the splitting
    is done on a subset of levels *S1*: observations where the *F1* feature is one
    of these levels compose the left branch child, all the others compose the right
    branch child.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This operation is then run again (independently) for each branch, recursively,
    until there's no more chance to split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the splits are completed, a leaf is created. Leaves denote output values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can immediately see that making the prediction is immediate: you just need
    to traverse the tree from the root to the leaves and, in each node, check whether
    a feature is below (or not) a threshold or, alternatively, has a value inside
    (or outside) a set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concluding remark, we discuss how to define the best feature to split.
    What about the best value or subset? Well, for regression trees, we use the criteria
    of the variance reduction: in each node, an extensive search is run among all
    features and among all values or levels in that feature. The combination that
    achieves the best possible variance in both the right branch and left branches,
    compared with the input set, is selected and marked as *best*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that regression trees decide, for each node, the optimal split. Such a
    local optimization approach unfortunately leads to a suboptimal result. In addition,
    it is advisable that the regression tree should be pruned; that is, you should
    remove some leaves to prevent overfitting (for example, by setting a minimum threshold
    to the variance reduction measure). Such are the drawbacks of regression trees.
    On the other hand, they are somehow accurate and relatively quick to train and
    test.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, regression trees are as easy as the other regressors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Regression tree wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: They can model non-linear behaviors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great for categorical features and numerical features, without normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same approach for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast training, fast prediction time, and small memory fingerprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Greedy algorithm: it doesn''t optimize the full solution, just the best choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn't work very well when the number of features is significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaves can be very specific. In this case, we need to "prune the tree", removing
    some nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging and boosting are two techniques used to combine learners. These techniques
    are classified under the generic name of **ensembles** (or meta-algorithm) because
    the ultimate goal is actually to ensemble *weak* learners to create a more sophisticated,
    but more accurate, model. There is no formal definition of a weak learner, but
    ideally it's a fast, sometimes linear model that not necessarily produces excellent
    results (it suffices that they are just better than a random guess). The final
    ensemble is typically a non-linear learner whose performance increases with the
    number of weak learners in the model (note that the relation is strictly non-linear).
    Let's now see how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bagging stands for **Bootstrap Aggregating**, and its ultimate goal is to reduce
    variance by averaging weak learners'' results. Let''s now see the code; we will
    explain how it works. As a dataset, we will reuse the Boston dataset (and its
    validation split) from the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BaggingRegressor` class, from the `submodule` ensemble of Scikit-learn,
    is the base class to create bagging regressors. It requires the weak learner (in
    the example, it''s a `SGDRegressor`), the total number of regressors (1,000),
    and the maximum number of features to be used in each regressor (80% of the total
    number). Then, the bagging learner is trained as with the other learners seen
    so far, with the method fit. At this point, for each weak learner:'
  prefs: []
  type: TYPE_NORMAL
- en: 80% of the features composing the *X* train dataset are selected at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weak learner is trained just on the selected features on a bootstrap with
    a replacement set of observations in the training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end, the bagging model contains 1,000 trained `SGDRegressors`. When a
    prediction is requested from the ensemble, each of the 1,000 weak learners makes
    its prediction, then the results are averaged, producing the ensemble prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that both training and prediction operations are per-weak learner;
    therefore they can be parallelized on multiple CPUs (that's why `n_jobs` is `-1`
    in the example; that is, we use all the cores).
  prefs: []
  type: TYPE_NORMAL
- en: The final result, in terms of MAE, should be better than a single `SGDRegressor`;
    on the other hand, the model is about 1,000 times more complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, ensembles are associated with decision or regression trees. In that
    case, the name of the regression ensemble changes to Random Forest Regressor (that
    is, a forest, composed of multiple trees). Since this technique is often used
    as the *default* bagging ensemble, there is an ad hoc class in Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'One additional feature of Random Forests is their ability to rank feature importance
    in the model (that is, they detect which features produce the highest variation
    of the predicted variable). Here''s the code; always remember to normalize the
    feature matrix first (we''ve already done it in the previous section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Bagging](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The list is sorted from the most important feature to the least important (for
    this ensemble). If you change the weak learner, or any other parameter, this list
    may change.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Boosting is a way to combine (ensemble) weak learners, primarily to reduce
    prediction bias. Instead of creating a pool of predictors, as in bagging, boosting
    produces a cascade of them, where each output is the input for the following learner.
    We''ll start with an example, exactly as we''ve done in the previous sub-section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `AdaBoostRegressor` class, from the `submodule` ensemble of Scikit-learn,
    is the base class to create a Boosted Regressor. As for the bagging, it requires
    the weak learner (an `SGDRegressor`), the total number of regressors (100), and
    the learning rate (0.01). Starting from an unfitted ensemble, for each weak learner
    the training is:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the training set, the cascade of already-fit learners produces a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error between the actual values and the predicted ones, multiplied by the
    learning rate, is computed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new weak learner is trained on that error set, and inserted as the last stage
    in the cascade of already trained learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the end of the training stage, the ensemble contains 100 trained `SGDRegressors`
    organized in a cascade. When a prediction is requested from the ensemble, the
    final value is a recursive operation: starting from the last stage, the output
    value is the value predicted by the previous stage plus the learning rate multiplied
    by the prediction of the current stage.'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate is similar to the one from the stochastic gradient descent.
    A smaller learning rate will require more steps to approach the results, but the
    granularity of the output will be better. A bigger rate will require fewer steps,
    but will probably approach a less accurate result.
  prefs: []
  type: TYPE_NORMAL
- en: Please note here that training and testing cannot be done independently on each
    weak learner, since to train a model you need the chain of outputs of the previous
    ones. This fact limits the CPU usage to only one, limiting the length of the cascade.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of boosting with Decision/Regression Trees, the Scikit-learn package
    offers a pre-build class called `GradientBoostingRegressor`. A short code snippet
    should suffice to demonstrate how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Even with Boosting, it is possible to rank feature importance. In fact, it''s
    the very same method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Ensemble wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pros are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Strong learners based on weak learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They enable stochastic learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The randomness of the process creates a robust solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Training time is considerable, as well as the memory footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning step (in the boosted ensemble) can be very tricky to properly set,
    similar to the update step (alpha) in the stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boosting Regressor with LAD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More than a new technique, this is an ensemble of technologies already seen
    in this book, with a new loss function, the **Least Absolute Deviations** (LAD).
    With respect to the least square function, seen in the previous chapter, with
    LAD the L1 norm of the error is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regressor learners based on LAD are typically robust but unstable, because
    of the multiple minima of the loss function (leading therefore to multiple best
    solutions). Alone, this loss function seems to bear little value, but paired with
    gradient boosting, it creates a very stable regressor, due to the fact that boosting
    overcomes LAD regression limitations. With the code, this is very simple to achieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Remember to specify to use the `'lad'` loss, otherwise the default least square
    (L²) is used. In addition, another loss function, `huber`, combines the least
    square loss and the least absolute deviation loss to create a loss function even
    more robust. To try it, just insert the string value `'huber'` instead of `'lad'`
    in the last run piece of code.
  prefs: []
  type: TYPE_NORMAL
- en: GBM with LAD wrap up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pros are that it combines the strength of a boosted ensemble to the LAD
    loss, producing a very stable and robust learner and the cons are that training
    time is very high (exactly the same as training N consecutive LAD learners, one
    after the other).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter concludes the long journey around regression methods we have taken
    throughout this book. We have seen how to deal with different kinds of regression
    modeling, how to pre-process data, and how to evaluate the results. In the present
    chapter, we glanced at some cutting-edge techniques. In the next, and last, chapter
    of the book, we apply regression in real-world examples and invite you to experiment
    with some concrete examples.
  prefs: []
  type: TYPE_NORMAL
