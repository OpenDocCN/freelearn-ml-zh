["```py\n$ cd gradflow\n$ export MLFLOW_TRACKING_URI=http://localhost:5000 \n$ make gradflow-light\n```", "```py\n    import pandas as pd\n    import mlflow\n    import xgboost as xgb\n    import mlflow.xgboost\n    import mlflow.pyfunc\n    ```", "```py\n    if __name__ == \"__main__\":\n        with mlflow.start_run(run_name=\"batch_scoring\") as run:\n            data=pd.read_csv(\"data/input.csv\",header=None)\n    ```", "```py\n            model_name = \"training-model-psystock\"\n            stage = 'Production'\n            model = mlflow.pyfunc.load_model(\n                    model_uri=f\"models:/{model_name}/{stage}\"\n            )\n    ```", "```py\n            y_probas=model.predict(data)\n    ```", "```py\n        y_preds = [1 if  y_proba > 0.5 else 0 for y_proba in y_probas]\n\n        data[len(data.columns)] =y_preds\n\n        result = data\n        result.to_csv(\"data/output.csv\")\n    ```", "```py\n    FROM continuumio/miniconda3\n    WORKDIR /batch-scoring/\n    RUN pip install mlflow==1.16.0\n    RUN pip install pandas==1.2.4\n    COPY batch_scoring.py   /batch-scoring/\n    COPY MLproject          /batch-scoring/\n    ENV MLFLOW_TRACKING_URI=http://localhost:5000\n    ENTRYPOINT [\"mlflow run . --no-conda\"]\n    ```", "```py\n    docker build . -t pystock-inference-batch\n    ```", "```py\n    docker run -i pystock-inference-batch\n    ```", "```py\ncd /gradflow/\nexport MLFLOW_TRACKING_URI=http://localhost:5000\nmlflow models serve -m \"models:/training-model-psystock/Production\" -p 6000\n```", "```py\n    FROM continuumio/miniconda3\n    WORKDIR /batch-scoring/\n    RUN pip install mlflow==1.16.0\n    ENV MLFLOW_TRACKING_URI=http://localhost:5000\n    ENTRYPOINT [\"mlflow models serve -m \"models:/training-model-psystock/Production\" -p 6000\"]\n    ```", "```py\n    docker build . -t pystock-inference-api\n    ```", "```py\n    docker run -i pystock-inference-api -p 6000:6000\n    ```", "```py\n    {\n      \"kube-context\": \"docker-for-desktop\",\n      \"repository-uri\": \"username/mlflow-kubernetes-example\",\n      \"kube-job-template-path\": \"/Users/username/path/to/kubernetes_job_template.yaml\"\n    }\n    ```", "```py\n    mlflow run . --backend kubernetes --backend-config kubernetes_config.json\n    ```", "```py\n    mlflow sagemaker build-and-push-container\n    ```", "```py\n    7777.The output should look like the following excerpt and you should be able to test your model locally:\n\n    ```", "```py\n\n    This will basically confirm that the image is working as expected and you should be able to run your API in SageMaker.\n    ```", "```py\n    aws ecr describe-images --repository-name mlflow-pyfunc \n    ```", "```py\n    export $REGION=your-aws-region\n    export $ROLE=your sagemaker-enabled-role\n    ```", "```py\n    mlflow sagemaker deploy -a pystock-api -m models:/training-model-psystock/Production –region-name $REGION -- $ROLE\n    ```", "```py\n    2021/05/08 21:09:12 INFO mlflow.sagemaker: The deployment operation completed successfully with message: \"The SageMaker endpoint was created successfully.\"\n    ```", "```py\n    aws sagemaker list-endpoints\n    ```", "```py\n    {\n        \"Endpoints\": [\n            {\n                \"EndpointName\": \"pystock-api\",\n                \"EndpointArn\": \"arn:aws:sagemaker:eu-west-1:123456789:endpoint/pystock-api\",\n                \"CreationTime\": \"2021-05-08T21:01:13.130000+02:00\",\n                \"LastModifiedTime\": \"2021-05-08T21:09:08.947000+02:00\",\n                \"EndpointStatus\": \"InService\"\n            }\n        ]\n    }\n    ```", "```py\n    import pandas\n    import boto3\n    features = pd.DataFrame([[1,0,1,1,0,1,0,1,0,1,0,1,0,1]])\n    payload = features.to_json(orient=\"split\")\n    result  = runtime.invoke_endpoint(\n                EndpointName='pystock-api', Body=payload, \n                ContentType='application/json')\n    preds = result['Body'].read().decode(\"ascii\")\n    print(preds)\n    ```", "```py\n    '[0.04279635474085808]\n    ```", "```py\n    mlflow sagemaker delete -a pystock-api --region-name $REGION\n    ```", "```py\n    2021/05/08 23:49:46 INFO mlflow.sagemaker: The deletion operation completed successfully with message: \"The SageMaker endpoint was deleted successfully.\"\n    2021/05/08 23:49:46 INFO mlflow.sagemaker: Cleaning up unused resources...\n    2021/05/08 23:49:47 INFO mlflow.sagemaker: Deleted associated endpoint configuration with arn: arn:aws:sagemaker:eu-west-1:123456789:endpoint-config/pystock-api-config-v-hznm3ttxwx-g8uavbzia\n    2021/05/08 23:49:48 INFO mlflow.sagemaker: Deleted associated model with arn: arn:aws:sagemaker:eu-west-1:123456789:model/pystock-api-model-4nly3634reqomejx1owtdg\n    ```"]