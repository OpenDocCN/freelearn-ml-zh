["```py\nimport math\nimport mldatasets\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler,\\\n                                  MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics, linear_model, tree, naive_bayes,\\\n                    neighbors, ensemble, neural_network, svm\nfrom rulefit import RuleFit\nimport statsmodels.api as sm\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nfrom interpret.perf import ROC\nimport tensorflow as tf\nfrom gaminet import GAMINet\nfrom gaminet.utils import plot_trajectory, plot_regularization,\\\n                    local_visualize, global_visualize_density,\\\n                    feature_importance_visualize\nimport matplotlib.pyplot as plt \n```", "```py\naad18_df = mldatasets.load(\"aa-domestic-delays-2018\") \n```", "```py\nprint(aad18_df.info()) \n```", "```py\nRangeIndex: 899527 entries, 0 to 899526\nData columns (total 23 columns):\nFL_NUM                  899527 non-null int64\nORIGIN                  899527 non-null object\nDEST                    899527 non-null object\nPLANNED_DEP_DATETIME    899527 non-null object\nCRS_DEP_TIME            899527 non-null int64\nDEP_TIME                899527 non-null float64\nDEP_DELAY               899527 non-null float64\nDEP_AFPH                899527 non-null float64\nDEP_RFPH                899527 non-null float64\nTAXI_OUT                899527 non-null float64\nWHEELS_OFF              899527 non-null float64\n    :          :  :    :\nWEATHER_DELAY           899527 non-null float64\nNAS_DELAY               899527 non-null float64\nSECURITY_DELAY          899527 non-null float64\nLATE_AIRCRAFT_DELAY     899527 non-null float64\ndtypes: float64(17), int64(3), object(3) \n```", "```py\naad18_df['PLANNED_DEP_DATETIME'] =\\\npd.to_datetime(aad18_df['PLANNED_DEP_DATETIME']) \n```", "```py\naad18_df['DEP_MONTH'] = aad18_df['PLANNED_DEP_DATETIME'].dt.month\naad18_df['DEP_DOW'] = aad18_df['PLANNED_DEP_DATETIME'].dt.dayofweek \n```", "```py\naad18_df = aad18_df.**drop**(['PLANNED_DEP_DATETIME'], axis=1) \n```", "```py\n#Create list with 10 hubs (with their IATA codes)\nhubs = ['CLT', 'ORD', 'DFW', 'LAX', 'MIA', 'JFK', 'LGA', 'PHL',\n        'PHX', 'DCA']\n#Boolean series for if ORIGIN or DEST are hubsis_origin_hub =\naad18_df['ORIGIN'].isin(hubs)\nis_dest_hub = aad18_df['DEST'].isin(hubs)\n#Use boolean series to set ORIGIN_HUB and DEST_HUB\naad18_df['ORIGIN_HUB'] = 0\naad18_df.loc[is_origin_hub, 'ORIGIN_HUB'] = 1\naad18_df['DEST_HUB'] = 0\naad18_df.loc[is_dest_hub, 'DEST_HUB'] = 1\n#Drop columns with codes\naad18_df = aad18_df.drop(['FL_NUM', 'ORIGIN', 'DEST'], axis=1) \n```", "```py\naad18_df.loc[aad18_df['ARR_DELAY'] > 15,\\\n['ARR_DELAY','CARRIER_DELAY']].head(10) \n```", "```py\naad18_df = aad18_df.drop(['ARR_DELAY'], axis=1) \n```", "```py\nrand = 9 \nnp.random.seed(rand)\ny = aad18_df['CARRIER_DELAY']\nX = aad18_df.drop(['CARRIER_DELAY'], axis=1).copy()\nX_train, X_test, y_train_reg, y_test_reg = **train_test_split**(\n    X, y, test_size=0.15, **random_state**=rand\n)\ny_train_class = y_train_reg.apply(lambda x: 1 if x > 15 else 0)\ny_test_class = y_test_reg.apply(lambda x: 1 if x > 15 else 0) \n```", "```py\ncorr = aad18_df.**corr**()\nabs(corr['CARRIER_DELAY']).**sort_values**(ascending=False) \n```", "```py\nCARRIER_DELAY         1.000000\nDEP_DELAY             0.703935\nARR_RFPH              0.101742\nLATE_AIRCRAFT_DELAY   0.083166\nDEP_RFPH              0.058659\nARR_AFPH              0.035135\nDEP_TIME              0.030941\nNAS_DELAY             0.026792\n:          :\nWEATHER_DELAY         0.003002\nSECURITY_DELAY        0.000460 \n```", "```py\nreg_models = {\n    #Generalized Linear Models (GLMs)\n    'linear':{'model': linear_model.LinearRegression()}, \n    'linear_poly':{\n        'model':make_pipeline(\n            PolynomialFeatures(degree=2),\n            linear_model.LinearRegression(fit_intercept=False)\n        )\n    },\n    'linear_interact':{\n        'model':make_pipeline(\n            PolynomialFeatures(interaction_only=True),\n            linear_model.LinearRegression(fit_intercept=False)\n        )\n    },\n    'ridge':{\n        'model': linear_model.RidgeCV(\n            alphas=[1e-3, 1e-2, 1e-1, 1])\n    },\n    #Trees  \n    'decision_tree':{\n        'model': tree.DecisionTreeRegressor(\n             max_depth=7, random_state=rand\n        )\n    },\n    #RuleFit\n    'rulefit':{\n        'model': RuleFit(\n             max_rules=150,\n             rfmode='regress',\n             random_state=rand\n        )\n    },\n    #Nearest Neighbors\n    'knn':{'model': neighbors.KNeighborsRegressor(n_neighbors=7)},\n    #Ensemble Methods\n    'random_forest':{\n        'model':ensemble.RandomForestRegressor(\n            max_depth=7, random_state=rand)\n    },\n    #Neural Networks\n    'mlp':{\n        'model':neural_network.MLPRegressor(\n            hidden_layer_sizes=(21,),\n            max_iter=500, \n            early_stopping=True,\n            random_state=rand\n        )\n    }\n} \n```", "```py\nfor model_name in reg_models.keys():\n    if model_name != 'rulefit':\n        fitted_model = reg_models[model_name]\\\n        ['model'].fit(X_train, y_train_reg)\n    else :\n        fitted_model = reg_models[model_name]['model'].\\\n        fit(X_train.values, y_train_reg.values, X_test.columns\n        )\n        y_train_pred = fitted_model.predict(X_train.values)\n        y_test_pred = fitted_model.predict(X_test.values)\n    reg_models[model_name]['fitted'] = fitted_model\n    reg_models[model_name]['preds'] = y_test_pred\n    reg_models[model_name]['RMSE_train'] =\\\n    math.sqrt(\n        metrics.mean_squared_error(y_train_reg, y_train_pred)\n    )\n    reg_models[model_name]['RMSE_test'] =\\\n    math.sqrt(metrics.mean_squared_error(y_test_reg, y_test_pred)\n    )\n    reg_models[model_name]['R2_test'] =\\\n    metrics.r2_score(y_test_reg, y_test_pred) \n```", "```py\nreg_metrics = pd.**DataFrame.from_dict**(\n    reg_models, 'index')[['RMSE_train', 'RMSE_test', 'R2_test']\n]\nreg_metrics.**sort_values**(by=**'RMSE_test'**).style.format(\n    {'RMSE_train': '{:.2f}', 'RMSE_test': '{:.2f}', \n     'R2_test': '{:.3f}'}\n).background_gradient(\n    cmap='viridis_r', low=0.1, high=1,\n    subset=['RMSE_train', 'RMSE_test']\n).background_gradient(\n    cmap='plasma', low=0.3, high=1, subset=['R2_test']\n) \n```", "```py\nclass_models = {\n    #Generalized Linear Models (GLMs)\n    'logistic':{'model': linear_model.LogisticRegression()},\n    'ridge':{\n        'model': linear_model.RidgeClassifierCV(\n            cv=5, alphas=[1e-3, 1e-2, 1e-1, 1],\n            class_weight='balanced'\n        )\n    },\n    #Tree\n    'decision_tree':{\n        'model': tree. DecisionTreeClassifier(max_depth=7,\n                                              random_state=rand)\n    },\n    #Nearest Neighbors\n    'knn':{'model': neighbors.KNeighborsClassifier(n_neighbors=7)},\n    #Naive Bayes\n    'naive_bayes':{'model': naive_bayes.GaussianNB()},\n    #Ensemble Methods\n    'gradient_boosting':{\n        'model':ensemble.\n        GradientBoostingClassifier(n_estimators=210)\n    },\n    'random_forest':{\n        'model':ensemble.RandomForestClassifier(\n            max_depth=11,class_weight='balanced', random_state=rand\n        )\n    },\n    #Neural Networks\n    'mlp':{\n        'model': make_pipeline(\n            StandardScaler(),\n            neural_network.MLPClassifier(\n                hidden_layer_sizes=(7,),\n                max_iter=500,\n                early_stopping=True,\n                random_state=rand\n            )\n        )\n    }\n} \n```", "```py\nprint(y_train_class[y_train_class==1].shape[0] y_train_class.shape[0]) \n```", "```py\ngradient_boosting of sklearn takes longer than the rest to train, so this can take a few minutes to run:\n```", "```py\nfor model_name in class_models.keys():\n    fitted_model = class_models[model_name]\n    ['model'].fit(X_train,y_train_class)\n    y_train_pred = fitted_model.predict(X_train.values)\n    if model_name == 'ridge':\n        y_test_pred = fitted_model.predict(X_test.values)\n    else:\n        y_test_prob = fitted_model.predict_proba(X_test.values)[:,1]\n        y_test_pred = np.where(y_test_prob > 0.5, 1, 0)\n     class_models[model_name]['fitted'] = fitted_model\n    class_models[model_name]['probs'] = y_test_prob\n    class_models[model_name]['preds'] = y_test_pred\n    class_models[model_name]['Accuracy_train'] =\\\n    metrics.**accuracy_score**(y_train_class, y_train_pred\n    )\n    class_models[model_name]['Accuracy_test'] =\\\n    metrics.**accuracy_score**(y_test_class, y_test_pred\n    )\n    class_models[model_name]['Recall_train'] =\\\n    metrics.**recall_score**(y_train_class, y_train_pred\n    )\n    class_models[model_name]['Recall_test'] =\\\n    metrics.**recall_score**(y_test_class, y_test_pred\n    )\n    if model_name != 'ridge':\n        class_models[model_name]['ROC_AUC_test'] =\\\n        metrics.**roc_auc_score**(y_test_class, y_test_prob)\n    else:\n        class_models[model_name]['ROC_AUC_test'] = np.nan\n    class_models[model_name]['F1_test'] =\\\n    metrics.**f1_score**(y_test_class, y_test_pred\n    )\n    class_models[model_name]['MCC_test'] =\\\n    metrics.**matthews_corrcoef**(y_test_class, y_test_pred\n    ) \n```", "```py\nclass_metrics = pd.**DataFrame.from_dict**(\n    class_models,'index')[['Accuracy_train', 'Accuracy_test',\n                           'Recall_train', 'Recall_test',\n                           'ROC_AUC_test', 'F1_test', 'MCC_test']\n]\nclass_metrics.**sort_values**(\n    by=**'ROC_AUC_test'**, ascending=False).\n    style.format(dict(zip(class_metrics.columns, ['{:.3f}']*7))\n).background_gradient(\ncmap='plasma', low=1, high=0.1, subset=['Accuracy_train',\n                                        'Accuracy_test']\n).background_gradient(\n    cmap='viridis',\n    low=1,\n    high=0.1,\n    subset=['Recall_train', 'Recall_test',\n            'ROC_AUC_test', 'F1_test', 'MCC_test']\n) \n```", "```py\n    plt.tick_params(axis = 'both', which = 'major')\n    fpr, tpr, _ = metrics.**roc_curve**(\n      y_test_class, class_models['naive_bayes']['probs'])\n    plt.plot(\n        fpr,\n        tpr,\n        label='ROC curve (area = %0.2f)'\n        % class_models['naive_bayes']['ROC_AUC_test']\n    )\n    plt.plot([0, 1], [0, 1], 'kâ€“') #random coin toss line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.legend(loc=\"lower right\") \n    ```", "```py\ncoefs_lm = reg_models['linear']['fitted'].coef_\nintercept_lm = reg_models['linear']['fitted'].intercept_\nprint('coefficients:%s' % coefs_lm)\nprint('intercept:%s' % intercept_lm) \n```", "```py\ncoefficients:   [ 0.0045 -0.0053 0.8941 0.0152 ...]\nintercept: -37.86 \n```", "```py\npd.DataFrame({'feature': X_train.columns.tolist(),\\\n              'coef': coefs_lm.tolist()}) \n```", "```py\nlinreg_mdl = sm.**OLS**(y_train_reg, sm.add_constant(X_train))\nlinreg_mdl = linreg_mdl.fit()\nprint(linreg_mdl.summary()) \n```", "```py\nsummary_df = linreg_mdl.**summary2**().tables[1]\nsummary_df = summary_df.**drop**(\n    ['const']).reset_index().rename(columns={'index':'feature'}\n)\nsummary_df['t_abs'] = abs(summary_df['t'])\nsummary_df.**sort_values**(by=**'t_abs'**, ascending=False).\n    style.format(\n        dict(zip(summary_df.columns[1:], ['{:.4f}']*7)\n    )\n).background_gradient(cmap='plasma_r', low=0, high=0.1,\\\n                      subset=['P>|t|', 't_abs']) \n```", "```py\n[2] The condition number is large, 5.69e+04\\. This might indicate that there are strong multicollinearity or other numerical problems. \n```", "```py\ncoefs_ridge = reg_models['ridge']['fitted'].coef_\ncoef_ridge_df =pd.DataFrame(\n    {\n        'feature':X_train.columns.values.tolist(),\n        'coef_linear': coefs_lm,\\\n        'coef_ridge': coefs_ridge\n    }\n)\ncoef_ridge_df['coef_regularization'] =\\\n    coef_ridge_df['coef_linear'] - coef_ridge_df['coef_ridge']\ncoef_ridge_df.style.background_gradient(\n    cmap='plasma_r', low=0, high=0.1 , subset=['coef_regularization']\n) \n```", "```py\nnum_alphas = 100\nalphas = np.**logspace**(0, 13, num_alphas)\nalphas_coefs = []\nfor alpha in alphas:\n    ridge = linear_model.**Ridge**(alpha=alpha).fit(\n      X_train, y_train_reg)\n    alphas_coefs.**append**(np.concatenate((ridge.coef_[:8],\n                                        ridge.coef_[9:]))) \n```", "```py\nplt.gca().invert_xaxis()\nplt.tick_params(axis = 'both', which = 'major')\nplt.plot(alphas, alphas_coefs)\nplt.xscale(\"log\")\nplt.xlabel('Alpha')\nplt.ylabel('Ridge coefficients')\nplt.grid()\nplt.show() \n```", "```py\nreg_models['linear_poly']['fitted'].\\\nget_params()['linearregression'].coef_.shape[0] \n```", "```py\nreg_models['linear_interact']['fitted'].\\\nget_params()['linearregression'].coef_.shape[0] \n```", "```py\ncoefs_log = class_models['logistic']['fitted'].**coef_**\nintercept_log = class_models['logistic']['fitted'].**intercept_**\nprint('coefficients:%s' % coefs_log)\nprint('intercept:%s' % intercept_log) \n```", "```py\ncoefficients: [[-6.31114061e-04 -1.48979793e-04  2.01484473e-01  1.32897749e-01 1.31740116e-05 -3.83761619e-04 -7.60281290e-02  ..]]\nintercept: [-0.20139626] \n```", "```py\nstdv = np.std(X_train, 0)\nabs(coefs_log.reshape(21,) * stdv).sort_values(ascending=False) \n```", "```py\nDEP_DELAY              8.92\nCRS_ELAPSED_TIME       6.03\nDISTANCE               5.31\nLATE_AIRCRAFT_DELAY    4.99\nNAS_DELAY              2.39\nWEATHER_DELAY          2.16\nTAXI_OUT               1.31\nSECURITY_DELAY         0.38\nARR_AFPH               0.32\nWHEELS_OFF        0.01\nPCT_ELAPSED_TIME    0.003 \n```", "```py\nfig, axes = plt.subplots(\n    nrows = 1, ncols = 1, figsize = (16,8), dpi=600)\ntree.plot_tree(\n    class_models['decision_tree']['fitted'],\n    feature_names=X_train.columns.values.tolist(),\n    filled = True, max_depth=2\n)\nfig.show() \n```", "```py\ntext_tree = tree.export_text(\n    class_models['decision_tree']['fitted'],\n    feature_names=X_train.columns.values.tolist()\n)\nprint(text_tree) \n```", "```py\ndt_imp_df = pd.DataFrame(\n    {\n        'feature':X_train.columns.values.tolist(),\n        'importance': class_models['decision_tree']['fitted'].\\\n                      **feature_importances_**\n    }\n).sort_values(by='importance', ascending=False)\ndt_imp_df \n```", "```py\nrulefit_df = reg_models['rulefit']['fitted'].get_rules()\nrulefit_df = rulefit_df[rulefit_df.coef !=0].\\\nsort_values(\n    by=\"importance\", ascending=False\n)\nrulefit_df \n```", "```py\nprint(X_test.loc[721043,:]) \n```", "```py\nCRS_DEP_TIME         655.00\nDEP_TIME            1055.00\nDEP_DELAY            240.00\nTAXI_OUT            35.00\nWHEELS_OFF            1130.00\nCRS_ARR_TIME        914.00\nCRS_ELAPSED_TIME        259.00\nDISTANCE            1660.00WEATHER_DELAY        0.00\nNAS_DELAY            22.00\nSECURITY_DELAY        0.00\nLATE_AIRCRAFT_DELAY    221.00\nDEP_AFPH             90.80\nARR_AFPH             40.43\nDEP_MONTH            10.00\nDEP_DOW            4.00\nDEP_RFPH            0.89\nARR_RFPH            1.06\nORIGIN_HUB            1.0\nDEST_HUB            0.00\nPCT_ELAPSED_TIME        1.084942\nName: 721043, dtype: float64 \n```", "```py\nprint(y_test_class[721043]) \n```", "```py\nprint(class_models['knn']['preds'][X_test.index.get_loc(721043)]) \n```", "```py\nprint(class_models['knn']['fitted'].\\\n      **kneighbors**(X_test.loc[721043,:].values.reshape(1,21), 7)) \n```", "```py\n(array([[143.3160128 , 173.90740076, 192.66705727, 211.57109221,\n         243.57211853, 259.61593993, 259.77507391]]),\narray([[105172, 571912,  73409,  89450,  77474, 705972, 706911]])) \n```", "```py\nprint(y_train_class.iloc[[105172, 571912, 73409, 89450, 77474,\\\n                          705972, 706911]]) \n```", "```py\n3813      0\n229062    1\n283316    0\n385831    0\n581905    1\n726784    1\n179364    0\nName: CARRIER_DELAY, dtype: int64 \n```", "```py\nprint(class_models['knn']['fitted'].**effective_metric_**) \n```", "```py\nprint(class_models['naive_bayes']['fitted'].class_prior_) \n```", "```py\narray([0.93871674, 0.06128326]) \n```", "```py\nprint(class_models['naive_bayes']['fitted'].**sigma_**) \n```", "```py\narray([[2.50123026e+05, 2.61324730e+05, ..., 1.13475535e-02],\n       [2.60629652e+05, 2.96009867e+05, ..., 1.38936741e-02]]) \n```", "```py\nprint(class_models['naive_bayes']['fitted'].**theta_**) \n```", "```py\narray([[1.30740577e+03, 1.31006271e+03, ..., 9.71131781e-01],\n       [1.41305545e+03, 1.48087887e+03, ..., 9.83974416e-01]]) \n```", "```py\n#Make new abbreviated versions of datasets\nfeature_samp = [\n    'DEP_DELAY',\n    'LATE_AIRCRAFT_DELAY',\n    'PCT_ELAPSED_TIME',\n    'DISTANCE',\n    'WEATHER_DELAY',\n    'NAS_DELAY',\n    'SECURITY_DELAY',\n    'CRS_ELAPSED_TIME'\n]\nX_train_abbrev2 = X_train[feature_samp]\nX_test_abbrev2 = X_test[feature_samp]\n#For sampling among observations\nnp.random.seed(rand)\nsample2_size = 0.1\nsample2_idx = np.random.choice(\n    X_train.shape[0], math.ceil(\n      X_train.shape[0]*sample2_size), replace=False\n) \n```", "```py\nebm_mdl = **ExplainableBoostingClassifier**()\nebm_mdl.fit(X_train_abbrev2.iloc[sample2_idx], y_train_class.iloc[sample2_idx]) \n```", "```py\nshow(ebm_mdl.**explain_global**()) \n```", "```py\nebm_lcl = ebm_mdl.**explain_local**(\n    X_test_abbrev2.iloc[76:77], y_test_class[76:77], name='EBM'\n)\nshow(ebm_lcl) \n```", "```py\nebm_perf = **ROC**(ebm_mdl.predict_proba).**explain_perf**(\n    X_test_abbrev2.iloc[sample_idx],y_test_class.iloc[sample_idx],\n    name='EBM'\n)\nshow(ebm_perf) \n```", "```py\nmeta_info = {col:{\"type\":\"continuous\"} for col in\\\n                                         X_train_abbrev.columns} \n```", "```py\nX_train_abbrev2 = X_train_abbrev.copy()\nX_test_abbrev2 = X_test_abbrev.copy()\nfor key in meta_info.keys():\n    scaler = MinMaxScaler()\n    X_train_abbrev2[[key]] =scaler.fit_transform(\n      X_train_abbrev2[[key]])\n    X_test_abbrev2[[key]] = scaler.transform(X_test_abbrev2[[key]])\n    meta_info[key][\"scaler\"] = scaler\nmeta_info[\"CARRIER_DELAY\"] = {\"type\":\"target\", \"values\":[\"no\", \"yes\"]}\nX_train_abbrev2 = X_train_abbrev2.to_numpy().astype(np.float32)\nX_test_abbrev2 = X_test_abbrev2.to_numpy().astype(np.float32)\ny_train_class2 = y_train_class.to_numpy().reshape(-1,1)\ny_test_class2 = y_test_class.to_numpy().reshape(-1,1) \n```", "```py\ngami_mdl = GAMINet(\n    meta_info=meta_info,\n    interact_num=8,\n    task_type=\"Classification\",\n    main_effect_epochs=80,\n    interaction_epochs=60,\n    tuning_epochs=40,\n    lr_bp=[0.0001] * 3,\n    early_stop_thres=[10] * 3,\n    interact_arch=[20] * 5,\n    subnet_arch=[20] * 5,\n    batch_size=200,\n    activation_func=tf.nn.relu,\n    heredity=True, \n    loss_threshold=0.01,\n    val_ratio=0.2,\n    verbose=True,\n    reg_clarity=1,\n    random_state=rand\n)\ngami_mdl.fit(X_train_abbrev2, y_train_class2) \n```", "```py\ndata_dict_logs = gami_mdl.summary_logs(save_dict=False)\nplot_trajectory(data_dict_logs)\nplot_regularization(data_dict_logs) \nFigure 3.19:\n```", "```py\ndata_dict_global = gami_mdl.global_explain(save_dict=True)\nfeature_importance_visualize(data_dict_global)\nplt.show() \n```", "```py\ndata_dict_local = gami_mdl.local_explain(\n    X_test_abbrev2[[73]], y_test_class2[[73]], save_dict=False\n)\nlocal_visualize(data_dict_local[0])\nplt.tight_layout()\nplt.show() \n```", "```py\ny_test_prob = gami_mdl.predict(X_test_abbrev2)\ny_test_pred = np.where(y_test_prob > 0.5, 1, 0)\nprint(\n    'accuracy: %.3g, recall: %.3g, roc auc: %.3g, f1: %.3g,\n      mcc: %.3g'\n    % (\n        metrics.accuracy_score(y_test_class2, y_test_pred),\n        metrics.recall_score(y_test_class2, y_test_pred),\n        metrics.roc_auc_score(y_test_class2, y_test_prob),\n        metrics.f1_score(y_test_class2, y_test_pred),\n        metrics.matthews_corrcoef(y_test_class2, y_test_pred)\n    )\n) \n```", "```py\naccuracy: 0.991, recall: 0.934, roc auc: 0.998,\nf1: 0.924, mcc: 0.919 \n```"]