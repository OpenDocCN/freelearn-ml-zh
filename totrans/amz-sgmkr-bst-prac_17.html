<html><head></head><body>
		<div id="_idContainer152">
			<h1 id="_idParaDest-192"><a id="_idTextAnchor258"/>Chapter 13:Well-Architected Machine Learning with Amazon SageMaker</h1>
			<p>When running workloads in the cloud, you want to make sure that the workload is architected correctly to take advantage of all that the cloud can offer. AWS Well-Architected Framework helps you with this, by providing a formal approach for learning best practices across five critical pillars applicable to any workload deployed to AWS. The pillars are operational excellence, security, reliability, performance efficiency, and cost optimization. </p>
			<p>The framework provides guidance on how to improve your architecture and make trade-offs between the pillars both during the initial development and continued updates of the workload. While you can use Well-Architected Framework to evaluate your workload from a general technology perspective, while building <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications, it would be great to have focused guidance across the five pillars specific to ML. AWS Machine Learning Lens provides this focused guidance, which you can use to compare and measure your ML workload on AWS against best practices. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">For an in-depth review of the Well-Architected Framework and Machine Learning Lens, please review these two white papers from AWS: <a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf">https://docs.aws.amazon.com/wellarchitected/latest/framework/wellarchitected-framework.pdf</a> and <a href="https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf</a>.</p>
			<p>So far in this book, we have discussed how to use different Amazon SageMaker capabilities across all phases of ML workloads. In this chapter, we will learn how to combine guidance from both the generic Well-Architected Framework and Machine Learning Lens and apply it to the end-to-end ML workloads built on SageMaker.</p>
			<p>Please note that this chapter does not introduce any new SageMaker features, but rather dives into how you can apply the capabilities you already know to build a well-architected ML workload. You will learn how SageMaker's specific capabilities are combined with other AWS services across the five pillars, with some of the capabilities playing a key role in multiple pillars. </p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Best practices for operationalizing ML workloads</li>
				<li>Best practices for securing ML workloads</li>
				<li>Best practices for building reliable ML workloads</li>
				<li>Best practices for building performant ML workloads</li>
				<li>Best practices for building cost-optimized ML workloads</li>
			</ul>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor259"/>Best practices for operationalizing ML workloads</h1>
			<p>Many organizations <a id="_idIndexMarker617"/>start their ML journey with a few experiments of building models to solve one or more business problems. Cloud platforms, in general, and ML platforms such as SageMaker make this experimentation easy by providing seamless access to elastic compute infrastructure and built-in support for various ML frameworks and algorithms. Once these experiments have proven successful, the next natural step is to move the models into production. Typically, at this time, organizations want to move out of the research-and-development phase and into operationalizing ML. </p>
			<p>The idea of MLOps is gaining popularity these days. MLOps, at a very high level, involves bringing together people, processes, and technology to integrate ML workloads into release management, CI/CD, and operations. Without diving into all the details of MLOps, in this section, we will discuss best practices for operationalizing ML workloads using technology. We will also discuss which SageMaker features play a role in various aspects of operationalizing ML workloads.</p>
			<p>Let's now look at best practices for operationalizing ML workloads on AWS in the following sections. </p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor260"/>Ensuring reproducibility </h2>
			<p>To successfully<a id="_idIndexMarker618"/> operationalize the end-to-end ML system, you must first ensure its reproducibility through versioned data, code, and artifacts. Best practice is to version all inputs used to create models, including training data, data preparation code, algorithm implementation code, parameters, and hyperparameters, in addition to all trained model artifacts. A versioning strategy is also about helping in the model-update phase and allowing for easy rollback to a specific known working version if a model update fails or if the updated model does not meet your requirements.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor261"/>Tracking ML artifacts</h2>
			<p>Iterative<a id="_idIndexMarker619"/> development of ML models <a id="_idIndexMarker620"/>using different algorithms and hyperparameters for each algorithm results in many training experiments and multiple model versions. Keeping track of these experiments and resulting models along with each model's lineage is important to meet auditing and compliance requirements. Model lineage also helps with root-cause analysis in case of degrading model performance.  </p>
			<p>While you can certainly build a custom tracking solution, best practice is to use a managed service such as SageMaker Experiments. Experiments allows you to track, organize, visualize, and compare ML models across all phases of the ML lifecycle including feature engineering, model training, model tuning, and model deployment. With SageMaker Experiments, you can easily choose to deploy or update the model with a specific version. Experiments also provides you with the model lineage capability. For a detailed discussion of SageMaker Experiments' capabilities, please refer to the <em class="italic">Amazon SageMaker Experiments</em> section of <a href="B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a><em class="italic">, Training and Tuning at Scale</em>.</p>
			<p>Additionally, you can also use the Amazon SageMaker ML Lineage Tracking capability, which keeps track of information about the individual steps of an ML workflow from data preparation to model deployment. With the information tracked, you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor262"/>Automating deployment pipelines</h2>
			<p>Automated <a id="_idIndexMarker621"/>pipelines minimize human intervention in moving a trained ML model from lower-level environments such as development and staging into a production environment. The aim is to have a codified deployment pipeline created with Infrastructure-as-Code and Configuration-as-Code, with manual and automated quality gates incorporated into the pipeline. Manual quality gates can ensure that models are promoted to the production environment only after ensuring that there are no operational concerns such as security exposure. Automated quality gates, on the other hand, can be used to evaluate model metrics such as precision, recall, or accuracy. Pipelines result in consistent deployment as well as providing the ability to reliably recreate ML-related resources across multiple environments with minimal human intervention. </p>
			<p>Using Amazon SageMaker Pipelines, you can build automated model workflows. You can build every step of the ML lifecycle as a pipeline step to develop and deploy models and monitor the pipelines. You can further manage dependencies between each step, build the correct sequence, and execute the steps automatically. A service that brings in CI/CD practices to ML workloads is SageMaker Projects. This service helps you move models from concept to production. Additionally, you can easily meet governance and audit standards using a combination of SageMaker Projects and SageMaker Pipelines, by automatically tracking code, datasets, and model versions through each step of the ML lifecycle. This enables you to go back and replay model-generation steps, troubleshoot problems, and reliably track the lineage of models at scale. For a detailed discussion of automated workflows and MLOps, please refer to <a href="B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 12</em></a><em class="italic">, Machine Learning Automated Workflows</em>.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor263"/>Monitoring production models </h2>
			<p>Continued<a id="_idIndexMarker622"/> monitoring of deployed models is a critical step in operationalizing ML workloads, since a model's performance and effectiveness may degrade over time. Ensuring that the model continues to meet your business needs starts with the identification of the metrics that measure both model-related metrics and business metrics. Ensure that all metrics critical to model evaluation against your business KPIs are defined early on and collected during monitoring.</p>
			<p>Once the metrics are identified, to ensure the continued high quality of the deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration with CloudWatch to proactively detect issues, raise alerts, and automate remediation actions. In addition to detecting model-quality degradation, you can monitor data drift, bias drift, and feature attribution drift to meet your reliability, regulatory, and model explainability requirements. </p>
			<p>CloudWatch alerts that have been triggered because of model monitoring can be used to automate activities such as invalidating the current model, reverting to an older model version, or retraining a new model based on new ground truth data. Updates to production models<a id="_idIndexMarker623"/> should consider trade-offs between the risk of introducing changes, the cost of retraining, and the potential value of having a newer model in production. For a detailed discussion of model monitoring, please refer to <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a><em class="italic">, Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While this section has focused on SageMaker-native approaches for operationalizing ML workloads, please note that similar automated pipelines can be built using a combination of SageMaker APIs and other AWS services such as CodePipeline, Step Functions, Lambda, and SageMaker Data Science SDK. Multiple MLOps architectures are documented along with sample code at <a href="https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml">https://github.com/aws-samples/mlops-amazon-sagemaker-devops-with-ml</a>.</p>
			<p>The following table summarizes the various AWS services and features applicable to operationalizing ML workloads:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/012.jpg" alt="Figure 13.1 – AWS Services used for operationalizing ML workloads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – AWS Services used for operationalizing ML workloads</p>
			<p>In the next<a id="_idIndexMarker624"/> section, you will learn how SageMaker integrates with other AWS services to enable secure ML workloads.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor264"/>Best practices for securing ML workloads</h1>
			<p>When securing <a id="_idIndexMarker625"/>an ML workload, you should take into consideration infrastructure and network security, authentication and authorization, encrypting data and model artifacts, logging and auditing, and meeting regulatory requirements. In this section, we will discuss best practices for security ML workloads using a combination of SageMaker and related AWS services.</p>
			<p>Let's now look at best practices for securing ML workloads on AWS in the following sections.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor265"/>Isolating the ML environment</h2>
			<p>To build<a id="_idIndexMarker626"/> secure ML workloads, you need an isolated compute and network environment. To achieve this for ML on SageMaker, deploy all resources such as notebooks, studio domain, training jobs, processing jobs, and endpoints within a <strong class="bold">Virtual</strong> <strong class="bold">Private</strong> <strong class="bold">Cloud</strong> (<strong class="bold">VPC</strong>). A VPC provides an isolated environment<a id="_idIndexMarker627"/> where all traffic between various SageMaker components flows within the network. You can add another layer of isolation by using security groups that include rules for both inbound and outbound traffic allowed by subnets within the VPC, thereby isolating your ML resources further. </p>
			<p>Even if you use SageMaker without a VPC, all resources run in an environment managed by AWS on single-tenancy EC2 instances, which ensures that your ML environments are isolated from other customers. However, deploying ML resources, such as training containers, in a VPC allows you to monitor all network traffic in and out of these resources using VPC Flow Logs. Additionally, you can use VPC endpoints and AWS PrivateLink to enable communication between SageMaker and other AWS services such as S3 or CloudWatch. This keeps all traffic flowing between the various services within the AWS <a id="_idIndexMarker628"/>network without exposing the traffic to the public internet. </p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor266"/>Disabling internet and root access </h2>
			<p>By default, SageMaker<a id="_idIndexMarker629"/> notebook instances are internet-enabled to allow you to download external libraries and customize your working environment. Additionally, root access is enabled on these notebooks, giving you the flexibility to leverage external libraries. </p>
			<p>Only use these default settings in a lower-level sandbox and development environments to figure out the optimal working notebook environment. In all other non-production and production environments, launch SageMaker resources in your own VPC and turn off root access to prevent downloading and installing unauthorized software. Import all necessary libraries into a private repository such as AWS CodeArtifact before you isolate your environment. This allows you to seamlessly download specific versions of libraries without having to reach out to the internet. </p>
			<p>Additionally, use codified lifecycle configurations to automate setting up the notebook environment. Similarly, training and deployed inference containers managed by SageMaker are internet-enabled by default. When launching training and inference resources, use <strong class="source-inline">VPCConfig</strong> and <strong class="source-inline">EnableNetworkIsolation</strong> flags to protect these resources from external network traffic. In this case, all downloads and uploads of data and model artifacts are routed through your VPC. At the same time, the training and inference containers remain isolated from the network and do not have access to any resource within your VPC or on the internet.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor267"/>Enforcing authentication and authorization </h2>
			<p>Implement <a id="_idIndexMarker630"/>a strong mechanism<a id="_idIndexMarker631"/> to determine who can access the ML resources (authentication) and what resources authenticated users can access (authorization). SageMaker is natively integrated with AWS IAM, a service used to manage access to all AWS services and resources. IAM allows you to define fine-grained access controls using IAM users, groups, roles, and policies. You can implement least-privilege access using a combination of identity-based policies to specify what an IAM user, role, or group can do and resource-based policies to specify who has access to the resource and what actions they can perform on it.</p>
			<p>When designing these IAM policies, it is tempting to start with wide-open IAM policies with good intentions of tightening them as you go. However, best practice is to start with tight policies that grant minimal required access and add additional permissions when required. Periodically review and refine policies to ensure that no unnecessary permissions are granted. The IAM service provides the Access Advisor capability, which shows you when various AWS services are last accessed by different entities such as IAM groups, users, roles, and <a id="_idIndexMarker632"/>policies. Use this<a id="_idIndexMarker633"/> information to refine the policies. All the service API calls are also logged by CloudTrail, and you can use the CloudTrail history to determine which permissions can be removed based on the usage patterns.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor268"/>Securing data and model artifacts </h2>
			<p>IAM policies <a id="_idIndexMarker634"/>can also be used for access-control<a id="_idIndexMarker635"/> of data and models in S3. Additionally, you can use a security service called Amazon Macie to protect and classify data in S3. Macie internally uses ML to automatically discover, classify, and protect<a id="_idIndexMarker636"/> sensitive data. It automatically recognizes<a id="_idIndexMarker637"/> sensitive data such as <strong class="bold">personally</strong> <strong class="bold">identifiable</strong> <strong class="bold">information</strong> (<strong class="bold">PII</strong>) or <strong class="bold">intellectual</strong> <strong class="bold">property</strong> (<strong class="bold">IP</strong>), providing visibility into data access and movement patterns. Macie continuously monitors for anomalies in data-access patterns and proactively generates alerts on unauthorized access and data leaks. </p>
			<p>The next important aspects to secure are data and model artifacts of an ML system, both at rest and in transit. To secure data in transit within a VPC, use <strong class="bold">Transport</strong> <strong class="bold">Layer</strong> <strong class="bold">Security</strong> (<strong class="bold">TLS</strong>). To<a id="_idIndexMarker638"/> secure data at rest, best practice is to use encryption to block malicious actors from reading your data and model artifacts. You can use either client-side or server-side encryption. SageMaker comes with built-in encryption capabilities to protect training data and model artifacts both at rest and in transit. For example, when launching a training job, you can specify the encryption key<a id="_idIndexMarker639"/> to be used. You have the flexibility <a id="_idIndexMarker640"/>of using SageMaker-managed keys, AWS-managed keys, or your own customer-managed keys.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor269"/>Logging, monitoring, and auditing </h2>
			<p>SageMaker<a id="_idIndexMarker641"/> is<a id="_idIndexMarker642"/> natively<a id="_idIndexMarker643"/> integrated with CloudWatch and CloudTrail. You can capture logs from SageMaker training, processing, and inference in CloudWatch, which can further be used for troubleshooting. All SageMaker (and other AWS services) API calls are logged by CloudTrail, allowing you to track down which IAM user, AWS account, or source IP address made the API call along with when the call occurred. </p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor270"/>Meeting regulatory requirements </h2>
			<p>For many<a id="_idIndexMarker644"/> organizations, ML solutions need to comply with regulatory standards and pass compliance certifications that vary significantly across countries and industries. Amazon SageMaker complies with a wide range of compliance programs, including PCI, HIPAA, SOC 1/2/3, FedRAMP, and ISO 9001/27001/27017/27018.</p>
			<p>The following table summarizes the various AWS services applicable to securing ML workloads:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/021.jpg" alt="Figure 13.2 – AWS services used for securing ML workloads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – AWS services used for securing ML workloads</p>
			<p>In the next<a id="_idIndexMarker645"/> section, you will learn how SageMaker integrates with other AWS services to build reliable ML workloads.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor271"/>Best practices for reliable ML workloads</h1>
			<p>For a reliable <a id="_idIndexMarker646"/>system, there are two considerations at the core: </p>
			<ul>
				<li>First, the ability to recover from planned and unplanned disruptions </li>
				<li>Second, the ability to meet unpredictable increases in traffic demands</li>
			</ul>
			<p>Ideally, the system should achieve both without affecting downstream applications and end consumers. In this section, we will discuss best practices for building reliable ML workloads using a combination of SageMaker and related AWS services.</p>
			<p>Let's now look at some best practices for securing ML workloads on AWS in the following sections.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor272"/>Recovering from failure </h2>
			<p>For an ML<a id="_idIndexMarker647"/> workload, the ability to recover gracefully should be part of all the steps that make up the iterative ML process. A failure can occur with data storage, data processing, model training, or model hosting, which may result from a variety of events ranging from system failure to human error. </p>
			<p>For ML on SageMaker, all data (and model artifacts) is typically saved in S3. This ensures decoupling between ML data and the computation processing. To prevent an inadvertent loss of data, best practice is to use a combination of IAM and S3 policies to ensure least privilege-based access to data. Additionally, use S3 versioning and object tagging to enable versioning and traceability of data (and model artifacts) for easy recovery or recreation in the event of failure. </p>
			<p>Next, consider the reliability of ML training, which is often a long, time-consuming process. It is not uncommon to see training jobs that run over multiple hours and even multiple days. If these long-running training jobs are disrupted due to a power outage, OS fault, or other unexpected error, having the ability to reliably resume from where the job stopped is <a id="_idIndexMarker648"/>critical. ML checkpointing should be used in this situation. On SageMaker, a few built-in algorithms and all supported deep learning frameworks provide the capability of turning on checkpointing when a training job is launched. When you enable checkpointing, SageMaker automatically saves snapshots of the model state during training. This enables you to reliably restart a training job from the last saved checkpoint.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor273"/>Tracking model origin </h2>
			<p>Let's say your <a id="_idIndexMarker649"/>training goes off without a hitch and you have a trained model artifact saved in an S3 bucket. What happens if you lose this model artifact due to human error, such as someone in your team deleting it by mistake? In a reliable ML system, you need to be able to recreate this model using the same data, version of the code, and parameters as the original model. Hence, it is important to keep track of all these aspects during training. Using SageMaker Experiments, you can keep track of all the steps and artifacts that went into creating a model so you can easily recreate the model as necessary. Another benefit of tracking with SageMaker Experiments is the ability to troubleshoot issues in production for reliable operation. </p>
			<p>In addition to relying on Experiments to be able to recreate a specific version of a model artifact, use a combination of IAM and S3 policies to ensure least privilege-based access to minimize the risk of accidental model-artifact deletion. Implement measures such as requiring MFA for model artifact deletion and storing a secondary copy of the artifact as required by your organization's disaster recovery strategy.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor274"/>Automating deployment pipelines </h2>
			<p>To ensure <a id="_idIndexMarker650"/>that all steps leading up to model deployment are executed consistently, use a CI/CD pipeline with access controls to enforce least privilege-based access. Deployment automation combined with manual and automated quality gates ensures that all changes can be effectively validated with dependent systems prior to deployment. Amazon SageMaker Pipelines has the capability to bring CI/CD practices to ML workloads for improved reliability. Codifying the CI/CD pipelines using SageMaker Pipelines provides you with an additional capability of dealing with the model endpoint itself being deleted inadvertently. Using the Infrastructure-as-Code approach, the endpoint can be recreated. This requires a well-defined versioning strategy in place for your data, code, algorithms, hyperparameters, model artifacts, container images, and more. Version everything and document your versioning strategy. For a detailed discussion of SageMaker Pipelines capabilities, please refer to the <em class="italic">Amazon SageMaker Pipelines</em> section of <a href="B17249_12_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 12</em></a><em class="italic">, Machine Learning Automated Workflows</em>.</p>
			<p>Additionally, follow the <em class="italic">train once and deploy everywhere</em> strategy. Because of the decoupled nature of the training process and results, you can share the trained model artifact across multiple environments. This prevents retraining in multiple environments and introducing unexpected changes to the model.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor275"/>Handling unexpected traffic patterns</h2>
			<p>Once the<a id="_idIndexMarker651"/> model is deployed, you must ensure the reliability of the deployed model in serving the inference requests. The model should be able to handle spikes in inference traffic and continue to operate at the quality necessary to meet the business requirements. </p>
			<p>To handle traffic spikes, deploy the model with the Autoscaling-enabled SageMaker real-time endpoint. With Autoscaling enabled, SageMaker automatically increases (and decreases) the computation capacity behind the hosted model in response to the dynamic shifts in the inference traffic. Autoscaling provided by SageMaker is horizontal scaling, meaning it adds new instances or removes existing instances to handle the inference traffic variations. </p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor276"/>Continuous monitoring of deployed model </h2>
			<p>To ensure the<a id="_idIndexMarker652"/> continued high quality of the deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration with CloudWatch to proactively detect issues, raise alerts, and automate remediation actions when a production model is not performing as expected. In addition to model quality, you can monitor data drift, bias drift, and feature-attribution drift to meet your reliability, regulatory, and model explainability requirements. Ensure that all metrics critical to model evaluation against your business KPIs are defined and monitored. For a detailed discussion of model monitoring, please refer to <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a><em class="italic">, Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</em>.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor277"/>Updating model with new versions </h2>
			<p>Finally, you <a id="_idIndexMarker653"/>must consider how to update a production model reliably. SageMaker endpoint production variants can be used to implement multiple deployment strategies such as A/B, Blue/Green, Canary, and Shadow deployments. The advanced deployment strategies along with detailed implementation are discussed in <a href="B17249_09_Final_JM_ePub.xhtml#_idTextAnchor163"><em class="italic">Chapter 9</em></a><em class="italic">, Updating Production Models Using Amazon SageMaker Endpoint Production Variants</em>. Depending on the model consumer's tolerance for risk and downtime, choose an appropriate deployment strategy. </p>
			<p>The following table summarizes the various AWS services applicable to building reliable ML workloads:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/03.jpg" alt="Figure 13.3 – AWS service capabilities used for reliable ML workloads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – AWS service capabilities used for reliable ML workloads</p>
			<p>In the next section, you<a id="_idIndexMarker654"/> will learn how SageMaker integrates with other AWS services to build reliable, performance-efficient workloads.</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor278"/>Best practices for building performant ML workloads</h1>
			<p>Given the <a id="_idIndexMarker655"/>compute- and time-intensive nature of ML workloads, it is important to choose the most performant resources appropriate for each individual phase of the workload. Computation, memory, and network bandwidth requirements are unique to each phase of the ML process. Besides the performance of the infrastructure, the performance of the model as measured by metrics such as accuracy is also important. In this section, we will discuss best practices to apply in selecting the most performant resources for building ML workloads on SageMaker. </p>
			<p>Let's now look at best practices for building performant ML workloads on AWS in the following sections.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor279"/>Rightsizing ML resources </h2>
			<p>SageMaker <a id="_idIndexMarker656"/>supports a variety of ML instance types with a varying combination of CPU, GPU, FPGA, memory, storage, and networking capacity. Each instance type, in turn, supports multiple instance sizes. So, you have a range of choices to choose from to suit your specific workload. The best practice is to choose different compute resource configurations for data processing, building, training, and hosting your ML model. This is made possible by the decoupled nature of SageMaker, which allows you to choose different instance types and sizes for different APIs. For example, you can choose <strong class="source-inline">ml.c5.medium</strong> for a notebook instance as your working environment, use a cluster of four <strong class="source-inline">ml.p3.large</strong> GPU instances for training, and finally host the trained model on two <strong class="source-inline">ml.m5.4xlarge</strong> instances with Elastic Inference attached. Additionally, in the SageMaker Studio environment, you can change the notebook instance type seamlessly without any interruption to your work. </p>
			<p>While you have the flexibility of choosing different compute options for different ML phases, how do you choose the specific instance types and sizes to use? This comes down to understanding your workload and experimentation. For example, if you know that the training framework and algorithm of your choice will need GPU support, choose a GPU cluster to train on. While it may be tempting to use GPUs for all training, traditional <a id="_idIndexMarker657"/>algorithms may not work well on GPUs due to the communication overheads involved. Some built-in algorithms, such as XGBoost, implement an open source algorithm that has been optimized for CPU computations. SageMaker also provides optimized versions of frameworks, such as TensorFlow and PyTorch, which include optimizations for high-performance training across Amazon EC2 instance families.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor280"/>Monitoring resource utilization </h2>
			<p>Once you<a id="_idIndexMarker658"/> make your initial choice of instances and kick off training, SageMaker training jobs emit CloudWatch metrics for resource utilization that you can use to improve your training runs the next time. Additionally, when you enable Debugger with your training jobs, SageMaker Debugger provides visibility into training jobs and the infrastructure a training job is executing on. Debugger also monitors and reports on the system resources such as CPU, GPU, and memory, providing you with insights into resource underutilization and bottlenecks. If you use TensorFlow or PyTorch for your deep learning training jobs, Debugger provides you with a view into framework metrics that can be used to speed up your training jobs. For a detailed discussion of Debugger's capabilities, please refer to <a href="B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133"><em class="italic">Chapter 7</em></a><em class="italic">,</em> <em class="italic">Profile Training Jobs with Amazon SageMaker Debugger.</em></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor281"/>Rightsizing hosting infrastructure </h2>
			<p>Once the<a id="_idIndexMarker659"/> model is trained and ready to be deployed to choose instances for real-time endpoints, consider what your target performance is. Target performance is a combination of how many requests to serve in each period and the desired latency for each request, for example, 10,000 requests per minute with a maximum of a 1 millisecond response time. Once you have the target performance in mind, perform load testing in a non-production environment to figure out the instance type, instance size, and number of instances to host the model on. Recommended best practice is to deploy the endpoint with at least two instances across two availability zones for high availability.</p>
			<p>Once you decide on the instance type to use, start with the minimum number of instances necessary to meet your steady-state traffic and take advantage of the Autoscaling capability of SageMaker hosting. Using Autoscaling, SageMaker can automatically scale the inference capacity depending on the utilization and request traffic thresholds you configure. Capacity adjustments to meet your performance requirements are<a id="_idIndexMarker660"/> done by updating the endpoint configuration without any downtime.</p>
			<p>Additionally, you can scale up the hosting infrastructure for deep learning models using Amazon <strong class="bold">Elastic</strong> <strong class="bold">Inference</strong> (<strong class="bold">EI</strong>). While training a deep learning model may need a full-fledged GPU, hosting<a id="_idIndexMarker661"/> a training deep learning model may need only a slice of GPU to function. EI allows you to accelerate deep learning inferences using SageMaker ML instances. Alternatively, if you have a large-scale ML inference application, you can run inferences on <strong class="source-inline">Inf1</strong> instances, which are best suited to applications such as search, recommendation engines, and computer vision, at a low cost.</p>
			<p>While real-time endpoints provide access to models deployed on SageMaker, some workloads may warrant inference at the edge due to latency requirements, for example, models used to determine defective product parts in a manufacturing plant. In such cases, the model needs to be deployed on cameras within the manufacturing plant. For such use cases, use SageMaker Neo and SageMaker Edge Manager to optimize, deploy, and manage models at the edge.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While real-time endpoints and models deployed at the edge provide synchronous predictions, batch transform is used for asynchronous inferences with more tolerance for longer response times. Use experimentation to determine the right instance type, size, and number of instances to be used for batch transform with job completion time in mind.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor282"/>Continuous monitoring of deployed model</h2>
			<p>Once the <a id="_idIndexMarker662"/>model is actively serving inference traffic, use SageMaker Model Monitor to continuously monitor ML models for data drift, model-quality performance, feature-importance drift, and bias drift. Behind the scenes, Model Monitor uses distributed processing jobs. As with batch processing, use experimentation and load testing to determine the processing job resources necessary to complete each scheduled monitoring job execution. For a detailed discussion of Model Monitor, please refer to <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a><em class="italic">, Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</em>.</p>
			<p>The following table summarizes the various SageMaker features and how they are applicable for building performant ML workloads:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/04.jpg" alt="Figure 13.4 – AWS service capabilities for building performant ML workloads.&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.4 – AWS service capabilities for building performant ML workloads.</p>
			<p>In the next section, you will learn how SageMaker integrates with other AWS services to build cost-optimized workloads.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor283"/>Best practices for cost-optimized ML workloads</h1>
			<p>For many <a id="_idIndexMarker663"/>organizations, the lost opportunity cost of not embracing disruptive technologies such as ML outweighs the ML costs. By implementing a few best practices, these organizations can get the best possible returns on their ML investment. In this section, we will discuss best practices to apply for cost-optimized ML workloads on SageMaker.</p>
			<p>Let's now look at best practices for building cost-optimized ML workloads on AWS in the following sections.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor284"/>Optimizing data labeling costs </h2>
			<p>Labeling of <a id="_idIndexMarker664"/>data used for ML training, typically done at the very beginning of the ML process, can be tedious, error-prone, and time-consuming. Labeling at scale consumes many working hours, making this an expensive task, too. To optimize cost for data labeling, use SageMaker Ground Truth. Ground Truth provides capabilities for data labeling at scale using a combination of human workforce and active learning. When active learning is enabled, a labeling task is routed to humans only if a model cannot confidently finish the task. The human-labeled data is then used to train the model to improve accuracy. Therefore, as the labeling job progresses, less and less data needs to be labeled by humans. This results in faster completion of the job at reduced costs. For a detailed discussion of Ground Truth capabilities, please refer to <a href="B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a><em class="italic">, Data Labeling with Amazon SageMaker Ground Truth</em>.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor285"/>Reducing experimentation costs with models from AWS Marketplace </h2>
			<p>ML is<a id="_idIndexMarker665"/> inherently iterative and experimental. Having to run multiple algorithms with different sets of hyperparameters each time leads to several training jobs before you can determine a model that meets your needs. All this training adds up in terms of time and costs. </p>
			<p>A big part of experimentation is the research and reuse of readily available pre-trained models that may suit your needs. AWS Marketplace for ML gives you a catalog of datasets and models made available by vendors vetted by AWS. You can subscribe to models that meet your needs and potentially save the time and costs involved in developing your own models. If you do, however, end up developing your own models, you<a id="_idIndexMarker666"/> can use the marketplace to monetize your models by making them available to others.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor286"/>Using AutoML to reduce experimentation time </h2>
			<p>If the<a id="_idIndexMarker667"/> marketplace<a id="_idIndexMarker668"/> models don't meet your needs or if your organization has the <strong class="bold">build rather than buy</strong> policy, first check whether your dataset and use case are suitable for AutoPilot. At the time of writing this book, AutoPilot supports tabular data and classification and regression problems. AutoPilot automatically analyzes datasets and builds multiple models with different combinations of algorithms and hyperparameters and finally selects the best algorithm for the list. This saves both time and cost. Additionally, the service provides transparency through two notebooks – a data preparation notebook and a model candidate selection notebook, which details all the behind-the-scenes steps performed by AutoPilot. So, even if you don't end up using the model built and recommended by AutoPilot, you can use these notebooks as a starting point for your own experimentation and modify them using your business domain knowledge.</p>
			<p>However, at the time of publication of this book, AutoPilot only supports regression and classification using tabular data. For other data types and problems, you will have to build and train your model. </p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor287"/>Iterating locally with small datasets </h2>
			<p>During<a id="_idIndexMarker669"/> ML experimentation, iterate with a smaller dataset in the SageMaker notebook's local environment first. Once you iron out details such as code bugs and data issues, you can scale up with the full dataset and distributed training clusters managed by SageMaker. This phased approach will let you iterate faster at lower costs. SageMaker SDK makes this easy by supporting <strong class="source-inline">instance-type = "local"</strong> for the training API so that you can reuse the same code in the local environment or on the distributed cluster. Note that at<a id="_idIndexMarker670"/> the time of publication, local mode only works in SageMaker notebook instances, not in the Studio environment.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor288"/>Rightsizing training infrastructure </h2>
			<p>When you<a id="_idIndexMarker671"/> are ready to launch a distributed training cluster, it is important to choose the right number and type of instances in the cluster. For built-in or custom algorithms that do not support distributed training, your cluster will always have a single instance. For algorithms and frameworks that do support distributed training, take advantage of data parallelism and model parallelism as discussed in <a href="B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a><em class="italic">, Training and Tuning at Scale, </em>to complete training faster, thereby reducing the overall training costs. </p>
			<p>While there are various instance types with different capacity configurations available, it is important to rightsize the training instances based on the ML algorithm used. For example, simple algorithms may not train faster on the larger instance types since they cannot take advantage of hardware parallelism. Even worse, they may even train slower due to high GPU communication overhead. Best practice for cost optimization is to start with a smaller instance, scale out first by adding more instances to the training cluster, and then scale up to more powerful instances. However, if you are using a deep learning framework and distributed training, best practice would be to scale up to more GPUs/CPUs on a single instance before scaling out because the network I/O involved may negatively impact the training performance.</p>
			<p>In addition to selecting the right infrastructure, you can also use optimized versions of ML frameworks that result in faster training. SageMaker provides optimized versions of multiple open source ML frameworks including TensorFlow, Chainer, Keras, and Theano. SageMaker versions of these popular frameworks are optimized for high performance on all SageMaker ML instances.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor289"/>Optimizing hyperparameter-tuning costs </h2>
			<p>Hyperparameter <a id="_idIndexMarker672"/>tuning is also an expensive task, using sophisticated search and algorithms. Best practice is to rely on the automated model tuning capability provided by managed SageMaker Automatic Model Tuning, also<a id="_idIndexMarker673"/> known as <strong class="bold">hyperparameter tuning</strong> (<strong class="bold">HPT</strong>). Automatic model tuning finds the best version of a model by running multiple training jobs using the algorithm and hyperparameter ranges specified by you. HPT then chooses the hyperparameter values that result in the best model as measured by the objective metric you specify. Behind the scenes, HPT uses ML techniques that can determine optimal hyperparameters with a limited number of training jobs. </p>
			<p>You can further speed up the HPT jobs using warm start mode. With warm start, you no longer must start an HPT job from scratch; instead, you can create a new HPT job based on one or more parent jobs. This allows you to reuse the training jobs conducted in the <a id="_idIndexMarker674"/>parent jobs as prior knowledge. Warm start allows you to reduce the time and cost associated with model tuning.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor290"/>Saving training costs with Managed Spot Training</h2>
			<p>SageMaker <a id="_idIndexMarker675"/>Managed <a id="_idIndexMarker676"/>Spot Training applies the cost-saving construct of Spot Instances and applies it to hyperparameter tuning and training. The Managed Spot Training capability takes advantage of checkpointing, to resume training jobs easily. Since you don't have to run the training from the start again, this reduces your overall training costs.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor291"/>Using insights and recommendations from Debugger </h2>
			<p>When<a id="_idIndexMarker677"/> it <a id="_idIndexMarker678"/>comes to deep learning on SageMaker, training with GPU is very powerful, but training costs can add up quickly. SageMaker Debugger provides insight into deep learning training both into the ML framework in use and the underlying compute resources. The deep profiler capability provides you with recommendations that you can implement to improve training performance and reduce resource wastage. For a detailed discussion of Debugger's capabilities, please refer to <a href="B17249_07_Final_JM_ePub.xhtml#_idTextAnchor133"><em class="italic">Chapter 7</em></a><em class="italic">,</em> <em class="italic">Profile Training Jobs with Amazon SageMaker Debugger</em>.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor292"/>Saving ML infrastructure costs with SavingsPlan </h2>
			<p>Once you<a id="_idIndexMarker679"/> enable <a id="_idIndexMarker680"/>SavingsPlan in your AWS account, it analyzes your ML resource usage within a time of your choice – the past 7, 30, or up to 60 days. The service then recommends the right plan to use to optimize costs. You can also select a pre-payment option from three different options: no upfront costs, partial upfront (50% or more), or all upfront. Once you configure these options, SavingsPlan provides you with details of how your monthly spend can be optimized. Additionally, it also suggests an hourly usage commitment that maximizes your savings. The plans cover all ML instance families, notebook instances, Studio instances, training instances, batch transform instances, real-time endpoint instances, Data Wrangler instances, and SageMaker Processing instances, thereby helping to optimize costs across various phases of ML workloads.</p>
			<p>While Managed Spot Training and SavingsPlan are both cost-saving approaches, they are not meant to be combined. With SavingsPlan, you are billed every hour of the commitment regardless of whether it is fully used. Best practice is to use SavingsPlan and Managed Spot Training usages separately. For example, use SavingsPlan for predictable steady-state recurring training workloads and Managed Spot Training for new training workloads and prototyping where you do not have a clear idea of monthly costs yet.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor293"/>Optimizing inference costs</h2>
			<p>Inference <a id="_idIndexMarker681"/>costs typically make up most ML costs. Inference costs are discussed in detail in <a href="B17249_10_Final_JM_ePub.xhtml#_idTextAnchor179"><em class="italic">Chapter 10</em></a><em class="italic">, Optimizing Model Hosting and Inference Costs</em>, which details several ways to improve inference performance while reducing inference costs. These methods include using batch inference where possible, deploying several models behind a single inference endpoint to reduce cost and help with advanced canary or blue/green deployments, scaling inference endpoints to meet demand, and using EI and SageMaker Neo to provide better inference performance at a lower cost.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor294"/>Stopping or terminating resources </h2>
			<p>Ensure <a id="_idIndexMarker682"/>that you terminate or at least<a id="_idIndexMarker683"/> stop the ML resources once you are done. While the instances for training, hyperparameter tuning, batch inferences, and processing jobs will be managed and automatically deleted by SageMaker, you are responsible for notebook instances, endpoint, and monitoring schedules. Stop or delete these resources to avoid unnecessary costs using automation with scripts that stop resources based on idle time or a schedule.</p>
			<p>The following table summarizes the various SageMaker features and how they are applicable for building cost-optimized ML workloads:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/05.jpg" alt="Figure 13.5 – AWS service capabilities for cost-optimized ML workloads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – AWS service capabilities for cost-optimized ML workloads</p>
			<p>This section concludes the discussion on applying best practices to build well-architected<a id="_idIndexMarker684"/> ML<a id="_idIndexMarker685"/> workloads on AWS. </p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor295"/>Summary</h1>
			<p>In this chapter, you reviewed the five pillars – operational excellence, security, reliability, performance, and cost optimization – that make up the Well-Architected Framework. You then dove into the best practices for each of these pillars, with an eye to applying these best practices to ML workloads. You learned how to use the SageMaker capabilities with related AWS services to build well-architected ML workloads on AWS. </p>
			<p>As you architect your ML applications, you typically must make trade-offs between the pillars depending on your organization's priorities. For example, when getting started with ML, cost-optimization may not be at the top of your mind but establishing operational standards may be important. However, as the number of ML workloads scale, cost-optimization could become an important consideration. By applying the best practices you learned in this chapter, you can architect and implement ML applications that meet your organization's needs and periodically evaluate your applications against the best practices.</p>
			<p>In the next chapter, you will apply all these best practices and see how to operate in multiple AWS environments that reflect the real world. </p>
		</div>
		<div id="_idContainer153">
			<table id="table001" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">AWS service/feature</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">How you should use it for securing ML</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>IAM</p>
						</td>
						<td class="No-Table-Style">
							<p>By implementing authentication, authorization, and access control through IAM users, groups, roles, and policies.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>IAM access advisor/CloudWatch Event history</p>
						</td>
						<td class="No-Table-Style">
							<p>By identifying opportunities to refine IAM policies.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>CloudWatch/CloudTrail</p>
						</td>
						<td class="No-Table-Style">
							<p>By collecting logs, implementing monitoring, and auditing.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>VPC</p>
						</td>
						<td class="No-Table-Style">
							<p>By providing infrastructure and network isolation. </p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>VPC endpoints</p>
						</td>
						<td class="No-Table-Style">
							<p>By routing traffic through the AWS network and avoiding exposure to the public internet.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Private links</p>
						</td>
						<td class="No-Table-Style">
							<p>By routing traffic through the AWS network and avoiding exposure to the public internet.</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>
	</body></html>