- en: The ML Kit SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss ML Kit, which was announced by Firebase at
    the Google I/O 2018\. This SDK packages Google's mobile machine learning offerings
    under a single umbrella.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile application developers may want to implement features in their mobile
    apps that require machine learning capabilities. However, they may not have knowledge
    of machine learning concepts and which algorithms to use for which scenarios,
    how to build the model, train the model, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: ML Kit tries to address this problem by identifying all the potential use cases
    for machine learning in the context of mobile devices, and providing ready-made
    APIs. If the correct inputs are passed to these, the required output is received,
    with no further coding required.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this kit enables the inputs to be passed either to on-device APIs
    that work offline, or to online APIs that are hosted in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: To top it all, ML Kit also provides options for developers with expertise in
    machine learning, allowing them to build their own models using TensorFlow/TensorFlow
    Lite, and them import them into the application and invoke them using ML Kit APIs.
  prefs: []
  type: TYPE_NORMAL
- en: ML Kit also offers further useful features, such as model upgrade and monitoring
    capabilities (if hosted with Firebase).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: ML Kit and its features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an image-labeling sample using ML Kit on-device APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the same sample using ML Kit cloud APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Face Detection application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ML Kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML Kit encompasses all the existing Google offerings for machine learning on
    mobile. It bundles the Google Cloud Vision API, TensorFlow Lite, and the Android
    Neural Networks API together in a single SDK, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e34862b1-59df-45ab-acca-1aebfdb354b5.png)'
  prefs: []
  type: TYPE_IMG
- en: ML Kit enables developers to utilize machine learning in their mobile applications
    for both Android and iOS apps, in a very easy way. Inference can be carried out
    by invoking APIs that are either on-device or on-cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of on-device APIs are that they work completely offline, and
    are more secure as no data is sent to the cloud. By contrast, on-cloud APIs do
    require network connectivity, and do send data off-device, but allow for greater
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML Kit offers APIs covering the following machine learning scenarios that may
    be required by mobile application developers:'
  prefs: []
  type: TYPE_NORMAL
- en: Image labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landmark detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barcode scanning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these APIs are implemented using complex machine learning algorithms. However,
    those details are wrapped. The mobile developer need not get into the details
    of which algorithms are used for implementing these APIs; all that needs to be
    done is to pass the desired data to the SDK, and in return the correct output
    will be received back, depending on which part of ML Kit is being used.
  prefs: []
  type: TYPE_NORMAL
- en: If the provided APIs don't cover a specific use case, you can build your own
    TensorFlow Lite model. ML Kit will help to host that model, and serve it to your
    mobile application.
  prefs: []
  type: TYPE_NORMAL
- en: Since Firebase ML Kit provides both on-device and on-cloud capabilities, developers
    can come up with innovative solutions to leverage either or both, based on the
    specific problem at hand. All they need to know is that on-device APIs are fast
    and work offline, while Cloud APIs utilize the Google Cloud platform to provide
    predictions with increased levels of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram describes the issues to consider when deciding between
    on-device or on-cloud APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ad6b4d2-8c3b-420a-96ae-4d80d493b816.png)'
  prefs: []
  type: TYPE_IMG
- en: ML Kit APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Not all APIs provided by ML Kit are supported in both on-device and on-cloud
    modes. The following table shows which APIs are supported in each mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d35b8fc6-71dc-4e0d-ae3c-e28c568a0796.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at the details of each API.
  prefs: []
  type: TYPE_NORMAL
- en: Text recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Kit's text recognition APIs help with the recognition of text in any Latin-based
    language, using the mobile device camera. They are available both on-device and
    on-cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The on-device API allows for recognition of sparse text, or text present in
    images. The cloud API does the same, but also allows for recognition of bulk text,
    such as in documents. The cloud API also supports recognition of more languages
    than device APIs are capable of.
  prefs: []
  type: TYPE_NORMAL
- en: Possible use cases for these APIs would be to recognize text in images, to scan
    for characters that may be embedded in images, or to automate tedious data entry.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ML Kit''s face detection API allows for the detection of faces in an image
    or video. Once the face is detected, we can apply the following refinements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Landmark detection**: Determining specific points of interest (landmarks)
    within the face, such as the eyes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Classifying the face based on certain characteristics,
    such as whether the eyes are open or closed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face tracking**: Recognizing and tracking the same face (in various positions)
    across different frames of video'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection can be done only on-device and in real time. There may be many
    use cases for mobile device applications, in which the camera captures an image
    and manipulates it based on landmarks or classifications, to produce selfies,
    avatars, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Barcode scanning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Kit's barcode-scanning API helps read data encoded using most standard barcode
    formats. It supports linear formats such as Codabar, Code 39, Code 93, Code 128,
    EAN-8, EAN-13, ITF, UPC-A, or UPC-E, as well as 2-D formats such as Aztec, Data
    Matrix, PDF417, or QR codes.
  prefs: []
  type: TYPE_NORMAL
- en: The API can recognize and scan barcodes regardless of their orientation. Any
    structured data that is stored as a barcode can be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: Image labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Kit's image-labeling APIs help recognize entities in an image. There is no
    need for any other metadata information to be provided for this entity recognition.
    Image labeling gives insight into the content of images. The ML Kit API provides
    the entities in the images, along with a confidence score for each one.
  prefs: []
  type: TYPE_NORMAL
- en: Image labeling is available both on-device and on-cloud, with the difference
    being the number of labels supported. The on-device API supports around 400 labels,
    while the cloud-based API supports up to 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: Landmark recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML Kit's landmark recognition API helps recognize well-known landmarks in
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: This API, when given an image as input, will provide the landmarks found in
    the image along with geographical coordinates and region information. The knowledge
    graph entity ID is also returned for the landmark. This ID is a string that uniquely
    identifies the landmark that was recognized.
  prefs: []
  type: TYPE_NORMAL
- en: Custom model inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the APIs provided out-of-the-box are not sufficient for your use case, ML
    Kit also provides the option to create your own custom model and deploy it through
    ML Kit.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a text recognition app using Firebase on-device APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started in ML Kit, you need to sign in to your Google account, activate
    your Firebase account, and create a Firebase project. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://firebase.google.com/.](https://firebase.google.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign in to your Google account, if you are not already signed in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Go to console in the menu bar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Add project to create a project and open it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now open Android Studio, and create a project with an empty activity. Note down
    the app package name that you have given while creating the project—for example,  `com.packt.mlkit.textrecognizationondevice`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, go to the Firebase console. In the Project overview menu, click Add app
    and give the required information. It will give you a JSON file to download. Add
    to the app folder of your project in project view in Android Studio, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e38470b-fdb7-42a7-a5ff-2880d1e01259.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, add the following lines of code to the manifest file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We need these permissions for our app to work. The next line tells the Firebase
    dependencies to download the **text recognition** (**OCR**) model from the Google
    server, and keep it in the device for inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The whole manifest file will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to add the Firebase dependencies to the project. To do so, we
    need to add the following lines to the project `build.gradle` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then open the module app `build.gradle` file, and add the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Also add the following line to the bottom of that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in your layout file, write the following `.xml` code to define the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's time to code your application's main activity class.
  prefs: []
  type: TYPE_NORMAL
- en: Please download the application code from Packt Github repository at [https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/mlkit](https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/mlkit).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are assuming you are already familiar with Android—so, we are discussing
    the code using Firebase functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will import the firebase libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line will declare the firebase text recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line will initialize the Firebase application context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line will get the on-device text recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet registers the on-click-event listener for the take-picture
    button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Creating a bitmap from the byte array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line creates a firebase image object to pass through the recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line passes the created image object to the recognizer for processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block will add the on-success listener. It will receive a
    firebase vision text object, which it in turn displays to the user in the form
    of a `Toast` message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block will add the `on-failure` listener. It will receive
    an exception object, which is in turn a display error message to the user in the
    form of a `Toast` message.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the preceding code, you will have the following output in your
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70a9a836-c149-4fc8-88af-fc888459126f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that you must be connected to the internet while installing this app, as
    Firebase needs to download the model to your device.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a text recognition app using Firebase on-cloud APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to convert the on-device app to a cloud app. The
    difference is that on-device apps download the model and store it on the device.
    This allows for a lower inference time, allowing the app to make quick predictions.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, cloud-based apps upload the image to the Google server, meaning
    inference will happen there. It won't work if you are not connected to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, why use a cloud-based model? Because on-device, the model has
    limited space and processing hardware, whereas Google's servers are scalable.
    The Google on-cloud text recognizer model is also able to decode multiple languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you need a Google Cloud subscription. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to your Firebase project console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the menu on the left, you will see that you are currently on the Spark Plan
    (the free tier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Upgrade, and follow the instructions to upgrade to the Blaze Plan, which
    is pay-as-you-go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to provide credit card or payment details for verification purposes—these
    will not be charged automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you subscribe, you will receive 1,000 Cloud Vision API requests free each
    month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This program can be tried only if you have a upgraded Blaze Plan and not a free
    tier account. The steps are given to create a upgraded account and please follow
    steps to get the account to try the program given.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Cloud Vision is not enabled for your project. To do so, you need
    to go to the following link: [https://console.cloud.google.com/apis/library/vision.googleapis.com/?authuser=0](https://console.cloud.google.com/apis/library/vision.googleapis.com/?authuser=0).
    In the top menu dropdown, select the Firebase project containing the Android app
    you added in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click Enable to enable this feature for your app. The page will look like the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa14a5d0-5a6b-4135-bb4a-bd85d8a04f02.png)'
  prefs: []
  type: TYPE_IMG
- en: Now return to your code, and make the following changes.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the application code in our Packt Github repository at:[ https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/Testrecognizationoncloud](https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/Testrecognizationoncloud).
  prefs: []
  type: TYPE_NORMAL
- en: All the other files, except the main activity, have no changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to import the preceding packages as dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will declare the document text recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code instantiates and assigns the cloud text recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code registers the on-click-event listener for the take-picture
    button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line creates a bitmap from the byte array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line creates a firebase image object to pass through the recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line passes the created image object to the recognizer for processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block will add the on-success listener. It will receive a
    FirebaseVision document text object, which is in turn displayed to the user in
    the form of a `Toast` message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block will add the on-failure listener. It will receive an
    exception object, which is in turn a display error message to the user in the
    form of a `Toast` message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Face detection using ML Kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will try to understand how face detection works with ML Kit. Face detection,
    which was previously part of the Mobile Vision API, has now been moved to ML Kit.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Google Developers page defines face detection as the process of automatically
    locating and detecting human faces in visual media (digital images or video).
    The detected face is reported at a position with an associated size and orientation.
    After the face is detected, we can search for landmarks present in the face such
    as the eyes and nose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some important terms to understand before we can move on to programming
    face detection with ML Kit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Face Orientation**: Detects faces at a range of different angles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face Recognition**: Determines whether two faces can belong to the same person.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face Tracking**: Refers to detecting faces in videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Landmark**: Refers to a point of interest within a face. This corresponds
    to the notable features on a face, such as the right eye, left eye, and nose base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Determines the presence of facial characteristics, such
    as open or closed eye or a smiling or serious face.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample solution for face detection using ML Kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now open Android Studio, and create a project with an empty activity. Note down
    the app package name that you have given while creating the project—for example,
    `com.packt.mlkit.facerecognization`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we are going to modify the text recognization code to predict faces. So,
    we are not changing the package names and other things. Just the code changes.
    The project structure is the same as shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfd38de1-3b33-45d5-b438-bc647837d680.png)'
  prefs: []
  type: TYPE_IMG
- en: It's time to code our application's main activity class. First we need to download
    the application code from the Packt GitHub repository at [https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/facerecognization](https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/facerecognization).
    and open the project in Android Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will add the following lines of code to the Gradle dependencies. Open
    the `build.gradle` file of the module app and add the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will  add the import statements to work with face detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following statement will declare the `FaceDetector` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create an object and assign it to the declared detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We declared a string object to save the prediction messages to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we will check whether the detector is operational; we also have a bitmap
    object that was obtained from the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a frame object, which `FaceDetector` class detect method needs
    to predict the face information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it successfully detects, it will return the face object array. The following
    code appends the information that each `nface` object has to our results string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If no faces are returned, then the following error message will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If the face size is not `0`, that means it already went through the `for` loop,
    which appended the faces information to our results text. Now we will add the
    total number of faces and end the result string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If the detector is not operational then the error message will be shown to
    the user as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the following code will show the results to the reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Running the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s time to run the app. For that, you will have to connect your mobile
    to your desktop through the USB debugging option in your mobile and install the
    app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/024b29f4-2d08-4cb6-9efd-7533311a3949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On running the app, you will have the following as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/766c66a2-276a-4889-8b0d-b89b66259545.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed ML Kit SDK, which was announced by Firebase at
    Google I/O 2018\. We covered different APIs provided by ML Kit, such as image
    labeling, text recognition, landmark detection, and more. We then created a text
    recognition app using on-device APIs, and then using on-cloud APIs. We also create
    an Face detection application by making minor changes in text recognition application.  In
    the next chapter, we will learn about a spam messages classifier and build a sample
    implementation of such a classifier for iOS.
  prefs: []
  type: TYPE_NORMAL
