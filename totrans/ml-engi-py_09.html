<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer208">
<h1 class="chapterNumber">9</h1>
<h1 class="chapterTitle" id="_idParaDest-186">Building an Extract, Transform, Machine Learning Use Case</h1>
<p class="normal">Similar to <em class="chapterRef">Chapter 8</em>, <em class="italic">Building an Example ML Microservice</em>, the aim of this chapter will be to try to crystallize a lot of the tools and techniques we have learned about throughout this book and apply them to a realistic scenario. This will be based on another use case introduced in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to ML Engineering</em>, where we imagined the need to cluster taxi ride data on a scheduled basis. So that we can explore some of the other concepts introduced throughout the book, we will assume as well that for each taxi ride, there is also a series of textual data from a range of sources, such as traffic news sites and transcripts of calls between<a id="_idIndexMarker1022"/> the taxi driver and the base, joined to the core ride information. We will then pass this data to a <strong class="keyWord">Large Language Model</strong> (<strong class="keyWord">LLM</strong>) for summarization. The result of this summarization can then be saved in the target data location alongside the basic ride date to provide important context for any downstream investigations or analysis of the taxi rides. We will also build on our previous knowledge of Apache Airflow, which we will use as the orchestration tool for our pipelines, by discussing some more advanced concepts to make your Airflow jobs more robust, maintainable, and scalable. We will explore this scenario so that we can outline the key decisions we would make if building a solution in the real world, as well as discuss how to implement it by leveraging what has been covered in other chapters.</p>
<p class="normal">This use case will allow us to explore what is perhaps the most used pattern in <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) solutions across the world—that of the batch inference process. Due to the nature of retrieving, transforming, and then performing ML on data, I have termed this <strong class="keyWord">Extract, Transform, Machine Learning</strong> (<strong class="keyWord">ETML</strong>).</p>
<p class="normal">We will work through this example in the following sections:</p>
<ul>
<li class="bulletList">Understanding the batch processing problem</li>
<li class="bulletList">Designing an ETML solution</li>
<li class="bulletList">Selecting the tools</li>
<li class="bulletList">Executing the build</li>
</ul>
<p class="normal">All of these topics will help us understand the particular decisions and steps we need to take in order to build a successful ETML solution.</p>
<p class="normal">In the next section, we will revisit the high-level problem introduced in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to ML Engineering</em>, and explore how to map the business requirements to technical solution requirements, given everything we have learned in the book so far.</p>
<h1 class="heading-1" id="_idParaDest-187">Technical requirements</h1>
<p class="normal">As in the other chapters, to create the environment to run the code examples in this chapter you can run:</p>
<pre class="programlisting con"><code class="hljs-con">conda env create –f mlewp-chapter09.yml
</code></pre>
<p class="normal">This will include installs of Airflow, PySpark, and some supporting packages. For the Airflow examples, we can just work locally, and assume that if you want to deploy to the cloud, you can follow the details given in <em class="chapterRef">Chapter 5</em>, <em class="italic">Deployment Patterns and Tools</em>. If you have run the above <code class="inlineCode">conda</code> command then you will have installed Airflow locally, along with PySpark and the Airflow PySpark connector package, so you can run Airflow as standalone with the following command in the terminal:</p>
<pre class="programlisting con"><code class="hljs-con">airflow standalone
</code></pre>
<p class="normal">This will then instantiate a local database and all relevant Airflow components. There will be a lot of output to the terminal, but near the end of the first phase of output, you should be able to spot details about the local server that is running, including a generated user ID and password. See <em class="italic">Figure 9.1</em> for an example.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer code  Description automatically generated with low confidence" height="206" src="../Images/B19525_09_01.png" width="825"/></figure>
<p class="packt_figref">Figure 9.1: Example of local login details created upon running Airflow in standalone mode. As the message says, do not use this mode for production!</p>
<p class="normal">If you navigate to the URL provided (in the second line of the screenshot you can see that the app is <code class="inlineCode">Listening at http://0.0.0.0:8080</code>), you will see a page like that shown in <em class="italic">Figure 9.2</em>, where you can use the local username and password to log in (see <em class="italic">Figure 9.3</em>). When you log in to the standalone version of Airflow, you are presented with many examples of DAGs and jobs that you can base your own workloads on.</p>
<figure class="mediaobject"><img alt="" height="565" role="presentation" src="../Images/B19525_09_02.png" width="825"/></figure>
<p class="packt_figref">Figure 9.2: The login page of the standalone Airflow instance running on a local machine.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="400" src="../Images/B19525_09_03.png" width="825"/></figure>
<p class="packt_figref">Figure 9.3: The landing page after logging in to the standalone Airflow instance. The page has been populated with a series of example DAGs.</p>
<p class="normal">Now that we have done some of the preliminary setup, let’s move on to discussing the details of the problem we will try to solve before we build out our solution.</p>
<h1 class="heading-1" id="_idParaDest-188">Understanding the batch processing problem</h1>
<p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to ML Engineering</em>, we saw the scenario<a id="_idIndexMarker1023"/> of a taxi firm that wanted to analyze anomalous rides at the end of every day. The customer had the following requirements:</p>
<ul>
<li class="bulletList">Rides should be clustered based on ride distance and time, and anomalies/outliers identified.</li>
<li class="bulletList">Speed (distance/time) was not to be used, as analysts would like to understand long-distance rides or those with a long duration.</li>
<li class="bulletList">The analysis should be carried out on a daily schedule.</li>
<li class="bulletList">The data for inference should be consumed from the company’s data lake.</li>
<li class="bulletList">The results should be made available for consumption by other company systems.</li>
</ul>
<p class="normal">Based on the description in the introduction to this chapter, we can now add some extra requirements:</p>
<ul>
<li class="bulletList">The system’s results should contain information on the rides classification as well as a summary of relevant textual data.</li>
<li class="bulletList">Only anomalous rides need to have textual data summarized.</li>
</ul>
<p class="normal">As we did in <em class="chapterRef">Chapter 2</em>, <em class="italic">The Machine Learning Development Process</em>, and <em class="chapterRef">Chapter 8</em>, <em class="italic">Building an Example ML Microservice</em>, we can now build out some user stories<a id="_idIndexMarker1024"/> from these requirements, as follows:</p>
<ul>
<li class="bulletList"><strong class="keyWord">User story 1</strong>: As an operations analyst or data scientist, I want to be given clear labels of rides that are anomalous when considering their ride times in minutes and ride distances in miles so that I can perform further analysis and modeling on the volume of anomalous rides. The criteria for what counts as an anomaly should be determined by an appropriate ML algorithm, which defines an anomaly with respect to the other rides for the same day.</li>
<li class="bulletList"><strong class="keyWord">User story 2</strong>: As an operations analyst or data scientist, I want to be provided with a summary of relevant textual data so that I can do further analysis and modeling on the reasons for some rides being anomalous.</li>
<li class="bulletList"><strong class="keyWord">User story 3</strong>: As an internal application developer, I want all output data sent to a central location, preferably in the cloud, so that I can easily build dashboards and other applications with this data.</li>
<li class="bulletList"><strong class="keyWord">User story 4</strong>: As an operations analyst or data scientist, I would like to receive a report every morning by 09.00. This report should clearly show which rides were anomalous or “normal” as defined by the selected ML algorithm. This will enable me to update my analyses and provide an update to the logistics managers.</li>
</ul>
<p class="normal">User story 1 should be taken care<a id="_idIndexMarker1025"/> of by our general clustering<a id="_idIndexMarker1026"/> approach, especially since we are using the <strong class="keyWord">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="keyWord">DBSCAN</strong>) algorithm, which provides a label of <em class="italic">-1</em> for outliers.</p>
<p class="normal">User story 2 can be accommodated by leveraging the capabilities of LLMs that we discussed in <em class="chapterRef">Chapter 7</em>, <em class="italic">Deep Learning, Generative AI, and LLMOps</em>. We can send the textual data we are given as part of the input batch to a GPT model with an appropriately formatted prompt; the prompt formatting can be done with LangChain or vanilla Python logic.</p>
<p class="normal">User story 3 means that we have to push the results to a location on the cloud that can then be picked up either by a data engineering pipeline or a web application<a id="_idIndexMarker1027"/> pipeline. To make this as flexible as possible, we will push results to an assigned <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>) <strong class="keyWord">Simple Storage Service</strong> (<strong class="keyWord">S3</strong>) bucket. We will initially export the data in the <strong class="keyWord">JavaScript Object Notation</strong> (<strong class="keyWord">JSON</strong>) format, which we have already met in several chapters, as this is a format<a id="_idIndexMarker1028"/> that is often used in application development and can be read in by most data engineering tools.</p>
<p class="normal">The final user story, user story 4, gives us guidance on the scheduling we require for the system. In this case, the requirements mean we should run a daily batch job.</p>
<p class="normal">Let’s tabulate these thoughts<a id="_idIndexMarker1029"/> in terms<a id="_idIndexMarker1030"/> of some ML solution technical requirements, as shown in <em class="italic">Table 9.1</em>.</p>
<table class="table-container" id="table001-5">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">User Story</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Details</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Technical Requirements</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">As an operations analyst or data scientist, I want to be given clear labels of rides that have anomalously long ride times or distances so that I can perform further analysis and modeling on the volume of anomalous rides.</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Algorithm type = anomaly detection/clustering/outlier detection.</li>
<li class="bulletList">Features = ride time and distance.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">2</p>
</td>
<td class="table-cell">
<p class="normal">As an operations analyst or data scientist, I want to be provided with a summary of relevant textual data so that I can do further analysis and modeling on the reasons for some rides being anomalous.</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Algorithm type = text summarization.</li>
<li class="bulletList">Potential models = transformers like BERT, and LLMs like GPT models.</li>
<li class="bulletList">Input requirements = formatted prompts.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">3</p>
</td>
<td class="table-cell">
<p class="normal">As an internal application developer, I want all output data sent to a central location, preferably in the cloud, so that I can easily build dashboards and other applications with this data.</p>
</td>
<td class="table-cell">
<p class="normal">System output destination = S3 on AWS.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">4</p>
</td>
<td class="table-cell">
<p class="normal">As an operations analyst or data scientist, I would like to see the output data for the previous day’s rides every morning so that I can update my analyses and provide an update to the logistics managers.</p>
</td>
<td class="table-cell">
<p class="normal">Batch frequency = daily.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.1: Translating user stories to technical requirements.</p>
<div class="note">
<p class="normal">The process of taking some user stories and translating them into potential technical requirements is a very important skill for an ML engineer, and it can really help speed up the design and implementation of a potential solution. For the rest of the chapter, we will use the information in <em class="italic">Table 9.1</em>, but to help you practice this skill, can you think of some other potential user stories for the scenario given and what technology requirements these might translate to? Here are some thoughts to get you started:</p>
<ul>
<li class="bulletList">A data scientist in the firm may want to try and build models to predict customer satisfaction based on a variety of features of the rides, including times and perhaps any of the traffic issues mentioned in the textual data. How often may they want this data? What data would they need? What would they do with it specifically?</li>
<li class="bulletList">The developers of the mobile app in the firm may want to have forecasts of expected ride times based on traffic and weather conditions to render for users. How could they do this? Can the data come in batches, or should it be an event-driven solution?</li>
<li class="bulletList">Senior management may want reports compiled of the firm’s performance across several variables in order to make decisions. What sort of data may they want to see? What ML models would provide more insight? How often does the data need to be prepared, and what solutions could the results be shown in?</li>
</ul>
</div>
<p class="normal">Now that we have done some<a id="_idIndexMarker1031"/> of the initial work required to understand what we need the system to do and how it may do it, we can now move on to bringing these together into some initial designs.</p>
<h1 class="heading-1" id="_idParaDest-189">Designing an ETML solution</h1>
<p class="normal">The requirements<a id="_idIndexMarker1032"/> clearly point us to a solution that takes in some data and augments it with ML inference, before outputting the data to a target location. Any design we come up with must encapsulate these steps. This is the description of any ETML solution, and this is one of the most used patterns in the ML world. In my opinion it will remain important for a long time to come as it is particularly suited to ML applications where:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Latency is not critical</strong>: If you can afford to run on a schedule and there are no high-throughput or low-latency response time requirements, then running as an ETML batch is perfectly acceptable.</li>
<li class="bulletList"><strong class="keyWord">You need to batch the data for algorithmic reasons</strong>: A great example of this is the clustering approach we will use here. There are ways to perform clustering in an online setting, where the model is continually updated as new data comes in, but some approaches are simpler if you have all the relevant data taken together in the relevant batch. Similar arguments can apply to deep learning models, which will require large batches of data to be processed on GPUs in parallel for maximum efficiency.</li>
<li class="bulletList"><strong class="keyWord">You do not have event-based or streaming mechanisms available</strong>: Many organizations may still operate in batch mode simply because they have to! It can require investment to move to appropriate platforms that work in a different mode, and this may not have always been made available.</li>
<li class="bulletList"><strong class="keyWord">It is simpler</strong>: Related to the previous point, getting event-based or streaming systems set up can take some learning for your team, whereas batching is relatively intuitive and easy to get started with.</li>
</ul>
<p class="normal">Now, let’s start discussing the design. The key elements our design has to cover were articulated in <em class="italic">Table 9.1</em>. We can then start to build out a design diagram that covers the most important aspects, including starting to pin down which technologies are used for which processes. <em class="italic">Figure 9.4</em> shows a simplified design diagram that starts to do this and shows how we can use an Airflow pipeline to pull data from an S3 bucket, storing our clustered data in S3 as an intermediate data storage step, before proceeding to summarize the text data using the LLM and exporting the final result to our final target S3 location.</p>
<figure class="mediaobject"><img alt="" height="351" role="presentation" src="../Images/B19525_09_04.png" width="825"/></figure>
<p class="packt_figref">Figure 9.4: High-level design for the ETML clustering and summarization system. Steps 1–3 of the overall pipelines are the clustering steps and 4–6 are the summarization steps.</p>
<p class="normal">In the next section, we will look at some potential tools we can use to solve this problem, given everything we have learned in previous chapters.</p>
<h1 class="heading-1" id="_idParaDest-190">Selecting the tools</h1>
<p class="normal">For this example, and pretty much whenever we have an ETML problem, our main considerations boil down<a id="_idIndexMarker1033"/> to a few simple things, namely the selection of the interfaces we need to build, the tools we need to perform the transformation and modeling at the scale we require, and how we orchestrate all of the pieces together. The next few sections will cover each of these in turn.</p>
<h2 class="heading-2" id="_idParaDest-191">Interfaces and storage</h2>
<p class="normal">When we execute the extract<a id="_idIndexMarker1034"/> and load parts of ETML, we need to consider<a id="_idIndexMarker1035"/> how to interface with the systems that store our data. It is important that whichever database or data technology we extract from, we use the appropriate tools to extract at whatever scale and pace we need. In this example, we can use S3 on AWS for our storage; our interfacing can be taken care of by the AWS <code class="inlineCode">boto3</code> library and the AWS CLI. Note that we could have selected a few other approaches, some of which are listed in <em class="italic">Table 9.2</em> along with their pros and cons.</p>
<table class="table-container" id="table002-3">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Potential Tools</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Pros</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Cons</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">The AWS CLI, S3, and <code class="inlineCode">boto3</code></p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Relatively simple to use, and extensive documentation.</li>
<li class="bulletList">Connects to a wide variety of other AWS tools and services.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Not cloud-agnostic.</li>
<li class="bulletList">Not applicable to other environments or technologies.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">SQL database and JDBC/ODBC connector</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Relatively tool-agnostic.</li>
<li class="bulletList">Cross-platform and cloud.</li>
<li class="bulletList">Optimized storage and querying possible.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Requires data modeling and database administration.</li>
<li class="bulletList">Not optimized for unstructured data.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Vendor-supplied cloud data warehouse via their API</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Often good documentation and examples to follow.</li>
<li class="bulletList">Good optimizations in place.</li>
<li class="bulletList">Modern platforms have good connectivity to other well-used platforms.</li>
<li class="bulletList">Managed services available across multiple clouds.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Requires data modeling and database administration.</li>
<li class="bulletList">Can sometimes support unstructured data but is not always easy to implement.</li>
<li class="bulletList">Can be expensive.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.2: Data storage and interface options for the ETML solution with some potential pros and cons.</p>
<p class="normal">Based on these options, it seems like using the AWS CLI, S3 and the <code class="inlineCode">boto3</code> package will be the simplest mechanism that provides the most flexibility in this scenario. In the next section, we will consider the decisions we must make around the scalability of our modeling approach. This is very important when working<a id="_idIndexMarker1036"/> with batches of data, which could be extremely<a id="_idIndexMarker1037"/> large in some cases.</p>
<h2 class="heading-2" id="_idParaDest-192">Scaling of models</h2>
<p class="normal">In <em class="chapterRef">Chapter 6</em>, <em class="italic">Scaling Up</em>, we discussed<a id="_idIndexMarker1038"/> some of the mechanisms to scale up our analytics and ML workloads. We should ask ourselves whether any of these, or even other methods, apply to the use case at hand and use them accordingly. This works the other way too: if we are looking at relatively small amounts of data, there is no need to provision a large amount of infrastructure, and there may be no need to spend time creating very optimized processing. Each case should be examined on its own merits and within its own context.</p>
<p class="normal">We have listed some of the options for the clustering part of the solution and their pros and cons in <em class="italic">Table 9.3</em>.</p>
<table class="table-container" id="table003-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Potential Tools</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Pros</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Cons</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Spark ML</p>
</td>
<td class="table-cell">
<p class="normal">Can scale to very large datasets.</p>
</td>
<td class="table-cell">
<p class="normal">Requires cluster management.</p>
<p class="normal">Can have large overheads for smaller datasets or processing requirements.</p>
<p class="normal">Relatively limited algorithm set.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Spark with pandas <strong class="keyWord">User-Defined Function</strong> (<strong class="keyWord">UDF</strong>)</p>
</td>
<td class="table-cell">
<p class="normal">Can scale to very large datasets.</p>
<p class="normal">Can use any Python-based algorithm.</p>
</td>
<td class="table-cell">
<p class="normal">Might not make sense for some problems where parallelization is not easily applicable.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Scikit-learn</p>
</td>
<td class="table-cell">
<p class="normal">Well known by many data scientists.</p>
<p class="normal">Can run on many different types of infrastructure.</p>
<p class="normal">Small overheads for training and serving.</p>
</td>
<td class="table-cell">
<p class="normal">Not very inherently scalable.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Ray AIR or Ray Serve</p>
</td>
<td class="table-cell">
<p class="normal">Relatively easy-to-use API.</p>
<p class="normal">Good integration with many ML libraries.</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Requires cluster management with new types of clusters (Ray clusters).</li>
<li class="bulletList">Newer skillset for ML engineers.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.3: Options for the modeling part of the ETML solution, with their pros and cons and with a particular focus on scalability and ease of use.</p>
<p class="normal">Given these options, and assuming<a id="_idIndexMarker1039"/> that the data volumes are not large for this example, we can comfortably stick with the Scikit-learn modeling approach, as this provides maximum flexibility and will likely be most easily usable by the data scientists in the team. It should be noted that the conversion of the Scikit-learn code to using a pandas UDF in Spark can be accomplished at a later date, with not too much work, if more scalable behavior is required.</p>
<p class="normal">As explained above, however, clustering is only one part of the “ML” in this ETML solution, the other being the text summarization<a id="_idIndexMarker1040"/> part. Some potential options and pros and cons are shown in <em class="italic">Table 9.4</em>.</p>
<table class="table-container" id="table004-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Potential Tools</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Pros</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Cons</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">GPT-X models from OpenAI (or another vendor)</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Simple to use – we’ve already met this in <em class="chapterRef">Chapter 7</em>, <em class="italic">Deep Learning, Generative AI, and LLMOps</em>.</li>
<li class="bulletList">Potentially the most performant models available.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Can become expensive.</li>
<li class="bulletList">Not as in control of the model.</li>
<li class="bulletList">Lack of visibility of data and model lineage.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Open-source LLMs</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">More transparent lineage of the data and model.</li>
<li class="bulletList">More stable (you are in control of the model).</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Large infrastructure requirements.</li>
<li class="bulletList">Requires very specialized skills if optimization is required.</li>
<li class="bulletList">More operational management is required (LLMOps).</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Any other non-LLM deep learning model, e.g., a BERT variant</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Can be easier to set up than some open-source LLMs.</li>
<li class="bulletList">Extensively studied and documented.</li>
<li class="bulletList">Easier to retrain and fine-tune (smaller models).</li>
<li class="bulletList">Vanilla MLOps applicable.</li>
<li class="bulletList">May not require prompt engineering.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">More operational overhead than an API call (but less than hosting an LLM).</li>
<li class="bulletList">Less performant.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.4: Tooling options for the text summarization component of the EMTL solution.</p>
<p class="normal">Now that we have explored<a id="_idIndexMarker1041"/> the tooling decisions we have to make around scalable ML models, we will move on to another important topic for ETML solutions—how we manage the scheduling of batch processing.</p>
<h2 class="heading-2" id="_idParaDest-193">Scheduling of ETML pipelines</h2>
<p class="normal">The kind of batch process that ETML<a id="_idIndexMarker1042"/> corresponds to is often something that ties in quite nicely with daily batches, but given the other two points outlined previously, we may need to be careful about when we schedule our jobs—for example, a step in our pipeline may need to connect to a production database that does not have a read replica (a copy of the database specifically just for reading). If this were the case, then we may cause major performance issues for users of any solutions utilizing that database if we start hammering it with queries at 9 a.m. on a Monday morning. Similarly, if we run overnight and want to load into a system that is undergoing other batch upload processes, we may create resource contention, slowing down the process. There is no <em class="italic">one-size-fits-all</em> answer here; it is just important to consider your options. We look at the pros and cons of using some of the tools we have met throughout the book for the scheduling and job management<a id="_idIndexMarker1043"/> of this problem in the following table:</p>
<table class="table-container" id="table005-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Potential Tools</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Pros</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Cons</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Apache Airflow</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Good scheduling management.</li>
<li class="bulletList">Relatively easy-to-use API.</li>
<li class="bulletList">Good documentation.</li>
<li class="bulletList">Cloud-hosted services are available, such as AWS <strong class="keyWord">Managed Workflows for Apache Airflow</strong> (<strong class="keyWord">MWAA</strong>).</li>
<li class="bulletList">Flexible for use across ML, data engineering, and other workloads.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Can take time to test pipelines.</li>
<li class="bulletList">Cloud services like MWAA can be expensive.</li>
<li class="bulletList">Airflow is relatively general-purpose (potentially also a pro), and does not have too much specific functionality for ML workloads.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">ZenML</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Relatively easy-to-use API.</li>
<li class="bulletList">Good documentation.</li>
<li class="bulletList">Designed for ML engineers.</li>
<li class="bulletList">Multiple useful MLOps integrations.</li>
<li class="bulletList">A cloud option is available.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Can take time to test pipelines.</li>
<li class="bulletList">Slightly steeper learning curve compared to Airflow.</li>
</ul>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Kubeflow</p>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Relatively easy-to-use API.</li>
<li class="bulletList">Good documentation.</li>
<li class="bulletList">Simplifies the use of Kubernetes substantially if that is required.</li>
</ul>
</td>
<td class="table-cell">
<ul>
<li class="bulletList">Requires an AWS variant for use on AWS.</li>
<li class="bulletList">Slightly steeper learning curve than the others.</li>
<li class="bulletList">Can be harder to debug at times due to Kubernetes under the hood.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.5: Pros and cons of using Apache Airflow to manage our scheduling.</p>
<p class="normal">Given what we have in <em class="italic">Tables 9.3</em>, <em class="italic">9.4</em>, and <em class="italic">9.5</em>, all of the options considered have really strong pros and not too many cons. This means that there are many possible combinations of technology that will allow us to solve our problems. The requirements for our problem have stipulated that we have relatively small datasets that need to be processed in batches every day, first with some kind of clustering or anomaly detection algorithm before further analysis using an LLM. We can see that selecting <code class="inlineCode">scikit-learn</code> for the modeling package, a GPT model from OpenAI called via the API, and Apache Airflow for orchestration can fit the bill. Again, this is not the only combination we could have gone with. It might be fun for you to take the example we work through in the rest of the chapter and try some of the other tools. Knowing multiple ways to do something is a key skill for an ML engineer that can help you adapt<a id="_idIndexMarker1044"/> to many different situations.</p>
<p class="normal">The next section will discuss how we can proceed with the execution of the solution, given this information.</p>
<h1 class="heading-1" id="_idParaDest-194">Executing the build</h1>
<p class="normal">Execution of the<a id="_idIndexMarker1045"/> build, in this case, will be very much<a id="_idIndexMarker1046"/> about how we take the <strong class="keyWord">proof-of-concept</strong> code shown in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to ML Engineering</em>, and then split this out into components that can be called by another scheduling tool such as Apache Airflow. </p>
<p class="normal">This will provide a showcase of how we can apply some of the ML engineering skills we learned throughout the book. In the next few sections, we will focus on how to build out an Airflow pipeline that leverages a series of different ML capabilities, creating a relatively complex solution in just a few lines of code.</p>
<h2 class="heading-2" id="_idParaDest-195">Building an ETML pipeline with advanced Airflow features</h2>
<p class="normal">We already discussed Airflow<a id="_idIndexMarker1047"/> in detail in <em class="chapterRef">Chapter 5</em>, <em class="italic">Deployment Patterns and Tools</em>, but there we covered <a id="_idIndexMarker1048"/>more of the details around how to deploy your DAGs on the cloud. Here we will focus on building in more advanced capabilities and control flows into your DAGs. We will work locally here on the understanding that when you want to deploy, you can use the process outlined in <em class="chapterRef">Chapter 5</em>.</p>
<p class="normal">First we will look at some good DAG design practices. Many of these are direct applications of some of the good software engineering practices we discussed throughout the book; for a good review of some of these you can go back to <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>. Here we will emphasize how these can be applied to Airflow:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Embody separation of concerns for your tasks</strong>: As discussed in <em class="chapterRef">Chapter 4</em>, separation of concerns is all about ensuring that specific pieces of code or software perform specific functions with minimal overlap. This can also be phrased as “atomicity,” using the analogy of building up your solution with “atoms” of specific, focused functionality. At the level of Airflow DAGs we can embody this principle by ensuring our DAGs are built of tasks that have one clear job to do in each case. So, in this example we clearly have the “extract,” “transform,” “ML,” and “load” stages, for which it makes sense to have specific tasks in each case. Depending on the complexity of those tasks they may even be split further. This also helps us to create good control flows and error handling, as it is far easier to anticipate, test, and manage failure modes for smaller, more atomic pieces of code. We will see this in practice with the code examples in this section.</li>
<li class="bulletList"><strong class="keyWord">Use retries</strong>: You can pass in several arguments to Airflow steps that help control how the task will operate under different circumstances. An important aspect of this is the concept of “retries,” which tells the task to, you guessed it, try the process again if there is a failure. This is a nice way to build in some resiliency to your system, as there could be all sorts of reasons for a temporary failure in a process, such as a drop in network connectivity if it includes a REST API call over HTTP. You can also introduce delays between retries and even exponential backoff, which is when your retries have increasing time delays between them. This can help if you hit an API with rate limits, for example, where the exponential backoff will mean the system is allowed to hit the endpoint again.</li>
<li class="bulletList"><strong class="keyWord">Mandate idempotency in your DAGs</strong>: Idempotency is the quality of code that returns the same result when run multiple times on the same inputs. It can easily be assumed that most programs work this way but that is definitely not the case. We have made extensive use of Python objects in this book that contain internal states, for example, ML models in <code class="inlineCode">scikit-learn</code> or neural networks in PyTorch. This means that it should not be taken for granted that idempotency is a feature of your solution. Idempotency is very useful, especially in ETML pipelines, because it means if you need to perform retries then you know that this will not lead to unexpected side effects. You can enforce idempotency at the level of your DAG by enforcing it at the level of your tasks that make up the DAG. The challenge for an EMTL application is that we obviously have ML models, which I’ve just mentioned can be a challenge for this concept! So, some thought is required to make sure that retries and the ML steps of your pipeline can play nicely together.</li>
<li class="bulletList"><strong class="keyWord">Leverage the Airflow operators and provider package ecosystem</strong>: Airflow comes with a large list of operators to perform all sorts of tasks, and there are several packages designed<a id="_idIndexMarker1049"/> to help integrate with other tools called <em class="italic">provider packages</em>. The advice here is to <strong class="keyWord">use them</strong>. This speaks to the points discussed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up,</em> about “not reinventing the wheel” and ensures that you can focus on building the appropriate logic you need for your workloads and system and not on creating boilerplate integrations. For example, we can use the Spark provider package. We can install it with:
        <pre class="programlisting con"><code class="hljs-con">pip install apache-airflow
pip install pyspark
pip install apache-airflow-providers-apache-spark
</code></pre>
<p class="normal">Then in a DAG<a id="_idIndexMarker1050"/> we can submit a Spark<a id="_idIndexMarker1051"/> application, for example one contained within a script called <code class="inlineCode">spark-script.py</code>, for a run with:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow.models <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.providers.apache.spark.operators.spark_jdbc <span class="hljs-keyword">import</span> SparkJDBCOperator
<span class="hljs-keyword">from</span> airflow.providers.apache.spark.operators.spark_sql <span class="hljs-keyword">import</span> SparkSqlOperator
<span class="hljs-keyword">from</span> airflow.providers.apache.spark.operators.spark_submit <span class="hljs-keyword">import</span> SparkSubmitOperator

DAG_ID = <span class="hljs-string">"spark_example"</span>
<span class="hljs-keyword">with</span> DAG(
    dag_id=DAG_ID,
    schedule=<span class="hljs-literal">None</span>,
    start_date=datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>),
    catchup=<span class="hljs-literal">False</span>,
    tags=[<span class="hljs-string">"example"</span>],
) <span class="hljs-keyword">as</span> dag:
    submit_job = SparkSubmitOperator(
        application=\
<span class="hljs-string">           "${SPARK_HOME}/examples/src/main/python/spark-script.py"</span>,
           task_id=<span class="hljs-string">"submit_job"</span>
    )
</code></pre></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Use</strong> <code class="inlineCode">with DAG() as dag</code>: In the example<a id="_idIndexMarker1052"/> above you will see<a id="_idIndexMarker1053"/> that we used this pattern. This context manager pattern is one of the three main ways you can define a DAG, the other two being using the DAG constructor and passing it to all tasks in your pipeline or using a decorator to convert a function to a DAG. The use of a context manager means, as in any usage of it in Python, that any resources defined within the context are cleaned up correctly even if the code block exists with an exception. The mechanism that uses the constructor requires passing <code class="inlineCode">dag=dag_name</code> into every task you define in the pipeline, which is quite laborious. Using the decorator is quite clean for basic DAGs but can become quite hard to read and maintain if you build more complex DAGs.</li>
<li class="bulletList"><strong class="keyWord">Remember to test (!)</strong>: After reading the rest of this book and becoming a confident ML engineer, I can hear you shouting at the page, “What about testing?!”, and you would be right to shout. Our code is only as good as the tests we can run on it. Luckily for us Airflow provides some out-of-the-box functionality that enables local testing and debugging of your DAGs. For debugging in your IDE or editor, if your DAG is called <code class="inlineCode">dag</code>, like in the example above, all you need to do is add the below snippet to your DAG definition file to run the DAG in a local, serialized Python process within your chosen debugger. This does not run the scheduler; it just runs the DAG steps in a single process, which means it fails fast and gives quick feedback to the developer:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    dag.test()
</code></pre>
<p class="normal">We can also use <code class="inlineCode">pytest</code> like we did in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>, and elsewhere in the book.</p></li>
</ul>
<p class="normal">Now that we have discussed some of the important concepts we can use, we will start to build up the Airflow DAG in some detail, and we will try and do this in a way that shows you how to build some resiliency into the solution.</p>
<p class="normal">First, it is important to note that, for this example, we will actually perform the ETML process twice: once for the clustering component and once for the text summarization. Doing it this way means that we can use <em class="italic">intermediary storage</em> in between the steps, in this case, AWS S3 again, in order to introduce some resiliency into the system. This is so because if the second step fails, it doesn’t mean the first step’s processing is lost. The example we will walk through does this in a relatively straightforward way, but as always in this book, remember that the concepts can be extended and adapted to use tooling and processes of your choice, as long as the fundamentals remain solid.</p>
<p class="normal">Let’s start building this DAG! It is a relatively short one that only contains two tasks; we will show the DAG first and then expand on the details:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> annotations
<span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> pendulum
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator
<span class="hljs-keyword">from</span> utils.summarize <span class="hljs-keyword">import</span> LLMSummarizer
<span class="hljs-keyword">from</span> utils.cluster <span class="hljs-keyword">import</span> Clusterer
<span class="hljs-keyword">import</span> logging

logging.basicConfig(level=logging.INFO)

<span class="hljs-comment"># Bucket name could be read in as an environment variable.</span>
bucket_name = <span class="hljs-string">"etml-data"</span>
date = datetime.datetime.now().strftime(<span class="hljs-string">"%Y%m%d"</span>)
file_name = <span class="hljs-string">f"taxi-rides-</span><span class="hljs-subst">{date}</span><span class="hljs-string">.json"</span>

<span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"etml_dag"</span>,
    start_date=pendulum.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>),
    schedule_interval=<span class="hljs-string">"@daily"</span>,
    catchup=<span class="hljs-literal">False</span>,
) <span class="hljs-keyword">as</span> dag:
    logging.info(<span class="hljs-string">"DAG started ..."</span>)
    logging.info(<span class="hljs-string">"Extracting and clustering data ..."</span>)
    extract_cluster_load_task = PythonOperator(
        task_id=<span class="hljs-string">"extract_cluster_save"</span>,
        python_callable=Clusterer(bucket_name, file_name).\
                                  cluster_and_label,
        op_kwargs={<span class="hljs-string">"features"</span>: [<span class="hljs-string">"ride_dist"</span>, <span class="hljs-string">"ride_time"</span>]}
    )
    
    logging.info(<span class="hljs-string">"Extracting and summarizing data ..."</span>)
    extract_summarize_load_task = PythonOperator(
        task_id=<span class="hljs-string">"extract_summarize"</span>,
        python_callable=LLMSummarizer(bucket_name, file_name).summarize
    )
    
    extract_cluster_load_task &gt;&gt; extract_summarize_load_task 
</code></pre>
<p class="normal">As you can see from the code<a id="_idIndexMarker1054"/> snippet, there are two tasks, and they follow<a id="_idIndexMarker1055"/> sequentially after one another using the <code class="inlineCode">&gt;&gt;</code> operator. Each task performs the following pieces of work:</p>
<ul>
<li class="bulletList"><code class="inlineCode">extract_cluster_load_task</code>: This task will extract the input data from the appropriate S3 bucket and perform some clustering using DBSCAN, before loading the original data joined to the model output to the intermediary storage location. For simplicity we will use the same bucket for intermediate storage, but this could be any location or solution that you have connectivity to.</li>
<li class="bulletList"><code class="inlineCode">extract_summarize_load_task</code>: Similarly, the first step here is to extract the data from S3 using the boto3 library. The next step is to take the data and then call the appropriate LLM to perform summarization on the text fields selected in the data, specifically those that contain information on the local news, weather, and traffic reports for the day of the batch run.</li>
</ul>
<p class="normal">You will have noticed upon reading the DAG that the reason the DAG definition is so short is that we have abstracted away most of the logic into subsidiary modules, in line with the principles of keeping it simple, separating our concerns, and applying modularity. See <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>, for an in-depth discussion of these and other important concepts.</p>
<p class="normal">The first component<a id="_idIndexMarker1056"/> that we use in the DAG is the functionality contained in the <code class="inlineCode">Clusterer</code> class under <code class="inlineCode">utils.cluster</code>. The full definition<a id="_idIndexMarker1057"/> of this class is given below. I have omitted standard imports for brevity:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> boto3
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN
<span class="hljs-keyword">from</span> utils.extractor <span class="hljs-keyword">import</span> Extractor

model_params = {
    <span class="hljs-string">'eps'</span>: <span class="hljs-number">0.3</span>,
    <span class="hljs-string">'min_samples'</span>: <span class="hljs-number">10</span>,
}

<span class="hljs-keyword">class</span> <span class="hljs-title">Clusterer</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self, bucket_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, </span>
<span class="hljs-params">        file_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, </span>
<span class="hljs-params">        model_params: </span><span class="hljs-built_in">dict</span><span class="hljs-params"> = model_params</span>
<span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:
        self.model_params = model_params
        self.bucket_name = bucket_name
        self.file_name = file_name
        
    <span class="hljs-keyword">def</span> <span class="hljs-title">cluster_and_label</span>(<span class="hljs-params">self, features: </span><span class="hljs-built_in">list</span>) -&gt; <span class="hljs-literal">None</span>:
        extractor = Extractor(self.bucket_name, self.file_name)
        df = extractor.extract_data()
        df_features = df[features]
        df_features = StandardScaler().fit_transform(df_features)
        db = DBSCAN(**self.model_params).fit(df_features)
        <span class="hljs-comment"># Find labels from the clustering</span>
        core_samples_mask = np.zeros_like(db.labels_, dtype=<span class="hljs-built_in">bool</span>)
        core_samples_mask[db.core_sample_indices_] = <span class="hljs-literal">True</span>
        labels = db.labels_
        <span class="hljs-comment"># Add labels to the dataset and return.</span>
        df[<span class="hljs-string">'label'</span>] = labels
        
        date = datetime.datetime.now().strftime(<span class="hljs-string">"%Y%m%d"</span>)
        boto3.client(<span class="hljs-string">'s3'</span>).put_object(
            Body=df.to_json(orient=<span class="hljs-string">'records'</span>), 
            Bucket=self.bucket_name, 
            Key=<span class="hljs-string">f"clustered_data_</span><span class="hljs-subst">{date}</span><span class="hljs-string">.json"</span>
        )
</code></pre>
<p class="normal">Note that the constructor of the class contains a reference to a default set of <code class="inlineCode">model_params</code>. These can be read in from a configuration file. I have included them here for visibility. The actual clustering and labeling method within the class is relatively simple; it just standardizes the incoming features, applies the DBSCAN clustering algorithm, and then exports the originally extracted dataset, which now includes the cluster labels. One important point to note is that the features used for clustering are supplied as a list so that this can be extended in the future if desired, or if richer data for clustering is available, simply by altering the <code class="inlineCode">op_kwargs</code> argument supplied to the first task’s <code class="inlineCode">PythonOperator</code> object in the DAG. </p>
<p class="normal">After the first task, which uses the <code class="inlineCode">Clusterer</code> class, runs successfully, a JSON is produced giving the source records and their cluster labels. Two random examples from the produced file are given in <em class="italic">Figure 9.5</em>.</p>
<figure class="mediaobject"><img alt="A picture containing text, screenshot, font, document  Description automatically generated" height="396" src="../Images/B19525_09_05.png" width="825"/></figure>
<p class="packt_figref">Figure 9.5: Two example records of the data produced after the clustering step in the ETML pipeline.</p>
<p class="normal">You may have noticed<a id="_idIndexMarker1058"/> that, at the top<a id="_idIndexMarker1059"/> of this example, another utility class is imported, the <code class="inlineCode">Extractor</code> class from the <code class="inlineCode">utils.extractor</code> module. This is simply a wrapper around some <code class="inlineCode">boto3</code> functionality and is defined below:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> boto3

<span class="hljs-keyword">class</span> <span class="hljs-title">Extractor</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, bucket_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, file_name: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-literal">None</span>:
        self.bucket_name = bucket_name
        self.file_name = file_name

    <span class="hljs-keyword">def</span> <span class="hljs-title">extract_data</span>(<span class="hljs-params">self</span>) -&gt; pd.DataFrame:
        s3 = boto3.client(<span class="hljs-string">'s3'</span>)
        obj = s3.get_object(Bucket=self.bucket_name, Key=self.file_name)
        df = pd.read_json(obj[<span class="hljs-string">'Body'</span>])
        <span class="hljs-keyword">return</span> df
</code></pre>
<p class="normal">Now let’s move on to the definition of the other class used in the DAG, the <code class="inlineCode">LLMSummarizer</code> from the <code class="inlineCode">utils.summarize</code> module:</p>
<pre class="programlisting code"><code class="hljs-code">openai.api_key = os.environ[<span class="hljs-string">'OPENAI_API_KEY'</span>]

<span class="hljs-keyword">class</span> <span class="hljs-title">LLMSummarizer</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, bucket_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, file_name: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-literal">None</span>:
        self.bucket_name = bucket_name
        self.file_name = file_name

    <span class="hljs-keyword">def</span> <span class="hljs-title">summarize</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        extractor = Extractor(self.bucket_name, self.file_name)
        df = extractor.extract_data()
        df[<span class="hljs-string">'summary'</span>] = <span class="hljs-string">''</span>
        
        df[<span class="hljs-string">'prompt'</span>] = df.apply(
                    <span class="hljs-keyword">lambda</span> x:self.format_prompt(
                        x[<span class="hljs-string">'news'</span>],
                        x[<span class="hljs-string">'</span><span class="hljs-string">weather'</span>],
                        x[<span class="hljs-string">'traffic'</span>]
                    ),
                    axis=<span class="hljs-number">1</span>
<span class="hljs-number">        </span>)
        df.loc[df[<span class="hljs-string">'label'</span>]==-<span class="hljs-number">1</span>, <span class="hljs-string">'summary'</span>] = df.loc[df[<span class="hljs-string">'label'</span>]==-<span class="hljs-number">1</span>,
               <span class="hljs-string">'prompt'</span>].apply(<span class="hljs-keyword">lambda</span> x: self.generate_summary(x))
        date = datetime.datetime.now().strftime(<span class="hljs-string">"</span><span class="hljs-string">%Y%m%d"</span>)
        boto3.client(<span class="hljs-string">'s3'</span>).put_object(
            Body=df.to_json(orient=<span class="hljs-string">'records'</span>), 
            Bucket=self.bucket_name, 
            Key=<span class="hljs-string">f"clustered_summarized_</span><span class="hljs-subst">{date}</span><span class="hljs-string">.json"</span>
        )
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">format_prompt</span>(<span class="hljs-params">self, news: </span><span class="hljs-built_in">str</span><span class="hljs-params">, weather: </span><span class="hljs-built_in">str</span><span class="hljs-params">, traffic: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:
        prompt = dedent(<span class="hljs-string">f'''</span>
<span class="hljs-string">            The following information describes conditions relevant to</span>
<span class="hljs-string">            taxi journeys through a single day in Glasgow, Scotland.</span>
<span class="hljs-string">            News: </span><span class="hljs-subst">{news}</span>
<span class="hljs-string">            Weather: </span><span class="hljs-subst">{weather}</span>
<span class="hljs-string">            Traffic: </span><span class="hljs-subst">{traffic}</span>
<span class="hljs-string">            Summarise the above information in 3 sentences or less.</span>
<span class="hljs-string">            '''</span>)
        <span class="hljs-keyword">return</span> prompt

    <span class="hljs-keyword">def</span> <span class="hljs-title">generate_summary</span>(<span class="hljs-params">self, prompt: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-comment"># Try chatgpt api and fall back if not working</span>
        <span class="hljs-keyword">try</span>:
            response = openai.ChatCompletion.create(
                model = <span class="hljs-string">"gpt-3.5-turbo"</span>,
                temperature = <span class="hljs-number">0.3</span>,
                messages = [{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt}]
            )
            <span class="hljs-keyword">return</span> response.choices[<span class="hljs-number">0</span>].message[<span class="hljs-string">'</span><span class="hljs-string">content'</span>]
        <span class="hljs-keyword">except</span>:
            response = openai.Completion.create(
                model=<span class="hljs-string">"text-davinci-003"</span>,
                prompt = prompt
            )
            <span class="hljs-keyword">return</span> response[<span class="hljs-string">'choices'</span>][<span class="hljs-number">0</span>][<span class="hljs-string">'text'</span>]
</code></pre>
<p class="normal">You can see that this class<a id="_idIndexMarker1060"/> follows a similar design pattern<a id="_idIndexMarker1061"/> to the one used in the <code class="inlineCode">Clusterer</code> class, only now the method that we utilize has the role of prompting an OpenAI LLM of our choosing, using a standard template that we have hardcoded. Again, this prompt template can be extracted out into a configuration file that is packaged up with the solution, but is shown here for visibility. The prompt asks that the LLM summarizes the relevant pieces of contextual information supplied, concerning local news, weather, and traffic reports, so that we have a concise summary that can be used for downstream analysis or to render in a user interface. A final important point to note here is that the method to generate the summary, which wraps a call to OpenAI APIs, has a <code class="inlineCode">try except</code> clause that will allow for a fallback to a different model if the first model call experiences any issues. At the time of writing in May 2023, OpenAI APIs still show some brittleness when it comes to latency and rate limits, so steps like this allow you to build in more resilient workflows.</p>
<p class="normal">An example output from the application of the <code class="inlineCode">LLMSummarizer</code> class when running the DAG is given in <em class="italic">Figure 9.6</em>.</p>
<figure class="mediaobject"><img alt="A picture containing text, screenshot, font  Description automatically generated" height="300" src="../Images/B19525_09_06.png" width="825"/></figure>
<p class="packt_figref">Figure 9.6: An example output from the LLMSummarizer; you can see that it takes in the news, weather, and traffic information and produces a concise summary that can help any downstream consumers of this data understand what it means for overall traffic conditions.</p>
<p class="normal">A potential area<a id="_idIndexMarker1062"/> of optimization in this piece of code<a id="_idIndexMarker1063"/> is around the prompt template used, as there is the potential for some nice prompt engineering to try and shape the output from the LLM to be more consistent. You could also use a tool like LangChain, which we met in <em class="chapterRef">Chapter 7</em>, <em class="italic">Deep Learning, Generative AI, and LLMOps</em>, to perform more complex prompting of the model. I leave this as a fun exercise for the reader.</p>
<p class="normal">Now that we have defined all the logic for our DAG and the components it uses, how will we actually configure<a id="_idIndexMarker1064"/> this to run, even in standalone mode? When we deployed a DAG to <strong class="keyWord">MWAA</strong>, the AWS-hosted and managed Airflow solution, in <em class="chapterRef">Chapter 4</em>, you may recall that we had to send our DAG to a specified bucket that was then read in by the system. </p>
<p class="normal">For your own hosted or local Airflow instances the same point applies; this time we need to send the DAG to a <code class="inlineCode">dags</code> folder, which is located in the <code class="inlineCode">$AIRFLOW_HOME</code> folder. If you have not explicitly configured this for your installation of Airflow you will use the default, which is typically in a folder called <code class="inlineCode">airflow</code> under your <code class="inlineCode">home</code> directory. To find this and lots of other useful information you can execute the following command, which produces the output shown in <em class="italic">Figure 9.7</em>:</p>
<pre class="programlisting con"><code class="hljs-con">airflow info
</code></pre>
<figure class="mediaobject"><img alt="A screenshot of a computer program  Description automatically generated with medium confidence" height="813" src="../Images/B19525_09_07.png" width="611"/></figure>
<p class="packt_figref">Figure 9.7: The output from the airflow info command.</p>
<p class="normal">Once you have found<a id="_idIndexMarker1065"/> the location of the <code class="inlineCode">$AIRFLOW_HOME</code> folder, if there isn’t a folder <a id="_idIndexMarker1066"/>called <code class="inlineCode">dags</code> already, create one. For simple, self-contained DAGs that do not use sub-modules, all you need to do to mock up deployment is to copy the DAG over to this folder, similar to how we sent our DAG to S3 in the example with MWAA in <em class="chapterRef">Chapter 5</em>. Since we use multiple sub-modules in this example we can either decide to install them as a package, using the techniques developed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>, and make sure that it is available in the Airflow environment, or we can simply send the sub-modules into the same <code class="inlineCode">dags</code> folder. For simplicity that is what we will do here, but please consult the official Airflow documentation for details on this.</p>
<p class="normal">Once we have copied over the code, if we access the Airflow UI, we should be able to see our DAG like in <em class="italic">Figure 9.8</em>. As long as the Airflow server is running, the DAG will run on the supplied schedule. You can also trigger manual runs in the UI for testing.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="292" src="../Images/B19525_09_08.png" width="825"/></figure>
<p class="packt_figref">Figure 9.8: The ETML DAG in the Airflow UI.</p>
<p class="normal">Running the DAG will result in the intermediary and final output JSON files being created in the S3 bucket, as shown in <em class="italic">Figure 9.9</em>.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with low confidence" height="405" src="../Images/B19525_09_09.png" width="825"/></figure>
<p class="packt_figref">Figure 9.9: The successful run of the DAG creates the intermediate and final JSON files.</p>
<p class="normal">And with that, we have now built an ETML pipeline that takes in some taxi ride data, clusters this based on ride distance and time, and then performs text summarization on some contextual<a id="_idIndexMarker1067"/> information<a id="_idIndexMarker1068"/> using an LLM.</p>
<h1 class="heading-1" id="_idParaDest-196">Summary</h1>
<p class="normal">This chapter has covered how to apply a lot of the techniques learned in this book, in particular from <em class="chapterRef">Chapter 2</em>, <em class="italic">The Machine Learning Development Process</em>, <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>, and <em class="chapterRef">Chapter 5</em>, <em class="italic">Deployment Patterns and Tools</em>, to a realistic application scenario. The problem, in this case, concerned clustering taxi rides to find anomalous rides and then performing NLP on some contextual text data to try and help explain those anomalies automatically. This problem was tackled using the ETML pattern, which I offered up as a way to rationalize typical batch ML engineering solutions. This was explained in detail. A design for a potential solution, as well as a discussion of some of the tooling choices any ML engineering team would have to go through, was covered. Finally, a deep dive into some of the key pieces of work that would be required to make this solution production-ready was performed. In particular we showed how you can use good object-orientated programming techniques to wrap ML functionality spanning the Scikit-learn package, the AWS <code class="inlineCode">boto3</code> library, and OpenAI APIs to use LLMs to create some complex functionality. We also explored in detail how to use more complex features of Airflow to orchestrate these pieces of functionality in ways that are resilient.</p>
<p class="normal">With that, you have not only completed this chapter but also the second edition of <em class="italic">Machine Learning Engineering with Python</em>, so congratulations! Throughout this book, we have covered a wide variety of topics related to ML engineering, from how to build your teams and what development processes could look like, all the way through to packaging, scaling, scheduling, deploying, testing, logging, and a whole bunch of stuff in between. We have explored AWS and managed cloud services, we have performed deep dives into open-source technologies that give you the ability to orchestrate, pipeline, and scale, and we have also explored the exciting new world of LLMs, generative AI, and LLMOps.</p>
<p class="normal">There are so many topics to cover in this fast-moving, ever-changing, and exhilarating world of ML engineering and MLOps that one book could never do the whole field justice. This second edition, however, has attempted to improve upon the first by providing some more breadth and depth in areas I think will be important to help develop the next generation of ML engineering talent. It is my hope that you come away from reading this book not only feeling well equipped but also as excited as I am every day to go out and build the future. If you work in this space, or you are moving into it, then you are doing so at what I believe is a truly unique time in history. As ML systems are required to keep becoming more powerful, pervasive, and performant, I believe the demand for ML engineering skills is only going to grow. I hope this book has given you the tools you need to take advantage of that, and that you have enjoyed the ride as much as I have!</p>
</div>
<div id="_idContainer210">
<h1 class="heading-1" id="_idParaDest-197">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussion with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/mle"><span class="url">https://packt.link/mle</span></a></p>
<p class="normal"><img alt="" height="177" role="presentation" src="../Images/QR_Code102810325355484.png" width="177"/></p>
</div>
</div></body></html>