- en: Ensemble Methods for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at a number of interesting machine learning algorithms,
    from classic methods such as linear regression to more advanced techniques such
    as deep neural networks. At various points, we pointed out that every algorithm
    has its own strengths and weaknesses—and we took note of how to spot and overcome
    these weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: However, wouldn't it be great if we could simply stack together a bunch of average
    classifiers to form a much stronger **ensemble** of classifiers?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will do just that. Ensemble methods are techniques that
    bind multiple different models together in order to solve a shared problem. Their
    use has become a common practice in competitive machine learning—making ...
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a short summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any OS—macOS, Windows, or Linux-based—with this book. We recommend
    you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided with the book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of ensemble methods is to combine the predictions of several individual
    estimators built with a given learning algorithm in order to solve a shared problem.
    Typically, an ensemble consists of two major components:'
  prefs: []
  type: TYPE_NORMAL
- en: A set of models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of decision rules that govern how the results of these models are combined
    into a single output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind ensemble methods has much to do with the *wisdom of the crowd*
    concept. Rather than the opinion of a single expert, we consider the collective
    opinion of a group of individuals. In the context of machine learning, these individuals
    would be classifiers or regressors. The idea is that if we just ask a large enough
    number of classifiers, one of them ought to get ...
  prefs: []
  type: TYPE_NORMAL
- en: Understanding averaging ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Averaging methods have a long history in machine learning and are commonly applied
    to fields such as molecular dynamics and audio signal processing. Such ensembles
    are typically seen as exact replicas of a given system.
  prefs: []
  type: TYPE_NORMAL
- en: An averaging ensemble is essentially a collection of models that train on the
    same dataset. Their results are then aggregated in a number of ways.
  prefs: []
  type: TYPE_NORMAL
- en: One common method involves creating multiple model configurations that take
    different parameter subsets as input. Techniques that take this approach are referred
    to collectively as bagging methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging methods come in many different flavors. However, they typically only
    differ in the way they draw random subsets of the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: Pasting methods draw random subsets of the samples without replacement of data
    samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging methods draw random subsets of the samples with replacement of data
    samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random subspace methods draw random subsets of the features but train on all
    data samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random patch methods draw random subsets of both samples and features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging ensembles can be used to reduce the variability of a model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, bagging methods can be realized using the `BaggingClassifier`
    and `BaggingRegressor` meta-estimators. These are meta-estimators because they
    allow us to build an ensemble from any other base estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a bagging classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can, for instance, build an ensemble from a collection of 10 *k*-NN classifiers
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BaggingClassifier` class provides a number of options to customize the
    ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: As shown in the preceding code, this specifies the number of
    base estimators in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_samples`: This denotes the number (or fraction) of samples to draw from
    the dataset to train each base estimator. We can set `bootstrap=True` to sample
    with replacement (effectively implementing bagging), or we can set `bootstrap=False`
    to implement ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a bagging regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly, we can use the `BaggingRegressor` class to form an ensemble of regressors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we could build an ensemble of decision trees to predict housing
    prices from the Boston dataset of [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn how to use a bagging regressor for forming
    an ensemble of regressors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax is almost identical to setting up a bagging classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we need to load and split the dataset as we did for the breast cancer
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can fit the bagging regressor on `X_train` and score it on `X_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we find a performance boost of roughly 5%, from 77.3%
    accuracy for a single decision tree to 82.7% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we wouldn't just stop here. Nobody said the ensemble needs to consist
    of 10 individual estimators, so we are free to explore different-sized ensembles.
    On top of that, the `max_samples` and `max_features` parameters allow for a great
    deal of customization.
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated version of bagged decision trees is called random forests,
    which we will talk about later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding boosting ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to building ensembles is through boosting. Boosting models
    use multiple individual learners in sequence to iteratively boost the performance
    of the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the learners used in boosting are relatively simple. A good example
    is a decision tree with only a single node—a decision stump. Another example could
    be a simple linear regression model. The idea is not to have the strongest individual
    learners, but quite the opposite—we want the individuals to be weak learners so
    that we get a superior performance only when we consider a large number of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration of the procedure, the training set is adjusted so that the
    next classifier is applied to the data points that ...
  prefs: []
  type: TYPE_NORMAL
- en: Weak learners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weak learners are classifiers that are only slightly correlated with the actual
    classification; they can be somewhat better than the random predictions. On the
    contrary, strong learners are arbitrarily well correlated with the correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is that you don't use just one but a broad set of weak learners,
    each one slightly better than random. Many instances of the weak learners can
    be pooled using boosting, bagging, and so on together to create a strong ensemble
    classifier. The benefit is that the final classifier will not lead to *overfitting*
    on your training data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, AdaBoost fits a sequence of weak learners on different weighted
    training data. It starts by predicting the training dataset and gives equal weight
    to each observation/sample. If the first learner prediction is incorrect, then
    it gives higher weight to the observation/sample that has been mispredicted. Since
    it is an iterative process, it continues to add learners until a limit is reached
    in the number of models or accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a boosting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For example, we can build a boosting classifier from a collection of 10 decision
    trees as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These classifiers support both binary and multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the `BaggingClassifier` class, the `GradientBoostingClassifier`
    class provides a number of options to customize the ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This denotes the number of base estimators in the ensemble.
    A large number of estimators typically results in better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`: This denotes the loss function (or cost function) to be optimized.
    Setting `loss=''deviance''` implements logistic regression ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a boosting regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing a boosted regressor follows the same syntax as the boosted classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We have seen earlier that a single decision tree can achieve 79.3% accuracy
    on the Boston dataset. A bagged decision tree classifier made of 10 individual
    regression trees achieved 82.7% accuracy. But how does a boosted regressor compare?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s reload the Boston dataset and split it into training and test sets.
    We want to make sure we use the same value for `random_state` so that we end up
    training and testing on the same subsets of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As it turns out, the boosted decision tree ensemble actually performs worse
    than the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This result might be confusing at first. After all, we used 10 times more classifiers
    than we did for the single decision tree. Why would our numbers get worse?
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this is a good example of an expert classifier being smarter than
    a group of weak learners. One possible solution is to make the ensemble larger.
    In fact, it is customary to use in the order of 100 weak learners in a boosted
    ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, when we retrain the ensemble on the Boston dataset, we get a test score
    of 89.8%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: What happens when you increase the number to `n_estimators=500`? There's a lot
    more we could do by playing with the optional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, boosting is a powerful procedure that allows you to get massive
    performance improvements by combining a large number of relatively simple learners.
  prefs: []
  type: TYPE_NORMAL
- en: A specific implementation of boosted decision trees is the AdaBoost algorithm,
    which we will talk about later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stacking ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the ensemble methods we have seen so far share a common design philosophy:
    to fit multiple individual classifiers to the data and incorporate their predictions
    with the help of some simple decision rules (such as averaging or boosting) into
    a final prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking ensembles, on the other hand, build ensembles with hierarchies. Here,
    individual learners are organized into multiple layers where the output of one
    layer of learners is used as training data for a model at the next layer. This
    way, it is possible to successfully blend hundreds of different models.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, discussing stacking ensembles in detail is beyond the scope of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: However, these models can be very powerful, as seen, ...
  prefs: []
  type: TYPE_NORMAL
- en: Combining decision trees into a random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular variation of bagged decision trees are the so-called random forests.
    These are essentially a collection of decision trees, where each tree is slightly
    different from the others. In contrast to bagged decision trees, each tree in
    a random forest is trained on a slightly different subset of data features.
  prefs: []
  type: TYPE_NORMAL
- en: Although a single tree of unlimited depth might do a relatively good job of
    predicting the data, it is also prone to overfitting. The idea behind random forests
    is to build a large number of trees, each of them trained on a random subset of
    data samples and features. Because of the randomness of the procedure, each tree
    in the forest will overfit the data in a slightly different way. The effect of
    overfitting can then be reduced by averaging the predictions of the individual
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the shortcomings of decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The effect of overfitting the dataset, which a decision tree often falls victim
    to, is best demonstrated through a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will return to the `make_moons` function from scikit-learn''s
    `datasets` module, which we previously used in [Chapter 8](790a10c4-635a-40da-ae5f-13946bc0e9fd.xhtml),
    *Discovering Hidden Structures with Unsupervised Learning*, to organize data into
    two interleaving half circles. Here, we choose to generate 100 data samples belonging
    to two half circles, in combination with some Gaussian noise with a standard deviation
    of `0.25`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can visualize this data using matplotlib and the `scatter`
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In OpenCV, random forests can be built using the `RTrees_create` function from
    the `ml` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree object provides a number of options, the most important of which are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setMaxDepth`: This sets the maximum possible depth of each tree in the ensemble.
    The actual obtained depth may be smaller if other termination criteria are met
    first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMinSampleCount`: This sets the minimum number of samples that a node can
    contain for it to get split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMaxCategories`: This sets the maximum number of categories allowed. Setting
    the number of categories to a smaller value than the actual number of classes
    in the data leads to subset estimation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setTermCriteria`: This sets the termination criteria of the algorithm. This
    is also where you set the number of trees in the forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we might have hoped for a `setNumTrees` method to set the number of
    trees in the forest (kind of the most important parameter of them all, no?), we
    instead need to rely on the `setTermCriteria` method. Confusingly, the number
    of trees is conflated with `cv2.TERM_CRITERA_MAX_ITER`, which is usually reserved
    for the number of iterations that an algorithm is run for, not for the number
    of estimators in an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can specify the number of trees in the forest by passing an integer, `n_trees`,
    to the `setTermCriteria` method. Here, we also want to tell the algorithm to quit
    once the score does not increase by at least `eps` from one iteration to the next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we are ready to train the classifier on the data from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The test labels can be predicted with the `predict` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using scikit-learn''s `accuracy_score`, we can evaluate the model on the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can pass the predicted labels to the `plot_decision_boundary`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/637629d2-7e17-4c9a-9a00-a58b68d8f480.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows the decision landscape of a random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a random forest with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alternatively, we can implement random forests using scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have a number of options to customize the ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This specifies the number of trees in the forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion`: This specifies the node-splitting criterion. Setting `criterion=''gini''`
    implements the Gini impurity, whereas setting `criterion=''entropy''` implements
    information gain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This specifies the number (or fraction) of features to consider
    at each node split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: This specifies the maximum depth of each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples`: This specifies the minimum number ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing extremely randomized trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests are already pretty arbitrary. But what if we wanted to take the
    randomness to its extreme?
  prefs: []
  type: TYPE_NORMAL
- en: In extremely randomized trees (see the `ExtraTreesClassifier` and `ExtraTreesRegressor`
    classes), the randomness is taken even further than in random forests. Remember
    how decision trees usually choose a threshold for every feature so that the purity
    of the node split is maximized? Extremely randomized trees, on the other hand,
    choose these thresholds at random. The best one of these randomly generated thresholds
    is then used as the splitting rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build an extremely randomized tree as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To illustrate the difference between a single decision tree, a random forest,
    and extremely randomized trees, let''s consider a simple dataset, such as the
    Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then fit and score the tree object the same way we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison, using a random forest would have resulted in the same performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, the same is true for a single tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So, what's the difference between them? To answer this question, we have to
    look at the decision boundaries. Fortunately, we have already imported our `plot_decision_boundary`
    helper function in the preceding section, so all we need to do is pass the different
    classifier objects to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a list of classifiers, where each entry in the list is a tuple
    that contains an index, a name for the classifier, and the classifier object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it''s easy to pass the list of classifiers to our helper function such
    that the decision landscape of every classifier is drawn in its own subplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84874c28-7c2e-4c95-ba4a-139c4b2d6901.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the differences between the three classifiers become clearer. We see the
    single tree drawing by far the simplest decision boundaries, splitting the landscape
    using horizontal decision boundaries. The random forest is able to more clearly
    separate the cloud of data points in the lower-left of the decision landscape.
    However, only extremely randomized trees were able to corner the cloud of data
    points toward the center of the landscape from all sides.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about all the different variations of tree ensembles, let's
    move on to a real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forests for face recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular dataset that we haven't talked much about yet is the Olivetti faces
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Olivetti face dataset was collected in 1990 by AT&T Laboratories Cambridge.
    The dataset comprises facial images of 40 distinct subjects, taken at different
    times and under different lighting conditions. In addition, subjects varied their
    facial expressions (open/closed eyes, smiling/not smiling) and their facial details
    (glasses/no glasses).
  prefs: []
  type: TYPE_NORMAL
- en: Images were then quantized into 256 grayscale levels and stored as unsigned
    8-bit integers. Because there are 40 distinct subjects, the dataset comes with
    40 distinct target labels. Recognizing faces thus constitutes an example of a
    multiclass classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like many other classic datasets, the Olivetti face dataset can be loaded using
    scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Although the original images consisted of 92 x 112 pixel images, the version
    available through scikit-learn contains images downscaled to *64 x 64* pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a sense of the dataset, we can plot some example images. Let''s pick
    eight indices from the dataset in random order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot these example images using matplotlib, but we need to make sure
    we reshape the column vectors to 64 x 64-pixel images before plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f68b62ef-a7ec-476b-8467-87c6fc2ba850.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see how all the faces are taken against a dark background and are portrait.
    The facial expressions vary drastically from image to image, making this an interesting
    classification problem. Try not to laugh at some of them!
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can pass the dataset to the classifier, we need to preprocess it following
    the best practices from [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we want to make sure that all example images have the same mean
    grayscale level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We repeat this procedure for every image to make sure the feature values of
    every data point (that is, a row in `X`) are centered around zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessed data can be visualized using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Training and testing the random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We continue to follow our best practice to split the data into training and
    test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we are ready to apply a random forest to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we want to create an ensemble with 50 decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we have a large number of categories (that is, 40), we want to make
    sure the random forest is set up to handle them accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can play with other optional arguments, such as the number of data points
    required in a node before it can be split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we might not want to limit the depth of each tree. This is again a
    parameter we will have to experiment with in the end. But for now, let''s set
    it to a large integer value, making the depth effectively unconstrained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can fit the classifier to the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the resulting depth of the tree using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This means that although we allowed the tree to go up to depth 1,000, in the
    end, only 25 layers were needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation of the classifier is done once again by predicting the labels
    first (`y_hat`) and then passing them to the `accuracy_score` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We find 87% accuracy, which turns out to be much better than with a single
    decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Not bad! We can play with the optional parameters to see whether we get better.
    The most important one seems to be the number of trees in the forest. We can repeat
    the experiment with a forest made from 1,000 trees instead of 50 trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: With this configuration, we get 94% accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we tried to improve the performance of our model through creative trial
    and error: we varied some of the parameters we deemed important and observed the
    resulting change in performance until we found a configuration that satisfied
    our expectations. We will learn more sophisticated techniques for improving a
    model in [Chapter 11](904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml), *Selecting
    the Right Model with Hyperparameter Tuning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting use case of decision tree ensembles is AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AdaBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the trees in the forest are trees of depth 1 (also known as **decision
    stumps**) and we perform boosting instead of bagging, the resulting algorithm
    is called **AdaBoost**.
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaBoost adjusts the dataset at each iteration by performing the following
    actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a decision stump
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the weighting of cases that the decision stump labeled incorrectly
    while reducing the weighting of correctly labeled cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This iterative weight adjustment causes each new classifier in the ensemble
    to prioritize training the incorrectly labeled cases. As a result, the model adjusts
    by targeting highly weighted data points.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, the stumps are combined to form a final classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AdaBoost in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although OpenCV provides a very efficient implementation of AdaBoost, it is
    hidden under the Haar cascade classifier. Haar cascade classifiers are a very
    popular tool for face detection, which we can illustrate through the example of
    the Lena image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the image in both color and grayscale, we load a pretrained Haar
    cascade:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier will then detect faces present in the image using the following
    function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the algorithm operates only on grayscale images. That''s why we saved
    two pictures of Lena, one to which we can apply the classifier (`img_gray`), and
    one on which we can draw the resulting bounding box (`img_bgr`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can plot the image using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output, with the location of the face indicated
    by a blue bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e4d1b54-f99e-423a-a8fd-e09aefbc989c.png)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, this screenshot contains only a single face. However, the preceding
    code will work even on images where multiple faces could be detected. Try it out!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AdaBoost in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In scikit-learn, AdaBoost is just another ensemble estimator. We can create
    an ensemble from 50 decision stumps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can load the breast cancer set once more and split it 75-25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `fit` and `score` AdaBoost using the familiar procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Combining different models into a voting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to combine different instances of the same classifier
    or regressor into an ensemble. In this chapter, we are going to take this idea
    a step further and combine conceptually different classifiers into what is known
    as a **voting classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind voting classifiers is that the individual learners in the ensemble
    don't necessarily need to be of the same type. After all, no matter how the individual
    classifiers arrived at their prediction, in the end, we are going to apply a decision
    rule that integrates all the votes of the individual classifiers. This is also
    known as a **voting scheme**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different voting schemes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two different voting schemes are common among voting classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: In **hard voting** (also known as **majority voting**), every individual classifier
    votes for a class, and the majority wins. In statistical terms, the predicted
    target label of the ensemble is the mode of the distribution of individually predicted
    labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In **soft voting**, every individual classifier provides a probability value
    that a specific data point belongs to a particular target class. The predictions
    are weighted by the classifier's importance and summed up. Then, the target label
    with the greatest sum of weighted probabilities wins the vote.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, let's assume we have three different classifiers in the ensemble
    that perform a ...
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a voting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at a simple example of a voting classifier that combines three
    different algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: A logistic regression classifier from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Gaussian Naive Bayes classifier from [Chapter 7](08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml),
    *Implementing a Spam Filter with Bayesian Learning*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random forest classifier from this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can combine these three algorithms into a voting classifier and apply it
    to the breast cancer dataset with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset, and split it into training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the individual classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Assign the individual classifiers to the voting ensemble. Here, we need to pass
    a list of tuples (`estimators`), where every tuple consists of the name of the
    classifier (a string of letters depicting a short name of each classifier) and
    the model object. The voting scheme can be either `voting='hard'` or `voting='soft'`.
    For now, we will choose** `voting='hard'`:**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the ensemble to the training data and score it on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to convince us that 95.1% is a great accuracy score, we can compare
    the ensemble''s performance to the theoretical performance of each individual
    classifier. We do this by fitting the individual classifiers to the data. Then,
    we will see that the logistic regression model achieves 94.4% accuracy on its
    own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the Naive Bayes classifier achieves 93.0% accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Last but not least, the random forest classifier also achieved 94.4% accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: All in all, we were just able to gain a good percentage in performance by combining
    three unrelated classifiers into an ensemble. Each of these classifiers might
    have made different mistakes on the training set, but that's OK because, on average,
    we need just two out of three classifiers to be correct.
  prefs: []
  type: TYPE_NORMAL
- en: Plurality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed ensemble methods. What we didn't mention
    earlier was how the results are aggregated across the individual models prepared
    by the ensemble techniques. The concept that is used for this is called **plurality**,which
    is nothing but voting. The higher the vote a class gets, the higher the chances
    of it being the final class. Imagine if we had three models prepared during ensemble
    techniques and 10 possible classes (think of them as digits from 0 to 9). Each
    model would choose one class based on the highest probability it obtained. Finally,
    the class with the maximum number of votes would be selected. This is the concept
    of plurality. In practice, plurality tries to bring benefit to both *k-*NN and
    Naive ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about how to improve various classifiers by combining
    them into an ensemble. We discussed how to average the predictions of different
    classifiers using bagging and how to have different classifiers correct each other's
    mistakes using boosting. A lot of time was spent discussing all possible ways
    to combine decision trees, be it decision stumps (AdaBoost), random forests, or
    extremely randomized trees. Finally, we learned how to combine even different
    types of classifiers in an ensemble by building a voting classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk more about how to compare the results of different
    classifiers by diving into the world of model selection and hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
