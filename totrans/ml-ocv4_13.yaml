- en: Ensemble Methods for Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分类的集成方法
- en: So far, we have looked at a number of interesting machine learning algorithms,
    from classic methods such as linear regression to more advanced techniques such
    as deep neural networks. At various points, we pointed out that every algorithm
    has its own strengths and weaknesses—and we took note of how to spot and overcome
    these weaknesses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了多种有趣的机器学习算法，从经典的线性回归到更高级的技术如深度神经网络。在各个点上，我们指出了每种算法都有其自身的优点和缺点——并且我们注意到了如何发现和克服这些缺点。
- en: However, wouldn't it be great if we could simply stack together a bunch of average
    classifiers to form a much stronger **ensemble** of classifiers?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们能够简单地堆叠一组平均分类器来形成一个更强大的**集成**分类器，那岂不是很好？
- en: In this chapter, we will do just that. Ensemble methods are techniques that
    bind multiple different models together in order to solve a shared problem. Their
    use has become a common practice in competitive machine learning—making ...
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将做这件事。集成方法是结合多个不同模型来解决共同问题的技术。它们在竞争性机器学习中已成为一种常见的做法——使...
- en: Technical requirements
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接获取本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10)。
- en: 'Here is a short summary of the software and hardware requirements:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是软件和硬件要求的一个简要总结：
- en: OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV版本4.1.x（4.1.0或4.1.1都完全可以）。
- en: Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python版本3.6（任何Python 3.x版本都行）。
- en: Anaconda Python 3 for installing Python and the required modules.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda Python 3用于安装Python和所需的模块。
- en: You can use any OS—macOS, Windows, or Linux-based—with this book. We recommend
    you have at least 4 GB RAM in your system.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows或基于Linux的操作系统——使用这本书。我们建议您的系统至少有4 GB RAM。
- en: You don't need to have a GPU to run the code provided with the book.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要GPU来运行书中提供的代码。
- en: Understanding ensemble methods
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成方法
- en: 'The goal of ensemble methods is to combine the predictions of several individual
    estimators built with a given learning algorithm in order to solve a shared problem.
    Typically, an ensemble consists of two major components:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法的目的是结合使用给定学习算法构建的几个单个估计器的预测，以解决一个共同的问题。通常，一个集成由两个主要组件组成：
- en: A set of models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组模型
- en: A set of decision rules that govern how the results of these models are combined
    into a single output
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组决策规则，用于控制这些模型的结果如何组合成一个单一输出。
- en: The idea behind ensemble methods has much to do with the *wisdom of the crowd*
    concept. Rather than the opinion of a single expert, we consider the collective
    opinion of a group of individuals. In the context of machine learning, these individuals
    would be classifiers or regressors. The idea is that if we just ask a large enough
    number of classifiers, one of them ought to get ...
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法背后的想法与“群体智慧”概念有很大关系。而不是单个专家的意见，我们考虑一群个人的集体意见。在机器学习的背景下，这些个人将是分类器或回归器。想法是，如果我们询问足够多的分类器，其中之一应该能够得到...
- en: Understanding averaging ensembles
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解平均集成
- en: Averaging methods have a long history in machine learning and are commonly applied
    to fields such as molecular dynamics and audio signal processing. Such ensembles
    are typically seen as exact replicas of a given system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 平均方法在机器学习中有悠久的历史，通常应用于分子动力学和音频信号处理等领域。这样的集成通常被视为给定系统的精确复制品。
- en: An averaging ensemble is essentially a collection of models that train on the
    same dataset. Their results are then aggregated in a number of ways.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 平均集成本质上是一组在相同数据集上训练的模型。然后，他们的结果以多种方式汇总。
- en: One common method involves creating multiple model configurations that take
    different parameter subsets as input. Techniques that take this approach are referred
    to collectively as bagging methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是创建多个模型配置，这些配置以不同的参数子集作为输入。采用这种方法的技巧被称为集合方法。
- en: 'Bagging methods come in many different flavors. However, they typically only
    differ in the way they draw random subsets of the training set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法有很多不同的风味。然而，它们通常只在抽取训练集随机子集的方式上有所不同：
- en: Pasting methods draw random subsets of the samples without replacement of data
    samples.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粘贴方法在不替换数据样本的情况下抽取样本的随机子集。
- en: Bagging methods draw random subsets of the samples with replacement of data
    samples.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋装方法在抽取样本时进行数据样本的替换。
- en: Random subspace methods draw random subsets of the features but train on all
    data samples.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机子空间方法从特征中抽取随机子集，但训练所有数据样本。
- en: Random patch methods draw random subsets of both samples and features.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机补丁方法从样本和特征中抽取随机子集。
- en: Averaging ensembles can be used to reduce the variability of a model's performance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用集成平均来减少模型性能的变异性。
- en: In scikit-learn, bagging methods can be realized using the `BaggingClassifier`
    and `BaggingRegressor` meta-estimators. These are meta-estimators because they
    allow us to build an ensemble from any other base estimator.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，可以使用`BaggingClassifier`和`BaggingRegressor`元估计器来实现袋装方法。这些是元估计器，因为它们允许我们从任何其他基础估计器构建集成。
- en: Implementing a bagging classifier
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个袋装分类器
- en: 'We can, for instance, build an ensemble from a collection of 10 *k*-NN classifiers
    as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以从一组10个*k*-NN分类器中构建一个集成，如下所示：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `BaggingClassifier` class provides a number of options to customize the
    ensemble:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingClassifier`类提供了一些选项来自定义集成：'
- en: '`n_estimators`: As shown in the preceding code, this specifies the number of
    base estimators in the ensemble.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：如前述代码所示，这指定了集成中基础估计器的数量。'
- en: '`max_samples`: This denotes the number (or fraction) of samples to draw from
    the dataset to train each base estimator. We can set `bootstrap=True` to sample
    with replacement (effectively implementing bagging), or we can set `bootstrap=False`
    to implement ...'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_samples`：这表示从数据集中抽取用于训练每个基础估计器的样本数量（或分数）。我们可以设置`bootstrap=True`以进行有替换的抽样（有效地实现袋装），或者我们可以设置`bootstrap=False`以实现...'
- en: Implementing a bagging regressor
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个袋装回归器
- en: Similarly, we can use the `BaggingRegressor` class to form an ensemble of regressors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用`BaggingRegressor`类来形成回归器的集成。
- en: For example, we could build an ensemble of decision trees to predict housing
    prices from the Boston dataset of [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以从第3章的波士顿数据集（[Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)，*监督学习的第一步*）中构建一个决策树集成来预测房价。
- en: 'In the following steps, you will learn how to use a bagging regressor for forming
    an ensemble of regressors:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习如何使用袋装回归器来形成回归器的集成：
- en: 'The syntax is almost identical to setting up a bagging classifier:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语法几乎与设置袋装分类器相同：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Of course, we need to load and split the dataset as we did for the breast cancer
    dataset:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，我们需要像处理乳腺癌数据集那样加载数据集并进行分割：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we can fit the bagging regressor on `X_train` and score it on `X_test`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以在`X_train`上拟合袋装回归器，并在`X_test`上进行评分：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding example, we find a performance boost of roughly 5%, from 77.3%
    accuracy for a single decision tree to 82.7% accuracy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个例子中，我们发现性能提升了大约5%，从单个决策树的77.3%准确率提升到82.7%准确率。
- en: Of course, we wouldn't just stop here. Nobody said the ensemble needs to consist
    of 10 individual estimators, so we are free to explore different-sized ensembles.
    On top of that, the `max_samples` and `max_features` parameters allow for a great
    deal of customization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不会仅仅止步于此。没有人说集成必须由10个单独的估计器组成，因此我们可以自由地探索不同大小的集成。除此之外，`max_samples`和`max_features`参数允许进行大量的定制。
- en: A more sophisticated version of bagged decision trees is called random forests,
    which we will talk about later in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装决策树的更复杂版本称为随机森林，我们将在本章后面讨论。
- en: Understanding boosting ensembles
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解提升集成
- en: Another approach to building ensembles is through boosting. Boosting models
    use multiple individual learners in sequence to iteratively boost the performance
    of the ensemble.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 构建集成的一种另一种方法是提升。提升模型使用多个单独的学习者在序列中迭代地提升集成的性能。
- en: Typically, the learners used in boosting are relatively simple. A good example
    is a decision tree with only a single node—a decision stump. Another example could
    be a simple linear regression model. The idea is not to have the strongest individual
    learners, but quite the opposite—we want the individuals to be weak learners so
    that we get a superior performance only when we consider a large number of individuals.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在提升中使用的学习器相对简单。一个好的例子是只有一个节点的决策树——决策树桩。另一个例子可以是简单的线性回归模型。想法不是拥有最强的单个学习器，而是相反——我们希望个体是弱学习器，这样当我们考虑大量个体时，才能获得更好的性能。
- en: At each iteration of the procedure, the training set is adjusted so that the
    next classifier is applied to the data points that ...
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在该过程的每次迭代中，训练集都会进行调整，以便下一个分类器应用于数据点，...
- en: Weak learners
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱学习器
- en: Weak learners are classifiers that are only slightly correlated with the actual
    classification; they can be somewhat better than the random predictions. On the
    contrary, strong learners are arbitrarily well correlated with the correct classification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 弱学习器是与实际分类仅略有相关性的分类器；它们可以比随机预测略好。相反，强学习器与正确分类有任意好的相关性。
- en: The idea here is that you don't use just one but a broad set of weak learners,
    each one slightly better than random. Many instances of the weak learners can
    be pooled using boosting, bagging, and so on together to create a strong ensemble
    classifier. The benefit is that the final classifier will not lead to *overfitting*
    on your training data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，你不仅仅使用一个，而是一系列广泛的弱学习器，每个都比随机略好。许多弱学习器的实例可以通过提升、袋装等方法一起汇总，以创建一个强大的集成分类器。好处是，最终的分类器不会导致训练数据上的*过拟合*。
- en: For example, AdaBoost fits a sequence of weak learners on different weighted
    training data. It starts by predicting the training dataset and gives equal weight
    to each observation/sample. If the first learner prediction is incorrect, then
    it gives higher weight to the observation/sample that has been mispredicted. Since
    it is an iterative process, it continues to add learners until a limit is reached
    in the number of models or accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，AdaBoost在具有不同加权训练数据的弱学习器上拟合一系列。它首先预测训练数据集，并给每个观察/样本相同的权重。如果第一个学习器预测错误，那么它会给预测错误的观察/样本更高的权重。由于这是一个迭代过程，它会继续添加学习器，直到模型数量或准确率达到限制。
- en: Implementing a boosting classifier
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现提升分类器
- en: 'For example, we can build a boosting classifier from a collection of 10 decision
    trees as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以从一组10个决策树中构建一个提升分类器，如下所示：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These classifiers support both binary and multiclass classification.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类器支持二进制和多类分类。
- en: 'Similar to the `BaggingClassifier` class, the `GradientBoostingClassifier`
    class provides a number of options to customize the ensemble:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与`BaggingClassifier`类类似，`GradientBoostingClassifier`类提供了一些选项来自定义集成：
- en: '`n_estimators`: This denotes the number of base estimators in the ensemble.
    A large number of estimators typically results in better performance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这表示集成中基估计器的数量。通常，估计器的数量越多，性能越好。'
- en: '`loss`: This denotes the loss function (or cost function) to be optimized.
    Setting `loss=''deviance''` implements logistic regression ...'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：这表示要优化的损失函数（或成本函数）。设置`loss=''deviance''`实现逻辑回归...'
- en: Implementing a boosting regressor
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现提升回归器
- en: 'Implementing a boosted regressor follows the same syntax as the boosted classifier:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实现提升回归器的语法与提升分类器相同：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have seen earlier that a single decision tree can achieve 79.3% accuracy
    on the Boston dataset. A bagged decision tree classifier made of 10 individual
    regression trees achieved 82.7% accuracy. But how does a boosted regressor compare?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，单个决策树在波士顿数据集上可以达到79.3%的准确率。由10个单独回归树组成的袋装决策树分类器达到了82.7%的准确率。但是提升回归器是如何比较的呢？
- en: 'Let''s reload the Boston dataset and split it into training and test sets.
    We want to make sure we use the same value for `random_state` so that we end up
    training and testing on the same subsets of the data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新加载波士顿数据集，并将其分为训练集和测试集。我们想确保使用相同的`random_state`值，以便最终在相同的数据子集上进行训练和测试：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As it turns out, the boosted decision tree ensemble actually performs worse
    than the previous code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，提升决策树集成实际上比之前的代码表现更差：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This result might be confusing at first. After all, we used 10 times more classifiers
    than we did for the single decision tree. Why would our numbers get worse?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果一开始可能会让人困惑。毕竟，我们使用的分类器比单棵决策树多10倍。为什么我们的数字会变差？
- en: 'You can see this is a good example of an expert classifier being smarter than
    a group of weak learners. One possible solution is to make the ensemble larger.
    In fact, it is customary to use in the order of 100 weak learners in a boosted
    ensemble:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这是一个专家分类器比一群弱学习器更聪明的良好例子。一个可能的解决方案是使集成更大。实际上，在提升集成中通常使用大约100个弱学习器：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, when we retrain the ensemble on the Boston dataset, we get a test score
    of 89.8%:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们用波士顿数据集重新训练集成时，我们得到了89.8%的测试分数：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What happens when you increase the number to `n_estimators=500`? There's a lot
    more we could do by playing with the optional parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你增加到`n_estimators=500`时会发生什么？我们可以通过调整可选参数做更多的事情。
- en: As you can see, boosting is a powerful procedure that allows you to get massive
    performance improvements by combining a large number of relatively simple learners.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，提升是一种强大的过程，它允许你通过结合大量相对简单的学习器来实现巨大的性能提升。
- en: A specific implementation of boosted decision trees is the AdaBoost algorithm,
    which we will talk about later in this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提升决策树的一个特定实现是AdaBoost算法，我们将在本章后面讨论。
- en: Understanding stacking ensembles
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解堆叠集成
- en: 'All the ensemble methods we have seen so far share a common design philosophy:
    to fit multiple individual classifiers to the data and incorporate their predictions
    with the help of some simple decision rules (such as averaging or boosting) into
    a final prediction.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所看到的所有集成方法都共享一个共同的设计理念：将多个个体分类器拟合到数据中，并借助一些简单的决策规则（如平均或提升）将它们的预测合并到一个最终预测中。
- en: Stacking ensembles, on the other hand, build ensembles with hierarchies. Here,
    individual learners are organized into multiple layers where the output of one
    layer of learners is used as training data for a model at the next layer. This
    way, it is possible to successfully blend hundreds of different models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，堆叠集成通过层次结构构建集成。在这里，个体学习器被组织成多个层次，其中一层学习器的输出被用作下一层模型训练的数据。这样，就有可能成功融合数百种不同的模型。
- en: Unfortunately, discussing stacking ensembles in detail is beyond the scope of
    this book.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，详细讨论堆叠集成超出了本书的范围。
- en: However, these models can be very powerful, as seen, ...
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型可以非常强大，正如我们所看到的，...
- en: Combining decision trees into a random forest
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将决策树组合成随机森林
- en: A popular variation of bagged decision trees are the so-called random forests.
    These are essentially a collection of decision trees, where each tree is slightly
    different from the others. In contrast to bagged decision trees, each tree in
    a random forest is trained on a slightly different subset of data features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 带包裹的决策树的一个流行变体是所谓的随机森林。这些本质上是一系列决策树，其中每棵树都与其他树略有不同。与带包裹的决策树不同，随机森林中的每棵树都在略微不同的数据特征子集上训练。
- en: Although a single tree of unlimited depth might do a relatively good job of
    predicting the data, it is also prone to overfitting. The idea behind random forests
    is to build a large number of trees, each of them trained on a random subset of
    data samples and features. Because of the randomness of the procedure, each tree
    in the forest will overfit the data in a slightly different way. The effect of
    overfitting can then be reduced by averaging the predictions of the individual
    trees.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一棵无限深度的单树可能对预测数据做相对较好的工作，但它也容易过拟合。随机森林背后的想法是构建大量树，每棵树都在随机子集的数据样本和特征上训练。由于过程的随机性，森林中的每棵树将以略微不同的方式过拟合数据。然后可以通过平均单个树的预测来减少过拟合的影响。
- en: Understanding the shortcomings of decision trees
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树的不足
- en: The effect of overfitting the dataset, which a decision tree often falls victim
    to, is best demonstrated through a simple example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树经常遇到的过拟合数据集的影响，最好通过一个简单的例子来展示。
- en: 'For this, we will return to the `make_moons` function from scikit-learn''s
    `datasets` module, which we previously used in [Chapter 8](790a10c4-635a-40da-ae5f-13946bc0e9fd.xhtml),
    *Discovering Hidden Structures with Unsupervised Learning*, to organize data into
    two interleaving half circles. Here, we choose to generate 100 data samples belonging
    to two half circles, in combination with some Gaussian noise with a standard deviation
    of `0.25`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个，我们将回到 scikit-learn 的 `datasets` 模块中的 `make_moons` 函数，我们在[第 8 章](790a10c4-635a-40da-ae5f-13946bc0e9fd.xhtml)，“使用无监督学习发现隐藏结构”中之前使用过，将数据组织成两个交替的半圆。在这里，我们选择生成
    100 个属于两个半圆的数据样本，并结合一些标准差为 `0.25` 的高斯噪声：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can visualize this data using matplotlib and the `scatter`
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 matplotlib 和 `scatter` 可视化这些数据
- en: Implementing our first random forest
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的第一个随机森林
- en: 'In OpenCV, random forests can be built using the `RTrees_create` function from
    the `ml` module:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenCV 中，可以使用 `ml` 模块中的 `RTrees_create` 函数构建随机森林：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The tree object provides a number of options, the most important of which are
    the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 树对象提供了一些选项，其中最重要的是以下这些：
- en: '`setMaxDepth`: This sets the maximum possible depth of each tree in the ensemble.
    The actual obtained depth may be smaller if other termination criteria are met
    first.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMaxDepth`: 这设置集成中每个树的最大可能深度。如果首先满足其他终止标准，实际获得的深度可能更小。'
- en: '`setMinSampleCount`: This sets the minimum number of samples that a node can
    contain for it to get split.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMinSampleCount`: 这设置一个节点可以包含的最小样本数，以便它可以被分割。'
- en: '`setMaxCategories`: This sets the maximum number of categories allowed. Setting
    the number of categories to a smaller value than the actual number of classes
    in the data leads to subset estimation.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMaxCategories`: 这设置允许的最大类别数。将类别数设置为小于数据中实际类别数的小值会导致子集估计。'
- en: '`setTermCriteria`: This sets the termination criteria of the algorithm. This
    is also where you set the number of trees in the forest.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setTermCriteria`: 这设置算法的终止标准。这也是你设置森林中树的数量的地方。'
- en: Although we might have hoped for a `setNumTrees` method to set the number of
    trees in the forest (kind of the most important parameter of them all, no?), we
    instead need to rely on the `setTermCriteria` method. Confusingly, the number
    of trees is conflated with `cv2.TERM_CRITERA_MAX_ITER`, which is usually reserved
    for the number of iterations that an algorithm is run for, not for the number
    of estimators in an ensemble.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可能希望有一个 `setNumTrees` 方法来设置森林中的树的数量（这可能是所有参数中最重要的一个，不是吗？），但我们实际上需要依赖于 `setTermCriteria`
    方法。令人困惑的是，树的数量与 `cv2.TERM_CRITERA_MAX_ITER` 相混淆，这通常是为算法运行的迭代次数保留的，而不是用于集成中的估计器数量。
- en: 'We can specify the number of trees in the forest by passing an integer, `n_trees`,
    to the `setTermCriteria` method. Here, we also want to tell the algorithm to quit
    once the score does not increase by at least `eps` from one iteration to the next:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将整数 `n_trees` 传递给 `setTermCriteria` 方法来指定森林中的树的数量。这里，我们还想告诉算法，如果分数在连续迭代中没有至少增加
    `eps`，则退出：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we are ready to train the classifier on the data from the preceding code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们就可以使用前面代码中的数据来训练分类器了：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The test labels can be predicted with the `predict` method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `predict` 方法预测测试标签：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using scikit-learn''s `accuracy_score`, we can evaluate the model on the test
    set:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `accuracy_score`，我们可以在测试集上评估模型：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After training, we can pass the predicted labels to the `plot_decision_boundary`
    function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以将预测标签传递给 `plot_decision_boundary` 函数：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will produce the following plot:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/637629d2-7e17-4c9a-9a00-a58b68d8f480.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/637629d2-7e17-4c9a-9a00-a58b68d8f480.png)'
- en: The preceding image shows the decision landscape of a random forest classifier.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了随机森林分类器的决策景观。
- en: Implementing a random forest with scikit-learn
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现随机森林
- en: 'Alternatively, we can implement random forests using scikit-learn:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 scikit-learn 实现随机森林：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we have a number of options to customize the ensemble:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有多个选项来自定义集成：
- en: '`n_estimators`: This specifies the number of trees in the forest.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`: 这指定森林中的树的数量。'
- en: '`criterion`: This specifies the node-splitting criterion. Setting `criterion=''gini''`
    implements the Gini impurity, whereas setting `criterion=''entropy''` implements
    information gain.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`: 这指定了节点分割的标准。将 `criterion=''gini''` 设置为 Gini 不纯度，而将 `criterion=''entropy''`
    设置为信息增益。'
- en: '`max_features`: This specifies the number (or fraction) of features to consider
    at each node split.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`: 这指定了在每个节点分裂时考虑的特征数量（或分数）。'
- en: '`max_depth`: This specifies the maximum depth of each tree.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 这指定了每个树的最大深度。'
- en: '`min_samples`: This specifies the minimum number ...'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples`: 这指定了最小数量 ...'
- en: Implementing extremely randomized trees
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现极端随机树
- en: Random forests are already pretty arbitrary. But what if we wanted to take the
    randomness to its extreme?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林已经很随意了。但如果我们想将随机性推向极致呢？
- en: In extremely randomized trees (see the `ExtraTreesClassifier` and `ExtraTreesRegressor`
    classes), the randomness is taken even further than in random forests. Remember
    how decision trees usually choose a threshold for every feature so that the purity
    of the node split is maximized? Extremely randomized trees, on the other hand,
    choose these thresholds at random. The best one of these randomly generated thresholds
    is then used as the splitting rule.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端随机树（请参阅`ExtraTreesClassifier`和`ExtraTreesRegressor`类）中，随机性比随机森林还要强。记得决策树通常为每个特征选择一个阈值，以使节点分裂的纯度最大化吗？另一方面，极端随机树则随机选择这些阈值。然后，使用这些随机生成的阈值中的最佳值作为分裂规则。
- en: 'We can build an extremely randomized tree as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下方式构建一个极端随机树：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To illustrate the difference between a single decision tree, a random forest,
    and extremely randomized trees, let''s consider a simple dataset, such as the
    Iris dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明单个决策树、随机森林和极端随机树之间的差异，让我们考虑一个简单的数据集，例如鸢尾花数据集：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can then fit and score the tree object the same way we did before:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像之前一样拟合和评分树对象：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For comparison, using a random forest would have resulted in the same performance:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，使用随机森林将产生相同的表现：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In fact, the same is true for a single tree:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于单个树也是如此：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, what's the difference between them? To answer this question, we have to
    look at the decision boundaries. Fortunately, we have already imported our `plot_decision_boundary`
    helper function in the preceding section, so all we need to do is pass the different
    classifier objects to it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它们之间有什么区别呢？为了回答这个问题，我们必须看看决策边界。幸运的是，我们已经在前面章节中导入了我们的`plot_decision_boundary`辅助函数，所以我们只需要将不同的分类器对象传递给它。
- en: 'We will build a list of classifiers, where each entry in the list is a tuple
    that contains an index, a name for the classifier, and the classifier object:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个分类器列表，其中列表中的每个条目都是一个包含索引、分类器名称和分类器对象的元组：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, it''s easy to pass the list of classifiers to our helper function such
    that the decision landscape of every classifier is drawn in its own subplot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，很容易将分类器列表传递给我们的辅助函数，以便每个分类器的决策景观都在其自己的子图中绘制：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result looks like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来像这样：
- en: '![](img/84874c28-7c2e-4c95-ba4a-139c4b2d6901.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84874c28-7c2e-4c95-ba4a-139c4b2d6901.png)'
- en: Now the differences between the three classifiers become clearer. We see the
    single tree drawing by far the simplest decision boundaries, splitting the landscape
    using horizontal decision boundaries. The random forest is able to more clearly
    separate the cloud of data points in the lower-left of the decision landscape.
    However, only extremely randomized trees were able to corner the cloud of data
    points toward the center of the landscape from all sides.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在三个分类器之间的差异变得更加清晰。我们看到单个树绘制了迄今为止最简单的决策边界，使用水平决策边界分割景观。随机森林能够更清楚地分离决策景观左下角的数据点云。然而，只有极端随机树能够从各个方向将数据点云推向景观的中心。
- en: Now that we know about all the different variations of tree ensembles, let's
    move on to a real-world dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了树集成所有不同的变体，让我们转向一个真实世界的数据集。
- en: Using random forests for face recognition
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行人脸识别
- en: A popular dataset that we haven't talked much about yet is the Olivetti faces
    dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有过多讨论的一个流行数据集是Olivetti人脸数据集。
- en: The Olivetti face dataset was collected in 1990 by AT&T Laboratories Cambridge.
    The dataset comprises facial images of 40 distinct subjects, taken at different
    times and under different lighting conditions. In addition, subjects varied their
    facial expressions (open/closed eyes, smiling/not smiling) and their facial details
    (glasses/no glasses).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Olivetti人脸数据集是在1990年由AT&T实验室剑桥收集的。该数据集包含40个不同主题的人脸图像，这些图像在不同的时间和不同的光照条件下拍摄。此外，主题人物还改变了他们的面部表情（睁眼/闭眼，微笑/不微笑）和面部细节（戴眼镜/不戴眼镜）。
- en: Images were then quantized into 256 grayscale levels and stored as unsigned
    8-bit integers. Because there are 40 distinct subjects, the dataset comes with
    40 distinct target labels. Recognizing faces thus constitutes an example of a
    multiclass classification task.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将图像量化为256个灰度级别，并存储为无符号8位整数。由于有40个不同的主题，数据集包含40个不同的目标标签。因此，识别面部构成了多类分类任务的例子。
- en: Loading the dataset
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'Like many other classic datasets, the Olivetti face dataset can be loaded using
    scikit-learn:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多其他经典数据集一样，Olivetti人脸数据集可以使用scikit-learn加载：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Although the original images consisted of 92 x 112 pixel images, the version
    available through scikit-learn contains images downscaled to *64 x 64* pixels.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始图像由92 x 112像素的图像组成，但通过scikit-learn提供的版本包含下采样到*64 x 64*像素的图像。
- en: 'To get a sense of the dataset, we can plot some example images. Let''s pick
    eight indices from the dataset in random order:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据集有一个大致的了解，我们可以绘制一些示例图像。让我们从数据集中随机选择八个索引：
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can plot these example images using matplotlib, but we need to make sure
    we reshape the column vectors to 64 x 64-pixel images before plotting:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用matplotlib绘制这些示例图像，但我们需要确保在绘图之前将列向量重塑为64 x 64像素的图像：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code produces the following output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![](img/f68b62ef-a7ec-476b-8467-87c6fc2ba850.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f68b62ef-a7ec-476b-8467-87c6fc2ba850.png)'
- en: You can see how all the faces are taken against a dark background and are portrait.
    The facial expressions vary drastically from image to image, making this an interesting
    classification problem. Try not to laugh at some of them!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到所有面孔都是对着深色背景拍摄的，而且是肖像。从一张图像到另一张图像，面部表情变化很大，这使得这是一个有趣的分类问题。尝试不要对其中的一些发笑！
- en: Preprocessing the dataset
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: Before we can pass the dataset to the classifier, we need to preprocess it following
    the best practices from [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将数据集传递给分类器之前，我们需要按照[第4章](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml)，*表示数据和工程特征*的最佳实践对其进行预处理。
- en: 'Specifically, we want to make sure that all example images have the same mean
    grayscale level:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们想要确保所有示例图像具有相同的平均灰度级别：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We repeat this procedure for every image to make sure the feature values of
    every data point (that is, a row in `X`) are centered around zero:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个图像重复此过程，以确保每个数据点的特征值（即`X`中的一行）都围绕零中心：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preprocessed data can be visualized using the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码可视化预处理后的数据：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Training and testing the random forest
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试随机森林
- en: 'We continue to follow our best practice to split the data into training and
    test sets:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续遵循我们的最佳实践，将数据分为训练集和测试集：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we are ready to apply a random forest to the data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们就准备好将随机森林应用于数据：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we want to create an ensemble with 50 decision trees:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们想要创建一个包含50个决策树的集成：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Because we have a large number of categories (that is, 40), we want to make
    sure the random forest is set up to handle them accordingly:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有大量的类别（即40个），我们想要确保随机森林能够相应地处理它们：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can play with other optional arguments, such as the number of data points
    required in a node before it can be split:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试其他可选参数，例如在节点可以分裂之前所需的数据点数量：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'However, we might not want to limit the depth of each tree. This is again a
    parameter we will have to experiment with in the end. But for now, let''s set
    it to a large integer value, making the depth effectively unconstrained:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可能不想限制每棵树的深度。这又是一个我们最终必须实验的参数。但就目前而言，让我们将其设置为一个大的整数值，使深度实际上不受约束：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can fit the classifier to the training data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将分类器拟合到训练数据：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can check the resulting depth of the tree using the following function:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下函数来检查树的最终深度：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This means that although we allowed the tree to go up to depth 1,000, in the
    end, only 25 layers were needed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着尽管我们允许树达到深度1,000，但最终只需要25层。
- en: 'The evaluation of the classifier is done once again by predicting the labels
    first (`y_hat`) and then passing them to the `accuracy_score` function:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的评估是通过首先预测标签（`y_hat`），然后将它们传递到`accuracy_score`函数来完成的：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We find 87% accuracy, which turns out to be much better than with a single
    decision tree:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现准确率为87%，这比使用单个决策树要好得多：
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Not bad! We can play with the optional parameters to see whether we get better.
    The most important one seems to be the number of trees in the forest. We can repeat
    the experiment with a forest made from 1,000 trees instead of 50 trees:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错！我们可以玩玩可选参数，看看我们是否能得到更好的结果。其中最重要的一个似乎是森林中的树的数量。我们可以用由1,000棵树组成的森林而不是50棵树的森林重复实验：
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: With this configuration, we get 94% accuracy!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个配置，我们得到了94%的准确率！
- en: 'Here, we tried to improve the performance of our model through creative trial
    and error: we varied some of the parameters we deemed important and observed the
    resulting change in performance until we found a configuration that satisfied
    our expectations. We will learn more sophisticated techniques for improving a
    model in [Chapter 11](904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml), *Selecting
    the Right Model with Hyperparameter Tuning*.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们试图通过创造性的试错来提高我们模型的性能：我们改变了一些我们认为重要的参数，并观察性能的变化，直到我们找到一个满足我们期望的配置。我们将在第11章[选择合适的模型与超参数调整](904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml)中学习更多用于提高模型性能的复杂技术。
- en: Another interesting use case of decision tree ensembles is AdaBoost.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树集成的一个有趣的应用案例是AdaBoost。
- en: Implementing AdaBoost
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现AdaBoost
- en: When the trees in the forest are trees of depth 1 (also known as **decision
    stumps**) and we perform boosting instead of bagging, the resulting algorithm
    is called **AdaBoost**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当森林中的树是深度为1的树（也称为**决策树**）并且我们执行提升而不是袋装时，得到的算法称为**AdaBoost**。
- en: 'AdaBoost adjusts the dataset at each iteration by performing the following
    actions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost在每次迭代中通过执行以下操作调整数据集：
- en: Selecting a decision stump
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择决策树
- en: Increasing the weighting of cases that the decision stump labeled incorrectly
    while reducing the weighting of correctly labeled cases
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加决策树错误标记的案例的权重，同时减少正确标记的案例的权重
- en: This iterative weight adjustment causes each new classifier in the ensemble
    to prioritize training the incorrectly labeled cases. As a result, the model adjusts
    by targeting highly weighted data points.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种迭代权重调整导致集成中的每个新分类器都会优先训练被错误标记的案例。因此，模型通过针对高权重的数据点进行调整。
- en: Eventually, the stumps are combined to form a final classifier.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这些树桩被组合成一个最终的分类器。
- en: Implementing AdaBoost in OpenCV
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中实现AdaBoost
- en: 'Although OpenCV provides a very efficient implementation of AdaBoost, it is
    hidden under the Haar cascade classifier. Haar cascade classifiers are a very
    popular tool for face detection, which we can illustrate through the example of
    the Lena image:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然OpenCV提供了AdaBoost的非常高效的实现，但它隐藏在Haar级联分类器之下。Haar级联分类器是面部检测中一个非常流行的工具，我们可以通过Lena图像的例子来说明：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After loading the image in both color and grayscale, we load a pretrained Haar
    cascade:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载彩色和灰度图像后，我们加载了一个预训练的Haar级联：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The classifier will then detect faces present in the image using the following
    function call:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器将使用以下函数调用检测图像中存在的面部：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Note that the algorithm operates only on grayscale images. That''s why we saved
    two pictures of Lena, one to which we can apply the classifier (`img_gray`), and
    one on which we can draw the resulting bounding box (`img_bgr`):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该算法仅在灰度图像上操作。这就是为什么我们保存了两张Lena的图片，一张可以应用分类器（`img_gray`），另一张可以绘制结果边界框（`img_bgr`）：
- en: '[PRE45]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we can plot the image using the following code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码绘制图像：
- en: '[PRE46]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This results in the following output, with the location of the face indicated
    by a blue bounding box:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下输出，其中面部的位置由一个蓝色边界框指示：
- en: '![](img/1e4d1b54-f99e-423a-a8fd-e09aefbc989c.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e4d1b54-f99e-423a-a8fd-e09aefbc989c.png)'
- en: Obviously, this screenshot contains only a single face. However, the preceding
    code will work even on images where multiple faces could be detected. Try it out!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这个截图只包含一个面部。然而，前面的代码即使在可以检测到多个面部的图像上也能工作。试试看！
- en: Implementing AdaBoost in scikit-learn
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现AdaBoost
- en: 'In scikit-learn, AdaBoost is just another ensemble estimator. We can create
    an ensemble from 50 decision stumps, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，AdaBoost只是另一个集成估计器。我们可以创建一个由50个决策树组成的集成，如下所示：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can load the breast cancer set once more and split it 75-25:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次加载乳腺癌数据集，并将其分成75-25：
- en: '[PRE48]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then, `fit` and `score` AdaBoost using the familiar procedure:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用熟悉的程序`fit`和`score` AdaBoost：
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Combining different models into a voting classifier
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将不同的模型组合成一个投票分类器
- en: So far, we have seen how to combine different instances of the same classifier
    or regressor into an ensemble. In this chapter, we are going to take this idea
    a step further and combine conceptually different classifiers into what is known
    as a **voting classifier**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何将同一分类器或回归器的不同实例组合成一个集成。在本章中，我们将把这个想法更进一步，将概念上不同的分类器组合成所谓的**投票分类器**。
- en: The idea behind voting classifiers is that the individual learners in the ensemble
    don't necessarily need to be of the same type. After all, no matter how the individual
    classifiers arrived at their prediction, in the end, we are going to apply a decision
    rule that integrates all the votes of the individual classifiers. This is also
    known as a **voting scheme**.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 投票分类器背后的想法是，集成中的单个学习器不一定需要是同一类型的。毕竟，无论单个分类器如何得出预测，最终我们都会应用一个决策规则，该规则整合了所有单个分类器的投票。这也被称为**投票方案**。
- en: Understanding different voting schemes
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不同的投票方案
- en: 'Two different voting schemes are common among voting classifiers:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在投票分类器中，有两种不同的投票方案是常见的：
- en: In **hard voting** (also known as **majority voting**), every individual classifier
    votes for a class, and the majority wins. In statistical terms, the predicted
    target label of the ensemble is the mode of the distribution of individually predicted
    labels.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**硬投票**（也称为**多数投票**）中，每个单独的分类器为某个类别投票，多数获胜。从统计学的角度来看，集成预测的目标标签是各个单独预测标签分布的众数。
- en: In **soft voting**, every individual classifier provides a probability value
    that a specific data point belongs to a particular target class. The predictions
    are weighted by the classifier's importance and summed up. Then, the target label
    with the greatest sum of weighted probabilities wins the vote.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**软投票**中，每个单独的分类器提供一个概率值，表示特定数据点属于特定目标类别的可能性。预测结果根据分类器的重要性加权并求和。然后，加权概率总和最大的目标标签赢得投票。
- en: For example, let's assume we have three different classifiers in the ensemble
    that perform a ...
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在集成中有三个不同的分类器执行...
- en: Implementing a voting classifier
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现投票分类器
- en: 'Let''s look at a simple example of a voting classifier that combines three
    different algorithms:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的投票分类器示例，它结合了三种不同的算法：
- en: A logistic regression classifier from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)，*监督学习的第一步*的逻辑回归分类器
- en: A Gaussian Naive Bayes classifier from [Chapter 7](08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml),
    *Implementing a Spam Filter with Bayesian Learning*
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[第7章](08148129-87ac-4042-944d-8e0a2bbbe0c5.xhtml)，*使用贝叶斯学习实现垃圾邮件过滤器*的高斯朴素贝叶斯分类器
- en: A random forest classifier from this chapter
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自本章的随机森林分类器
- en: 'We can combine these three algorithms into a voting classifier and apply it
    to the breast cancer dataset with the following steps:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这三个算法组合成一个投票分类器，并按照以下步骤将其应用于乳腺癌数据集：
- en: 'Load the dataset, and split it into training and test sets:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集，并将其分为训练集和测试集：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Instantiate the individual classifiers:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化单个分类器：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Assign the individual classifiers to the voting ensemble. Here, we need to pass
    a list of tuples (`estimators`), where every tuple consists of the name of the
    classifier (a string of letters depicting a short name of each classifier) and
    the model object. The voting scheme can be either `voting='hard'` or `voting='soft'`.
    For now, we will choose** `voting='hard'`:**
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单个分类器分配给投票集成。在这里，我们需要传递一个元组列表（`estimators`），其中每个元组都包含分类器的名称（表示每个分类器简短名称的字母字符串）和模型对象。投票方案可以是`voting='hard'`或`voting='soft'`。现在，我们将选择**`voting='hard'`**：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Fit the ensemble to the training data and score it on the test data:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集成拟合到训练数据，并在测试数据上评分：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In order to convince us that 95.1% is a great accuracy score, we can compare
    the ensemble''s performance to the theoretical performance of each individual
    classifier. We do this by fitting the individual classifiers to the data. Then,
    we will see that the logistic regression model achieves 94.4% accuracy on its
    own:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们相信95.1%是一个非常好的准确率，我们可以将集成性能与每个单独分类器的理论性能进行比较。我们通过将单独的分类器拟合到数据上来做到这一点。然后，我们将看到逻辑回归模型本身达到了94.4%的准确率：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Similarly, the Naive Bayes classifier achieves 93.0% accuracy:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，朴素贝叶斯分类器达到了93.0%的准确率：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Last but not least, the random forest classifier also achieved 94.4% accuracy:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，随机森林分类器也达到了94.4%的准确率：
- en: '[PRE56]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: All in all, we were just able to gain a good percentage in performance by combining
    three unrelated classifiers into an ensemble. Each of these classifiers might
    have made different mistakes on the training set, but that's OK because, on average,
    we need just two out of three classifiers to be correct.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们仅仅通过将三个无关的分类器组合成一个集成，就能够在性能上获得一个很好的百分比。每个分类器可能在训练集上犯不同的错误，但这没关系，因为平均来说，我们只需要三个分类器中有两个是正确的。
- en: Plurality
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多数投票
- en: In the previous sections, we discussed ensemble methods. What we didn't mention
    earlier was how the results are aggregated across the individual models prepared
    by the ensemble techniques. The concept that is used for this is called **plurality**,which
    is nothing but voting. The higher the vote a class gets, the higher the chances
    of it being the final class. Imagine if we had three models prepared during ensemble
    techniques and 10 possible classes (think of them as digits from 0 to 9). Each
    model would choose one class based on the highest probability it obtained. Finally,
    the class with the maximum number of votes would be selected. This is the concept
    of plurality. In practice, plurality tries to bring benefit to both *k-*NN and
    Naive ...
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了集成方法。我们之前没有提到的是，如何通过集成技术准备的单个模型来汇总结果。用于此的概念被称为**多数投票**，这实际上就是投票。一个类别获得的投票越多，它成为最终类别的可能性就越高。想象一下，如果我们有在集成技术中准备的三种模型和10个可能的类别（可以将它们视为从0到9的数字）。每个模型会根据获得最高概率的类别来选择一个类别。最后，获得最多投票的类别将被选中。这就是多数投票的概念。在实践中，多数投票试图为*k-*NN和朴素贝叶斯等算法带来好处。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about how to improve various classifiers by combining
    them into an ensemble. We discussed how to average the predictions of different
    classifiers using bagging and how to have different classifiers correct each other's
    mistakes using boosting. A lot of time was spent discussing all possible ways
    to combine decision trees, be it decision stumps (AdaBoost), random forests, or
    extremely randomized trees. Finally, we learned how to combine even different
    types of classifiers in an ensemble by building a voting classifier.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何通过将它们组合成一个集成来提高各种分类器的性能。我们讨论了如何使用袋装法平均不同分类器的预测，以及如何使用提升法让不同的分类器纠正彼此的错误。我们花费了大量时间讨论了所有可能的将决策树组合起来的方法，无论是决策树桩（AdaBoost）、随机森林还是极端随机树。最后，我们学习了如何通过构建投票分类器来将不同类型的分类器在集成中结合起来。
- en: In the next chapter, we will talk more about how to compare the results of different
    classifiers by diving into the world of model selection and hyperparameter tuning.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地讨论如何通过深入模型选择和超参数调整的世界来比较不同分类器的结果。
