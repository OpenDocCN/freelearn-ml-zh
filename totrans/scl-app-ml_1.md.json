["```py\nExample myInstance = new Example() ;\n```", "```py\nval myInstance = new Example()\n```", "```py\nval amountSpent = 200\n```", "```py\nval clientIds = List(\"123\", \"456\") // List is immutable\nclientIds(1) = \"589\" // Compile-time error\n```", "```py\ndef occurrencesOf[A](elem:A, collection:List[A]):List[Int] = {\n  for { \n    (currentElem, index) <- collection.zipWithIndex\n    if (currentElem == elem)\n  } yield index\n}\n```", "```py\nstatic <T> List<Integer> occurrencesOf(T elem, List<T> collection) {\n  List<Integer> occurrences = new ArrayList<Integer>() ;\n  for (int i=0; i<collection.size(); i++) {\n    if (collection.get(i).equals(elem)) {\n      occurrences.add(i) ;\n    }\n  }\n  return occurrences ;\n}\n```", "```py\nstatic <T> List<Integer> occurrencesOf(T elem, List<T> collection) {\n  List<Integer> occurences = new ArrayList<Integer>() ;\n  for (int i=0; i<collection.size(); i++) {\n    if (collection.get(i).equals(elem)) { \n      occurrences.add(i) ;\n    }\n  }\n  return occurrences ;\n}\n```", "```py\nclass User {\n  ...\n  val email:Option[Email]\n  ...\n}\n```", "```py\nscala> val nTosses = 100\nnTosses: Int = 100\n\nscala> def trial = (0 until nTosses).count { i =>\n util.Random.nextBoolean() // count the number of heads\n}\ntrial: Int\n\n```", "```py\nscala> trial\nInt = 51\n\n```", "```py\nscala> val nTrials = 100000\nnTrials: Int = 100000\n\nscala> (0 until nTrials).par.count { i => trial >= 60 }\nInt = 2745\n\n```", "```py\n$ git clone 'https://github.com/pbugnion/s4ds'\n\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n)\n```", "```py\nscala> import breeze.linalg._\nimport breeze.linalg._\n\n```", "```py\n$ sbt console\nscala> import breeze.linalg._\nimport breeze.linalg._\n\n```", "```py\nscala> val v = DenseVector(1.0, 2.0, 3.0)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\n\n```", "```py\nscala> v(1)\nDouble = 2.0\n\n```", "```py\nscala> v :* 2.0 // :* is 'element-wise multiplication'\nbreeze.linalg.DenseVector[Double] = DenseVector(2.0, 4.0, 6.0)\n\n```", "```py\nscala> v :+ DenseVector(4.0, 5.0, 6.0) // :+ is 'element-wise addition'\nbreeze.linalg.DenseVector[Double] = DenseVector(5.0, 7.0, 9.0)\n\n```", "```py\nscala> v :* 2 // element-wise multiplication by integer\n<console>:15: error: could not find implicit value for parameter op:\n...\n\n```", "```py\nscala> v :+ DenseVector(8.0, 9.0)\njava.lang.IllegalArgumentException: requirement failed: Vectors must have same length: 3 != 2\n...\n\n```", "```py\nscala> val v2 = DenseVector(4.0, 5.0, 6.0)\nbreeze.linalg.DenseVector[Double] = DenseVector(4.0, 5.0, 6.0)\n\nscala> v dot v2\nDouble = 32.0\n\n```", "```py\nscala> v + v2\nbreeze.linalg.DenseVector[Double] = DenseVector(5.0, 7.0, 9.0)\n\n```", "```py\nscala> 2.0 :* v + v2 // !! equivalent to 2.0 :* (v + v2)\nbreeze.linalg.DenseVector[Double] = DenseVector(10.0, 14.0, 18.0)\n\n```", "```py\nscala> 2.0 :* v :+ v2 // equivalent to (2.0 :* v) :+ v2\nbreeze.linalg.DenseVector[Double] = DenseVector(6.0, 9.0, 12.0)\n\n```", "```py\nscala> val m = DenseMatrix((1.0, 2.0, 3.0), (4.0, 5.0, 6.0))\nbreeze.linalg.DenseMatrix[Double] =\n1.0  2.0  3.0\n4.0  5.0  6.0\n\nscala> 2.0 :* m\nbreeze.linalg.DenseMatrix[Double] =\n2.0  4.0   6.0\n8.0  10.0  12.0\n\n```", "```py\nscala> DenseVector.ones[Double](5)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 1.0, 1.0, 1.0, 1.0)\n\nscala> DenseVector.zeros[Int](3)\nbreeze.linalg.DenseVector[Int] = DenseVector(0, 0, 0)\n\n```", "```py\nscala> linspace(0.0, 1.0, 10)\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.1111111111111111, ..., 1.0)\n\n```", "```py\nscala> DenseVector.tabulate(4) { i => 5.0 * i }\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 5.0, 10.0, 15.0)\n\nscala> DenseMatrix.tabulate[Int](2, 3) { \n (irow, icol) => irow*2 + icol \n}\nbreeze.linalg.DenseMatrix[Int] =\n0  1  2\n2  3  4\n\n```", "```py\nscala> DenseVector.rand(2)\nbreeze.linalg.DenseVector[Double] = DenseVector(0.8072865137359484, 0.5566507203838562)\n\nscala> DenseMatrix.rand(2, 3)\nbreeze.linalg.DenseMatrix[Double] =\n0.5755491874682879   0.8142161471517582  0.9043780212739738\n0.31530195124023974  0.2095094278911871  0.22069103504148346\n\n```", "```py\nscala> DenseVector(Array(2, 3, 4))\nbreeze.linalg.DenseVector[Int] = DenseVector(2, 3, 4)\n\n```", "```py\nscala> val l = Seq(2, 3, 4)\nl: Seq[Int] = List(2, 3, 4)\n\nscala> DenseVector(l :_ *)\nbreeze.linalg.DenseVector[Int] = DenseVector(2, 3, 4)\n\n```", "```py\nscala> val v = DenseVector.tabulate(5) { _.toDouble }\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 1.0, 2.0, 3.0, 4.0)\n\n```", "```py\nscala> v(-1) // last element\nDouble = 4.0\n\n```", "```py\nscala> v(1 to 3)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\n\nscala v(1 until 3) // equivalent to Python v[1:3]\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0)\n\nscala> v(v.length-1 to 0 by -1) // reverse view of v\nbreeze.linalg.DenseVector[Double] = DenseVector(4.0, 3.0, 2.0, 1.0, 0.0)\n\n```", "```py\nscala> val vSlice = v(2, 4) // Select elements at index 2 and 4\nbreeze.linalg.SliceVector[Int,Double] = breeze.linalg.SliceVector@9c04d22\n\n```", "```py\nscala> vSlice.toDenseVector\nbreeze.linalg.DenseVector[Double] = DenseVector(2.0, 4.0)\n\n```", "```py\nscala> val vSlice = v(2, 7) // there is no v(7)\nbreeze.linalg.SliceVector[Int,Double] = breeze.linalg.SliceVector@2a83f9d1\n\nscala> vSlice(0) // valid since v(2) is still valid\nDouble = 2.0\n\nscala> vSlice(1) // invalid since v(7) is out of bounds\njava.lang.IndexOutOfBoundsException: 7 not in [-5,5)\n ...\n\n```", "```py\nscala> val mask = DenseVector(true, false, false, true, true)\nbreeze.linalg.DenseVector[Boolean] = DenseVector(true, false, false, true, true)\n\n```", "```py\nscala> v(mask).toDenseVector\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 3.0, 4.0)\n\n```", "```py\nscala> val filtered = v(v :< 3.0) // :< is element-wise \"less than\"\nbreeze.linalg.SliceVector[Int,Double] = breeze.linalg.SliceVector@2b1edef3\n\nscala> filtered.toDenseVector\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 1.0, 2.0)\n\n```", "```py\nscala> val m = DenseMatrix((1.0, 2.0, 3.0), (5.0, 6.0, 7.0))\nm: breeze.linalg.DenseMatrix[Double] =\n1.0  2.0  3.0\n5.0  6.0  7.0\n\nscala> m(1, 2)\nDouble = 7.0\n\nscala> m(1, -1)\nDouble = 7.0\n\nscala> m(0 until 2, 0 until 2)\nbreeze.linalg.DenseMatrix[Double] =\n1.0  2.0\n5.0  6.0\n\n```", "```py\nscala> m(0 until 2, 0)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 5.0)\n\n```", "```py\nscala> m(::, 1)\nbreeze.linalg.DenseVector[Double] = DenseVector(2.0, 6.0)\n\n```", "```py\nscala> val v = DenseVector(1.0, 2.0, 3.0)\nv: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\n\nscala> v(1) = 22.0 // v is now DenseVector(1.0, 22.0, 3.0)\n\n```", "```py\nscala> v(0 until 2) := DenseVector(50.0, 51.0) // set elements at position 0 and 1\nbreeze.linalg.DenseVector[Double] = DenseVector(50.0, 51.0)\n\nscala> v\nbreeze.linalg.DenseVector[Double] = DenseVector(50.0, 51.0, 3.0)\n\n```", "```py\nscala> v(0 until 2) := 0.0 // equivalent to v(0 until 2) := DenseVector(0.0, 0.0)\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.0)\n\nscala> v\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.0, 3.0)\n\n```", "```py\nscala> val v = DenseVector(1.0, 2.0, 3.0)\nv: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\n\nscala> v :+= 4.0\nbreeze.linalg.DenseVector[Double] = DenseVector(5.0, 6.0, 7.0)\n\nscala> v\nbreeze.linalg.DenseVector[Double] = DenseVector(5.0, 6.0, 7.0)\n\n```", "```py\nscala> val v = DenseVector.tabulate(6) { _.toDouble }\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 1.0, 2.0, 3.0, 4.0, 5.0)\n\nscala> val viewEvens = v(0 until v.length by 2)\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 2.0, 4.0)\n\nscala> viewEvens := 10.0 // mutate viewEvens\nbreeze.linalg.DenseVector[Double] = DenseVector(10.0, 10.0, 10.0)\n\nscala> viewEvens\nbreeze.linalg.DenseVector[Double] = DenseVector(10.0, 10.0, 10.0)\n\nscala> v  // v has also been mutated!\nbreeze.linalg.DenseVector[Double] = DenseVector(10.0, 1.0, 10.0, 3.0, 10.0, 5.0)\n\n```", "```py\nscala> val copyEvens = v(0 until v.length by 2).copy\nbreeze.linalg.DenseVector[Double] = DenseVector(10.0, 10.0, 10.0)\n\n```", "```py\nscala> val m1 = DenseMatrix((2.0, 3.0), (5.0, 6.0), (8.0, 9.0))\nbreeze.linalg.DenseMatrix[Double] =\n2.0  3.0\n5.0  6.0\n8.0  9.0\n\nscala> val m2 = DenseMatrix((10.0, 11.0), (12.0, 13.0))\nbreeze.linalg.DenseMatrix[Double] \n10.0  11.0\n12.0  13.0\n\nscala> m1 * m2\n56.0   61.0\n122.0  133.0\n188.0  205.0\n\n```", "```py\nscala> val v = DenseVector(1.0, 2.0)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0)\n\nscala> m1 * v \nbreeze.linalg.DenseVector[Double] = DenseVector(8.0, 17.0, 26.0)\n\n```", "```py\nscala> val vt = v.t\nbreeze.linalg.Transpose[breeze.linalg.DenseVector[Double]] = Transpose(DenseVector(1.0, 2.0))\n\nscala> vt * m2\nbreeze.linalg.Transpose[breeze.linalg.DenseVector[Double]] = Transpose(DenseVector(34.0, 37.0))\n\n```", "```py\nscala> val data = HWData.load\nHWData [ 181 rows ]\n\nscala> data.genders\nbreeze.linalg.Vector[Char] = DenseVector(M, F, F, M, ... )\n\n```", "```py\nscala> val maleVector = DenseVector.fill(data.genders.length)('M')\nbreeze.linalg.DenseVector[Char] = DenseVector(M, M, M, M, M, M,... )\n\nscala> val isMale = (data.genders :== maleVector)\nbreeze.linalg.DenseVector[Boolean] = DenseVector(true, false, false, true ...)\n\n```", "```py\nscala> val maleHeights = data.heights(isMale)\nbreeze.linalg.SliceVector[Int,Double] = breeze.linalg.SliceVector@61717d42\n\nscala> maleHeights.toDenseVector\nbreeze.linalg.DenseVector[Double] = DenseVector(182.0, 177.0, 170.0, ...\n\n```", "```py\nscala> import breeze.numerics._\nimport breeze.numerics._\n\nscala> sum(I(isMale))\nDouble: 82.0\n\n```", "```py\nscala> import breeze.stats._\nimport breeze.stats._\n\nscala> mean(data.heights)\nDouble = 170.75690607734808\n\n```", "```py\nscala> mean(data.heights(isMale)) // mean male height\nDouble = 178.0121951219512\n\nscala> mean(data.heights(!isMale)) // mean female height\nDouble = 164.74747474747474\n\n```", "```py\nscala> val discrepancy = (data.weights - data.reportedWeights) / data.weights\nbreeze.linalg.Vector[Double] = DenseVector(0.0, 0.1206896551724138, -0.018867924528301886, -0.029411764705882353, ... )\n\n```", "```py\nscala> mean(discrepancy(isMale))\nres6: Double = -0.008451852933123775\n\nscala> stddev(discrepancy(isMale))\nres8: Double = 0.031901519634244195\n\n```", "```py\nscala> val overReportMask = (data.reportedHeights :> data.heights).toDenseVector\nbreeze.linalg.DenseVector[Boolean] = DenseVector(false, false, false, false...\n\nscala> sum(I(overReportMask :& isMale))\nDouble: 10.0\n\n```", "```py\nscala> import breeze.optimize._\nimport breeze.optimize._\n\n```", "```py\nscala> def f(xs:DenseVector[Double]) = sum(xs :^ 2.0)\nf: (xs: breeze.linalg.DenseVector[Double])Double\n\n```", "```py\nscala> def gradf(xs:DenseVector[Double]) = 2.0 :* xs\ngradf: (xs:breeze.linalg.DenseVector[Double])breeze.linalg.DenseVector[Double]\n\n```", "```py\nscala> val xs = DenseVector.ones[Double](3)\nbreeze.linalg.DenseVector[Double] = DenseVector(1.0, 1.0, 1.0)\n\nscala> f(xs)\nDouble = 3.0\n\nscala> gradf(xs)\nbreeze.linalg.DenseVector[Double] = DenseVector(2.0, 2.0, 2.0)\n\n```", "```py\nscala> val optTrait = new DiffFunction[DenseVector[Double]] {\n def calculate(xs:DenseVector[Double]) = (f(xs), gradf(xs))\n}\nbreeze.optimize.DiffFunction[breeze.linalg.DenseVector[Double]] = <function1>\n\n```", "```py\nscala> val minimum = minimize(optTrait, DenseVector(1.0, 1.0, 1.0))\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.0, 0.0)\n\n```", "```py\nscala> val approxOptTrait = new ApproximateGradientFunction(f)\nbreeze.optimize.ApproximateGradientFunction[Int,breeze.linalg.DenseVector[Double]] = <function1>\n\n```", "```py\nscala> approxOptTrait.gradientAt(DenseVector.ones(3))\nbreeze.linalg.DenseVector[Double] = DenseVector(2.00001000001393, 2.00001000001393, 2.00001000001393)\n\n```", "```py\nscala> minimize(approxOptTrait, DenseVector.ones[Double](3))\nbreeze.linalg.DenseVector[Double] = DenseVector(-5.000001063126813E-6, -5.000001063126813E-6, -5.000001063126813E-6)\n\n```", "```py\nscala> minimize(optTrait, DenseVector(1.0, 1.0, 1.0), L2Regularization(0.5))\nbreeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.0, 0.0)\n\n```", "```py\nimport breeze.linalg._\nimport breeze.numerics._\nimport breeze.optimize._\nimport breeze.stats._\n\nobject LogisticRegressionHWData extends App {\n\n  val data = HWData.load\n\n  // Rescale the features to have mean of 0.0 and s.d. of 1.0\n  def rescaled(v:DenseVector[Double]) =\n    (v - mean(v)) / stddev(v)\n\n  val rescaledHeights = rescaled(data.heights)\n  val rescaledWeights = rescaled(data.weights)\n\n  // Build the feature matrix as a matrix with \n  //181 rows and 3 columns.\n  val rescaledHeightsAsMatrix = rescaledHeights.toDenseMatrix.t\n  val rescaledWeightsAsMatrix = rescaledWeights.toDenseMatrix.t\n\n  val featureMatrix = DenseMatrix.horzcat(\n    DenseMatrix.ones[Double](rescaledHeightsAsMatrix.rows, 1),\n    rescaledHeightsAsMatrix,\n    rescaledWeightsAsMatrix\n  )\n\n  println(s\"Feature matrix size: ${featureMatrix.rows} x \" +s\"${featureMatrix.cols}\")\n\n  // Build the target variable to be 1.0 where a participant\n  // is male, and 0.0 where the participant is female.\n  val target = data.genders.values.map {\n    gender => if(gender == 'M') 1.0 else 0.0\n  }\n\n  // Build the loss function ready for optimization.\n  // We will worry about refactoring this to be more \n  // efficient later.\n  def costFunction(parameters:DenseVector[Double]):Double = {\n    val xBeta = featureMatrix * parameters\n    val expXBeta = exp(xBeta)\n    - sum((target :* xBeta) - log1p(expXBeta))\n  }\n\n  def costFunctionGradient(parameters:DenseVector[Double])\n  :DenseVector[Double] = {\n    val xBeta = featureMatrix * parameters\n    val probs = sigmoid(xBeta)\n    featureMatrix.t * (probs - target)\n  }\n\n  val f = new DiffFunction[DenseVector[Double]] {\n    def calculate(parameters:DenseVector[Double]) =\n      (costFunction(parameters), costFunctionGradient(parameters))\n  }\n\n  val optimalParameters = minimize(f, DenseVector(0.0, 0.0, 0.0))\n\n  println(optimalParameters)\n  // => DenseVector(-0.0751454743, 2.476293647, 2.23054540)\n}\n```", "```py\nobject LogisticRegressionHWData extends App {\n```", "```py\ndef rescaled(v:DenseVector[Double]) =\n  (v - mean(v)) / stddev(v)\n\nval rescaledHeights = rescaled(data.heights)\nval rescaledWeights = rescaled(data.weights)\n```", "```py\nval rescaledHeightsAsMatrix = rescaledHeights.toDenseMatrix.t\nval rescaledWeightsAsMatrix = rescaledWeights.toDenseMatrix.t\n```", "```py\nDenseMatrix.ones[Double](rescaledHeightsAsMatrix.rows, 1)\n```", "```py\nval featureMatrix = DenseMatrix.horzcat(\n  DenseMatrix.ones[Double](rescaledHeightsAsMatrix.rows, 1),\n  rescaledHeightsAsMatrix,\n  rescaledWeightsAsMatrix\n)\n```", "```py\nval target = data.genders.values.map {\n  gender => if(gender == 'M') 1.0 else 0.0\n}\n```", "```py\nval maleVector = DenseVector.fill(data.genders.size)('M')\nval target = I(data.genders :== maleVector)\n```", "```py\ndef costFunction(parameters:DenseVector[Double]):Double = {\n  val xBeta = featureMatrix * parameters \n  val expXBeta = exp(xBeta)\n  - sum((target :* xBeta) - log1p(expXBeta))\n}\n```", "```py\ncostFunction(DenseVector(0.0, 0.0, 0.0)) // 125.45963968135031\ncostFunction(DenseVector(0.0, 0.1, 0.1)) // 113.33336518036882\ncostFunction(DenseVector(0.0, -0.1, -0.1)) // 139.17134594294433\n```", "```py\ndef costFunctionGradient(parameters:DenseVector[Double])\n:DenseVector[Double] = {\n  val xBeta = featureMatrix * parameters \n  val probs = sigmoid(xBeta)\n  featureMatrix.t * (probs - target)\n}\n```", "```py\n val f = new DiffFunction[DenseVector[Double]] {\n   def calculate(parameters:DenseVector[Double]) = \n     (costFunction(parameters), costFunctionGradient(parameters))\n }\n```", "```py\n  val optimalParameters = minimize(f, DenseVector(0.0, 0.0, 0.0))\n```", "```py\nDenseVector(-0.0751454743, 2.476293647, 2.23054540)\n```", "```py\nimport breeze.linalg._\nimport breeze.numerics._\nimport breeze.optimize._\n\nclass LogisticRegression(\n    val training:DenseMatrix[Double], \n    val target:DenseVector[Double])\n{\n```", "```py\n  def costFunctionAndGradient(coefficients:DenseVector[Double])\n  :(Double, DenseVector[Double]) = {\n    val xBeta = training * coefficients\n    val expXBeta = exp(xBeta)\n    val cost = - sum((target :* xBeta) - log1p(expXBeta))\n    val probs = sigmoid(xBeta)\n    val grad = training.t * (probs - target)\n    (cost, grad)\n  }\n```", "```py\nlazy val optimalCoefficients = ???\n```", "```py\nprivate def calculateOptimalCoefficients\n:DenseVector[Double] = {\n  val f = new DiffFunction[DenseVector[Double]] {\n    def calculate(parameters:DenseVector[Double]) = \n      costFunctionAndGradient(parameters)\n  }\n\n  minimize(f, DenseVector.zeros[Double](training.cols))\n}\n\nlazy val optimalCoefficients = calculateOptimalCoefficients\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-viz\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n)\n```", "```py\n$ sbt console\n\nscala> import breeze.linalg._\nimport breeze.linalg._\n\nscala> import breeze.plot._\nimport breeze.plot._\n\nscala> import breeze.numerics._\nimport breeze.numerics._\n\n```", "```py\nscala> val x = linspace(-4.0, 4.0, 200)\nx: DenseVector[Double] = DenseVector(-4.0, -3.959798...\n\nscala> val fx = sigmoid(x)\nfx: DenseVector[Double] = DenseVector(0.0179862099620915,...\n\n```", "```py\nscala> val fig = Figure()\nfig: breeze.plot.Figure = breeze.plot.Figure@37e36de9\n\n```", "```py\nscala> val plt = fig.subplot(0)\nplt: breeze.plot.Plot = breeze.plot.Plot@171c2840\n\n```", "```py\nscala> plt += plot(x, fx)\nbreeze.plot.Plot = breeze.plot.Plot@63d6a0f8\n\n```", "```py\nscala> fig.refresh()\n\n```", "```py\nscala> fig.saveas(\"sigmoid.png\")\n\n```", "```py\nscala> val f2x = sigmoid(2.0*x)\nf2x: breeze.linalg.DenseVector[Double] = DenseVector(3.353501304664E-4...\n\nscala> val f10x = sigmoid(10.0*x)\nf10x: breeze.linalg.DenseVector[Double] = DenseVector(4.24835425529E-18...\n\nscala> plt += plot(x, f2x, name=\"S(2x)\")\nbreeze.plot.Plot = breeze.plot.Plot@63d6a0f8\n\nscala> plt += plot(x, f10x, name=\"S(10x)\")\nbreeze.plot.Plot = breeze.plot.Plot@63d6a0f8\n\nscala> fig.refresh()\n\n```", "```py\nscala> plt.legend = true\n\n```", "```py\nscala> plt.xlim = (-4.0, 4.0)\nplt.xlim: (Double, Double) = (-4.0,4.0)\n\n```", "```py\nscala> plt.yaxis\norg.jfree.chart.axis.NumberAxis = org.jfree.chart.axis.NumberAxis@0\n\n```", "```py\nscala> import org.jfree.chart.axis.NumberTickUnit\nimport org.jfree.chart.axis.NumberTickUnit\n\nscala> plt.yaxis.setTickUnit(new NumberTickUnit(0.1))\n\n```", "```py\nscala> plt.plot\norg.jfree.chart.plot.XYPlot = org.jfree.chart.plot.XYPlot@17e4db6c\n\n```", "```py\nscala> import org.jfree.chart.plot.ValueMarker\nimport org.jfree.chart.plot.ValueMarker\n\nscala> plt.plot.addDomainMarker(new ValueMarker(0.0))\n\nscala> plt.plot.addRangeMarker(new ValueMarker(1.0))\n\n```", "```py\nscala> plt.xlabel = \"x\"\nplt.xlabel: String = x\n\nscala> plt.ylabel = \"f(x)\"\nplt.ylabel: String = f(x)\n\n```", "```py\nscala> val data = HWData.load\ndata: HWData = HWData [ 181 rows ]\n\nscala> data.heights\nbreeze.linalg.DenseVector[Double] = DenseVector(182.0, ...\n\nscala> data.weights\nbreeze.linalg.DenseVector[Double] = DenseVector(77.0, 58.0...\n\n```", "```py\nscala> val fig = Figure(\"height vs. weight\")\nfig: breeze.plot.Figure = breeze.plot.Figure@743f2558\n\nscala> val plt = fig.subplot(0)\nplt: breeze.plot.Plot = breeze.plot.Plot@501ea274\n\nscala> plt += plot(data.heights, data.weights, '+',         colorcode=\"black\")\nbreeze.plot.Plot = breeze.plot.Plot@501ea274\n\n```", "```py\nscala> val features = DenseMatrix.horzcat(\n DenseMatrix.ones[Double](data.npoints, 1),\n data.heights.toDenseMatrix.t\n)\nfeatures: breeze.linalg.DenseMatrix[Double] =\n1.0  182.0\n1.0  161.0\n1.0  161.0\n1.0  177.0\n1.0  157.0\n...\n\nscala> import breeze.stats.regression._\nimport breeze.stats.regression._\n\nscala> val leastSquaresResult = leastSquares(features, data.weights)\nleastSquaresResult: breeze.stats.regression.LeastSquaresRegressionResult = <function1>\n\n```", "```py\nscala> leastSquaresResult.coefficients\nbreeze.linalg.DenseVector[Double] = DenseVector(-131.042322, 1.1521875)\n\n```", "```py\nscala> val Array(a, b) = leastSquaresResult.coefficients.toArray\na: Double = -131.04232269750622\nb: Double = 1.1521875435418725\n\n```", "```py\nscala> val dummyHeights = linspace(min(data.heights), max(data.heights), 200)\ndummyHeights: breeze.linalg.DenseVector[Double] = DenseVector(148.0, ...\n\nscala> val fittedWeights = a :+ (b :* dummyHeights)\nfittedWeights: breeze.linalg.DenseVector[Double] = DenseVector(39.4814...\n\nscala> plt += plot(dummyHeights, fittedWeights, colorcode=\"red\")\nbreeze.plot.Plot = breeze.plot.Plot@501ea274\n\n```", "```py\nscala> val label = f\"weight = $a%.4f + $b%.4f * height\"\nlabel: String = weight = -131.0423 + 1.1522 * height\n\n```", "```py\nscala> import org.jfree.chart.annotations.XYTextAnnotation\nimport org.jfree.chart.annotations.XYTextAnnotation\n\nscala> plt.plot.addAnnotation(new XYTextAnnotation(label, 175.0, 105.0))\n\n```", "```py\nscala> val fig = new Figure(\"Advanced scatter example\")\nfig: breeze.plot.Figure = breeze.plot.Figure@220821bc\n\nscala> val plt = fig.subplot(0)\nplt: breeze.plot.Plot = breeze.plot.Plot@668f8ae0\n\nscala> val xs = linspace(0.0, 1.0, 100)\nxs: breeze.linalg.DenseVector[Double] = DenseVector(0.0, 0.010101010101010102, 0.0202 ...\n\nscala> val sizes = 0.025 * DenseVector.rand(100) // random sizes\nsizes: breeze.linalg.DenseVector[Double] = DenseVector(0.014879265631723166, 0.00219551...\n\nscala> plt += scatter(xs, xs :^ 2.0, sizes.apply)\nbreeze.plot.Plot = breeze.plot.Plot@668f8ae0\n\n```", "```py\nscala> val palette = new GradientPaintScale(\n 0.0, 1.0, PaintScale.RedToGreen)\npalette: breeze.plot.GradientPaintScale[Double] = <function1>\n\nscala> palette(0.5) // half-way between red and green\njava.awt.Paint = java.awt.Color[r=127,g=127,b=0]\n\nscala> palette(1.0) // green\njava.awt.Paint = java.awt.Color[r=0,g=254,b=0]\n\n```", "```py\nscala> val palette = new GradientPaintScale(0.0, 1.0, PaintScale.MaroonToGold)\npalette: breeze.plot.GradientPaintScale[Double] = <function1>\n\nscala> val colors = DenseVector.rand(100).mapValues(palette)\ncolors: breeze.linalg.DenseVector[java.awt.Paint] = DenseVector(java.awt.Color[r=162,g=5,b=0], ...\n\nscala> plt += scatter(xs, xs :^ 2.0, sizes.apply, colors.apply)\nbreeze.plot.Plot = breeze.plot.Plot@8ff7e27\n\n```", "```py\nimport breeze.plot._\n\ndef subplotExample {\n  val data = HWData.load\n  val fig = new Figure(\"Subplot example\")\n\n  // upper subplot: plot index '0' refers to the first plot\n  var plt = fig.subplot(2, 1, 0)\n  plt += plot(data.heights, data.weights, '.')\n\n  // lower subplot: plot index '1' refers to the second plot\n  plt = fig.subplot(2, 1, 1)\n  plt += plot(data.heights, data.reportedHeights, '.', colorcode=\"black\")\n\n  fig.refresh\n}\n```", "```py\nimport breeze.plot._\nimport breeze.linalg._\n\nclass ScatterplotMatrix(val fig:Figure) {\n\n  /** Draw the histograms on the diagonal */\n  private def plotHistogram(plt:Plot)(\n  data:DenseVector[Double], label:String) {\n     plt += hist(data)\n     plt.xlabel = label\n  }\n\n  /** Draw the off-diagonal scatter plots */\n  private def plotScatter(plt:Plot)(\n    xdata:DenseVector[Double],\n    ydata:DenseVector[Double],\n    xlabel:String,\n    ylabel:String) {\n      plt += plot(xdata, ydata, '.')\n      plt.xlabel = xlabel\n      plt.ylabel = ylabel\n  }\n\n...\n```", "```py\n0 1 2 3\n4 5 6 7\n...\n```", "```py\n  private def selectPlot(ncols:Int)(irow:Int, icol:Int):Plot = {\n    fig.subplot(ncols, ncols, (irow)*ncols + icol)\n  }\n```", "```py\n  /** Draw a scatterplot matrix.\n    *\n    * This function draws a scatterplot matrix of the correlation\n    * between each pair of columns in `featureMatrix`.\n    *\n    * @param featureMatrix A matrix of features, with each column\n    *   representing a feature.\n    * @param labels Names of the features.\n    */\n  def plotFeatures(featureMatrix:DenseMatrix[Double], labels:List[String]) {\n    val ncols = featureMatrix.cols\n    require(ncols == labels.size,\n      \"Number of columns in feature matrix \"+ \"must match length of labels\"\n    )\n    fig.clear\n    fig.subplot(ncols, ncols, 0)\n\n    (0 until ncols) foreach { irow =>\n      val p = selectPlot(ncols)(irow, irow)\n      plotHistogram(p)(featureMatrix(::, irow), labels(irow))\n\n      (0 until irow) foreach { icol =>\n        val p = selectPlot(ncols)(irow, icol)\n        plotScatter(p)(\n          featureMatrix(::, irow),\n          featureMatrix(::, icol),\n          labels(irow),\n          labels(icol)\n        )\n      }\n    }\n  }\n}\n```", "```py\nimport breeze.linalg._\nimport breeze.numerics._\nimport breeze.plot._\n\nobject ScatterplotMatrixDemo extends App {\n\n  val data = HWData.load\n  val m = new ScatterplotMatrix(Figure(\"Scatterplot matrix demo\"))\n\n  // Make a matrix with three columns: the height, weight and\n  // reported weight data.\n  val featureMatrix = DenseMatrix.horzcat(\n    data.heights.toDenseMatrix.t,\n    data.weights.toDenseMatrix.t,\n    data.reportedWeights.toDenseMatrix.t\n  )\n  m.plotFeatures(featureMatrix,List(\"height\", \"weight\", \"reportedWeights\"))\n\n}\n```", "```py\nscala> val sentence = \"The quick brown fox jumped over the lazy dog\"\nsentence: String = The quick brown fox jumped ...\n\n```", "```py\nscala> val characters = sentence.toVector\nVector[Char] = Vector(T, h, e,  , q, u, i, c, k, ...)\n\n```", "```py\nscala> val charactersPar = characters.par\nParVector[Char] = ParVector(T, h, e,  , q, u, i, c, k,  , ...)\n\n```", "```py\nscala> val lettersPar = charactersPar.filter { _ != ' ' }\nParVector[Char] = ParVector(T, h, e, q, u, i, c, k, ...)\n\n```", "```py\nscala> val lowerLettersPar = lettersPar.map { _.toLower }\nParVector[Char] = ParVector(t, h, e, q, u, i, c, k, ...)\n\n```", "```py\nscala> val intermediateMap = lowerLettersPar.groupBy(identity)\nParMap[Char,ParVector[Char]] = ParMap(e -> ParVector(e, e, e, e), ...)\n\n```", "```py\nscala> val occurenceNumber = intermediateMap.mapValues { _.length }\nParMap[Char,Int] = ParMap(e -> 4, x -> 1, n -> 1, j -> 1, ...)\n\n```", "```py\nscala> var count = 0\ncount: Int = 0\n\nscala> (0 until 1000).par.foreach { i => count += 1 }\n\nscala> count\ncount: Int = 874 // not 1000!\n\n```", "```py\nscala> import scala.collection.mutable\nimport scala.collection.mutable\n\nscala> val occurenceNumber = mutable.Map.empty[Char, Int]\noccurenceNumber: mutable.Map[Char,Int] = Map()\n\nscala> lowerLettersPar.foreach { c => \n occurenceNumber(c) = occurenceNumber.getOrElse(c, 0) + 1\n}\n\nscala> occurenceNumber('e') // Should be 4\nInt = 2\n\n```", "```py\nscala> (0 until 1000).par.reduce {_ - _ } // should be -499500\nInt = 63620\n\n```", "```py\nscala> Vector(2, 0, 5).par.map { 10 / _ }\njava.lang.ArithmeticException: / by zero\n...\n\n```", "```py\nscala> import scala.util._\nimport scala.util._\n\nscala> Try { 2 + 2 }\nTry[Int] = Success(4)\n\n```", "```py\nscala> import scala.io.Source\nimport scala.io.Source\n\nscala> val html = Source.fromURL(\"http://www.google.com\")\nscala.io.BufferedSource = non-empty iterator\n\nscala> val html = Source.fromURL(\"garbage\")\njava.net.MalformedURLException: no protocol: garbage\n...\n\n```", "```py\nscala> Try { Source.fromURL(\"http://www.google.com\") }\nTry[BufferedSource] = Success(non-empty iterator)\n\nscala> Try { Source.fromURL(\"garbage\") }\nTry[BufferedSource] = Failure(java.net.MalformedURLException: no protocol: garbage)\n\n```", "```py\nscala> val URLs = Vector(\"http://www.google.com\", \n \"http://www.bbc.co.uk\",\n \"not-a-url\"\n)\nURLs: Vector[String] = Vector(http://www.google.com, http://www.bbc.co.uk, not-a-url)\n\nscala> val pages = URLs.par.map { url =>\n url -> Try { Source.fromURL(url) } \n}\npages: ParVector[(String, Try[BufferedSource])] = ParVector((http://www.google.com,Success(non-empty iterator)), (http://www.bbc.co.uk,Success(non-empty iterator)), (not-a-url,Failure(java.net.MalformedURLException: no protocol: not-a-url)))\n\n```", "```py\nscala> pages.collect { case(url, Success(it)) => url -> it.size }\nParVector[(String, Int)] = ParVector((http://www.google.com,18976), (http://www.bbc.co.uk,132893))\n\n```", "```py\nscala> val parRange = (0 to 100).par\nparRange: ParRange = ParRange(0, 1, 2, 3, 4, 5,...\n\nscala> parRange.tasksupport\nTaskSupport = scala.collection.parallel.ExecutionContextTaskSupport@311a0b3e\n\nscala> parRange.tasksupport.parallelismLevel\nInt = 8 // Number of threads to be used\n\n```", "```py\nscala> import scala.collection.parallel._\nimport scala.collection.parallel._\n\nscala> parRange.tasksupport = new ForkJoinTaskSupport(\n new scala.concurrent.forkjoin.ForkJoinPool(4)\n)\nparRange.tasksupport: scala.collection.parallel.TaskSupport = scala.collection.parallel.ForkJoinTaskSupport@6e1134e1\n\nscala> parRange.tasksupport.parallelismLevel\nInt: 4\n\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n)\n\n```", "```py\ntype CVFunction = (Seq[Int], Seq[Int]) => Double\n```", "```py\n// RandomSubsample.scala\n\nimport breeze.linalg._\nimport breeze.numerics._\n\n/** Random subsample cross-validation\n  * \n  * @param nElems Total number of elements in the training set.\n  * @param nCrossValidation Number of elements to leave out of training set.\n*/\nclass RandomSubsample(val nElems:Int, val nCrossValidation:Int) {\n\n  type CVFunction = (Seq[Int], Seq[Int]) => Double\n\n  require(nElems > nCrossValidation,\n    \"nCrossValidation, the number of elements \" +\n    \"withheld, must be < nElems\")\n\n  private val indexList = DenseVector.range(0, nElems)\n\n  /** Perform multiple random sub-sample CV runs on f\n    *\n    * @param nShuffles Number of random sub-sample runs.\n    * @param f user-defined function mapping from a list of\n    *   indices in the training set and a list of indices in the\n    *   test-set to a double indicating the out-of sample score\n    *   for this split.\n    * @returns DenseVector of the CV error for each random split.\n    */\n  def mapSamples(nShuffles:Int)(f:CVFunction)\n  :DenseVector[Double] = {\n    val cvResults = (0 to nShuffles).par.map { i =>\n\n      // Randomly split indices between test and training\n      val shuffledIndices = breeze.linalg.shuffle(indexList)\n      val Seq(testIndices, trainingIndices) =\n        split(shuffledIndices, Seq(nCrossValidation))\n\n // Apply f for this split\n      f(trainingIndices.toScalaVector, \n        testIndices.toScalaVector)\n    }\n    DenseVector(cvResults.toArray)\n  }\n}\n```", "```py\nclass RandomSubsample(val nElems:Int, val nCrossValidation:Int)\n```", "```py\nprivate val indexList = DenseVector.range(0, nElems)\n```", "```py\n  def mapSamples(nShuffles:Int)(f:CVFunction):DenseVector[Double] \n```", "```py\n    val cvResults = (0 to nShuffles).par.map { i =>\n      val shuffledIndices = breeze.linalg.shuffle(indexList)\n      val Seq(testIndices, trainingIndices) = \n        split(shuffledIndices, Seq(nCrossValidation))\n      f(trainingIndices.toScalaVector, testIndices.toScalaVector)\n    }\n```", "```py\nDenseVector(cvResults.toArray) \n```", "```py\n// Logistic Regression.scala\n\nclass LogisticRegression(\n  val training:DenseMatrix[Double],\n  val target:DenseVector[Double]\n) {\n  ...\n  /** Probability of classification for each row\n    * in test set.\n    */\n  def predictProbabilitiesMany(test:DenseMatrix[Double])\n  :DenseVector[Double] = {\n    val xBeta = test * optimalCoefficients\n    sigmoid(xBeta)\n  }\n\n  /** Predict the value of the target variable \n    * for each row in test set.\n    */\n  def classifyMany(test:DenseMatrix[Double])\n  :DenseVector[Double] = {\n    val probabilities = predictProbabilitiesMany(test)\n    I((probabilities :> 0.5).toDenseVector)\n  }\n  ...\n}\n```", "```py\n// RandomSubsampleDemo.scala\n\nimport breeze.linalg._\nimport breeze.linalg.functions.manhattanDistance\nimport breeze.numerics._\nimport breeze.stats._\n\nobject RandomSubsampleDemo extends App {\n\n  /* Load and pre-process data */\n  val data = HWData.load\n\n  val rescaledHeights:DenseVector[Double] =\n    (data.heights - mean(data.heights)) / stddev(data.heights)\n\n  val rescaledWeights:DenseVector[Double] =\n    (data.weights - mean(data.weights)) / stddev(data.weights)\n\n  val featureMatrix:DenseMatrix[Double] =\n    DenseMatrix.horzcat(\n      DenseMatrix.ones[Double](data.npoints, 1),\n      rescaledHeights.toDenseMatrix.t,\n      rescaledWeights.toDenseMatrix.t\n    )\n\n  val target:DenseVector[Double] = data.genders.values.map { \n    gender => if(gender == 'M') 1.0 else 0.0 \n  }\n\n  /* Cross-validation */\n  val testSize = 20\n  val cvCalculator = new RandomSubsample(data.npoints, testSize)\n\n  // Start parallel CV loop\n  val cvErrors = cvCalculator.mapSamples(1000) { \n    (trainingIndices, testIndices) =>\n\n    val regressor = new LogisticRegression(\n      data.featureMatrix(trainingIndices, ::).toDenseMatrix,\n      data.target(trainingIndices).toDenseVector\n    )\n    // Predictions on test-set\n    val genderPredictions = regressor.classifyMany(\n      data.featureMatrix(testIndices, ::).toDenseMatrix\n    )\n    // Calculate number of mis-classified examples\n    val dist = manhattanDistance(\n      genderPredictions, data.target(testIndices)\n    )\n    // Calculate mis-classification rate\n    dist / testSize.toDouble\n  }\n\n  println(s\"Mean classification error: ${mean(cvErrors)}\")\n}\n```", "```py\nscala> import scala.io._\nimport scala.io_\n\nscala> val url = \"http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG\"\nurl: String = http://dev.markitondemand.com/MODApis/Api/v2/Quote?symbol=GOOG\n\nscala> val response = Source.fromURL(url).mkString\nresponse: String = <StockQuote><Status>SUCCESS</Status>\n...\n\n```", "```py\nscala> import scala.concurrent._\nimport scala.concurrent._\n\nscala> import scala.util._\nimport scala.util._\n\nscala> import scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.ExecutionContext.Implicits.global\n\nscala> val response = Future { Source.fromURL(url).mkString }\nresponse: Future[String] = Promise$DefaultPromise@3301801b\n\n```", "```py\nscala> val response = Future { \n Thread.sleep(10000) // sleep for 10s\n Source.fromURL(url).mkString\n}\nresponse: Future[String] = Promise$DefaultPromise@231f98ef\n\n```", "```py\nscala> response.isCompleted\nBoolean = true\n\n```", "```py\nscala> response.value\nOption[Try[String]] = Some(Success(<StockQuote><Status>SUCCESS</Status>\n...\n\n```", "```py\nscala> response.onComplete {\n case Success(s) => println(s)\n case Failure(e) => println(s\"Error fetching page: $e\")\n}\n\nscala> \n// Wait for response to complete, then prints:\n<StockQuote><Status>SUCCESS</Status><Name>Alphabet Inc</Name><Symbol>GOOGL</Symbol><LastPrice>695.22</LastPrice><Chan...\n\n```", "```py\nlibraryDependencies += \"org.scala-lang\" % \"scala-xml\" % \"2.11.0-M4\"\n```", "```py\nscala> import scala.xml.XML\nimport scala.xml.XML\n\n```", "```py\nscala> val strResponse = Future { \n Thread.sleep(20000) // Sleep for 20s\n val res = Source.fromURL(url).mkString\n println(\"finished fetching url\")\n res\n}\nstrResponse: Future[String] = Promise$DefaultPromise@1dda9bc8\n\nscala> val xmlResponse = strResponse.map { s =>\n println(\"applying string to xml transformation\")\n XML.loadString(s) \n}\nxmlResponse: Future[xml.Elem] = Promise$DefaultPromise@25d1262a\n\n// wait while the remainder of the 20s elapses\nfinished fetching url\napplying string to xml transformation\n\nscala> xmlResponse.value\nOption[Try[xml.Elem]] = Some(Success(<StockQuote><Status>SUCCESS</Status>...\n\n```", "```py\nscala> val strResponse = Future { \n Source.fromURL(\"empty\").mkString \n}\n\nscala> val xmlResponse = strResponse.map { \n s => XML.loadString(s) \n}\n\nscala> xmlResponse.value \nOption[Try[xml.Elem]] = Some(Failure(MalformedURLException: no protocol: empty))\n\n```", "```py\n// BlockDemo.scala\nimport scala.concurrent._\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.duration._\n\nobject BlockDemo extends App {\n  val f = Future { Thread.sleep(10000) }\n  f.onComplete { _ => println(\"future completed\") }\n  // \"future completed\" is not printed\n}\n```", "```py\nAwait.ready(f, 1 minute)\n```", "```py\nscala> import java.util.concurrent.Executors\nimport java.util.concurrent.Executors\n\nscala> val ec = ExecutionContext.fromExecutorService(\n Executors.newFixedThreadPool(16)\n)\nec: ExecutionContextExecutorService = ExecutionContextImpl$$anon$1@1351ce60\n\n```", "```py\nscala> val f = Future { Thread.sleep(1000) } (ec)\nf: Future[Unit] = Promise$DefaultPromise@458b456\n\n```", "```py\nscala> implicit val context = ec\ncontext: ExecutionContextExecutorService = ExecutionContextImpl$$anon$1@1351ce60\n\n```", "```py\nscala> val f = Future { Thread.sleep(1000) }\nf: Future[Unit] = Promise$DefaultPromise@3c4b7755\n\n```", "```py\nscala> ec.shutdown()\n\n```", "```py\n// StockPriceDemo.scala\n\nimport scala.concurrent._\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.io._\nimport scala.xml.XML\nimport scala.util._\n\nobject StockPriceDemo extends App {\n\n /* Construct URL for a stock symbol */\n  def urlFor(stockSymbol:String) =\n    (\"http://dev.markitondemand.com/MODApis/Api/v2/Quote?\" + \n     s\"symbol=${stockSymbol}\")\n\n  /* Build a future that fetches the stock price */\n  def fetchStockPrice(stockSymbol:String):Future[BigDecimal] = {\n    val url = urlFor(stockSymbol)\n    val strResponse = Future { Source.fromURL(url).mkString }\n    val xmlResponse = strResponse.map { s => XML.loadString(s) }\n    val price = xmlResponse.map { \n      r => BigDecimal((r \\ \"LastPrice\").text) \n    }\n    price\n  }\n\n  /* Command line interface */\n  println(\"Enter symbol at prompt.\")\n  while (true) {\n    val symbol = readLine(\"> \") // Wait for user input\n    // When user puts in symbol, fetch data in background\n    // thread and print to screen when complete\n    fetchStockPrice(symbol).onComplete { res =>\n      println()\n      res match {\n        case Success(price) => println(s\"$symbol: USD $price\")\n        case Failure(e) => println(s\"Error fetching  $symbol: $e\")\n      }\n      print(\"> \") // Simulate the appearance of a new prompt\n    }\n  }\n\n}\n```", "```py\n[info] Running StockPriceDemo\nEnter symbol at prompt:\n> GOOG\n> MSFT\n>\nGOOG: USD 695.22\n>\nMSFT: USD 47.48\n> AAPL\n> \nAAPL: USD 111.01\n\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies += \"mysql\" % \"mysql-connector-java\" % \"5.1.36\"\n```", "```py\n$ sbt console\n\n```", "```py\nscala> import java.sql._\nimport java.sql._\n\n```", "```py\nscala> Class.forName(\"com.mysql.jdbc.Driver\")\nClass[_] = class com.mysql.jdbc.Driver\n\n```", "```py\nscala> val connection = DriverManager.getConnection(\n \"jdbc:mysql://127.0.0.1:3306/test\",\n \"root\", // username when connecting\n \"\" // password\n)\njava.sql.Connection = com.mysql.jdbc.JDBC4Connection@12e78a69\n\n```", "```py\n$ mysql\n\n```", "```py\nmysql> USE test;\nmysql> CREATE TABLE physicists (\n id INT(11) AUTO_INCREMENT PRIMARY KEY,\n name VARCHAR(32) NOT NULL\n);\n\n```", "```py\nscala> val statementString = \"\"\"\nCREATE TABLE physicists (\n id INT(11) AUTO_INCREMENT PRIMARY KEY,\n name VARCHAR(32) NOT NULL\n)\n\"\"\"\n\nscala> val statement = connection.prepareStatement(statementString)\nPreparedStatement = JDBC4PreparedStatement@c983201: CREATE TABLE ...\n\nscala> statement.executeUpdate()\nresults: Int = 0\n\n```", "```py\nscala> val statement = connection.prepareStatement(\"\"\"\n INSERT INTO physicists (name) VALUES ('Isaac Newton')\n\"\"\")\n\nscala> statement.executeUpdate()\nInt = 1\n\n```", "```py\nmysql> select * from physicists ;\n+----+--------------+\n| id | name         |\n+----+--------------+\n|  1 | Isaac Newton |\n+----+--------------+\n1 row in set (0.00 sec)\n\n```", "```py\nval statement = connection.prepareStatement(\"SQL statement string\")\nstatement.executeUpdate()\n\n```", "```py\nscala> val physicistNames = List(\"Marie Curie\", \"Albert Einstein\", \"Paul Dirac\")\n\n```", "```py\nscala> val statement = connection.prepareStatement(\"\"\"\n INSERT INTO physicists (name) VALUES (?)\n\"\"\")\nPreparedStatement = JDBC4PreparedStatement@621a8225: INSERT INTO physicists (name) VALUES (** NOT SPECIFIED **)\n\n```", "```py\nscala> statement.setString(1, \"Richard Feynman\")\n\n```", "```py\nscala> statement\ncom.mysql.jdbc.JDBC4PreparedStatement@5fdd16c3:\nINSERT INTO physicists (name) VALUES ('Richard Feynman')\n\n```", "```py\nscala> statement.addBatch()\n\n```", "```py\nscala> physicistNames.foreach { name => \n statement.setString(1, name)\n statement.addBatch()\n}\n\n```", "```py\nscala> statement.executeBatch\nArray[Int] = Array(1, 1, 1, 1)\n\n```", "```py\nmysql> SELECT * FROM physicists;\n+----+-----------------+\n| id | name            |\n+----+-----------------+\n|  1 | Isaac Newton    |\n|  2 | Richard Feynman |\n|  3 | Marie Curie     |\n|  4 | Albert Einstein |\n|  5 | Paul Dirac      |\n+----+-----------------+\n5 rows in set (0.01 sec)\n\n```", "```py\nscala> val statement = connection.prepareStatement(\"\"\"\n SELECT name FROM physicists\n\"\"\")\nPreparedStatement = JDBC4PreparedStatement@3c577c9d:\nSELECT name FROM physicists\n\n```", "```py\nscala> val results = statement.executeQuery()\nresults: java.sql.ResultSet = com.mysql.jdbc.JDBC4ResultSet@74a2e158\n\n```", "```py\nscala> results.next // Advance to the first record\nBoolean = true\n\n```", "```py\nscala> results.getString(\"name\")\nString = Isaac Newton\n\n```", "```py\nscala> results.getString(1) // first positional argument\nString = Isaac Newton\n\n```", "```py\nscala> results.next // advances the ResultSet by one record\nBoolean = true\n\nscala> results.getString(\"name\")\nString = Richard Feynman\n\n```", "```py\nscala> while(results.next) { println(results.getString(\"name\")) }\nMarie Curie\nAlbert Einstein\nPaul Dirac\n\n```", "```py\nscala> rs.getInt(\"field\")\n0\nscala> rs.wasNull // was the last item read null?\ntrue\n\n```", "```py\nscala> results.close\n\nscala> statement.close\n\nscala> connection.close\n\n```", "```py\n    import java.sql._\n    Class.forName(\"com.mysql.jdbc.Driver\")val connection = DriverManager.getConnection(\n      \"jdbc:mysql://127.0.0.1:3306/test\",\n      \"root\", // username when connecting\n      \"\" // password\n    )\n    ```", "```py\n    connection.prepareStatement(\"SELECT * FROM physicists\")\n    ```", "```py\n// WARNING: poor Scala code\nval connection = DriverManager.getConnection(url, user, password)\ntry {\n  // do something with connection\n}\nfinally {\n  connection.close()\n}\n```", "```py\n// SqlUtils.scala\n\nimport java.sql._\n\nobject SqlUtils {\n\n  /** Create an auto-closing connection using \n    * the loan pattern */\n  def usingConnection[T](\n    db:String,\n    host:String=\"127.0.0.1\",\n    user:String=\"root\",\n    password:String=\"\",\n    port:Int=3306\n  )(f:Connection => T):T = {\n\n    // Create the connection\n    val Url = s\"jdbc:mysql://$host:$port/$db\"\n    Class.forName(\"com.mysql.jdbc.Driver\")\n    val connection = DriverManager.getConnection(\n      Url, user, password)\n\n    // give the connection to the client, through the callable \n    // `f` passed in as argument\n    try {\n      f(connection)\n    }\n    finally {\n      // When client is done, close the connection\n      connection.close()\n    }\n  }\n}\n```", "```py\nscala> SqlUtils.usingConnection(\"test\") {\n connection => println(connection)\n}\ncom.mysql.jdbc.JDBC4Connection@46fd3d66\n\n```", "```py\nopen resource (eg. database connection, file ...)\nuse resource somehow // loan resource to client for this part.\nclose resource\n```", "```py\n// WARNING: Poor Scala code\nSqlUtils.usingConnection(\"test\") { connection =>\n  val statement = connection.prepareStatement(\n    \"SELECT * FROM physicists\")\n  val results = statement.executeQuery\n  // do something useful with the results\n  results.close\n  statement.close\n}\n```", "```py\nusingConnection(\"test\") { connection =>\n  connection.withQuery(\"SELECT * FROM physicists\") {\n    resultSet => // process results\n  }\n}\n```", "```py\n// RichConnection.scala\n\nimport java.sql.{Connection, ResultSet}\n\nclass RichConnection(val underlying:Connection) {\n\n  /** Execute a SQL query and process the ResultSet */\n  def withQuery[T](query:String)(f:ResultSet => T):T = {\n    val statement = underlying.prepareStatement(query)\n    val results = statement.executeQuery\n    try {\n      f(results) // loan the ResultSet to the client\n    }\n    finally {\n      // Ensure all the resources get freed.\n      results.close\n      statement.close\n    }\n  }\n}\n```", "```py\n// Warning: poor Scala code\nSqlUtils.usingConnection(\"test\") { connection =>\n  val richConnection = new RichConnection(connection)\n  richConnection.withQuery(\"SELECT * FROM physicists\") {\n    resultSet => // process resultSet\n  }\n}\n```", "```py\n// Implicits.scala\nimport java.sql.Connection\n\n// Implicit conversion methods are often put in \n// an object called Implicits.\nobject Implicits {\n  implicit def pimpConnection(conn:Connection) = \n    new RichConnection(conn)\n  implicit def depimpConnection(conn:RichConnection) =  \n    conn.underlying\n}\n```", "```py\n// Bring the conversion functions into the current scope\nimport Implicits._ \n\nSqlUtils.usingConnection(\"test\") { (connection:Connection) =>\n  connection.withQuery(\"SELECT * FROM physicists\") {\n    // Wow! It's like we have just added \n    // .withQuery to the JDBC Connection class!\n    resultSet => // process results\n  }\n}\n```", "```py\n// WARNING: poor Scala code\nimport Implicits._ // import implicit conversions\n\nSqlUtils.usingConnection(\"test\") { connection =>\n  connection.withQuery(\"SELECT * FROM physicists\") { resultSet =>\n    var names = List.empty[String]\n    while(resultSet.next) {\n      val name = resultSet.getString(\"name\")\n      names = name :: names\n    }\n names\n  }\n}\n//=> List[String] = List(Paul Dirac, Albert Einstein, Marie Curie, Richard Feynman, Isaac Newton)\n```", "```py\n// SqlUtils.scala\nobject SqlUtils {   \n  ... \n  def stream(results:ResultSet):Stream[ResultSet] = \n    if (results.next) { results #:: stream(results) }\n    else { Stream.empty[ResultSet] }\n}\n```", "```py\nimport Implicits._\n\nSqlUtils.usingConnection(\"test\") { connection =>\n  connection.withQuery(\"SELECT * FROM physicists\") { results =>\n    val resultsStream = SqlUtils.stream(results)\n    resultsStream.map { _.getString(\"name\") }.toVector\n  }\n}\n//=> Vector(Richard Feynman, Albert Einstein, Marie Curie, Paul Dirac)\n```", "```py\nmysql> CREATE TABLE physicists (\n id INT(11) AUTO_INCREMENT PRIMARY KEY,\n name VARCHAR(32) NOT NULL,\n gender ENUM(\"Female\", \"Male\") NOT NULL\n);\n\n```", "```py\n// Gender.scala\n\nobject Gender extends Enumeration {\n  val Male = Value\n  val Female = Value\n}\n```", "```py\nresultsStream.map { \n  rs => Gender.withName(rs.getString(\"gender\")) \n}.toVector\n```", "```py\n// results is a ResultSet instance\nval name = results.read[String](\"name\")\nval gender = results.read[Gender.Value](\"gender\")\n```", "```py\n// SqlReader.scala\n\nimport java.sql._\n\ntrait SqlReader[T] {\n  def read(results:ResultSet, field:String):T\n}\n```", "```py\n// SqlReader.scala\n\nobject SqlReader {\n  implicit object StringReader extends SqlReader[String] {\n    def read(results:ResultSet, field:String):String =\n      results.getString(field)\n  }\n\n  implicit object GenderReader extends SqlReader[Gender.Value] {\n    def read(results:ResultSet, field:String):Gender.Value =\n      Gender.withName(StringReader.read(results, field))\n  }\n}\n```", "```py\nimport SqlReader._\nval name = StringReader.read(results, \"name\")\nval gender = GenderReader.read(results, \"gender\")\n```", "```py\nGender.withName(results.getString(\"gender\"))\n```", "```py\nimplicitly[SqlReader[Gender.Value]].read(results, \"gender\")\nimplicitly[SqlReader[String]].read(results, \"name\")\n```", "```py\nscala> :paste\n\nimport Implicits._ // Connection to RichConnection conversion\nSqlUtils.usingConnection(\"test\") {\n _.withQuery(\"select * from physicists\") {\n rs => {\n rs.next() // advance to first record\n implicitly[SqlReader[Gender.Value]].read(rs, \"gender\")\n }\n }\n}\n\n```", "```py\n// RichResultSet.scala\n\nimport java.sql.ResultSet\n\nclass RichResultSet(val underlying:ResultSet) {\n  def read[T : SqlReader](field:String):T = {\n    implicitly[SqlReader[T]].read(underlying, field)\n  }\n}\n```", "```py\nimport Implicits._\n\nSqlUtils.usingConnection(\"test\") { connection =>\n  connection.withQuery(\"SELECT * FROM physicists\") {\n    results =>\n      val resultStream = SqlUtils.stream(results)\n      resultStream.map { row => \n val name = row.read[String](\"name\")\n val gender = row.read[Gender.Value](\"gender\")\n        (name, gender)\n      }.toVector\n  }\n}\n//=> Vector[(String, Gender.Value)] = Vector((Albert Einstein,Male), (Marie Curie,Female))\n```", "```py\n    implicit object StringReader extends SqlReader[T].\n    ```", "```py\ndef getColumn[T : SqlReader](field:String):Vector[T] = {\n  val resultStream = SqlUtils.stream(results)\n  resultStream.map { _.read[T](field) }.toVector\n}\n```", "```py\n// Physicist.scala\ncase class Physicist(\n  val name:String,\n  val gender:Gender.Value\n)\n```", "```py\n// PhysicistDao.scala\n\nimport java.sql.{ ResultSet, Connection }\nimport Implicits._ // implicit conversions\n\nobject PhysicistDao {\n\n  /* Helper method for reading a single row */\n  private def readFromResultSet(results:ResultSet):Physicist = {\n    Physicist(\n      results.read[String](\"name\"),\n      results.read[Gender.Value](\"gender\")\n    )\n  }\n\n  /* Read the entire 'physicists' table. */\n  def readAll(connection:Connection):Vector[Physicist] = {\n    connection.withQuery(\"SELECT * FROM physicists\") {\n      results =>\n        val resultStream = SqlUtils.stream(results)\n        resultStream.map(readFromResultSet).toVector\n    }\n  }\n}\n```", "```py\nobject PhysicistDaoDemo extends App {\n\n  val physicists = SqlUtils.usingConnection(\"test\") {\n    connection => PhysicistDao.readAll(connection)\n  }\n\n  // physicists is a Vector[Physicist] instance.\n  physicists.foreach { println }\n  //=> Physicist(Albert Einstein,Male)\n  //=> Physicist(Marie Curie,Female)\n}\n```", "```py\n// Transaction.scala\nimport java.sql.Date\n\ncase class Transaction(\n  id:Option[Int], // unique identifier\n  candidate:String, // candidate receiving the donation\n  contributor:String, // name of the contributor\n  contributorState:String, // contributor state\n  contributorOccupation:Option[String], // contributor job\n  amount:Long, // amount in cents\n  date:Date // date of the donation\n)\n```", "```py\nscala> val ohioData = FECData.loadOhio\ns4ds.FECData = s4ds.FECData@718454de\n\n```", "```py\nscala> val ohioTransactions = ohioData.transactions\nIterator[Transaction] = non-empty iterator\n\nscala> ohioTransactions.take(5).foreach(println)\nTransaction(None,Paul, Ron,BROWN, TODD W MR.,OH,Some(ENGINEER),5000,2011-01-03)\nTransaction(None,Paul, Ron,DIEHL, MARGO SONJA,OH,Some(RETIRED),2500,2011-01-03)\nTransaction(None,Paul, Ron,KIRCHMEYER, BENJAMIN,OH,Some(COMPUTER PROGRAMMER),20120,2011-01-03)\nTransaction(None,Obama, Barack,KEYES, STEPHEN,OH,Some(HR EXECUTIVE / ATTORNEY),10000,2011-01-03)\nTransaction(None,Obama, Barack,MURPHY, MIKE W,OH,Some(MANAGER),5000,2011-01-03)\n\n```", "```py\nscala> import slick.driver.MySQLDriver.simple._\nimport slick.driver.MySQLDriver.simple._\n\n```", "```py\nCREATE TABLE transactions(\n    id INT(11) AUTO_INCREMENT PRIMARY KEY,\n    candidate VARCHAR(254) NOT NULL,\n    contributor VARCHAR(254) NOT NULL,\n    contributor_state VARCHAR(2) NOT NULL,\n    contributor_occupation VARCHAR(254),\n    amount BIGINT(20) NOT NULL,\n    date DATE \n);\n```", "```py\nscala> 0.1 + 0.2\nDouble = 0.30000000000000004\n\n```", "```py\n// Tables.scala\n\nimport java.sql.Date\nimport slick.driver.MySQLDriver.simple._\n\n/** Singleton object for table definitions */\nobject Tables {\n\n  // Transactions table definition\n  class Transactions(tag:Tag)\n  extends Table[Transaction](tag, \"transactions\") {\n    def id = column[Int](\"id\", O.PrimaryKey, O.AutoInc)\n    def candidate = column[String](\"candidate\")\n    def contributor = column[String](\"contributor\")\n    def contributorState = column[String](\n      \"contributor_state\", O.DBType(\"VARCHAR(2)\"))\n    def contributorOccupation = column[Option[String]](\n      \"contributor_occupation\")\n    def amount = column[Long](\"amount\")\n    def date = column[Date](\"date\")\n\n    def * = (id.?, candidate, contributor, \n      contributorState, contributorOccupation, amount, date) <> (\n      Transaction.tupled, Transaction.unapply)\n  }\n\n  val transactions = TableQuery[Transactions]\n\n}\n```", "```py\ndef id = column[Int](\"id\", O.PrimaryKey, O.AutoInc)\n```", "```py\ndef contributorOccupation = column[Option[String]](\"contributor_occupation\")\n```", "```py\ndef * = (id.?, candidate, contributor, \ncontributorState, contributorOccupation, amount, date) <> (\nTransaction.tupled, Transaction.unapply)\n```", "```py\nval transactions = TableQuery[Transactions]\n```", "```py\nTables.transactions.filter {_.candidate === \"Obama, Barack\"}.list\n```", "```py\nscala> import slick.driver.MySQLDriver.simple._\nimport slick.driver.MySQLDriver.simple._\n\nscala> val db = Database.forURL(\n \"jdbc:mysql://127.0.0.1:3306/test\",\n driver=\"com.mysql.jdbc.Driver\"\n)\ndb: slick.driver.MySQLDriver.backend.DatabaseDef = slick.jdbc.JdbcBackend$DatabaseDef@3632d1dd\n\n```", "```py\nscala> db.withSession { implicit session =>\n // do something useful with the database\n println(session)\n}\nscala.slick.jdbc.JdbcBackend$BaseSession@af5a276\n\n```", "```py\nscala> implicit val session = db.createSession\nsession: slick.driver.MySQLDriver.backend.Session = scala.slick.jdbc.JdbcBackend$BaseSession@2b775b49\n\nscala> session.close\n\n```", "```py\nscala> db.withSession { implicit session =>\n Tables.transactions.ddl.create\n}\n\n```", "```py\nmysql> describe transactions ;\n+------------------------+--------------+------+-----+\n| Field                  | Type         | Null | Key |\n+------------------------+--------------+------+-----+\n| id                     | int(11)      | NO   | PRI |\n| candidate              | varchar(254) | NO   |     |\n| contributor            | varchar(254) | NO   |     |\n| contributor_state      | varchar(2)   | NO   |     |\n| contributor_occupation | varchar(254) | YES  |     |\n| amount                 | bigint(20)   | NO   |     |\n| date                   | date         | NO   |     |\n+------------------------+--------------+------+-----+\n7 rows in set (0.01 sec)\n\n```", "```py\nscala> import java.text.SimpleDateFormat\nimport java.text.SimpleDateFormat\n\nscala> val date = new SimpleDateFormat(\"dd-MM-yyyy\").parse(\"22-06-2010\")\ndate: java.util.Date = Tue Jun 22 00:00:00 BST 2010\n\nscala> val sqlDate = new java.sql.Date(date.getTime())\nsqlDate: java.sql.Date = 2010-06-22\n\nscala> val transaction = Transaction(\n None, \"Obama, Barack\", \"Doe, John\", \"TX\", None, 200, sqlDate\n)\ntransaction: Transaction = Transaction(None,Obama, Barack,Doe, John,TX,None,200,2010-06-22)\n\n```", "```py\nscala> db.withSession {\n implicit session => Tables.transactions += transaction\n}\nInt = 1\n\n```", "```py\nscala> val transactions = FECData.loadOhio.transactions\ntransactions: Iterator[Transaction] = non-empty iterator\n\n```", "```py\nscala> val transactions = FECData.loadAll.transactions\ntransactions: Iterator[Transaction] = non-empty iterator\n\n```", "```py\nscala> val batchSize = 100000\nbatchSize: Int = 100000\n\nscala> val transactionBatches = transactions.grouped(batchSize)\ntransactionBatches: transactions.GroupedIterator[Transaction] = non-empty iterator\n\n```", "```py\nscala> db.withSession { implicit session =>\n transactionBatches.foreach { \n batch => Tables.transactions ++= batch.toList\n }\n}\n\n```", "```py\nscala> db.withSession { implicit session =>\n transactionBatches.zipWithIndex.foreach { \n case (batch, batchNumber) =>\n println(s\"Processing row ${batchNumber*batchSize}\")\n Tables.transactions ++= batch.toList\n }\n}\nProcessing row 0\nProcessing row 100000\n...\n\n```", "```py\nscala> db.withSession { implicit session =>\n Tables.transactions.take(5).list\n}\nList[Tables.Transactions#TableElementType] = List(Transaction(Some(1),Obama, Barack,Doe, ...\n\n```", "```py\nscala> db.withSession { implicit session =>\n println(Tables.transactions.take(5).selectStatement)\n}\nselect x2.`id`, x2.`candidate`, x2.`contributor`, x2.`contributor_state`, x2.`contributor_occupation`, x2.`amount`, x2.`date` from (select x3.`date` as `date`, x3.`contributor` as `contributor`, x3.`amount` as `amount`, x3.`id` as `id`, x3.`candidate` as `candidate`, x3.`contributor_state` as `contributor_state`, x3.`contributor_occupation` as `contributor_occupation` from `transactions` x3 limit 5) x2\n\n```", "```py\n    scala> db.withSession { implicit session =>\n     Tables.transactions.map {\n     _.candidate \n     }.take(5).list \n    }\n    List[String] = List(Obama, Barack, Paul, Ron, Paul, Ron, Paul, Ron, Obama, Barack)\n\n    ```", "```py\n    scala> db.withSession { implicit session => \n     Tables.transactions.filter {\n     _.candidate === \"Obama, Barack\"\n     }.take(5).list\n    }\n    List[Tables.Transactions#TableElementType] = List(Transaction(Some(1),Obama, Barack,Doe, John,TX,None,200,2010-06-22), ...\n\n    ```", "```py\n    scala> db.withSession { implicit session => \n     Tables.transactions.filter { \n     _.candidate =!= \"Obama, Barack\"\n     }.take(5).list\n    }\n    List[Tables.Transactions#TableElementType] = List(Transaction(Some(2),Paul, Ron,BROWN, TODD W MR.,OH,...\n\n    ```", "```py\n    scala> db.withSession { implicit session => \n     Tables.transactions.sortBy { \n     _.date.desc \n     }.take(5).list\n    }\n    List[Tables.Transactions#TableElementType] = List(Transaction(Some(65536),Obama, Barack,COPELAND, THOMAS,OH,Some(COLLEGE TEACHING),10000,2012-01-02)\n\n    ```", "```py\n    scala> db.withSession { implicit session => \n     Tables.transactions.filter {\n     _.candidate === \"Obama, Barack\"\n     }.map { _.amount  }.sum.run\n    }\n    Option[Int] = Some(849636799) // (in cents)\n\n    ```", "```py\nscala> db.withSession { implicit session =>\n Tables.transactions.filter { \n _.candidate.startsWith(\"O\") \n }.take(5).list \n}\nList[Tables.Transactions#TableElementType] = List(Transaction(Some(1594098)...\n\n```", "```py\nscala> val dateParser = new SimpleDateFormat(\"dd-MM-yyyy\")\ndateParser: java.text.SimpleDateFormat = SimpleDateFormat\n\nscala> val startDate = new java.sql.Date(dateParser.parse(\"01-01-2011\").getTime())\nstartDate: java.sql.Date = 2011-01-01\n\nscala> val endDate = new java.sql.Date(dateParser.parse(\"01-02-2011\").getTime())\nendDate: java.sql.Date = 2011-02-01\n\nscala> db.withSession { implicit session =>\n Tables.transactions.filter { \n _.date.between(startDate, endDate)\n }.length.run \n}\nInt = 9772\n\n```", "```py\nscala> val candidateList = List(\"Obama, Barack\", \"Romney, Mitt\")\ncandidateList: List[String] = List(Obama, Barack, Romney, Mitt)\n\nscala> val donationCents = db.withSession { implicit session =>\n Tables.transactions.filter {\n _.candidate.inSet(candidateList)\n }.map { _.amount }.sum.run\n}\ndonationCents: Option[Long] = Some(2874484657)\n\nscala> val donationDollars = donationCents.map { _ / 100 }\ndonationDollars: Option[Long] = Some(28744846)\n\n```", "```py\nscala> db.withSession { implicit session =>\n Tables.transactions.filter { \n ! _.candidate.inSet(candidateList) \n }.map { _.amount }.sum.run\n}.map { _ / 100 }\nOption[Long] = Some(1930747)\n\n```", "```py\nscala> val grouped = Tables.transactions.groupBy { _.candidate }\ngrouped: scala.slick.lifted.Query[(scala.slick.lifted.Column[...\n\nscala> val aggregated = grouped.map {\n case (candidate, group) =>\n (candidate -> group.map { _.amount }.sum)\n}\naggregated: scala.slick.lifted.Query[(scala.slick.lifted.Column[...\n\nscala> val groupedDonations = db.withSession { \n implicit session => aggregated.list \n}\ngroupedDonations: List[(String, Option[Long])] = List((Bachmann, Michele,Some(7439272)),...\n\n```", "```py\nscala> val groupedDonationDollars = groupedDonations.map {\n case (candidate, donationCentsOption) =>\n candidate -> (donationCentsOption.getOrElse(0L) / 100)\n}\ngroupedDonationDollars: List[(String, Long)] = List((Bachmann, Michele,74392),...\n\nscala> groupedDonationDollars.sortBy { \n _._2 \n}.reverse.foreach { println }\n(Romney, Mitt,20248496)\n(Obama, Barack,8496347)\n(Paul, Ron,565060)\n(Santorum, Rick,334926)\n(Perry, Rick,301780)\n(Gingrich, Newt,277079)\n(Cain, Herman,210768)\n(Johnson, Gary Earl,83610)\n(Bachmann, Michele,74392)\n(Pawlenty, Timothy,42500)\n(Huntsman, Jon,23571)\n(Roemer, Charles E. 'Buddy' III,8579)\n(Stein, Jill,5270)\n(McCotter, Thaddeus G,3210)\n\n```", "```py\nscala> import slick.jdbc.meta.MTable\nimport slick.jdbc.meta.MTable\n\nscala> db.withSession { implicit session =>\n MTable.getTables(\"transactions\").list\n}\nList[scala.slick.jdbc.meta.MTable] = List(MTable(MQName(fec.transactions),TABLE,,None,None,None) ...)\n\n```", "```py\nscala> db.withSession { implicit session =>\n if(MTable.getTables(\"transactions\").list.nonEmpty) {\n Tables.transactions.ddl.drop\n }\n}\n\n```", "```py\nscala> db.withSession { implicit session =>\n val tableMeta = MTable.getTables(\"transactions\").first\n tableMeta.getPrimaryKeys.list\n}\nList[MPrimaryKey] = List(MPrimaryKey(MQName(test.transactions),id,1,Some(PRIMARY)))\n\n```", "```py\n{\n  \"login\": \"odersky\",\n  \"id\": 795990,\n  ...\n  \"public_repos\": 8,\n  \"public_gists\": 3,\n  \"followers\": 707,\n  \"following\": 0,\n  \"created_at\": \"2011-05-18T14:51:21Z\",\n  \"updated_at\": \"2015-09-15T15:14:33Z\"\n}\n```", "```py\n[\n  {\n    \"id\": 17335228,\n    \"name\": \"dotty\",\n    \"full_name\": \"odersky/dotty\",\n    ...\n  },\n  {\n    \"id\": 15053153,\n    \"name\": \"frontend\",\n    \"full_name\": \"odersky/frontend\",\n    ...\n  },\n  ...\n]\n```", "```py\nscala> import scala.io._\nimport scala.io._\n\nscala> val response = Source.fromURL(\n \"https://api.github.com/users/odersky\"\n).mkString\nresponse: String = {\"login\":\"odersky\",\"id\":795990, ...\n\n```", "```py\n// build.sbt\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies += \"org.json4s\" %% \"json4s-native\" % \"3.2.11\"\n```", "```py\nscala> import org.json4s._\nimport org.json4s._\n\nscala> import org.json4s.native.JsonMethods._\nimport org.json4s.native.JsonMethods._\n\n```", "```py\nscala> val jsonResponse = parse(response)\njsonResponse: org.json4s.JValue = JObject(List((login,JString(odersky)),(id,JInt(795990)),...\n\n```", "```py\nscala> val JObject(fields) = jsonResponse\nfields: List[JField] = List((login,Jstring(odersky)),...\n\n```", "```py\nval JObject(fields) = ...\n```", "```py\ncase class JObject(obj:List[JField])\n```", "```py\nscala> val firstField = fields.head\nfirstField: JField = (login,JString(odersky))\n\nscala> val JField(key, JString(value)) = firstField\nkey: String = login\nvalue: String = odersky\n\n```", "```py\nscala> val JField(key, JInt(value)) = firstField\nscala.MatchError: (login,JString(odersky)) (of class scala.Tuple2)\n...\n\n```", "```py\nscala> val JField(\"login\", JString(loginName)) = firstField\nloginName: String = odersky\n\n```", "```py\nscala> for {\n JField(key, JString(value)) <- fields\n} yield (key -> value)\nList[(String, String)] = List((login,odersky), (avatar_url,https://avatars.githubusercontent.com/...\n\n```", "```py\nscala> val followersList = for {\n JField(\"followers\", JInt(followers)) <- fields\n} yield followers\nfollowersList: List[Int] = List(707)\n\nscala> val followers = followersList.headOption\nblogURL: Option[Int] = Some(707)\n\n```", "```py\nscala> {\n for {\n JField(\"login\", JString(loginName)) <- fields\n JField(\"id\", JInt(id)) <- fields\n } yield (id -> loginName)\n}.headOption \nOption[(BigInt, String)] = Some((795990,odersky))\n\n```", "```py\nval JField(key, JInt(value)) = ...\n```", "```py\nscala> jsonResponse \\ \"login\"\norg.json4s.JValue = JString(odersky)\n\n```", "```py\nscala> val JString(loginName) = jsonResponse \\ \"login\"\nloginName: String = odersky\n\n```", "```py\n{\n  \"id\": 42269470,\n  \"name\": \"s4ds\",\n  ...\n  \"owner\": { \"login\": \"pbugnion\", \"id\": 1392879 ... }\n  ...\n}\n```", "```py\nscala> val jsonResponse = parse(Source.fromURL(\n \"https://api.github.com/repos/pbugnion/s4ds\"\n).mkString)\njsonResponse: JValue = JObject(List((id,JInt(42269470)), (name,JString(s4ds))...\n\nscala> val JString(ownerLogin) = jsonResponse \\ \"owner\" \\ \"login\"\nownerLogin: String = pbugnion\n\n```", "```py\n[\n  {\n    \"id\": 17335228,\n    \"name\": \"dotty\",\n    \"size\": 14699,\n    ...\n  },\n  {\n    \"id\": 15053153,\n    \"name\": \"frontend\",\n    \"size\": 392\n    ...\n  },\n  {\n    \"id\": 2890092,\n    \"name\": \"scala\",\n    \"size\": 76133,\n    ...\n  },\n  ...\n]\n```", "```py\nscala> val jsonResponse = parse(Source.fromURL(\n \"https://api.github.com/users/odersky/repos\"\n).mkString)\njsonResponse: JValue = JArray(List(JObject(List((id,JInt(17335228)), (name,Jstring(dotty)), ...\n\n```", "```py\nscala> jsonResponse \\ \"size\"\nJValue = JArray(List(JInt(14699), JInt(392), ...\n\n```", "```py\nscala> for {\n JInt(size) <- (jsonResponse \\ \"size\")\n} yield size\nList[BigInt] = List(14699, 392, 76133, 32010, 98166, 1358, 144, 273)\n\n```", "```py\nscala> case class User(id:Long, login:String)\ndefined class User\n\n```", "```py\nscala> implicit val formats = DefaultFormats\nformats: DefaultFormats.type = DefaultFormats$@750e685a\n\n```", "```py\nscala> val url = \"https://api.github.com/users/odersky\"\nurl: String = https://api.github.com/users/odersky\n\nscala> val jsonResponse = parse(Source.fromURL(url).mkString)\njsonResponse: JValue = JObject(List((login,JString(odersky)), ...\n\nscala> jsonResponse.extract[User]\nUser = User(795990,odersky)\n\n```", "```py\nscala> case class User(id:Long, userName:String)\ndefined class User\n\n```", "```py\nscala> jsonResponse.transformField { \n case(\"login\", n) => \"userName\" -> n \n}.extract[User]\nUser = User(795990,odersky)\n\n```", "```py\nscala> case class User(id:Long, login:Option[String])\ndefined class User\n\n```", "```py\nscala> jsonResponse.extract[User]\nUser = User(795990,Some(odersky))\n\nscala> jsonResponse.removeField { \n case(k, _) => k == \"login\" // remove the \"login\" field\n}.extract[User]\nUser = User(795990,None)\n\n```", "```py\n// GitHubUser.scala\n\nimport scala.io._\nimport org.json4s._\nimport org.json4s.native.JsonMethods._\n\nobject GitHubUser {\n\n  implicit val formats = DefaultFormats\n\n  case class User(id:Long, userName:String)\n\n  /** Query the GitHub API corresponding to `url` \n    * and convert the response to a User.\n    */\n  def fetchUserFromUrl(url:String):User = {\n    val response = Source.fromURL(url).mkString\n    val jsonResponse = parse(response)\n    extractUser(jsonResponse)\n  }\n\n  /** Helper method for transforming the response to a User */\n  def extractUser(obj:JValue):User = {\n    val transformedObject = obj.transformField {\n      case (\"login\", name) => (\"userName\", name)\n    }\n    transformedObject.extract[User]\n  }\n\n  def main(args:Array[String]) {\n    // Extract username from argument list\n    val name = args.headOption.getOrElse { \n      throw new IllegalArgumentException(\n        \"Missing command line argument for user.\")\n    }\n\n    val user = fetchUserFromUrl(\n      s\"https://api.github.com/users/$name\")\n\n    println(s\"** Extracted for $name:\")\n    println()\n    println(user)\n\n  }\n\n}\n```", "```py\n$ sbt\n> runMain GitHubUser pbugnion\n** Extracted for pbugnion:\nUser(1392879,pbugnion)\n\n```", "```py\n// GitHubUserConcurrent.scala\n\nimport scala.io._\nimport scala.concurrent._\nimport scala.concurrent.duration._\nimport ExecutionContext.Implicits.global\nimport scala.util._\n\nimport org.json4s._\nimport org.json4s.native.JsonMethods._\n\nobject GitHubUserConcurrent {\n\n  implicit val formats = DefaultFormats\n\n  case class User(id:Long, userName:String)\n\n  // Fetch and extract the `User` corresponding to `url`\n  def fetchUserFromUrl(url:String):Future[User] = {\n    val response = Future { Source.fromURL(url).mkString }\n    val parsedResponse = response.map { r => parse(r) }\n    parsedResponse.map { extractUser }\n  }\n\n  // Helper method for extracting a user from a JObject\n  def extractUser(jsonResponse:JValue):User = {\n    val o = jsonResponse.transformField {\n      case (\"login\", name) => (\"userName\", name)\n    }\n    o.extract[User]\n  }\n\n  def main(args:Array[String]) {\n    val names = args.toList\n\n    // Loop over each username and send a request to the API \n    // for that user \n    val name2User = for {\n      name <- names\n      url = s\"https://api.github.com/users/$name\"\n      user = fetchUserFromUrl(url)\n    } yield name -> user\n\n    // callback function\n    name2User.foreach { case(name, user) =>\n      user.onComplete {\n        case Success(u) => println(s\" ** Extracted for $name: $u\")\n        case Failure(e) => println(s\" ** Error fetching $name:$e\")\n      }\n    }\n\n    // Block until all the calls have finished.\n    Await.ready(Future.sequence(name2User.map { _._2 }), 1 minute)\n  }\n}\n```", "```py\n$ sbt\n> runMain GitHubUserConcurrent odersky derekwyatt not-a-user-675\n ** Error fetching user not-a-user-675: java.io.FileNotFoundException: https://api.github.com/users/not-a-user-675\n ** Extracted for odersky: User(795990,odersky)\n ** Extracted for derekwyatt: User(62324,derekwyatt)\n\n```", "```py\nAwait.ready(Future.sequence(name2User.map { _._2 }), 1 minute)\n```", "```py\nlibraryDependencies += \"org.scalaj\" %% \"scalaj-http\" % \"1.1.6\"\n```", "```py\nscala> import scalaj.http._\nimport scalaj.http._\n\n```", "```py\nscala> val request = Http(\"https://api.github.com/users/pbugnion\")\nrequest:scalaj.http.HttpRequest = HttpRequest(api.github.com/users/pbugnion,GET,...\n\n```", "```py\nscala> val authorizedRequest = request.header(\"Authorization\", \"token e836389ce ...\")\nauthorizedRequest:scalaj.http.HttpRequest = HttpRequest(api.github.com/users/pbugnion,GET,...\n\n```", "```py\nscala> val response = authorizedRequest.asString\nresponse:scalaj.http.HttpResponse[String] = HttpResponse({\"login\":\"pbugnion\",...\n\n```", "```py\n    scala> response.code \n    Int = 200\n\n    ```", "```py\n    scala> response.body \n    String = {\"login\":\"pbugnion\",\"id\":1392879,...\n\n    ```", "```py\n    scala> response.headers \n    Map[String,String] = Map(Access-Control-Allow-Credentials -> true, ...\n\n    ```", "```py\nscala> response.headers(\"X-RateLimit-Limit\")\nString = 5000\n\n```", "```py\nimport scalaj.http._\n```", "```py\n$ env\nHOME=/Users/pascal\nSHELL=/bin/zsh\n...\n\n```", "```py\n$ export GHTOKEN=\"e83638...\" # enter your token here\n\n```", "```py\n$ SET GHTOKEN=\"e83638...\"\n\n```", "```py\nlazy val token:Option[String] = sys.env.get(\"GHTOKEN\") orElse {\n  println(\"No token found: continuing without authentication\")\n  None\n}\n```", "```py\ndef fetchUserFromUrl(url:String):Future[User] = {\n  val baseRequest = Http(url)\n  val request = token match {\n    case Some(t) => baseRequest.header(\n      \"Authorization\", s\"token $t\")\n    case None => baseRequest\n  }\n  val response = Future { \n    request.asString.body \n  }\n  val parsedResponse = response.map { r => parse(r) }\n  parsedResponse.map(extractUser)\n}\n```", "```py\n{\n    _id: ObjectId(\"558e846730044ede70743be9\"),\n    name: \"Gandalf\",\n    age: 2000,\n    pseudonyms: [ \"Mithrandir\", \"Olorin\", \"Greyhame\" ],\n    possessions: [ \n        { name: \"Glamdring\", type: \"sword\" }, \n        { name: \"Narya\", type: \"ring\" }\n    ]\n}\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies += \"org.mongodb\" %% \"casbah\" % \"3.0.0\"\n```", "```py\nlibraryDependencies += \"org.slf4j\" % \"slf4j-nop\" % \"1.7.12\"\n```", "```py\n$ sbt console\nscala> import com.mongodb.casbah.Imports._\nimport com.mongodb.casbah.Imports._\n\nscala> val client = MongoClient()\nclient: com.mongodb.casbah.MongoClient = com.mongodb.casbah.MongoClient@4ac17318\n\n```", "```py\nscala> val client = MongoClient(\"192.168.1.1\", 27017)\nclient: com.mongodb.casbah.MongoClient = com.mongodb.casbah.MongoClient@584c6b02\n\n```", "```py\nscala> val db = client(\"github\")\ndb: com.mongodb.casbah.MongoDB = DB{name='github'}\n\n```", "```py\nscala> val coll = db(\"users\")\ncoll: com.mongodb.casbah.MongoCollection = users\n\n```", "```py\nscala> val username = \"USER\"\nusername: String = USER\n\nscala> val password = \"PASSWORD\"\npassword: String = PASSWORD\n\nscala> val uri = MongoClientURI(\n s\"mongodb://$username:$password@localhost/?authMechanism=SCRAM-SHA-1\"\n)\nuri: MongoClientURI = mongodb://USER:PASSWORD@localhost/?authMechanism=SCRAM-SHA-1\n\nscala> val mongoClient = MongoClient(uri)\nclient: com.mongodb.casbah.MongoClient = com.mongodb.casbah.MongoClient@4ac17318\n\n```", "```py\n// Credentials.scala\n\nimport com.mongodb.casbah.Imports._\n\nobject Credentials extends App {\n\n  val username = sys.env.getOrElse(\"MONGOUSER\",\n    throw new IllegalStateException(\n      \"Need a MONGOUSER variable in the environment\")\n  )\n  val password = sys.env.getOrElse(\"MONGOPASSWORD\",\n    throw new IllegalStateException(\n      \"Need a MONGOPASSWORD variable in the environment\")\n  )\n\n  val host = \"127.0.0.1\"\n  val port = 27017\n\n  val uri = s\"mongodb://$username:$password@$host:$port/?authMechanism=SCRAM-SHA-1\"\n\n  val client = MongoClient(MongoClientURI(uri))\n}\n```", "```py\n$ MONGOUSER=\"pascal\" MONGOPASSWORD=\"scalarulez\" sbt\n> runMain Credentials\n\n```", "```py\n{\n    id: <mongodb object id>,\n    login: \"pbugnion\",\n    github_id: 1392879,\n    repos: [ \n        {\n            name: \"scikit-monaco\",\n            id: 14821551,\n            language: \"Python\"\n        },\n        {\n            name: \"contactpp\",\n            id: 20448325,\n            language: \"Python\"\n        }\n    ]\n}\n```", "```py\nscala> val repo1 = DBObject(\"name\" -> \"scikit-monaco\", \"id\" -> 14821551, \"language\" -> \"Python\")\nrepo1: DBObject = { \"name\" : \"scikit-monaco\" , \"id\" : 14821551, \"language\" : \"Python\"}\n\n```", "```py\nscala> val fields:Map[String, Any] = Map(\n \"name\" -> \"contactpp\",\n \"id\" -> 20448325,\n \"language\" -> \"Python\"\n)\nMap[String, Any] = Map(name -> contactpp, id -> 20448325, language -> Python)\n\nscala> val repo2 = DBObject(fields.toList)\nrepo2: dDBObject = { \"name\" : \"contactpp\" , \"id\" : 20448325, \"language\" : \"Python\"}\n\n```", "```py\nscala> repo1(\"name\")\nAnyRef = scikit-monaco\n\n```", "```py\nscala> repo1 + (\"fork\" -> true)\nmutable.Map[String,Any] = { \"name\" : \"scikit-monaco\" , \"id\" : 14821551, \"language\" : \"python\", \"fork\" : true}\n\n```", "```py\nscala> repo1 ++ DBObject(\n \"locs\" -> 6342, \n \"description\" -> \"Python library for Monte Carlo integration\"\n)\nDBObject = { \"name\" : \"scikit-monaco\" , \"id\" : 14821551, \"language\" : \"Python\", \"locs\" : 6342 , \"description\" : \"Python library for Monte Carlo integration\"}\n\n```", "```py\nscala> val userDocument = DBObject(\n \"login\" -> \"pbugnion\", \n \"github_id\" -> 1392879, \n \"repos\" -> List(repo1, repo2)\n)\nuserDocument: DBObject = { \"login\" : \"pbugnion\" , ... }\n\nscala> val coll = MongoClient()(\"github\")(\"users\")\ncoll: com.mongodb.casbah.MongoCollection = users\n\nscala> coll += userDocument\ncom.mongodb.casbah.TypeImports.WriteResult = WriteResult{, n=0, updateOfExisting=false, upsertedId=null}\n\n```", "```py\nscala> val it = new GitHubUserIterator\nit: GitHubUserIterator = non-empty iterator\n\nscala> it.next // Fetch the first user\nUser = User(mojombo,1,List(Repo(...\n\n```", "```py\n// User.scala\ncase class User(login:String, id:Long, repos:List[Repo])\n\n// Repo.scala\ncase class Repo(name:String, id:Long, language:String)\n```", "```py\n// InsertUsers.scala\n\nimport com.mongodb.casbah.Imports._\n\nobject InsertUsers {\n\n  /** Function for reading GitHub token from environment. */\n  lazy val token:Option[String] = sys.env.get(\"GHTOKEN\") orElse {\n    println(\"No token found: continuing without authentication\")\n    None\n  }\n\n  /** Transform a Repo instance to a DBObject */\n  def repoToDBObject(repo:Repo):DBObject = DBObject(\n    \"github_id\" -> repo.id,\n    \"name\" -> repo.name,\n    \"language\" -> repo.language\n  )\n\n  /** Transform a User instance to a DBObject */\n  def userToDBObject(user:User):DBObject = DBObject(\n    \"github_id\" -> user.id,\n    \"login\" -> user.login,\n    \"repos\" -> user.repos.map(repoToDBObject)\n  )\n\n  /** Insert a list of users into a collection. */\n  def insertUsers(coll:MongoCollection)(users:Iterable[User]) {\n    users.foreach { user => coll += userToDBObject(user) }\n  }\n\n  /**  Fetch users from GitHub and passes them to `inserter` */\n  def ingestUsers(nusers:Int)(inserter:Iterable[User] => Unit) {\n    val it = new GitHubUserIterator(token)\n    val users = it.take(nusers).toList\n    inserter(users)\n  }\n\n  def main(args:Array[String]) {\n    val coll = MongoClient()(\"github\")(\"users\")\n    val nusers = 500\n    coll.dropCollection()\n    val inserter = insertUsers(coll)_\n    ingestUsers(inserter)(nusers)\n  }\n\n}\n```", "```py\n$ GHTOKEN=\"e83638...\" sbt\n$ runMain InsertUsers\n\n```", "```py\n$ mongo github --quiet --eval \"db.users.count()\"\n500\n\n```", "```py\ndef repoToDBObject(repo:Repo):DBObject = ...\ndef userToDBObject(user:User):DBObject = ...\n```", "```py\ndef insertUsers(coll:MongoCollection)(users:Iterable[User]) {\n  users.foreach { user => coll += userToDBObject(user) }\n}\n```", "```py\nval inserter = insertUsers(coll)_\n```", "```py\ndef ingestUsers(nusers:Int)(inserter:Iterable[User] => Unit) {\n  val it = new GitHubUserIterator(token)\n  val users = it.take(nusers).toList\n  inserter(users)\n}\n```", "```py\nscala> val it = (0 to 10)\nit: Range.Inclusive = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> it.grouped(3).foreach { println } // In batches of 3\nVector(0, 1, 2)\nVector(3, 4, 5)\nVector(6, 7, 8)\nVector(9, 10)\n\n```", "```py\n/**  Fetch users from GitHub and pass them to `inserter` */\ndef ingestUsers(nusers:Int)(inserter:Iterable[User] => Unit) {\n  val batchSize = 100\n  val it = new GitHubUserIterator(token)\n  print(\"Inserted #users: \")\n  it.take(nusers).grouped(batchSize).zipWithIndex.foreach {\n    case (users, batchNumber) =>\n      print(s\"${batchNumber*batchSize} \")\n      inserter(users)\n  }\n  println()\n}\n```", "```py\n$ GHTOKEN=\"2502761...\" sbt \n> runMain InsertUsers\n[info] Running InsertUsers\nInserted #users: 0 100 200 300 400\n[success] Total time: 215 s, completed 01-Nov-2015 18:44:30\n\n```", "```py\nscala> import com.mongodb.casbah.Imports._\nimport com.mongodb.casbah.Imports._\n\nscala> val collection = MongoClient()(\"github\")(\"users\")\nMongoCollection = users\n\nscala> val maybeUser = collection.findOne\nOption[collection.T] = Some({ \"_id\" : { \"$oid\" : \"562e922546f953739c43df02\"} , \"github_id\" : 1 , \"login\" : \"mojombo\" , \"repos\" : ...\n\n```", "```py\nscala> val user = maybeUser.get\ncollection.T = { \"_id\" : { \"$oid\" : \"562e922546f953739c43df02\"} , \"github_id\" : 1 , \"login\" : \"mojombo\" , \"repos\" : ...\n\n```", "```py\nscala> user(\"login\")\nAnyRef = mojombo\n\n```", "```py\nscala> user.getAs[String](\"login\")\nOption[String] = Some(mojombo)\n\n```", "```py\nscala> user.getAs[Int](\"login\")\nOption[Int] = None\n\n```", "```py\nscala> val loginName = user.getAs[String](\"login\") orElse { \n println(\"No login field found. Falling back to 'name'\")\n user.getAs[String](\"name\")\n}\nloginName: Option[String] = Some(mojombo)\n\n```", "```py\nscala> user.getAsOrElse[Int](\"id\", 5)\nInt = 1392879\n\n```", "```py\nscala> user.getAsOrElse[String](\"name\", \n throw new IllegalArgumentException(\n \"Missing value for name\")\n)\njava.lang.IllegalArgumentException: Missing value for name\n...\n\n```", "```py\nscala> user.getAsOrElse[List[DBObject]](\"repos\",\n List.empty[DBObject])\nList[DBObject] = List({ \"github_id\" : 26899533 , \"name\" : \"30daysoflaptops.github.io\" ...\n\n```", "```py\nscala> val userIterator = collection.find()\nuserIterator: collection.CursorType = non-empty iterator\n\n```", "```py\nscala> val userList = userIterator.toList\nList[DBObject] = List({ \"_id\" : { \"$oid\": ...\n\n```", "```py\n// RepoNumber.scala\n\nimport com.mongodb.casbah.Imports._\n\nobject RepoNumber {\n\n  /** Extract the number of repos from a DBObject\n    * representing a user.\n    */   \n  def extractNumber(obj:DBObject):Option[Int] = {\n    val repos = obj.getAs[List[DBObject]](\"repos\") orElse {\n      println(\"Could not find or parse 'repos' field\")\n      None\n    }\n    repos.map { _.size }\n  }\n\n  val collection = MongoClient()(\"github\")(\"users\")\n\n  def main(args:Array[String]) {    \n    val userIterator = collection.find()\n\n    // Convert from documents to Option[Int]\n    val repoNumbers = userIterator.map { extractNumber }\n\n    // Convert from Option[Int] to Int\n    val wellFormattedNumbers = repoNumbers.collect { \n      case Some(v) => v \n    }.toList\n\n    // Calculate summary statistics\n    val sum = wellFormattedNumbers.reduce { _ + _ }\n    val count = wellFormattedNumbers.size\n\n    if (count == 0) {\n      println(\"No repos found\")\n    }\n    else {\n      val mean = sum.toDouble / count.toDouble\n      println(s\"Total number of users with repos: $count\")\n      println(s\"Total number of repos: $sum\")\n      println(s\"Mean number of repos: $mean\")\n    }\n  }\n}\n```", "```py\n> runMain RepoNumber\nTotal number of users with repos: 500\nTotal number of repos: 9649\nMean number of repos: 19.298\n\n```", "```py\nscala> val objs = collection.find().toList\nList[DBObject] = List({ \"_id\" : { \"$oid\" : \"56365cec46f9534fae8ffd7f\"} ,...\n\n```", "```py\nscala> val query = DBObject(\"login\" -> \"mojombo\")\nquery: DBObject = { \"login\" : \"mojombo\"}\n\nscala> val objs = collection.find(query).toList\nList[DBObject] = List({ \"_id\" : { \"$oid\" : \"562e922546f953739c43df02\"} , \"login\" : \"mojombo\",...\n\n```", "```py\nscala> val query = DBObject(\"github_id\" -> \n DBObject(\"$gte\" -> 20, \"$lt\" -> 30))\nquery: DBObject = { \"github_id\" : { \"$gte\" : 20 , \"$lt\" : 30}}\n\nscala> collection.find(query).toList\nList[com.mongodb.casbah.Imports.DBObject] = List({ \"_id\" : { \"$oid\" : \"562e922546f953739c43df0f\"} , \"github_id\" : 23 , \"login\" : \"takeo\" , ...\n\n```", "```py\nscala> val query = DBObject(\"repos.language\" -> \"Scala\")\nquery: DBObject = { \"repos.language\" : \"Scala\"}\n\nscala> collection.find(query).toList\nList[DBObject] = List({ \"_id\" : { \"$oid\" : \"5635da4446f953234ca634df\"}, \"login\" : \"kevinclark\"...\n\n```", "```py\nscala> collection.find(\"github_id\" $gte 20 $lt 30).toList\nList[com.mongodb.casbah.Imports.DBObject] = List({ \"_id\" : { \"$oid\" : \"562e922546f953739c43df0f\"} , \"github_id\" : 23 , \"login\" : \"takeo\" , \"repos\" : ...\n\n```", "```py\nscala> collection.find(\"repos.language\" $eq \"Scala\").size\nInt = 30\n\n```", "```py\nscala> object Language extends Enumeration {\n val Scala, Java, JavaScript = Value\n}\ndefined object Language\n\n```", "```py\nscala> import org.bson.{BSON, Transformer}\nimport org.bson.{BSON, Transformer}\n\nscala> trait LanguageTransformer extends Transformer {\n def transform(o:AnyRef):AnyRef = o match {\n case l:Language.Value => l.toString\n case _ => o\n }\n}\ndefined trait LanguageTransformer\n\n```", "```py\nscala> BSON.addEncodingHook(\n classOf[Language.Value], new LanguageTransformer {})\n\n```", "```py\nscala> val repoObj = DBObject(\n \"github_id\" -> 1234L,\n \"language\" -> Language.Scala\n)\nrepoObj: DBObject = { \"github_id\" : 1234 , \"language\" : \"Scala\"}\n\n```", "```py\nimplicit object LanguageReader extends MongoReader[Language.Value] {\n  def read(v:Any):Language.Value = v match {\n    case s:String => Language.withName(s)\n  }\n}\n```", "```py\nval seedUser = \"odersky\" // the origin of the network\n\n// Users whose URLs need to be fetched \nval queue = mutable.Queue(seedUser) \n\n// set of users that we have already fetched \n// (to avoid re-fetching them)\nval fetchedUsers = mutable.Set.empty[String] \n\nwhile (queue.nonEmpty) {\n  val user = queue.dequeue\n  if (!fetchedUsers(user)) {\n    val followers = fetchFollowersForUser(user)\n    followers foreach { follower =>           \n      // add the follower to queue of people whose \n      // followers we want to find.\n      queue += follower\n    }\n    fetchedUsers += user\n  }\n}\n```", "```py\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies += \"com.typesafe.akka\" %% \"akka-actor\" % \"2.4.0\"\n```", "```py\nimport akka.actor._\n```", "```py\n// EchoActor.scala\nimport akka.actor._\n\nclass EchoActor extends Actor with ActorLogging {\n  def receive = {\n    case msg:String => \n      Thread.sleep(500)\n      log.info(s\"Received '$msg'\") \n  }\n}\n```", "```py\n// HelloAkka.scala\n\nimport akka.actor._\n\nobject HelloAkka extends App {\n\n  // We need an actor system before we can \n  // instantiate actors\n  val system = ActorSystem(\"HelloActors\")\n\n  // instantiate our two actors\n  val echo1 = system.actorOf(Props[EchoActor], name=\"echo1\")\n  val echo2 = system.actorOf(Props[EchoActor], name=\"echo2\")\n\n  // Send them messages. We do this using the \"!\" operator\n  echo1 ! \"hello echo1\"\n  echo2 ! \"hello echo2\"\n  echo1 ! \"bye bye\"\n\n  // Give the actors time to process their messages, \n  // then shut the system down to terminate the program\n  Thread.sleep(500)\n  system.shutdown\n}\n```", "```py\n[INFO] [07/19/2015 17:15:23.954] [HelloActor-akka.actor.default-dispatcher-2] [akka://HelloActor/user/echo1] Received 'hello echo1'\n[INFO] [07/19/2015 17:15:23.954] [HelloActor-akka.actor.default-dispatcher-3] [akka://HelloActor/user/echo2] Received 'hello echo2'\n[INFO] [07/19/2015 17:15:24.955] [HelloActor-akka.actor.default-dispatcher-2] [akka://HelloActor/user/echo1] Received 'bye bye'\n\n```", "```py\n// EchoActor.scala\n\nobject EchoActor { \n  case object EchoHello\n  case class EchoMessage(msg:String)\n}\n```", "```py\nclass EchoActor extends Actor with ActorLogging {\n  import EchoActor._ // import the message definitions\n  def receive = {\n    case EchoHello => log.info(\"hello\")\n    case EchoMessage(s) => log.info(s)  \n  }\n}\n```", "```py\necho1 ! EchoActor.EchoHello\necho2 ! EchoActor.EchoMessage(\"We're learning Akka.\")\n```", "```py\nval echoProps = Props[EchoActor]\n```", "```py\nclass TestActor(a:String, b:Int) extends Actor { ... }\n```", "```py\nval testProps = Props(classOf[TestActor], \"hello\", 2)\n```", "```py\nval system = ActorSystem(\"HelloActors\")\nval echo1 = system.actorOf(echoProps, name=\"hello-1\")\n```", "```py\nclass TestParentActor extends Actor {\n  val echoChild = context.actorOf(echoProps, name=\"hello-child\")\n  ...\n}\n```", "```py\nobject EchoActor {\n  def props:Props = Props[EchoActor]\n\n  // message case class definitions here\n}\n```", "```py\nval echoActor = system.actorOf(EchoActor.props)\n```", "```py\n// build.sbt\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n  \"org.json4s\" %% \"json4s-native\" % \"3.2.10\",\n  \"org.scalaj\" %% \"scalaj-http\" % \"1.1.4\",\n  \"com.typesafe.akka\" %% \"akka-actor\" % \"2.3.12\"\n)\n```", "```py\n// Fetcher.scala\nimport akka.actor._\nimport scalaj.http._\nimport scala.concurrent.Future\n\nobject Fetcher {\n  // message definitions\n  case class Fetch(login:String)\n\n  // Props factory definitions\n  def props(token:Option[String]):Props = \n    Props(classOf[Fetcher], token)\n  def props():Props = Props(classOf[Fetcher], None)\n}\n```", "```py\n// Fetcher.scala\nclass Fetcher(val token:Option[String])\nextends Actor with ActorLogging {\n  import Fetcher._ // import message definition\n\n  // We will need an execution context for the future.\n  // Recall that the dispatcher doubles up as execution\n  // context.\n  import context.dispatcher\n\n  def receive = {\n    case Fetch(login) => fetchUrl(login)\n  }\n\n  private def fetchUrl(login:String) {\n    val unauthorizedRequest = Http(\n      s\"https://api.github.com/users/$login/followers\")\n    val authorizedRequest = token.map { t =>\n      unauthorizedRequest.header(\"Authorization\", s\"token $t\")\n    }\n\n    // Prepare the request: try to use the authorized request\n    // if a token was given, and fall back on an unauthorized \n    // request\n    val request = authorizedRequest.getOrElse(unauthorizedRequest)\n\n    // Fetch from github\n    val response = Future { request.asString }\n    response.onComplete { r =>\n      log.info(s\"Response from $login: $r\")\n    }\n  }\n\n}\n```", "```py\n// FetcherDemo.scala\nimport akka.actor._\n\nobject FetcherDemo extends App {\n  import Fetcher._ // Import the messages\n\n  val system = ActorSystem(\"fetchers\")\n\n  // Read the github token if present.\n  val token = sys.env.get(\"GHTOKEN\")\n\n  val fetchers = (0 until 4).map { i =>\n    system.actorOf(Fetcher.props(token))\n  }\n\n  fetchers(0) ! Fetch(\"odersky\")\n  fetchers(1) ! Fetch(\"derekwyatt\")\n  fetchers(2) ! Fetch(\"rkuhn\")\n  fetchers(3) ! Fetch(\"tototoshi\")\n\n  Thread.sleep(5000) // Wait for API calls to finish\n  system.shutdown // Shut system down\n\n}\n```", "```py\n$ GHTOKEN=\"2502761...\" sbt run\n[INFO] [11/08/2015 16:28:06.500] [fetchers-akka.actor.default-dispatcher-2] [akka://fetchers/user/$d] Response from tototoshi: Success(HttpResponse([{\"login\":\"akr4\",\"id\":10892,\"avatar_url\":\"https://avatars.githubusercontent.com/u/10892?v=3\",\"gravatar_id\":\"\"...\n\n```", "```py\n// FetcherDemoWithScheduler.scala\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.duration._\n```", "```py\nsystem.scheduler.scheduleOnce(5.seconds) { system.shutdown }\n```", "```py\n// FetcherDemo.scala\nimport akka.routing._\n```", "```py\n// FetcherDemo.scala\n\n// Create a router with 4 workers of props Fetcher.props()\nval router = system.actorOf(\n  RoundRobinPool(4).props(Fetcher.props(token))\n)\n```", "```py\nList(\"odersky\", \"derekwyatt\", \"rkuhn\", \"tototoshi\").foreach { \n  login => router ! Fetch(login)\n}\n```", "```py\n// Fetcher.scala\nclass Fetcher(\n  val token:Option[String], \n  val responseInterpreter:ActorRef) \nextends Actor with ActorLogging {\n  ...\n}\n```", "```py\n// Fetcher.scala\ndef props(\n  token:Option[String], responseInterpreter:ActorRef\n):Props = Props(classOf[Fetcher], token, responseInterpreter)\n```", "```py\n// Fetcher.scala\nclass Fetcher(...) extends Actor with ActorLogging {\n  ...\n  def receive = {\n    case Fetch(login) => fetchFollowers(login)\n  }\n\n  private def fetchFollowers(login:String) {\n    val unauthorizedRequest = Http(\n      s\"https://api.github.com/users/$login/followers\")\n    val authorizedRequest = token.map { t =>\n      unauthorizedRequest.header(\"Authorization\", s\"token $t\")\n    }\n\n    val request = authorizedRequest.getOrElse(unauthorizedRequest)\n    val response = Future { request.asString }\n\n    // Wrap the response in an InterpretResponse message and\n    // forward it to the interpreter.\n    response.onComplete { r =>\n      responseInterpreter !\n        ResponseInterpreter.InterpretResponse(login, r)\n    }\n  }\n}\n```", "```py\n// ResponseInterpreter.scala\nimport akka.actor._\nimport scala.util._\n\nimport scalaj.http._\nimport org.json4s._\nimport org.json4s.native.JsonMethods._\n\nobject ResponseInterpreter {\n\n  // Messages\n  case class InterpretResponse(\n    login:String, response:Try[HttpResponse[String]]\n  )\n\n  // Props factory\n  def props(followerExtractor:ActorRef) = \n    Props(classOf[ResponseInterpreter], followerExtractor)\n}\n```", "```py\n// ResponseInterpreter.scala\nclass ResponseInterpreter(followerExtractor:ActorRef) \nextends Actor with ActorLogging {\n  // Import the message definitions\n  import ResponseInterpreter._\n\n  def receive = {\n    case InterpretResponse(login, r) => interpret(login, r)\n  }\n\n  // If the query was successful, extract the JSON response\n  // and pass it onto the follower extractor.\n  // If the query failed, or is badly formatted, throw an error\n  // We should also be checking error codes here.\n  private def interpret(\n    login:String, response:Try[HttpResponse[String]]\n  ) = response match {\n    case Success(r) => responseToJson(r.body) match {\n      case Success(jsonResponse) => \n        followerExtractor ! FollowerExtractor.Extract(\n          login, jsonResponse)\n      case Failure(e) => \n        log.error(\n          s\"Error parsing response to JSON for $login: $e\")\n    }\n    case Failure(e) => log.error(\n      s\"Error fetching URL for $login: $e\")\n  }\n\n  // Try and parse the response body as JSON. \n  // If successful, coerce the `JValue` to a `JArray`.\n  private def responseToJson(responseBody:String):Try[JArray] = {\n    val jvalue = Try { parse(responseBody) }\n    jvalue.flatMap {\n      case a:JArray => Success(a)\n      case _ => Failure(new IllegalStateException(\n        \"Incorrectly formatted JSON: not an array\"))\n    }\n  }\n}\n```", "```py\n// FollowerExtractor.scala\nimport akka.actor._\n\nimport org.json4s._\nimport org.json4s.native.JsonMethods._\n\nobject FollowerExtractor {\n\n  // Messages\n  case class Extract(login:String, jsonResponse:JArray)\n\n  // Props factory method\n  def props = Props[FollowerExtractor]\n}\n```", "```py\nclass FollowerExtractor extends Actor with ActorLogging {\n  import FollowerExtractor._\n  def receive = {\n    case Extract(login, followerArray) => {\n      val followers = extractFollowers(followerArray)\n      log.info(s\"$login -> ${followers.mkString(\", \")}\")\n    }\n  }\n\n  def extractFollowers(followerArray:JArray) = for {\n    JObject(follower) <- followerArray\n    JField(\"login\", JString(login)) <- follower\n  } yield login\n}\n```", "```py\n// FetchNetwork.scala\n\nimport akka.actor._\nimport akka.routing._\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.duration._\n\nobject FetchNetwork extends App {\n\n  import Fetcher._ // Import messages and factory method\n\n  // Get token if exists\n  val token = sys.env.get(\"GHTOKEN\")\n\n  val system = ActorSystem(\"fetchers\")\n\n  // Instantiate actors\n  val followerExtractor = system.actorOf(FollowerExtractor.props)\n  val responseInterpreter =   \n    system.actorOf(ResponseInterpreter.props(followerExtractor))\n\n  val router = system.actorOf(RoundRobinPool(4).props(\n    Fetcher.props(token, responseInterpreter))\n  )\n\n  List(\"odersky\", \"derekwyatt\", \"rkuhn\", \"tototoshi\") foreach {\n    login => router ! Fetch(login)\n  }\n\n  // schedule a shutdown\n  system.scheduler.scheduleOnce(5.seconds) { system.shutdown }\n\n}\n```", "```py\n$ GHTOKEN=\"2502761d...\" sbt run\n[INFO] [11/05/2015 20:09:37.048] [fetchers-akka.actor.default-dispatcher-3] [akka://fetchers/user/$a] derekwyatt -> adulteratedjedi, joonas, Psycojoker, trapd00r, tyru, ...\n[INFO] [11/05/2015 20:09:37.050] [fetchers-akka.actor.default-dispatcher-3] [akka://fetchers/user/$a] tototoshi -> akr4, yuroyoro, seratch, yyuu, ...\n[INFO] [11/05/2015 20:09:37.051] [fetchers-akka.actor.default-dispatcher-3] [akka://fetchers/user/$a] odersky -> misto, gkossakowski, mushtaq, ...\n[INFO] [11/05/2015 20:09:37.052] [fetchers-akka.actor.default-dispatcher-3] [akka://fetchers/user/$a] rkuhn -> arnbak, uzoice, jond3k, TimothyKlim, relrod, ...\n\n```", "```py\n// Fecther.scala\nobject Fetcher {\n  case class Fetch(url:String)\n  case object WorkAvailable\n\n  def props(\n    token:Option[String], \n    fetcherManager:ActorRef, \n    responseInterpreter:ActorRef):Props =\n      Props(classOf[Fetcher], \n        token, fetcherManager, responseInterpreter)\n}\n```", "```py\nclass Fetcher(\n  val token:Option[String], \n  val fetcherManager:ActorRef,\n  val responseInterpreter:ActorRef) \nextends Actor with ActorLogging {\n  import Fetcher._\n  import context.dispatcher\n\n  def receive = {\n    case Fetch(login) => fetchFollowers(login)\n    case WorkAvailable => \n      fetcherManager ! FetcherManager.GiveMeWork\n  }\n\n  private def fetchFollowers(login:String) {\n    val unauthorizedRequest = Http(\n      s\"https://api.github.com/users/$login/followers\")\n    val authorizedRequest = token.map { t =>\n      unauthorizedRequest.header(\"Authorization\", s\"token $t\")\n    }\n    val request = authorizedRequest.getOrElse(unauthorizedRequest)\n    val response = Future { request.asString }\n\n    response.onComplete { r =>\n      responseInterpreter ! \n        ResponseInterpreter.InterpretResponse(login, r)\n      fetcherManager ! FetcherManager.GiveMeWork\n    }\n  }\n\n}\n```", "```py\ndef receive = {\n  case GiveMeWork =>\n    login = // get next login to fetch\n    sender ! Fetcher.Fetch(login)\n  ...\n}\n```", "```py\ndef receive = {\n  case DoSomeWork =>\n    val work = Future { Thread.sleep(20000) ; 5 }\n    work.onComplete { result => \n      sender ! Complete(result) // NO!\n    }\n}\n```", "```py\ndef receive = {\n  case DoSomeWork =>\n    // bind the current value of sender to a val\n    val requestor = sender\n    val work = Future { Thread.sleep(20000) ; 5 }\n    work.onComplete { result => requestor ! Complete(result) }\n}\n```", "```py\n// receive method when the queue is empty\ndef receiveWhileEmpty: Receive = { \n    ... \n}\n\n// receive method when the queue is not empty\ndef receiveWhileNotEmpty: Receive = {\n    ...\n}\n```", "```py\ndef receive = receiveWhileEmpty\n```", "```py\n// FetcherManager.scala\nimport scala.collection.mutable\nimport akka.actor._\n\nobject FetcherManager {\n  case class AddToQueue(login:String)\n  case object GiveMeWork\n\n  def props(token:Option[String], nFetchers:Int) = \n    Props(classOf[FetcherManager], token, nFetchers)\n}\n```", "```py\n// FetcherManager.scala\n\nclass FetcherManager(val token:Option[String], val nFetchers:Int) \nextends Actor with ActorLogging {\n\n  import FetcherManager._\n\n  // queue of usernames whose followers we need to fetch\n  val fetchQueue = mutable.Queue.empty[String]\n\n  // set of users we have already fetched. \n  val fetchedUsers = mutable.Set.empty[String]\n\n  // Instantiate worker actors\n  val followerExtractor = context.actorOf(\n    FollowerExtractor.props(self))\n  val responseInterpreter = context.actorOf(\n    ResponseInterpreter.props(followerExtractor))\n  val fetchers = (0 until nFetchers).map { i =>\n    context.actorOf(\n      Fetcher.props(token, self, responseInterpreter))\n  }\n\n  // receive method when the actor has work:\n  // If we receive additional work, we just push it onto the\n  // queue.\n  // If we receive a request for work from a Fetcher,\n  // we pop an item off the queue. If that leaves the \n  // queue empty, we transition to the 'receiveWhileEmpty'\n  // method.\n  def receiveWhileNotEmpty:Receive = {\n    case AddToQueue(login) => queueIfNotFetched(login)\n    case GiveMeWork =>\n      val login = fetchQueue.dequeue\n      // send a Fetch message back to the sender.\n      // we can use the `sender` method to reply to a message\n      sender ! Fetcher.Fetch(login)\n      if (fetchQueue.isEmpty) { \n        context.become(receiveWhileEmpty) \n      }\n  }\n\n  // receive method when the actor has no work:\n  // if we receive work, we add it onto the queue, transition\n  // to a state where we have work, and notify the fetchers\n  // that work is available.\n  def receiveWhileEmpty:Receive = {\n    case AddToQueue(login) =>\n      queueIfNotFetched(login)\n      context.become(receiveWhileNotEmpty)\n      fetchers.foreach { _ ! Fetcher.WorkAvailable }\n    case GiveMeWork => // do nothing\n  }\n\n  // Start with an empty queue.\n  def receive = receiveWhileEmpty\n\n  def queueIfNotFetched(login:String) {\n    if (! fetchedUsers(login)) {\n      log.info(s\"Pushing $login onto queue\") \n      // or do something useful...\n      fetchQueue += login\n      fetchedUsers += login\n    }\n  }\n}\n```", "```py\n// FollowerExtractor.scala\nimport akka.actor._\nimport org.json4s._\nimport org.json4s.native.JsonMethods._\n\nobject FollowerExtractor {\n\n  // messages\n  case class Extract(login:String, jsonResponse:JArray)\n\n  // props factory method\n  def props(manager:ActorRef) = \n    Props(classOf[FollowerExtractor], manager)\n}\n\nclass FollowerExtractor(manager:ActorRef)\nextends Actor with ActorLogging {\n  import FollowerExtractor._\n\n  def receive = {\n    case Extract(login, followerArray) =>\n      val followers = extractFollowers(followerArray)\n      followers foreach { f => \n        manager ! FetcherManager.AddToQueue(f) \n      }\n  }\n\n  def extractFollowers(followerArray:JArray) = for {\n    JObject(follower) <- followerArray\n    JField(\"login\", JString(login)) <- follower\n  } yield login\n\n}\n```", "```py\n// FetchNetwork.scala\nimport akka.actor._\n\nobject FetchNetwork extends App {\n\n  // Get token if exists\n  val token = sys.env.get(\"GHTOKEN\")\n\n  val system = ActorSystem(\"GithubFetcher\")\n  val manager = system.actorOf(FetcherManager.props(token, 2))\n  manager ! FetcherManager.AddToQueue(\"odersky\")\n\n}\n```", "```py\n$ GHTOKEN=\"2502761d...\" sbt \"runMain FetchNetwork\"\n[INFO] [11/06/2015 06:31:04.614] [GithubFetcher-akka.actor.default-dispatcher-2] [akka://GithubFetcher/user/$a] Pushing odersky onto queue\n[INFO] [11/06/2015 06:31:05.563] [GithubFetcher-akka.actor.default-dispatcher-4] [akka://GithubFetcher/user/$a] Pushing misto onto queueINFO] [11/06/2015 06:31:05.563] [GithubFetcher-akka.actor.default-dispatcher-4] [akka://GithubFetcher/user/$a] Pushing gkossakowski onto queue\n^C\n\n```", "```py\n// ResponseInterpreter.scala\ndef receive = {\n  case InterpretResponse(\"misto\", r) => \n    throw new IllegalStateException(\"custom error\")\n  case InterpretResponse(login, r) => interpret(login, r)\n}\n```", "```py\n[ERROR] [11/07/2015 12:05:58.938] [GithubFetcher-akka.actor.default-dispatcher-2] [akka://GithubFetcher/user/$a/$b] custom error\njava.lang.IllegalStateException: custom error\n at ResponseInterpreter$\n ...\n[INFO] [11/07/2015 12:05:59.117] [GithubFetcher-akka.actor.default-dispatcher-2] [akka://GithubFetcher/user/$a] Pushing samfoo onto queue\n\n```", "```py\nval supervisorStrategy = OneForOneStrategy() {\n  case _:ActorInitializationException => Stop\n  case _:ActorKilledException => Stop\n  case _:DeathPactException => Stop\n  case _:Exception => Restart\n}\n```", "```py\nimport akka.actor.SupervisorStrategy._\n```", "```py\nclass FetcherManager(...) extends Actor with ActorLogging {\n\n  ...\n\n  override val supervisorStrategy = AllForOneStrategy() {\n    case _:ActorInitializationException => Stop\n    case _:ActorKilledException => Stop\n    case _:Exception => Stop\n  }\n\n  ...\n}\n```", "```py\n// FetcherManager.scala\n\nimport scala.io.Source \nimport scala.util._\nimport java.io._\n```", "```py\n// FetcherManager.scala\nobject FetcherManager {\n  ...\n  val fetchedUsersFileName = \"fetched-users.txt\"\n  val fetchQueueFileName = \"fetch-queue.txt\"\n}\n```", "```py\nclass FetcherManager(\n  val token:Option[String], val nFetchers:Int\n) extends Actor with ActorLogging {\n\n  ...\n\n  /** pre-start method: load saved state from text files */\n  override def preStart {\n    log.info(\"Running pre-start on fetcher manager\")\n\n    loadFetchedUsers\n    log.info(\n      s\"Read ${fetchedUsers.size} visited users from source\"\n    )\n\n    loadFetchQueue\n    log.info(\n      s\"Read ${fetchQueue.size} users in queue from source\"\n    )\n\n    // If the saved state contains a non-empty queue, \n    // alert the fetchers so they can start working.\n    if (fetchQueue.nonEmpty) {\n      context.become(receiveWhileNotEmpty)\n      fetchers.foreach { _ ! Fetcher.WorkAvailable }\n    }\n\n  }\n\n  /** Dump the current state of the manager */\n  override def postStop {\n    log.info(\"Running post-stop on fetcher manager\")\n    saveFetchedUsers\n    saveFetchQueue\n  }\n\n     /* Helper methods to load from and write to files */\n  def loadFetchedUsers {\n    val fetchedUsersSource = Try { \n      Source.fromFile(fetchedUsersFileName) \n    }\n    fetchedUsersSource.foreach { s =>\n      try s.getLines.foreach { l => fetchedUsers += l }\n      finally s.close\n    }\n  }\n\n  def loadFetchQueue {\n    val fetchQueueSource = Try { \n      Source.fromFile(fetchQueueFileName) \n    }\n    fetchQueueSource.foreach { s =>\n      try s.getLines.foreach { l => fetchQueue += l }\n      finally s.close\n    }\n  }\n\n  def saveFetchedUsers {\n    val fetchedUsersFile = new File(fetchedUsersFileName)\n    val writer = new BufferedWriter(\n      new FileWriter(fetchedUsersFile))\n    fetchedUsers.foreach { user => writer.write(user + \"\\n\") }\n    writer.close()\n  }\n\n  def saveFetchQueue {\n    val queueUsersFile = new File(fetchQueueFileName)\n    val writer = new BufferedWriter(\n      new FileWriter(queueUsersFile))\n    fetchQueue.foreach { user => writer.write(user + \"\\n\") }\n    writer.close()\n  }\n\n...\n}\n```", "```py\n// FetchNetwork.scala\nimport akka.actor._\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.duration._\n\nobject FetchNetwork extends App {\n\n  // Get token if exists\n  val token = sys.env.get(\"GHTOKEN\")\n\n  val system = ActorSystem(\"GithubFetcher\")\n  val manager = system.actorOf(FetcherManager.props(token, 2))\n\n  manager ! FetcherManager.AddToQueue(\"odersky\")\n\n  system.scheduler.scheduleOnce(30.seconds) { system.shutdown }\n\n}\n```", "```py\n$ tar xzf spark-1.5.2-bin-hadoop2.6.tgz\n\n```", "```py\nexport PATH=/path/to/spark/bin:$PATH\n```", "```py\n$env:Path += \";C:\\Program Files\\GnuWin32\\bin\"\n```", "```py\n$ spark-shell\nscala> \n\n```", "```py\nscala> val email = sc.textFile(\"ham/9-463msg1.txt\")\nemail: rdd.RDD[String] = MapPartitionsRDD[1] at textFile\n\n```", "```py\nscala> sc\nspark.SparkContext = org.apache.spark.SparkContext@459bf87c\n\n```", "```py\nscala> val words = email.flatMap { line => line.split(\"\\\\s\") }\nwords: rdd.RDD[String] = MapPartitionsRDD[2] at flatMap\n\n```", "```py\nscala> val words = for { \n line <- email\n word <- line.split(\"\\\\s\") \n} yield word\nwords: rdd.RDD[String] = MapPartitionsRDD[3] at flatMap\n\n```", "```py\nscala> words.take(5)\nArray[String] = Array(Subject:, tsd98, workshop, -, -)\n\n```", "```py\nscala> words.count\nLong = 939\n\n```", "```py\nscala> val nonAlphaNumericPattern = \"[^a-zA-Z0-9]\".r\nnonAlphaNumericPattern: Regex = [^a-zA-Z0-9]\n\nscala> val filteredWords = words.filter { \n word => nonAlphaNumericPattern.findFirstIn(word) == None \n}\nfilteredWords: rdd.RDD[String] = MapPartitionsRDD[4] at filter\n\nscala> filteredWords.take(5)\nArray[String] = Array(tsd98, workshop, 2nd, call, paper)\n\nscala> filteredWords.count\nLong = 627\n\n```", "```py\nscala> val words = \"the quick brown fox jumped over the dog\".split(\" \") \nwords: Array[String] = Array(the, quick, brown, fox, ...)\n\nscala> val wordsRDD = sc.parallelize(words)\nwordsRDD: RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:23\n\n```", "```py\nscala> val wordLengths = wordsRDD.map { _.length }\nwordLengths: RDD[Int] = MapPartitionsRDD[2] at map at <console>:25\n\nscala> wordLengths.collect\nArray[Int] = Array(3, 5, 5, 3, 6, 4, 3, 3)\n\n```", "```py\nval email = sc.textFile(...)\nval words = email.flatMap { line => line.split(\"\\\\s\") }\n```", "```py\nscala> val inp = sc.textFile(\"nonexistent\")\ninp: rdd.RDD[String] = MapPartitionsRDD[5] at textFile\n\n```", "```py\nscala> inp.count // number of lines\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/pascal/...\n\n```", "```py\nscala> filteredWords.toDebugString\n(2) MapPartitionsRDD[6] at filter at <console>:27 []\n |  MapPartitionsRDD[3] at flatMap at <console>:23 []\n |  MapPartitionsRDD[1] at textFile at <console>:21 []\n |  ham/9-463msg1.txt HadoopRDD[0] at textFile at <console>:21 []\n\n```", "```py\nscala> val rdd = sc.parallelize(List(\"quick\", \"brown\", \"quick\", \"dog\"))\n\n```", "```py\nval allTransactions = sc.textFile(\"transaction.log\")\nval interestingTransactions = allTransactions.filter { \n  _.contains(\"Account: 123456\")\n}\n```", "```py\nscala> words.persist\nrdd.RDD[String] = MapPartitionsRDD[3] at filter\n\n```", "```py\nscala> import org.apache.spark.storage.StorageLevel\nimport org.apache.spark.storage.StorageLevel\n\nscala> interestingTransactions.persist(\n StorageLevel.MEMORY_AND_DISK)\nrdd.RDD[String] = MapPartitionsRDD[3] at filter\n\n```", "```py\nscala> val email = sc.textFile(\"ham/9-463msg1.txt\")\nemail: rdd.RDD[String] = MapPartitionsRDD[1] at textFile\n\nscala> val words = email.flatMap { line => line.split(\"\\\\s\") }\nwords: rdd.RDD[String] = MapPartitionsRDD[2] at flatMap\n\n```", "```py\nscala> words.persist\n\n```", "```py\nscala> val wordsKeyValue = words.map { _ -> 1 }\nwordsKeyValue: rdd.RDD[(String, Int)] = MapPartitionsRDD[32] at map \n\nscala> wordsKeyValue.first\n(String, Int) = (Subject:,1)\n\n```", "```py\nscala> val wordCounts = wordsKeyValue.reduceByKey { _ + _ }\nwordCounts: rdd.RDD[(String, Int)] = ShuffledRDD[35] at reduceByKey\n\nscala> wordCounts.take(5).foreach { println }\n(university,6)\n(under,1)\n(call,3)\n(paper,2)\n(chasm,2)\n\n```", "```py\nscala> wordCounts.toDebugString\n(2) ShuffledRDD[36] at reduceByKey at <console>:30 []\n +-(2) MapPartitionsRDD[32] at map at <console>:28 []\n |  MapPartitionsRDD[7] at flatMap at <console>:23 []\n |      CachedPartitions: 2; MemorySize: 50.3 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  MapPartitionsRDD[3] at textFile at <console>:21 []\n |      CachedPartitions: 2; MemorySize: 5.1 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  ham/9-463msg1.txt HadoopRDD[2] at textFile at <console>:21 []\n\n```", "```py\nscala> wordCounts.collectAsMap\nscala.collection.Map[String,Int] = Map(follow -> 2, famous -> 1...\n\n```", "```py\nscala> val words = sc.parallelize(List(\"quick\", \"brown\",\"quick\", \"dog\"))\nwords: RDD[String] = ParallelCollectionRDD[25] at parallelize at <console>:21\n\nscala> val rdd = words.map { word => (word -> word.size) }\nrdd: RDD[(String, Int)] = MapPartitionsRDD[26] at map at <console>:23\n\nscala> rdd.collect\nArray[(String, Int)] = Array((quick,5), (brown,5), (quick,5), (dog,3))\n\n```", "```py\nscala> val spamEmail = sc.textFile(\"spam/spmsgb17.txt\")\nspamEmail: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[52] at textFile at <console>:24\n\nscala> val spamWords = spamEmail.flatMap { _.split(\"\\\\s\") }\nspamWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[53] at flatMap at <console>:26\n\nscala> val spamWordCounts = spamWords.map { _ -> 1 }.reduceByKey { _ + _ }\nspamWordsCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[55] at reduceByKey at <console>:30\n\nscala> spamWordCounts.take(5).foreach { println }\n(banner,3)\n(package,14)\n(call,1)\n(country,2)\n(offer,1)\n\n```", "```py\nscala> val commonWordCounts = wordCounts.join(spamWordCounts)\nres93: rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[58] at join at <console>:41\n\nscala> commonWordCounts.take(5).foreach { println }\n(call,(3,1))\n(include,(6,2))\n(minute,(2,1))\n(form,(1,7))\n((,(36,5))\n\n```", "```py\nscala> val leftWordCounts = wordCounts.leftOuterJoin(spamWordCounts)\nleftWordCounts: rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[64] at leftOuterJoin at <console>:40\n\nscala> leftWordCounts.take(5).foreach { println }\n(call,(3,Some(1)))\n(paper,(2,None))\n(chasm,(2,None))\n(antonio,(1,None))\n(event,(3,None))\n\n```", "```py\nscala> val defaultWordCounts = leftWordCounts.mapValues { \n case(leftValue, rightValue) => (leftValue, rightValue.getOrElse(0))\n}\norg.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[64] at leftOuterJoin at <console>:40\n\nscala> defaultwordCounts.take(5).foreach { println }\n(call,(3,1))\n(paper,(2,0))\n(chasm,(2,0))\n(antonio,(1,0))\n(event,(3,0))\n\n```", "```py\nscala> val counts = wordCounts.values.map { _.toDouble }\ncounts: rdd.RDD[Double] = MapPartitionsRDD[9] at map\n\n```", "```py\nscala> counts.stats\norg.apache.spark.util.StatCounter = (count: 397, mean: 2.365239, stdev: 5.740843, max: 72.000000, min: 1.000000)\n\n```", "```py\nscala> counts.histogram(5)\n(Array(1.0, 15.2, 29.4, 43.6, 57.8, 72.0),Array(391, 1, 3, 1, 1))\n\n```", "```py\nscala> counts.histogram(Array(1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0))\nres13: Array[Long] = Array(264, 94, 22, 11, 1, 4, 1)\n\n```", "```py\n// build.sbt file\n\nname := \"spam_mi\"\n\nscalaVersion := \"2.10.5\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.4.1\"\n)\n```", "```py\n$ spark-submit target/scala-2.10/spam_mi_2.10-0.1-SNAPSHOT.jar\n... runs the program\n```", "```py\n// build.sbt\n\nname := \"spam_mi\"\n\nscalaVersion := \"2.10.5\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.5.2\" % \"provided\",\n  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-viz\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n)\n```", "```py\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.0\")\n```", "```py\nlog4j.rootCategory=INFO, console\n```", "```py\nlog4j.rootCategory=WARN, console\n```", "```py\n$ export AWS_ACCESS_KEY_ID=ABCDEF...\n$ export AWS_SECRET_ACCESS_KEY=2dEf...\n```", "```py\n$ chmod 400 test_ec2.pem\n```", "```py\n$ ./spark-ec2 -k test_ec2 -i ~/path/to/certificate/test_ec2.pem -s 2 launch test_cluster\n```", "```py\n# shut down 'test_cluster'\n$ ./spark-ec2 stop test_cluster\n\n# start 'test_cluster'\n$ ./spark-ec2 -i test_ec2.pem start test_cluster\n\n# destroy 'test_cluster'\n$ ./spark-ec2 destroy test_cluster\n```", "```py\n// MutualInformation.scala\nimport org.apache.spark.{ SparkConf, SparkContext }\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\n\nobject MutualInformation extends App {\n\n  def wordFractionInFiles(sc:SparkContext)(fileGlob:String)\n  :(RDD[(String, Double)], Long) = {\n\n    // A set of punctuation words that need to be filtered out.\n    val wordsToOmit = Set[String](\n      \"\", \".\", \",\", \":\", \"-\", \"\\\"\", \"'\", \")\", \n      \"(\", \"@\", \"/\", \"Subject:\"\n    )\n\n    val messages = sc.wholeTextFiles(fileGlob)\n    // wholeTextFiles generates a key-value RDD of \n    // file name -> file content\n\n    val nMessages = messages.count()\n\n    // Split the content of each message into a Set of unique\n    // words in that message, and generate a new RDD mapping:\n    // message -> word\n    val message2Word = messages.flatMapValues {\n      mailBody => mailBody.split(\"\\\\s\").toSet\n    }\n\n    val message2FilteredWords = message2Word.filter { \n      case(email, word) => ! wordsToOmit(word) \n    }\n\n    val word2Message = message2FilteredWords.map { _.swap }\n\n    // word -> number of messages it appears in.\n    val word2NumberMessages = word2Message.mapValues { \n      _ => 1 \n    }.reduceByKey { _ + _ }\n\n    // word -> fraction of messages it appears in\n    val pPresent = word2NumberMessages.mapValues { \n      _ / nMessages.toDouble \n    }\n\n    (pPresent, nMessages)\n  }\n}\n```", "```py\n$ spark-shell --jars=target/scala-2.10/spam_mi_2.10-0.1-SNAPSHOT.jar\n\n```", "```py\nscala> import MutualInformation._\nimport MutualInformation._\n\nscala> val (fractions, nMessages) = wordFractionInFiles(sc)(\"ham/*\")\nfractions: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[13] at mapValues\nnMessages: Long = 2412\n\n```", "```py\nscala> fractions.take(5)\nArray[(String, Double)] = Array((rule-base,0.002902155887230514), (reunion,4.1459369817578774E-4), (embarrasingly,4.1459369817578774E-4), (mller,8.291873963515755E-4), (sapore,4.1459369817578774E-4))\n\n```", "```py\nscala> fractions.takeOrdered(5)(Ordering.by { - _._2 })\nres0: Array[(String, Double)] = Array((language,0.6737147595356551), (university,0.6048922056384743), (linguistic,0.5149253731343284), (information,0.45480928689883915), ('s,0.4369817578772803))\n\n```", "```py\n// MutualInformation.scala\nimport org.apache.spark.{ SparkConf, SparkContext }\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\n\nobject MutualInformation extends App {\n\n  def wordFractionInFiles(sc:SparkContext)(fileGlob:String)\n  :(RDD[(String, Double)], Long) = {\n    ...\n  }\n\n  val conf = new SparkConf().setAppName(\"lingSpam\")\n  val sc = new SparkContext(conf)\n\n```", "```py\n    /* Conditional probabilities RDD:\n       word -> P(present | spam) \n    */\n    val (pPresentGivenSpam, nSpam) = wordFractionInFiles(sc)(\"spam/*\")\n    val pAbsentGivenSpam = pPresentGivenSpam.mapValues { 1.0 - _ }\n    val (pPresentGivenHam, nHam) = wordFractionInFiles(sc)(\"ham/*\")\n    val pAbsentGivenHam = pPresentGivenHam.mapValues { 1.0 - _ }\n\n    // pSpam is the fraction of spam messages\n    val nMessages = nSpam + nHam\n    val pSpam = nSpam / nMessages.toDouble\n\n    // pHam is the fraction of ham messages\n    val pHam = 1.0 - pSpam \n\n    /* pPresentAndSpam is a key-value RDD of joint probabilities\n       word -> P(word present, spam) \n    */\n    val pPresentAndSpam = pPresentGivenSpam.mapValues { \n      _ * pSpam \n    }\n    val pPresentAndHam = pPresentGivenHam.mapValues { _ * pHam }\n    val pAbsentAndSpam = pAbsentGivenSpam.mapValues { _ * pSpam }\n    val pAbsentAndHam = pAbsentGivenHam.mapValues { _ * pHam }\n```", "```py\n    pPresentAndSpam.persist\n    pPresentAndHam.persist\n    pAbsentAndSpam.persist\n    pAbsentAndHam.persist\n```", "```py\n    val pJoined = pPresentAndSpam.fullOuterJoin(pPresentAndHam)\n    val pJoinedDefault = pJoined.mapValues {\n      case (presentAndSpam, presentAndHam) => \n        (presentAndSpam.getOrElse(0.5/nSpam * pSpam), \n        presentAndHam.getOrElse(0.5/nHam * pHam))\n    }\n```", "```py\n    val pPresent = pJoinedDefault.mapValues { \n      case(presentAndHam, presentAndSpam) => \n        presentAndHam + presentAndSpam \n    }\n    pPresent.persist\n\n    val pAbsent = pPresent.mapValues { 1.0 - _ }\n    pAbsent.persist\n```", "```py\n    def miTerm(\n      pXYs:RDD[(String, Double)], \n      pXs:RDD[(String, Double)], \n      pY: Double,\n      default: Double // for words absent in PXY\n    ):RDD[(String, Double)] = \n      pXs.leftOuterJoin(pXYs).mapValues { \n        case (pX, Some(pXY)) => pXY * math.log(pXY/(pX*pY)) \n        case (pX, None) => default * math.log(default/(pX*pY))\n    }\n```", "```py\n    val miTerms = List(\n      miTerm(pPresentAndSpam, pPresent, pSpam, 0.5/nSpam * pSpam),\n      miTerm(pPresentAndHam, pPresent, pHam, 0.5/nHam * pHam),\n      miTerm(pAbsentAndSpam, pAbsent, pSpam, 0.5/nSpam * pSpam),\n      miTerm(pAbsentAndHam, pAbsent, pHam, 0.5/nHam * pHam)\n    )\n```", "```py\n    val mutualInformation = miTerms.reduce { \n      (term1, term2) => term1.join(term2).mapValues { \n         case (l, r) => l + r \n      } \n    }\n```", "```py\n    mutualInformation.takeOrdered(20)(Ordering.by { - _._2 })\n      .foreach { println }\n```", "```py\n$ sbt package\n$ spark-submit target/scala-2.10/spam_mi_2.10-0.1-SNAPSHOT.jar\n(!,0.1479941771292119)\n(language,0.14574624861510874)\n(remove,0.11380645864246142)\n(free,0.1073496947123657)\n(university,0.10695975885487692)\n(money,0.07531772498093084)\n(click,0.06887598051593441)\n(our,0.058950906866052394)\n(today,0.05485248095680509)\n(sell,0.05385519653184113)\n(english,0.053509319455430575)\n(business,0.05299311289740539)\n(market,0.05248394151802276)\n(product,0.05096229706182162)\n(million,0.050233193237964546)\n(linguistics,0.04990172586630499)\n(internet,0.04974101556655623)\n(company,0.04941817269989519)\n(%,0.04890193809823071)\n(save,0.04861393414892205)\n```", "```py\nval nMessages = messages.count()\n```", "```py\n$ spark-shell\n\n```", "```py\nscala> case class PatientReadings(\n val patientId: Int,\n val heightCm: Int,\n val weightKg: Int,\n val age:Int, \n val isSmoker:Boolean \n)\ndefined class PatientReadings\n\n```", "```py\nscala> val readings = List(\n PatientReadings(1, 175, 72, 43, false),\n PatientReadings(2, 182, 78, 28, true),\n PatientReadings(3, 164, 61, 41, false),\n PatientReadings(4, 161, 62, 43, true)\n)\nList[PatientReadings] = List(...\n\n```", "```py\nscala> val readingsRDD = sc.parallelize(readings)\nreadingsRDD: RDD[PatientReadings] = ParallelCollectionRDD[0] at parallelize at <console>:25\n\n```", "```py\nscala> val readingsDF = readingsRDD.toDF\nreadingsDF: sql.DataFrame = [patientId: int, heightCm: int, weightKg: int, age: int, isSmoker: boolean]\n\n```", "```py\nscala> readingsDF.show\n+---------+--------+--------+---+--------+\n|patientId|heightCm|weightKg|age|isSmoker|\n+---------+--------+--------+---+--------+\n|        1|     175|      72| 43|   false|\n|        2|     182|      78| 28|    true|\n|        3|     164|      61| 41|   false|\n|        4|     161|      62| 43|    true|\n+---------+--------+--------+---+--------+\n\n```", "```py\nscala> val heightM = readingsDF(\"heightCm\") / 100.0 \nheightM: sql.Column = (heightCm / 100.0)\n\n```", "```py\nscala> val bmi = readingsDF(\"weightKg\") / (heightM*heightM)\nbmi: sql.Column = (weightKg / ((heightCm / 100.0) * (heightCm / 100.0)))\n\n```", "```py\nscala> val readingsWithBmiDF = readingsDF.withColumn(\"BMI\", bmi)\nreadingsWithBmiDF: sql.DataFrame = [heightCm: int, weightKg: int, age: int, isSmoker: boolean, BMI: double]\n\n```", "```py\nscala> readingsWithBmiDF.show\n+---------+--------+--------+---+--------+------------------+\n|patientId|heightCm|weightKg|age|isSmoker|               BMI|\n+---------+--------+--------+---+--------+------------------+\n|        1|     175|      72| 43|   false|23.510204081632654|\n|        2|     182|      78| 28|    true| 23.54788069073783|\n|        3|     164|      61| 41|   false|22.679952409280194|\n|        4|     161|      62| 43|    true|  23.9188302920412|\n+---------+--------+--------+---+--------+------------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter {\n readingsWithBmiDF(\"isSmoker\") \n}.show\n+---------+--------+--------+---+--------+-----------------+\n|patientId|heightCm|weightKg|age|isSmoker|              BMI|\n+---------+--------+--------+---+--------+-----------------+\n|        2|     182|      78| 28|    true|23.54788069073783|\n|        4|     161|      62| 43|    true| 23.9188302920412|\n+---------+--------+--------+---+--------+-----------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter { \n readingsWithBmiDF(\"weightKg\") > 70 \n}.show\n+---------+--------+--------+---+--------+------------------+\n|patientId|heightCm|weightKg|age|isSmoker|               BMI|\n+---------+--------+--------+---+--------+------------------+\n|        1|     175|      72| 43|   false|23.510204081632654|\n|        2|     182|      78| 28|    true| 23.54788069073783|\n+---------+--------+--------+---+--------+------------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter { $\"weightKg\" > 70 }.show\n+---------+--------+--------+---+--------+------------------+\n|patientId|heightCm|weightKg|age|isSmoker|               BMI|\n+---------+--------+--------+---+--------+------------------+\n|        1|     175|      72| 43|   false|23.510204081632654|\n|        2|     182|      78| 28|    true| 23.54788069073783|\n+---------+--------+--------+---+--------+------------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter(\"isSmoker\").show\n+---------+--------+--------+---+--------+-----------------+\n|patientId|heightCm|weightKg|age|isSmoker|              BMI|\n+---------+--------+--------+---+--------+-----------------+\n|        2|     182|      78| 28|    true|23.54788069073783|\n|        4|     161|      62| 43|    true| 23.9188302920412|\n+---------+--------+--------+---+--------+-----------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter { $\"age\" === 28 }.show\n+---------+--------+--------+---+--------+-----------------+\n|patientId|heightCm|weightKg|age|isSmoker|              BMI|\n+---------+--------+--------+---+--------+-----------------+\n|        2|     182|      78| 28|    true|23.54788069073783|\n+---------+--------+--------+---+--------+-----------------+\n\n```", "```py\nscala> readingsWithBmiDF.filter { $\"age\" !== 28 }.show\n+---------+--------+--------+---+--------+------------------+\n|patientId|heightCm|weightKg|age|isSmoker|               BMI|\n+---------+--------+--------+---+--------+------------------+\n|        1|     175|      72| 43|   false|23.510204081632654|\n|        3|     164|      61| 41|   false|22.679952409280194|\n|        4|     161|      62| 43|    true|  23.9188302920412|\n+---------+--------+--------+---+--------+------------------+\n\n```", "```py\nscala> val smokingDF = readingsWithBmiDF.groupBy(\n \"isSmoker\").agg(avg(\"BMI\"))\nsmokingDF: org.apache.spark.sql.DataFrame = [isSmoker: boolean, AVG(BMI): double]\n\n```", "```py\nscala> smokingDF.show\n+--------+------------------+\n|isSmoker|          AVG(BMI)|\n+--------+------------------+\n|    true|23.733355491389517|\n|   false|23.095078245456424|\n+--------+------------------+\n\n```", "```py\nscala> readingsDF.groupBy(\"isSmoker\").agg { \n avg($\"heightCm\"/100.0) \n}.show\n+--------+-----------------------+\n|isSmoker|AVG((heightCm / 100.0))|\n+--------+-----------------------+\n|    true|                  1.715|\n|   false|     1.6949999999999998|\n+--------+-----------------------+\n\n```", "```py\nscala> readingsDF.groupBy(floor($\"age\"/10)).agg(count(\"*\")).show\n+-----------------+--------+\n|FLOOR((age / 10))|count(1)|\n+-----------------+--------+\n|              4.0|       3|\n|              2.0|       1|\n+-----------------+--------+\n\n```", "```py\nscala> val bloodPressures = List((1 -> 110), (3 -> 100), (4 -> 125))\nbloodPressures: List[(Int, Int)] = List((1,110), (3,100), (4,125))\n\nscala> val bloodPressureRDD = sc.parallelize(bloodPressures)\nres16: rdd.RDD[(Int, Int)] = ParallelCollectionRDD[74] at parallelize at <console>:24\n\n```", "```py\nscala> val bloodPressureDF = bloodPressureRDD.toDF(\n \"patientId\", \"bloodPressure\")\nbloodPressureDF: DataFrame = [patientId: int, bloodPressure: int]\n\nscala> bloodPressureDF.show\n+---------+-------------+\n|patientId|bloodPressure|\n+---------+-------------+\n|        1|          110|\n|        3|          100|\n|        4|          125|\n+---------+-------------+\n\n```", "```py\nscala> readingsDF.join(bloodPressureDF, \n readingsDF(\"patientId\") === bloodPressureDF(\"patientId\")\n).show\n+---------+--------+--------+---+--------+---------+-------------+\n|patientId|heightCm|weightKg|age|isSmoker|patientId|bloodPressure|\n+---------+--------+--------+---+--------+---------+-------------+\n|        1|     175|      72| 43|   false|        1|          110|\n|        3|     164|      61| 41|   false|        3|          100|\n|        4|     161|      62| 43|    true|        4|          125|\n+---------+--------+--------+---+--------+---------+-------------+\n\n```", "```py\nscala> readingsDF.join(bloodPressureDF,\n readingsDF(\"patientId\") === bloodPressureDF(\"patientId\"),\n \"leftouter\"\n).show\n+---------+--------+--------+---+--------+---------+-------------+\n|patientId|heightCm|weightKg|age|isSmoker|patientId|bloodPressure|\n+---------+--------+--------+---+--------+---------+-------------+\n|        1|     175|      72| 43|   false|        1|          110|\n|        2|     182|      78| 28|    true|     null|         null|\n|        3|     164|      61| 41|   false|        3|          100|\n|        4|     161|      62| 43|    true|        4|          125|\n+---------+--------+--------+---+--------+---------+-------------+\n\n```", "```py\nscala> def likelyMale(height:Int, weight:Int):Boolean = {\n val rescaledHeight = (height - 171.0)/8.95\n val rescaledWeight = (weight - 65.7)/13.4\n -0.75 + 2.48*rescaledHeight + 2.23*rescaledWeight > 0\n}\n\n```", "```py\nscala> val likelyMaleUdf = sqlContext.udf.register(\n \"likelyMaleUdf\", likelyMale _)\nlikelyMaleUdf: org.apache.spark.sql.UserDefinedFunction = UserDefinedFunction(<function2>,BooleanType,List())\n\n```", "```py\nscala> val likelyMaleColumn = likelyMaleUdf(\n readingsDF(\"heightCm\"), readingsDF(\"weightKg\"))\nlikelyMaleColumn: org.apache.spark.sql.Column = UDF(heightCm,weightKg)\n\nscala> readingsDF.withColumn(\"likelyMale\", likelyMaleColumn).show\n+---------+--------+--------+---+--------+----------+\n|patientId|heightCm|weightKg|age|isSmoker|likelyMale|\n+---------+--------+--------+---+--------+----------+\n|        1|     175|      72| 43|   false|      true|\n|        2|     182|      78| 28|    true|      true|\n|        3|     164|      61| 41|   false|     false|\n|        4|     161|      62| 43|    true|     false|\n+---------+--------+--------+---+--------+----------+\n\n```", "```py\nscala> readingsDF.filter(\n ! likelyMaleUdf($\"heightCm\", $\"weightKg\")\n).show\n+---------+--------+--------+---+--------+\n|patientId|heightCm|weightKg|age|isSmoker|\n+---------+--------+--------+---+--------+\n|        3|     164|      61| 41|   false|\n|        4|     161|      62| 43|    true|\n+---------+--------+--------+---+--------+\n\n```", "```py\nscala> readingsDF.persist\nreadingsDF.type = [patientId: int, heightCm: int,...]\n\n```", "```py\nscala> import org.apache.spark.storage.StorageLevel\nimport org.apache.spark.storage.StorageLevel\n\nscala> readingsDF.persist(StorageLevel.MEMORY_AND_DISK)\nreadingsDF.type = [patientId: int, heightCm: int, ...]\n\n```", "```py\nscala> readingsDF.registerTempTable(\"readings\")\n\n```", "```py\nscala> sqlContext.tables\nDataFrame = [tableName: string, isTemporary: boolean]\n\n```", "```py\nscala> sqlContext.tables.show\n+---------+-----------+\n|tableName|isTemporary|\n+---------+-----------+\n| readings|       true|\n+---------+-----------+\n\n```", "```py\nscala> sqlContext.sql(\"SELECT * FROM readings\").show\n+---------+--------+--------+---+--------+\n|patientId|heightCm|weightKg|age|isSmoker|\n+---------+--------+--------+---+--------+\n|        1|     175|      72| 43|   false|\n|        2|     182|      78| 28|    true|\n|        3|     164|      61| 41|   false|\n|        4|     161|      62| 43|    true|\n+---------+--------+--------+---+--------+\n\n```", "```py\nscala> sqlContext.sql(\"\"\"\nSELECT \n patientId, \n likelyMaleUdf(heightCm, weightKg) AS likelyMale\nFROM readings\n\"\"\").show\n+---------+----------+\n|patientId|likelyMale|\n+---------+----------+\n|        1|      true|\n|        2|      true|\n|        3|     false|\n|        4|     false|\n+---------+----------+\n\n```", "```py\ncase class Weapon(name:String, weaponType:String)\ncase class LotrCharacter(name:String, val weapon:Weapon)\n```", "```py\nscala> val characters = List(\n LotrCharacter(\"Gandalf\", Weapon(\"Glamdring\", \"sword\")),\n LotrCharacter(\"Frodo\", Weapon(\"Sting\", \"dagger\")),\n LotrCharacter(\"Aragorn\", Weapon(\"Anduril\", \"sword\"))\n)\ncharacters: List[LotrCharacter] = List(LotrCharacter...\n\nscala> val charactersDF = sc.parallelize(characters).toDF\ncharactersDF: DataFrame = [name: string, weapon: struct<name:string,weaponType:string>]\n\nscala> charactersDF.printSchema\nroot\n |-- name: string (nullable = true)\n |-- weapon: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- weaponType: string (nullable = true)\n\nscala> charactersDF.show\n+-------+-----------------+\n|   name|           weapon|\n+-------+-----------------+\n|Gandalf|[Glamdring,sword]|\n|  Frodo|   [Sting,dagger]|\n|Aragorn|  [Anduril,sword]|\n+-------+-----------------+\n\n```", "```py\nscala> val weaponTypeColumn = charactersDF(\"weapon\")(\"weaponType\")\nweaponTypeColumn: org.apache.spark.sql.Column = weapon[weaponType]\n\n```", "```py\nscala> charactersDF.filter { weaponTypeColumn === \"sword\" }.show\n+-------+-----------------+\n|   name|           weapon|\n+-------+-----------------+\n|Gandalf|[Glamdring,sword]|\n|Aragorn|  [Anduril,sword]|\n+-------+-----------------+\n\n```", "```py\nscala> case class PatientNumbers(\n patientId:Int, phoneNumbers:List[String])\ndefined class PatientNumbers\n\nscala> val numbers = List(\n PatientNumbers(1, List(\"07929123456\")),\n PatientNumbers(2, List(\"07929432167\", \"07929234578\")),\n PatientNumbers(3, List.empty),\n PatientNumbers(4, List(\"07927357862\"))\n)\n\nscala> val numbersDF = sc.parallelize(numbers).toDF\nnumbersDF: org.apache.spark.sql.DataFrame = [patientId: int, phoneNumbers: array<string>]\n\n```", "```py\nscala> numbersDF.printSchema\nroot\n |-- patientId: integer (nullable = false)\n |-- phoneNumbers: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n```", "```py\nscala> val bestNumberColumn = numbersDF(\"phoneNumbers\")(0)\nbestNumberColumn: org.apache.spark.sql.Column = phoneNumbers[0]\n\nscala> numbersDF.withColumn(\"bestNumber\", bestNumberColumn).show\n+---------+--------------------+-----------+\n|patientId|        phoneNumbers| bestNumber|\n+---------+--------------------+-----------+\n|        1|   List(07929123456)|07929123456|\n|        2|List(07929432167,...|07929432167|\n|        3|              List()|       null|\n|        4|   List(07927357862)|07927357862|\n+---------+--------------------+-----------+\n\n```", "```py\nscala> val df = sqlContext.read.json(\"odersky_repos.json\")\ndf: DataFrame = [archive_url: string, assignees_url: ...]\n\n```", "```py\nscala> val reposDF = df.select(\"name\", \"language\", \"fork\", \"owner\")\nreposDF: DataFrame = [name: string, language: string, ...] \n\nscala> reposDF.show\n+----------------+----------+-----+--------------------+\n|            name|  language| fork|               owner|\n+----------------+----------+-----+--------------------+\n|           dotty|     Scala| true|[https://avatars....|\n|        frontend|JavaScript| true|[https://avatars....|\n|           scala|     Scala| true|[https://avatars....|\n|      scala-dist|     Scala| true|[https://avatars....|\n|scala.github.com|JavaScript| true|[https://avatars....|\n|          scalax|     Scala|false|[https://avatars....|\n|            sips|       CSS|false|[https://avatars....|\n+----------------+----------+-----+--------------------+\n\n```", "```py\nscala> reposDF.write.json(\"repos_short.json\")\n\n```", "```py\nscala> import org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.SaveMode\n\nscala> reposDF.write.mode(\n SaveMode.Overwrite).json(\"repos_short.json\")\n\n```", "```py\nscala> reposDF.write.parquet(\"repos_short.parquet\")\n\nscala> val newDF = sqlContext.read.parquet(\"repos_short.parquet\")\nnewDF: DataFrame = [name: string, language: string, fo...]\n\nscala> newDF.show\n+----------------+----------+-----+--------------------+\n|            name|  language| fork|               owner|\n+----------------+----------+-----+--------------------+\n|           dotty|     Scala| true|[https://avatars....|\n|        frontend|JavaScript| true|[https://avatars....|\n|           scala|     Scala| true|[https://avatars....|\n|      scala-dist|     Scala| true|[https://avatars....|\n|scala.github.com|JavaScript| true|[https://avatars....|\n|          scalax|     Scala|false|[https://avatars....|\n|            sips|       CSS|false|[https://avatars....|\n+----------------+----------+-----+--------------------+\n\n```", "```py\nval conf = new SparkConf().setAppName(\"applicationName\")\nval sc = new SparkContext(conf)\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n```", "```py\nimport sqlContext.implicits._\n\n```", "```py\nscala> val spamText = sc.wholeTextFiles(\"spam/*\")\nspamText: RDD[(String, String)] = spam/...\n\nscala> val hamText = sc.wholeTextFiles(\"ham/*\")\nhamText: RDD[(String, String)] = ham/...\n\n```", "```py\nscala> spamText.first\n(String, String) =\n(file:spam/spmsga1.txt,\"Subject: great part-time summer job! ...\")\n\nscala> spamText.count\nLong = 481\n\n```", "```py\nscala> case class LabelledDocument(\n fileName:String, \n text:String, \n category:String\n)\ndefined class LabelledDocument\n\nscala> val spamDocuments = spamText.map {\n case (fileName, text) => \n LabelledDocument(fileName, text, \"spam\")\n}\nspamDocuments: RDD[LabelledDocument] = MapPartitionsRDD[2] at map\n\nscala> val hamDocuments = hamText.map {\n case (fileName, text) => \n LabelledDocument(fileName, text, \"ham\")\n}\nhamDocuments: RDD[LabelledDocument] = MapPartitionsRDD[3] at map\n\n```", "```py\nscala> val allDocuments = spamDocuments.union(hamDocuments)\nallDocuments: RDD[LabelledDocument] = UnionRDD[4] at union\n\nscala> val documentsDF = allDocuments.toDF\ndocumentsDF: DataFrame = [fileName: string, text: string, category: string]\n\n```", "```py\nscala> documentsDF.persist\ndocumentsDF.type = [fileName: string, text: string, category: string]\n\nscala> documentsDF.show\n+--------------------+--------------------+--------+\n|            fileName|                text|category|\n+--------------------+--------------------+--------+\n|file:/Users/pasca...|Subject: great pa...|    spam|\n|file:/Users/pasca...|Subject: auto ins...|    spam|\n|file:/Users/pasca...|Subject: want bes...|    spam|\n|file:/Users/pasca...|Subject: email 57...|    spam|\n|file:/Users/pasca...|Subject: n't miss...|    spam|\n|file:/Users/pasca...|Subject: amaze wo...|    spam|\n|file:/Users/pasca...|Subject: help loa...|    spam|\n|file:/Users/pasca...|Subject: beat irs...|    spam|\n|file:/Users/pasca...|Subject: email 57...|    spam|\n|file:/Users/pasca...|Subject: best , b...|    spam|\n|...                                               |\n+--------------------+--------------------+--------+\n\nscala> documentsDF.groupBy(\"category\").agg(count(\"*\")).show\n+--------+--------+\n|category|COUNT(1)|\n+--------+--------+\n|    spam|     481|\n|     ham|    2412|\n+--------+--------+\n\n```", "```py\nscala> val Array(trainDF, testDF) = documentsDF.randomSplit(\n Array(0.7, 0.3))\ntrainDF: DataFrame = [fileName: string, text: string, category: string]\ntestDF: DataFrame = [fileName: string, text: string, category: string]\n\n```", "```py\nscala> trainDF.count / documentsDF.count.toDouble\nDouble = 0.7013480815762184\n\n```", "```py\nscala> import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.feature._\n\nscala> val tokenizer = new Tokenizer()\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_75559f60e8cf \n\n```", "```py\nscala> println(tokenizer.explainParams)\ninputCol: input column name (undefined)\noutputCol: output column name (default: tok_75559f60e8cf__output)\n\n```", "```py\nscala> tokenizer.setInputCol(\"text\").setOutputCol(\"words\")\norg.apache.spark.ml.feature.Tokenizer = tok_75559f60e8cf\n\n```", "```py\nscala> val tokenizedDF = tokenizer.transform(trainDF)\ntokenizedDF: DataFrame = [fileName: string, text: string, category: string, words: array<string>]\n\nscala> tokenizedDF.show\n+--------------+----------------+--------+--------------------+\n|      fileName|            text|category|               words|\n+--------------+----------------+--------+--------------------+\n|file:/Users...|Subject: auto...|    spam|[subject:, auto, ...|\n|file:/Users...|Subject: want...|    spam|[subject:, want, ...|\n|file:/Users...|Subject: n't ...|    spam|[subject:, n't, m...|\n|file:/Users...|Subject: amaz...|    spam|[subject:, amaze,...|\n|file:/Users...|Subject: help...|    spam|[subject:, help, ...|\n|file:/Users...|Subject: beat...|    spam|[subject:, beat, ...|\n|...                                                          |\n+--------------+----------------+--------+--------------------+\n\n```", "```py\nscala> val words = Array(\"the\", \"dog\", \"jumped\", \"over\", \"the\")\nwords: Array[String] = Array(the, dog, jumped, over, the)\n\nscala> val hashCodes = words.map { _.## }\nhashCodes: Array[Int] = Array(114801, 99644, -1148867251, 3423444, 114801)\n\n```", "```py\nscala> val indices = hashCodes.map { code => Math.abs(code % 16) }\nindices: Array[Int] = Array(1, 12, 3, 4, 1)\n\n```", "```py\nscala> val indexFrequency = indices.groupBy(identity).mapValues {\n _.size.toDouble\n}\nindexFrequency: Map[Int,Double] = Map(4 -> 1.0, 1 -> 2.0, 3 -> 1.0, 12 -> 1.0)\n\n```", "```py\nscala> import org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.linalg._\n\nscala> val termFrequencies = Vectors.sparse(16, indexFrequency.toSeq)\ntermFrequencies: linalg.Vector = (16,[1,3,4,12],[2.0,1.0,1.0,1.0])\n\n```", "```py\nscala> val hashingTF = (new HashingTF()\n .setInputCol(\"words\")\n .setOutputCol(\"features\")\n .setNumFeatures(1048576))\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_3b78eca9595c\n\nscala> val hashedDF = hashingTF.transform(tokenizedDF)\nhashedDF: DataFrame = [fileName: string, text: string, category: string, words: array<string>, features: vector]\n\nscala> hashedDF.select(\"features\").show\n+--------------------+\n|            features|\n+--------------------+\n|(1048576,[0,33,36...|\n|(1048576,[0,36,40...|\n|(1048576,[0,33,34...|\n|(1048576,[0,33,36...|\n|(1048576,[0,33,34...|\n|(1048576,[0,33,34...|\n+--------------------+\n\n```", "```py\nscala> import org.apache.spark.sql.Row\nimport org.apache.spark.sql.Row\n\nscala> val firstRow = hashedDF.select(\"features\").first\nfirstRow: org.apache.spark.sql.Row = ...\n\nscala> val Row(v:Vector) = firstRow\nv: Vector = (1048576,[0,33,36,37,...],[1.0,3.0,4.0,1.0,...])\n\n```", "```py\nscala> val indexer = (new StringIndexer()\n .setInputCol(\"category\")\n .setOutputCol(\"label\"))\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_16db03fd0546\n\nscala> val indexTransform = indexer.fit(trainDF)\nindexTransform: StringIndexerModel = strIdx_16db03fd0546\n\n```", "```py\nscala> indexTransform.labels\nArray[String] = Array(ham, spam)\n\n```", "```py\nscala> val labelledDF = indexTransform.transform(hashedDF)\nlabelledDF: org.apache.spark.sql.DataFrame = [fileName: string, text: string, category: string, words: array<string>, features: vector, label: double]\n\nscala> labelledDF.select(\"category\", \"label\").distinct.show\n+--------+-----+\n|category|label|\n+--------+-----+\n|     ham|  0.0|\n|    spam|  1.0|\n+--------+-----+\n\n```", "```py\nscala> import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.LogisticRegression\n\nscala> val classifier = new LogisticRegression().setMaxIter(50)\nclassifier: LogisticRegression = logreg_a5e921e7c1a1 \n\n```", "```py\nscala> println(classifier.explainParams)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfitIntercept: whether to fit an intercept term (default: true)\nlabelCol: label column name (default: label)\nmaxIter: maximum number of iterations (>= 0) (default: 100, current: 50)\nregParam: regularization parameter (>= 0) (default: 0.0)\nthreshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\ntol: the convergence tolerance for iterative algorithms (default: 1.0E-6)\n...\n\n```", "```py\nscala> val trainedClassifier = classifier.fit(labelledDF)\ntrainedClassifier: LogisticRegressionModel = logreg_353d18f6a5f0\n\n```", "```py\nscala> val labelledDFWithPredictions = trainedClassifier.transform(\n labelledDF)\nlabelledDFWithPredictions: DataFrame = [fileName: string, ...\n\nscala> labelledDFWithPredictions.select($\"label\", $\"prediction\").show\n+-----+----------+\n|label|prediction|\n+-----+----------+\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n+-----+----------+\n\n```", "```py\nscala> labelledDFWithPredictions.filter { \n $\"label\" !== $\"prediction\" \n}.count\nLong = 1\n\n```", "```py\nscala> import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.Pipeline\n\nscala> val pipeline = new Pipeline().setStages(\n Array(indexer, tokenizer, hashingTF, classifier)\n)\npipeline: Pipeline = pipeline_7488113e284d\n\n```", "```py\nscala> val fittedPipeline = pipeline.fit(trainDF)\nfittedPipeline: org.apache.spark.ml.PipelineModel = pipeline_089525c6f100\n\n```", "```py\nscala> val testDFWithPredictions = fittedPipeline.transform(testDF)\ntestDFWithPredictions: DataFrame = [fileName: string, ...\n\n```", "```py\nscala> testDFWithPredictions.filter { \n $\"label\" !== $\"prediction\" \n}.count\nLong = 20\n\n```", "```py\nscala> import org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.SaveMode\n\nscala> (labelledDFWithPredictions\n .select(\"fileName\", \"label\", \"prediction\", \"probability\")\n .write.mode(SaveMode.Overwrite)\n .parquet(\"transformedTrain.parquet\"))\n\nscala> (testDFWithPredictions\n .select(\"fileName\", \"label\", \"prediction\", \"probability\")\n .write.mode(SaveMode.Overwrite)\n .parquet(\"transformedTest.parquet\"))\n\n```", "```py\n// build.sbt\nname := \"spam_filter\"\n\nscalaVersion := \"2.10.5\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.5.2\" % \"provided\",\n  \"org.apache.spark\" %% \"spark-mllib\" % \"1.5.2\" % \"provided\",\n  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-viz\" % \"0.11.2\",\n  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n)\n```", "```py\n$ sbt assembly\n\n```", "```py\n$ spark-shell --jars=target/scala-2.10/spam_filter-assembly-0.1-SNAPSHOT.jar\n\n```", "```py\nscala> import breeze.plot._\nimport breeze.plot._\n\n```", "```py\nscala> val testDFWithPredictions = sqlContext.read.parquet(\n \"transformedTest.parquet\")\ntestDFWithPredictions: org.apache.spark.sql.DataFrame = [fileName: string, label: double, prediction: double, probability: vector]\n\n```", "```py\nscala> import org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vector\n\nscala> import org.apache.spark.sql.Row\nimport org.apache.spark.sql.Row\n\nscala> val scoresLabels = testDFWithPredictions.select(\n \"probability\", \"label\").map {\n case Row(probability:Vector, label:Double) => \n (probability(1), label)\n}\norg.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[3] at map at <console>:23\n\nscala> scoresLabels.take(5).foreach(println)\n(0.9999999967713409,1.0)\n(0.9999983827108793,1.0)\n(0.9982059900606365,1.0)\n(0.9999790713978142,1.0)\n(0.9999999999999272,1.0)\n\n```", "```py\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport mllib.evaluation.BinaryClassificationMetrics\n\nscala> val bm = new BinaryClassificationMetrics(scoresLabels)\nbm: BinaryClassificationMetrics = mllib.evaluation.BinaryClassificationMetrics@254ed9ba\n\n```", "```py\nscala> val rocArray = bm.roc.collect\nrocArray: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.16793893129770993), ...\n\n```", "```py\nscala> val falsePositives = rocArray.map { _._1 }\nfalsePositives: Array[Double] = Array(0.0, 0.0, 0.0, 0.0, 0.0, ...\n\nscala> val truePositives = rocArray.map { _._2 }\ntruePositives: Array[Double] = Array(0.0, 0.16793893129770993, 0.19083969465...\n\n```", "```py\nscala> import breeze.plot._\nimport breeze.plot.\n\nscala> val f = Figure()\nf: breeze.plot.Figure = breeze.plot.Figure@3aa746cd\n\nscala> val p = f.subplot(0)\np: breeze.plot.Plot = breeze.plot.Plot@5ed1438a\n\nscala> p += plot(falsePositives, truePositives)\np += plot(falsePositives, truePositives)\n\nscala> p.xlabel = \"false positives\"\np.xlabel: String = false positives\n\nscala> p.ylabel = \"true positives\"\np.ylabel: String = true positives\n\nscala> p.title = \"ROC\"\np.title: String = ROC\n\nscala> f.refresh\n\n```", "```py\nscala> p.xlim = (0.0, 0.1)\np.xlim: (Double, Double) = (0.0,0.1)\n\n```", "```py\nscala> import org.jfree.chart.axis.NumberTickUnit\nimport org.jfree.chart.axis.NumberTickUnit\n\nscala> p.xaxis.setTickUnit(new NumberTickUnit(0.01))\n\nscala> p.yaxis.setTickUnit(new NumberTickUnit(0.1))\n\n```", "```py\nscala> f.saveas(\"roc.png\")\n\n```", "```py\nscala> bm.areaUnderROC\nres21: Double = 0.9983061235861147\n\n```", "```py\nscala> val lrWithRegularization = (new LogisticRegression()\n .setMaxIter(50))\nlrWithRegularization: LogisticRegression = logreg_16b65b325526\n\nscala> lrWithRegularization.setElasticNetParam(0) lrWithRegularization.type = logreg_1e3584a59b3a\n\n```", "```py\nscala> val lambdas = Array(0.0, 1.0E-12, 1.0E-10, 1.0E-8)\nlambdas: Array[Double] = Array(0.0, 1.0E-12, 1.0E-10, 1.0E-8)\n\nscala> lambdas foreach { lambda =>\n lrWithRegularization.setRegParam(lambda)\n val pipeline = new Pipeline().setStages(\n Array(indexer, tokenizer, hashingTF, lrWithRegularization))\n val model = pipeline.fit(trainDF)\n val transformedTest = model.transform(testDF)\n val classificationError = transformedTest.filter { \n $\"prediction\" !== $\"label\"\n }.count\n println(s\"$lambda => $classificationError\")\n}\n0 => 20\n1.0E-12 => 20\n1.0E-10 => 20\n1.0E-8 => 23\n\n```", "```py\nscala> import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n\nscala> val paramGridBuilder = new ParamGridBuilder()\nparamGridBuilder: ParamGridBuilder = ParamGridBuilder@1dd694d0\n\n```", "```py\nscala> val lambdas = Array(0.0, 1.0E-12, 1.0E-10, 1.0E-8)\nArray[Double] = Array(0.0, 1.0E-12, 1.0E-10, 1.0E-8)\n\nscala> val elasticNetParams = Array(0.0, 1.0)\nelasticNetParams: Array[Double] = Array(0.0, 1.0)\n\nscala> paramGridBuilder.addGrid(\n lrWithRegularization.regParam, lambdas).addGrid(\n lrWithRegularization.elasticNetParam, elasticNetParams)\nparamGridBuilder.type = ParamGridBuilder@1dd694d0\n\n```", "```py\nscala> val paramGrid = paramGridBuilder.build\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n logreg_f7dfb27bed7d-elasticNetParam: 0.0,\n logreg_f7dfb27bed7d-regParam: 0.0\n}, {\n logreg_f7dfb27bed7d-elasticNetParam: 1.0,\n logreg_f7dfb27bed7d-regParam: 0.0\n} ...)\n\nscala> paramGrid.length\nInt = 8\n\n```", "```py\nscala> import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n\nscala> val evaluator = new BinaryClassificationEvaluator()\nevaluator: BinaryClassificationEvaluator = binEval_64b08538f1a2\n\nscala> println(evaluator.explainParams)\nlabelCol: label column name (default: label)\nmetricName: metric name in evaluation (areaUnderROC|areaUnderPR) (default: areaUnderROC)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n\n```", "```py\nscala> val pipeline = new Pipeline().setStages(Array(indexer, tokenizer, hashingTF, lrWithRegularization))\npipeline: Pipeline = pipeline_3ed29f72a4cc\n\nscala> val crossval = (new CrossValidator()\n .setEstimator(pipeline)\n .setEvaluator(evaluator)\n .setEstimatorParamMaps(paramGrid)\n .setNumFolds(3))\ncrossval: CrossValidator = cv_5ebfa1143a9d \n\n```", "```py\nscala> val cvModel = crossval.fit(trainDF)\ncvModel: CrossValidatorModel = cv_5ebfa1143a9d\n\n```", "```py\nscala> cvModel.transform(testDF).filter { \n $\"prediction\" !== $\"label\" \n}.count\nLong = 20\n\n```", "```py\nscala> cvModel.avgMetrics\nArray[Double] = Array(0.996427805316161, ...)\n\n```", "```py\nscala> val params2score = cvModel.getEstimatorParamMaps.zip(\n cvModel.avgMetrics)\nArray[(ml.param.ParamMap,Double)] = Array(({\n logreg_8f107aabb304-elasticNetParam: 0.0,\n logreg_8f107aabb304-regParam: 0.0\n},0.996427805316161),...\n\nscala> params2score.foreach {\n case (params, score) => \n val lambda = params(lrWithRegularization.regParam)\n val elasticNetParam = params(\n lrWithRegularization.elasticNetParam)\n val l2Orl1 = if(elasticNetParam == 0.0) \"L2\" else \"L1\"\n println(s\"$l2Orl1, $lambda => $score\")\n}\nL2, 0.0 => 0.996427805316161\nL1, 0.0 => 0.996427805316161\nL2, 1.0E-12 => 0.9964278053175655\nL1, 1.0E-12 => 0.9961429402772803\nL2, 1.0E-10 => 0.9964382546369551\nL1, 1.0E-10 => 0.9962223090037103\nL2, 1.0E-8 => 0.9964159754613495\nL1, 1.0E-8 => 0.9891008277659763\n\n```", "```py\n{\n    product_id: undefined,\n    product_name: undefined,\n    product_price: undefined,\n    ...\n}\n```", "```py\n[{\"name\":\"dotty\",\"language\":\"Scala\",\"is_fork\":true,\"size\":14653},\n{\"name\":\"frontend\",\"language\":\"JavaScript\",\"is_fork\":true,\"size\":392},\n{\"name\":\"legacy-svn-scala\",\"language\":\"Scala\",\"is_fork\":true,\"size\":296706},\n...\n```", "```py\n$ brew install typesafe-activator\n\n```", "```py\n$ ./path/to/activator/activator new\n\n```", "```py\n$ activator new\n\n```", "```py\n app\n    controllers\n       Application.scala\n    views\n        main.scala.html\n        index.scala.html\n build.sbt\n conf\n    application.conf\n    routes\n project\n    build.properties\n    plugins.sbt\n public\n    images\n       favicon.png\n    javascripts\n       hello.js\n    stylesheets\n        main.css\n test\n  ApplicationSpec.scala\n  IntegrationSpec.scala\n\n```", "```py\n$ ./activator\n[ghub-display] $ run\n\n```", "```py\n$ cat conf/routes\n# Home page\nGET     /                           controllers.Application.index\n...\n```", "```py\n// app/controllers/Application.scala\npackage controllers\n\nimport play.api._\nimport play.api.mvc._\n\nclass Application extends Controller {\n\n  def index = Action {\n    Ok(views.html.index(\"Your new application is ready.\"))\n  }\n\n}\n```", "```py\ndef index = Action {\n  Ok(\"hello, world\")\n}\n```", "```py\nAction {\n  Ok(views.html.index(\"Your new application is ready.\")) \n}\n```", "```py\n// app/views/main.scala.html\n@(title: String)(content: Html)\n\n<!DOCTYPE html>\n\n<html lang=\"en\">\n    <head>\n        <title>@title</title>\n        <!-- not so important stuff -->\n    </head>\n    <body>\n        @content\n    </body>\n</html>\n```", "```py\n$ activator console\nscala> import views.html._\nimport views.html._\n\nscala> val title = \"hello\"\ntitle: String = hello\n\nscala> val content = new play.twirl.api.Html(\"<b>World</b>\")\ncontent: play.twirl.api.Html = <b>World</b>\n\nscala> main(title)(content)\nres8: play.twirl.api.HtmlFormat.Appendable =\n<!DOCTYPE html>\n\n<html lang=\"en\">\n <head>\n <title>hello</title>\n <!-- not so important stuff -->\n </head>\n <body>\n <b>World</b>\n </body>\n</html>\n\n```", "```py\n// verb   // end-point              // Scala handler\nGET       /                         controllers.Application.index\n```", "```py\n// app/controllers/Application.scala\n\nclass Application extends Controller {\n\n  ...\n\n  def hello(name:String) = Action {\n    Ok(s\"hello, $name\")\n  }\n}\n```", "```py\n// conf/routes\nGET  /hello/:name             controllers.Application.hello(name)\n```", "```py\nGET /hello/person-:name        controllers.Application.hello(name)\n// ... matches /hello/person-Jim\n\nGET /hello/:name/picture  controllers.Application.pictureFor(name)\n// ... matches /hello/Jim/picture\n\nGET /hello/:first/:last controllers.Application.hello(first, last)\n// ... matches /hello/john/doe\n```", "```py\ndef hello(name:String) = Action {\n  Ok(\"hello, $name\")\n}\n```", "```py\n$ activator console\nscala> import play.api.mvc._\nimport play.api.mvc._\n\nscala> val res = Results.Ok(\"hello, world\")\nres: play.api.mvc.Result = Result(200, Map(Content-Type -> text/plain; charset=utf-8))\n\nscala> res.header.status\nInt = 200\n\nscala> res.header.headers\nMap[String,String] = Map(Content-Type -> text/plain; charset=utf-8)\n\nscala> res.body\nplay.api.libs.iteratee.Enumerator[Array[Byte]] = play.api.libs.iteratee.Enumerator$$anon$18@5fb83873\n\n```", "```py\nscala> import play.api.libs.json._\nimport play.api.libs.json._\n\nscala> val jsonObj = Json.obj(\"hello\" -> \"world\")\njsonObj: play.api.libs.json.JsObject = {\"hello\":\"world\"}\n\nscala> Results.Ok(jsonObj)\nplay.api.mvc.Result = Result(200, Map(Content-Type -> application/json; charset=utf-8))\n\n```", "```py\nscala> val htmlObj = views.html.index(\"hello\")\nhtmlObj: play.twirl.api.HtmlFormat.Appendable =\n\n<!DOCTYPE html>\n\n<html lang=\"en\">\n <head>\n...\n\nscala> Results.Ok(htmlObj)\nplay.api.mvc.Result = Result(200, Map(Content-Type -> text/html; charset=utf-8))\n\n```", "```py\ndef hello(name:String) = Action { request => \n  ...\n}\n```", "```py\ndef hello(name:String) = Action { request =>\n  val title = request.getQueryString(\"title\")\n  val titleString = title.map { _ + \" \" }.getOrElse(\"\")\n  Ok(s\"Hello, $titleString$name\")\n}\n```", "```py\nPOST      /hello            controllers.Application.helloPost\n```", "```py\ndef helloPost = Action(parse.text) { request =>\n  Ok(\"Hello. You told me: \" + request.body)\n}\n```", "```py\n$ curl --data \"I think that Scala is great\" --header \"Content-type:text/plain\"  127.0.0.1:9000/hello\n```", "```py\n// app/models/Repo.scala\n\npackage models\n\ncase class Repo (\n  val name:String,\n  val language:String,\n  val isFork: Boolean,\n  val size: Long\n)\n```", "```py\n// conf/routes\nGET   /api/repos/:username       controllers.Api.repos(username)\n```", "```py\n// app/controllers/Api.scala\npackage controllers\nimport play.api._\nimport play.api.mvc._\nimport play.api.libs.json._\n\nimport models.Repo\n\nclass Api extends Controller {\n\n  // Some dummy data.\n  val data = List[Repo](\n    Repo(\"dotty\", \"Scala\", true, 14315),\n    Repo(\"frontend\", \"JavaScript\", true, 392)\n  )\n\n  // Typeclass for converting Repo -> JSON\n  implicit val writesRepos = new Writes[Repo] {\n    def writes(repo:Repo) = Json.obj(\n      \"name\" -> repo.name,\n      \"language\" -> repo.language,\n      \"is_fork\" -> repo.isFork,\n      \"size\" -> repo.size\n    )\n  }\n\n  // The controller\n  def repos(username:String) = Action {\n\n    val repoArray = Json.toJson(data) \n    // toJson(data) relies on existence of \n    // `Writes[List[Repo]]` type class in scope\n\n    Ok(repoArray)\n  }\n}\n```", "```py\n[{\"name\":\"dotty\",\"language\":\"Scala\",\"is_fork\":true,\"size\":14315},{\"name\":\"frontend\",\"language\":\"JavaScript\",\"is_fork\":true,\"size\":392}]\n```", "```py\ntrait Writes[T] {\n  def writes(obj:T):Json\n}\n```", "```py\n// app/controllers/Api.scala\n\npackage controllers\n\nimport play.api._\nimport play.api.mvc._\nimport play.api.libs.ws.WS // query external APIs\nimport play.api.Play.current\nimport play.api.libs.json._ // parsing JSON\nimport play.api.libs.functional.syntax._\nimport play.api.libs.concurrent.Execution.Implicits.defaultContext\n\nimport models.Repo\n\nclass Api extends Controller {\n\n  // type class for Repo -> Json conversion\n  implicit val writesRepo = new Writes[Repo] {\n    def writes(repo:Repo) = Json.obj(\n      \"name\" -> repo.name,\n      \"language\" -> repo.language,\n      \"is_fork\" -> repo.isFork,\n      \"size\" -> repo.size\n    )\n  }\n\n  // type class for Github Json -> Repo conversion\n  implicit val readsRepoFromGithub:Reads[Repo] = (\n    (JsPath \\ \"name\").read[String] and\n    (JsPath \\ \"language\").read[String] and\n    (JsPath \\ \"fork\").read[Boolean] and\n    (JsPath \\ \"size\").read[Long]\n  )(Repo.apply _)\n\n  // controller\n  def repos(username:String) = Action.async {\n\n    // GitHub URL\n    val url = s\"https://api.github.com/users/$username/repos\"\n    val response = WS.url(url).get() // compose get request\n\n    // \"response\" is a Future\n    response.map { r =>\n      // executed when the request completes\n      if (r.status == 200) {\n\n        // extract a list of repos from the response body\n        val reposOpt = Json.parse(r.body).validate[List[Repo]]\n        reposOpt match {\n          // if the extraction was successful:\n          case JsSuccess(repos, _) => Ok(Json.toJson(repos))\n\n          // If there was an error during the extraction\n          case _ => InternalServerError\n        }\n      }\n      else {\n        // GitHub returned something other than 200\n        NotFound\n      }\n\n    }\n  }\n\n}\n```", "```py\n[{\"name\":\"dotty\",\"language\":\"Scala\",\"is_fork\":true,\"size\":14653},{\"name\":\"frontend\",\"language\":\"JavaScript\",\"is_fork\":true,\"size\":392},...\n```", "```py\nWS.url(url).withHeaders(\"Content-Type\" -> \"application/json\").get()\n```", "```py\nval token = \"2502761d...\"\nWS.url(url).withHeaders(\"Authorization\" -> s\"token $token\").get()\n```", "```py\n$ activator console\n\nscala> import play.api.libs.json._\nimport play.api.libs.json._\n\nscala> val s = \"\"\" \n { \"name\": \"dotty\", \"size\": 150, \"language\": \"Scala\", \"fork\": true }\n\"\"\"\ns: String = \"\n { \"name\": \"dotty\", \"size\": 150, \"language\": \"Scala\", \"fork\": true }\n\"\n\nscala> val parsedJson = Json.parse(s)\nparsedJson: play.api.libs.json.JsValue = {\"name\":\"dotty\",\"size\":150,\"language\":\"Scala\",\"fork\":true}\n\n```", "```py\nscala> parsedJson \\ \"name\"\nplay.api.libs.json.JsLookupResult = JsDefined(\"dotty\")\n\n```", "```py\nscala> parsedJson \\ \"age\"\nplay.api.libs.json.JsLookupResult = JsUndefined('age' is undefined on object: {\"name\":\"dotty\",\"size\":150,\"language\":\"Scala\",\"fork\":true})\n\n```", "```py\nscala> (parsedJson \\ \"name\").validate[String]\nplay.api.libs.json.JsResult[String] = JsSuccess(dotty,) \n\n```", "```py\nscala> (parsedJson \\ \"name\").validate[Int]\ndplay.api.libs.json.JsResult[Int] = JsError(List((,List(ValidationError(List(error.expected.jsnumber),WrappedArray())))))\n\n```", "```py\nscala> (parsedJson \\ \"age\").validate[Int]\nplay.api.libs.json.JsResult[Int] = JsError(List((,List(ValidationError(List('age' is undefined on object: {\"name\":\"dotty\",\"size\":150,\"language\":\"Scala\",\"fork\":true}),WrappedArray())))))\n\n```", "```py\nscala> val name = (parsedJson \\ \"name\").validate[String] match {\n case JsSuccess(n, _) => n\n case JsError(e) => throw new IllegalStateException(\n s\"Error extracting name: $e\")\n}\nname: String = dotty\n\n```", "```py\nscala> import play.api.libs.functional.syntax._\nimport play.api.libs.functional.syntax._\n\nscala> import models.Repo\nimport models.Repo\n\nscala> implicit val readsRepoFromGithub:Reads[Repo] = (\n (JsPath \\ \"name\").read[String] and\n (JsPath \\ \"language\").read[String] and\n (JsPath \\ \"fork\").read[Boolean] and\n (JsPath \\ \"size\").read[Long]\n)(Repo.apply _)\nreadsRepoFromGithub: play.api.libs.json.Reads[models.Repo] = play.api.libs.json.Reads$$anon$8@a198ddb\n\n```", "```py\nscala> val repoOpt = parsedJson.validate[Repo]\nplay.api.libs.json.JsResult[models.Repo] = JsSuccess(Repo(dotty,Scala,true,150),)\n\n```", "```py\nscala> val JsSuccess(repo, _) = repoOpt\nrepo: models.Repo = Repo(dotty,Scala,true,150)\n\n```", "```py\nlibraryDependencies ++= Seq(\n  \"org.webjars\" % \"requirejs\" % \"2.1.22\",\n  \"org.webjars\" % \"jquery\" % \"2.1.4\",\n  \"org.webjars\" % \"underscorejs\" % \"1.8.3\",\n  \"org.webjars\" % \"nvd3\" % \"1.8.1\",\n  \"org.webjars\" % \"d3js\" % \"3.5.6\",\n  \"org.webjars\" % \"bootstrap\" % \"3.3.6\"\n)\n```", "```py\n// app/views/index.scala.html\n<!DOCTYPE html>\n\n<html lang=\"en\">\n  <head>\n    <title>Github User display</title>\n    <link rel=\"stylesheet\" media=\"screen\" \n      href=\"@routes.Assets.versioned(\"stylesheets/main.css\")\">\n    <link rel=\"shortcut icon\" type=\"image/png\"\n      href=\"@routes.Assets.versioned(\"images/favicon.png\")\">\n    <link rel=\"stylesheet\" media=\"screen\" \n      href=@routes.Assets.versioned(\"lib/nvd3/nv.d3.css\") >\n    <link rel=\"stylesheet\" media=\"screen\"\n      href=@routes.Assets.versioned(\n      \"lib/bootstrap/css/bootstrap.css\")>\n  </head>\n\n  <body>\n    <div class=\"container\">\n\n      <!-- Title row -->\n      <div class=\"row\">\n        <h1>Github user search</h1>\n      </div>\n\n      <!-- User search row -->\n      <div class=\"row\">\n        <label>Github user: </label>\n        <input type=\"text\" id=\"user-selection\">\n        <span id=\"searching-span\"></span> <hr />\n      </div>\n\n      <!-- Results row -->\n      <div id=\"response\" class=\"row\"></div>\n    </div>\n  </body>\n</html>\n```", "```py\n# conf/routes\nGET   /      controllers.Application.index\n```", "```py\n// app/controllers/Application.scala\npackage controllers\n\nimport play.api._\nimport play.api.mvc._\n\nclass Application extends Controller {\n\n  def index = Action {\n    Ok(views.html.index())\n  }\n}\n```", "```py\n<script src=@routes.Assets.versioned(\"lib/jquery/jquery.js\") type=\"text/javascript\"></script>\n```", "```py\n// example.js\ndefine([\"jquery\", \"underscore\"], function($, _) {\n\n  // hide a div\n  function hide(div_name) {\n    $(div_name).hide() ;\n  }\n\n  // what the module exports.\n  return { \"hide\": hide }\n\n}) ;\n```", "```py\ndefine([\"example\"], function(example) {\n\n  function hide_all() {\n example.hide(\"#top\") ;\n example.hide(\"#bottom\") ;\n  }\n\n  return { \"hide_all\": hide_all } ;\n});\n```", "```py\nrequire([\"jquery\", \"example\"], function($, example) {\n  $(document).ready(function() {\n    example.hide(\"#header\") ;\n  });\n}) ;\n```", "```py\n// project/plugins.sbt\n\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-rjs\" % \"1.0.7\")\n```", "```py\npipelineStages := Seq(rjs)\n```", "```py\n// index.scala.html\n\n<html>\n  <head>\n...\n\n    <script\n      type=\"text/javascript\"\n      src=@routes.Assets.versioned(\"lib/requirejs/require.js\").url\n      data-main=@routes.Assets.versioned(\"javascripts/main.js\").url>\n    </script>\n\n  </head>\n...\n</html>\n```", "```py\n<script type=\"text/javascript\" \n  data-main=\"/assets/javascripts/main.js\" \n  src=\"img/require.min.js\">\n</script>\n```", "```py\n// public/javascripts/main.js\n\nrequire([], function() {\n  console.log(\"hello, JavaScript\"); \n});\n```", "```py\n// public/javascripts/main.js\n\n(function (requirejs) {\n  'use strict';\n\n  // -- RequireJS config --\n  requirejs.config({\n    // path to the web jars. These definitions allow us \n    // to use \"jquery\", rather than \"../lib/jquery/jquery\",\n    // when defining module dependencies.\n    paths: {\n      \"jquery\": \"../lib/jquery/jquery\",\n      \"underscore\": \"../lib/underscorejs/underscore\",\n      \"d3\": \"../lib/d3js/d3\",\n      \"nvd3\": \"../lib/nvd3/nv.d3\",\n      \"bootstrap\": \"../lib/bootstrap/js/bootstrap\"\n    },\n\n    shim: {\n      // hack to get nvd3 to work with requirejs.\n      // see this so question:\n      // http://stackoverflow.com/questions/13157704/how-to-integrate-d3-with-require-js#comment32647365_13171592        \n      nvd3: {\n        deps: [\"d3.global\"],\n        exports: \"nv\"\n      },\n      bootstrap : { deps :['jquery'] }\n    }\n\n  }) ;\n})(requirejs) ;\n\n// hack to get nvd3 to work with requirejs.\n// see this so question on Stack Overflow:\n// http://stackoverflow.com/questions/13157704/how-to-integrate-d3-with-require-js#comment32647365_13171592\ndefine(\"d3.global\", [\"d3\"], function(d3global) {\n  d3 = d3global;\n});\n\nrequire([], function() {\n  // Our application\n  console.log(\"hello, JavaScript\");\n}) ;\n```", "```py\n// public/javascripts/model.js\n\ndefine([], function(){\n   return {\n    ghubUser: \"\", // last name that was searched for\n    exists: true, // does that person exist on github?\n    repos: [] // list of repos\n  } ;\n});\n```", "```py\n> require([\"model\"], function(model) { console.log(model) ; }) \n{ghubUser: \"odersky\", exists: true, repos: Array}\n\n> require([\"model\"], function(model) { \n console.log(model.repos[0]); \n})\n{name: \"dotty\", language: \"Scala\", is_fork: true, size: 14653}\n\n```", "```py\n> $(window).on(\"custom-event\", function() { \n console.log(\"custom event received\") ; \n});\n\n```", "```py\n> $(window).trigger(\"custom-event\"); \ncustom event received\n\n```", "```py\n// public/javascripts/events.js\n\ndefine([\"jquery\"], function($) {\n\n  var bus = $(window) ; // widget to use as an event bus\n\n  function trigger(eventType) {\n    $(bus).trigger(eventType) ;\n  }\n\n  function on(eventType, f) {\n    $(bus).on(eventType, f) ;\n  }\n\n  return {\n    \"trigger\": trigger,\n    \"on\": on\n  } ;\n});\n```", "```py\n> require([\"events\"], function(events) {\n  // register event listener\n  events.on(\"hello_event\", function() {\n    console.log(\"Received event\") ;\n  }) ;\n}); \n```", "```py\n> require([\"events\"], function(events) {\n  // trigger the event\n  events.trigger(\"hello_event\") ;\n}) ;\n```", "```py\n> $.getJSON(\"/api/repos/odersky\", function(data) { \n console.log(\"API response:\");\n console.log(data);\n console.log(data[0]); \n}) ;\n{readyState: 1, getResponseHeader: function, ...}\n\nAPI response:\n[Object, Object, Object, Object, Object, ...]\n{name: \"dotty\", language: \"Scala\", is_fork: true, size: 14653}\n\n```", "```py\n> $.getJSON(\"/api/repos/junk123456\", function(data) { \n console.log(\"called on success\"); \n}).fail(function() { \n console.log(\"called on failure\") ; \n}) ;\n{readyState: 1, getResponseHeader: function, ...}\n\ncalled on failure\n\n```", "```py\n// public/javascripts/controller.js\ndefine([\"jquery\", \"events\", \"model\"], function($, events, model) {\n\n  function initialize() {\n    $(\"#user-selection\").change(function() {\n\n      var user = $(\"#user-selection\").val() ;\n      console.log(\"Fetching information for \" + user) ;\n\n      // Change cursor to a 'wait' symbol \n      // while we wait for the API to respond\n      $(\"*\").css({\"cursor\": \"wait\"}) ; \n\n      $.getJSON(\"/api/repos/\" + user, function(data) {\n        // Executed on success\n        model.exists = true ;\n        model.repos = data ;\n      }).fail(function() {\n        // Executed on failure\n        model.exists = false ;\n        model.repos = [] ;\n      }).always(function() {\n        // Always executed\n        model.ghubUser = user ;\n\n        // Restore cursor\n        $(\"*\").css({\"cursor\": \"initial\"}) ;\n\n        // Tell the rest of the application \n        // that the model has been updated.\n        events.trigger(\"model_updated\") ;\n      });\n    }) ;\n  } ;\n\n  return { \"initialize\": initialize };\n\n});\n```", "```py\n// public/javascripts/main.js\n\nrequire([\"controller\"], function(controller) {\n  controller.initialize();\n});\n```", "```py\n> require([\"events\", \"model\"], \nfunction(events, model) {\n  events.on(\"model_updated\", function () { \n    console.log(\"model_updated event received\"); \n    console.log(model); \n  });\n}); \n```", "```py\n// public/javascripts/responseView.js\n\ndefine([\"jquery\", \"model\", \"events\"],\nfunction($, model, events) {\n\n  var failedResponseHtml = \n    \"<div class='col-md-12'>Not found</div>\" ;\n\n  function initialize() {\n    events.on(\"model_updated\", function() {\n      if (model.exists) {\n        // success  we will fill this in later.\n        console.log(\"model exists\")\n      }\n      else {\n        // failure  the user entered\n        // is not a valid GitHub login \n        $(\"#response\").html(failedResponseHtml) ;\n      }\n    }) ;\n  }\n\n  return { \"initialize\": initialize } ;\n\n});\n```", "```py\n// public/javascripts/main.js\n\nrequire([\"controller\", \"responseView\"],\nfunction(controller, responseView) {\n  controller.initialize();\n  responseView.initialize() ;\n}) ;\n```", "```py\n// public/javascripts/responseView.js\n\ndefine([\"jquery\", \"model\", \"events\", \"repoTable\", \"repoGraph\"],\nfunction($, model, events, repoTable, repoGraph) {\n\n  // HTHML to inject when the model represents a valid user \n var successfulResponseHtml = \n \"<div class='col-md-6' id='response-table'></div>\" +\n \"<div class='col-md-6' id='response-graph'></div>\" ;\n\n  // HTML to inject when the model is for a non-existent user\n  var failedResponseHtml = \n    \"<div class='col-md-12'>Not found</div>\" ;\n\n  function initialize() {\n    events.on(\"model_updated\", function() {\n      if (model.exists) {\n $(\"#response\").html(successfulResponseHtml) ;\n repoTable.build(model, \"#response-table\") ;\n repoGraph.build(model, \"#response-graph\") ;\n      }\n      else {\n        $(\"#response\").html(failedResponseHtml) ;\n      }\n    }) ;\n  }\n\n  return { \"initialize\": initialize } ;\n\n});\n```", "```py\nvar successfulResponseHtml = \n  \"<div class='col-md-6' id='response-table'></div>\" +\n  \"<div class='col-md-6' id='response-graph'></div>\" ;\n```", "```py\n> require([\"underscore\"], function(_) {\n  var myTemplate = _.template(\n    \"Hello, <%= title %> <%= name %>!\"\n  ) ;\n});\n```", "```py\n> require([\"underscore\"], function(_) {\n  var myTemplate = _.template( ... ); \n  var person = { title: \"Dr.\", name: \"Odersky\" } ;\n  console.log(myTemplate(person)) ;\n});\n```", "```py\n// public/javascripts/repoTable.js\n\ndefine([\"underscore\", \"jquery\"], function(_, $) {\n\n  // Underscore template for each row\n  var rowTemplate = _.template(\"<tr>\" +\n    \"<td><%= name %></td>\" +\n    \"<td><%= language %></td>\" +\n    \"<td><%= size %></td>\" +\n    \"</tr>\") ;\n\n  // template for the table\n  var repoTable = _.template(\n    \"<table id='repo-table' class='table'>\" +\n      \"<thead>\" +\n        \"<tr>\" +\n          \"<th>Name</th><th>Language</th><th>Size</th>\" +\n        \"</tr>\" +\n      \"</thead>\" +\n      \"<tbody>\" +\n        \"<%= tbody %>\" +\n      \"</tbody>\" +\n    \"</table>\") ;\n\n  // Builds a table for a model\n  function build(model, divName) {\n    var tbody = \"\" ;\n    _.each(model.repos, function(repo) {\n      tbody += rowTemplate(repo) ;\n    }) ;\n    var table = repoTable({tbody: tbody}) ;\n    $(divName).html(table) ;\n  }\n\n  return { \"build\": build } ;\n}) ;\n```", "```py\n[ \n  { label: \"Scala\", size: 1234 },\n  { label: \"Python\", size: 4567 }\n]\n```", "```py\n// public/javascripts/repoGraph.js\n\ndefine([\"underscore\", \"d3\", \"nvd3\"], \nfunction(_, d3, nv) {\n\n  // Aggregate the repo size by language.\n  // Returns an array of objects like:\n  // [ { label: \"Scala\", size: 1245}, \n  //   { label: \"Python\", size: 432 } ]\n  function generateDataFromModel(model) {\n\n    // Build an initial object mapping each\n    // language to the repositories written in it\n    var language2Repos = _.groupBy(model.repos, \n      function(repo) { return repo.language ; }) ;\n\n    // Map each { \"language\":  [ list of repos ], ...} \n    // pairs to a single document { \"language\": totalSize }\n    // where totalSize is the sum of the individual repos.\n    var plotObjects = _.map(language2Repos, \n      function(repos, language) {\n        var sizes = _.map(repos, function(repo) { \n          return repo.size; \n        });\n        // Sum over the sizes using 'reduce'\n        var totalSize = _.reduce(sizes, \n          function(memo, size) { return memo + size; },\n        0) ;\n        return { label: language, size: totalSize } ;\n      }) ;\n\n     return plotObjects;\n  }\n```", "```py\n  // Build the chart.\n  function build(model, divName) {\n    var transformedModel = generateDataFromModel(model) ;\n    nv.addGraph(function() {\n\n      var height = 350;\n      var width = 350; \n\n      var chart = nv.models.pieChart()\n        .x(function (d) { return d.label ; })\n        .y(function (d) { return d.size ;})\n        .width(width)\n        .height(height) ;\n\n      d3.select(divName).append(\"svg\")\n        .datum(transformedModel)\n        .transition()\n        .duration(350)\n        .attr('width', width)\n        .attr('height', height)\n        .call(chart) ;\n\n      return chart ;\n    });\n  }\n\n  return { \"build\" : build } ;\n\n});\n```", "```py\nscala> val names = (\"Pascal\", \"Bugnion\")\nnames: (String, String) = (Pascal,Bugnion)\n\n```", "```py\nscala> val (firstName, lastName) = names\nfirstName: String = Pascal\nlastName: String = Bugnion\n\n```", "```py\nscala> val (firstName:String, lastName:String) = names\nfirstName: String = Pascal\nlastName: String = Bugnion\n\n```", "```py\nscala> val (firstName, middleName, lastName) = names\n<console>:13: error: constructor cannot be instantiated to expected type;\nfound   : (T1, T2, T3)\nrequired: (String, String)\n val (firstName, middleName, lastName) = names\n\n```", "```py\nscala> val (\"Pascal\", lastName) = names\nlastName: String = Bugnion\n\n```", "```py\nscala> val point = Array(1, 2, 3)\npoint: Array[Int] = Array(1, 2, 3)\n\nscala> val Array(x, y, z) = point\nx: Int = 1\ny: Int = 2\nz: Int = 3\n\n```", "```py\nscala> val point = Array(x, y, z)\npoint: Array[Int] = Array(1, 2, 3)\n\n```", "```py\nscala> val Array(x, _*) = point\nx: Int = 1\n\n```", "```py\nscala> val Array(x, xs @ _*) = point\nx: Int = 1\nxs: Seq[Int] = Vector(2, 3)\n\n```", "```py\nscala> case class Name(first: String, last: String)\ndefined class Name\n\nscala> val name = Name(\"Martin\", \"Odersky\")\nname: Name = Name(Martin,Odersky)\n\n```", "```py\nscala> val Name(firstName, lastName) = name\nfirstName: String = Martin\nlastName: String = Odersky\n\n```", "```py\nscala> def greet(name:Name) = name match {\n case Name(\"Martin\", \"Odersky\") => \"An honor to meet you\"\n case Name(first, \"Bugnion\") => \"Wow! A family member!\"\n case Name(first, last) => s\"Hello, $first\"\n}\ngreet: (name: Name)String\n\n```", "```py\nscala> val names = List(Name(\"Martin\", \"Odersky\"), \n Name(\"Derek\", \"Wyatt\"))\nnames: List[Name] = List(Name(Martin,Odersky), Name(Derek,Wyatt))\n\n```", "```py\nscala> for { Name(first, last) <- names } yield first\nList[String] = List(Martin, Derek)\n\n```", "```py\nscala> for { Name(\"Martin\", last) <- names } yield last\nList[String] = List(Odersky)\n\n```", "```py\nscala> case class Name(first: String, last: String)\ndefined class Name\n\nscala> Name.<tab>\napply   asInstanceOf   curried   isInstanceOf   toString   tupled   unapply\n\n```", "```py\nscala> val name = Name(\"Martin\", \"Odersky\")\nname: Name = Name(Martin,Odersky)\n\nscala> Name.unapply(name)\nOption[(String, String)] = Some((Martin,Odersky))\n\n```", "```py\nscala> object NonZeroDouble { \n def unapply(d:Double):Option[Double] = {\n if (d == 0.0) { None } else { Some(d) } \n }\n}\ndefined object NonZeroDouble\n\nscala> val NonZeroDouble(denominator) = 5.5\ndenominator: Double = 5.5\n\nscala> val NonZeroDouble(denominator) = 0.0\nscala.MatchError: 0.0 (of class java.lang.Double)\n ... 43 elided\n\n```", "```py\nscala> def safeDivision(numerator:Double, \n denominator:Double, fallBack:Double) =\n denominator match {\n case NonZeroDouble(d) => numerator / d\n case _ => fallBack\n }\nsafeDivision: (numerator: Double, denominator: Double, fallBack: Double)Double\n\nscala> safeDivision(5.0, 2.0, 100.0)\nDouble = 2.5\n\nscala> safeDivision(5.0, 0.0, 100.0)\nDouble = 100.0\n\n```", "```py\nscala> val Array(a, b) = Array(1, 2)\na: Int = 1\nb: Int = 2\n\n```", "```py\nscala> Array.unapplySeq(Array(1, 2))\nOption[IndexedSeq[Int]] = Some(Vector(1, 2))\n\n```", "```py\nscala> import breeze.linalg._\nimport breeze.linalg._\n\nscala> object DV {\n // Just need to convert to a Scala vector.\n def unapplySeq(v:DenseVector[Double]) = Some(v.toScalaVector)\n}\ndefined object DV\n\n```", "```py\nscala> val vec = DenseVector(1.0, 2.0, 3.0)\nvec: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\n\nscala> val DV(x, y, z) = vec\nx: Double = 1.0\ny: Double = 2.0\nz: Double = 3.0\n\n```"]