<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Seeing a Heartbeat with a Motion-Amplifying Camera</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"Remove everything that has no relevance to the story. If you say in the first chapter that there is a rifle hanging on the wall, in the second or third chapter it absolutely must go off. If it's not going to be fired, it shouldn't be hanging there."<br/>
<span>                                                                                                                    – </span>Anton Chekhov</div>
<div class="packt_quote">"King Julian: I don't know why the sacrifice didn't work. The science seemed so solid."<br/>
                                                                              – Madagascar: Escape 2 Africa (2008)</div>
<p>Despite their strange design and mysterious engineering, Q's gadgets always prove useful and reliable. Bond has such faith in the technology that he never even asks how to charge the batteries.</p>
<p>One of the more inventive ideas in the Bond franchise is that even a lightly equipped spy should be able to see and photograph concealed objects, anyplace, anytime. Let's consider a timeline of a few relevant gadgets in the movies, as follows:</p>
<ul>
<li><strong>1967 (<em>You Only Live Twice</em>)</strong>: An X-ray desk scans guests for hidden firearms.</li>
<li><strong>1979 (<em>Moonraker</em>)</strong>: A cigarette case contains an X-ray imaging system that is used to reveal the tumblers of a safe's combination lock.</li>
<li><strong>1989 (<em>License to Kill</em>)</strong>: A Polaroid camera takes X-ray photos. Oddly enough, its flash is a visible, red laser.</li>
<li><strong>1995 (<em>GoldenEye</em>)</strong>: A tea tray contains an X-ray scanner that can photograph documents beneath the tray.</li>
<li><strong>1999 (<em>The World is Not Enough</em>)</strong>: Bond wears a stylish pair of blue-lensed glasses that can see through one layer of clothing to reveal concealed weapons. According to the <em>James Bond Encyclopedia</em> (2007), which is an official guide to the movies, the glasses display infrared video after applying special processing to them. Despite using infrared, they are commonly called <strong>X-ray specs</strong>, a misnomer.</li>
</ul>
<p class="NormalPACKT">These gadgets deal with unseen wavelengths of light (or radiation) and are broadly comparable to real-world devices such as airport security scanners and night-vision goggles. However, it remains difficult to explain how Bond's equipment is so compact and how it takes such clear pictures under diverse lighting conditions and through diverse materials. Moreover, if Bond's devices are active scanners (meaning they emit X-ray radiation or infrared light), they will be clearly visible to other spies using similar hardware.</p>
<p class="NormalPACKT">To take another approach, what if we avoid unseen wavelengths of light but instead focus on unseen frequencies of motion? Many things move in a pattern that is too fast or too slow for us to easily notice. Suppose a man is standing in one place. If he shifts one leg more than the other, perhaps he is concealing a heavy object, such as a gun, on the side that he shifts more. Equally, we might also fail to notice deviations from a pattern; suppose the same man has been looking straight ahead but suddenly, when he believes no one is looking, his eyes dart to one side. Is he watching someone?</p>
<p class="NormalPACKT">We can make motions of a certain frequency more visible by repeating them, like a delayed afterimage or a ghost, with each repetition appearing fainter (or less opaque) than the last. The effect is analogous to an echo or a ripple and is achieved using an algorithm called <strong><span class="KeyWordPACKT">Eulerian video magnification</span></strong>.</p>
<p>By applying this technique, we will build a desktop app that allows us to simultaneously see the present and selected slices of the past. The idea of experiencing multiple images simultaneously is, to me, quite natural because, for the first 26 years of my life, I had <strong>strabismus</strong>—commonly called a <strong>lazy eye</strong>—which caused double vision. A surgeon corrected my eyesight and gave me depth perception but in memory of strabismus, I would like to name this application <kbd>Lazy Eyes</kbd>.</p>
<p>Let's take a closer look, or two or more closer looks, at the fast-paced, moving world that we share with other secret agents.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li class="h1">Understanding what Eulerian video magnification can do</li>
<li class="h1">Extracting repeating signals from video using the fast Fourier transform</li>
<li class="h1">Compositing two images using image pyramids</li>
<li class="h1">Implementing the Lazy Eyes app</li>
<li class="h1">Configuring and testing the app for various motions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li><strong>A Python environment with the following modules</strong>: OpenCV, NumPy, SciPy, PyFFTW, wxPython</li>
</ul>
<p>Where not otherwise noted, setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>. Setup instructions for PyFFTW are covered in the current chapter, in the section <em>Choosing and setting up an FFT library</em>. Always refer to the setup instructions for any version requirements. Basic instructions for running Python code are covered in <a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml"/><a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml">Appendix C</a>, <em>Running with Snakes (or, First Steps with Python)</em>.</p>
<p class="mce-root">The complete project for this chapter can be found in this book's GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a>, in the <kbd>Chapter007</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning the Lazy Eyes app</h1>
                </header>
            
            <article>
                
<p>Of all our apps, <kbd>Lazy Eyes</kbd> has the simplest user interface. It just shows a live video feed with a special effect that highlights motion. The parameters of the effect are quite complex and, moreover, modifying them at runtime would have a big effect on performance. Thus, we do not provide a user interface to reconfigure the effect, but we do provide many parameters in code to allow a programmer to create many variants of the effect and the app. Below the video panel, the app displays the current frame rate, measured in <strong>frames per second</strong> (<strong>FPS</strong>). The following screenshot illustrates one configuration of the app. This screenshot shows me eating cake. Because my hands and face are moving, we see an effect that looks like light and dark waves rippling near moving edges (the effect is more graceful in a live video than in a screenshot):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-167 image-border" src="assets/14bb7071-cc2e-424e-8fc3-42809889ee68.png" style="width:16.08em;height:14.17em;"/></p>
<div class="packt_infobox">For more screenshots and an in-depth discussion of the parameters, see the <em>Configuring and testing the app for various motions </em>section later in this chapter.</div>
<p>Regardless of how it is configured, the app loops through the following actions:</p>
<ol>
<li>Capture an image.</li>
<li>Copy and downsample the image while applying a blur filter and, optionally, an edge-finding filter. We will downsample using so-called <strong>image pyramids</strong>, which will be discussed in the <em>Compositing two images using image pyramids</em> section later in this chapter. The purpose of downsampling is to achieve a higher frame rate by reducing the amount of image data that's used in subsequent operations. The purpose of applying a blur filter and, optionally, an edge-finding filter is to create halos that are useful in amplifying motion.</li>
<li>Store the downsampled copy in a history of frames, with a timestamp. The history has a fixed capacity. Once it is full, the oldest frame is overwritten to make room for the new one.</li>
<li>If the history is not yet full, continue to the next iteration of the loop.</li>
<li>Calculate and display the average frame rate based on the timestamps of the frames in the history.</li>
<li>Decompose the history into a list of frequencies describing fluctuations (motion) at each pixel. The decomposition function is called a <strong>fast Fourier transform</strong> (<strong>FFT</strong>). We will discuss it in the<em>Extracting repeating signals from video using the fast Fourier transform </em>section later in this chapter.</li>
<li>Set all frequencies to zero except a chosen range of interest. In other words, filter out the data on motions that are faster or slower than certain thresholds.</li>
<li>Recompose the filtered frequencies into a series of images that are motion maps. Areas that are still (with respect to our chosen range of frequencies) become dark, and areas that are moving remain bright. The <kbd>recomposition</kbd> function is called an <strong>inverse fast Fourier transform</strong> (<strong>IFFT</strong>), which we will discuss later.</li>
<li>Upsample the latest motion map (again, using image pyramids), intensify it, and overlay it additively atop the original camera image.</li>
<li>Show the resulting composite image.</li>
</ol>
<p>That's it—a simple plan that requires rather nuanced implementation and configuration. So, with that in mind, let's prepare ourselves by doing a little background research first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding what Eulerian video magnification can do</h1>
                </header>
            
            <article>
                
<p>Eulerian video magnification is inspired by a model in fluid mechanics called <strong>Eulerian specification of the flow field</strong>. Let's consider a moving, fluid body, such as a river. The Eulerian specification describes the river's velocity at a given position and time. The velocity would be fast in the mountains in springtime and slow at the river's mouth in winter. The velocity would also be slower at a silt-saturated point at the river's bottom, compared to a point where the river's surface hits a rock and sprays. An alternative to the Eulerian specification is the <strong>Lagrangian specification</strong>, which describes the position of a given particle at a given time. For example, a given bit of silt might make its way down from the mountains to the river's mouth over a period of many years and then spend eons drifting around a tidal basin.</p>
<div class="packt_infobox">For a more formal description of the Eulerian specification, the Lagrangian specification, and their relationship, refer to the following Wikipedia article at <span class="URLPACKT"><a href="http://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field">http://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field</a></span>.</div>
<p>The Lagrangian specification is analogous to many computer vision tasks in which we model the motion of a particular object or feature over time. However, the Eulerian specification is analogous to our current task, in which we model any motion occurring in a particular position and a particular window of time. Having modeled a motion from an Eulerian perspective, we can visually exaggerate the motion by overlaying the model's results for a blend of positions and times.</p>
<p>Let's set a baseline for our expectations of Eulerian video magnification by studying other people's projects, which include the following:</p>
<ul>
<li>Michael Rubenstein's webpage at MIT (<a href="http://people.csail.mit.edu/mrub/vidmag/"><span class="URLPACKT">http://people.csail.mit.edu/mrub/vidmag/</span></a>): Gives an abstract of his team's pioneering work on Eulerian video magnification, along with demo videos.</li>
<li>Bryce Drennan's eulerian-magnification library (<a href="https://github.com/brycedrennan/eulerian-magnification"><span class="URLPACKT">https://github.com/brycedrennan/eulerian-magnification</span></a>): Implements the algorithm using NumPy, SciPy, and OpenCV. This implementation is good inspiration for us, but is designed for processing prerecorded videos and is not sufficiently optimized for real-time input.</li>
</ul>
<p>Now, let's move on and understand the functions that are the building blocks of these projects and ours.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting repeating signals from video using the fast Fourier transform</h1>
                </header>
            
            <article>
                
<p>An audio signal is typically visualized as a bar chart or wave. The bar or wave is high when the sound is loud and low when it is soft. We recognize that a repetitive sound, such as a metronome's beat, makes repetitive peaks and valleys in the visualization. When audio has multiple channels (such as a stereo or surround-sound recording), each channel can be considered a separate signal and can be visualized as a separate bar chart or wave.</p>
<p>Similarly, in a video, every channel of every pixel can be considered a separate signal, rising and falling (becoming brighter and dimmer) over time. Imagine that we use a stationary camera to capture a video of a metronome. In this case, certain pixel values will rise and fall at a regular interval as they capture the passage of the metronome's needle. If the camera has an attached microphone, its signal values will rise and fall at the same interval. Based on either the audio or the video, we can then measure the metronome's frequency—its <strong>beats per minute</strong> (<strong>bpm</strong>) or its beats per second (Hertz or Hz). Conversely, if we change the metronome's bpm setting, the effect on both the audio and the video will be predictable. From this thought experiment, we can learn that a signal—be it audio, video, or any other kind—can be expressed as a function of time and, <em>equivalently</em>, a function of frequency.</p>
<p>Consider the following pair of graphs. They express the same signal, first as a function of time and then as a function of frequency. Within the time domain, we see one wide peak and valley (in other words, a tapering effect) spanning many narrow peaks and valleys. Within the frequency domain, we can see a low-frequency peak and a high-frequency peak, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/320f1e75-511e-4178-84d5-d1367685ee8d.png" style="width:28.33em;height:17.17em;"/></p>
<p>The transformation from the time domain to the frequency domain is called the <strong>Fourier transform</strong> (<strong>FT</strong>). Conversely, the transformation from the frequency domain to the time domain is called the <strong>inverse Fourier transform</strong>. Within the digital world, signals are discrete, not continuous, so we use the terms <strong>discrete Fourier transform</strong> (<strong>DFT</strong>) and <strong>inverse discrete Fourier transform</strong> (<strong>IDFT</strong>). There is a variety of efficient algorithms for computing the DFT or IDFT, and such an algorithm might be described as a FFT or IFFT.</p>
<div class="packt_infobox">For algorithmic descriptions, refer to the following Wikipedia article at <a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform"><span class="URLPACKT">http://en.wikipedia.org/wiki/Fast_Fourier_transform</span></a>.</div>
<p>The result of the FT (including its discrete variants) is a function that maps a frequency to an amplitude and phase. The <strong>amplitude</strong> represents the magnitude of the frequency's contribution to the signal. The <strong>phase</strong> represents a temporal shift; it determines whether the frequency's contribution starts on a high or a low. Typically, the amplitude and phase are encoded in a complex number, <kbd>a+bi</kbd>, where <kbd>amplitude=sqrt(a^2+b^2)</kbd> and <kbd>phase=atan2(a, b)</kbd>.</p>
<div class="packt_infobox">For an explanation of complex numbers, see the following Wikipedia article: <span class="URLPACKT"><a href="http://en.wikipedia.org/wiki/Complex_number">http://en.wikipedia.org/wiki/Complex_number</a></span>.</div>
<p>The FFT and IFFT are fundamental to a field of computer science called <strong>digital signal processing</strong>. Many signal processing applications, including <kbd>Lazy Eyes</kbd>, involve taking the signal's FFT, modifying or removing certain frequencies in the FFT result, and then reconstructing the filtered signal in the time domain using the IFFT. For example, this approach allows us to amplify certain frequencies while leaving others unchanged.</p>
<p>Now, where do we find this functionality?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing and setting up an FFT library</h1>
                </header>
            
            <article>
                
<p>Several Python libraries provide FFT and IFFT implementations that can process NumPy arrays (and thus OpenCV images). The five major contenders are as follows:</p>
<ul>
<li><em>NumPy</em>, which provides FFT and IFFT implementations in a module called <kbd>numpy.fft</kbd> (<a href="http://docs.scipy.org/doc/numpy/reference/routines.fft.html"><span class="URLPACKT">http://docs.scipy.org/doc/numpy/reference/routines.fft.html</span></a>). The module also offers other signal processing functions for working with the output of the FFT.</li>
<li><em>SciPy</em>, which provides FFT and IFFT implementations in a module called <kbd>scipy.fftpack</kbd> (<a href="http://docs.scipy.org/doc/scipy/reference/fftpack.html"><span class="URLPACKT">http://docs.scipy.org/doc/scipy/reference/fftpack.html</span></a>). This SciPy module is closely based on the <kbd>numpy.fft</kbd> module, but adds some optional arguments and dynamic optimizations based on the input format. The SciPy module also adds more signal processing functions for working with the output of the FFT.</li>
<li><em>OpenCV</em> itself has implementations of FFT (<kbd>cv2.dft</kbd>) and IFT (<kbd>cv2.idft</kbd>). The following official tutorial provides examples and a comparison to NumPy's FFT implementation: <a href="https://docs.opencv.org/master/d8/d01/tutorial_discrete_fourier_transform.html">https://docs.opencv.org/master/d8/d01/tutorial_discrete_fourier_transform.html</a>. Note that OpenCV's FFT and IFT interfaces are not directly interoperable with the <kbd>numpy.fft</kbd> and <kbd>scipy.fftpack</kbd> modules, which offer a broader range of signal processing functionality. (They format the data very differently.)</li>
<li><em>PyFFTW</em> (<a href="https://hgomersall.github.io/pyFFTW/"><span class="URLPACKT">https://hgomersall.github.io/pyFFTW/</span></a>), which is a Python wrapper around a C library called the <strong>Fastest Fourier Transform in the West</strong> (<strong>FFTW</strong>) (<a href="http://www.fftw.org/"><span class="URLPACKT">http://www.fftw.org/</span></a>). FFTW provides multiple implementations of FFT and IFFT. At runtime, it dynamically selects implementations that are well-optimized for given input formats, output formats, and system capabilities. Optionally, it takes advantage of multithreading (and its threads may run on multiple CPU cores, as the implementation releases Python's <strong>Global Interpreter Lock</strong> (<strong>GIL</strong>)). PyFFTW provides optional interfaces matching NumPy's and SciPy's FFT and IFFT functions. These interfaces have a low overhead cost (thanks to good caching options that are provided by PyFFTW) and they help to ensure that PyFFTW is interoperable with a broad range of signal processing functionality, as implemented in <kbd>numpy.fft</kbd> and <kbd>scipy.fftpack</kbd>.</li>
<li><em>Reinka</em> (<span class="URLPACKT"><a href="http://reikna.publicfields.net/en/latest/">http://reikna.publicfields.net/en/latest/</a></span>), which is a Python library for GPU-accelerated computations, uses either PyCUDA (<a href="http://mathema.tician.de/software/pycuda/"><span class="URLPACKT">http://mathema.tician.de/software/pycuda/</span></a>) or PyOpenCL (<span class="URLPACKT"><a href="http://mathema.tician.de/software/pyopencl/">http://mathema.tician.de/software/pyopencl/</a></span>) as a backend. Reinka provides FFT and IFFT implementations in a module called <kbd>reikna.fft</kbd>. Reinka internally uses PyCUDA or PyOpenCL arrays (not NumPy arrays) and provides interfaces for conversion from NumPy arrays to these GPU arrays and back. The converted NumPy output is compatible with other signal processing functionality, as implemented in <kbd>numpy.fft</kbd> and <kbd>scipy.fftpack</kbd>. However, this compatibility comes at a high overhead cost due to the need to lock, read, and convert the contents of GPU memory.</li>
</ul>
<p class="NormalPACKT">NumPy, SciPy, OpenCV, and PyFFTW are open source libraries under the BSD license. Reinka is an open-source library under the MIT license.</p>
<p>I recommend PyFFTW because of its optimizations and its interoperability (at a low overhead cost), and all the other functionality that interests us in NumPy, SciPy, and OpenCV. For a tour of PyFFTW's features, including its NumPy- and SciPy-compatible interfaces, see the official tutorial at <span class="URLPACKT"><a href="https://hgomersall.github.io/pyFFTW/sphinx/tutorial.html">https://hgomersall.github.io/pyFFTW/sphinx/tutorial.html</a></span>.</p>
<p>Depending on our platform, we can set up PyFFTW in one of the following ways:</p>
<ul>
<li>On Mac, the third-party MacPorts package manager offers PyFFTW packages for some versions of Python, currently including Python 3.6 but not Python 3.7. To install PyFFTW with MacPorts, open a Terminal and run something like the following command (but substitute your Python version if it differs from <kbd>py36</kbd>):</li>
</ul>
<pre style="padding-left: 60px"><strong>$ sudo port install py36-pyfftw</strong></pre>
<ul>
<li>Alternatively, on any system, use Python's package manager, pip, to install PyFFTW. Open a command prompt and run something like the following command (depending on your system, you might need to replace <kbd>pip</kbd> with <kbd>pip3</kbd> in order to install PyFFTW for Python 3):</li>
</ul>
<pre style="padding-left: 60px"><strong>$ pip install --user pyFFTW</strong></pre>
<p>Some versions of pip's <kbd>pyFFTW</kbd> package have installation bugs that affect some systems. If <kbd>pip</kbd> fails to install the <kbd>pyFFTW</kbd> package, try again, but manually specify version 10.4 of the package by running the following command:</p>
<pre><strong>$ pip install --user pyFFTW==0.10.4</strong></pre>
<div class="packt_infobox">Note that some old versions of the library are called <kbd>PyFFTW3</kbd>. We do not want <kbd>PyFFTW3</kbd>. On Ubuntu 18.04 and its derivatives, the <kbd>python-fftw</kbd> package in the system's standard apt repository is an old <kbd>PyFFTW3</kbd> version.</div>
<p>We have our FFT and IFFT needs covered by the Fastest Fourier Transform in the West (and if we were cowboys instead of secret agents, we could say, <em>Cover me!</em>). For additional signal processing functionality, we will use SciPy, which can be set up in the way we described in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>, in the <em>Setting up a development machine</em> section.</p>
<p>Signal processing is not the only new material that we must learn about for Lazy Eyes, so let's look at other functionality that is provided by OpenCV.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compositing two images using image pyramids</h1>
                </header>
            
            <article>
                
<p>Running an FFT on a full-resolution video feed would be slow. The resulting frequencies may also reflect localized phenomena at each captured pixel, so that the motion map (the result of filtering the frequencies and then applying the IFFT) might appear noisy and overly sharp. To address these problems, we need a cheap, blurry downsampling technique. However, we also want the option to enhance edges, which are important to our perception of motion.</p>
<p>Our need for a blurry downsampling technique is fulfilled by a <strong>Gaussian image pyramid</strong>. A <strong>Gaussian filter</strong> blurs an image by making each output pixel a weighted average of multiple input pixels in the neighborhood. An image pyramid is a series in which each image is a fraction of the width and height of the previous image. Often, the fraction is one half. The halving of image dimensions is achieved by <em>decimation</em>, meaning that every other pixel is simply omitted. A Gaussian image pyramid is constructed by applying a Gaussian filter before each decimation operation.</p>
<p>Our need to enhance edges in downsampled images is fulfilled by a <strong>Laplacian image pyramid</strong>, which is constructed in the following manner. Suppose we have already constructed a Gaussian image pyramid. We take the image at level <kbd>i+1</kbd> in the Gaussian pyramid, upsample it by duplicating pixels, and apply a Gaussian filter to it again. We then subtract the result from the image at level <kbd>i</kbd> in the Gaussian pyramid to produce the corresponding image at level <kbd>i</kbd> of the Laplacian pyramid. Thus, the Laplacian image is the difference between a blurry, downsampled image and an even blurrier image that was downsampled, downsampled again, and upsampled.</p>
<p>You might wonder how such an algorithm is a form of edge-finding. Consider that edges are areas of local contrast, while non-edges are areas of local uniformity. If we blur a uniform area, it is still uniform—there is zero difference. If we blur a contrasting area, however, it becomes more uniform—there is a non-zero difference. Thus, the difference can be used to find edges.</p>
<div class="packt_infobox">
<p>The Gaussian and Laplacian image pyramids are described in detail in the following journal article: E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. "Pyramid methods in image processing". RCA Engineer, Vol. 29, No. 6, November/Dececember 1984. <span>It can be downloaded from</span> <a href="http://persci.mit.edu/pub_pdfs/RCA84.pdf">http://persci.mit.edu/pub_pdfs/RCA84.pdf</a><span>.</span></p>
</div>
<p class="NormalPACKT">Besides using image pyramids to downsample the FFT's input, we can also use them to upsample the most recent frame of the IFFT's output. This upsampling step is necessary for creating an overlay that matches the size of the original camera image so that we can composite the two. Like in the construction of the Laplacian pyramid, upsampling consists of duplicating pixels and applying a Gaussian filter.</p>
<p>OpenCV implements the relevant downsizing and upsizing functions as <kbd>cv2.pyrDown</kbd> and <kbd>cv2.pyrUp</kbd>. These functions are useful in compositing two images in general (whether or not signal processing is involved), because they allows us to soften differences while preserving edges. The OpenCV documentation includes a good tutorial on this topic at <a href="https://docs.opencv.org/master/dc/dff/tutorial_py_pyramids.html">https://docs.opencv.org/master/dc/dff/tutorial_py_pyramids.html</a>.</p>
<p>Now that we are armed with the necessary knowledge, it's time to implement <kbd>Lazy Eyes</kbd>!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the Lazy Eyes app</h1>
                </header>
            
            <article>
                
<p>Let's create a new folder for Lazy Eyes and, in this folder, create copies of or links to the <kbd>ResizeUtils.py</kbd> and <kbd>WxUtils.py</kbd> files from any of our previous Python projects, such as <span><kbd>The Living Headlights</kbd> from </span><a href="b4619968-1f90-45f0-8a77-3505624bc0c0.xhtml">Chapter 5</a>, <em>Equipping Your Car with a Rearview Camera and Hazard Detection</em>. Alongside the copies or links, let's create a new file, <kbd>LazyEyes.py</kbd>. Edit it and enter the following <kbd>import</kbd> statements:</p>
<pre>import collections<br/>import threading<br/>import timeit<br/><br/>import numpy<br/>import cv2<br/>import wx<br/><br/>import pyfftw.interfaces.cache<br/>from pyfftw.interfaces.scipy_fftpack import fft<br/>from pyfftw.interfaces.scipy_fftpack import ifft<br/>from scipy.fftpack import fftfreq<br/><br/>import ResizeUtils<br/>import WxUtils</pre>
<p>Besides the modules that we have used in the previous projects, we are now using the standard library's <kbd>collections</kbd> module for efficient collections, as well as the <kbd>timeit</kbd> module for precise timing. For the first time, we are also using the signal processing functionality from PyFFTW and SciPy.</p>
<p>Like our other Python applications, <kbd>Lazy Eyes</kbd> is implemented as a class that extends <kbd>wx.Frame</kbd>. The following code block contains the declarations of the class and its initializer:</p>
<pre>class LazyEyes(wx.Frame):<br/><br/>    def __init__(self, maxHistoryLength=360,<br/>                 minHz=5.0/6.0, maxHz=1.0,<br/>                 amplification=32.0, numPyramidLevels=2,<br/>                 useLaplacianPyramid=True,<br/>                 useGrayOverlay=True,<br/>                 numFFTThreads=4, numIFFTThreads=4,<br/>                 cameraDeviceID=0, imageSize=(640, 480),<br/>                 title='Lazy Eyes'):</pre>
<p class="NormalPACKT">The initializer's arguments affect the app's frame rate and the manner in which motion is amplified. These effects are discussed in detail in the section <span class="ItalicsPACKT"><em>Configuring and testing the app for various motions </em></span>later in this chapter. The following is just a brief description of the arguments:</p>
<ul>
<li><kbd>maxHistoryLength</kbd> is the number of frames (including the current frame and preceding frames) that are analyzed for motion.</li>
<li><kbd>minHz</kbd> and <kbd>maxHz</kbd>, respectively, define the slowest and fastest motions that are amplified.</li>
<li><kbd>amplification</kbd> is the scale of the visual effect. A higher value means motion is highlighted more brightly.</li>
<li><kbd>numPyramidLevels</kbd> is the number of pyramid levels by which frames are downsampled before signal processing is done. Each level corresponds to downsampling by a factor of <kbd>2</kbd>. Our implementation assumes <kbd>numPyramidLevels&gt;0</kbd>.</li>
<li>If <kbd>useLaplacianPyramid</kbd> is <kbd>True</kbd>, frames are downsampled using a Laplacian pyramid before signal processing is done. The implication is that only edge motion is highlighted. Alternatively, if <kbd>useLaplacianPyramid</kbd> is <kbd>False</kbd>, a Gaussian pyramid is used, and motion in all areas is highlighted.</li>
<li>If <kbd>useGrayOverlay</kbd> is <kbd>True</kbd>, frames are converted to grayscale before signal processing is done. The implication is that motion is only highlighted in areas of grayscale contrast. Alternatively, if <kbd>useGrayOverlay</kbd> is <kbd>False</kbd>, motion is highlighted in areas that have contrast in any color channel.</li>
<li><kbd>numFFTThreads</kbd> and <kbd>numIFFTThreads</kbd>, are the numbers of threads that are used in FFT and IFFT computations, respectively.</li>
<li><kbd>cameraDeviceID</kbd> and <kbd>imageSize</kbd> are our usual capture parameters.</li>
</ul>
<p>The initializer's implementation begins in the same way as our other Python apps. It sets flags to indicate that the app is running and should be mirrored by default. It creates the capture object and configures its resolution to match the requested width and height, if possible. Failing that, the device's fallback capture resolution is used. The initializer also declares variables to store images, and creates a lock to manage thread-safe access to the images. The relevant code is as follows:</p>
<pre>        self.mirrored = True<br/><br/>        self._running = True<br/><br/>        self._capture = cv2.VideoCapture(cameraDeviceID)<br/>        size = ResizeUtils.cvResizeCapture(<br/>                self._capture, imageSize)<br/>        w, h = size<br/><br/>        self._image = None<br/><br/>        self._imageFrontBuffer = None<br/>        self._imageFrontBufferLock = threading.Lock()</pre>
<p class="NormalPACKT">Next, we need to determine the shape of the history of frames. We already know that it has at least three dimensions—a number of frames, and a width and height for each frame. The width and height are downsampled from the capture width and height based on the number of pyramid levels. If we are concerned with color motion, and not just grayscale motion, the history also has a fourth dimension that consists of three color channels. The following code calculates the history's shape:</p>
<pre>        self._useGrayOverlay = useGrayOverlay<br/>        if useGrayOverlay:<br/>            historyShape = (maxHistoryLength,<br/>                            h &gt;&gt; numPyramidLevels,<br/>                            w &gt;&gt; numPyramidLevels)<br/>        else:<br/>            historyShape = (maxHistoryLength,<br/>                            h &gt;&gt; numPyramidLevels,<br/>                            w &gt;&gt; numPyramidLevels, 3)</pre>
<p>Note the use of <kbd>&gt;&gt;</kbd>, the right bitshift operator, <span>i</span><span>n the preceding code; it's used</span> to divide the dimensions by a power of two. The power is equal to the number of pyramid levels.</p>
<p class="NormalPACKT">We now need to store the specified maximum history length. For the frames in the history, we will create a NumPy array of the shape we just determined. For timestamps of the frames, we will create a <span class="KeyWordPACKT"><strong>double-ended queue</strong> (<strong>deque</strong>)</span>, a type of collection that allows us to cheaply add or remove elements from either end, as shown in the following code:</p>
<pre>        self._maxHistoryLength = maxHistoryLength<br/>        self._history = numpy.empty(historyShape,<br/>                                    numpy.float32)<br/>        self._historyTimestamps = collections.deque() </pre>
<p class="NormalPACKT">We will store the remaining arguments because we will need to pass them to the pyramid functions and signal processing functions for each frame later, as follows:</p>
<pre>        self._numPyramidLevels = numPyramidLevels<br/>        self._useLaplacianPyramid = useLaplacianPyramid<br/><br/>        self._minHz = minHz<br/>        self._maxHz = maxHz<br/>        self._amplification = amplification<br/><br/>        self._numFFTThreads = numFFTThreads<br/>        self._numIFFTThreads = numIFFTThreads</pre>
<div class="packt_infobox">
<p><span>To ensure meaningful error messages and early termination in the case of invalid arguments, we could add code such as the following for each argument:</span></p>
<p><kbd>assert numPyramidLevels &gt; 0, \</kbd><br/>
<kbd>        'numPyramidLevels must be positive.'</kbd></p>
<p><span>For brevity, such assertions are omitted from our code samples.</span></p>
</div>
<p>We now need to call the following two functions to tell PyFFTW to cache its data structures (notably, its NumPy arrays) for a period of at least 1.0 seconds from their last use. (The default is 0.1 seconds.) Caching is a critical optimization for the PyFFTW interfaces that we are using, so we will choose a period that is more than long enough to keep the cache alive from frame to frame, as follows:</p>
<pre>        pyfftw.interfaces.cache.enable()<br/>        pyfftw.interfaces.cache.set_keepalive_time(1.0)</pre>
<p>As shown in the following code, the initializer's implementation ends with code to set up a window, event bindings, a video panel, a layout, and a background thread, which are all familiar tasks from our previous Python projects:</p>
<pre>        style = wx.CLOSE_BOX | wx.MINIMIZE_BOX | \<br/>                wx.CAPTION | wx.SYSTEM_MENU | \<br/>                wx.CLIP_CHILDREN<br/>        wx.Frame.__init__(self, None, title=title,<br/>                          style=style, size=size)<br/><br/>        self.Bind(wx.EVT_CLOSE, self._onCloseWindow)<br/><br/>        quitCommandID = wx.NewId()<br/>        self.Bind(wx.EVT_MENU, self._onQuitCommand,<br/>                  id=quitCommandID)<br/>        acceleratorTable = wx.AcceleratorTable([<br/>            (wx.ACCEL_NORMAL, wx.WXK_ESCAPE,<br/>             quitCommandID)<br/>        ])<br/>        self.SetAcceleratorTable(acceleratorTable)<br/><br/>        self._videoPanel = wx.Panel(self, size=size)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_ERASE_BACKGROUND,<br/>                self._onVideoPanelEraseBackground)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_PAINT, self._onVideoPanelPaint)<br/><br/>        self._videoBitmap = None<br/><br/>        self._fpsStaticText = wx.StaticText(self)<br/><br/>        border = 12<br/><br/>        controlsSizer = wx.BoxSizer(wx.HORIZONTAL)<br/>        controlsSizer.Add(self._fpsStaticText, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL)<br/><br/>        rootSizer = wx.BoxSizer(wx.VERTICAL)<br/>        rootSizer.Add(self._videoPanel)<br/>        rootSizer.Add(controlsSizer, 0,<br/>                      wx.EXPAND | wx.ALL, border)<br/>        self.SetSizerAndFit(rootSizer)<br/><br/>        self._captureThread = threading.Thread(<br/>                target=self._runCaptureLoop)<br/>        self._captureThread.start()</pre>
<p>We must now modify our usual <kbd>_onCloseWindow</kbd> callback to disable PyFFTW's cache. Disabling the cache ensures that resources are freed and that PyFFTW's threads terminate normally. The callback's implementation is shown in the following code:</p>
<pre>    def _onCloseWindow(self, event):<br/>        self._running = False<br/>        self._captureThread.join()<br/>        pyfftw.interfaces.cache.disable()<br/>        self.Destroy()</pre>
<p>The escape key is bound to our usual <kbd>_onQuitCommand</kbd> callback, which just closes the app, as follows:</p>
<pre>    def _onQuitCommand(self, event):<br/>        self.Close() </pre>
<p>The video panel's erase and paint events are bound to our usual callbacks, <kbd>_onVideoPanelEraseBackground</kbd> and <kbd>_onVideoPanelPaint</kbd>, as shown in the following code:</p>
<pre>    def _onVideoPanelEraseBackground(self, event):<br/>        pass<br/><br/>    def _onVideoPanelPaint(self, event):<br/>    <br/>        self._imageFrontBufferLock.acquire()<br/><br/>        if self._imageFrontBuffer is None:<br/>            self._imageFrontBufferLock.release()<br/>            return<br/><br/>        # Convert the image to bitmap format.<br/>        self._videoBitmap = \<br/>            WxUtils.wxBitmapFromCvImage(self._imageFrontBuffer)<br/><br/>        self._imageFrontBufferLock.release()<br/><br/>        # Show the bitmap.<br/>        dc = wx.BufferedPaintDC(self._videoPanel)<br/>        dc.DrawBitmap(self._videoBitmap, 0, 0)</pre>
<p>The loop running on our background thread is similar to the one used in other Python apps. For each frame, it calls a helper function, <kbd>_applyEulerianVideoMagnification</kbd>. The loop's implementation is as follows:</p>
<pre>    def _runCaptureLoop(self):<br/><br/>        while self._running:<br/>            success, self._image = self._capture.read(<br/>                    self._image)<br/>            if self._image is not None:<br/>                self._applyEulerianVideoMagnification()<br/>                if (self.mirrored):<br/>                    self._image[:] = numpy.fliplr(self._image)<br/><br/>                # Perform a thread-safe swap of the front and<br/>                # back image buffers.<br/>                self._imageFrontBufferLock.acquire()<br/>                self._imageFrontBuffer, self._image = \<br/>                        self._image, self._imageFrontBuffer<br/>                self._imageFrontBufferLock.release()<br/><br/>                # Send a refresh event to the video panel so<br/>                # that it will draw the image from the front<br/>                # buffer.<br/>                self._videoPanel.Refresh()</pre>
<p>The <kbd>_applyEulerianVideoMagnification</kbd> helper function is quite long, so we will consider its implementation in several chunks. First, we need to create a timestamp for the frame and copy the frame to a format that is more suitable for processing. Specifically, we will use a floating point with either one gray channel or three color channels, depending on the configuration, as shown in the following code:</p>
<pre>    def _applyEulerianVideoMagnification(self):<br/><br/>        timestamp = timeit.default_timer()<br/><br/>        if self._useGrayOverlay:<br/>            smallImage = cv2.cvtColor(<br/>                    self._image, cv2.COLOR_BGR2GRAY).astype(<br/>                            numpy.float32)<br/>        else:<br/>            smallImage = self._image.astype(numpy.float32)</pre>
<p>Using this copy, we will calculate the appropriate level in the Gaussian or Laplacian pyramid, as follows:</p>
<pre>        # Downsample the image using a pyramid technique.<br/>        i = 0<br/>        while i &lt; self._numPyramidLevels:<br/>            smallImage = cv2.pyrDown(smallImage)<br/>            i += 1<br/>        if self._useLaplacianPyramid:<br/>            smallImage[:] -= \<br/>                cv2.pyrUp(cv2.pyrDown(smallImage))</pre>
<p>For the purposes of the history and signal processing functions, we will refer to this pyramid level as <em>the image </em>or <em>the frame</em>.</p>
<p>Next, we need to check the number of history frames that have been filled so far. If the history has more than one unfilled frame (meaning the history still won't be full after adding the frame), we will append and timestamp the new image, before returning it early, so that no signal processing is done until a later frame. This can be seen in the following code:</p>
<pre>        historyLength = len(self._historyTimestamps)<br/><br/>        if historyLength &lt; self._maxHistoryLength - 1:<br/><br/>            # Append the new image and timestamp to the<br/>            # history.<br/>            self._history[historyLength] = smallImage<br/>            self._historyTimestamps.append(timestamp)<br/><br/>            # The history is still not full, so wait.<br/>            return</pre>
<p>If the history is just one frame short of being full (meaning the history will be full after adding this frame), we will append the new image and timestamp, as follows:</p>
<pre>        if historyLength == self._maxHistoryLength - 1:<br/>            # Append the new image and timestamp to the<br/>            # history.<br/>            self._history[historyLength] = smallImage<br/>            self._historyTimestamps.append(timestamp)</pre>
<p>If the history is already full, we will drop the oldest image and timestamp and append the new image and timestamp, as follows:</p>
<pre>        else:<br/>            # Drop the oldest image and timestamp from the<br/>            # history and append the new ones.<br/>            self._history[:-1] = self._history[1:]<br/>            self._historyTimestamps.popleft()<br/>            self._history[-1] = smallImage<br/>            self._historyTimestamps.append(timestamp)<br/><br/>        # The history is full, so process it.</pre>
<div class="packt_infobox">
<div>
<p><span>The history of image data is a NumPy array and, as such, we are using the terms <em>append </em>and <em>drop </em>loosely. NumPy arrays are immutable, meaning that they cannot grow or shrink. Moreover, we are not recreating this array because it is large, and reallocating each frame would be expensive. Instead, we are just overwriting data within the array by moving old data leftward and copying new data in.</span></p>
</div>
</div>
<p class="NormalPACKT">Based on the timestamps, we will calculate the average time per frame in the history, and we will display the frame rate, as seen in the following code:</p>
<pre>        # Find the average length of time per frame.<br/>        startTime = self._historyTimestamps[0]<br/>        endTime = self._historyTimestamps[-1]<br/>        timeElapsed = endTime - startTime<br/>        timePerFrame = \<br/>                timeElapsed / self._maxHistoryLength<br/>        fps = 1.0 / timePerFrame<br/>        wx.CallAfter(self._fpsStaticText.SetLabel,<br/>                     'FPS: %.1f' % fps)</pre>
<p>We will then proceed with a combination of signal processing functions, collectively called a <strong>temporal bandpass filter</strong>. This filter blocks (zeros out) some frequencies and allows others to pass and remain unchanged. Our first step in implementing this filter is to run the <kbd>pyfftw.interfaces.scipy_fftpack.fft</kbd> function using the history and number of threads as arguments. Also, with the <kbd>axis=0</kbd> argument, we will specify that the history's first axis is the time axis, as follows:</p>
<pre>        # Apply the temporal bandpass filter.<br/>        fftResult = fft(self._history, axis=0,<br/>                        threads=self._numFFTThreads)</pre>
<p>We will pass the FFT result and the time per frame to the <kbd>scipy.fftpack.fftfreq</kbd> function. This function will then return an array of midpoint frequencies (in Hz, in our case) corresponding to the indices in the FFT result. (This array answers the question, <em>Which frequency is the midpoint of the bin of frequencies represented by index</em> <kbd>i</kbd> <em>in the FFT?</em>) We will find the indices whose midpoint frequencies lie closest to our initializer's <kbd>minHz</kbd> and <kbd>maxHz</kbd> parameters (a minimum of absolute value difference). Then, we will modify the FFT result by setting the data to zero in all ranges that do not represent frequencies of interest, as follows:</p>
<pre>        frequencies = fftfreq(<br/>                self._maxHistoryLength, d=timePerFrame)<br/>        lowBound = (numpy.abs(<br/>                frequencies - self._minHz)).argmin()<br/>        highBound = (numpy.abs(<br/>                frequencies - self._maxHz)).argmin()<br/>        fftResult[:lowBound] = 0j<br/>        fftResult[highBound:-highBound] = 0j<br/>        fftResult[-lowBound:] = 0j</pre>
<div>
<div class="packt_infobox">
<p><span>The FFT result is symmetrical—</span><kbd>fftResult[i]</kbd> <span>and</span> <kbd>fftResult[-i-1]</kbd> <span>pertain to the same bin of frequencies. Thus, we modify the FFT result symmetrically.</span></p>
<p><span>Remember, the Fourier transform maps a frequency to a complex number that encodes an amplitude and phase. Thus, while the indices of the FFT result correspond to frequencies, the values contained at those indices are complex numbers. Zero as a complex number is written in Python as</span> <kbd>0+0j</kbd> <span>or</span> <kbd>0j</kbd><span>.</span></p>
</div>
</div>
<p>Having filtered out the frequencies that do not interest us, we will now finish applying the temporal bandpass filter by passing the data to the <kbd>pyfftw.interfaces.scipy_fftpack.ifft</kbd> function, as follows:</p>
<pre>        ifftResult = ifft(fftResult, axis=0,<br/>                          threads=self._numIFFTThreads) </pre>
<p>From the IFFT result, we will take the most recent frame. It should somewhat resemble the current camera frame, but should be black in areas that do not exhibit recent motion that matches our parameters. We will multiply this filtered frame so that the non-black areas become bright. Then, we will upsample it (using a pyramid technique) and add the result to the current camera frame so that areas of motion are lit up. The relevant code, which concludes the <kbd>_applyEulerianVideoMagnification</kbd> method, is as follows:</p>
<pre>        # Amplify the result and overlay it on the<br/>        # original image.<br/>        overlay = numpy.real(ifftResult[-1]) * \<br/>                          self._amplification<br/>        i = 0<br/>        while i &lt; self._numPyramidLevels:<br/>            overlay = cv2.pyrUp(overlay)<br/>            i += 1<br/>        if self._useGrayOverlay:<br/>            overlay = cv2.cvtColor(overlay,<br/>                                   cv2.COLOR_GRAY2BGR)<br/>        cv2.add(self._image, overlay, self._image,<br/>                dtype=cv2.CV_8U)</pre>
<p>This concludes the implementation of the <kbd>LazyEyes</kbd> class. Our module's <kbd>main</kbd> function just instantiates and runs the app, as seen in the following code:</p>
<pre>def main():<br/>    app = wx.App()<br/>    lazyEyes = LazyEyes()<br/>    lazyEyes.Show()<br/>    app.MainLoop()<br/><br/>if __name__ == '__main__':<br/>    main() </pre>
<p>That's all! Now, it's time to run the app and stay still while it builds up its history of frames. Until the history is full, the video feed will not show any special effects. At the history's default length of 360 frames, it fills in about 50 seconds on a machine. Once it is full, you should start to see ripples moving through the video feed in areas of recent motion—or perhaps even everywhere if the camera moves or the lighting or exposure changes. The ripples should then gradually settle and disappear in areas of the scene that become still, with new ripples appearing in new areas of motion. Feel free to experiment on your own. Now, let's discuss a few recipes for configuring and testing the parameters of the <kbd>LazyEyes</kbd> class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring and testing the app for various motions</h1>
                </header>
            
            <article>
                
<p>Currently, our <kbd>main</kbd> function initializes the <kbd>LazyEyes</kbd> object with the default parameters. If we were to fill in the same parameter values explicitly, we would have the following statement:</p>
<pre>    lazyEyes = LazyEyes(maxHistoryLength=360,<br/>                        minHz=5.0/6.0, maxHz=1.0,<br/>                        amplification=32.0,<br/>                        numPyramidLevels=2,<br/>                        useLaplacianPyramid=True,<br/>                        useGrayOverlay=True,<br/>                        numFFTThreads=4,<br/>                        numIFFTThreads=4,<br/>                        imageSize=(640, 480))</pre>
<p>This recipe calls for a capture resolution of <em>640 x 480</em> and a signal processing resolution of <em>160 x 120</em> (as we are downsampling by <kbd>2</kbd> pyramid levels, or a factor of <kbd>4</kbd>). We are amplifying the motion only at frequencies of 0.833 Hz to 1.0 Hz, only at edges (as we are using the Laplacian pyramid), only in grayscale, and only over a history of 360 frames (about 20 to 40 seconds, depending on the frame rate). Motion is exaggerated by a factor of <kbd>32</kbd>. These settings are suitable for many subtle upper-body movements such as a person's head swaying side to side, shoulders heaving with breathing, nostrils flaring, eyebrows rising and falling, and eye scanning to and fro. For performance, FFT and IFFT are each using <kbd>4</kbd> threads.</p>
<p>How the app looks when it runs with its default parameters is shown in the following screenshot. Moments before taking the screenshot, I smiled before returning to my normal expression. Note that my eyebrows and mustache are visible in multiple positions, including their current low positions and their previous high positions. For the sake of capturing the motion amplification effect in a still image, this gesture is quite exaggerated. However, in a moving video, we can see the amplification of more subtle movements, too:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-169 image-border" src="assets/d8fdda93-ce9d-45d0-aac9-526c0806f5ff.png" style="width:33.25em;height:29.25em;"/></p>
<p>The following screenshot illustrates an example where my eyebrows appear taller after being raised and then lowered:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-170 image-border" src="assets/0c2fde30-5d3f-43cc-af6c-1547965bb234.png" style="width:34.58em;height:30.50em;"/></p>
<p>The parameters interact with each other in complex ways. Consider the following relationships:</p>
<ul>
<li>Frame rate is greatly affected by the size of the input data for the FFT and IFFT functions. The size of the input data is determined by <kbd>maxHistoryLength</kbd> (a shorter length provides less input and thus a faster frame rate), <kbd>numPyramidLevels</kbd> (more levels implies less input), <kbd>useGrayOverlay</kbd> (<kbd>True</kbd> implies less input), and <kbd>imageSize</kbd> (a smaller size is less input).</li>
<li>Frame rate is also greatly affected by the level of multithreading of the FFT and IFFT functions, as determined by <kbd>numFFTThreads</kbd> and <kbd>numIFFTThreads</kbd> (a greater number of threads is faster up to some point).</li>
<li>Frame rate is slightly affected by <kbd>useLaplacianPyramid</kbd> (<kbd>False</kbd> implies a faster frame rate), as the Laplacian algorithm requires extra steps beyond the Gaussian.</li>
<li>Frame rate determines the amount of time that <kbd>maxHistoryLength</kbd> represents.</li>
<li>Frame rate and <kbd>maxHistoryLength</kbd> determine how many repetitions of motion (if any) can be captured in the <kbd>minHz</kbd> to <kbd>maxHz</kbd> range. The number of captured repetitions, together with <kbd>amplification</kbd>, determines how greatly a motion or a deviation from the motion will be amplified.</li>
<li>The inclusion or exclusion of noise is affected by <kbd>minHz</kbd> and <kbd>maxHz</kbd> (depending on which frequencies of noise are characteristic of the camera), <kbd>numPyramidLevels</kbd> (more levels implies a less noisy image), <kbd>useLaplacianPyramid</kbd> (<kbd>True</kbd> is less noisy), <kbd>useGrayOverlay</kbd> (<kbd>True</kbd> is less noisy), and <kbd>imageSize</kbd> (a smaller size implies a less noisy image).</li>
<li>The inclusion or exclusion of motion is affected by <kbd>numPyramidLevels</kbd> (fewer means the amplification is more inclusive of small motions), <kbd>useLaplacianPyramid</kbd> (<kbd>False</kbd> is more inclusive of motion in non-edge areas), <kbd>useGrayOverlay</kbd> (<kbd>False</kbd> is more inclusive of motion in areas of color contrast), <kbd>minHz</kbd> (a lower value is more inclusive of slow motion), <kbd>maxHz</kbd> (a higher value is more inclusive of fast motion), and <kbd>imageSize</kbd> (a bigger size is more inclusive of small motions).</li>
<li>Subjectively, the visual effect is always more impressive when the frame rate is high, noise is excluded, and small motions are included. Again subjectively, other conditions for including or excluding motion (edge versus non-edge, grayscale contrast versus color contrast, or fast versus slow) are application-dependent.</li>
</ul>
<p>Now, let's try our hand at reconfiguring <kbd>Lazy Eyes</kbd>, starting with the <kbd>numFFTThreads</kbd> and <kbd>numIFFTThreads</kbd> parameters. We want to determine the numbers of threads that maximize Lazy Eyes' frame rate on a given machine. The more CPU cores there are, the more threads one can gainfully use. However, experimentation is the best guide for picking a number.</p>
<p>Run <kbd>LazyEyes.py</kbd>. Once the history fills up, the history's average FPS will be displayed in the lower left corner of the window. Wait until this average FPS value stabilizes. It might take a minute for the average to adjust to the effect of the FFT and IFFT functions. Take note of the FPS value, close the app, adjust the thread count parameters, and test again. Repeat these steps until you feel that you have enough data to pick a good number of threads to use on the relevant hardware.</p>
<div>
<div class="packt_infobox">
<p>By activating additional CPU cores, multithreading can cause your system's temperature to rise. As you experiment, monitor your machine's temperature, fans, and CPU usage statistics. If you become concerned, reduce the number of FFT and IFFT threads. Having a sub-optimal frame rate is better than overheating your machine.</p>
</div>
</div>
<p>Now, experiment with other parameters to see how they affect FPS; the <kbd>numPyramidLevels</kbd>, <kbd>useGrayOverlay</kbd>, and <kbd>imageSize</kbd> parameters should all have a considerable effect. At a threshold of approximately 12 FPS, a series of frames starts to look like continuous motion instead of <em>a slide show</em>. The higher the frame rate, the smoother the motion will appear. Traditionally, hand-drawn animated movies run at 12 drawings per second for most scenes, and 24 drawings per second for fast action.</p>
<div>
<div class="packt_infobox">
<p>Besides the software parameters, external factors can also greatly affect the frame rate. Examples include the camera parameters, the lens parameters, and the scene's brightness.</p>
</div>
</div>
<p>Let's try another recipe. Whereas our default recipe accentuates motion at edges that have high grayscale contrast, this next recipe accentuates motion in all areas (edge or non-edge) that have either high color or grayscale contrast. By considering three color channels instead of one grayscale channel, we are tripling the amount of data that is processed by the FFT and IFFT. To offset this change, we need to cut each dimension of the capture resolution to half of its default value, thus reducing the amount of data to <em>1/2 * 1/2 = 1/4</em> times the default amount. As a net change, the FFT and IFFT process <em>3 * 1/4 = 3/4</em> times the default amount of data, a small decrease. The following initialization statement shows our new recipe's parameters:</p>
<pre>    lazyEyes = LazyEyes(useLaplacianPyramid=False,<br/>                        useGrayOverlay=False,<br/>                        imageSize=(320, 240)) </pre>
<p>Note that we are still using the default values for most parameters. If you found non-default values that work well for <kbd>numFFTThreads</kbd> and <kbd>numIFFTThreads</kbd> on your machine, enter them as well.</p>
<p>The following screenshots show the effects of our new recipe. Let's look at a non-extreme example first. I was typing on my laptop when this was taken. Note the halos around my arms, which move a lot when I type, and a slight distortion and discoloration of my left cheek (your left in this mirrored image). My left cheek twitches a little when I think. Apparently, it is a tic already known to my friends and family, but rediscovered by me with the help of computer vision:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-171 image-border" src="assets/2a624d19-c9ea-4e30-bb24-4641e5378363.png" style="width:24.83em;height:23.50em;"/></p>
<p>If you are viewing the color version of this image in the e-book, you should see that the halos around my arms take a green hue from the shirt and a red hue from the sofa. Similarly, the halos on my cheek take a magenta hue from my skin and a brown hue from my hair.</p>
<p>Now, let's consider a more fanciful example. If we were Jedi instead of secret agents, we might wave a steel ruler in the air and pretend it was a lightsaber. While testing the theory that Lazy Eyes could make the ruler <em>look like a real lightsaber</em>, I took the following screenshot. This screenshot shows two pairs of light and dark lines in two places where I was waving the lightsaber ruler. One of the pairs of lines passes through each of my shoulders. The Light Side (the light line) and the Dark Side (the dark line) show opposite ends of the ruler's path as it moved. The lines are especially clear in the color version in the e-book:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-172 image-border" src="assets/bd02af24-f872-40ae-b53f-79cd91a93532.png" style="width:24.08em;height:22.67em;"/></p>
<p>Finally, the moment for which we have all been waiting—a recipe for amplifying a heartbeat! If you have a heart rate monitor, start by measuring your heart rate. Mine is approximately 87 <strong>beats per minute</strong> (<strong>bpm</strong>) as I type these words and listen to inspiring ballads by Canadian folk singer Stan Rogers. To convert bpm to Hz, divide the bpm value by 60 (the number of seconds per minute), which gives (87 / 60) Hz = 1.45 Hz in my case. The most visible effect of a heartbeat is that a person's skin changes color, becoming more red or purple when blood is pumped through an area. Thus, let's modify our second recipe, which is able to amplify color motions in non-edge areas. Choosing a frequency range centered on 1.45 Hz, we have the following initializer:</p>
<pre>    lazyEyes = LazyEyes(minHz=1.4, maxHz=1.5,<br/>                        useLaplacianPyramid=False,<br/>                        useGrayOverlay=False,<br/>                        imageSize=(320, 240)) </pre>
<p>Customize <kbd>minHz</kbd> and <kbd>maxHz</kbd> based on your own heart rate. Remember to also specify <kbd>numFFTThreads</kbd> and <kbd>numIFFTThreads</kbd> if non-default values work best for you on your machine.</p>
<p>Even when amplified, a heartbeat is difficult to show in still images; it is much clearer in the live video when running the app. However, take a look at the following pair of screenshots. My skin in the left-hand screenshot is more yellow (and lighter), whereas in the right-hand screenshot it is more purple (and darker). For comparison, note that there is no change in the cream-colored curtains in the background:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-173 image-border" src="assets/471e21b7-e4d8-41a7-bbee-6a29e4e94f42.png" style="width:37.75em;height:17.75em;"/></p>
<p>Three recipes are a good start, and they're certainly enough to fill a cooking TV show. So, why not go and observe some other motions in your environment, try to estimate their frequencies, and then configure <kbd>Lazy Eyes</kbd> to amplify them. How do they look with grayscale amplification versus color amplification? Edge (Laplacian) versus area (Gaussian)? What about when different history lengths, pyramid levels, and amplification multipliers are used?</p>
<div>
<div class="packt_infobox">
<p>Check this book's GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a>, for additional recipes, and feel free to share your own by mailing me at <a href="mailto:josephhowse@nummist.com">josephhowse@nummist.com</a>.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter has introduced the relationship between computer vision and digital signal processing. We have considered a video feed as a collection of many signals—one for each channel value of each pixel—and we have learned that repetitive motions create wave patterns in some of these signals. We have used the fast Fourier transform and its inverse to create an alternative video stream that only sees certain frequencies of motion. Finally, we have superimposed this filtered video atop the original to amplify the selected frequencies of motion. There, we summarized Eulerian video magnification in 100 words!</p>
<p>Our implementation adapts Eulerian video magnification to real-time by running the FFT repeatedly on a sliding window of recently captured frames, rather than running it once on an entire prerecorded video. We have considered optimizations such as limiting our signal processing to grayscale, recycling large data structures rather than recreating them, and using several threads.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Seeing things in different light</h1>
                </header>
            
            <article>
                
<p>Although we began this chapter by presenting Eulerian video magnification as a useful technique for visible light, it is also applicable to other kinds of light or radiation. For example, a person's blood beneath the skin (in veins and bruises) is more visible when imaged in <strong>ultraviolet</strong> (<strong>UV</strong>) or in <strong>near infrared</strong> (<strong>NIR</strong>) than in visible light. This is because blood is darker in UV light than in visible light, and skin is more transparent in NIR light than in visible light. Thus, a UV or NIR video might be an even better input when trying to magnify a person's pulse.</p>
<p>We will experiment with invisible light in the next chapter, <a href="5d2f960a-10ed-4efe-a195-47843cdf608b.xhtml">Chapter 8</a>, <em>Stopping Time and Seeing like a Bee</em>. Q's gadgets will inspire us once again!</p>


            </article>

            
        </section>
    </body></html>