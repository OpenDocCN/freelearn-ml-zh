<html><head></head><body>
		<div id="_idContainer082">
			<h1 id="_idParaDest-89"><a id="_idTextAnchor106"/><em class="italic">Chapter 6</em>: Introducing ML Systems Architecture</h1>
			<p>In this chapter, you will learn about general principles of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) systems architecture in the broader context of <strong class="bold">Software Engineering</strong> (<strong class="bold">SWE</strong>) and common issues with deploying models in production in a reliable way. You will also have the opportunity to follow along with architecting our ML systems. We will briefly look at how with MLflow, in conjunction with other relevant tools, we can build reliable and scalable ML platforms.</p>
			<p>Specifically, we will look at the following sections in this chapter:</p>
			<ul>
				<li>Understanding challenges with ML systems and projects</li>
				<li>Surveying state-of-the-art ML platforms</li>
				<li>Architecting the PsyStock ML platform</li>
			</ul>
			<p>You will follow a process of understanding the problem, studying different solutions from lead companies in the industry, and then developing your own relevant architecture. This three-step approach is transferrable to any future ML system that you want to develop.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor107"/>Technical requirements</h1>
			<p>For this chapter, you will need to meet the following prerequisites:</p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of <strong class="source-inline">docker-compose</strong> installed. Please follow the instructions at <a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>.</li>
				<li>Access to Git in the command line and installed as described at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows).</li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your ML platform installed locally as described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Your Data Science Workbench</em>.</li>
			</ul>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor108"/>Understanding challenges with ML systems and projects</h1>
			<p>Implementing a <a id="_idIndexMarker204"/>product leveraging ML can be a laborious task as some new concepts need to be introduced in the book around best practices of ML systems architecture.</p>
			<p>So far in this book, we have shown how MLflow can enable the everyday model developer to have a platform to manage the ML life cycle from iteration on model development up to storing their models on the model registry.</p>
			<p>In summary, at this stage, we have managed to create a platform for the model developer to craft their models and publish the models in a central repository. This is the ideal stage to start unlocking potential in the business value of the models created. In an ML system, to make the leap from model development to a model in production, a change of mindset and approach is needed. After unlocking the value and crafting models, the exploitation phase begins, which is where having an ML systems architecture can set the tone of the deployments and operations of your models.</p>
			<p>ML systems are a specialization of the traditional SWE area and so we can and should leverage the body of knowledge in the SWE realm to architect our systems. Relevant concepts in SWE to our context are the following:</p>
			<ul>
				<li><strong class="bold">Separation of concerns</strong>: A complete system should be separated into different components. Each of the components of the system should be independent and focused on doing one thing well. For instance, a training component should be specialized in training and not doing scoring at the same time.</li>
				<li><strong class="bold">Autonomy</strong>: Each component of the system should stand as an independent autonomous unit and be deployable independently. For example, the deployment of your API system should be independent of the deployment of the training system.</li>
				<li><strong class="bold">Resilience</strong>: One consequence of the separation of concerns and modularity is that we must make sure that if one component of the wider system is faulty, this doesn't affect independent components. If a batch scoring mechanism of a machine platform is broken, it shouldn't affect the real-time system.</li>
				<li><strong class="bold">Scalability</strong>: We should be <a id="_idIndexMarker205"/>able to scale the different components of our system independently and in accordance with its workload.</li>
				<li><strong class="bold">Testability</strong>: This represents the ability of a system to being tested and its functionality being verified against a set of representative inputs. In ML systems, this is particularly hard given the non-deterministic nature of models.</li>
				<li><strong class="bold">Continuous deployment/delivery</strong>: This represents the ability to deploy systems in shorter cycles with almost no friction between a change in the code, configuration, models, or data, in the ML case, to have the new version of the system.</li>
				<li><strong class="bold">Composability</strong>: We should be able to reuse the components of our systems in future projects to increase the return on investment. So, an ML engineer needs to be sure that the code and components being developed are easily reusable and/or interoperable with other systems. </li>
				<li><strong class="bold">Maintainability</strong>: This is the ease at which a system can be modified, fixed, and improved to meet and adapt to the demands of a changing environment.</li>
			</ul>
			<p>At this stage, we can briefly introduce and refine our use case, of stock prediction, to develop our ML platform in the PsyStock company.</p>
			<p>Based on the work done so far in prototyping models to <strong class="bold">predict the price of Bitcoin</strong>, the business development department of the company decided to start its first product as a <strong class="bold">Prediction API for cryptocurrencies</strong> as they are becoming a popular technology in the corporate world. A team was assembled that decided to investigate challenges and state-of-the-art platforms, and then architect the company's own platform.</p>
			<p>An ML project generally involves many departments of a company. Imagining the hypothetical case of PsyStock, a typical <a id="_idIndexMarker206"/>ML project team involves the following stakeholders:</p>
			<ul>
				<li><strong class="bold">Data science team</strong>: Responsible for building and developing the model with the goal of achieving the highest accuracy on their prediction of cryptocurrency prices and market movements.</li>
				<li><strong class="bold">ML/data engineering team</strong>: Responsible for the engineering components, including data acquisition, preparation, training, and deployment, and is interested in the system being correctly deployed and running on spec in production.</li>
				<li><strong class="bold">Infrastructure team</strong>: Responsible for providing compute and infrastructure resources. Expects that the system will not cause an operational load to the team.</li>
				<li><strong class="bold">Product team</strong>: Provides integration with the web platforms and the overall software of the company and drives the feature creation, ensuring a speedy inference speed.</li>
				<li><strong class="bold">Business development/marketing team</strong>: Packages and markets the product and monitors the business performance of the product.</li>
			</ul>
			<p>In the next section, we will understand general challenges in ML systems and projects.</p>
			<p>ML is an important <a id="_idIndexMarker207"/>application of technology to help unlock value in organizations using data. In the application of ML in the business world, there are no standard practices defined and a big number of organizations struggle to get products backed by ML in production.</p>
			<p>In the real world, a naive way to move models into production would consist of the following steps:</p>
			<ol>
				<li>Data scientist produces a model in a notebook environment and implements the code in R.</li>
				<li>The data scientist shares the notebook with the engineering team, signaling that they're ready to send their model to production.</li>
				<li>The engineering team reimplements the training process in a language that they can understand, in this case, Python.</li>
				<li>A long process of trial and error until the data science team and engineering team are in agreement that the model produced by the bespoke training system is producing equivalent outputs to the original one.</li>
				<li>A new system is <a id="_idIndexMarker208"/>created and developed to score systems and the engineering team notes a high latency. The model is sent to redevelopment as it can't be redeveloped in the current status.</li>
			</ol>
			<p>The situation described in the previous paragraph is more common than you might imagine. It is described in detail in the paper by <em class="italic">D. Sculley et al., Hidden Technical Debt in Machine Learning Systems (2015)</em>. The following risk factors and technical debt related to implementing ML platforms naively were identified by a team at Google:</p>
			<ul>
				<li><strong class="bold">Boundary erosion</strong>: ML systems by their nature mix signals of different logical domains. Maintaining clear logic of business domains as is possible in SWE is challenging. Another natural issue is the temptation of using a model output as input of a third model, <em class="italic">A</em>, and changes might have unexpected effects in model <em class="italic">B</em>.</li>
				<li><strong class="bold">Costly data dependencies</strong>: Fresh, accurate, and dependable data is the most important ingredient of an ML system. For example, in the cryptocurrency case, in order to be able to predict, an external API might be consulted in combination with social network sentiment signals. At a given point, one of the data signals might be unavailable, making one of the components unavailable. Data distributions in the real world can change, causing the model inference to be irrelevant in the real world.</li>
				<li><strong class="bold">Feedback loops</strong>: In some contexts, the model influences the data selected for training. Credit scoring is a good example of such a case. A model that decides who gets credit for the next re-training of the model will select training data from the population that was affected by the model. Analyzing the effect of the model on the ground data is important to take into consideration when developing your model.</li>
				<li><strong class="bold">System-level anti patterns</strong>: ML systems are notoriously known for harboring glue code with different packages and without proper abstractions. In some cases, multiple languages are used to implement in the library given the iterative nature of developing code in a notebook.</li>
				<li><strong class="bold">Configuration management</strong>: Generally left as an afterthought in ML systems, information about the configuration that yielded a particular result is paramount for the <a id="_idIndexMarker209"/>post-analysis of models and deployment. Not using established configuration management practices can introduce errors in ML pipelines.</li>
				<li><strong class="bold">Monitoring and testing</strong>: Integration testing and unit testing are common patterns in SWE projects that due to the stochastic nature of ML projects are harder to implement.</li>
			</ul>
			<p>One important practice to tackle the challenges in ML systems is to add extensive tests on critical parts of the process, on your code, during model training and when running on your system, as shown in <em class="italic">Figure 6.1</em>:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/image0012.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Testing in ML systems extracted from https://research.google/pubs/pub46555/</p>
			<p>What <em class="italic">Figure 6.1</em> illustrates is one approach to address technical debt by testing the different parts of the system through standard software practices with the addition of specialized monitoring <a id="_idIndexMarker210"/>for data predictions. The important new additions are tests on data and tests on the model, so testing incoming data and training data and at the same time being able to monitor these tests and decide whether the system passes the relevant criteria is critical.</p>
			<p>MLflow as a platform addresses some of the issues referred to in this section as problems for ML systems. MLflow is focused on a specific set of dimensions of the ML technical debt and is a good pillar component to create an ML platform.</p>
			<p>In the next section, we will look at some examples of state-of-the-art robust ML engineering systems, to guide our development.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor109"/>Surveying state-of-the-art ML platforms</h1>
			<p>At a high level, a <a id="_idIndexMarker211"/>mature ML system has the components outlined in <em class="italic">Figure 6.2</em>. These components are ideally independent and responsible for one particular feature of the system:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/image0023.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Components of an ML platform</p>
			<p>Following the lead from SWE modularization, these general components allow us to compare different ML platforms and also specify our PsyStock requirements for each of the components. The components that we choose to use as a reference for architecture comparison are the following:</p>
			<ul>
				<li><strong class="bold">Data and feature management</strong>: The component of data and feature management is responsible for data acquisition, feature generation, storing, and serving the modules upstream.</li>
				<li><strong class="bold">Training infrastructure</strong>: The component that handles the process of the training of models, scheduling, consuming features, and producing a final model.</li>
				<li><strong class="bold">Deployment and inference</strong>: The responsibility of this unit is for the deployment inference and batch scoring of a model. It is the external face of the system and is accessible to external systems.</li>
				<li><strong class="bold">Performance and monitoring</strong>: A component that handles observability, metrics posted by different systems, and monitoring systems in production.</li>
				<li><strong class="bold">Model management</strong>: Manages model artifact versions and the life cycle of models.</li>
				<li><strong class="bold">Workflow management</strong>: The component responsible for orchestrating batch workflows and processing pipelines.</li>
			</ul>
			<p>After having <a id="_idIndexMarker212"/>described the different components of an ML platform, we will look at some examples starting with Uber's Michelangelo.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor110"/>Getting to know Michelangelo</h2>
			<p>Uber was one of the <a id="_idIndexMarker213"/>first companies to document widely their realization that an ML platform was critical to unlocking value on the data produced.</p>
			<p>The internal <a id="_idIndexMarker214"/>motivations at Uber to build the platform were the following:</p>
			<ul>
				<li>Limited impact of ML due to huge resources needed when translating a local model into production.</li>
				<li>Unreliable ML and data pipelines.</li>
				<li>Engineering teams had to create custom serving containers and systems for the systems at hand.</li>
				<li>Inability to scale ML projects.</li>
			</ul>
			<p>The following <em class="italic">Figure 6.3</em> (retrieved from <a href="https://eng.uber.com/michelangelo-machine-learning-platform">https://eng.uber.com/michelangelo-machine-learning-platform</a>) shows the different components of Michelangelo. One significant component is the data component of the Uber infrastructure decoupling real-time data infrastructure with streaming systems such as Kafka to acquire data from the outside from where the data flows to a training process, and from there to scoring in both real-time and offline mode. Distinctive features are a separation of the batch world and real-time world and the existence of generic prediction services for API and batch systems:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/image0033.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure.6.3 – Michelangelo architecture </p>
			<p>The components <a id="_idIndexMarker215"/>that we choose to use as a <a id="_idIndexMarker216"/>reference for architecture comparison are the following:</p>
			<ul>
				<li><strong class="bold">Data and feature management</strong>: It consists of a centralized data store with all the features that are needed to serve models and train models. The feature data store can be accessed in real time and in batch. For the batch scenario, they use a database system called Hive and for real time, they use Cassandra.</li>
				<li><strong class="bold">Training infrastructure</strong>: Distributed training infrastructure with a tool called <strong class="bold">Horovod</strong> (<a href="https://github.com/horovod/horovod">https://github.com/horovod/horovod</a>) with specialized and bespoke components and enhanced reporting. It provides custom metrics for each type of model (deep learning, models, feature importance, and so on). The output of the training job is the model repo using as a backend the Cassandra database.</li>
				<li><strong class="bold">Deployment and inference</strong>: The systems deployed through standard SWE practices (CI/CD, rollbacks on metrics monitoring, and so on), generally compiled as artifacts served over Uber data centers. A prediction service that receives a request and based on header information routes pre-loads the right model and feeds the prediction vector, using an internal DSL that is able to query for further data augmentation on the serving layer of the feature store.</li>
				<li><strong class="bold">Performance and monitoring</strong>: It leverages the general centralized logging system of the <a id="_idIndexMarker217"/>company. For monitoring predictions, metrics <a id="_idIndexMarker218"/>are produced of predictions and real-world values and differences are logged. The errors of the model can in this way be analyzed and monitored.</li>
				<li><strong class="bold">Model management</strong>: Models are compiled as artifacts and stored in a Cassandra data store.</li>
				<li><strong class="bold">Workflow management</strong>: Provides an API for wiring the pipelines. It contains a management plane with a UI that allows the management of models and deployments. Workflow management is API-driven and can be managed in either Python or Java from the outside.</li>
			</ul>
			<p>The clear advantage for a company such as Uber to have built their own system is agility and the ability to cater to their very specific use case.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor111"/>Getting to know Kubeflow</h2>
			<p>Kubeflow in some way is an <a id="_idIndexMarker219"/>open source platform for the ML life <a id="_idIndexMarker220"/>cycle for <strong class="bold">Kubernetes</strong> environments. It's basically an ecosystem of tools that <a id="_idIndexMarker221"/>work together to provide the main components of an ML platform. Kubeflow was initially developed at Google and it's currently a very active open source project.</p>
			<p><strong class="bold">Kubernetes</strong> is one of the leading open source computational environments that allows flexibility in allocating computing and storage resources for containerized workloads. It was created originally at Google. In order to understand Kubeflow, a basic understanding of Kubernetes is needed. The following official documentation link contains the prerequisites to understand the basics: <a href="https://kubernetes.io/docs/concepts/overview/">https://kubernetes.io/docs/concepts/overview/</a>.</p>
			<p>As shown in <em class="italic">Figure 6.4</em>, it uses the foundation of Kubernetes and provides a set of applications for the ML workflow where different tools compatible with the standards set by Kubeflow can be coalesced to provide a coherent set of services:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/image0043.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Retrieved from https://www.kubeflow.org/docs/started/kubeflow-overview/</p>
			<p>The components that we <a id="_idIndexMarker222"/>choose to use as a reference for <a id="_idIndexMarker223"/>architecture comparison are the following:</p>
			<ul>
				<li><strong class="bold">Data and feature management</strong>: Kubeflow provides integration with big data tools such as Spark and others. A component of the ecosystem used for data and feature management is called Feast, an open source feature for ML.</li>
				<li><strong class="bold">Training infrastructure</strong>: Kubeflow provides specific types of Kubeflow operators for common models such as, for instance, TensorFlow, PyTorch, and custom-made ones. The training jobs will basically be specific Kubernetes jobs.</li>
				<li><strong class="bold">Deployment and inference</strong>: Kubeflow provides multiple integrations with third-party tools such as TensorFlow Serving, Seldon Core, and KFServing with different trade-offs and maturity levels.</li>
				<li><strong class="bold">Performance and monitoring</strong>: Prometheus is a general tool used for monitoring within the Kubernetes environment and can be leveraged in this context.</li>
				<li><strong class="bold">Model management</strong>: Not a specific tool for managing models but tools such as MLflow can be added to cover the model management life cycle.</li>
				<li><strong class="bold">Workflow management</strong>: Workflow management is leveraged through a specific tool called Kubeflow Pipelines built on top of a generic pipeline tool for Kubernetes called Argo Workflows. It allows multi-step pipelines to be built in code.</li>
			</ul>
			<p>After looking at reference <a id="_idIndexMarker224"/>architectures, we will now <a id="_idIndexMarker225"/>spend time crafting our own, armed with the state-of-the-art knowledge available in the industry.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor112"/>Architecting the PsyStock ML platform</h1>
			<p>There is a <a id="_idIndexMarker226"/>set of desirable tenets that we can define for our ML platform based on a distillation of the research on best practices and example reference architectures. The main tenets that <a id="_idIndexMarker227"/>we want to maintain in our platform are the following:</p>
			<ul>
				<li><strong class="bold">Leverage</strong> <strong class="bold">open systems and standards</strong>: Using open systems such as the ones available in MLflow allows longevity and flexibility to leverage the open source community advances and power to extend the company ML platform at a lower cost.</li>
				<li><strong class="bold">Favor</strong> <strong class="bold">scalable solutions</strong>: A company needs to be prepared for a future surge in growth; although this is the first version, the ability to surge on-demand from training and perspective needs to be in place.</li>
				<li><strong class="bold">Integrated</strong> <strong class="bold">reliable data life cycle</strong>: Data is the center of gravity of the ML platform and should be managed in a reliable and traceable manner at scale.</li>
				<li><strong class="bold">Follow</strong> <strong class="bold">SWE best practices</strong>: For example, separation of concerns, testability, CI/CD, observability, and modularity.</li>
				<li><strong class="bold">Maintain</strong> <strong class="bold">vendor and cloud independence</strong>: PsyStock being a start-up is operating in a very dynamic environment and in different geographies with access to <a id="_idIndexMarker228"/>different clouds and, in some cases, with compliance requirements of not moving the data from the given geography. So, being cloud-agnostic and being able to have workloads in different environments is a competitive advantage.</li>
			</ul>
			<p>These tenets will <a id="_idIndexMarker229"/>allow us to frame our systems architecture within an open and low-cost solution for the company and allow the flexibility of running on the different systems on-premises, in the cloud, or local.</p>
			<p>We have previously defined the business requirements of the prediction use cases, namely detection of the movement of cryptocurrency and value prediction. To leverage this and other use cases, the creation of an ML platform is critical to the company.</p>
			<p>Now, armed with the knowledge from the research and description of state-of-the-art systems, we will next define eliciting the features of our ML platform.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor113"/>Describing the features of the ML platform</h2>
			<p>Creating a <a id="_idIndexMarker230"/>specification of features is extremely important to keep the development efforts focused on a narrow set of features that unlock value to users of the platform. In this section, we will elicit the features that will realize the best value of an ML platform.</p>
			<p>In our system, we want to be able to have the following features:</p>
			<ul>
				<li><strong class="bold">Feature: Schedule training jobs</strong>: A data scientist needs to be able to schedule training jobs for their models using configuration or equivalent code.</li>
				<li><strong class="bold">Feature: Deploy seamlessly different models developed from the data science workbench</strong>: The company already has a data science workbench developed in <em class="italic">Chapter 3</em>, <em class="italic">Your Data Science Workbench</em>. We want to be able to leverage all the work previously done so models developed on the platform can be deployed in production.</li>
				<li><strong class="bold">Feature: Allow recalibration of models in presence of new data</strong>: When new data arrives in a specific location, a new model needs to be generated automatically and stored in a model registry accessible to systems and humans of the platform.</li>
				<li><strong class="bold">Feature: Submit and configure batch scoring jobs</strong>: The platform should allow the relevant users to configure and schedule batch jobs in the presence of new data.</li>
				<li><strong class="bold">Feature: Efficient inference API-based scoring for the following APIs</strong>: Given a model it <a id="_idIndexMarker231"/>should be a feature of the platform the creation of matching API using the model schema.</li>
			</ul>
			<p>After discussing the ideal features of an ML system, we will start in the next section with architecting the system at a high level.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor114"/>High-level systems architecture</h2>
			<p>We will now <a id="_idIndexMarker232"/>focus on defining the building blocks of our architecture and the different data flows between the different components.</p>
			<p>Based on the features specified and tenets of the previous section, our ML platform and solution should contain the following components as described by the architecture diagram in <em class="italic">Figure 6.5</em>.</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/image0053.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure.6.5 – Architectural diagram of an ML platform</p>
			<p>This diagram is <a id="_idIndexMarker233"/>specifically agnostic from technology choices as this will be done in the respective upcoming chapters where each of the components' engineering<a id="_idTextAnchor115"/> will be explored fully.</p>
			<ol>
				<li value="1"><strong class="bold">Data and feature management</strong>: This is executed by the <strong class="bold">Feature / Data Layer</strong>, which receives feature registrations from the workbench and allows the registration of datasets from the workbench. The data layer provides data to the training environment.</li>
				<li><strong class="bold">Training infrastructure</strong>: The training infrastructure component allows the schedule of training of jobs based on a request from the data science workbench.</li>
				<li><strong class="bold">Deployment and inference</strong>: The deployment environment consumes models to execute either in batch or in real time prompted either by data in the data layer or by requests via production systems.</li>
				<li><strong class="bold">Performance and monitoring</strong>: This is accomplished through the central component of monitoring and metrics that all the systems surrounding the component publish metrics into.</li>
				<li><strong class="bold">Model management</strong>: Encapsulated by the component of <strong class="bold">Model Registry</strong>, which contains a store and the associated life cycles projects. The input comes primarily from the training jobs and the data science workbench.</li>
				<li><strong class="bold">Workflow management</strong>: This is a component that allows the orchestration of the different systems. For example, it allows scheduling jobs and dependency management, enforcing the order of execution. For example, an inference batch job can only be <a id="_idIndexMarker234"/>executed after a training job. This can be achieved through the operating system using a Cron system or through more sophisticated workflow tools such as Airflow.</li>
			</ol>
			<p>We will next briefly touch on how we will realize the ideas outlined in this section with <strong class="bold">MLflow</strong>.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor116"/>MLflow and other ecosystem tools</h2>
			<p>MLflow is a tool that was <a id="_idIndexMarker235"/>created by the open source community to address a <a id="_idIndexMarker236"/>gap in open systems for ML systems, focused on reproducibility, model management, and deployment. MLflow is by no means a complete tool; it needs other components and is a central part of an ML solution when its strengths are leveraged.</p>
			<p>In recent years, systems such as <strong class="bold">Kubeflow</strong> have <a id="_idIndexMarker237"/>been emerging in the Kubernetes world to help manage the infrastructure side of ML systems and being the actual deployment environment.</p>
			<p><strong class="bold">Minio</strong> is a storage system that <a id="_idIndexMarker238"/>ships with Kubeflow that will be used as an agnostic storage mechanism for metadata and datasets and provides an abstraction for storage on both cloud and local environments.</p>
			<p>Having identified best practices in the industry with regards to ML platforms, outlining our requirements, and, in this section, architecting our ML platform, we will spend the next four chapters of the book building each of the components of our platform.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor117"/>Summary</h1>
			<p>In this chapter, we introduced the concepts involved in architecting ML systems, mapped stakeholders, identified common issues and best practices, and outlined the initial architecture. We identified critical building blocks of an ML systems architecture on the data layer and modeling and inference layer. The interconnection between the components was stressed and a specification of features was outlined.</p>
			<p>We also addressed how MLflow can be leveraged in your ML platform and the shortcomings that can be complemented by other reference tools.</p>
			<p>In the next chapters and section of the book, we will focus on applying the concepts learned so far to real-life systems and we will practice by implementing the architecture of the PsyStock ML platform. We will have one chapter dedicated to each component, starting from specification up to the implementation of the component with practical examples.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor118"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation at the following links: </p>
			<ul>
				<li><a href="https://www.mlflow.org/docs/latest/models.html">https://www.mlflow.org/docs/latest/models.html</a></li>
				<li>High interest of technical debt – <a href="https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a></li>
				<li><strong class="bold">CS 329S</strong>: ML systems design, <em class="italic">Chip Huyen</em> – <a href="https://cs329s.stanford.edu">https://cs329s.stanford.edu</a>, 2021</li>
			</ul>
		</div>
	</body></html>