<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer028">
<h1 class="chapter-number" id="_idParaDest-72"><a id="_idTextAnchor072"/>3</h1>
<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Preparing for ML Development</h1>
<p>In <em class="italic">Part 2</em> of the book, we will examine the ML process. We will start from the preparation work, which includes ML problem framing to define an ML problem; data preparation and feature engineering to get the data ready; followed by the ML model development phases, which include model training, model validation, model testing, and model deployment. We will end <em class="italic">Part 2</em> with neural networks and DL.</p>
<p>In this chapter, will discuss the two ML preparation tasks: ML problem framing and data preparation. We will address the following questions for the problem we are solving:</p>
<ul>
<li>What are the business requirements?</li>
<li>Is ML the best way to solve the problem?</li>
<li>What are the inputs and outputs for the problem?</li>
<li>Where is my data?</li>
<li>How do I measure the success of the ML solution?</li>
<li>Is the data ready?</li>
<li>How do I collect my data?</li>
<li>How do I transform and construct my data?</li>
<li>How do I select features for the ML model?</li>
</ul>
<p>It is very important that we identify the business requirements, understand the problem and its inputs/outputs, establish the business success measurements, and collect, transform, and construct high-quality datasets before model training and deployment. Through this process, we will learn and develop the following skills:</p>
<ul>
<li>Defining and understanding a business problem</li>
<li>Translating it to an ML problem</li>
<li>Defining and measuring the success of the business problem </li>
<li>Defining high-quality datasets</li>
<li>Collecting data</li>
<li>Transforming and constructing data</li>
<li>Feature engineering</li>
</ul>
<p>Let’s keep these questions and skills in mind as we go through this chapter.</p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Starting from business requirements</h1>
<p>A typical <a id="_idIndexMarker140"/>ML process starts by defining business requirements. Follow the following steps to define the business requirements of the problem:</p>
<ol>
<li>Clearly define the business outcome that your ML solution is supposed to achieve, among all the stakeholders. For example, for a prediction ML problem, we need to define a range of accuracy that is acceptable by the business and agreed upon by all the stakeholders.</li>
<li>Clearly define the data source of the ML problem. All ML projects are based on loads of data. You need to clearly define what the reliable data sources are, including training data, evaluation data, testing data, and a feed of regularly updated data.</li>
<li>Clearly define the frequency of ML model updating (since data distributions drift over time), and the strategies for maintaining production during the model updating times.</li>
<li>Clearly define the financial indications of the ML product or project. Understand any limitations such as resource availability and budget planning, and so on.</li>
<li>Clearly define the rules, policies, and regulations for the problem.</li>
</ol>
<p>Let us look at an example of the problem and the business requirements:</p>
<ul>
<li><strong class="bold">Example 1</strong>: A real estate company called Zeellow does great business buying and selling properties in the United States. Due to the nature of the business, accurately predicting house prices is critical for Zeellow. Over the past few years, they have accumulated a large amount of historical data for US houses.</li>
</ul>
<p>Here, the business <a id="_idIndexMarker141"/>outcome is accurately predicting house prices in the United States. It is agreed by business stakeholders that more than 2% prediction error is not acceptable. The data source is defined as the in-house historical property database. Due to database updates, the model needs to be updated every month. There are two data scientists and two data engineers working full-time on the project, and enough funding has been provided. There are no regulations about the house data and the ML problem.</p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor075"/>Defining ML problems</h1>
<p>After we have identified the business requirements, we need to define the problem by identifying the <a id="_idIndexMarker142"/>features and target of the problem. For <em class="italic">Example 1</em>, the house price is the target, and features are the house attributes that affect the house price, such as the location, the house size (total square footage), the age of the house, the number of bedrooms and bathrooms of the house, and so on. <em class="italic">Table 3.1</em> shows a small sample dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Table 3.1 – Example 1 dataset " height="372" src="image/Table_3.1.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 – Example 1 dataset</p>
<p>The problem is then defined as building a model among the features and the target and discovering their relationships. During the problem definition process, we will understand the problem better, decide whether ML is the best solution for the problem, and to what category the problem belongs.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Is ML the best solution?</h2>
<p>When facing a problem, the first thing we need to do is choose the best modeling/solution for the problem. For example, given the initial position and <a id="_idIndexMarker143"/>speed of a physical object, its mass, and the forces acting on it, how can we precisely predict its position at any time <em class="italic">t</em>? For this problem, a traditional mathematical model, based on Newton’s laws in the classic mechanical world, works much better than any ML models!</p>
<p>While scientific modeling provides the mathematical relationship between the prediction target and features, there are many problems that are very hard or even impossible to build a mathematical model for, and ML may be the best way to solve these problems. How do we know whether ML is the best way to solve a given problem? There are several conditions that need to be checked when judging whether ML is a potentially good solution for a problem:</p>
<ul>
<li>There is a pattern among the features and the predicting target. For <em class="italic">Example 1</em>, we know the house price will be related to house features such as location, the total square footage, age, and so on, and there are patterns between the house price and its features.</li>
<li>The existing pattern or relationship cannot be modeled using mathematics or science. For <em class="italic">Example 1</em>, the relationships between the price of the house and the features cannot be mathematically formulated. </li>
<li>There is plenty of quality data available. For <em class="italic">Example 1</em>, Zeellow has accumulated a large amount of historical data for US houses, including prices and their features.</li>
<li>In <em class="italic">Example 1</em>, Zeellow needs to predict house prices. Apparently, there are relationships between the house price and the features of the house, but it is very difficult to build a mathematical model to describe the relationships. Since we have enough historical data, ML is potentially a good way to solve the problem.</li>
</ul>
<p>Let’s look at more examples and see whether ML is the best solution for them:</p>
<ul>
<li><strong class="bold">Example 2</strong>: Zeellow Mortgage is a subsidiary of Zeellow and is a mortgage business in the States. They have also accumulated a large amount of historical data on mortgage applicants and are trying to automate the decision process of approval or denial for new applications.</li>
<li><strong class="bold">Example 3</strong>: Zeellow Appraisal is a subsidiary of Zeellow and they evaluate the prices of existing houses when they are under contract. One good approximation <a id="_idIndexMarker144"/>for that is to see how similar properties are priced, and this leads to the grouping of properties.</li>
</ul>
<p>After we examine these two problems and check their conditions, we can decide whether ML is the way to solve the problems. Further, we will look at the categories of ML problems and what types our three examples belong to.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>ML problem categories</h2>
<p>ML problems can be divided into several categories. For <em class="italic">Example 1</em>, Zeellow needs to predict house prices, which is a continuous value, compared to <em class="italic">Example 2</em> where ML needs to <a id="_idIndexMarker145"/>predict an approval (yes) or denial (no) for a mortgage application. An ML problem that outputs a continuous value is called <strong class="bold">Regression</strong>, while a <a id="_idIndexMarker146"/>problem that outputs discrete values (two or more) is called <strong class="bold">Classification</strong>. If there <a id="_idIndexMarker147"/>are two outputs (yes and no), then <a id="_idIndexMarker148"/>it’s called <strong class="bold">Binary classification</strong>. If there are more than <a id="_idIndexMarker149"/>two outputs, then it’s called <strong class="bold">Multiclass classification</strong>.</p>
<p>In both <em class="italic">Example 1</em> and <em class="italic">Example 2</em>, we let the machine learn from the existing dataset that is labeled with results. <em class="italic">Example 1</em>’s datasets are for houses that have been sold in the past few years and thus include house location, the number of bedrooms and bathrooms, the age of the house and the sale price. <em class="italic">Example 2</em>’s datasets include the mortgage applicant’s gender, age, income, marital status, and so on, and whether the application <a id="_idIndexMarker150"/>was approved or denied. Since the inputs for both examples are labeled, they are called <strong class="bold">supervised learning</strong>. Input data such as the house’s location, the number of bedrooms and bathrooms, and the age of the house in <em class="italic">Example 1</em>, and the applicant’s gender, age, income, and marital status in <em class="italic">Example 2</em>, are called <strong class="bold">features</strong> since they reflect the datasets attributes (features).</p>
<p><strong class="bold">Unsupervised learning problems</strong>, on the other hand, have inputs that are not labeled. For <em class="italic">Example 3</em>, Zeellow <a id="_idIndexMarker151"/>Appraisal needs to divide houses into different groups, and each group has similar features. The focus here is not on the house prices but to identify meaningful patterns in the data and split the houses into groups. Since we do not label the datasets, it is an unsupervised learning problem.</p>
<p>Another type <a id="_idIndexMarker152"/>of machine learning problem is <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>). In RL, you don’t collect examples with labels. You set up the model (agent) and <a id="_idIndexMarker153"/>a reward function to reward the agent when it performs a task. With reinforcement learning, the agent can learn very <a id="_idIndexMarker154"/>quickly how to outperform humans. More details can be found on the Wikipedia page (<a href="https://en.wikipedia.org/wiki/Reinforcement_learning">https://en.wikipedia.org/wiki/Reinforcement_learning</a>).</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>ML model inputs and outputs</h2>
<p>In an ML problem, the inputs are usually datasets, including all kinds of data formats/media such as numerical data, images, audio, and so on. Input datasets are extremely <a id="_idIndexMarker155"/>important and will decide how good the ML model will be. </p>
<p>For supervised learning, labeled data is used to train the model, which is a piece of software representing what a machine has learned from the training data. Inputs to supervised learning are marked datasets of features and targets, and the model learns by comparing the labeled target values with the model outputs to find errors. The error<strong class="bold"> </strong>is what we want to optimize. Note we refer to the optimization of the error, not the minimization of the error. In other words, minimizing the error to zero may not generate the best model.</p>
<p>Within supervised learning, the output for a regression model is a continuous numerical value, and for a classification model, the output is a discrete value indicating a category (yes/no for binary classifications).</p>
<p>For unsupervised learning, input data is not labeled, and usually, the objective is to find the input data patterns <a id="_idIndexMarker156"/>and group <a id="_idIndexMarker157"/>them into different categories, called <strong class="bold">clustering</strong> or <strong class="bold">grouping</strong>.</p>
<p>For reinforcement learning, the input is a state, and the output is the action performed by ML. Between input and output, we have a function that takes a state as input and returns an action as output. </p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor079"/>Measuring ML solutions and data readiness</h1>
<p>After we define the problem and conclude that ML is a potentially good solution to the problem, we need <a id="_idIndexMarker158"/>to set up a way of measuring the problem <a id="_idIndexMarker159"/>solution and whether it’s ready for production deployment. For <em class="italic">Example 1</em>, we need to have a consensus as to what range is acceptable for the house prediction errors and we can use the ML model in production.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>ML model performance measurement</h2>
<p>To evaluate the performance of ML solutions, we use ML metrics. For regression models, there <a id="_idIndexMarker160"/>are three metrics: mean square error, mean absolute error, and r-square. For classification models, we use the confusion matrix. We will discuss that more in the following chapters.</p>
<p>Is the ML solution ready to be deployed? We need to circle back to the model’s original business goals in the ML problem framing:</p>
<ul>
<li>For Zeellow, is predicting a house price with 95% accuracy good enough?</li>
<li>For Zeellow Mortgage, are we allowed to make a decision with 95% confidence?</li>
<li>For Zeellow Appraisal, can we categorize a house into the proper group with 95% accuracy?</li>
</ul>
<p>After we have evaluated the ML model with the testing datasets and confirmed that the model reaches the business requirements, we will be ready to deploy it into production.</p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/>Data readiness</h2>
<p>Data plays such <a id="_idIndexMarker161"/>a significant role in the machine learning process that the quality of data has a huge impact on the model performance, the so-called <em class="italic">garbage in, garbage out</em>. An ML model’s accuracy relies on many factors, including the size and quality of the dataset. The perception that <em class="italic">The more data, the more model accuracy</em> is not always true. In real time, it is a big challenge to obtain a big amount of clean, high-quality data. Often in an ML project, we spend a big portion of time collecting and preparing the datasets. Depending on the ML problem we need to solve, there are several ways to collect data and sources to collect it from. For example, we can go with the following:</p>
<ul>
<li>Historical data collected by companies, such as user data and media data</li>
<li>Publicly available data from research institutes and government agencies</li>
</ul>
<p>How much <a id="_idIndexMarker162"/>data is enough for my ML model, and how do we measure the quality of our data? It depends on the type of machine learning problem you want to solve. As part of the problem framing process, we need to check and make sure we have enough high-quality data to go with building an ML solution. Next, we will discuss more details about data preparation and feature engineering.</p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Collecting data</h1>
<p><strong class="bold">Data collection</strong> is collecting <a id="_idIndexMarker163"/>the source data and storing it in a central safe location. In the data collection phase, we try to answer the following questions: </p>
<ul>
<li>What is the nature of the problem and do we have the right data for it?</li>
<li>Where is the data and do we have access to the data?</li>
<li>What can we do to ingest all the data into one central repository?</li>
<li>How do we safeguard the central data repository?</li>
</ul>
<p>These questions are crucial in any ML project because, in a real business, data is typically spread across many different heterogeneous source systems, and bringing all the source data together to form a dataset may involve huge challenges.</p>
<p>A common <a id="_idIndexMarker164"/>data collection and consolidation process called <strong class="bold">Extract, Transform, and Load</strong> (<strong class="bold">ETL</strong>) has the following steps:</p>
<ol>
<li value="1"><strong class="bold">Extract</strong>: Pull the data from the various sources to a single location.</li>
<li><strong class="bold">Transform</strong>: During data extraction and consolidation, we may need to change the data format, modify some data, remove duplicates, and so on. </li>
<li><strong class="bold">Load</strong>: Data is loaded into a single repository, such as Google Cloud Storage or Google BigQuery.</li>
</ol>
<p>During this ETL process, we also need to address the data size, quality, and security:</p>
<ul>
<li><strong class="bold">Data size</strong>: How much data do we need to get useful ML results? While the answer is <a id="_idIndexMarker165"/>dependent on the ML problem, the rule of thumb is that the training datasets will be several times more than the model’s trainable parameters. For example, a typical regression problem may need ten times as many observations as features. A typical image classification problem may require tens of thousands of images to create a high-accuracy image classifier. Generally speaking, simple models trained with large datasets perform better than complex models with small datasets. </li>
<li><strong class="bold">Data quality</strong>: This <a id="_idIndexMarker166"/>can include the following:<ul><li><strong class="bold">Reliability</strong>: Are the data sources reliable? Is the dataset labeled correctly? Is the dataset filtered properly? Are there duplicates or missing values?</li>
<li><strong class="bold">Feature representation</strong>: Does the dataset represent the useful ML features? Are there any outliers? </li>
<li><strong class="bold">Consistency between training and production data</strong>: Are there any skews that exist between the datasets for training and for production? </li>
</ul></li>
<li><strong class="bold">Data security</strong>: Is the <a id="_idIndexMarker167"/>data secure? Do we need to <a id="_idIndexMarker168"/>encrypt the data? Is there any <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) in the dataset? Are there any laws or regulatory requirements for accessing the dataset?</li>
</ul>
<p>After the data is collected and stored in a central safe repository, we need to construct and transform it into the right format so that it can be used for ML model training. We will discuss this in the next section.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor083"/>Data engineering</h1>
<p>The objectives of data engineering are to make sure that the datasets represent the real ML problem <a id="_idIndexMarker169"/>and have the right format for ML model training. Often, we use statistical techniques to sample, balance, and scale datasets, and handle missing values and outliers in the datasets. This section covers the following:</p>
<ul>
<li>Sampling data with sub-datasets</li>
<li>Balancing dataset classes</li>
<li>Transforming data</li>
</ul>
<p>Let us start with data sampling and balancing.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>Data sampling and balancing</h2>
<p>Data sampling is a <a id="_idIndexMarker170"/>statistical analysis technique used to select, manipulate, and <a id="_idIndexMarker171"/>analyze a representative subset in a larger dataset. Sampling <a id="_idIndexMarker172"/>data plays an important role in data <a id="_idIndexMarker173"/>construction. When sampling data, you need to be very careful not to introduce biased factors. For more details, please refer to <a href="https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/sampling">https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/sampling</a>.</p>
<p>A classification dataset has more than two dataset classes. We call the classes that make up a large proportion of the set <strong class="bold">majority classes</strong>, and those that make up a small proposition <strong class="bold">minority classes</strong>. When the dataset has skewed class proportions – meaning the proportion of the minority classes is significantly less than that of the majority classes, it is an imbalanced dataset, and we need to balance it using statistical techniques called <strong class="bold">downsampling</strong> and <strong class="bold">upweighting</strong>. Let’s consider a fraud detection dataset with 1 positive and 200 negatives. The model training process will not reflect the real problem since the positive proportion is so small. In this case, we will need to process the dataset in two steps:</p>
<ul>
<li><strong class="bold">Downsampling</strong>: Extract <a id="_idIndexMarker174"/>data examples from the dominant class to balance the classes. With a factor of 50 downsampling, the proportion will be 40:1 after downsampling.</li>
<li><strong class="bold">Upweighting</strong>: Increase <a id="_idIndexMarker175"/>the dominant class weight by the same factor of 50 (the same as the downsampling factor) during ML model training.</li>
</ul>
<p>Some ML <a id="_idIndexMarker176"/>libraries have provided built-in features <a id="_idIndexMarker177"/>to facilitate the process. For more details about the techniques and why we perform the previous steps, refer to <a href="https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data">https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data</a>.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor085"/>Numerical value transformation</h2>
<p>For a dataset <a id="_idIndexMarker178"/>that has numeric features <a id="_idIndexMarker179"/>covering distinctly different ranges (for example, the age feature in a mortgage application approval ML model), it is strongly recommended to normalize the dataset since it will help algorithms such as gradient descent to converge. Common ways to normalize data are the following:</p>
<ul>
<li>Scaling to a range</li>
<li>Clipping</li>
<li>Log scaling</li>
<li>Bucketing/binning</li>
</ul>
<h3>Scaling to a range</h3>
<p><strong class="bold">Scaling to a range</strong> normalization <a id="_idIndexMarker180"/>is converting floating-point feature values from their natural range (for example, the age range of 0-90 ) into a standard range (for example, 0 to 1, or  -1 to +1). When you know the approximate range (upper and lower bounds) of your data, and the data is approximately uniformly distributed across that range, then it is a good normalization practice. For example, most age values fall between 0 and 90, and every part of the range has a substantial number of people, thus normalizing age values is a common practice.</p>
<h3>Feature clipping</h3>
<p><strong class="bold">Feature clipping</strong> caps all feature values above (or below) a certain value to a fixed value. If your <a id="_idIndexMarker181"/>dataset contains extreme outliers, feature clipping may be a good practice. For example, you could clip all temperature values above 80 to be exactly 80. Feature clipping can be applied before or after other normalizations.</p>
<h3>Log scaling</h3>
<p><strong class="bold">Log scaling</strong> computes <a id="_idIndexMarker182"/>the log of your feature values, thus compressing a wide data range to a narrow range. When a handful of the data values have many points and most of the other values have few points, <em class="italic">log scaling</em> becomes a good transformation method. For example, movie ratings are good business use cases for <em class="italic">log scaling</em>, since most movies have very few ratings, while a few movies have lots of ratings.</p>
<h3>Bucketing</h3>
<p><strong class="bold">Bucketing</strong> is also <a id="_idIndexMarker183"/>called binning. It transforms <a id="_idIndexMarker184"/>numeric features into categorical features, using a set of thresholds. A good example is transforming house prices into low, medium, and high categories, for better modeling.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>Categorical value transformation</h2>
<p>Categorical values are discrete values that aren’t in an ordered relationship, with each value marking <a id="_idIndexMarker185"/>a category. If categorical <a id="_idIndexMarker186"/>data doesn’t have any order to it, for example, a <em class="italic">color</em> feature that has values such as red, green, and blue, and there is no preference among the categories, if you assign values of <em class="italic">1</em>, <em class="italic">2</em>, and <em class="italic">3</em> to represent <em class="italic">red</em>, <em class="italic">green</em>, and <em class="italic">blue</em>, respectively, the model might interpret blue as more important than red, since it has a higher numeric value. We often <a id="_idIndexMarker187"/>encode non-ordinal data into multiple columns or features, called <strong class="bold">one-hot encoding</strong>. <em class="italic">Figure 3.2</em> shows the one-hot encoding transformation for the color feature – red is 100, blue is 010, and green is 001:</p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 3.2 – One-hot encoding for the “color” feature " height="332" src="image/Figure_3.2.jpg" width="1347"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – One-hot encoding for the “color” feature</p>
<p>One-hot <a id="_idIndexMarker188"/>encoding transforms the non-ordinal <a id="_idIndexMarker189"/>categorical values into numerical values without introducing any ordinal bias. It is used widely in ML data transformations. </p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor087"/>Missing value handling</h2>
<p>When preparing the dataset, we often see missing data. For example, some columns in your dataset <a id="_idIndexMarker190"/>might be missing because of a data collection error, or data wasn’t collected on a particular feature. Missing data can make it <a id="_idIndexMarker191"/>difficult to accurately interpret the relationship between the feature and the target variable and dealing with missing data is an important step in data preparation.  </p>
<p>Based on what has caused the missing data, the total dataset size, and the proportion of missing values, we can either drop the whole feature or impute the missing values. For example, if a row or column has a large percentage of missing values, <em class="italic">dropping</em> the entire row or column may be a viable option. If the missing values are randomly spread throughout the dataset and it’s only a small portion of its rows or columns, then <em class="italic">imputation</em> may be a better option. For categorical variables, we can usually replace missing values with mean, median, or most frequent values. For numerical or continuous variables, we typically use the mean or median to impute. Sometimes, we also encounter nulls or zeros, but those should be approached with care as zero can be a value in a column while an ETL pipeline will replace all missing values with zero.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor088"/>Outlier processing</h2>
<p>Often, we also <a id="_idIndexMarker192"/>see outliers – data points that lie at an <a id="_idIndexMarker193"/>abnormal distance from other values in the dataset. Outliers can make it harder for models to predict accurately because they skew values away from the other more normal values that are related to that feature. Depending on the causes of the outliers, you want to clean them up, or transform them to add richness <a id="_idIndexMarker194"/>to your dataset (some algorithms have built-in functions to handle outliers):</p>
<ul>
<li><strong class="bold">Deleting the outlier or imputing the outlier</strong>: If your outlier is based on an artificial error, such as incorrectly entered data</li>
<li><strong class="bold">Transforming the outlier</strong>: Taking the natural log of a value to reduce the outlier’s influence on the overall dataset</li>
</ul>
<p>Through the <a id="_idIndexMarker195"/>previously shown process of data construction and transformation, the dataset is ready. And now it’s time to go to the next step: checking and selecting the features (the variables that affect the model target) – the process called feature engineering.</p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/>Feature engineering</h1>
<p><strong class="bold">Feature engineering</strong> is the <a id="_idIndexMarker196"/>process of selecting and transforming the most relevant features in ML modeling. It is one of the most important steps in the ML learning process. Feature engineering includes <strong class="bold">feature selection</strong> and <strong class="bold">feature synthesis</strong> (transformation).</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor090"/>Feature selection</h2>
<p>For an ML <a id="_idIndexMarker197"/>problem that has a lot of features extracted <a id="_idIndexMarker198"/>during the initial phase, feature selection is used to reduce the number of those features (input variables), so that we can focus on the features that are most useful to a model to predict the target variable. After you extract features for the problem, you need to use feature selection methods to choose the most appropriate features for model training. Depending on whether ML training is needed, there are two main types of feature selection methods you can use – filter methods and wrapper methods:</p>
<ul>
<li><strong class="bold">Filter methods</strong> use statistical <a id="_idIndexMarker199"/>techniques to <a id="_idIndexMarker200"/>evaluate and score the relationship between each input variable and the target variable. The scores are used to compare the features and decide the input variables that will be used in the model.</li>
<li><strong class="bold">Wrapper methods</strong> create many models with different subsets of input features <a id="_idIndexMarker201"/>and perform model training and compare <a id="_idIndexMarker202"/>their performances. The feature subsets fitting the best-performing model according to a performance metric will be selected. Wrapper methods need ML training on different subsets.</li>
</ul>
<h2 id="_idParaDest-91"><a id="_idTextAnchor091"/>Feature synthesis</h2>
<p>A synthetic feature is created algorithmically, usually with a combination of the real features <a id="_idIndexMarker203"/>using arithmetic operations such as addition, subtraction, multiplication, and division to train machine learning models. <strong class="bold">Feature synthesis</strong> provides <a id="_idIndexMarker204"/>great insights into data patterns and helps model training for some ML problems.</p>
<p>After data collection, construction and transformation, and feature engineering, our data is ready for the next stage – modeling development.</p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor092"/>Summary</h1>
<p>In this chapter, we discussed preparations for an ML process. Starting from business requirements, you need to understand the problem and see if ML is the best solution for it. You then define the ML problem, set up performance measurement, and identify the data to be used for ML modeling to make sure we have a high-quality dataset. </p>
<p>Data plays such an important role! We have also discussed data preparation and feature engineering in this chapter. From data collection and construction to data transformation, feature selection, and feature synthesis, data pipelines prepare the dataset for ML model training. Mastering these data preparation and feature engineering skills will provide us with insights into the data and help us in model development. In the next chapter, we will discuss the ML model development process, from model training and validation to model testing and deployment.</p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor093"/>Further reading</h1>
<p>For further insights into what you've learned in this chapter, you can refer to the following links:</p>
<ul>
<li><a href="https://developers.google.com/machine-learning/problem-framing%20">https://developers.google.com/machine-learning/problem-framing</a></li>
<li><a href="https://developers.google.com/machine-learning/data-prep">https://developers.google.com/machine-learning/data-prep</a></li>
</ul>
</div>
<div>
<div id="_idContainer029">
</div>
</div>
</div>
</body></html>