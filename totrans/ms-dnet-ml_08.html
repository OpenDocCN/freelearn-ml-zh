<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Feature Selection and Optimization"><div class="book" id="1ENBI2-a18db0be6c20485ba81f22e43ca13055"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Feature Selection and Optimization</h1></div></div></div><p class="calibre6">In software engineering, there is an old saying: <span class="strong"><em class="calibre11">make it work first, then make it fast</em></span>. In this book, we have adopted the strategy to <span class="strong"><em class="calibre11">make it run, then make it better</em></span>. Many of the models that we covered in the initial chapters were correct in a very limited sense and could stand some optimization to make them more correct. This chapter is all about <span class="strong"><em class="calibre11">making it better</em></span>.</p></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Feature Selection and Optimization">
<div class="book" title="Cleaning data"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec42" class="calibre1"/>Cleaning data</h1></div></div></div><p class="calibre6">As we saw in <a class="calibre1" title="Chapter 5. Time Out – Obtaining Data" href="part0036_split_000.html#12AK81-a18db0be6c20485ba81f22e43ca13055">Chapter 5</a>, <span class="strong"><em class="calibre11">Time Out – Obtaining Data</em></span>, obtaining and shaping the data (which is often the<a id="id406" class="calibre1"/> largest problem in many projects) is a snap using F# type providers. However, once our data is local and shaped, our work in preparing the data for machine learning is not complete. There might still be abnormalities in each frame. Things like null values, empty values, and values outside a reasonable range need to be addressed. If you come from an R background, you will be familiar with <code class="literal">null.omit</code> and <code class="literal">na.omit</code>, which remove all of the rows from a data frame. We can achieve functional equivalence in F# by applying a filter function to the data. In the filter, you can search for null if it is a reference type, or <code class="literal">.isNone</code> if the column is an option type. While this is effective, it is a bit of a blunt hammer because you are throwing out a row that might have valid values in the other fields when only one field has an inappropriate value.</p><p class="calibre6">Another way to handle missing data is to replace it with a value that will not skew an analysis. Like most things in data science, there are plenty of opinions on the different techniques, and I won't go into too much detail here. Rather, I want to make you aware of the issue and show you a common way to remediate it:</p><p class="calibre6">Go into Visual Studio and create a Visual F# Windows Library project called <code class="literal">FeatureCleaning</code>:</p><div class="mediaobject"><img src="../images/00104.jpeg" alt="Cleaning data" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Locate<a id="id407" class="calibre1"/> <code class="literal">Script1.fsx</code> in the <span class="strong"><strong class="calibre7">Solution Explorer</strong></span> and rename it <code class="literal">CleanData.fsx</code>:</p><div class="mediaobject"><img src="../images/00105.jpeg" alt="Cleaning data" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Open that script file, and replace the existing code with this:</p><div class="informalexample"><pre class="programlisting">type User = {Id: int; FirstName: string; LastName: string; Age: float}
let users = [|{Id=1; FirstName="Jim"; LastName="Jones"; Age=25.5};
              {Id=2; FirstName="Joe"; LastName="Smith"; Age=10.25};
              {Id=3; FirstName="Sally"; LastName="Price"; Age=1000.0};|]</pre></div><p class="calibre6">Sending this to the FSI gives us the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">type User =</strong></span>
<span class="strong"><strong class="calibre7">  {Id: int;</strong></span>
<span class="strong"><strong class="calibre7">   FirstName: string;</strong></span>
<span class="strong"><strong class="calibre7">   LastName: string;</strong></span>
<span class="strong"><strong class="calibre7">   Age: float;}</strong></span>
<span class="strong"><strong class="calibre7">val users : User [] = [|{Id = 1;</strong></span>
<span class="strong"><strong class="calibre7">                         FirstName = "Jim";</strong></span>
<span class="strong"><strong class="calibre7">                         LastName = "Jones";</strong></span>
<span class="strong"><strong class="calibre7">                         Age = 25.5;}; {Id = 2;</strong></span>
<span class="strong"><strong class="calibre7">                                        FirstName = "Joe";</strong></span>
<span class="strong"><strong class="calibre7">                                        LastName = "Smith";</strong></span>
<span class="strong"><strong class="calibre7">                                        Age = 10.25;}; {Id = 3;</strong></span>
<span class="strong"><strong class="calibre7">                                                        FirstName </strong></span>
<span class="strong"><strong class="calibre7">= "Sally";</strong></span>
<span class="strong"><strong class="calibre7">                                                        LastName = "Price";</strong></span>
<span class="strong"><strong class="calibre7">                                                        Age = 1000.0;}|]</strong></span>
</pre></div><p class="calibre6"><code class="literal">User</code> is a record type that represents the users of an application while <code class="literal">users</code> is an array of three users. It looks pretty vanilla except user 3, Sally Price, has an age of <code class="literal">1000.0</code>. What we want to do is take that age out but still keep Sally's record. To do that, let's remove 1,000 and replace it with the average of the ages of all <a id="id408" class="calibre1"/>of remaining users. Go back to the script file and enter this:</p><div class="informalexample"><pre class="programlisting">
let validUsers = Array.filter(fun u -&gt; u.Age &lt; 100.0) users
let averageAge = Array.averageBy(fun u -&gt; u.Age) validUsers

let invalidUsers = 
    users 
    |&gt; Array.filter(fun u -&gt; u.Age &gt;= 100.0) 
    |&gt; Array.map(fun u -&gt; {u with Age = averageAge})

let users' = Array.concat [validUsers; invalidUsers]
</pre></div><p class="calibre6">Sending this to the FSI should give you the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val averageAge : float = 17.875</strong></span>
<span class="strong"><strong class="calibre7">val invalidUsers : User [] = [|{Id = 3;</strong></span>
<span class="strong"><strong class="calibre7">                                FirstName = "Sally";</strong></span>
<span class="strong"><strong class="calibre7">                                LastName = "Price";</strong></span>
<span class="strong"><strong class="calibre7">                                Age = 17.875;}|]</strong></span>
<span class="strong"><strong class="calibre7">val users' : User [] = [|{Id = 1;</strong></span>
<span class="strong"><strong class="calibre7">                          FirstName = "Jim";</strong></span>
<span class="strong"><strong class="calibre7">                          LastName = "Jones";</strong></span>
<span class="strong"><strong class="calibre7">                          Age = 25.5;}; {Id = 2;</strong></span>
<span class="strong"><strong class="calibre7">                                         FirstName = "Joe";</strong></span>
<span class="strong"><strong class="calibre7">                                         LastName = "Smith";</strong></span>
<span class="strong"><strong class="calibre7">                                         Age = 10.25;}; {Id = 3;</strong></span>
<span class="strong"><strong class="calibre7">                                                         FirstName = "Sally";</strong></span>
<span class="strong"><strong class="calibre7">                                                         LastName = "Price";</strong></span>
<span class="strong"><strong class="calibre7">                                                         Age = 17.875;}|]</strong></span>
</pre></div><p class="calibre6">Notice that we create a subarray of the valid users and then get their average ages. We then create a subarray of invalid users and map in the average age. Since F# does not like mutability, we <a id="id409" class="calibre1"/>create a new record for each of the invalid users and use the <code class="literal">with</code> syntax effectively, creating a new record that has all the same values as the original record, except the age. We then wrap up by concatenating the valid users and the updated user back into a single array. Although this is a fairly rudimentary technique, it can be surprisingly effective. As you get further into machine learning, you will develop and refine your own techniques for dealing with invalid data—and you have to keep in mind that the model that you are using will dictate how you clean that data. In some models, taking the average might throw things off.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Selecting data"><div class="book" id="1FLS42-a18db0be6c20485ba81f22e43ca13055"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec43" class="calibre1"/>Selecting data</h1></div></div></div><p class="calibre6">When <a id="id410" class="calibre1"/>we are confronted with<a id="id411" class="calibre1"/> a large number of independent variables, we often run into the problem of which values to select. In addition, the variable might be binned, combined with other variables, or altered—all of which might<a id="id412" class="calibre1"/> make or break a particular <a id="id413" class="calibre1"/>model.</p></div>

<div class="book" title="Selecting data">
<div class="book" title="Collinearity"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec69" class="calibre1"/>Collinearity</h2></div></div></div><p class="calibre6">Collinearity is when we have multiple x variables that are highly related to each other; they have a high degree of <a id="id414" class="calibre1"/>correlation. When using regressions, you always have to be on the watch for collinearity as you can't be sure which individual variable really affects the outcome variable. Here is a classic example. Suppose you wanted to measure the happiness of a college student. You have the following input variables: age, sex, money available for beer, money available for textbooks. In this case, there is a direct relationship between money available for beer and money available for<a id="id415" class="calibre1"/> textbooks. The more money spent on textbooks, the less there is available for beer. To solve for collinearity, you can do a couple of things:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Drop one of the highly-correlated variables. In this case, perhaps drop money available for text books.</li><li class="listitem">Combine correlated variables into a single variable. In this case, perhaps just have a category of money in checking account.</li></ul></div><p class="calibre6">A common way to test for collinearity is to run your multiple regressions several times, each time removing one <code class="literal">x</code> variable. If there is not a dramatic change when two different variables are removed, they are good candidates for collinearity. In addition, you can always do a visual scan of the correlation matrix of the <code class="literal">x</code> variables, which you can do using Accord.Net with the <code class="literal">Tools.Corrlelation</code> method. Let's take a look at this. Go back into Visual Studio and add a new script file called <code class="literal">Accord.fsx</code>. Open the NuGet Package Manager Console and add in Accord:</p><div class="informalexample"><pre class="programlisting">PM&gt; install-package Accord.Statistics
Next, go into the script file and enter this:
#r "../packages/Accord.Math.3.0.2/lib/net40/Accord.Math.dll"
#r "../packages/Accord.Statistics.3.0.2/lib/net40/Accord.Statistics.dll"

open Accord.Statistics

//Age 
//Sex - 1 or 0
//money for textbooks
//money for beer

let matrix = array2D [ [ 19.0;1.0;50.0;10.0]; 
                       [18.0;0.0;40.0;15.0]; 
                       [21.0;1.0;10.0;40.0]]
let correlation = Tools.Correlation(matrix)</pre></div><p class="calibre6">This represents three students who we interviewed. We asked each their age, their gender, how much money they had for textbooks, and how much money they had for beer. The first student is a 19-year-old, female, had $50.00 for text books, and $10.00 for beer.</p><p class="calibre6">When you send this to<a id="id416" class="calibre1"/> the FSI, you get the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val correlation : float [,] =</strong></span>
<span class="strong"><strong class="calibre7">  [[1.0; 0.755928946; -0.8386278694; 0.8824975033]</strong></span>
<span class="strong"><strong class="calibre7">   [0.755928946; 1.0; -0.2773500981; 0.3592106041]</strong></span>
<span class="strong"><strong class="calibre7">   [-0.8386278694; -0.2773500981; 1.0; -0.9962709628]</strong></span>
<span class="strong"><strong class="calibre7">   [0.8824975033; 0.3592106041; -0.9962709628; 1.0]]</strong></span>
</pre></div><p class="calibre6">It is a bit hard to read, so I reformatted it:</p><div class="informalexample"><table border="1" class="calibre19"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24"> </th><th valign="bottom" class="calibre24">
<p class="calibre25">Age</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre25">Gender</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre25">$ Books</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre25">$ Beer</p>
</th></tr></thead><tbody class="calibre26"><tr class="calibre23"><td valign="top" class="calibre27">
<p class="calibre25"><span><strong class="calibre29">Age</strong></span></p>
</td><td valign="top" class="calibre27">
<p class="calibre25">1.0</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.76</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.84</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.88</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre27">
<p class="calibre25"><span><strong class="calibre29">Sex</strong></span></p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.76</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">1.0</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.28</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.35</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre27">
<p class="calibre25"><span><strong class="calibre29">$ Books</strong></span></p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.84</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.28</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">1.0</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.99</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre27">
<p class="calibre25"><span><strong class="calibre29">$ Beer</strong></span></p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.88</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">0.35</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">-0.99</p>
</td><td valign="top" class="calibre27">
<p class="calibre25">1.0</p>
</td></tr></tbody></table></div><p class="calibre6">Notice the diagonal values in matrix, 1.0, which means that age is perfectly correlated with age, sex is perfectly correlated with sex, and so on. The key thing from this example is that there is an almost perfect negative correlation between the amount of money for books and the amount of money for beer: it is <code class="literal">-0.99</code>. What this means is that, if you have more money for books, you have less for beer, which makes sense. By reading the correlation matrix, you can get a quick understanding of what variables are correlated and can possibly be removed.</p><p class="calibre6">A related topic to collinearity is to always keep your <code class="literal">y</code> variable as independent as possible from the <code class="literal">x</code> variable. For example, if you made a regression where you were trying to pick the amount of money available for beer for our student, you would not pick any independent variable that related to the amount of money the student has. Why? Because they are measuring the same thing.</p></div></div>

<div class="book" title="Selecting data">
<div class="book" title="Feature selection"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec70" class="calibre1"/>Feature selection</h2></div></div></div><p class="calibre6">A related topic to collinearity is feature selection. If you have a whole mess of <code class="literal">x</code> variables, how do you decide which ones will be the best ones for your analysis? You can start picking and choosing, but that is time-consuming and can possibly lead to errors. Instead of guessing, there are some modeling techniques that run simulations across all your data to determine the best combination of <code class="literal">x</code> variables to use. One of the most common techniques is called forward-selection step-wise regression. Consider a data frame that has five independent variables and one dependent variable:</p><div class="mediaobject"><img src="../images/00106.jpeg" alt="Feature selection" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Using forward-selection step-wise regression, the technique starts out with a single variable, runs a regression, and calculates (in this case) a rmse:</p><div class="mediaobject"><img src="../images/00107.jpeg" alt="Feature selection" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Next, the technique goes back and adds in another variable and calculates the rmse:</p><div class="mediaobject"><img src="../images/00108.jpeg" alt="Feature selection" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Next, the technique goes back and further adds in another variable and calculates the rmse:</p><div class="mediaobject"><img src="../images/00109.jpeg" alt="Feature selection" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">By now, you probably have the idea. Depending on the implementation, the stepwise might be re-run with different combinations of independent variables and/or different test and training sets. When the step-wise is done, you can have a good idea about what features are important and what can be discarded.</p><p class="calibre6">Let's take a look at a step-wise regression example in Accord. Go back to your script and enter this code (note that this is verbatim from the Accord help file on stepwise regression):</p><div class="informalexample"><pre class="programlisting">#r "../packages/Accord.3.0.2/lib/net40/Accord.dll"
open Accord.Statistics.Analysis

//Age/Smoking
let inputs = [|[|55.0;0.0|];[|28.0;0.0|];
               [|65.0;1.0|];[|46.0;0.0|];
               [|86.0;1.0|];[|56.0;1.0|];
               [|85.0;0.0|];[|33.0;0.0|];
               [|21.0;1.0|];[|42.0;1.0|];
               [|33.0;0.0|];[|20.0;1.0|];
               [|43.0;1.0|];[|31.0;1.0|];
               [|22.0;1.0|];[|43.0;1.0|];
               [|46.0;0.0|];[|86.0;1.0|];
               [|56.0;1.0|];[|55.0;0.0|];|]

//Have Cancer
let output = [|0.0;0.0;0.0;1.0;1.0;1.0;0.0;0.0;0.0;1.0;
               0.0;1.0;1.0;1.0;1.0;1.0;0.0;1.0;1.0;0.0|]

let regression = 
    StepwiseLogisticRegressionAnalysis(inputs, output, [|"Age";"Smoking"|],"Cancer")</pre></div><p class="calibre6">Send this to the FSI to get the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val inputs : float [] [] =</strong></span>
<span class="strong"><strong class="calibre7">  [|[|55.0; 0.0|]; [|28.0; 0.0|]; [|65.0; 1.0|]; [|46.0; 0.0|]; [|86.0; 1.0|];</strong></span>
<span class="strong"><strong class="calibre7">    [|56.0; 1.0|]; [|85.0; 0.0|]; [|33.0; 0.0|]; [|21.0; 1.0|]; [|42.0; 1.0|];</strong></span>
<span class="strong"><strong class="calibre7">    [|33.0; 0.0|]; [|20.0; 1.0|]; [|43.0; 1.0|]; [|31.0; 1.0|]; [|22.0; 1.0|];</strong></span>
<span class="strong"><strong class="calibre7">    [|43.0; 1.0|]; [|46.0; 0.0|]; [|86.0; 1.0|]; [|56.0; 1.0|]; [|55.0; 0.0|]|]</strong></span>
<span class="strong"><strong class="calibre7">val output : float [] =</strong></span>
<span class="strong"><strong class="calibre7">  [|0.0; 0.0; 0.0; 1.0; 1.0; 1.0; 0.0; 0.0; 0.0; 1.0; 0.0; 1.0; 1.0; 1.0; 1.0;</strong></span>
<span class="strong"><strong class="calibre7">    1.0; 0.0; 1.0; 1.0; 0.0|]</strong></span>
<span class="strong"><strong class="calibre7">val regression : StepwiseLogisticRegressionAnalysis</strong></span>
</pre></div><p class="calibre6">As you can tell from the comments in the code, the inputs are 20 fictional people that have been recently screened for cancer. The features are their ages and whether or not they smoke. The output is whether the person actually did have cancer.</p><p class="calibre6">Go back to the script and add this:</p><div class="informalexample"><pre class="programlisting">let results = regression.Compute()
let full = regression.Complete;
let best = regression.Current;

full.Coefficients

best.Coefficients</pre></div><p class="calibre6">When you send this to the FSI, you will see something very interesting. The <code class="literal">full.Coefficients</code> returns all of the variables but the <code class="literal">best.Coefficients</code> returns this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val it : NestedLogisticCoefficientCollection =</strong></span>
<span class="strong"><strong class="calibre7">  seq</strong></span>
<span class="strong"><strong class="calibre7">    [Accord.Statistics.Analysis.NestedLogisticCoefficient</strong></span>
<span class="strong"><strong class="calibre7">       {Confidence = 0.0175962716285245, 1.1598020423839;</strong></span>
<span class="strong"><strong class="calibre7">        ConfidenceLower = 0.01759627163;</strong></span>
<span class="strong"><strong class="calibre7">        ConfidenceUpper = 1.159802042;</strong></span>
<span class="strong"><strong class="calibre7">        LikelihoodRatio = null;</strong></span>
<span class="strong"><strong class="calibre7">        Name = "Intercept";</strong></span>
<span class="strong"><strong class="calibre7">        OddsRatio = 0.1428572426;</strong></span>
<span class="strong"><strong class="calibre7">        StandardError = 1.068502877;</strong></span>
<span class="strong"><strong class="calibre7">        Value = -1.945909451;</strong></span>
<span class="strong"><strong class="calibre7">        Wald = 0.0685832853132018;};</strong></span>
<span class="strong"><strong class="calibre7">     Accord.Statistics.Analysis.NestedLogisticCoefficient</strong></span>
<span class="strong"><strong class="calibre7">       {Confidence = 2.63490696729824, 464.911388747606;</strong></span>
<span class="strong"><strong class="calibre7">        ConfidenceLower = 2.634906967;</strong></span>
<span class="strong"><strong class="calibre7">        ConfidenceUpper = 464.9113887;</strong></span>
<span class="strong"><strong class="calibre7">        LikelihoodRatio = null;</strong></span>
<span class="strong"><strong class="calibre7">        Name = "Smoking";</strong></span>
<span class="strong"><strong class="calibre7">        OddsRatio = 34.99997511;</strong></span>
<span class="strong"><strong class="calibre7">        StandardError = 1.319709922;</strong></span>
<span class="strong"><strong class="calibre7">        Value = 3.55534735;</strong></span>
<span class="strong"><strong class="calibre7">        Wald = 0.00705923290736891;}]</strong></span>
</pre></div><p class="calibre6">You can now see that <code class="literal">Smoking</code> is the most important variable when predicting cancer. If two or more variables were considered important, Accord would have told you the number 1 variable, then the next one, and so on. Stepwise regressions are a bit on the outs these days as the community has moved to Lasso and some other techniques. However, it is still an important tool in your toolkit and is something that you should know about.</p></div></div>

<div class="book" title="Selecting data">
<div class="book" title="Normalization"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec71" class="calibre1"/>Normalization</h2></div></div></div><p class="calibre6">Sometimes our models can be improved by adjusting the data. I am not talking about "adjusting numbers" in the Enron accounting or US politician sense. I am talking about adjusting the data using some standard scientific techniques that might improve the model's accuracy. The general term for this is <span class="strong"><em class="calibre11">normalization</em></span>.</p><p class="calibre6">There are many different ways to normalize data. I want to show you two common ones that work well with regressions. First, if your data is clustered together, you can take the log of the values to help tease out relationships that might otherwise be hidden. For example, look at our scatterplot of product reviews from the beginning of <a class="calibre1" title="Chapter 2. AdventureWorks Regression" href="part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055">Chapter 2</a>, <span class="strong"><em class="calibre11">AdventureWorks Regression</em></span>. Notice that most of the order quantity centered around 250 to 1,000.</p><div class="mediaobject"><img src="../images/00110.jpeg" alt="Normalization" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">By applying the log to the order quantity and doing the same kind of scatterplot, you can see the relationship much more clearly:</p><div class="mediaobject"><img src="../images/00111.jpeg" alt="Normalization" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Note that taking the log typically does not change the relationship among the dependent and independent variables, so you can use it safely in replacement of the natural values in regressions.</p><p class="calibre6">If you go back to the solution in <a class="calibre1" title="Chapter 2. AdventureWorks Regression" href="part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055">Chapter 2</a>, <span class="strong"><em class="calibre11">AdventureWorks Regression</em></span>, you can open up the regression project and add a new file called <code class="literal">Accord.Net4.fsx</code>. Copy and paste in the contents from <code class="literal">Accord.Net2.fsx</code>. Next, replace the data reader lines of code with this:</p><div class="informalexample"><pre class="programlisting">while reader.Read() do
    productInfos.Add({ProductID=reader.GetInt32(0);
       AvgOrders=(float)(reader.GetDecimal(1));
       AvgReviews=log((float)(reader.GetDecimal(2)));
       ListPrice=(float)(reader.GetDecimal(3));})</pre></div><p class="calibre6">Sending this to the REPL, we get the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val regression : MultipleLinearRegression =</strong></span>
<span class="strong"><strong class="calibre7">  y(x0, x1) = 35.4805245757214*x0 + -0.000897944878777119*x1 + -36.7106228824185</strong></span>
<span class="strong"><strong class="calibre7">val error : float = 687.122625</strong></span>
<span class="strong"><strong class="calibre7">val a : float = 35.48052458</strong></span>
<span class="strong"><strong class="calibre7">val b : float = -0.0008979448788</strong></span>
<span class="strong"><strong class="calibre7">val c : float = -36.71062288</strong></span>
<span class="strong"><strong class="calibre7">val mse : float = 7.083738402</strong></span>
<span class="strong"><strong class="calibre7">val rmse : float = 2.661529335</strong></span>
<span class="strong"><strong class="calibre7">val r2 : float = 0.3490097415</strong></span>
</pre></div><p class="calibre6">Notice the change. We are taking the <code class="literal">log()</code> of our <code class="literal">x</code> variables. Also, notice that our <code class="literal">r2</code> slightly decreases. The reason for this is that although the log does not change the relationship among <code class="literal">AvgReviews</code>, it does impact how it relates to the other <code class="literal">x</code> variables and potentially the <code class="literal">y</code> variable. You can see, in this case, that it didn't do much.</p><p class="calibre6">Besides using log, we can trim outliers. Going back to our graph, do you notice that lonely dot at 2.2 average order quantity/3.90 average review?</p><div class="mediaobject"><img src="../images/00112.jpeg" alt="Normalization" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Looking at all of the other data points, we would expect that a 3.90 average review should have a 2.75 average order quantity at least. Although we might want to dive into the details to figure out what is going on, we'll save that exercise for another day. Right now, what it is really doing is messing up our model. Indeed, the biggest criticism of regressions is that<a id="id417" class="calibre1"/> they are overly sensitive to outliers. Let's look at a simple example. Go to <a class="calibre1" title="Chapter 2. AdventureWorks Regression" href="part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055">Chapter 2</a>, <span class="strong"><em class="calibre11">AdventureWorks Regression</em></span>, regression project and create a new script, called <code class="literal">Accord5.fsx</code>. Copy the first part of the code from <code class="literal">Accord1.fsx</code> into it:</p><div class="informalexample"><pre class="programlisting">#r "../packages/Accord.3.0.2/lib/net40/Accord.dll"
#r "../packages/Accord.Statistics.3.0.2/lib/net40/Accord.Statistics.dll"
#r "../packages/Accord.Math.3.0.2/lib/net40/Accord.Math.dll"

open Accord
open Accord.Statistics.Models.Regression.Linear

let xs = [| [|15.0;130.0|];[|18.0;127.0|];[|15.0;128.0|]; [|17.0;120.0|];[|16.0;115.0|] |]
let y = [|3.6;3.5;3.8;3.4;2.6|]

let regression = MultipleLinearRegression(2,true)
let error = regression.Regress(xs,y)

let a = regression.Coefficients.[0]
let b = regression.Coefficients.[1]

let sse = regression.Regress(xs, y)
let mse = sse/float xs.Length 
let rmse = sqrt(mse)
let r2 = regression.CoefficientOfDetermination(xs,y)</pre></div><p class="calibre6">Next, let's add a child <a id="id418" class="calibre1"/>prodigy who is bored with school so he has a low GPA. Add in a student with an age of 10, an IQ of 150, and a GPA of 1.0:</p><div class="informalexample"><pre class="programlisting">let xs = [| [|15.0;130.0|];[|18.0;127.0|];[|15.0;128.0|]; [|17.0;120.0|];[|16.0;115.0|];[|10.0;150.0|] |]

let y = [|3.6;3.5;3.8;3.4;2.6;1.0|]</pre></div><p class="calibre6">Sending the entire script to the REPL gives us the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val regression : MultipleLinearRegression =</strong></span>
<span class="strong"><strong class="calibre7">  y(x0, x1) = 0.351124295971452*x0 + 0.0120748957392838*x1 + -3.89166344210844</strong></span>
<span class="strong"><strong class="calibre7">val error : float = 1.882392837</strong></span>
<span class="strong"><strong class="calibre7">val a : float = 0.351124296</strong></span>
<span class="strong"><strong class="calibre7">val b : float = 0.01207489574</strong></span>
<span class="strong"><strong class="calibre7">val sse : float = 1.882392837</strong></span>
<span class="strong"><strong class="calibre7">val mse : float = 0.3137321395</strong></span>
<span class="strong"><strong class="calibre7">val rmse : float = 0.5601179693</strong></span>
<span class="strong"><strong class="calibre7">val r2 : float = 0.6619468116</strong></span>
</pre></div><p class="calibre6">Notice what happens to our model. Our <code class="literal">r2</code> moves from 0.79 to 0.66 and our rmse climbs from 0.18 to 0.56! Holy cow, that's dramatic! As you can guess, how you deal with outliers will have a large impact on your model. If the intention of the model is to predict a majority of students' GPAs, we can safely remove the outlier because it's not typical. Another way of handling outliers is to use a model that does a better job of dealing with them.</p><p class="calibre6">With that under our belts, let's try it with real data. Add a new script file and call it <code class="literal">AccordDotNet6.fsx</code>. Copy and paste all of <code class="literal">AccordDotNet2.fsx</code> into it. Next, locate these lines:</p><div class="informalexample"><pre class="programlisting">        while reader.Read() do
            productInfos.Add({ProductID=reader.GetInt32(0);
                                AvgOrders=(float)(reader.GetDecimal(1));
                                AvgReviews=(float)(reader.GetDecimal(2));
                                ListPrice=(float)(reader.GetDecimal(3));})

        let xs = productInfos |&gt; Seq.map(fun pi -&gt; [|pi.AvgReviews; pi.ListPrice|]) |&gt; Seq.toArray
        let y = productInfos |&gt; Seq.map(fun pi -&gt; pi.AvgOrders) |&gt; Seq.toArray

And replace them with these:
        while reader.Read() do
            productInfos.Add({ProductID=reader.GetInt32(0);
                                AvgOrders=(float)(reader.GetDecimal(1));
                                AvgReviews=(float)(reader.GetDecimal(2));
                                ListPrice=(float)(reader.GetDecimal(3));})

        let productInfos' = productInfos |&gt; Seq.filter(fun pi -&gt; pi.ProductID &lt;&gt; 757)

        let xs = productInfos' |&gt; Seq.map(fun pi -&gt; [|pi.AvgReviews; pi.ListPrice|]) |&gt; Seq.toArray
        let y = productInfos' |&gt; Seq.map(fun pi -&gt; pi.AvgOrders) |&gt; Seq.toArray</pre></div><p class="calibre6">Sending this<a id="id419" class="calibre1"/> to the REPL, we get the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val regression : MultipleLinearRegression =</strong></span>
<span class="strong"><strong class="calibre7">  y(x0, x1) = 9.89805316193142*x0 + -0.000944004141999501*x1 + -26.8922595356297</strong></span>
<span class="strong"><strong class="calibre7">val error : float = 647.4688586</strong></span>
<span class="strong"><strong class="calibre7">val a : float = 9.898053162</strong></span>
<span class="strong"><strong class="calibre7">val b : float = -0.000944004142</strong></span>
<span class="strong"><strong class="calibre7">val c : float = -26.89225954</strong></span>
<span class="strong"><strong class="calibre7">val mse : float = 6.744467277</strong></span>
<span class="strong"><strong class="calibre7">val rmse : float = 2.59701122</strong></span>
<span class="strong"><strong class="calibre7">val r2 : float = 0.3743706412</strong></span>
</pre></div><p class="calibre6">The <code class="literal">r2</code> moves up from 0.35 to 0.37 and our rmse drops from 2.65 to 2.59. Quite an improvement for removing one data point! Feel free to move this change over to the AdventureWorks project if you want. I am not going to walk you through it, but you now have the skills to do it independently. Dropping outliers is a very powerful way to make regressions more accurate, but there's a cost. Before we start dropping data elements that don't work from our model, we have to use some judgement. In fact, there are textbooks devoted to the science of what to do with outliers and missing data. We are not going to get into that in this book, other than acknowledge that the issue exists and to advise you to use some common sense when dropping elements.</p></div></div>

<div class="book" title="Selecting data">
<div class="book" title="Scaling"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec72" class="calibre1"/>Scaling</h2></div></div></div><p class="calibre6">I want to acknowledge a common misperception about normalization and units of measure. You might notice that the different <code class="literal">x</code> variables have significantly different units of measure in <a class="calibre1" title="Chapter 2. AdventureWorks Regression" href="part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055">Chapter 2</a>, <span class="strong"><em class="calibre11">AdventureWorks Regression</em></span>, and <a class="calibre1" title="Chapter 3. More AdventureWorks Regression" href="part0029_split_000.html#RL0A2-a18db0be6c20485ba81f22e43ca13055">Chapter 3</a>, <span class="strong"><em class="calibre11">More AdventureWorks Regression</em></span>. In our examples, the Units of Customer Review is a 1-5 rating and the Price of<a id="id420" class="calibre1"/> Bikes is 0-10,000 US dollars. You might think that comparing such a large range of numbers would adversely affect the model. Without going into details, you can be rest assured that regressions are immune to different units of measure.</p><p class="calibre6">However, other models (especially classification and clustering models like k-NN, k-means, and PCA) are impacted. When we created these kinds of models in <a class="calibre1" title="Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers" href="part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055">Chapter 6</a>, <span class="strong"><em class="calibre11">AdventureWorks Redux – k-NN and Naïve Bayes Classifiers</em></span>, and <a class="calibre1" title="Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than One" href="part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055">Chapter 7</a>, <span class="strong"><em class="calibre11">Traffic Stops and Crash Locations – When Two Datasets Are Better Than One</em></span>, we ran a risk that we were getting erroneous results because the data was not scaled. Fortunately, the features we selected, and the libraries we used (Numl.net and Accord), bailed us out. Numl.NET automatically scales input variables in all of the classification models. Depending on the type of model, Accord might scale for you. For example, in the PCA we wrote in <a class="calibre1" title="Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than One" href="part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055">Chapter 7</a>, <span class="strong"><em class="calibre11">Traffic Stops and Crash Locations – When Two Datasets Are Better Than One</em></span>, we passed in an input parameter called <code class="literal">AnalysisMethod.Center</code> on this line:</p><div class="informalexample"><pre class="programlisting">let pca = new PrincipalComponentAnalysis(pcaInput.ToArray(), AnalysisMethod.Center)</pre></div><p class="calibre6">This scales the input variables to the mean, which is good enough for our analysis. When we did the k-NN in <a class="calibre1" title="Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers" href="part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055">Chapter 6</a>, <span class="strong"><em class="calibre11">AdventureWorks Redux – k-NN and Naïve Bayes Classifiers</em></span>, using Accord, we did not scale the data because our two input variables were categorical (<code class="literal">MartialStatus</code> and <code class="literal">Gender</code>) with only two possibilities (married or not, male or female) and you only need to scale continuous variables or categorical variables with more than two values. If we had used a continuous variable or a three-factor categorical variable in the k-NN, we would have had to scale it.</p><p class="calibre6">Let's walk through a quick example of scaling using Accord. Open up the <code class="literal">FeatureCleaning</code> solution from this chapter and add a new script file called <code class="literal">AccordKNN</code>:</p><div class="mediaobject"><img src="../images/00113.jpeg" alt="Scaling" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Go into the NuGet<a id="id421" class="calibre1"/> Package Manager Console and enter this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">PM&gt; install-package Accord.MachineLearning</strong></span>
</pre></div><p class="calibre6">Go into the <code class="literal">AccordKNN.fsx</code> file and add the code we used in <a class="calibre1" title="Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers" href="part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055">Chapter 6</a>, <span class="strong"><em class="calibre11">AdventureWorks Redux </em></span><span class="strong"><em class="calibre11">–</em></span><span class="strong"><em class="calibre11">k-NN and Naïve Bayes Classifiers</em></span>, for students who study and drink beer:</p><div class="informalexample"><pre class="programlisting">#r "../packages/Accord.3.0.2/lib/net40/Accord.dll"
#r "../packages/Accord.Math.3.0.2/lib/net40/Accord.Math.dll"
#r "../packages/Accord.Statistics.3.0.2/lib/net40/Accord.Statistics.dll"
#r "../packages/Accord.MachineLearning.3.0.2/lib/net40/Accord.MachineLearning.dll"

open Accord
open Accord.Math
open Accord.MachineLearning
open Accord.Statistics.Analysis

let inputs = [|[|5.0;1.0|];[|4.5;1.5|];
             [|5.1;0.75|];[|1.0;3.5|];
             [|0.5;4.0|];[|1.25;4.0|]|]
let outputs = [|1;1;1;0;0;0|]

let classes = 2
let k = 3
let knn = KNearestNeighbors(k, classes, inputs, outputs)

let input = [|5.0;0.5|]
let output = knn.Compute(input)</pre></div><p class="calibre6">Now, let's scale the data so that studying and drinking beer are equivalent. We are going to take the simplest methodology of scaling called <span class="strong"><em class="calibre11">mean scaling</em></span>. Go back to the script and enter this:</p><div class="informalexample"><pre class="programlisting">let studyingAverage = inputs |&gt; Array.map(fun i -&gt; i.[0]) |&gt; Array.average
let drinkingAverage = inputs |&gt; Array.map(fun i -&gt; i.[1]) |&gt; Array.average

let scaledInputs = inputs |&gt; Array.map(fun i -&gt; [|i.[0]/studyingAverage; i.[1]/drinkingAverage|])
let scaledKNN = KNearestNeighbors(k, classes, scaledInputs, outputs)</pre></div><p class="calibre6">When you send<a id="id422" class="calibre1"/> this to the REPL, you will see the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val studyingAverage : float = 2.891666667</strong></span>
<span class="strong"><strong class="calibre7">val drinkingAverage : float = 2.458333333</strong></span>
<span class="strong"><strong class="calibre7">val scaledInputs : float [] [] =</strong></span>
<span class="strong"><strong class="calibre7">  [|[|1.729106628; 0.406779661|]; [|1.556195965; 0.6101694915|];</strong></span>
<span class="strong"><strong class="calibre7">    [|1.763688761; 0.3050847458|]; [|0.3458213256; 1.423728814|];</strong></span>
<span class="strong"><strong class="calibre7">    [|0.1729106628; 1.627118644|]; [|0.4322766571; 1.627118644|]|]</strong></span>
<span class="strong"><strong class="calibre7">val scaledKNN : KNearestNeighbors</strong></span>
</pre></div><p class="calibre6">Notice that the inputs are now relative to their means. The person who studied five hours and drank one beer now studied 73% more than the average and drank 41% less than the average. This k-NN model is now scaled and will give a better "apples to apples" comparison when used in practice.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Overfitting and cross validation"><div class="book" id="1GKCM2-a18db0be6c20485ba81f22e43ca13055"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec44" class="calibre1"/>Overfitting and cross validation</h1></div></div></div><p class="calibre6">If you remember from Chapters 2, 3, and 4, one of the problems with our methodology when building models was that we were guilty of overfitting. Overfitting, the bane of predictive analytics, is what happens when we build a model that does a great job with past data but then falls apart when <a id="id423" class="calibre1"/>new data is introduced. This phenomenon is not just for data science; it happens a lot<a id="id424" class="calibre1"/> in our society: Professional athletes get lucrative contracts and then fail to live up to their prior performances; fund managers get hefty salary bumps because of last year's performance, and the list goes on.</p></div>

<div class="book" title="Overfitting and cross validation">
<div class="book" title="Cross validation – train versus test"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec73" class="calibre1"/>Cross validation – train versus test</h2></div></div></div><p class="calibre6">Unlike the Yankees, who never seem to learn, our profession has learned from its mistakes and has a great, if imperfect, tool to combat overfitting. We use the methodology of train/test/eval to build several models and then select the best one not based on how well it did against an existing dataset, but how it does against data it has never seen before. To <a id="id425" class="calibre1"/>accomplish that, we take our source data, import it, clean it, and split it into two subsets: training and testing. We then build our model on the training set and, if it seems viable, apply our test data to the model. If the model is still valid, we can think about pushing it to production. This is represented graphically as follows:</p><div class="mediaobject"><img src="../images/00114.jpeg" alt="Cross validation – train versus test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">But there is one more step we can add. We can split our data several times and build new models to be validated. The actual splitting of the dataset is its own science, but typically each time the base dataset is split into <span class="strong"><strong class="calibre7">Training</strong></span> and <span class="strong"><strong class="calibre7">Testing</strong></span> subsets, the records are selected randomly. That means if you split your base data five times, you will have five completely different training and test subsets:</p><div class="mediaobject"><img src="../images/00115.jpeg" alt="Cross validation – train versus test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">This kind of technique can be more important than the actual model selection. Both Accord and Numl do<a id="id426" class="calibre1"/> some kind of splitting under the hoods and in this book, we will trust that they are doing a good job. However, once you start working on models in the wild, you will want to dedicate a certain amount of time on every project for cross validation.</p></div></div>

<div class="book" title="Overfitting and cross validation">
<div class="book" title="Cross validation – the random and mean test"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec74" class="calibre1"/>Cross validation – the random and mean test</h2></div></div></div><p class="calibre6">Going back to our k-NN example of students that studied and drank beer, how do we know if we are predicting accurately? If we want to guess whether a student passed or not, we could just flip a coin: heads they pass, tails they fail. The assumption in our analysis is that the number of hours studying and the number of beers consumed have some kind of<a id="id427" class="calibre1"/> causality on the exam outcome. If our model does no better than a <a id="id428" class="calibre1"/>coin flip, then it is not a model worth using. Open up Visual Studio and go back to the <code class="literal">AccordKNN.fsx</code> file. At the bottom, enter in the following code:</p><div class="informalexample"><pre class="programlisting">let students = [|0..5|]
let random = System.Random()
let randomPrediction = 
    students 
    |&gt; Array.map(fun s -&gt; random.Next(0,2))</pre></div><p class="calibre6">Sending this to the FSI, we get the following (your results will be different):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val students : int [] = [|0; 1; 2; 3; 4; 5|]</strong></span>
<span class="strong"><strong class="calibre7">val random : System.Random</strong></span>
<span class="strong"><strong class="calibre7">val randomPrediction : int [] = [|0; 1; 0; 0; 1; 1|]</strong></span>
</pre></div><p class="calibre6">Now, let's enter in some information about each student: the number of hours they studied and the number of beers they drank and run the unscaled k-NN on it:</p><div class="informalexample"><pre class="programlisting">let testInputs = [|[|5.0;1.0|];[|4.0;1.0|];
                 [|6.2;0.5|];[|0.0;2.0|];
                 [|0.5;4.0|];[|3.0;6.0|]|]

let knnPrediction =
    testInputs
    |&gt; Array.map(fun ti -&gt; knn.Compute(ti))</pre></div><p class="calibre6">Sending this<a id="id429" class="calibre1"/> to the REPL gives us the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val testInputs : float [] [] =</strong></span>
<span class="strong"><strong class="calibre7">  [|[|5.0; 1.0|]; [|4.0; 1.0|]; [|6.2; 0.5|]; [|0.0; 2.0|]; [|0.5; 4.0|];</strong></span>
<span class="strong"><strong class="calibre7">    [|3.0; 6.0|]|]</strong></span>
<span class="strong"><strong class="calibre7">val knnPrediction : int [] = [|1; 1; 1; 0; 0; 0|]</strong></span>
</pre></div><p class="calibre6">Finally, let's see how they actually did on the exam. Add this to the script:</p><div class="informalexample"><pre class="programlisting">let actual = [|1;1;1;0;0;0|]</pre></div><p class="calibre6">Sending this<a id="id430" class="calibre1"/> to the FSI gives us the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val actual : int [] = [|1; 1; 1; 0; 0; 0|]</strong></span>
</pre></div><p class="calibre6">Combining these arrays together in a chart, will give us the following:</p><div class="mediaobject"><img src="../images/00116.jpeg" alt="Cross validation – the random and mean test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">If we then scored how well the random test and k-NN did predicting the actual results, we can see that the random test <a id="id431" class="calibre1"/>correctly predicted the result 66% of the time and k-NN correctly predicted the result 100% of the time:</p><div class="mediaobject"><img src="../images/00117.jpeg" alt="Cross validation – the random and mean test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Because our k-NN did better than the random coin flip, we can consider the model useful.</p><p class="calibre6">This kind of yes/no random test works well when our model is a logistic regression or a classification model like k-NN, but what about when the dependent (<span class="strong"><em class="calibre11">Y</em></span>) variable is a continuous value<a id="id432" class="calibre1"/> like in a linear regression? In that case, instead of using a random coin flip, we can plug in the mean of the known values. If the outcome predicts better than the mean, we probably have a good model. If it does worse than the mean, we need to rethink our model. For example, consider predicting average bike reviews from AdventureWorks:</p><div class="mediaobject"><img src="../images/00118.jpeg" alt="Cross validation – the random and mean test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">When you compare the predicted to the actual (taking the absolute value to account for being both<a id="id433" class="calibre1"/> higher and lower) and then aggregate the results, you can see that <a id="id434" class="calibre1"/>our linear regression did a better job in predicting the rating than the mean:</p><div class="mediaobject"><img src="../images/00119.jpeg" alt="Cross validation – the random and mean test" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">If you are thinking we have already done something like this in Chapters 2 and 3, you are right—this is the same concept as the RMSE.</p></div></div>

<div class="book" title="Overfitting and cross validation">
<div class="book" title="Cross validation – the confusion matrix and AUC"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec75" class="calibre1"/>Cross validation – the confusion matrix and AUC</h2></div></div></div><p class="calibre6">Going back to our k-NN example, imagine that we ran our k-NN against many students. Sometimes the k-NN <a id="id435" class="calibre1"/>guessed correctly, sometimes the k-NN did not. There are actually four possible outcomes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">k-NN predicted that the student would pass and they did pass</li><li class="listitem">k-NN predicted that<a id="id436" class="calibre1"/> the student would fail and they did fail</li><li class="listitem">k-NN predicted that the student would pass and they failed</li><li class="listitem">k-NN predicted that the student would fail and they passed</li></ul></div><p class="calibre6">Each of these outcomes has a special name:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre7">Predict Pass and Did Pass</strong></span>: True Positive</li><li class="listitem"><span class="strong"><strong class="calibre7">Predict Fail and Did Fail</strong></span>: True Negative</li><li class="listitem"><span class="strong"><strong class="calibre7">Predict Pass and Failed</strong></span>: False Positive</li><li class="listitem"><span class="strong"><strong class="calibre7">Predict Fail and Passed</strong></span>: False Negative</li></ul></div><p class="calibre6">And in a chart format, it <a id="id437" class="calibre1"/>would look like this:</p><div class="mediaobject"><img src="../images/00120.jpeg" alt="Cross validation – the confusion matrix and AUC" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Sometimes the <a id="id438" class="calibre1"/>False Positive is called a Type I error and the False Negative is called a Type II error.</p><p class="calibre6">If we ran the k-NN against 100 students, we could add values to that chart like this:</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="Cross validation – the confusion matrix and AUC" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">Reading this chart, 52 students passed the exam. Of that, we correctly predicted 50 of them would pass, but we incorrectly predicted two of the passing students would fail. Similarly, 43 failed the exam (must have been a tough exam!), 40 of which we correctly predicted would fail, and three we incorrectly predicted would pass. This matrix is often called a <span class="strong"><em class="calibre11">confusion matrix</em></span>.</p><p class="calibre6">With this confusion<a id="id439" class="calibre1"/> matrix, we can then do some basic statistics like:</p><p class="calibre6"><span class="strong"><em class="calibre11">Accuracy = True Positives + True Negatives / Total Population = (50 + 40) / 100 = 90%</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">True Positive Rate (TPR) = True Positives / Total Positives = 50 / 52 = 96%</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">False Negative Rate (FNR) = False Negatives / Total Positives = 2 / 52 = 4%</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">False Positive Rate (FPR) = False Positives / Total Negatives = 3 / 43 = 7%</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">True Negative Rate (TNR) = True Negatives / Total Negatives = 40 / 43 = 93%</em></span></p><p class="calibre6">(Note that TPR is sometimes called Sensitivity, the FNR is sometimes called Miss Rate, the False Positive Rate is sometimes called Fall-Out and the TNR is sometimes called Specificity.)</p><p class="calibre6"><span class="strong"><em class="calibre11">Positive Likelihood Ratio (LR+) = TPR / FPR = 96 % / 1 – 93% = 13.8</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">Negative Likelihood Ratio (LR-) = FNR / TNR = 4% / 93% = .04</em></span></p><p class="calibre6"><span class="strong"><em class="calibre11">Diagnostic Odds Ratio (DOR) = LR+ / LR- = 33.3</em></span></p><p class="calibre6">Since the DOR is<a id="id440" class="calibre1"/> greater than 1, we know that the model is working well.</p><p class="calibre6">Putting this into code, we could handwrite these formulas, but Accord.Net has already taken care of this for us. Go back into Visual Studio and open <code class="literal">AccordKNN.fsx</code>. At the bottom, enter in this code:</p><div class="informalexample"><pre class="programlisting">let positiveValue = 1
let negativeValue = 0

let confusionMatrix = ConfusionMatrix(knnPrediction,actual,positiveValue,negativeValue)</pre></div><p class="calibre6">On the next line, type <code class="literal">confusionMatrix</code> and hit dot to see all of the properties that are available to you:</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="Cross validation – the confusion matrix and AUC" class="calibre8"/></div><p class="calibre9"> </p><p class="calibre6">This is a very useful class indeed. Let's select the odds ratio:</p><div class="informalexample"><pre class="programlisting">confusionMatrix.OddsRatio</pre></div><p class="calibre6">And then send the <a id="id441" class="calibre1"/>entire code block to the FSI:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre7">val positiveValue : int = 1</strong></span>
<span class="strong"><strong class="calibre7">val negativeValue : int = 0</strong></span>
<span class="strong"><strong class="calibre7">val confusionMatrix : ConfusionMatrix = TP:3 FP:0, FN:0 TN:3</strong></span>
<span class="strong"><strong class="calibre7">val it : float = infinity</strong></span>
</pre></div><p class="calibre6">Since our k-NN is was 100% accurate, we got an odds ratio of infinity (and beyond). In a real-world model, the odds ratio <a id="id442" class="calibre1"/>would obviously be much lower.</p></div></div>

<div class="book" title="Overfitting and cross validation">
<div class="book" title="Cross validation – unrelated variables"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec76" class="calibre1"/>Cross validation – unrelated variables</h2></div></div></div><p class="calibre6">There is one more technique that I want to cover to for cross-validation—adding in unrelated variables and <a id="id443" class="calibre1"/>seeing the impact on the model. If your model is truly useful, it should be able to handle extraneous "noise" variables without significantly impacting the model's result. As we saw in <a class="calibre1" title="Chapter 2. AdventureWorks Regression" href="part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055">Chapter 2</a>, <span class="strong"><em class="calibre11">AdventureWorks Regression</em></span>, any additional variable will have a positive impact on most models, so this is a measure of degree. If adding an unrelated variable makes the model seem much more accurate, then the model itself is suspect. However, if the extra variable only has a marginal impact, then our model can be considered solid.</p><p class="calibre6">Let's see this in action. Go back into <code class="literal">AccordKNN.fsx</code> and add the following code at the bottom:</p><div class="informalexample"><pre class="programlisting">let inputs' = [|[|5.0;1.0;1.0|];[|4.5;1.5;11.0|];
               [|5.1;0.75;5.0|];[|1.0;3.5;8.0|];
               [|0.5;4.0;1.0|];[|1.25;4.0;11.0|]|]

let knn' = KNearestNeighbors(k, classes, inputs', outputs)

let testInputs' = [|[|5.0;1.0;5.0|];[|4.0;1.0;8.0|];
                   [|6.2;0.5;12.0|];[|0.0;2.0;2.0|];
                   [|0.5;4.0;6.0|];[|3.0;6.0;5.0|]|]

let knnPrediction' =
    testInputs'
    |&gt; Array.map(fun ti -&gt; knn'.Compute(ti))</pre></div><p class="calibre6">I added a third variable that represents each student's zodiac symbol (1.0 = Aquarius, 2.0 = Pisces, and so on). When I passed in the same test input (also with random zodiac symbols), the <a id="id444" class="calibre1"/>predictions were the same as the original k-NN.</p><div class="informalexample"><pre class="programlisting">val knnPrediction' : int [] = [|1; 1; 1; 0; 0; 0|]</pre></div><p class="calibre6">We can conclude that the extra variable, although it had an impact at some point in the modeling process, was not important enough to alter our original model. We can then use this model with a higher degree of confidence.</p></div></div>
<div class="book" title="Summary" id="1HIT81-a18db0be6c20485ba81f22e43ca13055"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec45" class="calibre1"/>Summary</h1></div></div></div><p class="calibre6">This chapter is a bit different than other machine learning books that you might have read because it did not introduce any new models, but instead concentrated on the dirty job on gathering, cleaning, and selecting your data. Although not as glamorous, it is absolutely essential that you have a firm grasp on these concepts because they will often make or break a project. In fact, many projects spend over 90% of their time acquiring data, cleaning the data, selecting the correct features, and building the appropriate cross-validation methodology. In this chapter, we looked at cleaning data and how to account for missing and incomplete data. Next, we looked at collinearity and normalization. Finally, we wrapped up with some common cross-validation techniques.</p><p class="calibre6">We are going to apply all of these techniques in the coming chapters. Up next, let's go back to the AdventureWorks company and see if we can help them improve their production process using a machine learning model based on how the human brain works.</p></div></body></html>