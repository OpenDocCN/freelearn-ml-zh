<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer066">
<h1 id="_idParaDest-59"><em class="italic"><a id="_idTextAnchor058"/>Chapter 5</em>: Feature Selection</h1>
<p>Depending on how you began your data analytic work and your own intellectual interests, you might have a different perspective on the topic of <strong class="bold">feature selection</strong>. You might think, <em class="italic">yeah, yeah, it is an important topic, but I really want to get to the model building</em>. Or, at the other extreme, you might view feature selection as at the core of model building and believe that you are 90% of the way toward having your model once you have chosen your features. For now, let's just agree that we should spend a good chunk of time understanding the relationships between features – and their relationship to a target if we are building a supervised model – before we do any serious model specification.</p>
<p>It is helpful to approach our feature selection work with the attitude that less is more. If we can reach nearly the same degree of accuracy or explain as much of the variance with fewer features, we should select the simpler model. Sometimes, we can actually get better accuracy with fewer features. This can be hard to wrap our brains around, and even be a tad disappointing for those of us who cut our teeth on building models that told rich and complicated stories.</p>
<p>But we are less concerned with parameter estimates than with the accuracy of our predictions when fitting machine learning models. Unnecessary features can contribute to overfitting and tax hardware resources.</p>
<p>We can sometimes spend months specifying the features of our model, even when there is a limited number of columns in the data. Bivariate correlations, such as those created in <a href="B17978_02_ePub.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Examining Bivariate and Multivariate Relationships between Features and Targets</em>, give us some sense of what to expect, but the importance of a feature can vary significantly once other potentially explanatory features are introduced. The feature may no longer be significant, or, conversely, may only be significant when other features are included. Two features might be so highly correlated that including both of them offers very little additional information than including just one.</p>
<p>This chapter takes a close look at feature selection techniques applicable to a variety of predictive modeling tasks. Specifically, we will explore the following topics:</p>
<ul>
<li>Selecting features for classification models</li>
<li>Selecting features for regression models</li>
<li>Using forward and backward feature selection</li>
<li>Using exhaustive feature selection</li>
<li>Eliminating features recursively in a regression model</li>
<li>Eliminating features recursively in a classification model</li>
<li>Using Boruta for feature selection</li>
<li>Using regularization and other embedded methods</li>
<li>Using principal component analysis</li>
</ul>
<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Technical requirements</h1>
<p>We will work with the <strong class="source-inline">feature_engine</strong>, <strong class="source-inline">mlxtend</strong>, and <strong class="source-inline">boruta</strong> packages in this chapter, in addition to the <strong class="source-inline">scikit-learn</strong> library. You can use <strong class="source-inline">pip</strong> to install these packages. I have chosen a dataset with a small number of observations for our work in this chapter, so the code should work fine even on suboptimal workstations. </p>
<p class="callout-heading">Note</p>
<p class="callout">We will work exclusively in this chapter with data from The National Longitudinal Survey of Youth, conducted by the United States Bureau of Labor Statistics. This survey started with a cohort of individuals in 1997 who were born between 1980 and 1985, with annual follow-ups each year through 2017. We will work with educational attainment, household demographic, weeks worked, and wage income data. The wage income column represents wages earned in 2016. The NLS dataset can be downloaded for public use at <a href="https://www.nlsinfo.org/investigator/pages/search">https://www.nlsinfo.org/investigator/pages/search</a>.</p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Selecting features for classification models</h1>
<p>The most straightforward feature selection<a id="_idIndexMarker386"/> methods are based<a id="_idIndexMarker387"/> on each feature's relationship with a target variable. The next two sections examine techniques for determining the <em class="italic">k</em> best features based on their linear or non-linear relationship <a id="_idIndexMarker388"/>with the target. These<a id="_idIndexMarker389"/> are known as filter methods. They are <a id="_idIndexMarker390"/>also sometimes called univariate methods<a id="_idIndexMarker391"/> since they evaluate the relationship between the feature and the target independent of the impact of other features.</p>
<p>We use somewhat different strategies when the target is categorical than when it is continuous. We'll go over the former in this section and the latter in the next.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Mutual information classification for feature selection with a categorical target</h2>
<p>We can use <strong class="bold">mutual information</strong> classification or <strong class="bold">analysis of variance</strong> (<strong class="bold">ANOVA</strong>) tests to<a id="_idIndexMarker392"/> select features<a id="_idIndexMarker393"/> when we have a categorical<a id="_idIndexMarker394"/> target. We will try mutual information classification first, and then ANOVA for comparison.</p>
<p>Mutual information is a measure of how much information about a variable is provided by knowing the value of another variable. At the extreme, when features are completely independent, the mutual information score is 0.</p>
<p>We can use <strong class="source-inline">scikit-learn</strong>'s <strong class="source-inline">SelectKBest</strong> class to select the <em class="italic">k</em> features that have the highest predictive strength based on mutual information classification or some other appropriate measure. We can use hyperparameter tuning to select the value of <em class="italic">k</em>. We can also examine the scores of all features, whether they were identified as one of the <em class="italic">k</em> best or not, as we will see in this section.</p>
<p>Let's first try mutual information classification to identify features that are related to completing a bachelor's degree. Later, we will compare that with using ANOVA F-values as the basis for selection:</p>
<ol>
<li>We start by importing <strong class="source-inline">OneHotEncoder</strong> from <strong class="source-inline">feature_engine</strong> to encode some of the data, and <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">scikit-learn</strong> to create training and testing data. We will also need <strong class="source-inline">scikit-learn</strong>'s <strong class="source-inline">SelectKBest</strong>, <strong class="source-inline">mutual_info_classif</strong>, and <strong class="source-inline">f_classif</strong> modules for our feature selection:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.feature_selection import SelectKBest,\</p><p class="source-code">  mutual_info_classif, f_classif</p></li>
<li>We load NLS data<a id="_idIndexMarker395"/> that has a binary<a id="_idIndexMarker396"/> variable for having completed a bachelor's degree and features possibly related<a id="_idIndexMarker397"/> to degree attainment: <strong class="bold">Scholastic Assessment Test</strong> (<strong class="bold">SAT</strong>) score, high school GPA, parental educational attainment and income, and gender. Observations with missing values for any feature have been removed. We then create training and testing DataFrames, encode the <strong class="source-inline">gender</strong> feature, and scale the other data:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['gender','satverbal','satmath',</p><p class="source-code">  'gpascience', 'gpaenglish','gpamath','gpaoverall',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">X_train_enc = ohe.fit_transform(X_train)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="callout-heading">Note</p><p class="callout">We will do a complete case analysis of the NLS data throughout this chapter; that is, we will remove all observations that have missing values for any of the features. This is not usually a good approach and is particularly problematic when data is not missing at random or when there is a large number of missing values for one or more features. In such cases, it would be better to use some of the approaches that we used in <a href="B17978_03_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying and Fixing Missing Values</em>. We will do a complete case analysis in this chapter to keep the examples as straightforward as possible.</p></li>
<li>Now we are ready to select features<a id="_idIndexMarker398"/> for our model of bachelor's degree<a id="_idIndexMarker399"/> completion. One approach is to use mutual information classification. To do that, we set the <strong class="source-inline">score_func</strong> value of <strong class="source-inline">SelectKBest</strong> to <strong class="source-inline">mutual_info_classif</strong> and indicate that we want the five best features. Then, we call <strong class="source-inline">fit</strong> and use the <strong class="source-inline">get_support</strong> method to get the five best features:<p class="source-code">ksel = SelectKBest(score_func=mutual_info_classif, k=5)</p><p class="source-code">ksel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[ksel.get_support()]</p><p class="source-code">selcols</p><p class="source-code">Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')</p></li>
<li>If we also want to see the score<a id="_idIndexMarker400"/> for each feature, we can use the <strong class="source-inline">scores_</strong> attribute, though we need<a id="_idIndexMarker401"/> to do a little work to associate the scores with a particular feature name and sort the scores in descending order:<p class="source-code">pd.DataFrame({'score': ksel.scores_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','score']).\</p><p class="source-code">   sort_values(['score'], ascending=False)</p><p class="source-code"><strong class="bold">        feature              score</strong></p><p class="source-code"><strong class="bold">5       gpaoverall           0.108</strong></p><p class="source-code"><strong class="bold">1       satmath</strong><strong class="bold">              0.074</strong></p><p class="source-code"><strong class="bold">3       gpaenglish           0.072</strong></p><p class="source-code"><strong class="bold">0       satverbal            0.069</strong></p><p class="source-code"><strong class="bold">2       gpascience           0.047</strong></p><p class="source-code"><strong class="bold">4       gpamath              0.038</strong></p><p class="source-code"><strong class="bold">8       parentincome</strong><strong class="bold">         0.024</strong></p><p class="source-code"><strong class="bold">7       fatherhighgrade      0.022</strong></p><p class="source-code"><strong class="bold">6       motherhighgrade      0.022</strong></p><p class="source-code"><strong class="bold">9       gender_Female        0.015</strong></p><p class="callout-heading">Note</p><p class="callout">This is a stochastic process, so we will get different results each time we run it. </p></li>
</ol>
<p>To get the same results each time, you can pass a partial function to <strong class="source-inline">score_func</strong>: </p>
<p class="source-code">from functools import partial</p>
<p class="source-code">SelectKBest(score_func=partial(mutual_info_classif, </p>
<p class="source-code">                               random_state=0), k=5) </p>
<ol>
<li value="5">We can create a DataFrame with just the important features<a id="_idIndexMarker402"/> using the <strong class="source-inline">selcols</strong> array we created<a id="_idIndexMarker403"/> using <strong class="source-inline">get_support</strong>. (We could have used the <strong class="source-inline">transform</strong> method of <strong class="source-inline">SelectKBest</strong> instead. This would have returned the values of the selected features as a NumPy array.)<p class="source-code">X_train_analysis = X_train_enc[selcols] </p><p class="source-code">X_train_analysis.dtypes</p><p class="source-code"><strong class="bold">satverbal       float64</strong></p><p class="source-code"><strong class="bold">satmath         float64</strong></p><p class="source-code"><strong class="bold">gpascience      float64</strong></p><p class="source-code"><strong class="bold">gpaenglish</strong><strong class="bold">      float64</strong></p><p class="source-code"><strong class="bold">gpaoverall      float64</strong></p><p class="source-code"><strong class="bold">dtype: object</strong></p></li>
</ol>
<p>That is all we need to do to select<a id="_idIndexMarker404"/> the <em class="italic">k</em> best features<a id="_idIndexMarker405"/> for our model using mutual information.</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>ANOVA F-value for feature selection with a categorical target</h2>
<p>Alternatively, we can use ANOVA<a id="_idIndexMarker406"/> instead of mutual<a id="_idIndexMarker407"/> information. ANOVA evaluates how different the mean for a feature is for each target class. This is a good metric for univariate feature selection when we can assume a linear relationship between features and the target and our features are normally distributed. If those assumptions do not hold, mutual information classification is a better choice.</p>
<p>Let's try using ANOVA for our feature selection. We can set the <strong class="source-inline">score_func</strong> parameter of <strong class="source-inline">SelectKBest</strong> to <strong class="source-inline">f_classif</strong> to select based on ANOVA:</p>
<pre class="source-code">ksel = SelectKBest(score_func=f_classif, k=5)</pre>
<pre class="source-code">ksel.fit(X_train_enc, y_train.values.ravel())</pre>
<pre class="source-code">selcols = X_train_enc.columns[ksel.get_support()]</pre>
<pre class="source-code">selcols</pre>
<pre class="source-code">Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')</pre>
<pre class="source-code">pd.DataFrame({'score': ksel.scores_,</pre>
<pre class="source-code">  'feature': X_train_enc.columns},</pre>
<pre class="source-code">   columns=['feature','score']).\</pre>
<pre class="source-code">   sort_values(['score'], ascending=False)</pre>
<pre class="source-code"><strong class="bold">       feature                score</strong></pre>
<pre class="source-code"><strong class="bold">5      gpaoverall           119.471</strong></pre>
<pre class="source-code"><strong class="bold">3      gpaenglish           108.006</strong></pre>
<pre class="source-code"><strong class="bold">2      gpascience            96.824</strong></pre>
<pre class="source-code"><strong class="bold">1</strong><strong class="bold">      satmath               84.901</strong></pre>
<pre class="source-code"><strong class="bold">0      satverbal             77.363</strong></pre>
<pre class="source-code"><strong class="bold">4      gpamath               60.930</strong></pre>
<pre class="source-code"><strong class="bold">7      fatherhighgrade       37.481</strong></pre>
<pre class="source-code"><strong class="bold">6      motherhighgrade       29.377</strong></pre>
<pre class="source-code"><strong class="bold">8</strong><strong class="bold">      parentincome          22.266</strong></pre>
<pre class="source-code"><strong class="bold">9      gender_Female         15.098</strong></pre>
<p>This selected the same features as were selected when we used mutual information. Showing the scores gives us some indication of whether the selected value for <em class="italic">k</em> makes sense. For example, there is a greater drop in score from the fifth- to the sixth-best feature (77-61) than from the fourth to the fifth (85-77). There is an even bigger decline from the sixth to the seventh, however (61-37), suggesting that we should at least consider a value for <em class="italic">k</em> of 6. </p>
<p>ANOVA tests, and the mutual information classification we did<a id="_idIndexMarker408"/> earlier, do not take into account features that are only important in multivariate analysis. For example, <strong class="source-inline">fatherhighgrade</strong> might matter among individuals with similar GPA or SAT scores. We use multivariate feature selection methods<a id="_idIndexMarker409"/> later in this chapter. We do more univariate feature selection in the next section where we explore selection techniques appropriate for continuous targets.</p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Selecting features for regression models</h1>
<p><strong class="bold">Regression models</strong> have a continuous target. The statistical techniques<a id="_idIndexMarker410"/> we used in the previous section<a id="_idIndexMarker411"/> are not appropriate for such<a id="_idIndexMarker412"/> targets. Fortunately, <strong class="source-inline">scikit-learn</strong>'s selection module provides several options for selecting features when building regression models. (By regression models here, I do not mean linear regression models. I am only referring to <em class="italic">models with continuous targets</em>.) Two good options are selection based on F-tests and selection based on mutual information<a id="_idIndexMarker413"/> for regression. Let's start<a id="_idIndexMarker414"/> with F-tests.</p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>F-tests for feature selection with a continuous target</h2>
<p>The F-statistic is a measure of the strength<a id="_idIndexMarker415"/> of the linear correlation<a id="_idIndexMarker416"/> between a target and a single regressor. <strong class="source-inline">Scikit-learn</strong> has an <strong class="source-inline">f_regression</strong> scoring function, which returns F-statistics. We can use it with <strong class="source-inline">SelectKBest</strong> to select features based on that statistic. </p>
<p>Let's use F-statistics to select features for a model of wages. We use mutual information for regression in the next section to select features for the same target:</p>
<ol>
<li value="1">We start by importing the one-hot encoder from <strong class="source-inline">feature_engine</strong> and <strong class="source-inline">train_test_split</strong> and <strong class="source-inline">SelectKBest</strong> from <strong class="source-inline">scikit-learn</strong>. We also import <strong class="source-inline">f_regression</strong> to get F-statistics later:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.feature_selection import SelectKBest, f_regression</p></li>
<li>Next, we load the NLS data, including educational attainment, parental income, and wage income data:<p class="source-code">nls97wages = pd.read_csv("data/nls97wages.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome',</p><p class="source-code">  'completedba']</p></li>
<li>Then, we create training<a id="_idIndexMarker417"/> and testing DataFrames, encode the <strong class="source-inline">gender</strong> feature, and scale<a id="_idIndexMarker418"/> the training data. We need to scale the target in this case since it is continuous:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97wages[feature_cols],\</p><p class="source-code">  nls97wages[['wageincome']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">X_train_enc = ohe.fit_transform(X_train)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Male']])</p><p class="source-code">y_train = \</p><p class="source-code">  pd.DataFrame(scaler.fit_transform(y_train),</p><p class="source-code">  columns=['wageincome'], index=y_train.index)</p><p class="callout-heading">Note</p><p class="callout">You may have noticed that we are not encoding or scaling the testing data. We will need to do that eventually to validate our models. We will introduce validation later in this chapter and go over it in much more detail in the next chapter.</p></li>
<li>Now, we are ready<a id="_idIndexMarker419"/> to select features. We set <strong class="source-inline">score_func</strong> of <strong class="source-inline">SelectKBest</strong> to <strong class="source-inline">f_regression</strong> and indicate<a id="_idIndexMarker420"/> that we want the five best features. The <strong class="source-inline">get_support</strong> method of <strong class="source-inline">SelectKBest</strong> returns <strong class="source-inline">True</strong> for each feature that was selected:<p class="source-code">ksel = SelectKBest(score_func=f_regression, k=5)</p><p class="source-code">ksel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[ksel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satmath', 'gpascience', 'parentincome',</strong></p><p class="source-code"><strong class="bold"> 'completedba','gender_Male'],</strong></p><p class="source-code"><strong class="bold">      dtype='object')</strong></p></li>
<li>We can use the <strong class="source-inline">scores_</strong> attribute<a id="_idIndexMarker421"/> to see the score<a id="_idIndexMarker422"/> for each feature:<p class="source-code">pd.DataFrame({'score': ksel.scores_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','score']).\</p><p class="source-code">   sort_values(['score'], ascending=False)</p><p class="source-code"> </p><p class="source-code"><strong class="bold">              feature              score</strong></p><p class="source-code"><strong class="bold">1             satmath              45</strong></p><p class="source-code"><strong class="bold">9             completedba          38</strong></p><p class="source-code"><strong class="bold">10            gender_Male          26</strong></p><p class="source-code"><strong class="bold">8</strong><strong class="bold">             parentincome         24</strong></p><p class="source-code"><strong class="bold">2             gpascience           21</strong></p><p class="source-code"><strong class="bold">0             satverbal            19</strong></p><p class="source-code"><strong class="bold">5             gpaoverall           17</strong></p><p class="source-code"><strong class="bold">4             gpamath              13</strong></p><p class="source-code"><strong class="bold">3</strong><strong class="bold">             gpaenglish           10</strong></p><p class="source-code"><strong class="bold">6             motherhighgrade       9</strong></p><p class="source-code"><strong class="bold">7             fatherhighgrade       8</strong></p></li>
</ol>
<p>The disadvantage of the F-statistic<a id="_idIndexMarker423"/> is that it assumes a linear relationship between each feature<a id="_idIndexMarker424"/> and the target. When that assumption does not make sense, we can use mutual information for regression instead.</p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Mutual information for feature selection with a continuous target</h2>
<p>We can also<a id="_idIndexMarker425"/> use <strong class="source-inline">SelectKBest</strong> to select features using mutual information<a id="_idIndexMarker426"/> for regression:</p>
<ol>
<li value="1">We need to set the <strong class="source-inline">score_func</strong> parameter of <strong class="source-inline">SelectKBest</strong> to <strong class="source-inline">mutual_info_regression</strong>, but there is a small complication. To get the same results each time we run the feature selection, we need to set a <strong class="source-inline">random_state</strong> value. As we discussed in the previous section, we can use a partial function for that. We pass <strong class="source-inline">partial(mutual_info_regression, random_state=0)</strong> to the score function.</li>
<li>We can then run the <strong class="source-inline">fit</strong> method and use <strong class="source-inline">get_support</strong> to get the selected features. We can use the <strong class="source-inline">scores_</strong> attribute to give us the score for each feature:<p class="source-code">from functools import partial</p><p class="source-code">ksel = SelectKBest(score_func=\</p><p class="source-code">  partial(mutual_info_regression, random_state=0),</p><p class="source-code">  k=5)</p><p class="source-code">ksel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[ksel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satmath', 'gpascience', 'fatherhighgrade', 'completedba','gender_Male'],dtype='object')</strong></p><p class="source-code">pd.DataFrame({'score': ksel.scores_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','score']).\</p><p class="source-code">   sort_values(['score'], ascending=False)</p><p class="source-code"><strong class="bold">           feature               score</strong></p><p class="source-code"><strong class="bold">1          satmath               0.101</strong></p><p class="source-code"><strong class="bold">10         gender_Male           0.074</strong></p><p class="source-code"><strong class="bold">7</strong><strong class="bold">          fatherhighgrade       0.047</strong></p><p class="source-code"><strong class="bold">2          gpascience            0.044</strong></p><p class="source-code"><strong class="bold">9          completedba           0.044</strong></p><p class="source-code"><strong class="bold">4          gpamath               0.016</strong></p><p class="source-code"><strong class="bold">8          parentincome          0.015</strong></p><p class="source-code"><strong class="bold">6</strong><strong class="bold">          motherhighgrade       0.012</strong></p><p class="source-code"><strong class="bold">0          satverbal             0.000</strong></p><p class="source-code"><strong class="bold">3          gpaenglish            0.000</strong></p><p class="source-code"><strong class="bold">5          gpaoverall            0.000</strong></p></li>
</ol>
<p>We get fairly similar results with mutual information for regression as we did with F-tests. <strong class="source-inline">parentincome</strong> was selected with F-tests and <strong class="source-inline">fatherhighgrade</strong> with mutual information. Otherwise, the same features are selected. </p>
<p>A key advantage of mutual information<a id="_idIndexMarker427"/> for regression compared<a id="_idIndexMarker428"/> with F-tests is that it does not assume a linear relationship between the feature and the target. If that assumption turns out to be unwarranted, mutual information is a better approach. (Again, there is also some randomness in the scoring and the score for each feature can bounce around within a limited range.)</p>
<p class="callout-heading">Note</p>
<p class="callout">Our choice of <strong class="source-inline">k=5</strong> to get the five best features is quite arbitrary. We can make it much more scientific with some hyperparameter tuning. We will go over tuning in the next chapter.</p>
<p>The feature selection methods<a id="_idIndexMarker429"/> we have used so far are known as <em class="italic">filter methods</em>. They examine the univariate relationship between each feature and the target. They are a good starting point. Similar to our discussion in previous chapters of the usefulness of having correlations<a id="_idIndexMarker430"/> handy before we start examining<a id="_idIndexMarker431"/> multivariate relationships, it is helpful to at least explore filter methods. Often, though, our model fitting will require taking into account features that are important, or not, when other features are also included. To do that, we need to use wrapper or embedded methods for feature selection. We explore wrapper methods in the next few sections, starting with forward and backward feature selection.</p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Using forward and backward feature selection</h1>
<p>Forward and backward feature selection, as their names suggest, select features by adding them one by one – or subtracting them for backward selection – and assessing the impact on model performance after each iteration. Since both methods assess that performance<a id="_idIndexMarker432"/> based on a given algorithm, they are considered <strong class="bold">wrapper</strong> selection methods.</p>
<p>Wrapper feature selection methods<a id="_idIndexMarker433"/> have two advantages over the filter methods we have explored so far. First, they evaluate the importance of features as other features are included. Second, since features are evaluated based on their contribution to the performance of a specific algorithm, we get a better sense of which features will ultimately matter. For example, <strong class="source-inline">satmath</strong> seemed to be an important feature based on our results from the previous section. But it is possible that <strong class="source-inline">satmath</strong> is only important when we use a particular model, say linear regression, and not an alternative such as decision tree regression. Wrapper selection methods can help us discover that.</p>
<p>The main disadvantage<a id="_idIndexMarker434"/> of wrapper methods is that they can be quite expensive computationally since they retrain the model after each iteration. We will look at both forward and backward feature selection in this section.</p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Using forward feature selection</h2>
<p><strong class="bold">Forward feature selection</strong> starts by identifying a subset of features<a id="_idIndexMarker435"/> that individually have a significant relationship with a target, not unlike the filter methods. But it then evaluates all possible combinations of the selected features for the combination that performs best with the chosen algorithm.</p>
<p>We can use forward feature selection to develop a model of bachelor's degree completion. Since wrapper methods require<a id="_idIndexMarker436"/> us to choose an algorithm, and this is a binary target, let's use <strong class="source-inline">scikit-learn</strong>'s <strong class="bold">random forest classifier</strong>. We will also need the <strong class="source-inline">feature_selection</strong> module of <strong class="source-inline">mlxtend</strong> to do the iteration required to select features:</p>
<ol>
<li value="1">We start by importing the necessary libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from mlxtend.feature_selection import SequentialFeatureSelector</p></li>
<li>Then, we load the NLS data<a id="_idIndexMarker437"/> again. We also create a training DataFrame, encode the <strong class="source-inline">gender</strong> feature, and standardize the remaining features:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">X_train_enc = ohe.fit_transform(X_train)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p></li>
<li>We create a random forest classifier object and then pass that object to the feature selector of <strong class="source-inline">mlxtend</strong>. We indicate<a id="_idIndexMarker438"/> that we want it to select five features and that it should forward select. (We can also use the sequential feature selector to select backward.) After running <strong class="source-inline">fit</strong>, we can use the <strong class="source-inline">k_feature_idx_</strong> attribute to get the list of selected features:<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)</p><p class="source-code">sfs = SequentialFeatureSelector(rfc, k_features=5,</p><p class="source-code">  forward=True, floating=False, verbose=2,</p><p class="source-code">  scoring='accuracy', cv=5)</p><p class="source-code">sfs.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[list(sfs.k_feature_idx_)]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satverbal', 'satmath', 'gpaoverall',</strong></p><p class="source-code"><strong class="bold">'parentincome', 'gender_Female'], dtype='object')</strong></p></li>
</ol>
<p>You might recall from the first section of this chapter that our univariate feature selection for the completed bachelor's degree target gave us somewhat different results:</p>
<p class="source-code"><strong class="bold">Index(['satverbal', 'satmath', 'gpascience',</strong></p>
<p class="source-code"><strong class="bold"> 'gpaenglish', 'gpaoverall'], dtype='object')</strong></p>
<p>Three of the features – <strong class="source-inline">satmath</strong>, <strong class="source-inline">satverbal</strong>, and <strong class="source-inline">gpaoverall</strong> – are the same. But our forward feature selection has identified <strong class="source-inline">parentincome</strong> and <strong class="source-inline">gender_Female</strong> as more important than <strong class="source-inline">gpascience</strong> and <strong class="source-inline">gpaenglish</strong>, which were selected in the univariate analysis. Indeed, <strong class="source-inline">gender_Female</strong> had among the lowest scores in the earlier analysis. These differences likely reflect the advantages of wrapper feature selection methods. We can identify features that are not important unless other features are included, and we are evaluating the effect on the performance of a particular algorithm, in this case, random forest classification.</p>
<p>One disadvantage<a id="_idIndexMarker439"/> of forward selection is that <em class="italic">once a feature is selected, it is not removed, even though it may decline in importance as additional features are added</em>. (Recall that forward feature selection adds features iteratively based on the contribution of that feature to the model.)</p>
<p>Let's see whether our results vary with backward feature selection.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Using backward feature selection</h2>
<p>Backward feature selection<a id="_idIndexMarker440"/> starts with all features and eliminates the least important. It then repeats this process with the remaining features. We can use <strong class="source-inline">mlxtend</strong>'s <strong class="source-inline">SequentialFeatureSelector</strong> for backward selection in pretty much the same way we used it for forward selection.</p>
<p>We instantiate a <strong class="source-inline">RandomForestClassifier</strong> object from the <strong class="source-inline">scikit-learn</strong> library and then pass it to <strong class="source-inline">mlxtend</strong>'s sequential feature selector:</p>
<pre class="source-code">rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)</pre>
<pre class="source-code">sfs = SequentialFeatureSelector(rfc, k_features=5,</pre>
<pre class="source-code">  forward=False, floating=False, verbose=2,</pre>
<pre class="source-code">  scoring='accuracy', cv=5)</pre>
<pre class="source-code">sfs.fit(X_train_enc, y_train.values.ravel())</pre>
<pre class="source-code">selcols = X_train_enc.columns[list(sfs.k_feature_idx_)]</pre>
<pre class="source-code">selcols</pre>
<pre class="source-code"><strong class="bold">Index(['satverbal', 'gpascience', 'gpaenglish',</strong></pre>
<pre class="source-code"><strong class="bold"> 'gpaoverall', 'gender_Female'], dtype='object')</strong></pre>
<p>Perhaps unsurprisingly, we get different results for our feature selection. <strong class="source-inline">satmath</strong> and <strong class="source-inline">parentincome</strong> are no longer selected, and <strong class="source-inline">gpascience</strong> and <strong class="source-inline">gpaenglish</strong> are.</p>
<p>Backward feature selection<a id="_idIndexMarker441"/> has the opposite drawback to forward feature selection. <em class="italic">Once a feature has been removed, it is not re-evaluated, even though its importance may change with different feature mixtures</em>. Let's try exhaustive feature selection instead.</p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Using exhaustive feature selection</h1>
<p>If your results from forward and backward selection are unpersuasive, and you do not mind running a model while you go out for coffee or lunch, you can try exhaustive feature selection. <strong class="bold">Exhaustive feature selection</strong> trains a given model on all possible combinations<a id="_idIndexMarker442"/> of features and selects the best subset of features. But it does this at a price. As the name suggests, this procedure might exhaust both system resources and your patience.</p>
<p>Let's use exhaustive feature selection<a id="_idIndexMarker443"/> for our model of bachelor's degree completion:</p>
<ol>
<li value="1">We start by loading the required libraries, including the <strong class="source-inline">RandomForestClassifier</strong> and <strong class="source-inline">LogisticRegression</strong> modules from <strong class="source-inline">scikit-learn</strong> and <strong class="source-inline">ExhaustiveFeatureSelector</strong> from <strong class="source-inline">mlxtend</strong>. We also import the <strong class="source-inline">accuracy_score</strong> module so that we can evaluate a model with the selected features:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from mlxtend.feature_selection import ExhaustiveFeatureSelector</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
<li>Next, we load the NLS educational attainment<a id="_idIndexMarker444"/> data and create training and testing DataFrames:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p></li>
<li>Then, we encode and scale the training and testing data:<p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>We create a random forest classifier<a id="_idIndexMarker445"/> object and pass it to <strong class="source-inline">mlxtend</strong>'s <strong class="source-inline">ExhaustiveFeatureSelector</strong>. We tell the feature selector to evaluate all combinations of one to five features and return the combination with the highest accuracy in predicting degree attainment. After running <strong class="source-inline">fit</strong>, we can use the <strong class="source-inline">best_feature_names_</strong> attribute to get the selected features:<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, max_depth=2,n_jobs=-1, random_state=0)</p><p class="source-code">efs = ExhaustiveFeatureSelector(rfc, max_features=5,</p><p class="source-code">  min_features=1, scoring='accuracy', </p><p class="source-code">  print_progress=True, cv=5)</p><p class="source-code">efs.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">efs.best_feature_names_</p><p class="source-code"><strong class="bold">('satverbal', 'gpascience', 'gpamath', 'gender_Female')</strong></p></li>
<li>Let's evaluate the accuracy of this model. We first need to transform the training and testing data to include only the four selected features. Then, we can fit the random forest classifier again with just those features and generate the predicted values for bachelor's degree completion. We can then calculate the percentage of the time<a id="_idIndexMarker446"/> we predicted the target correctly, which is 67%:<p class="source-code">X_train_efs = efs.transform(X_train)</p><p class="source-code">X_test_efs = efs.transform(X_test)</p><p class="source-code">rfc.fit(X_train_efs, y_train.values.ravel())</p><p class="source-code">y_pred = rfc.predict(X_test_efs)</p><p class="source-code">confusion = pd.DataFrame(y_pred, columns=['pred'],</p><p class="source-code">  index=y_test.index).\</p><p class="source-code">  join(y_test)</p><p class="source-code">confusion.loc[confusion.pred==confusion.completedba].shape[0]\</p><p class="source-code">  /confusion.shape[0]</p><p class="source-code"><strong class="bold">0.6703296703296703</strong></p></li>
<li>We get the same answer if we just use scikit-learn's <strong class="source-inline">accuracy score</strong> instead. (We calculate it in the previous step because it is pretty straightforward and it gives us a better sense of what is meant by accuracy in this case.)<p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.6703296703296703</strong></p><p class="callout-heading">Note</p><p class="callout">The accuracy score is often used to assess the performance of a classification model. We will lean on it in this chapter, but other measures might be equally or more important depending on the purposes of your model. For example, we are sometimes more concerned with sensitivity, the ratio of our correct positive predictions to the number of actual positives. We examine the evaluation of classification models in detail in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation.</em></p></li>
<li>Let's now try exhaustive feature <a id="_idIndexMarker447"/>selection with a logistic model:<p class="source-code">lr = LogisticRegression(solver='liblinear')</p><p class="source-code">efs = ExhaustiveFeatureSelector(lr, max_features=5,</p><p class="source-code">  min_features=1, scoring='accuracy', </p><p class="source-code">  print_progress=True, cv=5)</p><p class="source-code">efs.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">efs.best_feature_names_</p><p class="source-code"><strong class="bold">('satmath', 'gpascience', 'gpaenglish', 'motherhighgrade', 'gender_Female')</strong></p></li>
<li>Let's look at the accuracy of the logistic model. We get a fairly similar accuracy score:<p class="source-code">X_train_efs = efs.transform(X_train_enc)</p><p class="source-code">X_test_efs = efs.transform(X_test_enc)</p><p class="source-code">lr.fit(X_train_efs, y_train.values.ravel())</p><p class="source-code">y_pred = lr.predict(X_test_efs)</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.6923076923076923</strong></p></li>
<li>One key advantage of the logistic model is that it is much faster to train, which really makes a difference with exhaustive<a id="_idIndexMarker448"/> feature selection. If we time the training for each model (probably not a good idea to do that on your computer unless it's a pretty high-end machine or you don't mind walking away from your computer for a while), we see a substantial difference in average training time – from an amazing 5 minutes for the random forest to 4 seconds for the logistic regression. (Of course, the absolute numbers are machine-dependent.)<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, max_depth=2, </p><p class="source-code">  n_jobs=-1, random_state=0)</p><p class="source-code">efs = ExhaustiveFeatureSelector(rfc, max_features=5,</p><p class="source-code">  min_features=1, scoring='accuracy', </p><p class="source-code">  print_progress=True, cv=5)</p><p class="source-code">%timeit efs.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code"><strong class="bold">5min 8s ± 3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)</strong></p><p class="source-code">lr = LogisticRegression(solver='liblinear')</p><p class="source-code">efs = ExhaustiveFeatureSelector(lr, max_features=5,</p><p class="source-code">  min_features=1, scoring='accuracy', </p><p class="source-code">  print_progress=True, cv=5)</p><p class="source-code">%timeit efs.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code"><strong class="bold">4.29 s ± 45.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</strong></p></li>
</ol>
<p>Exhaustive feature selection<a id="_idIndexMarker449"/> can provide very clear guidance about the features to select, as I have mentioned, but that may come at too high a price for many projects. It may actually be better suited for <em class="italic">diagnostic work</em> than for use in a machine learning pipeline. If a linear model is appropriate, it can lower the computational costs considerably. </p>
<p>Wrapper methods, such as forward, backward, and exhaustive feature selection, tax system resources because they need to be trained with each iteration, and the more difficult the chosen algorithm is to implement, the more this is an issue. <strong class="bold">Recursive feature elimination</strong> (<strong class="bold">RFE</strong>) is something of a compromise between the simplicity of filter<a id="_idIndexMarker450"/> methods and the better information provided by wrapper methods. It is similar to backward feature selection, except it simplifies the removal of a feature at each iteration by basing it on the model's overall performance rather than re-evaluating each feature. We explore recursive feature selection in the next two sections.</p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Eliminating features recursively in a regression model</h1>
<p>A popular wrapper method is RFE. This method starts with all features, removes the lowest weighted one (based on a coefficient or feature importance measure), and repeats the process until the best-fitting model has been identified. When a feature is removed, it is given a ranking reflecting the point at which it was removed.</p>
<p>RFE can be used for both regression models<a id="_idIndexMarker451"/> and classification models. We<a id="_idIndexMarker452"/> will start by using it in a regression model:</p>
<ol>
<li value="1">We import the necessary libraries, three of which we have not used yet: the <strong class="source-inline">RFE</strong>, <strong class="source-inline">RandomForestRegressor</strong>, and <strong class="source-inline">LinearRegression</strong> modules from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.feature_selection import RFE</p><p class="source-code">from sklearn.ensemble import RandomForestRegressor</p><p class="source-code">from sklearn.linear_model import LinearRegression</p></li>
<li>Next, we load the NLS data<a id="_idIndexMarker453"/> on wages and create training<a id="_idIndexMarker454"/> and testing DataFrames:<p class="source-code">nls97wages = pd.read_csv("data/nls97wages.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','motherhighgrade',</p><p class="source-code">  'fatherhighgrade','parentincome','gender','completedba']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97wages[feature_cols],\</p><p class="source-code">  nls97wages[['weeklywage']], test_size=0.3, random_state=0)</p></li>
<li>We need to encode the <strong class="source-inline">gender</strong> feature and standardize the other features and the target (<strong class="source-inline">wageincome</strong>). We do not do any encoding or scaling of <strong class="source-inline">completedba</strong>, which is a binary feature:<p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = feature_cols[:-2]</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Male','completedba']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Male','completedba']])</p><p class="source-code">scaler.fit(y_train)</p><p class="source-code">y_train, y_test = \</p><p class="source-code">  pd.DataFrame(scaler.transform(y_train),</p><p class="source-code">  columns=['weeklywage'], index=y_train.index),\</p><p class="source-code">  pd.DataFrame(scaler.transform(y_test),</p><p class="source-code">  columns=['weeklywage'], index=y_test.index)</p></li>
</ol>
<p>Now, we are ready<a id="_idIndexMarker455"/> to do some recursive feature <a id="_idIndexMarker456"/>selection. Since RFE is a wrapper method, we need to choose an algorithm around which the selection will be <em class="italic">wrapped</em>. Random forests for regression make sense in this case. We are modeling a continuous target and do not want to assume a linear relationship between the features and the target.</p>
<ol>
<li value="4">RFE is fairly easy to implement with <strong class="source-inline">scikit-learn</strong>. We instantiate an RFE object, telling it what estimator we want in the process. We indicate <strong class="source-inline">RandomForestRegressor</strong>. We then fit the model and use <strong class="source-inline">get_support</strong> to get the selected features. We limit <strong class="source-inline">max_depth</strong> to <strong class="source-inline">2</strong> to avoid overfitting:<p class="source-code">rfr = RandomForestRegressor(max_depth=2)</p><p class="source-code">treesel = RFE(estimator=rfr, n_features_to_select=5)</p><p class="source-code">treesel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[treesel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold"> Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')</strong></p></li>
</ol>
<p>Note that this gives us<a id="_idIndexMarker457"/> a somewhat different list of features<a id="_idIndexMarker458"/> than using a filter method (with F-tests) for the wage income target. <strong class="source-inline">gpaoverall</strong> and <strong class="source-inline">motherhighgrade</strong> are selected here and not the <strong class="source-inline">gender</strong> flag or <strong class="source-inline">gpascience</strong>.</p>
<ol>
<li value="5">We can use the <strong class="source-inline">ranking_</strong> attribute to see when each of the eliminated features was removed:<p class="source-code">pd.DataFrame({'ranking': treesel.ranking_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','ranking']).\</p><p class="source-code">   sort_values(['ranking'], ascending=True)</p><p class="source-code"><strong class="bold">           feature                ranking</strong></p><p class="source-code"><strong class="bold">1          satmath</strong><strong class="bold">                1</strong></p><p class="source-code"><strong class="bold">5          gpaoverall             1</strong></p><p class="source-code"><strong class="bold">8          parentincome           1</strong></p><p class="source-code"><strong class="bold">9          gender_Male            1</strong></p><p class="source-code"><strong class="bold">10         completedba            1</strong></p><p class="source-code"><strong class="bold">6          motherhighgrade</strong><strong class="bold">        2</strong></p><p class="source-code"><strong class="bold">2          gpascience             3</strong></p><p class="source-code"><strong class="bold">0          satverbal              4</strong></p><p class="source-code"><strong class="bold">3          gpaenglish             5</strong></p><p class="source-code"><strong class="bold">4          gpamath                6</strong></p><p class="source-code"><strong class="bold">7          fatherhighgrade</strong><strong class="bold">        7</strong></p></li>
</ol>
<p><strong class="source-inline">fatherhighgrade</strong> was removed after the first interaction and <strong class="source-inline">gpamath</strong> after the second.</p>
<ol>
<li value="6">Let's run some test statistics. We fit only the selected features<a id="_idIndexMarker459"/> on a random forest regressor model. The <strong class="source-inline">transform</strong> method<a id="_idIndexMarker460"/> of the RFE selector gives us just the selected features with <strong class="source-inline">treesel.transform(X_train_enc)</strong>. We can use the <strong class="source-inline">score</strong> method<a id="_idIndexMarker461"/> to get the r-squared value, also known as the coefficient of determination. R-squared is a measure<a id="_idIndexMarker462"/> of the percentage of total variation explained by our model. We get a very low score, indicating that our model explains only a little of the variation. (Note that this is a stochastic process, so we will likely get different results each time we fit the model.)<p class="source-code">rfr.fit(treesel.transform(X_train_enc), y_train.values.ravel())</p><p class="source-code">rfr.score(treesel.transform(X_test_enc), y_test)</p><p class="source-code"><strong class="bold">0.13612629794428466</strong></p></li>
<li>Let's see whether we get any better results using RFE with a linear regression model. This model returns the same features as the random forest regressor:<p class="source-code">lr = LinearRegression()</p><p class="source-code">lrsel = RFE(estimator=lr, n_features_to_select=5)</p><p class="source-code">lrsel.fit(X_train_enc, y_train)</p><p class="source-code">selcols = X_train_enc.columns[lrsel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')</strong></p></li>
<li>Let's evaluate the linear model:<p class="source-code">lr.fit(lrsel.transform(X_train_enc), y_train)</p><p class="source-code">lr.score(lrsel.transform(X_test_enc), y_test)</p><p class="source-code"><strong class="bold">0.17773742846314056</strong></p></li>
</ol>
<p>The linear model is not really much better than the random forest model. This is likely a sign that, collectively, the features<a id="_idIndexMarker463"/> available to us only capture<a id="_idIndexMarker464"/> a small part of the variation in wages per week. This is an important reminder that we can identify several significant features and still have a model with limited explanatory power. (Perhaps it is also good news that our scores on standardized tests, and even our degree attainment, are important but not determinative of our wages many years later.)</p>
<p>Let's try RFE with a classification model.</p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Eliminating features recursively in a classification model</h1>
<p>RFE can also be a good<a id="_idIndexMarker465"/> choice for classification problems. We can<a id="_idIndexMarker466"/> use RFE to select features for a model of bachelor's degree completion. You may recall that we used exhaustive feature selection to select features for that model earlier in this chapter. Let's see whether we get better accuracy or an easier-to-train model with RFE:</p>
<ol>
<li value="1">We import the same libraries we have been working with so far in this chapter:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.feature_selection import RFE</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
<li>Next, we create training<a id="_idIndexMarker467"/> and testing data<a id="_idIndexMarker468"/> from the NLS educational attainment data:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, </p><p class="source-code">  random_state=0)</p></li>
<li>Then, we encode and scale the training and testing data:<p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>We instantiate a random forest<a id="_idIndexMarker469"/> classifier and pass<a id="_idIndexMarker470"/> it to the RFE selection method. We can then fit the model and get the selected features.<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, max_depth=2, </p><p class="source-code">  n_jobs=-1, random_state=0)</p><p class="source-code">treesel = RFE(estimator=rfc, n_features_to_select=5)</p><p class="source-code">treesel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[treesel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')</strong></p></li>
<li>We can also show how the features are ranked by using the RFE <strong class="source-inline">ranking_</strong> attribute:<p class="source-code">pd.DataFrame({'ranking': treesel.ranking_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','ranking']).\</p><p class="source-code">   sort_values(['ranking'], ascending=True)</p><p class="source-code"> </p><p class="source-code"><strong class="bold">           feature                 ranking</strong></p><p class="source-code"><strong class="bold">0          </strong><strong class="bold">satverbal               1</strong></p><p class="source-code"><strong class="bold">1          satmath                 1</strong></p><p class="source-code"><strong class="bold">2          gpascience              1</strong></p><p class="source-code"><strong class="bold">3          gpaenglish              1</strong></p><p class="source-code"><strong class="bold">5          gpaoverall              1</strong></p><p class="source-code"><strong class="bold">4          </strong><strong class="bold">gpamath                 2</strong></p><p class="source-code"><strong class="bold">8          parentincome            3</strong></p><p class="source-code"><strong class="bold">7          fatherhighgrade         4</strong></p><p class="source-code"><strong class="bold">6          motherhighgrade         5</strong></p><p class="source-code"><strong class="bold">9          gender_Female           6</strong></p></li>
<li>Let's look<a id="_idIndexMarker471"/> at the accuracy of a model<a id="_idIndexMarker472"/> with the selected features using the same random forest classifier we used for our baseline model:<p class="source-code">rfc.fit(treesel.transform(X_train_enc), y_train.values.ravel())</p><p class="source-code">y_pred = rfc.predict(treesel.transform(X_test_enc))</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.684981684981685</strong></p></li>
</ol>
<p>Recall that we had 67% accuracy with the exhaustive feature selection. We get about the same accuracy here. The benefit of RFE<a id="_idIndexMarker473"/> though is that it can be<a id="_idIndexMarker474"/> significantly easier to train than exhaustive feature selection.</p>
<p>Another option among wrapper and wrapper-like feature selection methods is the <strong class="bold">Boruta</strong> library. Originally developed<a id="_idIndexMarker475"/> as an R package, it can now be used with any <strong class="source-inline">scikit-learn</strong> ensemble method. We use it with <strong class="source-inline">scikit-learn</strong>'s random forest classifier in the next section.</p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Using Boruta for feature selection</h1>
<p>The Boruta package takes a unique approach<a id="_idIndexMarker476"/> to feature selection, though it has some<a id="_idIndexMarker477"/> similarities with wrapper methods. For each feature, Boruta creates a shadow feature, one with the same range of values as the original feature but with shuffled values. It then evaluates whether the original feature offers more information than the shadow feature, gradually removing features providing the least information. Boruta outputs confirmed, tentative, and rejected features with each iteration.</p>
<p>Let's use the Boruta package to select features for a classification model of bachelor's degree completion (you can install the Boruta package with <strong class="source-inline">pip</strong> if you have not yet installed it):</p>
<ol>
<li value="1">We start by loading the necessary libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from boruta import BorutaPy</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
<li>We load the NLS educational<a id="_idIndexMarker478"/> attainment data again and create<a id="_idIndexMarker479"/> the training and test DataFrames:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p></li>
<li>Next, we encode and scale the training and test data:<p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>We run Boruta feature selection<a id="_idIndexMarker480"/> in much the same way that we ran RFE feature<a id="_idIndexMarker481"/> selection. We use random forest as our baseline method again. We instantiate a random forest classifier and pass it to Boruta's feature selector. We then fit the model, which stops at <strong class="source-inline">100</strong> iterations, identifying <strong class="source-inline">9</strong> features that provide information: <p class="source-code">rfc = RandomForestClassifier(n_estimators=100, </p><p class="source-code">  max_depth=2, n_jobs=-1, random_state=0)</p><p class="source-code">borsel = BorutaPy(rfc, random_state=0, verbose=2)</p><p class="source-code">borsel.fit(X_train_enc.values, y_train.values.ravel())</p><p class="source-code"><strong class="bold">BorutaPy finished running.</strong></p><p class="source-code"><strong class="bold">Iteration:</strong><strong class="bold">            100 / 100</strong></p><p class="source-code"><strong class="bold">Confirmed:            9</strong></p><p class="source-code"><strong class="bold">Tentative:            1</strong></p><p class="source-code"><strong class="bold">Rejected:             </strong>0</p><p class="source-code">selcols = X_train_enc.columns[borsel.support_]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpamath', 'gpaoverall', 'motherhighgrade', 'fatherhighgrade', 'parentincome', 'gender_Female'], dtype='object')</strong></p></li>
<li>We can use the <strong class="source-inline">ranking_</strong> property to view<a id="_idIndexMarker482"/> the rankings of the<a id="_idIndexMarker483"/> features:<p class="source-code">pd.DataFrame({'ranking': borsel.ranking_,</p><p class="source-code">  'feature': X_train_enc.columns},</p><p class="source-code">   columns=['feature','ranking']).\</p><p class="source-code">   sort_values(['ranking'], ascending=True)</p><p class="source-code"><strong class="bold">           feature               ranking</strong></p><p class="source-code"><strong class="bold">0          satverbal             1</strong></p><p class="source-code"><strong class="bold">1          satmath               1</strong></p><p class="source-code"><strong class="bold">2          gpascience            1</strong></p><p class="source-code"><strong class="bold">3          gpaenglish            1</strong></p><p class="source-code"><strong class="bold">4</strong><strong class="bold">          gpamath               1</strong></p><p class="source-code"><strong class="bold">5          gpaoverall            1</strong></p><p class="source-code"><strong class="bold">6          motherhighgrade       1</strong></p><p class="source-code"><strong class="bold">7          fatherhighgrade       1</strong></p><p class="source-code"><strong class="bold">8          parentincome          1</strong></p><p class="source-code"><strong class="bold">9</strong><strong class="bold">          gender_Female         2</strong></p></li>
<li>To evaluate the model's accuracy, we fit the random forest classifier model with just the selected features. We can then make predictions for the testing data and compute accuracy:<p class="source-code">rfc.fit(borsel.transform(X_train_enc.values), y_train.values.ravel())</p><p class="source-code">y_pred = rfc.predict(borsel.transform(X_test_enc.values))</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.684981684981685</strong></p></li>
</ol>
<p>Part of Boruta's appeal is the persuasiveness of its selection of each feature. If a feature has been selected, then it likely does provide information that is not captured by combinations of features that exclude it. However, it is quite computationally expensive, not unlike exhaustive feature selection. It can help us sort out which features matter, but it may not always be suitable for pipelines where training speed matters.</p>
<p>The last few sections<a id="_idIndexMarker484"/> have shown some of the advantages<a id="_idIndexMarker485"/> and some disadvantages of wrapper feature selection methods. We explore embedded selection methods in the next section. These methods provide more information than filter methods but without the computational costs of wrapper methods. They do this by embedding feature selection into the training process. We will explore embedded methods with the same data we have worked with so far.</p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Using regularization and other embedded methods</h1>
<p><strong class="bold">Regularization</strong> methods are embedded methods. Like wrapper<a id="_idIndexMarker486"/> methods, embedded methods evaluate features relative to a given algorithm. But they are not as expensive computationally. That is because feature selection is built into the algorithm already and so happens as the model is being trained.</p>
<p>Embedded models use the following process:</p>
<ol>
<li value="1">Train a model.</li>
<li>Estimate each<a id="_idIndexMarker487"/> feature's importance to the model's predictions.</li>
<li>Remove features with low importance.</li>
</ol>
<p>Regularization accomplishes<a id="_idIndexMarker488"/> this by adding a penalty to any model to constrain the parameters. <strong class="bold">L1 regularization</strong>, also referred to as <strong class="bold">lasso regularization</strong>, shrinks some of the coefficients<a id="_idIndexMarker489"/> in a regression model to 0, effectively eliminating those features.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Using L1 regularization</h2>
<ol>
<li value="1">We will use L1 regularization with logistic regression<a id="_idIndexMarker490"/> to select features for a bachelor's degree attainment model:We need to first import the required libraries, including a module we will be using for the first time, <strong class="source-inline">SelectFromModel</strong> from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.feature_selection import SelectFromModel</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
<li>Next, we load NLS data on educational attainment:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, </p><p class="source-code">  random_state=0)</p></li>
<li>Then, we encode<a id="_idIndexMarker491"/> and scale the training and testing data:<p class="source-code">ohe = OneHotEncoder(drop_last=True, </p><p class="source-code">                    variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>Now we are ready to do feature selection based on logistic regression with an L1 penalty:<p class="source-code">lr = LogisticRegression(C=1, penalty="l1", </p><p class="source-code">                        solver='liblinear')</p><p class="source-code">regsel = SelectFromModel(lr, max_features=5)</p><p class="source-code">regsel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[regsel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satmath', 'gpascience', 'gpaoverall', </strong></p><p class="source-code"><strong class="bold">'fatherhighgrade', 'gender_Female'], dtype='object')</strong></p></li>
<li>Let's evaluate the accuracy of the model. We get an accuracy score of <strong class="source-inline">0.68</strong>:<p class="source-code">lr.fit(regsel.transform(X_train_enc), </p><p class="source-code">       y_train.values.ravel())</p><p class="source-code">y_pred = lr.predict(regsel.transform(X_test_enc))</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.684981684981685</strong></p></li>
</ol>
<p>This gives us fairly similar<a id="_idIndexMarker492"/> results to that of the forward feature selection for bachelor's degree completion. We used a random forest classifier as a wrapper method in that example.</p>
<p>Lasso regularization is a good choice for feature selection in a case like this, particularly when performance is a key concern. It does, however, assume a linear relationship between the features and the target, which might not be appropriate. Fortunately, there are embedded feature selection methods that do not make that assumption. A good alternative to logistic regression<a id="_idIndexMarker493"/> for the embedded model is a random forest classifier. We try that next with the same data.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Using a random forest classifier</h2>
<p>In this section, let's use<a id="_idIndexMarker494"/> a random forest classifier:</p>
<ol>
<li value="1">We can use <strong class="source-inline">SelectFromModel</strong> to use a random forest classifier rather than logistic regression:<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, </p><p class="source-code">  max_depth=2, n_jobs=-1, random_state=0)</p><p class="source-code">rfcsel = SelectFromModel(rfc, max_features=5)</p><p class="source-code">rfcsel.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">selcols = X_train_enc.columns[rfcsel.get_support()]</p><p class="source-code">selcols</p><p class="source-code"><strong class="bold">Index(['satverbal', 'gpascience', 'gpaenglish', </strong></p><p class="source-code"><strong class="bold">  'gpaoverall'], dtype='object')</strong></p></li>
</ol>
<p>This actually selects very different features from the lasso regression. <strong class="source-inline">satmath</strong>, <strong class="source-inline">fatherhighgrade</strong>, and <strong class="source-inline">gender_Female</strong> are no longer selected, while <strong class="source-inline">satverbal</strong> and <strong class="source-inline">gpaenglish</strong> are. This is likely partly due to the relaxation of the assumption of linearity.</p>
<ol>
<li value="2">Let's evaluate the accuracy of the random forest classifier model. We get an accuracy score of <strong class="bold">0.67</strong>. This is pretty much the same score that we got with the lasso regression:<p class="source-code">rfc.fit(rfcsel.transform(X_train_enc), </p><p class="source-code">        y_train.values.ravel())</p><p class="source-code">y_pred = rfc.predict(rfcsel.transform(X_test_enc))</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.673992673992674</strong></p></li>
</ol>
<p>Embedded methods are generally less CPU-/GPU-intensive than wrapper methods but can nonetheless produce good results. With our models of bachelor's degree completion in this section, we get the same accuracy as we did with our models based on exhaustive feature selection.</p>
<p>Each of the methods we have discussed so far has important use cases, as we have discussed. However, we have not yet really <a id="_idIndexMarker495"/>discussed one very challenging feature selection problem. What do you do if you simply have too many features, many of which independently account for something important in your model? By too many, here I mean that there are so many features that the model cannot run efficiently, either for training or for predicting target values. How can we reduce the feature set without sacrificing some of the predictive power of our model? In that situation, <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) might be a good approach. We'll discuss PCA in the next section.</p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Using principal component analysis</h1>
<p>A very different approach to feature selection than any of the methods we have discussed so far is PCA. PCA<a id="_idIndexMarker496"/> allows us to replace the existing feature set with a limited number of components, each of which explains an important amount of the variance. It does this by finding a component that captures the largest amount of variance, followed by a second component that captures the largest amount of remaining variance, and then a third component, and so on. One key advantage of this approach is that these components, known as <strong class="bold">principal components</strong>, are uncorrelated. We discuss PCA in detail in <a href="B17978_15_ePub.xhtml#_idTextAnchor170"><em class="italic">Chapter 15</em></a>, <em class="italic">Principal Component Analysis</em>.</p>
<p>Although I include PCA<a id="_idIndexMarker497"/> here as a feature selection approach, it is probably better to think of it as a tool for dimension reduction. We use it for feature selection when we need to limit the number of dimensions without sacrificing too much explanatory power. </p>
<p>Let's work with the NLS data again and use PCA to select features for a model of bachelor's degree completion:</p>
<ol>
<li value="1">We start by loading the necessary libraries. The only module we have not already used in this chapter is <strong class="source-inline">scikit-learn</strong>'s <strong class="source-inline">PCA</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
<li>Next, we create training<a id="_idIndexMarker498"/> and testing DataFrames once again:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpascience',</p><p class="source-code">  'gpaenglish','gpamath','gpaoverall','gender',</p><p class="source-code">  'motherhighgrade', 'fatherhighgrade','parentincome']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3,</p><p class="source-code">  random_state=0)</p></li>
<li>We need to scale and encode the data. Scaling is particularly important with PCA:<p class="source-code">ohe = OneHotEncoder(drop_last=True, </p><p class="source-code">                    variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>Now, we instantiate a <strong class="source-inline">PCA</strong> object<a id="_idIndexMarker499"/> and fit a model:<p class="source-code">pca = PCA(n_components=5)</p><p class="source-code">pca.fit(X_train_enc)</p></li>
<li>The <strong class="source-inline">components_</strong> attribute of the <strong class="source-inline">PCA</strong> object returns the scores of all 10 features on each of the 5 components. The features that drive the first component most are those with scores that have the highest absolute value. In this case, that is <strong class="source-inline">gpaoverall</strong>, <strong class="source-inline">gpaenglish</strong>, and <strong class="source-inline">gpascience</strong>. For the second component, the most important features are <strong class="source-inline">motherhighgrade</strong>, <strong class="source-inline">fatherhighgrade</strong>, and <strong class="source-inline">parentincome</strong>. <strong class="source-inline">satverbal</strong> and <strong class="source-inline">satmath</strong> drive the third component.</li>
</ol>
<p>In the following output, columns <strong class="bold">0</strong> through <strong class="bold">4</strong> are the five principal components:</p>
<p class="source-code">pd.DataFrame(pca.components_,</p>
<p class="source-code">  columns=X_train_enc.columns).T</p>
<p class="source-code"><strong class="bold">                   0</strong><strong class="bold">       1      2       3       4</strong></p>
<p class="source-code"><strong class="bold">satverbal         -0.34   -0.16  -0.61   -0.02   -0.19</strong></p>
<p class="source-code"><strong class="bold">satmath           -0.37</strong><strong class="bold">   -0.13  -0.56    0.10    0.11</strong></p>
<p class="source-code"><strong class="bold">gpascience        -0.40    0.21   0.18    0.03    0.02</strong></p>
<p class="source-code"><strong class="bold">gpaenglish        -0.40</strong><strong class="bold">    0.22   0.18    0.08   -0.19</strong></p>
<p class="source-code"><strong class="bold">gpamath           -0.38    0.24   0.12    0.08    0.23</strong></p>
<p class="source-code"><strong class="bold">gpaoverall        -0.43</strong><strong class="bold">    0.25   0.23   -0.04   -0.03</strong></p>
<p class="source-code"><strong class="bold">motherhighgrade   -0.19   -0.51   0.24   -0.43   -0.59</strong></p>
<p class="source-code"><strong class="bold">fatherhighgrade   -0.20</strong><strong class="bold">   -0.51   0.18   -0.35    0.70</strong></p>
<p class="source-code"><strong class="bold">parentincome      -0.16   -0.46   0.28    0.82   -0.08</strong></p>
<p class="source-code"><strong class="bold">gender_Female     -0.02</strong><strong class="bold">    0.08   0.12   -0.04   -0.11</strong></p>
<p>Another way to understand these scores<a id="_idIndexMarker500"/> is that they indicate how much each feature contributes to the component. (Indeed, if for each component, you square each of the 10 scores and then sum the squares, you get a total of 1.)</p>
<ol>
<li value="6">Let's also examine how much of the variance in the features is explained by each component. The first component accounts for 46% of the variance alone, followed by an additional 19% for the second component. We can use NumPy's <strong class="source-inline">cumsum</strong> method to see how much of feature variance is explained by the five components cumulatively. We can explain 87% of the variance in the 10 features with 5 components:<p class="source-code">pca.explained_variance_ratio_</p><p class="source-code"><strong class="bold">array([0.46073387, 0.19036089, 0.09295703, 0.07163009, 0.05328056])</strong></p><p class="source-code">np.cumsum(pca.explained_variance_ratio_)</p><p class="source-code"><strong class="bold">array([0.46073387, 0.65109476, 0.74405179, 0.81568188, 0.86896244])</strong></p></li>
<li>Let's transform our features in the testing data based on these five principal components. This returns a NumPy<a id="_idIndexMarker501"/> array with only the five principal components. We look at the first few rows. We also need to transform the testing DataFrame:<p class="source-code">X_train_pca = pca.transform(X_train_enc)</p><p class="source-code">X_train_pca.shape</p><p class="source-code"><strong class="bold">(634, 5)</strong></p><p class="source-code">np.round(X_train_pca[0:6],2)</p><p class="source-code"><strong class="bold">array([[ 2.79, -0.34,  </strong><strong class="bold">0.41,  1.42, -0.11],</strong></p><p class="source-code"><strong class="bold">       [-1.29,  0.79,  1.79, -0.49, -0.01],</strong></p><p class="source-code"><strong class="bold">       [-1.04, -0.72, -0.62, -0.91,  0.27],</strong></p><p class="source-code"><strong class="bold">       [-0.22, -0.8 , -0.83, -0.75,  0.59],</strong></p><p class="source-code"><strong class="bold">       [ 0.11, -0.56,  </strong><strong class="bold">1.4 ,  0.2 , -0.71],</strong></p><p class="source-code"><strong class="bold">       [ 0.93,  0.42, -0.68, -0.45, -0.89]])</strong></p><p class="source-code">X_test_pca = pca.transform(X_test_enc)</p></li>
</ol>
<p>We can now fit a model of bachelor's degree completion using these principal components. Let's run a random forest classification.</p>
<ol>
<li value="8">We first create a random forest classifier object. We then pass the training data with the principal components and the target values to its <strong class="source-inline">fit</strong> method. We pass the testing data with the components to the classifier's <strong class="source-inline">predict</strong> method and then get an accuracy score:<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, </p><p class="source-code">  max_depth=2, n_jobs=-1, random_state=0)</p><p class="source-code">rfc.fit(X_train_pca, y_train.values.ravel())</p><p class="source-code">y_pred = rfc.predict(X_test_pca)</p><p class="source-code">accuracy_score(y_test, y_pred)</p><p class="source-code"><strong class="bold">0.7032967032967034</strong></p></li>
</ol>
<p>A dimension reduction technique such as PCA can be a good option when the feature selection challenge is that we have highly correlated features and we want to reduce the number of dimensions without substantially reducing the explained variance. In this example, the high school GPA features moved together, as did the parental education and income levels and the SAT features. They became the key features for our first three components. (An argument can be made that our model could have had just those three components since together they accounted for 74% of the variance of the features.)</p>
<p>There are several modifications to PCA<a id="_idIndexMarker502"/> that might be useful depending on your data and modeling objectives. This includes strategies to handle outliers and regularization. PCA can also be extended to situations where the components are not linearly separable by using kernels. We discuss PCA in detail in <a href="B17978_15_ePub.xhtml#_idTextAnchor170"><em class="italic">Chapter 15</em></a><em class="italic">,</em> <em class="italic">Principal Component Analysis</em>.</p>
<p>Let's summarize what we've learned in this chapter. </p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>Summary</h1>
<p>In this chapter, we went over a range of feature selection methods, from filter to wrapper to embedded methods. We also saw how they work with categorical and continuous targets. For wrapper and embedded methods, we considered how they work with different algorithms.</p>
<p>Filter methods are very easy to run and interpret and are easy on system resources. However, they do not take other features into account when evaluating each feature. Nor do they tell us how that assessment might vary by the algorithm used. Wrapper methods do not have any of these limitations but they are computationally expensive. Embedded methods are often a good compromise, selecting features based on multivariate relationships and a given algorithm without taxing system resources as much as wrapper methods. We also explored how a dimension reduction method, PCA, could improve our feature selection.</p>
<p>You also probably noticed that I slipped in a little bit of model validation during this chapter. We will go over model validation in much more detail in the next chapter.</p>
</div>
</div>
</body></html>