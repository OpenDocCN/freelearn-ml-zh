<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer032">
			<h1 id="_idParaDest-42" class="chapter-number"><a id="_idTextAnchor041"/>3</h1>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>It’s All About Data – Options to Store and Transform   ML Datasets</h1>
			<p>The real work on a machine learning project only starts once the required data is available in the project development environment. Sometimes, when the data changes very frequently or the use case requires real-time data, we may need to set up some data pipelines to ensure that the required data is always available for analysis and modeling purposes. The best way to transfer, store, or transform data also depends on the size, type, and nature of the underlying data. Raw data, as collected in the real world, is often massive in size and may belong to multiple types, such as text, audio, images, videos, and so on. Due to the varying nature, size, and type of real-world data, it becomes really important to set up the correct infrastructure for storing, transferring, transforming, and analyzing the data <span class="No-Break">at scale.</span></p>
			<p>In this chapter, we will learn about the different options for moving data to the Google Cloud environment, different data storage systems, and efficient ways to apply transformations to <span class="No-Break">large-scale data.</span></p>
			<p>In this chapter, we will look at the following topics <span class="No-Break">about data:</span></p>
			<ul>
				<li>Moving data to <span class="No-Break">Google Cloud</span></li>
				<li>Where to <span class="No-Break">store data</span></li>
				<li><span class="No-Break">Transforming data</span></li>
			</ul>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Moving data to Google Cloud</h1>
			<p>When we start <a id="_idIndexMarker102"/>a machine learning project on <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), the <a id="_idIndexMarker103"/>very first step is <a id="_idIndexMarker104"/>to move all our project-related data to the Google Cloud environment. While transferring data to the cloud, the key things to focus on are reliability, security, scalability, and the ease of managing the transfer process. With these points in mind, Google Cloud provides four major data transfer utilities to <a id="_idIndexMarker105"/>meet customer requirements across a variety of <a id="_idIndexMarker106"/>use cases. In general, these utilities are useful for any kind of data transfer purposes, including data center migration, data backup, content storage, and machine learning. As our current focus is on making data available for machine learning use cases, we can utilize any of the following <span class="No-Break">transfer solutions:</span></p>
			<ul>
				<li>Google Cloud Storage <span class="No-Break">Transfer tools</span></li>
				<li>BigQuery Data <span class="No-Break">Transfer Service</span></li>
				<li>Storage <span class="No-Break">Transfer Service</span></li>
				<li><span class="No-Break">Transfer Appliance</span></li>
			</ul>
			<p>Let’s understand each of these <span class="No-Break">transfer solutions.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Google Cloud Storage Transfer tools</h2>
			<p>This option <a id="_idIndexMarker107"/>is suitable when our dataset is not too big (up to a few TB is fine), and we wish to store it in <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) buckets (GCS is an object-type storage system, very similar to the local filesystem in our computers; we will <a id="_idIndexMarker108"/>learn more about it in the next section). Google Cloud provides tools for uploading data into these GCS buckets directly from our computers. We can upload one or more files or even a folder containing files using one of the following methods and also track the progress of uploads using the <strong class="bold">upload progress</strong> window from the Google <span class="No-Break">Cloud console.</span></p>
			<p>Here are the three methods for uploading files or folders to a <span class="No-Break">GCS bucket:</span></p>
			<ul>
				<li>Using the Google Cloud <span class="No-Break">console UI</span></li>
				<li>Using the <span class="No-Break">command line</span></li>
				<li>Using the REST API (<span class="No-Break">JSON API)</span></li>
			</ul>
			<p>Let’s look at these methods in <span class="No-Break">more detail.</span></p>
			<p>Using the Google Cloud <span class="No-Break">Console UI</span></p>
			<p>It is quite easy to upload files or a folder containing files to GCS using the Cloud Console UI. When <a id="_idIndexMarker109"/>we upload a folder, the hierarchical structure inside the folder is also preserved. Follow these simple <a id="_idIndexMarker110"/>steps to upload data into a GCS bucket using <span class="No-Break">the UI:</span></p>
			<ol>
				<li>Open a browser and go to the <strong class="bold">Google Cloud </strong><span class="No-Break"><strong class="bold">Console</strong></span><span class="No-Break"> page.</span></li>
				<li>From the left pane, click on <strong class="bold">Cloud Storage</strong> and open the <strong class="bold">Buckets</strong> page. It will list all the existing buckets in <span class="No-Break">our project.</span></li>
				<li>Click on the name of the relevant bucket if it already exists; otherwise, create a new bucket to store uploaded files <span class="No-Break">or folders.</span></li>
				<li>Once we are inside the bucket, we will see bucket details and existing content. Now, we can directly upload the data using the <strong class="bold">Upload Files</strong> or <strong class="bold">Upload Folder</strong> button. The UI also provides options for creating a new folder and downloading or <span class="No-Break">deleting files.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Folder uploads are only supported using the Chrome browser. It may not work with <span class="No-Break">other browsers.</span></p>
			<h3>Using the command line</h3>
			<p>Google Cloud also provides an open source command-line utility called <strong class="bold">GSUTIL</strong>. We can utilize GSUTIL for <a id="_idIndexMarker111"/>scripted data transfers and also to manage our <a id="_idIndexMarker112"/>GCS buckets using simple commands. For large-scale streaming data, it supports multi-threaded/multi-processing data transfer for pushing script output. It can operate in <em class="italic">rsync</em> mode and transfer incremental copies <span class="No-Break">of data.</span></p>
			<p>GSUTIL commands are quite similar to Unix commands. See the following example for copying a file into the GCS bucket we <span class="No-Break">created previously:</span></p>
			<pre class="console">
$ gsutil cp Desktop/file.txt gs://my-bucket-name</pre>			<p>Similarly, we can list the content of a bucket using the <span class="No-Break"><strong class="source-inline">ls</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ gsutil ls gs://my-bucket-name
$ gsutil ls gs://my-bucket-name/my-data-folder</pre>			<h3>REST API (JSON API)</h3>
			<p>The JSON API interface lets us access or manipulate GCS data programmatically. This method <a id="_idIndexMarker113"/>is more suitable for <a id="_idIndexMarker114"/>software developers who are familiar with web programming and creating applications that consume web services using HTTP requests. For example, we can use the following HTTP request to list the objects of a <span class="No-Break">particular bucket:</span></p>
			<pre class="console">
GET https://storage.googleapis.com/storage/v1/b/my-bucket/o</pre>			<p class="callout-heading">Important note</p>
			<p class="callout">To use <a id="_idIndexMarker115"/>the preceding methods to access, manipulate, or upload data, we must have the proper <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions. The project owner can provide a list of relevant permissions to the project <span class="No-Break">development team.</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>BigQuery Data Transfer Service</h2>
			<p>BigQuery Data <a id="_idIndexMarker116"/>Transfer Service currently supports <a id="_idIndexMarker117"/>loading data from Google <strong class="bold">Software-as-a-Service</strong> (<strong class="bold">SaaS</strong>) apps, external <a id="_idIndexMarker118"/>cloud providers, data warehouses such as <strong class="bold">Teradata</strong> or <strong class="bold">Redshift</strong>, and a few <a id="_idIndexMarker119"/>third-party sources. Once data is available, we can directly perform analytics or machine learning right inside <strong class="bold">BigQuery</strong> (<strong class="bold">BQ</strong>). It can also <a id="_idIndexMarker120"/>be used as a data warehousing solution; we will learn more about BQ in the <span class="No-Break">coming sections.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Storage Transfer Service</h2>
			<p>Compared <a id="_idIndexMarker121"/>to GSUTIL, Storage Transfer Service is a managed service that is suitable for transferring data quickly and securely between object and file storage systems across different clouds (AWS and Azure), on-premises, or within different buckets in Google Cloud. The data transfer process is really fast as it utilizes a high bandwidth network. It also handles retries and provides detailed <span class="No-Break">transfer logging.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Transfer Appliance</h2>
			<p>This option is suitable when we want to migrate a really large dataset and don’t have much bandwidth. Transfer Appliance is a physical device with a high memory capacity that can be <a id="_idIndexMarker122"/>utilized for transferring and securely shipping data to a Google upload facility, where data is uploaded to cloud storage. We can order the appliance from the Cloud Console, and once we receive the device, we can start copying our data. Finally, we can ship the appliance back to Google to transfer data into a specified <span class="No-Break">GCS bucket.</span></p>
			<p>For most machine learning-related use cases, the first two methods should be enough to transfer data fast, securely, and consistently. Next, let’s learn more about the GCS and BQ data storage systems on <span class="No-Break">Google Cloud.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Where to store data</h1>
			<p>GCS and BQ are two recommended options for storing any machine learning use case-related datasets <a id="_idIndexMarker123"/>for high security and efficiency purposes. If the underlying data is structured or semi-structured, BQ is the recommended option due to its off-the-shelf features for manipulating or processing structured datasets. If the data contains images, videos, audio, and unstructured data, then GCS is the suitable option to store it. Let’s learn about these two data storage systems in <span class="No-Break">more detail.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>GCS – object storage</h2>
			<p>A significant amount of data that we collect from real-world applications is in unstructured form. Some examples are images, videos, emails, audio files, web pages, and sensor <a id="_idIndexMarker124"/>data. Managing and storing such huge amounts <a id="_idIndexMarker125"/>of unstructured data affordably and efficiently is quite challenging. Nowadays, object storage has become a preferable solution for storing such large amounts of static data and backups. Object storage is a computer data architecture that’s designed to handle large amounts of structured data efficiently. Each data object in an object-based storage system is considered a distinct unit bundled with metadata and a unique identifier that is useful in quickly retrieving and <span class="No-Break">locating data.</span></p>
			<p>GCS is an object-based storage system in Google Cloud. As it is cloud-based, GCS data can be accessed globally and provides massive scale. It is a suitable option for small to large enterprises to store their large amounts of data in a cost-effective and easily retrievable fashion. Object storage is more efficient for applications where you write the data once but <a id="_idIndexMarker126"/>have to read it very frequently. While it is extremely good for static data, it’s not a good solution for dynamic data. If data is constantly <a id="_idIndexMarker127"/>changing, we will have to write the entire data object again and again to modify it, which <span class="No-Break">is inefficient.</span></p>
			<p>GCS is frequently used by machine learning practitioners on Google Cloud due to its variety of benefits. Here are <a id="_idIndexMarker128"/>some common benefits of storing data in an object storage system such <span class="No-Break">as GCS:</span></p>
			<ul>
				<li><strong class="bold">Massively scalable</strong>: Object storage can be expanded infinitely by simply adding more servers <span class="No-Break">or devices.</span></li>
				<li><strong class="bold">Less complex</strong>: Unlike a file storage system, there is no hierarchy or folder structure in object storage, so retrieval is <span class="No-Break">quite simple.</span></li>
				<li><strong class="bold">Searchability</strong>: It is easy to search for a specific object as metadata is also part of the object. We can use tags to make objects <span class="No-Break">more filterable.</span></li>
				<li><strong class="bold">Resiliency</strong>: There’s no fear of data loss as it can automatically replicate data and store it across multiple devices or <span class="No-Break">geographical locations.</span></li>
				<li><strong class="bold">Low cost</strong>: Object storage is cost-effective and thus ideal for storing large amounts of data. Secondly, we only pay for the capacity <span class="No-Break">we use.</span></li>
			</ul>
			<p>With so <a id="_idIndexMarker129"/>many advantages, there are also some limitations of object-based <span class="No-Break">storage systems:</span></p>
			<ul>
				<li>Due to latency concerns, they cannot be used in place of traditional databases when designing <span class="No-Break">web applications</span></li>
				<li>They’re not suitable for situations when data is changing rapidly and lots of file writes are required <span class="No-Break">very frequently</span></li>
				<li>They’re not very compatible with operating system mounting and require additional clients or adapters to <span class="No-Break">work with</span></li>
			</ul>
			<p>Now that we have a fair idea about the advantages and limitations of object storage systems, we will be able to utilize them based on relevant requirements in future projects. Now, let’s learn more <span class="No-Break">about BQ.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>BQ – data warehouse</h2>
			<p>BQ is a fully managed and serverless data warehouse available on Google Cloud. It is a petabyte-scale platform <a id="_idIndexMarker130"/>that enables scalable analysis on <a id="_idIndexMarker131"/>large datasets. BQ’s serverless architecture supports SQL queries for slicing and dicing large datasets. Its analysis engine is very scalable and supports distributed analysis such that we can query terabytes of data in seconds and petabytes in minutes. BQ also supports machine learning, meaning we can train and test common ML <a id="_idIndexMarker132"/>models within BQ using just a few SQL-like commands. We will learn about <strong class="bold">BigQuery Machine Learning</strong> (<strong class="bold">BQML</strong>) in the <span class="No-Break">coming chapters.</span></p>
			<p>Behind the scenes, BQ stores data in a columnar storage format that is optimized for analytical queries. Data inside BQ is presented in a database via tables with rows and columns. It provides <a id="_idIndexMarker133"/>full support for <strong class="bold">Atomicity, Consistency, Isolation, and Durability</strong> (<strong class="bold">ACID</strong>) properties, similar to a transactional database management system. BQ provides high availability for data by automatically replicating it across multiple locations and regions in Google Cloud. In addition to the data that is present inside BQ storage, it also provides flexibility to query data from external sources, including GCS, Bigtable, Google Sheets, <span class="No-Break">and Spanner.</span></p>
			<p>If a machine learning use case involves structured or semi-structured data, BQ can be the best place to store and analyze it. In subsequent chapters, we will learn more about how BQ is an extremely useful tool for data analysts and machine learning practitioners. Here are some <a id="_idIndexMarker134"/>common benefits of using BQ as a data <span class="No-Break">warehousing solution:</span></p>
			<ul>
				<li><strong class="bold">Speed and scale</strong>: With BQ, querying through massive datasets only takes seconds. BQ was designed to analyze and store very large amounts of datasets with ease. It can scale seamlessly from petabytes <span class="No-Break">to exabytes.</span></li>
				<li><strong class="bold">Real-time analytics</strong>: BQ supports streaming data ingestion and makes it immediately available for querying. Its integration with the BI tool Looker Studio allows it to provide real-time and interactive <span class="No-Break">analytical capabilities.</span></li>
				<li><strong class="bold">Machine learning</strong>: BQ provides capabilities to build and operationalize machine learning <a id="_idIndexMarker135"/>models on both structured and unstructured data using simple SQL – in a very short time. BQ models can be directly exported so that they can be served on the Vertex AI <span class="No-Break">prediction service.</span></li>
				<li><strong class="bold">Security</strong>: BQ provides strong security and fine-grained governance controls. Data is encrypted at rest and in transit <span class="No-Break">by default.</span></li>
				<li><strong class="bold">Multi-cloud support</strong>: BQ allows for data analysis across multiple cloud platforms. BQ can run analysis on data where it is located without having to move it to a different location, which makes it more cost-effective <span class="No-Break">and flexible.</span></li>
			</ul>
			<p>We now have <a id="_idIndexMarker136"/>a fair idea of two popular data storage systems – GCS and BQ – and their benefits and limitations. Depending on the use case, we should now be capable of choosing the right place to store our machine learning datasets. Next, we’ll dig into data transformation options on <span class="No-Break">Google Cloud.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Transforming data</h1>
			<p>Raw data present in real-world applications is often unstructured and noisy. Thus, it cannot be fed <a id="_idIndexMarker137"/>directly to machine learning algorithms. We often need to apply several transformations on raw data and convert it into a format that is well supported by machine learning algorithms. In this section, we will learn about multiple options for transforming data in a scalable and efficient way on Google Cloud. </p>
			<p>Here are three common options for data transformation in the <span class="No-Break">GCP environment:</span></p>
			<ul>
				<li>Ad hoc transformation within <span class="No-Break">Jupyter Notebooks</span></li>
				<li>Cloud <span class="No-Break">Data Fusion</span></li>
				<li>Dataflow pipelines for scalable <span class="No-Break">data transformations</span></li>
			</ul>
			<p>Let’s learn about these three methods in <span class="No-Break">more detail.</span></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Ad hoc transformations within Jupyter Notebook</h2>
			<p>Machine learning algorithms are mathematical and can only understand numeric data. For example, in computer vision problems, images are converted into numerical pixel values <a id="_idIndexMarker138"/>before they’re fed into a model. Similarly, in the case of audio data, it is often converted <a id="_idIndexMarker139"/>into a time-frequency domain using different transformations, such as <strong class="bold">Fast Fourier Transformation</strong> (<strong class="bold">FFT</strong>). If data is in a tabular format and contains rows and columns, some of the <a id="_idIndexMarker140"/>columns might contain non-numeric or categorical types of data. These categorical columns are first converted into numeric form using a suitable transformation and then fed to a machine learning model or neural network. Some of these transformations can be directly applied in Jupyter notebooks using <span class="No-Break">Python functionalities.</span></p>
			<p>After reading data into a Jupyter Notebook, we can apply any desired transformations to make the dataset ready for modeling. Once the dataset is ready, we can save it somewhere (for example, BQ or GCS) with a version number so that it can be read directly into multiple different model training experiments. Machine learning practitioners often create and save multiple versions of training datasets by applying different transformations or feature engineering. It makes it easier to compare the performance of experiments over different versions of data. Let’s learn about some common transformations that are very frequently applied in machine <span class="No-Break">learning projects.</span></p>
			<p>At a high level, data can be classified into the following <span class="No-Break">two categories:</span></p>
			<ul>
				<li><strong class="bold">Numeric data</strong>: As the name suggests, this data is numeric or quantifiable – for example, age, height, <span class="No-Break">and weight</span></li>
				<li><strong class="bold">Categorical data</strong>: This data is in string format or is qualitative data – for example, gender, language, <span class="No-Break">and accent</span></li>
			</ul>
			<p>Now, let’s learn about some common transformations that can be applied to these two types of <span class="No-Break">data columns.</span></p>
			<h3>Handling numeric data</h3>
			<p>Numerical <a id="_idIndexMarker141"/>data can either be discrete <a id="_idIndexMarker142"/>or continuous. Discrete data is countable, for example, the number of players in a basketball team or the number of cities in a country. Discrete data can only take certain values, such as 10, 22, 35, 41, and so on. On the other hand, any data that <a id="_idIndexMarker143"/>is measurable is called continuous data – for example, the height of a person or the distance between two racing cars. Continuous data can virtually take any value, such as 2.3, 11, 0.0001, and <span class="No-Break">so on.</span></p>
			<p>Two common kinds of transformations are applied to <span class="No-Break">numeric data:</span></p>
			<ul>
				<li><strong class="bold">Normalizing</strong>: This is done to bring different numeric columns to the same scale. It is <a id="_idIndexMarker144"/>recommended to normalize numerical data columns to the same scale as it helps in the convergence of gradient descent-based ML algorithms. If a data column has very large values, then normalizing it can prevent <strong class="bold">NaN</strong> errors while <span class="No-Break">training models.</span></li>
				<li><strong class="bold">Bucketing</strong>: In this type of transformation, numeric data is converted into categorical data. Bucketing is usually done on continuous numeric data when there is no <a id="_idIndexMarker145"/>linear relationship between the numeric column and the target column. For example, a car manufacturing company observes that the cars that have the lowest price or the highest price are less frequently sold, but the cars with mid-range prices are more frequently sold. In this case, if we want to predict the number of cars that are sold using car price as a feature, it will be beneficial to bucketize the car price into price ranges. In this way, the model will be able to identify the mid-range bucket in which most of the cars are sold and assign more weight to <span class="No-Break">this feature.</span></li>
			</ul>
			<h3>Handling categorical data</h3>
			<p>Categorical <a id="_idIndexMarker146"/>data can be divided into two categories – <strong class="bold">ordered</strong> and <strong class="bold">nominal</strong>. Ordered <a id="_idIndexMarker147"/>categorical data <a id="_idIndexMarker148"/>has some order associated <a id="_idIndexMarker149"/>with it – for example, movie ratings (worst, bad, good, excellent) and feedback (negative, neutral, positive). Ordered data can always be marked on a scale. Nominal data, on the other hand, has no order. Some examples of nominal data include gender, country, <span class="No-Break">and language.</span></p>
			<p>Some algorithms, such as decision trees, can work well with categorical data, but most machine learning algorithms cannot handle categorical data directly. These algorithms require <a id="_idIndexMarker150"/>the categorical data to be converted <a id="_idIndexMarker151"/>into numerical form. While converting the categorical data <a id="_idIndexMarker152"/>into numerical form, some challenges are faced by machine <span class="No-Break">learning practitioners:</span></p>
			<ul>
				<li><strong class="bold">High cardinality</strong>: Cardinality means uniqueness in data. A high cardinality data column might have lots of different values – for example, ZIP codes in <span class="No-Break">country-level data.</span></li>
				<li><strong class="bold">Rare or frequent occurrences</strong>: A data column might have some values that rarely occur or some values that occur very frequently. In both cases, this column would not be significant enough to make an impact on the model due to very high or very <span class="No-Break">low variance.</span></li>
				<li><strong class="bold">Dynamic values</strong>: A data column that keeps changing some values from time to time – for example, in a city column, if new cities are added or removed <span class="No-Break">very frequently.</span></li>
			</ul>
			<p>The best way to overcome these challenges highly depends on the kind of problem or data we are dealing with. Now, let’s learn about some common methods of converting categorical data into <span class="No-Break">numerical form:</span></p>
			<ul>
				<li><strong class="bold">Label encoding</strong>: In this technique, we replace categorical data with integer values <a id="_idIndexMarker153"/>from 0 to <em class="italic">N</em>-1. Here, each integer <a id="_idIndexMarker154"/>represents a value from a categorical data column with <em class="italic">N</em> unique values. For example, if there is a categorical data column that represents colors and 10 unique color values are possible, in this case, each color will be mapped and replaced with an integer from the range of 0 to 9. This method may not be ideal for all cases as the model might consider numeric values as weights assigned to the data. Thus, this method is more suitable for ordinal <span class="No-Break">categorical data.</span></li>
				<li><strong class="bold">One-hot encoding</strong>: In label <a id="_idIndexMarker155"/>encoding, data <a id="_idIndexMarker156"/>values are represented with integers (such as 0, 1, 2, and 3). Machine learning models might mistake these integer values to consider some kind of order. For example, a value encoded with the number 2 might be given two times the priority than another value that is encoded with the number 1, but this is a wrong assumption if values in data are not ordered (such as color names and city names). This issue with label <a id="_idIndexMarker157"/>encoding can be avoided using the one-hot encoding technique. In this method, each unique value from data is considered as a new <a id="_idIndexMarker158"/>binary column (here, <em class="italic">binary column</em> represents the presence or absence of that value in a given row). For example, if a color data column can have 10 different colors, it will be encoded into 10 new columns, each representing one color. The values in these columns will be <strong class="source-inline">1</strong> if that color is present in a given row; otherwise, it will be <strong class="source-inline">0</strong>. One-hot encoding is often preferred for encoding <span class="No-Break">categorical data.</span></li>
				<li><strong class="bold">Embeddings</strong>: As one-hot encoding creates a new column for each unique value, this <a id="_idIndexMarker159"/>may create a very <a id="_idIndexMarker160"/>sparse representation of data when the number of unique values is large in number. For example, if we have a ZIP code column with 20k unique ZIP codes, the one-hot encoding method will create 20k new binary columns. Such sparse data takes a lot of memory to store and increases the complexity of machine learning training. To handle and represent such categorical data columns with a large number of unique values, dense embeddings can be used. These embeddings, however, are often generated using a neural network, so it’s an off-the-shelf encoding technique. These embeddings encode each value from a categorical column into a small dense vector of real numbers. One simple method to train and generate these embeddings is using the inbuilt Keras <span class="No-Break">embedding layer.</span></li>
			</ul>
			<p>Now that we have a good understanding of common data transformations that are easy to apply in Jupyter notebooks using Python, let’s learn about some more scalable ways of data transformation on GCP, such as Cloud Data Fusion <span class="No-Break">and Dataflow.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Cloud Data Fusion</h2>
			<p>Cloud Data Fusion is a fully managed service on GCP for quickly building and managing scalable data pipelines. Using the Data Fusion UI, we can build and deploy data pipelines without writing <a id="_idIndexMarker161"/>a single line of code (using a visual point-and-click interface). The Data Fusion UI lets us build scalable data integration solutions to clean, prepare, blend, transform, and transfer data in a fully managed way (which means we do not need to manage infrastructure). Cloud Data Fusion offers hundreds of prebuilt transformations <a id="_idIndexMarker162"/>for both batch and real-time data processing and quickly building <span class="No-Break">ETL/ELT pipelines.</span></p>
			<p>Some key <a id="_idIndexMarker163"/>features of Cloud Data Fusion are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Portability</strong>: Cloud Data Fusion is built using an open source project called <strong class="bold">CDAP</strong>, thus ensuring data <span class="No-Break">pipeline portability</span></li>
				<li><strong class="bold">Simple integration</strong>: Cloud Data Fusion’s easy integration with Google Cloud functionalities such as GCS, Dataproc, and BigQuery makes development faster and easier, <span class="No-Break">ensuring security</span></li>
				<li><strong class="bold">No-code pipelines</strong>: Even non-technical users can build data pipelines quickly using Cloud Data Fusion’s <span class="No-Break">web interface</span></li>
				<li><strong class="bold">Hybrid</strong>: Since Cloud Data Fusion is an open source project, it provides flexibility to build standardized data pipelines across hybrid and <span class="No-Break">multi-cloud environments</span></li>
				<li><strong class="bold">Security</strong>: Cloud Data Fusion provides enterprise-grade security and access management with Google Cloud for data protection <span class="No-Break">and compliance.</span></li>
			</ul>
			<p>In addition to the web UI, we can also use command-line tools to create and manage Cloud Data Fusion instances and pipelines. Next, let’s learn about another data transformation tool on GCP – <span class="No-Break">Dataflow</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Dataflow pipelines for scalable data transformations</h2>
			<p>Dataflow is <a id="_idIndexMarker164"/>a managed service <a id="_idIndexMarker165"/>for executing data processing or transformation pipelines on Google Cloud that are developed using <strong class="bold">Apache Beam SDK</strong>. It supports unified batch and stream data processing that is fast, cost-effective, and serverless (which means we do not need to manage infrastructure). Because Dataflow is serverless, it lets us focus on expressing the business logic of our data pipeline (using SQL or code) without worrying about operational tasks and infrastructure management. Due to its streaming nature, Dataflow is ideal for building real-time pipelines for use cases such as anomaly detection, pattern recognition, <span class="No-Break">and forecasting.</span></p>
			<p>By combining Dataflow with other managed Google Cloud services, we can simplify many aspects of productionizing data pipelines compared to self-managed solutions. For example, it can be combined with Google Cloud offerings such as Pub/Sub and BQ to develop <a id="_idIndexMarker166"/>a streaming solution that can ingest, process, and analyze fluctuating volumes of real-time data and generate <a id="_idIndexMarker167"/>invaluable real-time business insights. As it is managed, it provisions and scales the required resources automatically and thus reduces the time and complexity for data engineers or data analysts working on stream <span class="No-Break">analytics solutions.</span></p>
			<p>Some key <a id="_idIndexMarker168"/>features of Dataflow are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Smart autoscaling</strong>: Dataflow supports both horizontal and vertical scaling of worker nodes. Scaling is performed automatically in such a way that the utilization of worker nodes and other pipeline scaling requirements are met in an efficient (cost-effective) or <span class="No-Break">best-fit manner.</span></li>
				<li><strong class="bold">Real-time pipelines</strong>: Dataflow’s streaming nature is useful in building real-time stream analytics, machine learning forecasting, and anomaly detection pipelines. It is also useful for synchronizing or replicating data across multiple data sources (such as BQ, PostgreSQL, or Cloud Spanner) with <span class="No-Break">minimal latency.</span></li>
				<li><strong class="bold">Dataflow SQL</strong>: Dataflow streaming pipelines can be built using simple SQL commands right <span class="No-Break">from BQ.</span></li>
				<li><strong class="bold">Flexible scheduling</strong>: Batch processing jobs that need scheduling (such as overnight jobs) can <a id="_idIndexMarker169"/>easily be scheduled using Dataflow<strong class="bold"> Flexible Resource Scheduling</strong> (<strong class="bold">FlexRS</strong>) in a <span class="No-Break">cost-effective setting.</span></li>
			</ul>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Summary</h1>
			<p>Managing data effectively is really important for saving time, cost, and complexity for every organization. A machine learning practitioner should be aware of the best options for transferring, storing, and transforming data to build machine learning solutions more efficiently. In this chapter, we learned about multiple ways of bringing data into the Google Cloud environment. We discussed the best options for storing it based on the characteristics of the data. Finally, we discussed multiple different tools and methods for transforming/processing data in a <span class="No-Break">scalable manner.</span></p>
			<p>After reading this chapter, you should feel confident about choosing the best option for moving or transferring data into your Google Cloud environment based on the requirements of the use case. Choosing the best place to store data and the best strategy to analyze and transform data should be easier as we now know the pros and cons of different options. In the next chapter, we will deep dive into <strong class="bold">Vertex AI Workbench</strong>, which is a managed notebook platform within <span class="No-Break">Vertex AI.</span></p>
		</div>
	</div>
</div>
</body></html>