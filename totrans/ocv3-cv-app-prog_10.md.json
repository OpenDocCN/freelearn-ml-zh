["```py\n    // Convert keypoints into Point2f \n    std::vector<cv::Point2f> selPoints1, selPoints2; \n    std::vector<int> pointIndexes1, pointIndexes2; \n    cv::KeyPoint::convert(keypoints1,selPoints1,pointIndexes1); \n    cv::KeyPoint::convert(keypoints2,selPoints2,pointIndexes2); \n\n```", "```py\n    // Compute F matrix from 7 matches \n    cv::Mat fundamental= cv::findFundamentalMat(  \n                             selPoints1,      // 7 points in first image \n                             selPoints2,      // 7 points in second image \n                             cv::FM_7POINT);  // 7-point method \n\n```", "```py\n    // draw the left points corresponding epipolar \n    // lines in right image  \n    std::vector<cv::Vec3f> lines1; \n    cv::computeCorrespondEpilines(  \n                     selPoints1,  // image points  \n                     1,           // in image 1 (can also be 2) \n                     fundamental, // F matrix \n                     lines1);     // vector of epipolar lines \n    // for all epipolar lines \n    for (vector<cv::Vec3f>::const_iterator it= lines1.begin(); \n                it!=lines1.end(); ++it) { \n      // draw the line between first and last column \n      cv::line(image2, cv::Point(0,-(*it)[2]/(*it)[1]),\n               cv::Point(image2.cols,\n                         -((*it)[2]+(*it)[0]*image2.cols)/(*it)[1]), \n                         cv::Scalar(255,255,255)); \n    } \n\n```", "```py\n    class RobustMatcher { \n     private: \n      // pointer to the feature point detector object \n      cv::Ptr<cv::FeatureDetector> detector; \n      // pointer to the feature descriptor extractor object \n      cv::Ptr<cv::DescriptorExtractor> descriptor; \n      int normType; \n      float ratio;         // max ratio between 1st and 2nd NN \n      bool refineF;        // if true will refine the F matrix \n      bool refineM;        // if true will refine the matches  \n      double distance;     // min distance to epipolar \n      double confidence;   // confidence level (probability) \n\n     public: \n\n      RobustMatcher(const cv::Ptr<cv::FeatureDetector> &detector,         \n                    const cv::Ptr<cv::DescriptorExtractor> &descriptor=  \n                              cv::Ptr<cv::DescriptorExtractor>()):\n                    detector(detector), descriptor(descriptor),                \n                    normType(cv::NORM_L2), ratio(0.8f),  \n                    refineF(true), refineM(true),  \n                    confidence(0.98), distance(1.0) { \n\n          // in this case use the associated descriptor \n          if (!this->descriptor) {  \n            this->descriptor = this->detector; \n        }  \n      } \n\n```", "```py\n    // Match feature points using RANSAC \n    // returns fundamental matrix and output match set \n    cv::Mat match(cv::Mat& image1, cv::Mat& image2,     // input images \n                  std::vector<cv::DMatch>& matches,     // output matches \n                  std::vector<cv::KeyPoint>& keypoints1,//output keypoints \n                  std::vector<cv::KeyPoint>& keypoints2) {  \n\n       // 1\\. Detection of the feature points \n      detector->detect(image1,keypoints1); \n      detector->detect(image2,keypoints2); \n\n      // 2\\. Extraction of the feature descriptors \n      cv::Mat descriptors1, descriptors2; \n      descriptor->compute(image1,keypoints1,descriptors1); \n      descriptor->compute(image2,keypoints2,descriptors2); \n\n      // 3\\. Match the two image descriptors \n      // (optionally apply some checking method) \n\n      // Construction of the matcher with crosscheck \n      cv::BFMatcher matcher(normType,   //distance measure \n                            true);      //crosscheck flag \n      // match descriptors \n      std::vector<cv::DMatch> outputMatches; \n      matcher.match(descriptors1,descriptors2,outputMatches); \n\n      // 4\\. Validate matches using RANSAC \n      cv::Mat fundamental= ransacTest(outputMatches,        \n                                      keypoints1, keypoints2,   \n                                      matches); \n      // return the found fundamental matrix \n      return fundamental; \n    } \n\n```", "```py\n    // Prepare the matcher (with default parameters) \n    // SIFT detector and descriptor \n    RobustMatcher rmatcher(cv::xfeatures2d::SIFT::create(250)); \n\n    // Match the two images \n    std::vector<cv::DMatch> matches; \n\n    std::vector<cv::KeyPoint> keypoints1, keypoints2; \n    cv::Mat fundamental = rmatcher.match(image1, image2,    \n                                         matches,         \n                                         keypoints1, keypoints2); \n\n```", "```py\n    // Identify good matches using RANSAC \n    // Return fundamental matrix and output matches \n    cv::Mat ransacTest(const std::vector<cv::DMatch>& matches,\n                       std::vector<cv::KeyPoint>& keypoints1,  \n                       std::vector<cv::KeyPoint>& keypoints2,  \n                       std::vector<cv::DMatch>& outMatches) { \n\n      // Convert keypoints into Point2f \n      std::vector<cv::Point2f> points1, points2; \n      for (std::vector<cv::DMatch>::const_iterator it= matches.begin(); \n           it!= matches.end(); ++it) { \n\n        // Get the position of left keypoints \n        points1.push_back(keypoints1[it->queryIdx].pt); \n        // Get the position of right keypoints \n        points2.push_back(keypoints2[it->trainIdx].pt); \n      } \n\n      // Compute F matrix using RANSAC \n      std::vector<uchar> inliers(points1.size(),0); \n      cv::Mat fundamental=  \n         cv::findFundamentalMat( points1,\n                         points2,       // matching points \n                         inliers,       // match status (inlier or outlier)   \n                         cv::FM_RANSAC, // RANSAC method \n                         distance,      // distance to epipolar line \n                         confidence);   // confidence probability \n\n      // extract the surviving (inliers) matches \n      std::vector<uchar>::const_iterator itIn= inliers.begin(); \n      std::vector<cv::DMatch>::const_iterator itM= matches.begin(); \n      // for all matches \n      for ( ;itIn!= inliers.end(); ++itIn, ++itM) { \n        if (*itIn) { // it is a valid match \n        outMatches.push_back(*itM); \n      } \n    } \n    return fundamental; \n   } \n\n```", "```py\n    // Convert the keypoints in support set into Point2f  \n    points1.clear(); \n    points2.clear(); \n    for (std::vector<cv::DMatch>::const_iterator it=  \n                                      outMatches.begin(); \n         it!= outMatches.end(); ++it) { \n      // Get the position of left keypoints \n      points1.push_back(keypoints1[it->queryIdx].pt); \n      // Get the position of right keypoints \n      points2.push_back(keypoints2[it->trainIdx].pt); \n    } \n\n    // Compute 8-point F from all accepted matches \n    fundamental= cv::findFundamentalMat(  \n                      points1,points2, // matching points \n                      cv::FM_8POINT);  // 8-point method solved using SVD \n\n```", "```py\n    std::vector<cv::Point2f> newPoints1, newPoints2; \n    // refine the matches \n    correctMatches(fundamental,             // F matrix \n                   points1, points2,        // original position \n                   newPoints1, newPoints2); // new position \n\n```", "```py\n    // Find the homography between image 1 and image 2 \n    std::vector<char> inliers; \n    cv::Mat homography= cv::findHomography(       \n                            points1,\n                            points2,    // corresponding points \n                            inliers,    // outputed inliers matches  \n                            cv::RANSAC, // RANSAC method \n                            1.);     //max distance to reprojection point \n\n```", "```py\n    // Warp image 1 to image 2 \n    cv::Mat result; \n    cv::warpPerspective(image1,       // input image \n                        result,       // output image \n                        homography,   // homography \n                        cv::Size(2*image1.cols,image1.rows));  \n                        // size of output image \n\n```", "```py\n    // Copy image 1 on the first half of full image \n    cv::Mat half(result,cv::Rect(0,0,image2.cols,image2.rows)); \n    image2.copyTo(half);    // copy image2 to image1 roi \n\n```", "```py\n    // Read input images \n    std::vector<cv::Mat> images; \n    images.push_back(cv::imread(\"parliament1.jpg\")); \n    images.push_back(cv::imread(\"parliament2.jpg\")); \n\n    cv::Mat panorama;   // output panorama \n    // create the stitcher \n    cv::Stitcher stitcher = cv::Stitcher::createDefault(); \n    // stitch the images \n    cv::Stitcher::Status status = stitcher.stitch(images, panorama); \n\n```", "```py\n    class TargetMatcher { \n      private: \n      // pointer to the feature point detector object \n      cv::Ptr<cv::FeatureDetector> detector; \n      // pointer to the feature descriptor extractor object \n      cv::Ptr<cv::DescriptorExtractor> descriptor; \n      cv::Mat target;           // target image \n      int normType;             // to compare descriptor vectors \n      double distance;          // min reprojection error \n      int numberOfLevels;       // pyramid size \n      double scaleFactor;       // scale between levels \n      // the pyramid of target images and its keypoints \n      std::vector<cv::Mat> pyramid; \n      std::vector<std::vector<cv::KeyPoint>> pyrKeypoints; \n      std::vector<cv::Mat> pyrDescriptors; \n\n```", "```py\n    // Prepare the matcher \n    TargetMatcher tmatcher(cv::FastFeatureDetector::create(10), \n                           cv::BRISK::create()); \n    tmatcher.setNormType(cv::NORM_HAMMING); \n\n```", "```py\n    // set the target image \n    tmatcher.setTarget(target); \n\n```", "```py\n    // match image with target \n    tmatcher.detectTarget(image, corners); \n\n```", "```py\n   // draw the target corners on the image \n    if (corners.size() == 4) { // we have a detection \n\n      cv::line(image, cv::Point(corners[0]),  \n               cv::Point(corners[1]),\n               cv::Scalar(255, 255, 255), 3); \n      cv::line(image, cv::Point(corners[1]),  \n               cv::Point(corners[2]), \n               cv::Scalar(255, 255, 255), 3); \n      cv::line(image, cv::Point(corners[2]),  \n               cv::Point(corners[3]),\n               cv::Scalar(255, 255, 255), 3); \n      cv::line(image, cv::Point(corners[3]),  \n               cv::Point(corners[0]),\n               cv::Scalar(255, 255, 255), 3); \n    } \n\n```", "```py\n    // Set the target image \n    void setTarget(const cv::Mat t) { \n\n      target= t; \n      createPyramid(); \n    } \n    // create a pyramid of target images \n    void createPyramid() { \n\n      // create the pyramid of target images \n      pyramid.clear(); \n      cv::Mat layer(target); \n      for (int i = 0;  \n           i < numberOfLevels; i++) { // reduce size at each layer \n        pyramid.push_back(target.clone()); \n        resize(target, target, cv::Size(), scaleFactor, scaleFactor); \n      } \n\n      pyrKeypoints.clear(); \n      pyrDescriptors.clear(); \n      // keypoint detection and description in pyramid \n      for (int i = 0; i < numberOfLevels; i++) { \n        // detect target keypoints at level i \n        pyrKeypoints.push_back(std::vector<cv::KeyPoint>()); \n        detector->detect(pyramid[i], pyrKeypoints[i]); \n        // compute descriptor at level i \n        pyrDescriptors.push_back(cv::Mat()); \n        descriptor->compute(pyramid[i],  \n                            pyrKeypoints[i],\n                            pyrDescriptors[i]); \n      } \n    } \n\n```", "```py\n    // detect the defined planar target in an image \n    // returns the homography and \n    // the 4 corners of the detected target \n    cv::Mat detectTarget( \n                  const cv::Mat& image, // position of the \n                                        // target corners (clock-wise) \n                  std::vector<cv::Point2f>& detectedCorners) { \n\n      // 1\\. detect image keypoints \n      std::vector<cv::KeyPoint> keypoints; \n      detector->detect(image, keypoints); \n      // compute descriptors \n      cv::Mat descriptors; \n      descriptor->compute(image, keypoints, descriptors); \n\n      std::vector<cv::DMatch> matches; \n      cv::Mat bestHomography; \n      cv::Size bestSize; \n      int maxInliers = 0; \n      cv::Mat homography; \n\n      // Construction of the matcher   \n      cv::BFMatcher matcher(normType); \n\n      // 2\\. robustly find homography for each pyramid level \n      for (int i = 0; i < numberOfLevels; i++) { \n        // find a RANSAC homography between target and image \n        matches.clear(); \n        // match descriptors \n        matcher.match(pyrDescriptors[i], descriptors, matches); \n        // validate matches using RANSAC \n        std::vector<cv::DMatch> inliers; \n        homography = ransacTest(matches, pyrKeypoints[i],  \n                                keypoints, inliers); \n\n        if (inliers.size() > maxInliers) { // we have a better H \n          maxInliers = inliers.size(); \n          bestHomography = homography; \n          bestSize = pyramid[i].size(); \n        } \n\n      } \n\n      // 3\\. find the corner position on the image using best homography \n      if (maxInliers > 8) { // the estimate is valid \n\n        //target corners at best size \n        std::vector<cv::Point2f> corners; \n        corners.push_back(cv::Point2f(0, 0)); \n        corners.push_back(cv::Point2f(bestSize.width - 1, 0)); \n        corners.push_back(cv::Point2f(bestSize.width - 1,  \n                                      bestSize.height - 1)); \n        corners.push_back(cv::Point2f(0, bestSize.height - 1)); \n\n        // reproject the target corners \n        cv::perspectiveTransform(corners, detectedCorners, bestHomography); \n      } \n\n      return bestHomography; \n    } \n\n```"]