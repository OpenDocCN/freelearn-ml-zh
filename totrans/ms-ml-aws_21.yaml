- en: Deploying Models Built in AWS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS中构建的模型部署
- en: At this point, we have our models built in AWS and would like to ship them to
    production. We know that there is a variety of different contexts in which models
    should be deployed. In some cases, it's as easy as generating a CSV of actions
    that would be fed to some system. Often we just need to deploy a web service capable
    of making predictions. However, there are special circumstances in which we need
    to deploy these models to complex, low-latency, or edge systems. In this chapter,
    we will look at the different ways to deploy machine learning models to production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在AWS中构建了模型，并希望将它们部署到生产环境中。我们知道模型应该部署在不同的环境中。在某些情况下，这就像生成一个CSV文件，其中包含将被输入到某个系统中的操作一样简单。通常我们只需要部署一个能够进行预测的Web服务。然而，在某些特殊情况下，我们需要将这些模型部署到复杂、低延迟或边缘系统中。在本章中，我们将探讨将机器学习模型部署到生产环境的不同方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: SageMaker model deployment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker模型部署
- en: Apache Spark model deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark模型部署
- en: SageMaker model deployment
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker模型部署
- en: 'In [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, we deployed our first model with SageMaker. At that point,
    we had trained our classifier using **BlazingText** and stored it in a variable
    called `bt_model`. To deploy the model, we just need to call the `deploy` method
    stating the number and kinds of machines to use:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类Twitter流*，我们使用SageMaker部署了我们的第一个模型。在那个阶段，我们已经使用**BlazingText**训练了我们的分类器，并将其存储在一个名为`bt_model`的变量中。要部署模型，我们只需调用`deploy`方法，并指定要使用的机器数量和类型：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: SageMaker can balance the requests made to the endpoint across the number of
    instances and automatically scale up or down the depending on the service load.
    Details can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker可以在实例数量之间平衡对端点的请求，并根据服务负载自动扩展或缩减。详细信息请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)。
- en: 'Once we invoke the `deploy` method, an endpoint should appear in the AWS SageMaker
    console at [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker).
    The following screenshot shows the endpoint for our BlazingText example:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们调用`deploy`方法，一个端点应该出现在AWS SageMaker控制台中的[https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)。以下截图显示了我们的BlazingText示例端点：
- en: '![](img/312a1b7d-c26a-4093-adc1-bf6501be796d.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/312a1b7d-c26a-4093-adc1-bf6501be796d.png)'
- en: 'By clicking on the endpoint in the console, we can find further details:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在控制台中点击端点，我们可以找到更多详细信息：
- en: '![](img/3624986e-15d0-4dbe-8000-eb3eb6fe5240.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3624986e-15d0-4dbe-8000-eb3eb6fe5240.png)'
- en: 'In particular, we can see that the endpoint has a specific URL in which the
    service is hosted. If we attempt to call this URL directly via HTTP tools, such
    as `curl`, we would get the following result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们可以看到端点有一个特定的URL，服务就在那里托管。如果我们尝试通过HTTP工具，如`curl`直接调用此URL，我们会得到以下结果：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '{"message":"Missing Authentication Token"}'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '{"message":"Missing Authentication Token"}'
- en: 'This is because every request made to SageMaker endpoints must be properly
    signed to ensure authentication. Only users with role permissions to call the
    Amazon SageMaker InvokeEndpoint API will be allowed to make calls to SageMaker
    endpoints. In order for the HTTP service behind SageMaker to be able to identify
    and authenticate the caller, the `http` request needs to be properly signed. More
    information about signing requests can be found at [https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html).
    An alternative to signing the requests—if we want to expose our model endpoint
    publicly—would be to create a lambda function in AWS and expose it behind an API
    Gateway. More information about how to do that can be found here: [https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为向SageMaker端点发出的每个请求都必须正确签名以确保身份验证。只有具有调用Amazon SageMaker InvokeEndpoint
    API角色权限的用户才能允许调用SageMaker端点。为了让SageMaker背后的HTTP服务能够识别和验证调用者，HTTP请求需要正确签名。有关签名请求的更多信息，请参阅[https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html)。如果我们想公开我们的模型端点，请求签名的替代方案是创建一个AWS中的lambda函数，并通过API网关公开它。有关如何做到这一点的更多信息，请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html)。
- en: Fortunately, if we are calling the endpoint from within an AWS instance, we
    can avoid manually signing the requests by using the `sagemaker` library. Let's
    recap how such calls can be made.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如果我们从AWS实例内部调用端点，我们可以通过使用`sagemaker`库来避免手动签名请求。让我们回顾一下如何进行此类调用。
- en: 'As usual, we first import the necessary Python libraries:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先导入必要的Python库：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, if we know the name of the endpoint, we can create a `RealTimePredictor`
    instance in order to make real-time predictions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们知道端点的名称，我们可以创建一个`RealTimePredictor`实例来进行实时预测：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this case, we are using `json_serializer`, which is a convenient and human-readable
    format for our example. To invoke the endpoint, we just need to call the `predict()`
    method:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用的是`json_serializer`，这是一种方便且易于阅读的格式。要调用端点，我们只需调用`predict()`方法：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can go back to [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*, for an interpretation of this output, but the
    important point here is that the `RealTimePredictor` instance did all the proper
    authentication, request signing, and endpoint invocation on our behalf.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以回到[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类Twitter流*，来解释这个输出，但这里的重要点是`RealTimePredictor`实例代表我们完成了所有适当的身份验证、请求签名和端点调用。
- en: 'In addition to the URL and basic information about the endpoint, the AWS console
    also shows the endpoint configuration:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了端点的URL和基本信息外，AWS控制台还显示了端点配置：
- en: '![](img/87d39a59-79e7-4d24-a8b5-49e2182db1cd.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87d39a59-79e7-4d24-a8b5-49e2182db1cd.png)'
- en: 'Through the configuration, we can follow the model and training job that originated
    from this endpoint. Let''s follow the link to inspect the originating model. We
    then get the following screen:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过配置，我们可以跟踪从这个端点起源的模型和训练作业。让我们点击链接来检查起源模型。然后我们得到以下屏幕：
- en: '![](img/8bfb8d93-53da-47e1-bd60-5b3e3263f15b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8bfb8d93-53da-47e1-bd60-5b3e3263f15b.png)'
- en: In the model description, we can find details such as the S3 location of the
    model. This model serialization is specific to each kind of model. In [Chapter
    4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User Behavior with
    Tree-Based Methods*, we saw that the format of such a model was conveniently in
    an `xgboost` pickle-serialized-compatible format.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型描述中，我们可以找到诸如模型的S3位置等详细信息。这种模型序列化针对每种类型的模型都是特定的。在第4章[使用基于树的预测用户行为](af506fc8-f482-453e-8162-93a676b2e737.xhtml)中，我们看到了这种模型的格式方便地采用了`xgboost`
    pickle序列化兼容格式。
- en: You may also have noticed that there is an image associated to this model. SageMaker
    creates an image of the machine that hosts this model in the Amazon **Elastic
    Container Registry** (**ECR**). Typically these are Docker images under the hood.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到了，这个模型关联了一个图像。SageMaker在Amazon **弹性容器注册库**（**ECR**）中创建了一个托管此模型的机器的镜像。通常这些是底层的Docker镜像。
- en: 'The following link is a great resource on the inner workings of deployment
    and how containerization works within SageMaker: [https://sagemaker-workshop.com/custom/containers.html](https://sagemaker-workshop.com/custom/containers.html).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是一个关于部署内部工作原理以及SageMaker中容器化工作方式的优秀资源：[https://sagemaker-workshop.com/custom/containers.html](https://sagemaker-workshop.com/custom/containers.html)。
- en: Apache Spark model deployment
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark模型部署
- en: Apache Spark does not come with an out-of-the-box method for exposing models
    as endpoints, like SageMaker does. However, there are easy ways to load Spark
    models on standard web services using the serialization and deserialization capabilities
    of Spark's ML package. In this section, we will show how to deploy the model we
    created in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, to serve predictions through a simple
    endpoint. To do this, we will save a trained model to disk so that we can ship
    that model to the machine that is serving the model through an endpoint.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark没有像SageMaker那样提供直接将模型作为端点暴露的现成方法。然而，有简单的方法可以使用Spark ML包的序列化和反序列化功能在标准网络服务上加载Spark模型。在本节中，我们将展示如何部署我们在[第3章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)中创建的模型，即使用回归算法预测房屋价值，通过一个简单的端点提供预测。为此，我们将保存一个训练好的模型到磁盘，以便我们可以将该模型发送到通过端点提供模型的机器。
- en: 'We''ll start by training our model. In [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, we loaded the housing data into a dataframe:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先开始训练我们的模型。在[第3章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)中，*使用回归算法预测房屋价值*，我们将房屋数据加载到一个dataframe中：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To simplify this example, we''re going to use a reduced set of features to
    build a model that will be exposed as an endpoint. Of all the features, we are
    going to select just three training features (`crim`, `zn`, and `indus`):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个例子，我们将使用一组减少的特征来构建一个作为端点公开的模型。在所有特征中，我们将只选择三个训练特征（`crim`、`zn`和`indus`）：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You might recall that `medv` was the actual house value (which is the value
    we''re trying to predict). Now that we have our dataframe, we can create a `pipeline`
    just like we did before:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得`medv`是实际房屋价值（这是我们试图预测的值）。现在我们有了我们的dataframe，我们可以创建一个`pipeline`，就像我们之前做的那样：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the model instance, we can save it to disk by calling the `save()` method:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型实例，我们可以通过调用`save()`方法将其保存到磁盘：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This serialized model representation can then be shipped to the location in
    which we want to serve predictions (for example, a web server). In such a context,
    we can load back the model by invoking the `PipelineModel.load()` static method,
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种序列化模型表示可以发送到我们想要提供预测的位置（例如，一个网络服务器）。在这种情况下，我们可以通过调用`PipelineModel.load()`静态方法来重新加载模型，如下所示：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s use this model to obtain predictions for the first few rows of our reduced
    dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个模型来获取我们减少的数据集的前几行的预测：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的输出如下：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Look at how the `pipeline` model started from the raw CSV and applied all the
    transformation steps in the pipeline to finish with a prediction. Of course, it's
    not as interesting to obtain predictions from our training dataset. Realistically,
    on an endpoint serving predictions, we want to receive arbitrary values of our
    three features and obtain a prediction. At the time of this writing, Apache Spark
    can only obtain predictions given a dataframe. So, each time we want to obtain
    predictions for a few values, we need to construct a dataframe, even if we just
    need to find the prediction for a single row.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 看看`pipeline`模型是如何从原始CSV文件开始，应用管道中的所有转换步骤，最终完成预测。当然，从我们的训练数据集中获取预测并不那么有趣。从现实的角度来看，在提供预测的端点上，我们希望接收我们三个特征的所有可能值并获得预测。在撰写本文时，Apache
    Spark只能根据dataframe获取预测。因此，每次我们想要为几个值获取预测时，我们需要构建一个dataframe，即使我们只需要找到单行数据的预测。
- en: 'Suppose we want to find the prediction for this combination of features: `crim=0.00632`,
    `zn=18.0`, `indus=2.31`. The first step is to define the schema of our features
    as Spark will expect the dataframe to be in the exact format that was used for
    training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要找到以下特征组合的预测：`crim=0.00632`、`zn=18.0`、`indus=2.31`。第一步是定义我们特征的架构，因为Spark期望dataframe的格式与训练时使用的格式完全相同。
- en: 'We define the schema as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了以下模式：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding schema definition, we place the names and types of each field.
    With the schema in place, we can construct a one-row dataframe with the feature
    values we''re interested in:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的模式定义中，我们放置了每个字段的名称和类型。有了这个模式，我们可以构建一个包含我们感兴趣的特征值的单行dataframe：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is how the dataframe looks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是dataframe的样式：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With this short dataframe and the loaded model, we can obtain predictions for
    our arbitrary features:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个简短的dataframe和加载的模型，我们可以为我们任意特征获得预测：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the output of the preceding command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一个命令的输出：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, with the preceding ideas in mind, how can we construct an endpoint capable
    of serving this model? The simplest way is to use packages, such as Flask, that
    allow us to easily expose an endpoint on any machine of our choice. Details about
    flask can be found at [http://flask.pocoo.org](http://flask.pocoo.org/). To run
    a flask web service, we just need to write a Python file that knows how to respond
    to different endpoint requests. In our case, we will just create one endpoint
    to respond with a prediction given the values of our three features. We will implement
    a simple `GET` endpoint in which the three features will be passed as URL params.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到前面的想法，我们如何构建一个能够提供这个模型的服务端点？最简单的方法是使用允许我们轻松在任何选择的机器上暴露端点的包，例如Flask。有关Flask的详细信息，请参阅[http://flask.pocoo.org](http://flask.pocoo.org/)。要运行一个flask网络服务，我们只需编写一个Python文件，该文件知道如何响应不同的端点请求。在我们的案例中，我们将只创建一个端点，根据我们三个特征的值提供预测。我们将实现一个简单的`GET`端点，其中三个特征将作为URL参数传递。
- en: 'The call to the service when running on our local host will be as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地主机上运行时调用服务的命令如下：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the output of the service:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务的输出：
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To start the flask service on the machine, perform these three steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要在机器上启动flask服务，执行以下三个步骤：
- en: Create a python file that specifies how to respond to the endpoint. We will
    name this file `deploy_flask.py`.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Python文件，指定如何响应端点。我们将把这个文件命名为`deploy_flask.py`。
- en: Set the `FLASK_APP` environment variable to point to the python file we just
    created.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`FLASK_APP`环境变量设置为指向我们刚刚创建的Python文件。
- en: Run the `flask run` command.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`flask run`命令。
- en: 'In `deploy_flask.py`, we put together the preceding ideas regarding how to
    load the model and construct the dataframe for prediction:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在`deploy_flask.py`中，我们将关于如何加载模型和为预测构建dataframe的前面想法整合在一起：
- en: '[PRE20]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The only new parts in the `deploy_flask.py` file are the initialization of
    the flask app and the definition of the `predict` method, in which we extract
    the three features granted as URL params. Next, we set the mentioned environmental
    variable and run the service:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在`deploy_flask.py`文件中，唯一的新部分是flask应用的初始化和`predict`方法的定义，其中我们提取了作为URL参数授予的三个特征。接下来，我们设置提到的环境变量并运行服务：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the logs, you can see how the service and Spark are initialized, as well
    as calls made to the service:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志中，你可以看到服务和Spark的初始化过程，以及调用服务的操作：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As the flask logs mention, if you are thinking about serious production load,
    consider running flask behind a WSGI server. More information about this can be
    found in the flask documentation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如flask日志所述，如果你在考虑严肃的生产负载，考虑在WSGI服务器后面运行flask。更多关于这方面的信息可以在flask文档中找到。
- en: 'SageMaker is also able to host any arbitrary model. To do so, we need to create
    a Docker image that responds to two endpoints: `/ping` and `/invocations`. It''s
    that simple. In our case, the `/invocations` endpoint would use the loaded Spark
    model to respond with the predictions. Once the Docker image is created, we need
    to upload it to AWS ECR. As soon as it''s loaded on ECR, we can create a SageMaker
    model just by providing the ECR image identifier.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker也能够托管任何任意模型。为此，我们需要创建一个响应两个端点的Docker镜像：`/ping`和`/invocations`。就这么简单。在我们的案例中，`/invocations`端点将使用加载的Spark模型来响应预测。一旦Docker镜像创建完成，我们需要将其上传到AWS
    ECR。一旦它被加载到ECR上，我们只需提供ECR镜像标识符就可以创建一个SageMaker模型。
- en: 'In the AWS Console (or through the API), choose to create a model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS控制台（或通过API）中，选择创建模型：
- en: '![](img/214256d3-0872-4212-9a24-73e1d2fcc383.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/214256d3-0872-4212-9a24-73e1d2fcc383.png)'
- en: 'Once you provide the basic model details, input the ECR location of your custom
    inference endpoint:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你提供了基本模型详情，输入你自定义推理端点的ECR位置：
- en: '![](img/6512da6a-4c59-4b7c-8033-f0e0ee290158.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6512da6a-4c59-4b7c-8033-f0e0ee290158.png)'
- en: Like any SageMaker model, you can deploy it to an endpoint with the usual means.
    We won't go through the process of the Docker image creation in this chapter,
    but notebooks are available at our GitHub repository ([https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws))
    under [Chapter 16](6bc1a319-1195-4c30-8de8-09c795076f10.xhtml), *Deploying Models
    Built in AWS*, that explain how to do so.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何 SageMaker 模型一样，您可以使用常规方法将其部署到端点。在本章中，我们不会介绍 Docker 镜像创建的过程，但您可以在我们的 GitHub
    仓库中找到笔记本，网址为 [https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws)，在
    [第 16 章](6bc1a319-1195-4c30-8de8-09c795076f10.xhtml)，*在 AWS 中构建的模型部署*，其中解释了如何进行部署。
- en: Even if your production environment is outside of AWS, SageMaker and Spark in
    EMR can be of great use, as models can be trained in AWS offline and shipped to
    a different environment. Also, the artifacts created by AWS as models can usually
    be obtained and used offline (this was the case for the `xgboost` model). If you
    need to port the Spark ML models to an environment in which you can't instantiate
    a local Spark session or need a very low-latency predictor, consider using the
    following tool: [https://github.com/TrueCar/mleap](https://github.com/TrueCar/mleap).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您的生产环境不在 AWS 内，SageMaker 和 EMR 中的 Spark 也可以非常有用，因为模型可以在 AWS 离线训练并发送到不同的环境。此外，AWS
    创建的模型工件通常可以离线获取和使用（例如 `xgboost` 模型）。如果您需要将 Spark ML 模型移植到无法实例化本地 Spark 会话的环境或需要一个非常低延迟的预测器，请考虑使用以下工具：[https://github.com/TrueCar/mleap](https://github.com/TrueCar/mleap)。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how models are deployed through SageMaker and
    covered how the endpoints are defined and invoked. Through the use of Spark's
    model serialization and deserialization, we illustrated how models can be shipped
    to other environments, such as a custom web service implementation in flask. Finally,
    we outlined how your Spark model (or any other arbitrary model) can be served
    through SageMaker by registering a custom Docker image in AWS ECR.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过 SageMaker 部署模型，并介绍了端点的定义和调用方法。通过使用 Spark 的模型序列化和反序列化，我们展示了模型如何被发送到其他环境，例如
    flask 中的自定义 Web 服务实现。最后，我们概述了如何通过在 AWS ECR 中注册自定义 Docker 镜像，将您的 Spark 模型（或任何其他任意模型）通过
    SageMaker 提供服务。
- en: Exercises
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Why do SageMaker endpoints respond with a missing authentication token message
    when you attempt to access the service directly?
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么当您尝试直接访问服务时，SageMaker 端点会响应一个缺少身份验证令牌的消息？
- en: Name two alternatives to solve the preceding problem.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出两种解决上述问题的替代方案。
- en: Provide two means to deploy a model built on Apache Spark onto an endpoint.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供两种将基于 Apache Spark 构建的模型部署到端点的方法。
- en: Using our flask example as a basis, construct a Docker image that servers the
    `/invocations` and `/ping` endpoint and then deploys a model through SageMaker.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以我们的 flask 示例为基础，构建一个 Docker 镜像，该镜像提供 `/invocations` 和 `/ping` 端点，然后通过 SageMaker
    部署一个模型。
