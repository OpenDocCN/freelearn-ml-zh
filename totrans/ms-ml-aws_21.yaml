- en: Deploying Models Built in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have our models built in AWS and would like to ship them to
    production. We know that there is a variety of different contexts in which models
    should be deployed. In some cases, it's as easy as generating a CSV of actions
    that would be fed to some system. Often we just need to deploy a web service capable
    of making predictions. However, there are special circumstances in which we need
    to deploy these models to complex, low-latency, or edge systems. In this chapter,
    we will look at the different ways to deploy machine learning models to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, we deployed our first model with SageMaker. At that point,
    we had trained our classifier using **BlazingText** and stored it in a variable
    called `bt_model`. To deploy the model, we just need to call the `deploy` method
    stating the number and kinds of machines to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: SageMaker can balance the requests made to the endpoint across the number of
    instances and automatically scale up or down the depending on the service load.
    Details can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we invoke the `deploy` method, an endpoint should appear in the AWS SageMaker
    console at [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker).
    The following screenshot shows the endpoint for our BlazingText example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/312a1b7d-c26a-4093-adc1-bf6501be796d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By clicking on the endpoint in the console, we can find further details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3624986e-15d0-4dbe-8000-eb3eb6fe5240.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In particular, we can see that the endpoint has a specific URL in which the
    service is hosted. If we attempt to call this URL directly via HTTP tools, such
    as `curl`, we would get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '{"message":"Missing Authentication Token"}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because every request made to SageMaker endpoints must be properly
    signed to ensure authentication. Only users with role permissions to call the
    Amazon SageMaker InvokeEndpoint API will be allowed to make calls to SageMaker
    endpoints. In order for the HTTP service behind SageMaker to be able to identify
    and authenticate the caller, the `http` request needs to be properly signed. More
    information about signing requests can be found at [https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html).
    An alternative to signing the requests—if we want to expose our model endpoint
    publicly—would be to create a lambda function in AWS and expose it behind an API
    Gateway. More information about how to do that can be found here: [https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, if we are calling the endpoint from within an AWS instance, we
    can avoid manually signing the requests by using the `sagemaker` library. Let's
    recap how such calls can be made.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we first import the necessary Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, if we know the name of the endpoint, we can create a `RealTimePredictor`
    instance in order to make real-time predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we are using `json_serializer`, which is a convenient and human-readable
    format for our example. To invoke the endpoint, we just need to call the `predict()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can go back to [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*, for an interpretation of this output, but the
    important point here is that the `RealTimePredictor` instance did all the proper
    authentication, request signing, and endpoint invocation on our behalf.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the URL and basic information about the endpoint, the AWS console
    also shows the endpoint configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87d39a59-79e7-4d24-a8b5-49e2182db1cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Through the configuration, we can follow the model and training job that originated
    from this endpoint. Let''s follow the link to inspect the originating model. We
    then get the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bfb8d93-53da-47e1-bd60-5b3e3263f15b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the model description, we can find details such as the S3 location of the
    model. This model serialization is specific to each kind of model. In [Chapter
    4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User Behavior with
    Tree-Based Methods*, we saw that the format of such a model was conveniently in
    an `xgboost` pickle-serialized-compatible format.
  prefs: []
  type: TYPE_NORMAL
- en: You may also have noticed that there is an image associated to this model. SageMaker
    creates an image of the machine that hosts this model in the Amazon **Elastic
    Container Registry** (**ECR**). Typically these are Docker images under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link is a great resource on the inner workings of deployment
    and how containerization works within SageMaker: [https://sagemaker-workshop.com/custom/containers.html](https://sagemaker-workshop.com/custom/containers.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark does not come with an out-of-the-box method for exposing models
    as endpoints, like SageMaker does. However, there are easy ways to load Spark
    models on standard web services using the serialization and deserialization capabilities
    of Spark's ML package. In this section, we will show how to deploy the model we
    created in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, to serve predictions through a simple
    endpoint. To do this, we will save a trained model to disk so that we can ship
    that model to the machine that is serving the model through an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by training our model. In [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, we loaded the housing data into a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To simplify this example, we''re going to use a reduced set of features to
    build a model that will be exposed as an endpoint. Of all the features, we are
    going to select just three training features (`crim`, `zn`, and `indus`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You might recall that `medv` was the actual house value (which is the value
    we''re trying to predict). Now that we have our dataframe, we can create a `pipeline`
    just like we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the model instance, we can save it to disk by calling the `save()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This serialized model representation can then be shipped to the location in
    which we want to serve predictions (for example, a web server). In such a context,
    we can load back the model by invoking the `PipelineModel.load()` static method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use this model to obtain predictions for the first few rows of our reduced
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Look at how the `pipeline` model started from the raw CSV and applied all the
    transformation steps in the pipeline to finish with a prediction. Of course, it's
    not as interesting to obtain predictions from our training dataset. Realistically,
    on an endpoint serving predictions, we want to receive arbitrary values of our
    three features and obtain a prediction. At the time of this writing, Apache Spark
    can only obtain predictions given a dataframe. So, each time we want to obtain
    predictions for a few values, we need to construct a dataframe, even if we just
    need to find the prediction for a single row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to find the prediction for this combination of features: `crim=0.00632`,
    `zn=18.0`, `indus=2.31`. The first step is to define the schema of our features
    as Spark will expect the dataframe to be in the exact format that was used for
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the schema as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding schema definition, we place the names and types of each field.
    With the schema in place, we can construct a one-row dataframe with the feature
    values we''re interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the dataframe looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With this short dataframe and the loaded model, we can obtain predictions for
    our arbitrary features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, with the preceding ideas in mind, how can we construct an endpoint capable
    of serving this model? The simplest way is to use packages, such as Flask, that
    allow us to easily expose an endpoint on any machine of our choice. Details about
    flask can be found at [http://flask.pocoo.org](http://flask.pocoo.org/). To run
    a flask web service, we just need to write a Python file that knows how to respond
    to different endpoint requests. In our case, we will just create one endpoint
    to respond with a prediction given the values of our three features. We will implement
    a simple `GET` endpoint in which the three features will be passed as URL params.
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to the service when running on our local host will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the flask service on the machine, perform these three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a python file that specifies how to respond to the endpoint. We will
    name this file `deploy_flask.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `FLASK_APP` environment variable to point to the python file we just
    created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `flask run` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `deploy_flask.py`, we put together the preceding ideas regarding how to
    load the model and construct the dataframe for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The only new parts in the `deploy_flask.py` file are the initialization of
    the flask app and the definition of the `predict` method, in which we extract
    the three features granted as URL params. Next, we set the mentioned environmental
    variable and run the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the logs, you can see how the service and Spark are initialized, as well
    as calls made to the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As the flask logs mention, if you are thinking about serious production load,
    consider running flask behind a WSGI server. More information about this can be
    found in the flask documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker is also able to host any arbitrary model. To do so, we need to create
    a Docker image that responds to two endpoints: `/ping` and `/invocations`. It''s
    that simple. In our case, the `/invocations` endpoint would use the loaded Spark
    model to respond with the predictions. Once the Docker image is created, we need
    to upload it to AWS ECR. As soon as it''s loaded on ECR, we can create a SageMaker
    model just by providing the ECR image identifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the AWS Console (or through the API), choose to create a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/214256d3-0872-4212-9a24-73e1d2fcc383.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you provide the basic model details, input the ECR location of your custom
    inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6512da6a-4c59-4b7c-8033-f0e0ee290158.png)'
  prefs: []
  type: TYPE_IMG
- en: Like any SageMaker model, you can deploy it to an endpoint with the usual means.
    We won't go through the process of the Docker image creation in this chapter,
    but notebooks are available at our GitHub repository ([https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws))
    under [Chapter 16](6bc1a319-1195-4c30-8de8-09c795076f10.xhtml), *Deploying Models
    Built in AWS*, that explain how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your production environment is outside of AWS, SageMaker and Spark in
    EMR can be of great use, as models can be trained in AWS offline and shipped to
    a different environment. Also, the artifacts created by AWS as models can usually
    be obtained and used offline (this was the case for the `xgboost` model). If you
    need to port the Spark ML models to an environment in which you can't instantiate
    a local Spark session or need a very low-latency predictor, consider using the
    following tool: [https://github.com/TrueCar/mleap](https://github.com/TrueCar/mleap).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how models are deployed through SageMaker and
    covered how the endpoints are defined and invoked. Through the use of Spark's
    model serialization and deserialization, we illustrated how models can be shipped
    to other environments, such as a custom web service implementation in flask. Finally,
    we outlined how your Spark model (or any other arbitrary model) can be served
    through SageMaker by registering a custom Docker image in AWS ECR.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do SageMaker endpoints respond with a missing authentication token message
    when you attempt to access the service directly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two alternatives to solve the preceding problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide two means to deploy a model built on Apache Spark onto an endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using our flask example as a basis, construct a Docker image that servers the
    `/invocations` and `/ping` endpoint and then deploys a model through SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
