- en: 'Chapter 10: Optimizing Model Hosting and Inference Costs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The introduction of more powerful computers (notably with **graphical processing
    units**, or **GPUs**) and powerful **machine learning** (**ML**) frameworks such
    as TensorFlow has resulted in a generational leap in ML capabilities. As ML practitioners,
    our purview now includes optimizing the use of these new capabilities to maximize
    the value we get for the time and money we spend.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll learn how to use multiple deployment strategies to meet
    your training and inference requirements. You'll learn when to get and store inferences
    in advance versus getting them on demand, how to scale inference services to meet
    fluctuating demand, and how to use multiple models for model testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time inference versus batch inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying multiple models behind a single inference endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling inference endpoints to meet inference traffic demands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Elastic Inference for deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing models with SageMaker Neo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is in the `CH10` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time inference versus batch inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker provides two ways to obtain inferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time inference** lets you get a single inference per request, or a small
    number of inferences, with very low latency from a live inference endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch inference** lets you get a large number of inferences from a batch
    processing job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch inference is more efficient and more cost-effective. Use it whenever your
    inference requirements allow. We'll explore batch inference first, and then pivot
    to real-time inference.
  prefs: []
  type: TYPE_NORMAL
- en: Batch inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, we can make inferences in advance and store them for later use.
    For example, if you want to generate product recommendations for users on an e-commerce
    site, those recommendations may be based on the users' prior purchases and which
    products you want to promote the next day. You can generate the recommendations
    nightly and store them for your e-commerce site to call up when the users browse
    the site.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several options for storing batch inferences. Amazon DynamoDB is
    a common choice for several reasons, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is fast. You can look up single values within a few milliseconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is scalable. You can store millions of values at a low cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best access pattern for DynamoDB is looking up values by a high-cardinality
    primary key. This fits well with many inference usage patterns, for example, when
    we want to look up a stored recommendation for an individual user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use other data stores, including DocumentDB and Aurora, depending on
    your access patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `CH10` folder of the GitHub repository, you''ll find the `optimize.ipynb`
    notebook. The *Real-time and Batch Inference* section of this repository walks
    you through performing both batch and real-time inference using a simple XGBoost
    model. The following code lets you run a batch inference job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This job takes approximately 3 minutes to run.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you deploy a SageMaker model to a real-time inference endpoint, SageMaker
    deploys the model artifact and your inference code (packaged in a Docker image)
    to one or more inference instances. You now have a live API endpoint for inference,
    and you can invoke it from other software services on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'You pay for the inference endpoints (instances) as long as they are running.
    Use real-time inference in the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The inferences are dependent on *context*. For example, if you want to recommend
    a video to watch, the inference may depend on the show your user just finished.
    If you have a large video catalog, you can't generate all the possible permutations
    of recommendations in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may need to provide inferences for *new events*. For example, if you are
    trying to classify a credit card transaction as fraudulent or not, you need to
    wait until your user actually attempts a transaction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code deploys an inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the endpoint is live, we can obtain inferences using the endpoint we just
    deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using our simple XGBoost model, an inference takes approximately 30 milliseconds
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Cost comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a scenario where we want to predict the measurements for the next day
    for all of our weather stations and make them available for lookup on an interactive
    website. We have approximately 11,000 unique stations and 7 different parameters
    to predict for each station.
  prefs: []
  type: TYPE_NORMAL
- en: With a real-time endpoint using the `ml.m5.2xlarge` instance type, we pay $0.538
    per hour, or approximately $387 per month. With batch inference, we pay $1.075
    per hour for an `ml.m5.4xlarge` instance. The job takes 3 minutes to run per day,
    or 90 minutes per month. That's about $1.61.
  prefs: []
  type: TYPE_NORMAL
- en: The batch inference approach is typically much more cost-effective if you do
    not need context-sensitive real-time predictions. Serving predictions out of a
    NoSQL database is a better option.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying multiple models behind a single inference endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A SageMaker inference endpoint is a logical entity that actually holds a load
    balancer and one or more instances of your inference container. You can deploy
    either multiple versions of the same model or entirely different models behind
    a single endpoint. In this section, we'll look at these two use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple versions of the same model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A SageMaker endpoint lets you host multiple models that serve different percentages
    of traffic for incoming requests. That capability supports common **continuous
    integration** (**CI**)/**continuous delivery** (**CD**) practices such as canary
    and blue/green deployments. While these practices are similar, they have slightly
    different purposes, as explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: A **canary deployment** means that you let the new version of a model host a
    small percentage of traffic that lets you test a new version of the model on a
    subset of traffic until you are satisfied that it is working well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **blue/green deployment** means that you run two versions of the model at
    the same time, keeping an older version around for quick failover if a problem
    occurs in the new version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, these are variations on a theme. In SageMaker, you designate how
    much traffic each model variant handles. For canary deployments, you'd start with
    a small fraction (usually 1-5%) for the new model versions. For blue/green deployments,
    you'd use 100% for the new version but flip back to 0% if a problem occurs.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to accomplish these deployment modes. For example, you
    can use two inference endpoints and handle traffic shaping using DNS (Route 53),
    a load balancer, or Global Accelerator. But managing the traffic through SageMaker
    simplifies your operational burden and reduces cost, as you don't have to have
    two endpoints running.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *A/B Testing* section of the notebook, we''ll create another version
    of the model and create a new endpoint that uses both models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by training another version of the model with a hyperparameter
    change (maximum tree depth of `10` instead of `5`), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define endpoint variants for each model version. The most important
    parameter here is `initial_weight`, which specifies how much traffic should go
    to each model version. By setting both versions to `1`, the traffic will split
    evenly between them. For an A/B test, you might start with weights of `20` for
    the existing version and `1` for the new version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we deploy a new model using the following two model variants:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can test the new endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You''ll see output that looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the traffic is flipping between the two versions of the model according
    to the weights we specified. In a production use case, you should automate the
    model endpoint update in your CI/CD or MLOps automation tools.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In other cases, you may need to run entirely different models. For example,
    perhaps you want one model to serve weather inferences for the United States and
    another model to serve weather inferences for Germany. You can build models that
    are sensitive to differences between these two countries. You can host both models
    behind the same endpoint and direct traffic to them based on the incoming request.
  prefs: []
  type: TYPE_NORMAL
- en: Or, for an A/B test, you might want to control which traffic goes to your new
    model version rather than letting a load balancer perform random weighted distribution.
    If you have an application server that identifies which consumers should use the
    new model version, you can direct that traffic to a specific model behind an inference
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Multiple models in a single endpoint* notebook section, we''ll walk
    through an example of creating models optimized for different air quality parameters.
    When we want a prediction, we specify which type of parameter we want, and the
    endpoint directs our request to the appropriate model. This use case is quite
    realistic; it may turn out that it''s difficult to predict both particulate matter
    (`PM25`) and ozone (`O3`) using the same model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''re going to prepare new datasets that only contain data for a single
    parameter by creating a Spark processing job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll repeat the preceding step for `PM25` and `O3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will train new XGBoost models against the single-parameter training
    sets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the multi-model class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we deploy the multi-model endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, the endpoint does not actually have any models behind it. We
    need to add them next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''re ready to test the endpoint. Download two test files, one for each parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the files and get inferences, specifying which model we want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we've seen how to deploy multiple models for testing or other purposes,
    let's turn to handling fluctuating traffic demands.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling inference endpoints to meet inference traffic demands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we need a real-time inference endpoint, the processing power requirements
    may vary based on incoming traffic. For example, if we are providing air quality
    inferences for a mobile application, usage will likely fluctuate based on time
    of day. If we provision the inference endpoint for peak load, we will pay too
    much during off-peak times. If we provision the inference endpoint for a smaller
    load, we may hit performance bottlenecks during peak times. We can use inference
    endpoint auto-scaling to adjust capacity to demand.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of scaling, vertical and horizontal. **Vertical scaling**
    means that we adjust the size of an individual endpoint instance. **Horizontal
    scaling** means that we adjust the number of endpoint instances. We prefer horizontal
    scaling as it results in less disruption for end users; a load balancer can redistribute
    traffic without having an impact on end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four steps to configure autoscaling for a SageMaker inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the minimum and maximum number of instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a scaling metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the scaling policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the cooldown period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although you can set up autoscaling automatically using the API, in this section,
    we''ll go through the steps in the console. To begin, go to the **Endpoints**
    section of the SageMaker console, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Endpoints listed in the SageMaker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Endpoints listed in the SageMaker console
  prefs: []
  type: TYPE_NORMAL
- en: 'Select one of your endpoints, and in the section called **Endpoint runtime
    settings**, choose **Configure auto scaling**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Endpoint runtime settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Endpoint runtime settings
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's walk through the more detailed inference endpoint settings.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the minimum and maximum capacity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can set boundaries on the minimum and maximum number of instances an endpoint
    can use. These boundaries let you protect against surges in demand that will result
    in unexpected costs. If you anticipate periodic spikes, build a circuit breaker
    into your application to shed load before it hits the inference endpoint. The
    following screenshot shows these settings in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Setting minimum and maximum capacity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Setting minimum and maximum capacity
  prefs: []
  type: TYPE_NORMAL
- en: If your load is highly variable, you can start with a small instance type and
    scale up aggressively. This prevents you from paying for a larger instance type
    that you don't always need.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a scaling metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to decide when to trigger a scaling action. We do that by specifying
    a CloudWatch metric. By default, SageMaker provides two useful metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`InvocationsPerInstance` reports the number of inference requests sent to each
    endpoint instance over some time period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelLatency` is the time in microseconds to respond to inference requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend `ModelLatency` as a metric for autoscaling, as it reports on the
    end user experience. Setting the actual value for the metric will depend on your
    requirements and some observation of endpoint performance over time. For example,
    you may find that latency over 100 milliseconds results in a degraded user experience
    if the inference result passes through several other services that add their own
    latency before the result reaches the end user.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the scaling policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can choose between **target tracking** and **step scaling**. Target tracking
    policies are more useful and try to adjust capacity to keep some target metric
    within a given boundary. Step scaling policies are more advanced and increase
    capacity in incremental steps.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the cooldown period
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **cooldown period** is how long the endpoint will wait after one scaling
    action before starting another scaling action. If you let the endpoint respond
    instantaneously, you'd end up scaling too often. As a general rule, scale up aggressively
    and scale down conservatively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to configure the target metric value and
    cooldown period if you use the default scaling policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Setting a target metric value and cooldown period'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Setting a target metric value and cooldown period
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at another optimization technique for deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Using Elastic Inference for deep learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you examine the overall cost of ML, you may be surprised to see that the
    bulk of your monthly cost comes from real-time inference endpoints. Training jobs,
    while potentially resource-intensive, run for some time and then terminate. Managed
    notebook instances can be shut down during off hours. But inference endpoints
    run 24 hours a day, 7 days a week. If you are using a deep learning model, inference
    endpoint costs become more pronounced, as instances with dedicated GPU capacity
    are more expensive than other comparable instances.
  prefs: []
  type: TYPE_NORMAL
- en: When you obtain inferences from a deep learning model, you do not need as much
    GPU capacity as you need during training. **Elastic Inference** lets you attach
    fractional GPU capacity to regular EC2 instances or **Elastic Container Service**
    (**ECS**) containers. As a result, you can get deep learning inferences quickly
    at a reduced cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Elastic Inference* section in the notebook shows how to attach an Elastic
    Inference accelerator to an endpoint, as you can see in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider a case where we need some GPU capacity for inference. Let''s consider
    three options for the instance type and compare the cost. Assume that we run the
    endpoint for 720 hours per month. The next table compares the monthly cost for
    different inference options, using published prices at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Inference cost comparison'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Inference cost comparison
  prefs: []
  type: TYPE_NORMAL
- en: You'll need to look at your specific use case and figure out the best combination
    of RAM, CPU, network throughput, and GPU capacity that meets your performance
    requirements at the lowest cost. If your inferences are entirely GPU-bound, the
    Inferentia instance will probably give you the best price-performance balance.
    If you need more traditional compute resources with some GPU, the P2/P3 family
    will work well. If you need very little overall capacity, Elastic Inference provides
    the cheapest GPU option.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll cover one more optimization technique for models
    deployed to specific hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing models with SageMaker Neo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how Elastic Inference can reduce inference costs
    for deep learning models. Similarly, SageMaker Neo lets you improve inference
    performance and reduce costs by compiling trained ML models for better performance
    on specific platforms. While that will help in general, it's particularly effective
    when you are trying to run inference on low-powered edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use SageMaker Neo, you simply start a compilation job with a trained
    model in a supported framework. When the compilation job completes, you can deploy
    the artifact to a SageMaker endpoint or to an edge device using the *Greengrass*
    IoT platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Model optimization with SageMaker Neo* section in the notebook demonstrates
    how to compile our XGBoost model for use on a hosted endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to get the length (number of features) of an input record:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll compile one of our trained models. We need to specify the target
    platform, which in this case is just a standard `ml_m5` family:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the compilation job finishes, we can deploy the compiled model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s test the endpoint to see whether we see a speed-up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After sending in a few invocation requests, let''s check the CloudWatch metrics.
    Back in the console page for the compiled endpoint, click on **View invocation
    metrics** in the **Monitor** section, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The Monitor section of the endpoint console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – The Monitor section of the endpoint console
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll now see the CloudWatch metrics console, as seen in the following screenshot.
    Here, choose the **ModelLatency** and **OverheadLatency** metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – CloudWatch metrics console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_10_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – CloudWatch metrics console
  prefs: []
  type: TYPE_NORMAL
- en: The model latency in my simple tests showed 10 milliseconds for a regular XGBoost
    endpoint and went down to 9 milliseconds after compiling with Neo. The impact
    of a compiled model will be much more significant if you are using a deep learning
    model on a lower-powered device.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at several ways to improve inference performance
    and reduce inference cost. These methods include using batch inference where possible,
    deploying several models behind a single inference endpoint to reduce costs and
    help with advanced canary or blue/green deployments, scaling inference endpoints
    to meet demand, and using Elastic Inference and SageMaker Neo to provide better
    inference performance at a lower cost.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss monitoring and other important operational
    aspects of ML.
  prefs: []
  type: TYPE_NORMAL
