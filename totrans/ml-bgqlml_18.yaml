- en: '*Chapter 14*: BigQuery ML Tips and Best Practices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQuery ML has the great advantage of democratizing the use of **Machine Learning**
    (**ML**) for data and business analysts. In fact, BigQuery ML enables users without
    any programming experience to implement advanced ML algorithms. Even though BigQuery
    ML is designed to simplify and automatize the creation of a ML model, there are
    some best practices and tips that should be adopted during the development life
    cycle of a ML algorithm to obtain an effective performance from it.
  prefs: []
  type: TYPE_NORMAL
- en: Having a background in data science can help us in further improving the performance
    of our ML models and in avoiding pitfalls during the implementation of a use case.
    In this chapter, we'll learn how to choose the right technique for each specific
    business scenario and will learn about the tools we can leverage to improve the
    performance of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a typical ML development life cycle, we''ll go through the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right BigQuery ML algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using BigQuery ML for online predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right BigQuery ML algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn why it is so important to define a clear business
    objective before implementing a ML model, and we'll understand which BigQuery
    ML algorithm is suitable for each specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A **data scientist** is a professional in charge of the collection, analysis,
    and understanding of large amounts of data. This role typically requires a mix
    of skills, such as matching statistics and coding.
  prefs: []
  type: TYPE_NORMAL
- en: A **data analyst** is different from a data scientist. A data analyst is more
    focused on industry knowledge and business processes rather than on coding and
    programming skills. People in this role have huge experience in data manipulation
    and visualization and are able to present relevant business insights derived from
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get meaningful results in ML, it is necessary to define a clear
    business objective. Before starting on the actual implementation of the ML model,
    data analysts and data scientists should clearly define the business goal they
    wish to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most famous techniques to set up clear goals is known as the **Specific,
    Measurable, Attainable, Relevant, and Time-based** (**SMART**) framework. In this
    paradigm, each letter represents a specific feature that our final goal should
    satisfy, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specific**: It is necessary to define a clear and precise business objective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurable**: In order to understand if a BigQuery ML model satisfies our
    criteria, we need to select one or more **Key Performance Indicators** (**KPIs**)
    such as the **Receiver Operating Characteristic** (**ROC**), the **Area Under
    the Curve** (**AUC**) value, or the **Mean** **Absolute Error** (**MAE**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attainable**: We need to analyze the complexity of the use case that we want
    to solve and set the right expectations—for example, we cannot expect that our
    BigQuery ML model will predict the right values 100% of the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevant**: We need to focus our efforts on the most important use cases,
    as it may be that some business scenarios can bring a limited business advantage
    to our company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-based**: As data analysts and data scientists, we have limited resources
    in terms of time. Focusing on the right goals is fundamental in order to generate
    value for our company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can apply the SMART framework to the ML field to help us in choosing the
    right use cases and the right BigQuery ML algorithm to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, in the following table, you can visualize the SMART framework
    applied to the use case that we have developed in [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),  *Predicting
    Numerical Values with Linear Regression*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – The SMART framework applied to a ML use case'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – The SMART framework applied to a ML use case
  prefs: []
  type: TYPE_NORMAL
- en: Once we have defined our business objective using the SMART approach, we are
    ready to select the best BigQuery ML algorithms that can help us in addressing
    the business scenario.
  prefs: []
  type: TYPE_NORMAL
- en: According to the business objective that we want to achieve and the training
    dataset that we can leverage, we can identify the BigQuery ML algorithms that
    can potentially solve our use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we can see a summary of all the BigQuery ML techniques
    that we can use to develop our ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – The BigQuery ML algorithms at a glance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – The BigQuery ML algorithms at a glance
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to achieve our goal, we can navigate to the table represented in *Figure
    14.2* and clearly find the BigQuery ML algorithm that can solve our use case.
    For example, for the business scenario that we''ve analyzed in *Figure 14.1*,
    we can assert the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery Public dataset that we used in [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),
    *Predicting Numerical Values with Linear Regression*, is a labeled dataset because,
    for each record, it includes the trip duration of the past rides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected outcome is a continuous real number. In fact, the goal of the use
    case is to predict the trip duration, expressed in minutes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By using the table presented in *Figure 14.2*, we notice that we can use one
    of the following BigQuery ML algorithms to address our prediction use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosted Tree – XGBoost – Regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Neural Network – Regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned how we can set a clear business objective. Then,
    we understood which BigQuery ML technique we can use to address our business goal.
    In the next section, we'll focus on preparing the datasets to get effective ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn about which techniques we can apply to ensure that
    the data we will use to build our ML model is correct and produces the desired
    results. After that, we'll discover the strategies that we can use to segment
    the datasets into training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Working with high-quality data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll understand the characteristics that our datasets should
    have in order to develop effective BigQuery ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Since ML models learn from data, it's very important to feed our ML algorithms
    with high-quality data, especially during the training phase. Since **data quality**
    is a very broad topic, it would require a specific book to analyze it in detail.
    For this reason, we will focus only on main data quality concepts in relation
    to the building of a ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality** is a discipline that includes processes, professionals, technologies,
    and best practices to identify and correct anomalies, errors, and defects in data.'
  prefs: []
  type: TYPE_NORMAL
- en: This practice is fundamental in supporting business decisions with trusted and
    affordable data insights.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the quality of a dataset according to different data quality
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see the different dimensions used to measure
    the quality of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – The data quality dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – The data quality dimensions
  prefs: []
  type: TYPE_NORMAL
- en: 'We can evaluate the quality of a dataset based on the following dimensions:
    **Accuracy**, **Completeness**, **Consistency**, **Timeliness**, **Validity**,
    and **Uniqueness**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we'll explain each data quality dimension and why
    it is important for the realization of a ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Accuracy** refers to the information that is available in our dataset. If
    a numerical value is wrong and doesn''t reflect reality, this will affect the
    effectiveness of the BigQuery ML model built on top of it.'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering information that is inaccurate is not an easy task, but we can apply
    some data quality checks to identify relevant issues in the data—for example,
    we can execute queries to identify and eventually remove records that contain
    incorrect values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see a typical example of inaccurate data, with
    the presence of a negative value to express the age of a person:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – An example of a table with inaccurate values in the Age column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – An example of a table with inaccurate values in the Age column
  prefs: []
  type: TYPE_NORMAL
- en: Using `MAX` and `MIN` operators in `SELECT` queries can be a good way to find
    wrong values in the columns. These records are called outliers because they present
    very different values from the other records in the same column. Executing some
    preliminary **Structured Query Language** (**SQL**) queries to extract the maximum
    and the minimum values of the features and the label can be very useful in helping
    identify the most relevant errors in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in *Figure 14.4*, Alice's age will be identified as an outlier
    in the **Age** column. In these cases, we can think about filtering out records
    with non-realistic values.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`NULL` fields, these fields will affect the performance of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see an example of incomplete data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – An example of a table with incomplete values in the Age column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – An example of a table with incomplete values in the Age column
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent the presence of incomplete records, the most effective solution
    is to apply specific filters in the query to exclude records with missing values.
    A typical completeness check is based on adding a `WHERE` clause when the feature
    or the label of the model should not be empty and should be different from `NULL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet of code, you can see an example of a `SELECT` statement
    with completeness checks applied to the `<TEXT_FIELD>` placeholder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we're applying completeness checks on the table represented
    by the `<TABLE_NAME>` placeholder. The quality check verifies that the `<TEXT_FIELD>`
    field is not equal to `NULL` and is not empty.
  prefs: []
  type: TYPE_NORMAL
- en: If a record presents incomplete fields, we can choose to exclude this record
    from the dataset or to replace missing values with default ones to fill the gaps.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Achieving **consistency** in data is one of the most complex tasks to perform.
    In enterprise contexts, the same data is usually stored in multiple locations
    and may be expressed with different formats or units of measurement. When a column
    presents a value incompatible with the values in other columns of a dataset, the
    data is inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can imagine a table with a column that contains a temperature
    expressed in °C and another column with the same temperature in °F. If the two
    values should represent the same temperature but are not well calculated, the
    table will present an inconsistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see that the second record presents an inconsistency
    in the temperature values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – An example of a table with inconsistent values in the Temperature
    columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – An example of a table with inconsistent values in the Temperature
    columns
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.6*, the second record—which corresponds to the temperature measured
    in **Paris**—presents an inconsistency between the °C and the °F scale.
  prefs: []
  type: TYPE_NORMAL
- en: To check the consistency of the data, we should usually apply validation checks
    on multiple fields that can reside in the same table or in different tables.
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Timeliness** is particularly important when we need to use a BigQuery ML
    model. When we train a ML model, we need to be sure that all the features used
    during the training stage will be available when the ML model is executed.'
  prefs: []
  type: TYPE_NORMAL
- en: In the business scenario of [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),
    *Predicting Numerical Values with Linear Regression*, we used the start and stop
    stations of a bike rental company to predict the trip duration. In this case,
    we've trained the BigQuery ML model leveraging the start and stop station, but
    if the stop station is not available at the prediction time, the ML model becomes
    inapplicable and worthless.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this common error, we need to check that all the features used to train
    the model will also be available during the prediction phase when the ML model
    will generate the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Validity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **validity** of a value is strictly related to the expected format that
    a field should have. If a column contains date values expressed in the format
    *DD/MM/YYYY* and one of the records presents the format *DD-MM-YYYY*, the validity
    of the record is compromised.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you will notice that the second record presents a non-valid
    value for the **Birth Date** field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – An example of a table with invalid values in the Birth Date
    column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.7 – An example of a table with invalid values in the Birth Date column
  prefs: []
  type: TYPE_NORMAL
- en: To meet this dimension, we must check that all the values of a column are stored
    in the same format and in a homogeneous way. In order to check the exact format
    of each field, we can apply regular expressions to the values in the columns.
  prefs: []
  type: TYPE_NORMAL
- en: Uniqueness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Unique** information means that there is exactly one record in a table to
    represent a specific item. Data duplication can happen due to several reasons,
    such as bugs in the data ingestion process, or uncontrolled interruptions and
    restarts during data loading.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent these errors, we need to know the fields that compose the
    **Primary Key** (**PK**) of our records, and we need to check that the **PK**
    matches one—and only one—record.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In a table, the **PK** is the minimum set of columns that uniquely identify
    a row.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of duplicated records can lead the ML model to learn from occurrences
    that are not actual but generated by technical errors.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discovered all the data quality dimensions to check before implementing
    our BigQuery ML model, let's take a look the techniques we can use to segment
    the datasets into training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting the datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll learn how to easily segment the datasets to support the
    training, validation, and test phases of a BigQuery ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most BigQuery ML algorithms, we need to split the initial dataset into
    three different sets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **training** dataset represents the sample of data that we'll use to train
    our BigQuery ML model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **validation** dataset is different from the training dataset and we can
    use it to evaluate the model's performance. We perform validation on completely
    new data that is different from the sample data used in the training stage. In
    this phase, we can also tune the hyperparameters of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **test** set is the dataset used to finally apply the model and verify its
    results and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Splitting the initial dataset into these three subsets can be cumbersome, but
    if you have enough data, you can apply the following rule of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: 80% of the data for the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10% for the validation set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining 10% for the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we're working on large amounts of data, we can reduce the percentage of observations
    used for the validation and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see a graphical representation of the
    optimal splitting strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – The 80/10/10 splitting strategy for ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – The 80/10/10 splitting strategy for ML
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve the best results, the split procedure should be as random
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, you can see how to apply this rule of thumb by
    using the `MOD` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the example, the records stored in the table represented by the placeholder
    `<TABLE_NAME>`, are split into three different sets according to the value of
    the field dataframe. In this case, the MOD function returns a value from 0 to
    10\. Using this function allows us to group the records into three different sets.
    By leveraging the clauses `MOD(<RECORD_KEY>, 10)<8, MOD(<RECORD_KEY>, 10) = 8
    and MOD(<RECORD_KEY>, 10) = 9`, we can split the records into the `'training',
    'evaluation'` and `'test'` sets.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've understood how to segment the datasets according to our needs
    for training, validation, and testing, let's look at the understanding of the
    feature engineering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll understand which techniques we can use to improve the
    features of a BigQuery ML model before the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering** is the practice of applying preprocessing functions
    on raw data, to extract features useful for training a ML model. Creating preprocessed
    features can significantly improve the performance of a ML model.'
  prefs: []
  type: TYPE_NORMAL
- en: By design, BigQuery ML automatically applies feature engineering during the
    training phase when we use the `CREATE MODEL` function, but it also allows us
    to apply preprocessing transformations as well.
  prefs: []
  type: TYPE_NORMAL
- en: In order to automatically apply the feature engineering operations during the
    training and the prediction stage, we can include all the pre-processing functions
    into the `TRANSFORM` clause when we train the BigQuery ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the following code example, we need to use the `TRANSFORM`
    clause before the `OPTIONS` clause, and after the `CREATE MODEL` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In BigQuery ML, there are two different types of feature engineering function,
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalar** functions apply on a single record'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytic** functions calculate the results on all the rows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following list, you can take a look at the most important feature engineering
    functions that you can apply in BigQuery ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ML.BUCKETIZE`: This is used to convert a numerical expression into a categorical
    field—for example, you can use this function to convert the age of a person into
    *Young*, *Middle*, or *Old* buckets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.FEATURE_CROSS`: This is used to combine two features into a unique feature.
    For example, if in a dataset we have the gender and the place of birth of a person,
    we can combine these two features to simplify our BigQuery ML model. This technique
    is particularly indicated when we''ve correlated features and we want to include
    both the information in our ML model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.QUANTILE_BUCKETIZE`: This is very similar to the `ML.BUCKETIZE` function.
    In this case, the function is analytic and applies on all records in the dataset.
    The splitting of records into buckets is based on the quantiles of an entire set
    of records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **Quantile** is the specific portion of a dataset. For example, it identifies
    how many values are above or below a certain threshold value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`ML.MIN_MAX_SCALER`: This is an analytic function that returns a value from
    zero to one according to the distribution of the values in the entire record set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.STANDARD_SCALER`: This is an analytic function that allows us to use the
    standard deviation and mean of the record set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an entire list of feature engineering and preprocessing functions, you
    can visit the official BigQuery documentation at the following link: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about feature engineering, let's move on to hyperparameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discover the most important hyperparameters that we can
    tune in BigQuery ML.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning** is the practice of choosing the best set of parameters
    to train a specific ML model. A hyperparameter influences and controls the learning
    process during the ML training stage.'
  prefs: []
  type: TYPE_NORMAL
- en: By design, BigQuery ML uses default hyperparameters to train a model, but advanced
    users can manually change them to influence the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In BigQuery ML, we can specify the hyperparameters in the `OPTIONS` clause
    as optional parameters. The most relevant hyperparameters, depending on the model,
    that we can change before starting the training of a BigQuery ML model are listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`L1_REG`: This is a regularization parameter that we can use to prevent overfitting
    by keeping the weights of the model close to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L2_REG`: This is a second regularization parameter that we can use to prevent
    overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_ITERATIONS`: This represents the maximum number of iterations that BigQuery
    ML will perform to train the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE`: This is a parameter that affects how much the model changes according
    to the error of the previous iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_REL_PROGRESS`: This is the minimum improvement that is necessary to continue
    the training after an iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_CLUSTERS`: This is used for K-Means algorithms and represents the number
    of clusters that the model will create.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HIDDEN_UNITS`: This is used in **Deep Neural Networks** (**DNNs**) and represents
    the number of hidden layers in a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of all the hyperparameters that you can apply with BigQuery
    ML, we suggest visiting the official documentation at [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create).
  prefs: []
  type: TYPE_NORMAL
- en: Using BigQuery ML for online predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll understand how we can use a BigQuery ML model in a synchronous
    and online manner.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery ML represents a huge opportunity to democratize ML techniques for business
    and data analysts. When BigQuery ML is trained and ready to use, we can invoke
    it directly in BigQuery using a SQL query or we can export it into TensorFlow
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirements of each use case drive the prediction type that we should
    adopt, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: We use **online prediction** when we want to enable request-response applications
    and when getting an immediate prediction is critical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We adopt **batch prediction** to process large volumes of data when we don't
    need immediate predictions—for example, scheduling daily or weekly jobs that calculate
    predictions on the data collected since the last job execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While using BigQuery SQL statements is more suitable for batch predictions on
    a large number of records stored in a BigQuery table, the possibility of exporting
    BigQuery ML models into TensorFlow opens new opportunities in terms of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see the life cycle of a BigQuery ML model
    from the training phase to the deployment phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – Using a BigQuery ML model for online predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – Using a BigQuery ML model for online predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding diagram, we see that in the first step **(1)** of the BigQuery
    ML model life cycle we train the ML model by leveraging the training dataset stored
    in BigQuery. In this first step, the ML model goes through the three main stages
    of the development cycle, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train**: In this stage, the BigQuery ML model learns from the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate**: In this step, we evaluate the KPIs of the model and we can tune
    the hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test**: In this last phase, we finally test the BigQuery ML model on the
    test dataset to get the predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the BigQuery ML model satisfies our expectations in terms of performance,
    we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the model in BigQuery **(2a)** through SQL by leveraging data that is already
    stored in tables. This approach typically works well for batch predictions—for
    example, we can periodically run the BigQuery SQL statement to execute the model
    every day or every week by generating the new predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export the model into the TensorFlow `SavedFormat` **(2b)**, as we described
    in [*Chapter 13*](B16722_13_Final_ASB_ePub.xhtml#_idTextAnchor184), *Running TensorFlow
    Models with BigQuery ML*. This second approach is particularly suitable for running
    the ML model outside of BigQuery on other TensorFlow-compatible platforms. The
    same approach can be adopted by exporting the ML model into the XGBoost Booster
    format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we''ve exported the BigQuery ML model, we can deploy the ML algorithm
    to one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Cloud AI Platform Prediction** **(3a)**: With this Google Cloud module,
    we can deploy trained ML models in the cloud and leverage the cloud infrastructure
    to serve the model and generate online predictions. This cloud service automatically
    provisions and manages the infrastructure resources to run the ML model and can
    scale up according to the number of requests that come from client applications.
    The deployment on Google Cloud AI Platform Prediction automatically generates
    a **Representational State Transfer** (**REST**) endpoint that can be used to
    invoke the model through **HyperText Transfer Protocol** (**HTTP**) requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of approach is particularly useful when we have multiple client applications
    that should interact with our ML model and we don't want to have to worry about
    the infrastructure maintenance of the service. As a prerequisite, we need to consider
    that we can only use this kind of approach if an internet connection is available.
    In fact, to invoke the REST **Application Programming Interface** (**API**) exposed
    by the cloud platform, we need to perform HTTP requests from the client applications
    to the cloud.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An **on-premise machine** where it can be deployed using containers **(3b)**.
    TensorFlow models, in fact, can be deployed using containers by leveraging a **TensorFlow
    Serving Docker** container. To understand the deployment steps of a TensorFlow
    model in a **Docker** container, you can check out the documentation at [https://www.tensorflow.org/tfx/serving/docker](https://www.tensorflow.org/tfx/serving/docker).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **container** is a virtualization mechanism that runs on top of a single operating
    system. **Docker** is a container engine used to deploy containerized applications.
    The container engine allocates the hardware resources for each application and
    manages the scalability of the virtual infrastructure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can use this approach when the ML model needs to run under specific conditions,
    such as during an absence of internet connectivity, or on sensitive data. In fact,
    when we deploy the on-premise machine ML model, the cloud infrastructure is no
    longer involved in the predictions and there is no data transfer between the on-premise
    environment and the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In both deployment scenarios, the ML model can be invoked by using HTTP requests,
    passing the input parameters in the request payload. At the end of the ML model
    execution, the predictions are returned into the response payload.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned the most important tips and best practices that
    we can apply during the implementation of a ML use case with BigQuery ML.
  prefs: []
  type: TYPE_NORMAL
- en: We've analyzed the importance of data preparation; we started looking at the
    data quality aspects; then, we've learned how we can easily split the data to
    get balanced training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how we can further improve a ML model's performance using
    BigQuery ML functions for feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we focused our attention on tuning hyperparameters. When we train
    a model, BigQuery ML allows us to choose different parameters, and these variables
    influence the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have understood why it is so important to deploy BigQuery ML models
    on other platforms so that we get online predictions and satisfy near-real-time
    business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on finishing reading the book! You should now be able to use
    BigQuery ML for your business scenarios and use cases. I suggest you continue
    to keep constantly informed about this topic, which is so interesting and is evolving
    so quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Further resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BigQuery ML Create Model**: [https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery ML preprocessing functions**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE MODEL` **statement for importing TensorFlow models**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML EVALUATE` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML PREDICT` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exporting a BigQuery ML model for online prediction**: [https://cloud.google.com/bigquery-ml/docs/export-model-tutorial](https://cloud.google.com/bigquery-ml/docs/export-model-tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
