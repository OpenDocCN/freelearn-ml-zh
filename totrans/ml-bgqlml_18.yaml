- en: '*Chapter 14*: BigQuery ML Tips and Best Practices'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：BigQuery ML 技巧和最佳实践'
- en: BigQuery ML has the great advantage of democratizing the use of **Machine Learning**
    (**ML**) for data and business analysts. In fact, BigQuery ML enables users without
    any programming experience to implement advanced ML algorithms. Even though BigQuery
    ML is designed to simplify and automatize the creation of a ML model, there are
    some best practices and tips that should be adopted during the development life
    cycle of a ML algorithm to obtain an effective performance from it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery ML 具有民主化数据分析师和业务分析师使用 **机器学习（ML**）的优势。实际上，BigQuery ML 允许没有任何编程经验的用户实施高级
    ML 算法。尽管 BigQuery ML 设计用于简化并自动化 ML 模型的创建，但在 ML 算法的开发生命周期中，仍有一些最佳实践和技巧应该被采用，以获得有效的性能。
- en: Having a background in data science can help us in further improving the performance
    of our ML models and in avoiding pitfalls during the implementation of a use case.
    In this chapter, we'll learn how to choose the right technique for each specific
    business scenario and will learn about the tools we can leverage to improve the
    performance of ML models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数据科学背景可以帮助我们在进一步提高 ML 模型的性能以及避免在用例实施过程中的陷阱。在本章中，我们将学习如何为每个特定的业务场景选择正确的技术，并了解我们可以利用的工具来提高
    ML 模型的性能。
- en: 'Following a typical ML development life cycle, we''ll go through the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 按照典型的 ML 开发生命周期，我们将讨论以下主题：
- en: Choosing the right BigQuery ML algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的 BigQuery ML 算法
- en: Preparing the datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集
- en: Understanding feature engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: Tuning hyperparameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Using BigQuery ML for online predictions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BigQuery ML 进行在线预测
- en: Choosing the right BigQuery ML algorithm
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的 BigQuery ML 算法
- en: In this section, we'll learn why it is so important to define a clear business
    objective before implementing a ML model, and we'll understand which BigQuery
    ML algorithm is suitable for each specific use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习在实施 ML 模型之前定义一个明确的企业目标为什么如此重要，并了解哪些 BigQuery ML 算法适用于每个特定的用例。
- en: Important note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A **data scientist** is a professional in charge of the collection, analysis,
    and understanding of large amounts of data. This role typically requires a mix
    of skills, such as matching statistics and coding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据科学家**是负责收集、分析和理解大量数据的专业人士。这个角色通常需要统计和编码技能的混合。'
- en: A **data analyst** is different from a data scientist. A data analyst is more
    focused on industry knowledge and business processes rather than on coding and
    programming skills. People in this role have huge experience in data manipulation
    and visualization and are able to present relevant business insights derived from
    data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析师**与数据科学家不同。数据分析师更专注于行业知识和业务流程，而不是编码和编程技能。这个角色的人拥有大量数据操作和可视化的经验，能够从数据中提出相关的业务洞察。'
- en: In order to get meaningful results in ML, it is necessary to define a clear
    business objective. Before starting on the actual implementation of the ML model,
    data analysts and data scientists should clearly define the business goal they
    wish to achieve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在机器学习（ML）中获得有意义的成果，有必要定义一个明确的企业目标。在开始实际实施 ML 模型之前，数据分析师和数据科学家应该清楚地定义他们希望实现的企业目标。
- en: 'One of the most famous techniques to set up clear goals is known as the **Specific,
    Measurable, Attainable, Relevant, and Time-based** (**SMART**) framework. In this
    paradigm, each letter represents a specific feature that our final goal should
    satisfy, as outlined here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一种设置明确目标最著名的技巧被称为 **具体、可衡量、可实现、相关和基于时间（SMART**）框架。在这个范式下，每个字母代表我们的最终目标应该满足的特定特征，如下所述：
- en: '**Specific**: It is necessary to define a clear and precise business objective.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体性**：有必要定义一个明确且精确的企业目标。'
- en: '**Measurable**: In order to understand if a BigQuery ML model satisfies our
    criteria, we need to select one or more **Key Performance Indicators** (**KPIs**)
    such as the **Receiver Operating Characteristic** (**ROC**), the **Area Under
    the Curve** (**AUC**) value, or the **Mean** **Absolute Error** (**MAE**).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可衡量性**：为了理解 BigQuery ML 模型是否满足我们的标准，我们需要选择一个或多个 **关键绩效指标（KPI**），例如 **接收者操作特征（ROC**），**曲线下面积（AUC**）值，或
    **平均绝对误差（MAE**）。'
- en: '**Attainable**: We need to analyze the complexity of the use case that we want
    to solve and set the right expectations—for example, we cannot expect that our
    BigQuery ML model will predict the right values 100% of the time.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevant**: We need to focus our efforts on the most important use cases,
    as it may be that some business scenarios can bring a limited business advantage
    to our company.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-based**: As data analysts and data scientists, we have limited resources
    in terms of time. Focusing on the right goals is fundamental in order to generate
    value for our company.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can apply the SMART framework to the ML field to help us in choosing the
    right use cases and the right BigQuery ML algorithm to use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, in the following table, you can visualize the SMART framework
    applied to the use case that we have developed in [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),  *Predicting
    Numerical Values with Linear Regression*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – The SMART framework applied to a ML use case'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – The SMART framework applied to a ML use case
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Once we have defined our business objective using the SMART approach, we are
    ready to select the best BigQuery ML algorithms that can help us in addressing
    the business scenario.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: According to the business objective that we want to achieve and the training
    dataset that we can leverage, we can identify the BigQuery ML algorithms that
    can potentially solve our use case.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we can see a summary of all the BigQuery ML techniques
    that we can use to develop our ML models:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – The BigQuery ML algorithms at a glance'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – The BigQuery ML algorithms at a glance
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to achieve our goal, we can navigate to the table represented in *Figure
    14.2* and clearly find the BigQuery ML algorithm that can solve our use case.
    For example, for the business scenario that we''ve analyzed in *Figure 14.1*,
    we can assert the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery Public dataset that we used in [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),
    *Predicting Numerical Values with Linear Regression*, is a labeled dataset because,
    for each record, it includes the trip duration of the past rides.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected outcome is a continuous real number. In fact, the goal of the use
    case is to predict the trip duration, expressed in minutes.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By using the table presented in *Figure 14.2*, we notice that we can use one
    of the following BigQuery ML algorithms to address our prediction use case:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosted Tree – XGBoost – Regression**'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Neural Network – Regression**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned how we can set a clear business objective. Then,
    we understood which BigQuery ML technique we can use to address our business goal.
    In the next section, we'll focus on preparing the datasets to get effective ML
    models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the datasets
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn about which techniques we can apply to ensure that
    the data we will use to build our ML model is correct and produces the desired
    results. After that, we'll discover the strategies that we can use to segment
    the datasets into training, validation, and test sets.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Working with high-quality data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll understand the characteristics that our datasets should
    have in order to develop effective BigQuery ML models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Since ML models learn from data, it's very important to feed our ML algorithms
    with high-quality data, especially during the training phase. Since **data quality**
    is a very broad topic, it would require a specific book to analyze it in detail.
    For this reason, we will focus only on main data quality concepts in relation
    to the building of a ML model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality** is a discipline that includes processes, professionals, technologies,
    and best practices to identify and correct anomalies, errors, and defects in data.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: This practice is fundamental in supporting business decisions with trusted and
    affordable data insights.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the quality of a dataset according to different data quality
    dimensions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see the different dimensions used to measure
    the quality of the data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – The data quality dimensions'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_003.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – The data quality dimensions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'We can evaluate the quality of a dataset based on the following dimensions:
    **Accuracy**, **Completeness**, **Consistency**, **Timeliness**, **Validity**,
    and **Uniqueness**.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we'll explain each data quality dimension and why
    it is important for the realization of a ML model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Accuracy** refers to the information that is available in our dataset. If
    a numerical value is wrong and doesn''t reflect reality, this will affect the
    effectiveness of the BigQuery ML model built on top of it.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Discovering information that is inaccurate is not an easy task, but we can apply
    some data quality checks to identify relevant issues in the data—for example,
    we can execute queries to identify and eventually remove records that contain
    incorrect values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see a typical example of inaccurate data, with
    the presence of a negative value to express the age of a person:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – An example of a table with inaccurate values in the Age column'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_004.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – An example of a table with inaccurate values in the Age column
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Using `MAX` and `MIN` operators in `SELECT` queries can be a good way to find
    wrong values in the columns. These records are called outliers because they present
    very different values from the other records in the same column. Executing some
    preliminary **Structured Query Language** (**SQL**) queries to extract the maximum
    and the minimum values of the features and the label can be very useful in helping
    identify the most relevant errors in the dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For example, in *Figure 14.4*, Alice's age will be identified as an outlier
    in the **Age** column. In these cases, we can think about filtering out records
    with non-realistic values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`NULL` fields, these fields will affect the performance of our model.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see an example of incomplete data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – An example of a table with incomplete values in the Age column'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_005.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – An example of a table with incomplete values in the Age column
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent the presence of incomplete records, the most effective solution
    is to apply specific filters in the query to exclude records with missing values.
    A typical completeness check is based on adding a `WHERE` clause when the feature
    or the label of the model should not be empty and should be different from `NULL`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet of code, you can see an example of a `SELECT` statement
    with completeness checks applied to the `<TEXT_FIELD>` placeholder:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding example, we're applying completeness checks on the table represented
    by the `<TABLE_NAME>` placeholder. The quality check verifies that the `<TEXT_FIELD>`
    field is not equal to `NULL` and is not empty.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: If a record presents incomplete fields, we can choose to exclude this record
    from the dataset or to replace missing values with default ones to fill the gaps.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Achieving **consistency** in data is one of the most complex tasks to perform.
    In enterprise contexts, the same data is usually stored in multiple locations
    and may be expressed with different formats or units of measurement. When a column
    presents a value incompatible with the values in other columns of a dataset, the
    data is inconsistent.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can imagine a table with a column that contains a temperature
    expressed in °C and another column with the same temperature in °F. If the two
    values should represent the same temperature but are not well calculated, the
    table will present an inconsistency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you can see that the second record presents an inconsistency
    in the temperature values:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – An example of a table with inconsistent values in the Temperature
    columns'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_006.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – An example of a table with inconsistent values in the Temperature
    columns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.6*, the second record—which corresponds to the temperature measured
    in **Paris**—presents an inconsistency between the °C and the °F scale.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: To check the consistency of the data, we should usually apply validation checks
    on multiple fields that can reside in the same table or in different tables.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Timeliness** is particularly important when we need to use a BigQuery ML
    model. When we train a ML model, we need to be sure that all the features used
    during the training stage will be available when the ML model is executed.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In the business scenario of [*Chapter 4*](B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061),
    *Predicting Numerical Values with Linear Regression*, we used the start and stop
    stations of a bike rental company to predict the trip duration. In this case,
    we've trained the BigQuery ML model leveraging the start and stop station, but
    if the stop station is not available at the prediction time, the ML model becomes
    inapplicable and worthless.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this common error, we need to check that all the features used to train
    the model will also be available during the prediction phase when the ML model
    will generate the predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Validity
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **validity** of a value is strictly related to the expected format that
    a field should have. If a column contains date values expressed in the format
    *DD/MM/YYYY* and one of the records presents the format *DD-MM-YYYY*, the validity
    of the record is compromised.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, you will notice that the second record presents a non-valid
    value for the **Birth Date** field:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – An example of a table with invalid values in the Birth Date
    column'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_007.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.7 – An example of a table with invalid values in the Birth Date column
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: To meet this dimension, we must check that all the values of a column are stored
    in the same format and in a homogeneous way. In order to check the exact format
    of each field, we can apply regular expressions to the values in the columns.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Uniqueness
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Unique** information means that there is exactly one record in a table to
    represent a specific item. Data duplication can happen due to several reasons,
    such as bugs in the data ingestion process, or uncontrolled interruptions and
    restarts during data loading.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent these errors, we need to know the fields that compose the
    **Primary Key** (**PK**) of our records, and we need to check that the **PK**
    matches one—and only one—record.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: In a table, the **PK** is the minimum set of columns that uniquely identify
    a row.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The presence of duplicated records can lead the ML model to learn from occurrences
    that are not actual but generated by technical errors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discovered all the data quality dimensions to check before implementing
    our BigQuery ML model, let's take a look the techniques we can use to segment
    the datasets into training, validation, and test sets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting the datasets
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll learn how to easily segment the datasets to support the
    training, validation, and test phases of a BigQuery ML model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'For most BigQuery ML algorithms, we need to split the initial dataset into
    three different sets, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The **training** dataset represents the sample of data that we'll use to train
    our BigQuery ML model.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **validation** dataset is different from the training dataset and we can
    use it to evaluate the model's performance. We perform validation on completely
    new data that is different from the sample data used in the training stage. In
    this phase, we can also tune the hyperparameters of the model.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **test** set is the dataset used to finally apply the model and verify its
    results and performance.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Splitting the initial dataset into these three subsets can be cumbersome, but
    if you have enough data, you can apply the following rule of thumb:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 80% of the data for the training set.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10% for the validation set.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining 10% for the test set.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we're working on large amounts of data, we can reduce the percentage of observations
    used for the validation and test sets.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see a graphical representation of the
    optimal splitting strategy:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – The 80/10/10 splitting strategy for ML'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_008.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – The 80/10/10 splitting strategy for ML
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve the best results, the split procedure should be as random
    as possible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, you can see how to apply this rule of thumb by
    using the `MOD` function:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the example, the records stored in the table represented by the placeholder
    `<TABLE_NAME>`, are split into three different sets according to the value of
    the field dataframe. In this case, the MOD function returns a value from 0 to
    10\. Using this function allows us to group the records into three different sets.
    By leveraging the clauses `MOD(<RECORD_KEY>, 10)<8, MOD(<RECORD_KEY>, 10) = 8
    and MOD(<RECORD_KEY>, 10) = 9`, we can split the records into the `'training',
    'evaluation'` and `'test'` sets.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've understood how to segment the datasets according to our needs
    for training, validation, and testing, let's look at the understanding of the
    feature engineering techniques.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature engineering
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll understand which techniques we can use to improve the
    features of a BigQuery ML model before the training stage.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering** is the practice of applying preprocessing functions
    on raw data, to extract features useful for training a ML model. Creating preprocessed
    features can significantly improve the performance of a ML model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: By design, BigQuery ML automatically applies feature engineering during the
    training phase when we use the `CREATE MODEL` function, but it also allows us
    to apply preprocessing transformations as well.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In order to automatically apply the feature engineering operations during the
    training and the prediction stage, we can include all the pre-processing functions
    into the `TRANSFORM` clause when we train the BigQuery ML model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the following code example, we need to use the `TRANSFORM`
    clause before the `OPTIONS` clause, and after the `CREATE MODEL` statement:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In BigQuery ML, there are two different types of feature engineering function,
    outlined as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalar** functions apply on a single record'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**函数作用于单个记录'
- en: '**Analytic** functions calculate the results on all the rows'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析性**函数在所有行上计算结果'
- en: 'In the following list, you can take a look at the most important feature engineering
    functions that you can apply in BigQuery ML:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，您可以查看在 BigQuery ML 中可以应用的最重要特征工程函数：
- en: '`ML.BUCKETIZE`: This is used to convert a numerical expression into a categorical
    field—for example, you can use this function to convert the age of a person into
    *Young*, *Middle*, or *Old* buckets.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ML.BUCKETIZE`: 这个函数用于将数值表达式转换为分类字段——例如，您可以使用此函数将一个人的年龄转换为 *Young*（年轻）、*Middle*（中年）或
    *Old*（老年）桶。'
- en: '`ML.FEATURE_CROSS`: This is used to combine two features into a unique feature.
    For example, if in a dataset we have the gender and the place of birth of a person,
    we can combine these two features to simplify our BigQuery ML model. This technique
    is particularly indicated when we''ve correlated features and we want to include
    both the information in our ML model.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ML.FEATURE_CROSS`: 这个函数用于将两个特征组合成一个独特的特征。例如，如果在数据集中我们有一个人的性别和出生地，我们可以将这些两个特征组合起来简化我们的
    BigQuery ML 模型。当我们在相关特征中并且想要在我们的机器学习模型中包含这两个信息时，这种技术特别适用。'
- en: '`ML.QUANTILE_BUCKETIZE`: This is very similar to the `ML.BUCKETIZE` function.
    In this case, the function is analytic and applies on all records in the dataset.
    The splitting of records into buckets is based on the quantiles of an entire set
    of records.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ML.QUANTILE_BUCKETIZE`: 这个函数与 `ML.BUCKETIZE` 函数非常相似。在这种情况下，该函数是分析性的，并应用于数据集中的所有记录。记录分割成桶是基于整个记录集的百分位数。'
- en: Important note
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The **Quantile** is the specific portion of a dataset. For example, it identifies
    how many values are above or below a certain threshold value.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**分位数**是数据集的特定部分。例如，它标识有多少值高于或低于某个阈值。'
- en: '`ML.MIN_MAX_SCALER`: This is an analytic function that returns a value from
    zero to one according to the distribution of the values in the entire record set.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ML.MIN_MAX_SCALER`: 这是一个分析函数，根据整个记录集中的值分布返回一个从零到一的值。'
- en: '`ML.STANDARD_SCALER`: This is an analytic function that allows us to use the
    standard deviation and mean of the record set.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ML.STANDARD_SCALER`: 这是一个分析函数，允许我们使用记录集的标准差和平均值。'
- en: 'For an entire list of feature engineering and preprocessing functions, you
    can visit the official BigQuery documentation at the following link: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征工程和预处理的完整函数列表，您可以访问以下链接的官方 BigQuery 文档：[https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions)。
- en: Now that we have learned about feature engineering, let's move on to hyperparameter
    tuning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了特征工程，让我们继续学习超参数调整。
- en: Tuning hyperparameters
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: In this section, we'll discover the most important hyperparameters that we can
    tune in BigQuery ML.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨在 BigQuery ML 中可以调整的最重要超参数。
- en: Important note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Hyperparameter tuning** is the practice of choosing the best set of parameters
    to train a specific ML model. A hyperparameter influences and controls the learning
    process during the ML training stage.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调整**是选择最佳参数集以训练特定机器学习模型的实践。超参数在机器学习训练阶段影响并控制学习过程。'
- en: By design, BigQuery ML uses default hyperparameters to train a model, but advanced
    users can manually change them to influence the training process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 按设计，BigQuery ML 使用默认超参数来训练模型，但高级用户可以手动更改它们以影响训练过程。
- en: 'In BigQuery ML, we can specify the hyperparameters in the `OPTIONS` clause
    as optional parameters. The most relevant hyperparameters, depending on the model,
    that we can change before starting the training of a BigQuery ML model are listed
    here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BigQuery ML 中，我们可以在 `OPTIONS` 子句中指定超参数作为可选参数。在开始训练 BigQuery ML 模型之前，我们可以更改的最相关的超参数（取决于模型）如下所示：
- en: '`L1_REG`: This is a regularization parameter that we can use to prevent overfitting
    by keeping the weights of the model close to zero.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L1_REG`: 这是一个正则化参数，我们可以使用它通过保持模型权重接近零来防止过拟合。'
- en: '`L2_REG`: This is a second regularization parameter that we can use to prevent
    overfitting.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L2_REG`: 这是一个第二正则化参数，我们可以使用它来防止过拟合。'
- en: '`MAX_ITERATIONS`: This represents the maximum number of iterations that BigQuery
    ML will perform to train the model.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE`: This is a parameter that affects how much the model changes according
    to the error of the previous iteration.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_REL_PROGRESS`: This is the minimum improvement that is necessary to continue
    the training after an iteration.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_CLUSTERS`: This is used for K-Means algorithms and represents the number
    of clusters that the model will create.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HIDDEN_UNITS`: This is used in **Deep Neural Networks** (**DNNs**) and represents
    the number of hidden layers in a network.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of all the hyperparameters that you can apply with BigQuery
    ML, we suggest visiting the official documentation at [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Using BigQuery ML for online predictions
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll understand how we can use a BigQuery ML model in a synchronous
    and online manner.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery ML represents a huge opportunity to democratize ML techniques for business
    and data analysts. When BigQuery ML is trained and ready to use, we can invoke
    it directly in BigQuery using a SQL query or we can export it into TensorFlow
    format.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirements of each use case drive the prediction type that we should
    adopt, as outlined here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We use **online prediction** when we want to enable request-response applications
    and when getting an immediate prediction is critical.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We adopt **batch prediction** to process large volumes of data when we don't
    need immediate predictions—for example, scheduling daily or weekly jobs that calculate
    predictions on the data collected since the last job execution.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While using BigQuery SQL statements is more suitable for batch predictions on
    a large number of records stored in a BigQuery table, the possibility of exporting
    BigQuery ML models into TensorFlow opens new opportunities in terms of applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see the life cycle of a BigQuery ML model
    from the training phase to the deployment phase:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – Using a BigQuery ML model for online predictions'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_14_009.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – Using a BigQuery ML model for online predictions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding diagram, we see that in the first step **(1)** of the BigQuery
    ML model life cycle we train the ML model by leveraging the training dataset stored
    in BigQuery. In this first step, the ML model goes through the three main stages
    of the development cycle, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '**Train**: In this stage, the BigQuery ML model learns from the training data.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate**: In this step, we evaluate the KPIs of the model and we can tune
    the hyperparameters.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test**: In this last phase, we finally test the BigQuery ML model on the
    test dataset to get the predictions.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the BigQuery ML model satisfies our expectations in terms of performance,
    we can do the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Use the model in BigQuery **(2a)** through SQL by leveraging data that is already
    stored in tables. This approach typically works well for batch predictions—for
    example, we can periodically run the BigQuery SQL statement to execute the model
    every day or every week by generating the new predictions.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export the model into the TensorFlow `SavedFormat` **(2b)**, as we described
    in [*Chapter 13*](B16722_13_Final_ASB_ePub.xhtml#_idTextAnchor184), *Running TensorFlow
    Models with BigQuery ML*. This second approach is particularly suitable for running
    the ML model outside of BigQuery on other TensorFlow-compatible platforms. The
    same approach can be adopted by exporting the ML model into the XGBoost Booster
    format.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we''ve exported the BigQuery ML model, we can deploy the ML algorithm
    to one of the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Cloud AI Platform Prediction** **(3a)**: With this Google Cloud module,
    we can deploy trained ML models in the cloud and leverage the cloud infrastructure
    to serve the model and generate online predictions. This cloud service automatically
    provisions and manages the infrastructure resources to run the ML model and can
    scale up according to the number of requests that come from client applications.
    The deployment on Google Cloud AI Platform Prediction automatically generates
    a **Representational State Transfer** (**REST**) endpoint that can be used to
    invoke the model through **HyperText Transfer Protocol** (**HTTP**) requests.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of approach is particularly useful when we have multiple client applications
    that should interact with our ML model and we don't want to have to worry about
    the infrastructure maintenance of the service. As a prerequisite, we need to consider
    that we can only use this kind of approach if an internet connection is available.
    In fact, to invoke the REST **Application Programming Interface** (**API**) exposed
    by the cloud platform, we need to perform HTTP requests from the client applications
    to the cloud.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An **on-premise machine** where it can be deployed using containers **(3b)**.
    TensorFlow models, in fact, can be deployed using containers by leveraging a **TensorFlow
    Serving Docker** container. To understand the deployment steps of a TensorFlow
    model in a **Docker** container, you can check out the documentation at [https://www.tensorflow.org/tfx/serving/docker](https://www.tensorflow.org/tfx/serving/docker).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **container** is a virtualization mechanism that runs on top of a single operating
    system. **Docker** is a container engine used to deploy containerized applications.
    The container engine allocates the hardware resources for each application and
    manages the scalability of the virtual infrastructure.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can use this approach when the ML model needs to run under specific conditions,
    such as during an absence of internet connectivity, or on sensitive data. In fact,
    when we deploy the on-premise machine ML model, the cloud infrastructure is no
    longer involved in the predictions and there is no data transfer between the on-premise
    environment and the cloud.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In both deployment scenarios, the ML model can be invoked by using HTTP requests,
    passing the input parameters in the request payload. At the end of the ML model
    execution, the predictions are returned into the response payload.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned the most important tips and best practices that
    we can apply during the implementation of a ML use case with BigQuery ML.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We've analyzed the importance of data preparation; we started looking at the
    data quality aspects; then, we've learned how we can easily split the data to
    get balanced training, validation, and test sets.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how we can further improve a ML model's performance using
    BigQuery ML functions for feature engineering.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: After that, we focused our attention on tuning hyperparameters. When we train
    a model, BigQuery ML allows us to choose different parameters, and these variables
    influence the training stage.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have understood why it is so important to deploy BigQuery ML models
    on other platforms so that we get online predictions and satisfy near-real-time
    business scenarios.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on finishing reading the book! You should now be able to use
    BigQuery ML for your business scenarios and use cases. I suggest you continue
    to keep constantly informed about this topic, which is so interesting and is evolving
    so quickly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Further resources
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BigQuery ML Create Model**: [https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery ML preprocessing functions**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE MODEL` **statement for importing TensorFlow models**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML EVALUATE` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML PREDICT` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exporting a BigQuery ML model for online prediction**: [https://cloud.google.com/bigquery-ml/docs/export-model-tutorial](https://cloud.google.com/bigquery-ml/docs/export-model-tutorial)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
