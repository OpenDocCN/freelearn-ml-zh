<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer042">
<h1 class="chapter-number" id="_idParaDest-40"><a id="_idTextAnchor061"/>2</h1>
<h1 id="_idParaDest-41"><a id="_idTextAnchor062"/>Building and Using a Dataset</h1>
<p>The data collection and curation process is one of the most important stages in model building. It is also one of the most time-consuming. Typically, data can come from many sources; for example, customer records, transaction data, or stock lists. Nowadays, with the timely conjunction of big data, fast, high-capacity SSDs (to store big data), and GPUs (to process big data), it is easier for individuals to collect, store, and <span class="No-Break">process data.</span></p>
<p>In this chapter, you will learn about finding and accessing pre-existing, ready-made data sources that can be used to train your model. We will also look at ways to create your own datasets, transforming datasets so that they are useful for your problem, and we will also see how non-English datasets can <span class="No-Break">be utilized.</span></p>
<p>In the remainder of this book, we will be using a selection of the datasets listed in this chapter to train and test a range of classifiers. When we do this, we will want to assess how well the classifiers work on each of the datasetsâ€”one of the major lessons of this book is that different classifiers work well with different datasets, and to see how well a classifier works with a given dataset, we will need ways of measuring performance. We will therefore end this chapter by looking at metrics for assessing the performance of a classifier on <span class="No-Break">a dataset.</span></p>
<p>In this chapter, weâ€™ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Ready-made <span class="No-Break">data sources</span></li>
<li>Creating your <span class="No-Break">own dataset</span></li>
<li>Other <span class="No-Break">data sources</span></li>
<li><span class="No-Break">Transforming data</span></li>
<li><span class="No-Break">Non-English datasets</span></li>
<li><span class="No-Break">Evaluation</span></li>
</ul>
<h1 id="_idParaDest-42"><a id="_idTextAnchor063"/>Ready-made data sources</h1>
<p>There<a id="_idIndexMarker172"/> are lots of places where ready-made data is available and usually freely downloadable. These are typically referred to as <strong class="bold">public data sources</strong> and <a id="_idIndexMarker173"/>are usually made available by companies, institutions, and organizations that are happy to either share their data (perhaps for publicity, or to entice others to share) or to act as a repository for others to make their data easily searchable and accessible. Clearly, these will only be useful to you if your need matches the data source, but if it does, it can be a great starting point or even a supplement to your own data. The good news is that these data sources usually cover a wide range of domains, so itâ€™s likely youâ€™ll find <span class="No-Break">something useful.</span></p>
<p>We'll now discuss some of these public data sources (in no <span class="No-Break">particular order):</span></p>
<ul>
<li><strong class="bold">Kaggle</strong>: Founded in <a id="_idIndexMarker174"/>2010, Kaggle is a part of Google and, according to <em class="italic">Wikipedia</em>, is an â€œonline community of data scientists and machine learning practitioners ". Kaggle<a id="_idIndexMarker175"/> has many features, but it is best known for its competitions in which anyone (for example, individuals and organizations) can publish a competition (typically a data science task) for participants to enter and compete to win prizes (sometimes in the form of cash!). However, Kaggle also allows users to find and publish datasets. Users can upload their datasets to Kaggle and also download datasets published <span class="No-Break">by others.</span></li>
</ul>
<p>Everything on Kaggle is completely free, including datasets, and although the datasets are open source (open and free for download, modification, and reuse), for some datasets, it will be necessary to refer to the license to ascertain the purposes for which the dataset can be used. For example, some datasets may not be used for academic publications or commercial purposes. Users are also allowed to upload code to process the dataset, post comments against the dataset, and upvote the dataset so that others know that it is a reliable and useful dataset. There are also various other <strong class="bold">Activity Overview</strong> metrics, such as when the dataset was downloaded, how many times the dataset was downloaded, and how many times it was viewed. Since there are so many datasets (approximately 170,000 at the time of writing) these metrics can help you decide whether a dataset is worth<a id="_idIndexMarker176"/> downloading <span class="No-Break">or not.</span></p>
<p><span class="No-Break">URL: </span><span class="No-Break">https://www.kaggle.com/datasets</span></p>
<ul>
<li><strong class="bold">Hugging Face</strong>: The <a id="_idIndexMarker177"/>Hugging Face Hub is a <a id="_idIndexMarker178"/>community-driven collection of datasets that span a variety of domains and tasksâ€”for example, <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>), and audio. Each dataset<a id="_idIndexMarker179"/> is a Git repository<a id="_idIndexMarker180"/> that has the scripts required to download the data and generate splits for training, evaluation, and testing. There are approximately 10,000 datasets that can be filtered by the <span class="No-Break">following criteria:</span><ul><li>Task categories (text classification, QA, text <span class="No-Break">generation, etc.)</span></li><li>Tasks (language modeling, multi-class classification, language <span class="No-Break">inference, etc.)</span></li><li>Language (English, French, <span class="No-Break">German, etc.)</span></li><li>Multilinguality (monolingual, multilingual, <span class="No-Break">translation, etc.)</span></li><li>Size (10-100K, 1K-10K, <span class="No-Break">100K-1M, etc.)</span></li><li>License (CC by 4.0, MIT, <span class="No-Break">others, etc.)</span></li></ul></li>
</ul>
<p>Each dataset page includes a view of the first 100 rows of the dataset and also has a handy feature that allows you to copy the code to load a dataset. Some datasets also contain a loading script, which also allows you to easily load the dataset. Where the dataset does not include this loading script, the data is usually<a id="_idIndexMarker181"/> stored directly in the repository, in CSV, JSON, or <a id="_idIndexMarker182"/><span class="No-Break">Parquet format.</span></p>
<p><span class="No-Break">URL: </span><a href="https://huggingface.co/datasets"><span class="No-Break">https://huggingface.co/datasets</span></a></p>
<ul>
<li><strong class="bold">TensorFlow Datasets</strong>: TensorFlow Datasets<a id="_idIndexMarker183"/> offers a set of datasets that are<a id="_idIndexMarker184"/> suitable for use not just with TensorFlow but also with other Python <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) frameworks. Each dataset is presented as a class to allow the building of efficient data input <a id="_idIndexMarker185"/>pipelines and user-friendly <span class="No-Break">input processes.</span></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://www.tensorflow.org/datasets"><span class="No-Break">https://www.tensorflow.org/datasets</span></a></p>
<ul>
<li><strong class="bold">Papers With Code</strong>: This is a site that contains, as the name suggests, research papers along <a id="_idIndexMarker186"/>with their code implementations. At the time of writing, there <a id="_idIndexMarker187"/>are around 7,000 ML datasets available to freely download. These can be searched by using the <span class="No-Break">following filters:</span><ul><li>Modality (text, images, video, <span class="No-Break">audio, etc.)</span></li><li>Task (QA, object detection, image classification, text <span class="No-Break">classification, etc.)</span></li><li>Language (English, Chinese, German, <span class="No-Break">French, etc.)</span></li></ul></li>
</ul>
<p>According to its <em class="italic">About</em> page, all datasets are licensed under the CC BY-SA license, allowing anyone to use the datasets as long as the creator(s) are acknowledged. Usefully, each dataset lists papers that utilize the dataset, associated benchmarks, code, and similar datasets, and explains how the dataset can be loaded from within popular frameworks such <span class="No-Break">as TensorFlow.</span></p>
<p>Papers <a id="_idIndexMarker188"/>With Code also encourages users to share their datasets with the community. The process is relatively simple and involves registering, uploading the dataset, and providing links and information (e.g. description, modality, task, language, etc.) about<a id="_idIndexMarker189"/> <span class="No-Break">the dataset.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Papers With Codeâ€™s <em class="italic">About</em> page states that although the core team is based at Meta AI Research no data is shared with any Meta <span class="No-Break">Platforms product.</span></p>
<p><span class="No-Break">URL: </span><a href="https://paperswithcode.com/datasets"><span class="No-Break">https://paperswithcode.com/datasets</span></a></p>
<ul>
<li><strong class="bold">IEEE DataPort</strong>: IEEE DataPort<a id="_idIndexMarker190"/> is an online data repository that was created, and is <a id="_idIndexMarker191"/>owned by, the <strong class="bold">Institute of Electrical and Electronics Engineers</strong> (<strong class="bold">IEEE</strong>), a <a id="_idIndexMarker192"/>professional association for electronic engineering, electrical engineering, and associated disciplines. At the time of writing, there are around 6,000 datasets available. These can <a id="_idIndexMarker193"/>be searched by using either free-text search terms (for example, title, author, or <strong class="bold">digital object identifier</strong> (<strong class="bold">DOI</strong>)) or filters, such as <span class="No-Break">the following:</span><ul><li>Category (<strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>), CV, <a id="_idIndexMarker194"/><span class="No-Break">ML, etc.)</span></li><li>Type (Standard, <span class="No-Break">Open Access)</span></li></ul></li>
</ul>
<p>Open <a id="_idIndexMarker195"/>Access datasets allow free access to all users, whereas accessing Standard datasets requires an IEEE paid subscription. IEEE DataPort also offers three options (Standard, Open Access, and Competition) for users to upload their datasets. Standard and Competition are free to upload and access; however, Open <a id="_idIndexMarker196"/>Access requires the purchase of an Open <span class="No-Break">Access credit.</span></p>
<p><span class="No-Break">URL: </span><a href="https://ieee-dataport.org/datasets"><span class="No-Break">https://ieee-dataport.org/datasets</span></a></p>
<ul>
<li><strong class="bold">Google Dataset Search</strong>: Google Dataset Search<a id="_idIndexMarker197"/> is a search engine for <a id="_idIndexMarker198"/>datasets that features a simple keyword search engine (similar to the Google search page we all know and love) that allows users to find datasets that are themselves hosted in repositories (e.g. Kaggle) across the web. Results can then be filtered by the <span class="No-Break">following criteria:</span><ul><li>Last updated (past <span class="No-Break">month, year)</span></li><li>Download format (text, tabular, document, <span class="No-Break">image, etc.)</span></li><li>Usage rights (commercial use <span class="No-Break">allowed/not allowed)</span></li><li>Topic (architecture and urban planning, computing, <span class="No-Break">engineering, etc.)</span></li><li>Free <span class="No-Break">or paid</span></li></ul></li>
</ul>
<p>The website <a id="_idIndexMarker199"/>states that the search engine only came out of beta in 2020, hence there may be more features added later. Being part of the Google ecosystem, it also allows users to easily bookmark datasets to return to later. As one would expect with Google, there is data available on a vast range of topics, from mobile apps to fast<a id="_idIndexMarker200"/> food and everything <span class="No-Break">in between.</span></p>
<p><span class="No-Break">URL: </span><a href="https://datasetsearch.research.google.com"><span class="No-Break">https://datasetsearch.research.google.com</span></a></p>
<ul>
<li><strong class="bold">BigQuery public datasets</strong>: BigQuery<a id="_idIndexMarker201"/> is a <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) product <a id="_idIndexMarker202"/>that was built to provide serverless, cost-effective, highly scalable data warehouse capabilities. Hence, BigQuery is used to host and access public datasets, making them publicly <a id="_idIndexMarker203"/>available for users to integrate into their applications via projects. Although the datasets are free, users must pay for the queries that are performed on the data. However, at the time of writing, the first 1 TB per month is free. There are many ways to access BigQuery public datasets: by using the<a id="_idIndexMarker204"/> Google Cloud console, by using the <a id="_idIndexMarker205"/>BigQuery REST API, or through Google <span class="No-Break">Analytics Hub.</span></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://cloud.google.com/bigquery/public-data"><span class="No-Break">https://cloud.google.com/bigquery/public-data</span></a></p>
<ul>
<li><strong class="bold">Google Public Data Explorer</strong>: Google Public Data Explorer<a id="_idIndexMarker206"/> is a web-based tool <a id="_idIndexMarker207"/>that makes it easy to explore and visualize datasets as line graphs, bar graphs, plots, or on maps. It provides data from about 135 organizations and academic institutions such as The <a id="_idIndexMarker208"/>World Bank, The <strong class="bold">World Trade Organization</strong> (<strong class="bold">WTO</strong>), Eurostat, and the US Census Bureau. Users are also able to upload, visualize, and share their own data by making use of Googleâ€™s <strong class="bold">Dataset Publishing Language</strong> (<strong class="bold">DSPL</strong>) data<a id="_idIndexMarker209"/> format. Where the system really shines is when the charts are animated over time, making <a id="_idIndexMarker210"/>it easy even for non-scientists to understand the impact and <span class="No-Break">gain</span><span class="No-Break"><a id="_idIndexMarker211"/></span><span class="No-Break"> insights.</span></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://www.google.com/publicdata/directory"><span class="No-Break">https://www.google.com/publicdata/directory</span></a></p>
<ul>
<li><strong class="bold">UCI Machine Learning Repository</strong>: The <strong class="bold">University of California Irvine</strong> (<strong class="bold">UCI</strong>) Machine Learning<a id="_idIndexMarker212"/> Repository was created as an <a id="_idIndexMarker213"/>FTP archive in 1987 by graduate students at UCI. It is a<a id="_idIndexMarker214"/> free (registration not required) collection of approximately 600 datasets that are available for the ML community. The main website is rudimentary and outdated with a Google-powered search and no filtering capabilities, but (at the time of writing) a new version is in beta testing and offers the ability to search using the <span class="No-Break">following filters:</span><ul><li>Characteristics (text, tabular, sequential, time-series, <span class="No-Break">image, etc.)</span></li><li>Subject area (business, computer science, engineering, <span class="No-Break">law, etc.)</span></li><li>Associated tasks (classification, regression, <span class="No-Break">clustering, etc.)</span></li><li>Number of attributes (fewer than 10, 10-100, more <span class="No-Break">than 100)</span></li><li>Number of instances (fewer than 10, 10-100, more <span class="No-Break">than 100)</span></li><li>Attribute types (numerical, <span class="No-Break">categorical, mixed)</span></li></ul></li>
</ul>
<p>The datasets in the repository are donated by different authors and organizations, hence each dataset has individual license requirements. The site states that to use the datasets, citation information should be used, and usage policies and licenses <a id="_idIndexMarker215"/>should <span class="No-Break">be checked.</span></p>
<p><span class="No-Break">URL: </span><a href="https://archive.ics.uci.edu"><span class="No-Break">https://archive.ics.uci.edu</span></a></p>
<ul>
<li><strong class="bold">Registry of Open Data on AWS</strong>: The <a id="_idIndexMarker216"/>Registry of Open Data on AWS (short for <a id="_idIndexMarker217"/>Amazon Web Services) is a <a id="_idIndexMarker218"/>centralized repository that makes it easy to find publicly available datasets. These datasets are not provided by Amazon, as they are owned by government organizations, researchers, businesses, and individuals. The registry can be used to discover and share datasets. There are approximately 330 datasets available, and these are accessed via the AWS Data Exchange service (an online marketplace offering thousands of datasets). Being Amazon, much of this infrastructure is tied to the core AWS services; for example, datasets can be used with AWS resources and easily integrated into AWS cloud-based applications. As an example, it only takes minutes to provision an Amazon <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) instance<a id="_idIndexMarker219"/> and <a id="_idIndexMarker220"/>start working with <span class="No-Break">the data.</span></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://registry.opendata.aws"><span class="No-Break">https://registry.opendata.aws</span></a></p>
<ul>
<li><strong class="bold">US Government open data</strong>: Launched in 2009, <em class="italic">Data.gov</em> is managed and hosted by the US<a id="_idIndexMarker221"/> General Services Administration and was created and launched<a id="_idIndexMarker222"/> by the US Government to provide access to federal, state, and local datasets. There are approximately 320,000 datasets that are made available in open, machine-readable formats, while continuing to maintain privacy and security, and can be searched by keyword or filtered by the <span class="No-Break">following criteria:</span><ul><li><span class="No-Break">Location</span></li><li>Topic (local government, climate, <span class="No-Break">energy, etc.)</span></li><li>Topic category (health, flooding <span class="No-Break">water, etc.)</span></li><li>Dataset <span class="No-Break">type (geospatial)</span></li><li>Format (CSV, HTML, <span class="No-Break">XML, etc.)</span></li><li>Organization type (federal, state, <span class="No-Break">local, etc.)</span></li><li>Organization (NASA, state, <span class="No-Break">department, etc.)</span></li><li><span class="No-Break">Publisher</span></li><li><span class="No-Break">Bureau</span></li></ul></li>
</ul>
<p>The datasets are made<a id="_idIndexMarker223"/> available for free and without restriction, although they do advise that<a id="_idIndexMarker224"/> non-federal data available may have a different <span class="No-Break">licensing method.</span></p>
<p><span class="No-Break">URL: </span><a href="https://data.gov"><span class="No-Break">https://data.gov</span></a></p>
<ul>
<li><strong class="bold">data.gov.uk</strong>: Similarly, the <em class="italic">data.gov.uk</em> site allows users to find public sector, non-personal data published <a id="_idIndexMarker225"/>by the UK central government, UK local authorities, and UK public bodies. The datasets are typically hosted on AWS. There <a id="_idIndexMarker226"/>are approximately <a id="_idIndexMarker227"/>52,000 datasets that can be filtered by the <span class="No-Break">following criteria:</span><ul><li><span class="No-Break">Publisher (council)</span></li><li>Topic (business and economy, crime and justice, <span class="No-Break">education, etc.)</span></li><li>Format (CSV, HTML, <span class="No-Break">XLS, etc.)</span></li></ul></li>
</ul>
<p>The datasets are free (registration required), and licensing appears to be a mix, with some<a id="_idIndexMarker228"/> being <strong class="bold">Open Government License</strong> (<strong class="bold">OGL</strong>), which permits anyone to copy, distribute, or exploit the data, and others <a id="_idIndexMarker229"/>requiring <strong class="bold">Freedom of Information</strong> (<strong class="bold">FOI</strong>) requests <a id="_idIndexMarker230"/>for <span class="No-Break">the dataset.</span></p>
<p><span class="No-Break">URL: </span><a href="https://ukdataservice.ac.uk"><span class="No-Break">https://ukdataservice.ac.uk</span></a></p>
<ul>
<li><strong class="bold">Microsoft Azure Open Datasets</strong>: This is a <a id="_idIndexMarker231"/>curated repository of <a id="_idIndexMarker232"/>datasets that can be used to train models. However, there are only about 50 datasets, covering areas such as transport, health, and labor, as well as some common datasets. There are no charges for using most of <a id="_idIndexMarker233"/><span class="No-Break">the datasets.</span></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://azure.microsoft.com/en-us/products/open-datasets/"><span class="No-Break">https://azure.microsoft.com/en-us/products/open-datasets/</span></a></p>
<ul>
<li><strong class="bold">Microsoft Research Open Data</strong>: This<a id="_idIndexMarker234"/> is another collection of free datasets from Microsoft and contains datasets useful for areas such<a id="_idIndexMarker235"/> as NLP and CV. Again, there are only about 100 datasets, which can be searched by text or can be filtered by the <span class="No-Break">following criteria:</span><ul><li>Category (computer science, math, <span class="No-Break">physics, etc.)</span></li><li>Format (CSV, DOCX, <span class="No-Break">JPG, etc.)</span></li><li>License (Creative <a id="_idIndexMarker236"/>Commons, legacy Microsoft Research Data License <span class="No-Break">Agreement, etc.)</span></li></ul></li>
</ul>
<p><span class="No-Break">URL: </span><a href="https://msropendata.com"><span class="No-Break">https://msropendata.com</span></a></p>
<p>The preceding list is intended as an indicative, non-exhaustive guide for those who are unsure where to go to get data and provides examples of repositories from a number of organizations. There are also â€œrepositories of repositoriesâ€ where lists of dataset repositories are maintained, and these are good places to start searching for data. These include sites such as the <a id="_idIndexMarker237"/>DataCite Commons Repository Finder (<a href="https://repositoryfinder.datacite.org">https://repositoryfinder.datacite.org</a>) and the Registry of Research Data <a id="_idIndexMarker238"/>Repositories <a href="https://re3data.org/">https://re3data.org/</a>, which offers researchers an overview of existing repositories for <span class="No-Break">research data.</span></p>
<p>It should also be noted that <a id="_idIndexMarker239"/>some of the most common popular datasets are also <a id="_idIndexMarker240"/>easily available from within Python packages such as TensorFlow, <strong class="bold">scikit-learn</strong> (<strong class="bold">sklearn</strong>), and<a id="_idIndexMarker241"/> the <strong class="bold">Natural Language </strong><span class="No-Break"><strong class="bold">Toolkit</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NLTK</strong></span><span class="No-Break">).</span></p>
<p>In this section, we saw how we can access ready-made data sources. However, sometimes these are inadequate, so let us next see how we can create our own <span class="No-Break">data sources.</span></p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor064"/>Creating your own dataset</h1>
<p>Although we have seen several sources<a id="_idIndexMarker242"/> where datasets can be obtained, sometimes it is necessary to build your own datasets either using your own data or using data from other sources. This may be because the available datasets are not adequate for our problem, and this approach also brings some additional benefits, <span class="No-Break">as follows:</span></p>
<ul>
<li>Creating your own dataset can eliminate the challenges associated with third-party datasets that often have licensing terms or <span class="No-Break">usage restrictions.</span></li>
<li>There are no fees to pay (although building the dataset will <span class="No-Break">incur costs).</span></li>
<li>If the dataset is being created using your own data, there are no ownership issues. If not, then it is your responsibility to consider ownership issues, and appropriate steps should <span class="No-Break">be taken.</span></li>
<li>You have complete ownership and flexibility in how you use <span class="No-Break">the data.</span></li>
<li>A fuller understanding of the data is gained as part of building <span class="No-Break">the dataset.</span></li>
</ul>
<p>Creating your own dataset also comes with increased responsibility; in other words, if there are any errors, issues, or biases, there will be only one person <span class="No-Break">to blame!</span></p>
<p>Clearly, many <a id="_idIndexMarker243"/>types of data can be collectedâ€”for example, financial data, data <a id="_idIndexMarker244"/>from <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices, and data from databases. However, since the purpose of this book is the emotional analysis of text, we will demonstrate some ways to collect textual data to <span class="No-Break">build datasets.</span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor065"/>Data from PDF files</h2>
<p>The <strong class="bold">Portable Document Format</strong> (<strong class="bold">PDF</strong>) format<a id="_idIndexMarker245"/> is one of the <a id="_idIndexMarker246"/>most popular and widely used digital file formats and is used to present and exchange documents. Many organizations use the PDF format to publish documentation, release notes, and other document types because files can be <span class="No-Break">read anywhere,</span></p>
<p>on any device, as long as (free) tools such as Adobe Acrobat Reader are installed. Consequently, this makes PDF files a good place to look for data. Luckily for us, Python<a id="_idIndexMarker247"/> has a number of libraries to help us extract text from PDF files, as <span class="No-Break">listed here:</span></p>
<ul>
<li><span class="No-Break">PyPDF4</span></li>
<li><span class="No-Break">PDFMiner</span></li>
<li><span class="No-Break">PDFplumber</span></li>
</ul>
<p>There are many others, but these seem to be the most popular. For this example, due to our previous experiences, we will <span class="No-Break">use PyPDF4.</span></p>
<p>Firstly, we need to ensure that the PyPDF4 module is installed. Hereâ€™s the command we run to <span class="No-Break">achieve this:</span></p>
<pre class="source-code">
pip install PyPDF4</pre> <p>Then, we need to import the package and set up a variable that contains the name of the file we wish to process. For this example, a sample PDF was downloaded <span class="No-Break">from </span><a href="https://www.jbc.org/article/S0021-9258(19)52451-6/pdf"><span class="No-Break">https://www.jbc.org/article/S0021-9258(19)52451-6/pdf</span></a><span class="No-Break">:</span></p>
<pre class="source-code">
import PyPDF4file_name = "PIIS0021925819524516.pdf"</pre>
<p>Next, we need to set up some objects that will actually allow us to read the PDF file, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
file = open(file_name,'rb')pdf_reader = PyPDF4.PdfFileReader(file)</pre>
<p>PyPDF4 can also extract metadata (data about the file) from the PDF. Hereâ€™s how to <span class="No-Break">do that:</span></p>
<pre class="source-code">
metadata = pdf_reader.getDocumentInfo()print (f"Title: {metadata.title}")
print (f"Author: {metadata.author}")
print (f"Subject: {metadata.subject}")</pre>
<p>The output shows the title, author, and subject of the document (there are also other <span class="No-Break">fields available):</span></p>
<pre class="source-code">
Title: PROTEIN MEASUREMENT WITH THE FOLIN PHENOL REAGENTAuthor: Oliver H. Lowry
Subject: Journal of Biological Chemistry, 193 (1951) 265-275. doi:10.1016/S0021-9258(19)52451-6</pre>
<p>We can<a id="_idIndexMarker248"/> also get a count of the number of pages in the document by executing the <span class="No-Break">following code:</span></p>
<pre class="source-code">
pages = pdf_reader.numPagesprint(f"Pages: {pages}")</pre>
<p>The output shows the number of pages in <span class="No-Break">the document:</span></p>
<pre class="source-code">
Pages: 11</pre> <p>We can now iterate through each page, extract the text, and write it to a database or a file, <span class="No-Break">like so:</span></p>
<pre class="source-code">
page = 0while page &lt; pages:
Â Â Â Â pdf_page = pdf_reader.getPage(page)
Â Â Â Â print(pdf_page.extractText())
Â Â Â Â page+=1
Â Â Â Â # write to a database here</pre>
<p>Thatâ€™s it! We have extracted the text from a PDF file and can use it to build a dataset and ultimately use it to train a model (after cleaning and preprocessing). Of course, in reality, we would wrap this into a function and iterate a folder of files to create a proper dataset, so letâ€™s <span class="No-Break">do that.</span></p>
<p>First, we need to import the appropriate libraries and set up a folder where the PDF files are, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
import PyPDF4from pathlib import Path
folder = "./"</pre>
<p>Now, we<a id="_idIndexMarker249"/> can refactor the code we had originally and reengineer it in the form of some handy <span class="No-Break">reusable functions:</span></p>
<pre class="source-code">
def print_metadata(pdf_reader):Â Â Â Â # print the meta data
Â Â Â Â metadata = pdf_reader.getDocumentInfo()
Â Â Â Â print (f"Title: {metadata.title}")
Â Â Â Â print (f"Author: {metadata.author}")
Â Â Â Â print (f"Subject: {metadata.subject}")
def save_content(pdf_reader):
Â Â Â Â # print number of pages in pdf file
Â Â Â Â pages = pdf_reader.numPages
Â Â Â Â print(f"Pages: {pages}")
Â Â Â Â # get content for each page
Â Â Â Â page = 0
Â Â Â Â while page &lt; pages:
Â Â Â Â Â Â Â Â pdf_page = pdf_reader.getPage(page)
Â Â Â Â Â Â Â Â print(pdf_page.extractText())
Â Â Â Â Â Â Â Â page+=1
Â Â Â Â Â Â Â Â # write each page to a database here</pre>
<p>Note how, in <strong class="source-inline">save_content,</strong> there is a placeholder where you would normally write the extracted content to <span class="No-Break">a database.</span></p>
<p>And finally, hereâ€™s the main code where we iterate the folder and, for each PDF file, extract <span class="No-Break">the content:</span></p>
<pre class="source-code">
pathlist = Path(folder).rglob('*.pdf')for file_name in pathlist:
Â Â Â Â file_name = str(file_name)
Â Â Â Â pdf_file = open(file_name,'rb')
Â Â Â Â pdf_reader = PyPDF4.PdfFileReader(pdf_file)
Â Â Â Â print (f"File name: {file_name}")
Â Â Â Â print_metadata(pdf_reader)
Â Â Â Â save_content(pdf_reader)
Â Â Â Â pdf_file.close()</pre>
<p>As we have<a id="_idIndexMarker250"/> seen, extracting text from PDF files is pretty straightforward. Letâ€™s now see how we can get data from <span class="No-Break">the internet.</span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor066"/>Data from web scraping</h2>
<p>Nowadays, there<a id="_idIndexMarker251"/> is so much publicly available data on the web in the form of (for example) news, blogs, and social media that it makes sense to gather (â€œharvestâ€) and make use of this. The process of extracting data from a website <a id="_idIndexMarker252"/>is known as <strong class="bold">web scraping</strong>, and although this can be done manually, that would not be an efficient use of time and resources, especially when there are plenty of tools to help automate the process. The steps to do this are something <span class="No-Break">like this:</span></p>
<ol>
<li>Identify a root URL (a <span class="No-Break">starting point).</span></li>
<li>Download the <span class="No-Break">page content.</span></li>
<li>Process/clean/format the <span class="No-Break">downloaded text.</span></li>
<li>Save the <span class="No-Break">cleaned text.</span></li>
</ol>
<p>Although there are no hard and fast rules, there are some rules of etiquette that will stop your program from getting blocked and some rules that will make scraping easier that should <span class="No-Break">be followed:</span></p>
<ul>
<li>Add a delay between each scrape request so the site does not <span class="No-Break">get overloaded</span></li>
<li>Scrape during <span class="No-Break">non-peak hours</span></li>
</ul>
<p>Do note <a id="_idIndexMarker253"/>here that it is important that data is only scraped from sources that allow it, as unauthorized scraping can infringe terms of service and intellectual property rights and may even have legal ramifications. It is also a sensible idea to examine the metadata as it may provide guidance on whether the data is sensitive or private, data provenance, permissions, and restrictions on use. Being respectful of source permissions and data sensitivity are important considerations in responsible and ethical <span class="No-Break">web scraping.</span></p>
<p><span class="No-Break">Letâ€™s begin!</span></p>
<p>Firstly, we need to ensure that the Beautiful Soup module is installed. We can do that using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
pip install beautifulsoup4</pre> <p class="callout-heading">Note</p>
<p class="callout">To prevent any unforeseen errors, please ensure that the following versions <span class="No-Break">are installed:</span></p>
<p class="callout">Beautiful <span class="No-Break">Soup 4.11.2</span></p>
<p class="callout"><span class="No-Break">lxml 4.9.3</span></p>
<p>We then import the <span class="No-Break">required libraries:</span></p>
<pre class="source-code">
import bs4 as bsimport re
import time
from urllib.request import urlopen</pre>
<p>We also need a URL to start scraping from. Hereâ€™s <span class="No-Break">an example:</span></p>
<pre class="source-code">
ROOT_URL = "https://en.wikipedia.org/wiki/Emotion"</pre> <p>We now <a id="_idIndexMarker254"/>need to separate interesting, relevant content from the non-useful elements of a web page, such as the menu, header, and footer. Every website has its own set of design styles and conventions and will display its content in its own unique manner. For the website we chose, we found that looking for three consecutive <strong class="source-inline">&lt;p&gt;</strong> tags homed in on the content part of the page. Itâ€™s very likely that this logic will be different for the website you are scraping from. To find these <strong class="source-inline">&lt;p&gt;</strong> tags, we define a <strong class="bold">regular expression</strong> (<strong class="bold">regex</strong>), <span class="No-Break">as</span><span class="No-Break"><a id="_idIndexMarker255"/></span><span class="No-Break"> follows:</span></p>
<pre class="source-code">
p = re.compile(r'((&lt;p[^&gt;]*&gt;(.(?!&lt;/p&gt;))*.&lt;/p&gt;\s*){3,})',Â Â Â Â re.DOTALL)</pre>
<p>We now need to request the HTML for the website and extract the paragraphs using the regex. This text can then be cleaned (for example, any inline HTML removed) and saved to <span class="No-Break">a database:</span></p>
<pre class="source-code">
def get_url_content(url):Â Â Â Â with urlopen(url) as url:
Â Â Â Â Â Â Â Â raw_html = url.read().decode('utf-8')
Â Â Â Â Â Â Â Â for match in p.finditer(raw_html):
Â Â Â Â Â Â Â Â Â Â Â Â paragraph = match.group(1)
Â Â Â Â Â Â Â Â Â Â Â Â # clean up, extract HTML and save to database</pre>
<p>However, we can go one step further. By extracting the hyperlinks from this page, we can get our program to keep scraping deeper into the website. This is where the previous commentary on best practices should <span class="No-Break">be applied:</span></p>
<pre class="source-code">
def get_url_content(url):Â Â Â Â with urlopen(url) as url:
Â Â Â Â Â Â Â Â raw_html = url.read().decode('utf-8')
Â Â Â Â Â Â Â Â # clean up, extract HTML and save to database
Â Â Â Â Â Â Â Â for match in p.finditer(raw_html):
Â Â Â Â Â Â Â Â Â Â Â Â paragraph = match.group(1)
Â Â Â Â Â Â Â Â Â Â Â Â soup = bs.BeautifulSoup(paragraph,'lxml')
Â Â Â Â Â Â Â Â Â Â Â Â for link in soup.findAll('a'):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â new_url = (link.get('href'))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # add a delay between each scrape
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â time.sleep(1)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â get_url_content(new_url)</pre>
<p>Finally, we need some code to start <span class="No-Break">the scrape:</span></p>
<pre class="source-code">
raw_html = get_url_content(ROOT_URL)</pre> <p>To prevent the<a id="_idIndexMarker256"/> program from ending up in a loop, a list of visited URLs should be maintained and checked before scraping each URLâ€”we have left this as an exercise for <span class="No-Break">the reader.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If you get a <strong class="source-inline">&lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)&gt;</strong> error, you can use this link to resolve <span class="No-Break">it:</span><span class="No-Break"><span class="hidden"> </span></span><a href="https://stackoverflow.com/a/70495761/5457712"><span class="No-Break">https://stackoverflow.com/a/70495761/5457712</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor067"/>Data from RSS feeds</h2>
<p><strong class="bold">RSS</strong> (short for <strong class="bold">Really Simple Syndication</strong>) is a <a id="_idIndexMarker257"/>relatively old <a id="_idIndexMarker258"/>technology. Once upon a time, it was used to collate all the latest news into a web browser. Nowadays, it is not as popular as it once was but is still used by many to stay up to date. Most news providers provide RSS feeds on <span class="No-Break">their websites.</span></p>
<p>An RSS feed<a id="_idIndexMarker259"/> is typically an <strong class="bold">Extensible Markup Language</strong> (<strong class="bold">XML</strong>) document<a id="_idIndexMarker260"/> that includes a URL to a web page (that can be scraped, as we have seen), full or summarized text, and metadata such as the publication date and the <span class="No-Break">authorâ€™s name.</span></p>
<p>Letâ€™s see how we can create a dataset of <span class="No-Break">news headlines.</span></p>
<p>As usual, firstly we need to ensure that the module we need is installed. <strong class="source-inline">feedparser</strong> is a Python library that works with feeds in all known formats. You can install it using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
pip install feedparser</pre> <p>Then, we import it, <span class="No-Break">like so:</span></p>
<pre class="source-code">
import feedparser</pre> <p>We also need a feed URL to work with. Hereâ€™s <span class="No-Break">an example:</span></p>
<pre class="source-code">
RSS_URL = "http://feeds.bbci.co.uk/news/rss.xml"</pre> <p>Then, it is a simple<a id="_idIndexMarker261"/> matter of downloading the feed and extracting the relevant parts. For news headlines, we envisage that the summary contains more information, so it should be saved to <span class="No-Break">a database:</span></p>
<pre class="source-code">
def process_feed(rss_url):Â Â Â Â feed = feedparser.parse(rss_url)
Â Â Â Â # attributes of the feed
Â Â Â Â print (feed['feed']['title'])
Â Â Â Â print (feed['feed']['link'])
Â Â Â Â print (feed.feed.subtitle)
Â Â Â Â for post in feed.entries:
Â Â Â Â Â Â Â Â print (post.link)
Â Â Â Â Â Â Â Â print (post.title)
Â Â Â Â Â Â Â Â # save to database
Â Â Â Â Â Â Â Â print (post.summary)</pre>
<p>Finally, we need some code to start <span class="No-Break">the process:</span></p>
<pre class="source-code">
process_feed(RSS_URL)</pre> <p>The output shows the URL, title, and summary from each element in <span class="No-Break">the feed:</span></p>
<pre class="source-code">
BBC News - Homehttps://www.bbc.co.uk/news/
BBC News - Home
https://www.bbc.co.uk/news/world-asia-63155169?at_medium=RSS&amp;at_campaign=KARANGA
Thailand: Many children among dead in nursery attack
An ex-police officer killed at least 37 people at a childcare centre before killing himself and his family.
https://www.bbc.co.uk/news/world-asia-63158837?at_medium=RSS&amp;at_campaign=KARANGA
Thailand nursery attack: Witnesses describe shocking attack
There was terror and confusion as sleeping children were attacked by the former policeman.
https://www.bbc.co.uk/news/science-environment-63163824?at_medium=RSS&amp;at_campaign=KARANGA
UK defies climate warnings with new oil and gas licences
More than 100 licences are expected to be granted for new fossil fuel exploration in the North Sea.</pre>
<p>Let us next take a <a id="_idIndexMarker262"/>look at how a more robust technology, APIs, can be used to <span class="No-Break">download data.</span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor068"/>Data from APIs</h2>
<p>X (formerly Twitter) is a <a id="_idIndexMarker263"/>fantastic place to <a id="_idIndexMarker264"/>obtain text data; it offers an easy-to-use API. It is free to start off with, and there are many Python libraries available that can be used to call <span class="No-Break">the API.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing, the free X (Twitter) API is in a state of flux, and it may no longer be possible to use the <span class="No-Break"><strong class="source-inline">tweepy</strong></span><span class="No-Break"> API.</span></p>
<p>Given that<a id="_idIndexMarker265"/>, later on in this book, we work with tweets, it is sensible at this point to learn how to extract tweets from Twitter. For this, we need a package called <strong class="source-inline">tweepy</strong>. Use the following command to <span class="No-Break">install </span><span class="No-Break"><strong class="source-inline">tweepy</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
pip install tweepy</pre> <p>Next, we need to sign up for an account and generate some keys, so proceed <span class="No-Break">as follows:</span></p>
<ol>
<li>Go to <a href="https://developer.twitter.com/en">https://developer.twitter.com/en</a> and sign up for <span class="No-Break">an account.</span></li>
<li>Go <span class="No-Break">to </span><a href="https://developer.twitter.com/en/portal/projects-and-apps"><span class="No-Break">https://developer.twitter.com/en/portal/projects-and-apps</span></a><span class="No-Break">.</span></li>
<li>Click <strong class="bold">Create App</strong> in the <strong class="bold">Standalone </strong><span class="No-Break"><strong class="bold">Apps</strong></span><span class="No-Break"> section.</span></li>
<li>Give your app a name, and make a note of the <strong class="bold">API Key</strong>, <strong class="bold">API Key Secret</strong>, and <strong class="bold">Bearer </strong><span class="No-Break"><strong class="bold">Token</strong></span><span class="No-Break"> values.</span></li>
<li>Click <strong class="bold">App Settings</strong> and then click the <strong class="bold">Keys and </strong><span class="No-Break"><strong class="bold">Tokens</strong></span><span class="No-Break"> tab.</span></li>
<li>On this page, click <strong class="bold">Generate</strong> in the <strong class="bold">Access Token and Secret</strong> section and again make a note of <span class="No-Break">these values.</span></li>
</ol>
<p>We are now ready to use these keys to get some tweets from Twitter! Letâ€™s run the <span class="No-Break">following code:</span></p>
<pre class="source-code">
import tweepyimport time
BEARER_TOKEN = "YOUR_KEY_HERE"
ACCESS_TOKEN = "YOUR_KEY_HERE"
ACCESS_TOKEN_SECRET = "YOUR_KEY_HERE"
CONSUMER_KEY = "YOUR_KEY_HERE"
CONSUMER_SECRET ="YOUR_KEY_HERE"</pre>
<p class="callout-heading">Note</p>
<p class="callout">You must replace the <strong class="source-inline">YOUR_KEY_HERE</strong> token with your <span class="No-Break">own keys.</span></p>
<p>We then<a id="_idIndexMarker266"/> create a class with a subclassed special method called <strong class="source-inline">on_tweet</strong> that is triggered when a tweet is received from this stream. The code is actually pretty simple and looks <span class="No-Break">like this:</span></p>
<pre class="source-code">
client = tweepy.Client(BEARER_TOKEN, CONSUMER_KEY,Â Â Â Â CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
auth = tweepy.OAuth1UserHandler(CONSUMER_KEY,
Â Â Â Â CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth)
class TwitterStream(tweepy.StreamingClient):
Â Â Â Â def on_tweet(self, tweet):
Â Â Â Â Â Â Â Â print(tweet.text)
Â Â Â Â Â Â Â Â time.sleep(0.2)
Â Â Â Â Â Â Â Â # save to database
stream = TwitterStream(bearer_token=BEARER_TOKEN)</pre>
<p>Tweepy insists that â€œrulesâ€ are added to filter the stream, so letâ€™s add a rule that states we are only <a id="_idIndexMarker267"/>interested in tweets that contain the <strong class="source-inline">#</strong><span class="No-Break"><strong class="source-inline">lfc</strong></span><span class="No-Break"> hashtag:</span></p>
<pre class="source-code">
stream.add_rules(tweepy.StreamRule("#lfc"))print(stream.get_rules())
stream.filter()
<strong class="bold">Response(data=[StreamRule(value='#lfc', tag=None,</strong>
Â Â Â Â <strong class="bold">Â id='1579970831714844672')], includes={}, errors=[],</strong>
Â Â Â Â <strong class="bold">meta={'sent': '2022-10-12T23:02:31.158Z',</strong>
Â Â Â Â <strong class="bold">'result_count': 1})</strong>
<strong class="bold">RT @TTTLLLKK: Rangers Fans after losing 1 - 7 (SEVEN) to Liverpool. Sad Song </strong>ğŸ¶<strong class="bold"> #LFC #RFC https://t.co/CvTVEGRBU1</strong>
<strong class="bold">Too bad Liverpool aren't in the Scottish league. Strong enough to definitely finish in the Top 4. #afc #lfc</strong>
<strong class="bold">RT @LFCphoto: VAR GOAL CHECK</strong>
<strong class="bold">#LFC #RANLIV #UCL #Elliott @MoSalah https://t.co/7A7MUzW0Pa</strong>
<strong class="bold">Nah we getting cooked on Sunday https://t.co/bUhQcFICUg</strong>
<strong class="bold">RT @LFCphoto: #LFC #RANLIV #UCL https://t.co/6DrbZ2b9NT</strong></pre>
<p class="callout-heading">Note</p>
<p class="callout">See here for more about <a id="_idIndexMarker268"/>Tweepy <span class="No-Break">rules: </span><a href="https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule"><span class="No-Break">https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule</span></a><span class="No-Break">.</span></p>
<p class="callout">Heavy use of the X (Twitter) API may need a <span class="No-Break">paid package.</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor069"/>Other data sources</h1>
<p>We have listed some of the commonly used sources of data in the previous section. However, there are probably many thousands of other free datasets available. You just need to know where to look. The following is a list of some of the interesting ones that we came across as part of our work <a id="_idIndexMarker269"/>on emotion analysis. There are probably many more available all over <span class="No-Break">the internet:</span></p>
<ul>
<li>Dr. Saif Mohammad is a Senior <a id="_idIndexMarker270"/>Research Scientist at the <strong class="bold">National Research Council</strong> (<strong class="bold">NRC</strong>) Canada. He has published many papers and has been heavily involved with <em class="italic">SemEval</em>, as one of the organizers, for many years. He has also published many different, free-for-research purposes datasets that have been used primarily for competition purposes. Many of these are listed on his website at  http://saifmohammad.com/WebPages/SentimentEmotionLabeledData.xhtml, although<a id="_idIndexMarker271"/> some are better described on the associated competition page, as <span class="No-Break">presented here:</span><ul><li>The <strong class="bold">Emotion Intensity</strong> (<strong class="bold">EmoInt</strong>) dataset<a id="_idIndexMarker272"/> has four datasets for four <span class="No-Break">emotions (</span><span class="No-Break">http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.xhtml</span><span class="No-Break">).</span></li><li>The <strong class="bold">Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</strong> (<strong class="bold">WASSA</strong>) dataset is a total of 3,960 English tweets, each <a id="_idIndexMarker273"/>labeled with an emotion of anger, fear, joy, and sadness. Each tweet also has a real-valued score between 0 and 1, indicating the deg<a id="_idTextAnchor070"/>ree or intensity of the emotion felt by the <span class="No-Break">speaker (</span><a href="https://wt-public.emm4u.eu/wassa2017/"><span class="No-Break">https://wt-public.emm4u.eu/wassa2017/</span></a><span class="No-Break">).</span></li><li><em class="italic">SemEval</em> (Mohammad et al., 2018) is an annual competition in which teams of researchers from all over the world work on tasks where they develop systems to categorize datasets. The exact task varies from year to year. It has been running intermittently since 1998, but since 2012, it has become an annual event. A number of datasets have come about from this competition, <span class="No-Break">as follows:</span><ul><li><strong class="bold">2018 Task E-c</strong>: A dataset containing tweets classified as â€œneutral or no emotionâ€ or as 1, or more, of 11 given emotions that best represent the mental state of <span class="No-Break">the tweeter.</span></li><li><strong class="bold">2018 Task EI-reg</strong>: A<a id="_idIndexMarker274"/> dataset containing tweets labeled for emotion (anger, fear, joy, sadness), and for intensity, a real-valued score between 0 and 1, with a score of 1 indicating that the highest amount of emotion was inferred and a score of 0 indicating the lowest amount of emotion was inferred. The authors note that these scores have no inherent meaning; they are only used as a mechanism to convey that the instances with higher scores correspond to a greater degree of emotion than instances with <span class="No-Break">lower scores.</span></li><li><strong class="bold">2018 Task EI-oc</strong>: A dataset <a id="_idIndexMarker275"/>containing tweets labeled for emotion (anger, fear, joy, sadness) and one of four ordinal classes of the intensity of emotion that best represented the mental state of <span class="No-Break">the tweeter.</span>
These datasets are all available <span class="No-Break">from </span><span class="No-Break">https://competitions.codalab.org/competitions/17751</span><span class="No-Break">.</span></li></ul></li><li>Across the competition years, there also seem to be plenty of datasets labeled for sentiment, <span class="No-Break">as follows:</span><ul><li><a href="https://alt.qcri.org/semeval2014/task9/"><span class="No-Break">https://alt.qcri.org/semeval2014/task9/</span></a></li><li><a href="https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools"><span class="No-Break">https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools</span></a></li><li><a href="https://alt.qcri.org/semeval2017/task4/"><span class="No-Break">https://alt.qcri.org/semeval2017/task4/</span></a></li></ul></li><li>The <a id="_idIndexMarker276"/>Hashtag Emotion Corpus dataset contains tweets with emotion word <span class="No-Break">hashtags (</span><a href="http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip"><span class="No-Break">http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip</span></a><span class="No-Break">).</span></li></ul></li>
<li>The <strong class="bold">International Survey On Emotion Antecedents And Reactions</strong> (<strong class="bold">ISEAR</strong>) dataset contains<a id="_idIndexMarker277"/> reports of situations in which student respondents had experienced emotions (joy, fear, anger, sadness, disgust, shame, and <span class="No-Break">guilt) (</span><a href="https://www.unige.ch/cisa/research/materials-and-online-research/research-material/"><span class="No-Break">https://www.unige.ch/cisa/research/materials-and-online-research/research-material/</span></a><span class="No-Break">).</span></li>
<li>A popular dataset labeled for sentiment (negative, neutral, <span class="No-Break">positive) (</span><a href="https://data.mendeley.com/datasets/z9zw7nt5h2"><span class="No-Break">https://data.mendeley.com/datasets/z9zw7nt5h2</span></a><span class="No-Break">).</span></li>
<li>A dataset labeled for six emotions (anger, fear, joy, love, sadness, <span class="No-Break">surprise) (</span><a href="https://github.com/dair-ai/emotion_dataset"><span class="No-Break">https://github.com/dair-ai/emotion_dataset</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Contextualized Affect Representations for Emotion Recognition</strong> (<strong class="bold">CARER</strong>) is an emotion <a id="_idIndexMarker278"/>dataset collected through noisy labels, annotated for six<a id="_idIndexMarker279"/> emotions (anger, fear, joy, love, sadness, and <span class="No-Break">surprise) (</span><a href="https://paperswithcode.com/dataset/emotion"><span class="No-Break">https://paperswithcode.com/dataset/emotion</span></a><span class="No-Break">).</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">It is vitally important to consider data privacy and ethical concerns when using <span class="No-Break">these datasets.</span></p>
<p>We have seen how to access ready-made data sources and how to create your own data source. However, there may be occasions where these are good but not quite what we are looking for. Letâ€™s see how we can tackle <span class="No-Break">that problem.</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor071"/>Transforming data</h1>
<p>Although we have<a id="_idIndexMarker280"/> seen that there are many sentiment and emotion datasets available, it is rare that a dataset meets all the exact requirements. However, there are ways to tackle <span class="No-Break">this problem.</span></p>
<p>We have seen some datasets labeled for sentiment, some for emotion, and also others labeled for more exotic things such as valence that do not seem to fit into what we are looking for: the emotion analysis problem. However, in certain circumstances, it is still possible to repurpose and use these. For example, if a dataset contains emotions, we can transform this into a sentiment dataset simply by assuming that the â€œangerâ€ emotion is a negative sentiment and the â€œjoyâ€ emotion is a positive sentiment. A degree of subjectivity and manual analysis of the individual dataset is then required to determine which emotions would constitute a good substitute for the neutral sentiment. In our experience, this is typically <span class="No-Break">not straightforward.</span></p>
<p><strong class="bold">Data transformation</strong> is the <a id="_idIndexMarker281"/>name given to the process of applying changes to a dataset to make that dataset more useful for your purpose. This could include adding data, removing data, or any process that makes that data more useful. Let us consider some examples from the <em class="italic">Other data sources</em> section and see how we can <span class="No-Break">repurpose them.</span></p>
<p>The EI-reg dataset, as <a id="_idIndexMarker282"/>described previously, contains tweets that were annotated for emotion (anger, fear, joy, sadness) and for intensity with a score between 0 and 1. We can sensibly guess that scores around and under 0.5 are not going to be indicative of highly emotive tweets and hence tweets with scores under 0.5 can be removed and the remaining tweets used to create a reduced, but potentially more <span class="No-Break">useful dataset.</span></p>
<p>The EI-oc dataset<a id="_idIndexMarker283"/> also contained tweets that were annotated for emotion (anger, fear, joy, sadness) and one of four ordinal classes of the intensity of emotion that best represented the mental state of the tweeter, <span class="No-Break">as follows:</span></p>
<ul>
<li><em class="italic">0</em>: No emotion can <span class="No-Break">be inferred</span></li>
<li><em class="italic">1</em>: Low amount of emotion can <span class="No-Break">be inferred</span></li>
<li><em class="italic">2</em>: Moderate amount of emotion can <span class="No-Break">be inferred</span></li>
<li><em class="italic">3</em>: High amount of emotion can <span class="No-Break">be inferred</span></li>
</ul>
<p>Again, we can sensibly guess that by removing tweets with scores lower than 3, we will get a dataset much better tuned to <span class="No-Break">our needs.</span></p>
<p>These are relatively <a id="_idIndexMarker284"/>straightforward ideas of how datasets can be repurposed to extract data that is strongly emotive to create new datasets. However, any good dataset needs to be balanced, so let us now return to the problem of creating neutral tweets and see how this may be done. The following example assumes that the dataset is already downloaded and available; you can download it from <span class="No-Break">here: </span><a href="http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip"><span class="No-Break">http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip</span></a><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">You may get an <strong class="bold">EI-reg-En-train.zip canâ€™t be downloaded securely</strong> error. In that case, simply click the <span class="No-Break"><strong class="bold">Keep</strong></span><span class="No-Break"> option.</span></p>
<p>The code is <span class="No-Break">shown here:</span></p>
<pre class="source-code">
import pandas as pddata_file_path = "EI-reg-En-anger-train.txt"
df = pd.read_csv(data_file_path, sep='\t')
# drop rows where the emotion is not strong
df[df['Intensity Score'] &lt;= 0.2]</pre>
<p>A quick eyeball scan of the results shows tweets such as <span class="No-Break">the following:</span></p>
<table class="T---Table _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="T---Table">
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold">ID</strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold">Tweet</strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold">Affect</strong></span></p>
<p><span class="No-Break"><strong class="bold">Dimension</strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold">Intensity</strong></span></p>
<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">2017-En-40665</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span lang="en-US" xml:lang="en-US">i love the word fret so much and im </span><span class="No-Break" lang="en-US" xml:lang="en-US">in heaven</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">anger</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.127</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">2017-En-11066</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span lang="en-US" xml:lang="en-US">I donâ€™t like pineapple I only eat them on pizza, they lose the sting when they </span><span class="No-Break" lang="en-US" xml:lang="en-US">get cooked.</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">anger</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.192</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">2017-En-41007</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span lang="en-US" xml:lang="en-US">hate to see yâ€™all frown but Iâ€™d rather see him </span><span class="No-Break" lang="en-US" xml:lang="en-US">smiling </span><span class="No-Break" lang="en-US" xml:lang="en-US">ğŸ’•âœ¨</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">anger</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.188</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 â€“ Sample anger tweets with intensity scores &lt; 0.2</p>
<p>We can see that there are<a id="_idIndexMarker285"/> tweets that, although they are low in anger intensity, are high in some other emotion and hence not neutral, and so we need some way to remove these. Clearly, it is the words themselves that tell us whether the tweet is neutral or not (for example, â€œloveâ€ or â€œhateâ€). There are plenty of lists of emotion words freely available on the internet; downloading one (<a href="https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf">https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf</a>) and printing the results shows the <span class="No-Break">following output:</span></p>
<pre class="source-code">
AbusedAdmired
Afraid
.
.
.
Lonely
Loved
Mad
.
.
.</pre>
<p>It is then a trivial <a id="_idIndexMarker286"/>matter, as a first pass, to remove tweets that contain any of these words. However, we can see that while tweet 2017-En-40665 says â€œlove," the emotion words list says â€œLoved." This is problematic because this will prevent the tweet from being flagged as non-neutral. To address this, we simply have to stem or lemmatize (see <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> for further details) both the tweet and the emotion words list, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
stemmer = PorterStemmer()def stem(sentence):
Â Â Â Â res = (" ".join([stemmer.stem(i) for i in
Â Â Â Â Â Â Â Â sentence.split()]))
Â Â Â Â return res
# create some new columns
emotion_words['word_stemmed'] = emotion_words['word']
df['Tweet_stemmed'] = df['Tweet']
# stem the tweets and the emotions words list
df['Tweet_stemmed'] = df['Tweet_stemmed'].apply(stem)
emotion_words['word_stemmed'] = emotion_words[
Â Â Â Â 'word_stemmed'].apply(stem)
# remove tweets that contain an emotion word
res = []
dropped = []
for _, t_row in df.iterrows():
Â Â Â Â tweet = t_row["Tweet_stemmed"]
Â Â Â Â add = True
Â Â Â Â for _, e_row in emotion_words.iterrows():
Â Â Â Â Â Â Â Â emotion_word = e_row["word_stemmed"]
Â Â Â Â Â Â Â Â if emotion_word in tweet:
Â Â Â Â Â Â Â Â Â Â Â Â add = False
Â Â Â Â Â Â Â Â Â Â Â Â break
Â Â Â Â if add:
Â Â Â Â Â Â Â Â res.append(t_row["Tweet"])
Â Â Â Â else:
Â Â Â Â Â Â Â Â dropped.append(t_row["Tweet"])</pre>
<p>When we look at<a id="_idIndexMarker287"/> the results, hereâ€™s what <span class="No-Break">we see:</span></p>
<pre class="source-code">
@Kristiann1125 lol wow i was gonna say really?! haha have you seen chris or nah? you dont even snap me anymore dude!And Republicans, you, namely Graham, Flake, Sasse and others are not safe from my wrath, hence that Hillary Hiney-Kissing ad I saw about you
@leepg \n\nLike a rabid dog I pulled out the backs of my cupboards looking for a bakewell..Found a french fancie &amp;amp; a mini batternburg #Winner!</pre>
<p>These are the tweets that <span class="No-Break">were dropped:</span></p>
<pre class="source-code">
@xandraaa5 @amayaallyn6 shut up hashtags are cool #offendedit makes me so fucking irate jesus. nobody is calling ppl who like hajime abusive stop with the strawmen lmao
Lol Adam the Bull with his fake outrage...</pre>
<p>We can see by and large this simplistic method has done a reasonable job. However, there are still cases that have slipped throughâ€”for example, where the hashtag contained an <span class="No-Break">emotion word:</span></p>
<pre class="source-code">
Ever put your fist through your laptops screen? If so its time for a new one lmao #rage #anger #hp</pre> <p>It is easy to update<a id="_idIndexMarker288"/> the code to catch this. Weâ€™ll leave that as an exercise for you, but the results must be examined carefully to catch edge cases such <span class="No-Break">as this.</span></p>
<p>So far, we have looked solely at English datasets. However, there are many non-English datasets available. These may be useful when the dataset you need either does not exist or does exist but perhaps does not have enough data in it, or is imbalanced, and hence needs to be augmented. This is where we can turn to <span class="No-Break">non-English datasets.</span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor072"/>Non-English datasets</h1>
<p>Often, finding a dataset to train your <a id="_idIndexMarker289"/>model is the most challenging part of the project. There may be occasions where a dataset is available but it is in a different languageâ€”this is where translation can be used to make that dataset useful for your task. There are a number of different ways to translate a dataset, as <span class="No-Break">listed here:</span></p>
<ul>
<li>Ask someone you know, who knows <span class="No-Break">the language</span></li>
<li>Employ a specialist <span class="No-Break">translation company</span></li>
<li>Use an online translation service (e.g. Google Translate) either through the GUI or via <span class="No-Break">an API</span></li>
</ul>
<p>Clearly, the first two are the preferred options; however, they come with an associated cost in terms of effort, time, and money. The third option is also a good option, especially if there is a lot of data that needs translating. However, this option should be used with care because (as we will see) translation services have nuances, and each can produce <span class="No-Break">different results.</span></p>
<p>There are lots of different translation services available (e.g. Google, Bing, Yandex, etc.) and lots of Python packages to utilize these (e.g. TextBlob, Googletrans, translate-api, etc.). We will use <strong class="bold">translate-api</strong> because<a id="_idIndexMarker290"/> it is easy to install, supports lots of translation services, and lets you start translating with only a few lines of code. Consider the <span class="No-Break">following tweet:</span></p>
<p>ØªÙˆØªØ± <span class="No-Break">ÙØ²</span><span class="No-Break"> </span><span class="No-Break">Ù‚Ù„Ø¨ÙŠ</span></p>
<p>First, we need to<a id="_idIndexMarker291"/> install the package, <span class="No-Break">like so:</span></p>
<pre class="source-code">
pip install translators</pre> <p>The code itself is <span class="No-Break">deceptively simple:</span></p>
<pre class="source-code">
import translators as tsphrase = 'ØªÙˆØªØ± ÙØ² Ù‚Ù„Ø¨ÙŠ'
FROM_LANG = 'ar'
TO_LANG = 'en'
text = ts.translate_text(phrase, translator='google',
Â Â Â Â from_language=FROM_LANG , to_language=TO_LANG)
print (res)</pre>
<p>This will produce the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Twitter win my heart</pre> <p>Letâ€™s see what happens when we try one of the other <span class="No-Break">translation providers:</span></p>
<pre class="source-code">
ts.baidutext = ts.translate_text(phrase, translator='bing',
Â Â Â Â from_language=FROM_LANG ,
Â Â Â Â to_language=TO_LANG)print (res)</pre>
<p>The output is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
Tension broke my heart</pre> <p>We can see that<a id="_idIndexMarker292"/> the results are not the same! This is why it is generally a good idea to get translation service results verified by a <span class="No-Break">native speaker.</span></p>
<p>It can be seen, however, that the preceding code can easily be used to translate a complete dataset into English, thus generating new data for <span class="No-Break">our model.</span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor073"/>Evaluation</h1>
<p>Once we have chosen <a id="_idIndexMarker293"/>a dataset, we will want to use it to train a classifier and see how well that classifier works. Assume that we have a dataset stored in the <strong class="source-inline">dataset</strong> variable and a classifier stored in <strong class="source-inline">classifier</strong>. The first thing we have to do is to split the dataset into two partsâ€”one, stored in <strong class="source-inline">training</strong>, to be used for training the classifier, and one, stored in <strong class="source-inline">testing</strong>, for testing it. There are two obvious constraints on the way we do this split, as <span class="No-Break">outlined here:</span></p>
<ul>
<li><strong class="source-inline">training</strong> and <strong class="source-inline">testing</strong> must be disjoint. <strong class="bold">This is essential</strong>. If they are not, then there is a trivial classifier that will get everything 100% correctâ€”namely, just remember all the examples you have seen. Even ignoring this trivial case, classifiers will generally perform better on datasets that they have been trained on than on unseen cases, but when a classifier is deployed in the field, the vast majority of cases will be unknown to it, so testing should always be done on <span class="No-Break">unseen data.</span></li>
<li>The way that data is collected can often introduce bias. To take a simple example, tweets are often collected in chronological orderâ€”that is, tweets that were written on the same day will often occur together in the dataset. But if everyone was very happy about some topic on days 1 to 90 of the collection process and very unhappy about it on days 91 to 100, then there would be lots of happy tweets at the start of the dataset and lots of unhappy ones at the end. If we chose the first 90 days of the dataset for training and the final day for testing, our classifier would probably overstate the likelihood that a tweet in the test set <span class="No-Break">is happy.</span></li>
</ul>
<p>Therefore, it makes sense to randomize the data before splitting it into training and testing sections. However, if we do this, we should make sure that we always randomize it in the same way; otherwise, we will be unable to compare different classifiers on the same data since the randomization will mean that they are being trained and tested on <span class="No-Break">different datasets.</span></p>
<p>There is a third issue to be <a id="_idIndexMarker294"/>considered. If we do not have all that much data, we will want to use as much of it as possible for trainingâ€”in general, the more training data we have, the better our classifier will perform, so we do not want to waste too much of the data on testing. On the other hand, if we do not have much data for testing, then our tests cannot be relied on to give <span class="No-Break">reliable results.</span></p>
<p>The solution is to <a id="_idIndexMarker295"/>use <strong class="bold">cross-fold validation</strong> (sometimes called <strong class="bold">X-fold validation</strong>). We construct a series <a id="_idIndexMarker296"/>of <strong class="bold">folds</strong>, where each fold splits the data<a id="_idIndexMarker297"/> into N-T points for training and T points for testing (N is the size of the whole dataset, while T is the number of points we want to use for testing). If we do this N/T times, using a different set for testing in each fold, we will eventually use all the data points <span class="No-Break">for testing.</span></p>
<p>How big should T be? If we use very small sections for testing, we will have as much data as we can for training for each fold, but we will have to do a lot of rounds of training and testing. If we use large sections for testing, then we will have less data for training for each fold, but we will not have to do as many rounds. Suppose we have a dataset with 1000K data points. If we split it into two folds, using 500K points for training and 500K for testing, we will have quite substantial training sets (the scores for most of the classifiers we will be looking at in the remainder of the book flatten out well <span class="No-Break">before we</span></p>
<p>have 500K training points) and we will use all the data for testing (half of it in the first fold and the other half in the second), and we will only have to do two rounds of training and testing. If, on the other hand, we just have 1,000 data points, splitting it into two folds, each of which has 500 points for training and 500 for testing, will give us very small training sets. It would be better to split it into 10 folds, each with 900 points for training and 100 for testing, or even into 100 folds, each with 990 points for training and 10 for testing, or even 1,000 folds, each with 999 points for training and 1 for testing. Whichever we choose, we will use every point for testing exactly once, but if we use small test sets, we will maximize the size of the training set at the cost of carrying out more rounds of training <span class="No-Break">and testing.</span></p>
<p>The following <strong class="source-inline">makeSplits</strong> function<a id="_idIndexMarker298"/> will divide a dataset into <em class="italic">f</em> folds, each with <em class="italic">N*(1-1/f)</em> points for training and <em class="italic">N/f</em> for testing, and apply a classifier <span class="No-Break">to each:</span></p>
<pre class="source-code">
def makeSplits(dataset, classifier, f):Â Â Â Â scores = []
Â Â Â Â N = len(dataset)/f
Â Â Â Â # Randomize the dataset, but *make sure that you always shuffle
Â Â Â Â # it the same way*
Â Â Â Â random.seed(0)
Â Â Â Â random.shuffle(pruned)
Â Â Â Â for i in range(f):
Â Â Â Â Â Â Â Â # test is everything from i*N to (i+1)*N,
Â Â Â Â Â Â Â Â # train is everything else
Â Â Â Â Â Â Â Â test = dataset[i*N:(i+1)*N]
Â Â Â Â Â Â Â Â train, = dataset[:i*N]+dataset[(i+1)*N:]
Â Â Â Â Â Â Â Â clsf = classifier.train(training)
Â Â Â Â Â Â Â Â score = clsf.test(test)
Â Â Â Â Â Â Â Â scores.append(score)
Â Â Â Â return scores</pre>
<p>In the remainder of this book, we use 10 folds for datasets with fewer than 20K data points and 5 folds for datasets with more than 20K data points. If we have 20K data points, using 5 folds will give us 16K points for training in each fold, which is typically enough to get a reasonable model, so since we will <em class="italic">always</em> eventually use every data point for testing, no matter how many or how few folds we use, this seems like a <span class="No-Break">reasonable compromise.</span></p>
<p>In the preceding definition of <strong class="source-inline">makeSplits</strong>, <strong class="source-inline">classifier.train(training)</strong> trained the classifier and <strong class="source-inline">clsf.test(test)</strong> returned a score. For both of these tasks, we need to know which class each point in the dataset ought to belong toâ€”we need a set of <strong class="bold">Gold Standard</strong> values. Without a set<a id="_idIndexMarker299"/> of Gold Standard values, the training phase does not know what it is supposed to be learning and the testing phase does not know whether the classifier is returning the right results. We will therefore assume that each data point has a Gold Standard label: how can we use these labels to assess<a id="_idIndexMarker300"/> the performance of <span class="No-Break">a classifier?</span></p>
<p>Consider a classifier that is required to assign each data point to one of a set C1, C2, â€¦, Cn of classes, and let <strong class="source-inline">tweet.GS</strong> and <strong class="source-inline">tweet.predicted</strong> be the Gold Standard value and the label assigned by the classifier. There are three possibilities: <strong class="source-inline">tweet.GS</strong> and <strong class="source-inline">tweet.predicted</strong> are the same, they are different, and the classifier simply fails to assign a value to <strong class="source-inline">tweet.predicted</strong>. If the classifier always assigns a value, then it is easy enough to calculate its accuracy since this is just the proportion of all cases that the classifier <span class="No-Break">gets right:</span></p>
<pre class="source-code">
def accuracy(dataset):Â Â Â Â return sum(x.GS==x.predicted for x in
Â Â Â Â Â Â Â Â dataset)/len(dataset)</pre>
<p>However, most classifiers allow for the third optionâ€”that is, in some cases, they can simply not provide an answer. This is a sensible thing to allow: if you ask a person whether one of their friends is happy or not, they might say yes, they might say no, but they might perfectly reasonably say they donâ€™t know. If you donâ€™t have any evidence that tells you whether something belongs to a given class, the only sensible thing to do is to say that you <span class="No-Break">donâ€™t know.</span></p>
<p>This holds true for ML algorithms just as much as it does for people. It is always better for a classifier to say it is uncertain than for it to say the wrong thing. This does, however, make the task of comparing two classifiers less straightforward. Is a classifier that says â€œI donâ€™t knowâ€ in 95% of cases but gets the remaining 5% right better or is it worse than one that never admits it is uncertain but only gets 85% of <span class="No-Break">cases right?</span></p>
<p>There is no single <a id="_idIndexMarker301"/>answer to this question. Suppose that giving a wrong answer would be disastrousâ€”for example, if you thought someone might have taken some poison but the only known antidote is lethal for people who have not taken it. In that case, the classifier that frequently says it doesnâ€™t know but is always right when it does say something is better than the one that always makes a decision but is quite often wrong. If, on the other hand, giving the wrong answer wonâ€™t really matterâ€”for example, if you are considering giving someone statins because you think they might be prone to heart problems, then the one that always makes a decision but is sometimes wrong will be better. If you are likely to have heart problems, then taking statins is a good idea, and doing so when you donâ€™t need to is unlikely to lead to problems. Therefore, we have to be able to combine scores flexibly to allow for <span class="No-Break">different situations.</span></p>
<p>We start by defining four useful parameters, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">True positives</strong> (<strong class="bold">TP</strong>): The number of times that the classifier predicts that a tweet belongs to class <a id="_idIndexMarker302"/>C and the Gold Standard says that it does belong <span class="No-Break">to C.</span></li>
<li><strong class="bold">False positives</strong> (<strong class="bold">FP</strong>): The <a id="_idIndexMarker303"/>number of times that the classifier predicts that a tweet belongs to C and the Gold Standard says that it does not belong <span class="No-Break">to C.</span></li>
<li><strong class="bold">False negatives</strong> (<strong class="bold">FN</strong>): The<a id="_idIndexMarker304"/> number of times that the classifier makes no prediction but the Gold Standard says it does belong to some class (so it is not right, but it is not <span class="No-Break">really wrong).</span></li>
<li><strong class="bold">True negatives</strong> (<strong class="bold">TN</strong>): The <a id="_idIndexMarker305"/>number of times that the classifier makes no prediction and the Gold Standard also says that the item in question does not belong to any of the available classes. In cases where each item belongs to exactly one class, this group is always empty, and it is typically not used in the assessment <span class="No-Break">of classifiers.</span></li>
</ul>
<p>Given these parameters, we can provide a number of metrics, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Precision</strong>: How <a id="_idIndexMarker306"/>often the classifier is right when it makes <span class="No-Break">a predictionâ€”</span><span class="No-Break"><img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="37" src="image/1.png" style="vertical-align:-0.169em;height:0.879em;width:5.568em" width="232"/></span><span class="No-Break">.</span></li>
<li><strong class="bold">Recall</strong>: How <a id="_idIndexMarker307"/>many of the cases where it should make a prediction it makes the <span class="No-Break">right oneâ€”</span><span class="No-Break"><img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="30" src="image/2.png" style="vertical-align:-0.044em;height:0.717em;width:1.557em" width="65"/></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="37" src="image/3.png" style="vertical-align:-0.169em;height:0.879em;width:4.147em" width="173"/></span></span><span class="No-Break">.</span></li>
<li><strong class="bold">F-measure</strong>: As noted <a id="_idIndexMarker308"/>previously, sometimes precision matters more (diagnosing a poison when the antidote is potentially lethal), while sometimes recall matters more (prescribing statins to someone who may have heart problems). F-measure, defined as <img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;Ã—&lt;/mml:mo&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;Ã—&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;Ã—&lt;/mml:mo&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="37" src="image/4.png" style="vertical-align:-0.169em;height:0.879em;width:11.578em" width="482"/> for some <em class="italic">a</em> between 0 and 1, allows us to balance these two: we choose <em class="italic">a</em> to be greater than 0.5 if precision matters more than recall and less than 0.5 if recall matters more than precision. Setting <em class="italic">a</em> equal to 0.5 provides a midway point, usually <span class="No-Break">called F1-measure.</span></li>
<li><strong class="bold">Macro F1</strong> and <strong class="bold">micro F1</strong>: When a task involves several classes, there are two ways of calculating F1. You <a id="_idIndexMarker309"/>can take all the cases where the classifier makes a <a id="_idIndexMarker310"/>decision and use these to calculate P and R, and thence F1, or you can calculate F1 for each class and then take the average over all classes. The first of these is called micro F1 and the second is macro F1. If one class contains many more cases than the others, then micro F1 can be misleading. Suppose that 99% of people with symptoms of poisoning actually have indigestion. Then, a classifier that classifies all <a id="_idIndexMarker311"/>cases of people with symptoms of poisoning actually having indigestion will have overall scores of P = 0.99 and R = 0.99, for a micro F1 score of 0.99. But that means that no one will ever get treated with the antidote: the individual P and R scores would be 0.99 and 1 for indigestion and 0.0, 0.0 for poisoning, for individual F1 scores of 0.99 and 0, averaging out at 0.495. In general, micro F1 gives more weight to the majority classes and provides an overestimate of the scores for the <span class="No-Break">minority cases.</span></li>
<li>The <strong class="bold">Jaccard measure</strong> provides<a id="_idIndexMarker312"/> an alternative way of combining TP, FP, and FN, using <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="28" src="image/5.png" style="vertical-align:-0.000em;height:0.673em;width:1.205em" width="51"/><span class="_-----MathTools-_Math_Variable"><img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="37" src="image/6.png" style="vertical-align:-0.169em;height:0.879em;width:6.070em" width="253"/></span>. Given that simple F1 is easily shown to be the same as <img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.5&lt;/mml:mn&gt;&lt;mml:mo&gt;Ã—&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="37" src="image/7.png" style="vertical-align:-0.169em;height:0.879em;width:9.997em" width="417"/><span class="_-----MathTools-_Math_Base"><img alt="&lt;mml:math  &gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.5&lt;/mml:mn&gt;&lt;mml:mo&gt;Ã—&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="37" src="image/8.png" style="vertical-align:-0.169em;height:0.877em;width:0.345em" width="14"/></span> it is clear that the Jaccard measure and simple F1 will always provide the same ranking, with the Jaccard score always less than F1 (unless they are both 1). There is thus very little to choose between macro F1 and the Jaccard measure, but since some authors use one when comparing classifiers and others the other, we will give macro F1, micro F1, and Jaccard in all tables where we <span class="No-Break">compare classifiers.</span></li>
</ul>
<p>A number of the<a id="_idIndexMarker313"/> datasets we will be looking at allow tweets to have arbitrary numbers of labels. Many tweets express no emotions at all, so we need to allow for cases where the Gold Standard does not assign anything, and quite a few express multiple emotions. We will refer to datasets of this kind as <strong class="bold">multi-label datasets</strong>. This must <a id="_idIndexMarker314"/>be distinguished from datasets where there are several classes (<strong class="bold">multi-class datasets</strong>) and the task is to assign each tweet to exactly one of the options. As we will<a id="_idIndexMarker315"/> see, multi-label datasets are significantly harder to work with than single-label multi-class datasets. As far as metrics are concerned, true negatives (which are not used in any of the metrics given previously) become more significant for multi-label tasks, particularly for tasks where the Gold Standard may assign no labels at all to a tweet. We will look in detail at multi-label tasks in <a href="B18714_10.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Multiclassifiers</em>. For now, we will just note that training and testing should be carried out using multiple folds to make sure that every data point gets used exactly once for testing, with smaller test sets (and hence more folds) for small datasets, and that macro F1 and Jaccard scores are the most useful metrics for <span class="No-Break">comparing classifiers.</span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor074"/>Summary</h1>
<p>There is no doubt that finding suitable data can be a challenge, but there are ways and means to mitigate that. For example, there are plenty of repositories with comprehensive search features that allow you to find <span class="No-Break">relevant datasets.</span></p>
<p>In this chapter, we started by looking at public data sources and went through some of the most popular ones. We saw that many datasets are free, but access to some required a subscription to the repository. Even with the existence of these repositories, there is still sometimes a need to â€œroll your ownâ€ dataset, so we looked at the benefit of doing that and some ways in which we might collect our own data and create our own datasets. We then discussed some niche places to find datasets specific to the emotion analysis problemâ€”for example, from competition websites. Datasets often contain sensitive information about individuals, such as their personal beliefs, behaviors, and mental health status, hence we noted that it is crucial to consider data privacy and ethics concerns when using datasets. We also looked at how we could take datasets that were similar to what we required and how to transform them into something more useful. Finally, we looked at how we could take a non-English dataset and transform it into our target language, and also the problems of <span class="No-Break">doing so.</span></p>
<p>We also considered issues that arise when we are trying to evaluate classifiers, introducing the notion of cross-fold validation and looking at a number of metrics that can be used for assessing classifiers. We noted that splitting the data into a large number of folds, each with a small set for testing, is important when you have a fairly small dataset. Doing, for instance, 10-fold cross-validation is not necessarily more rigorous than using 5 folds: if we have a lot of data, then using a few folds, each with a large amount of test data, is a perfectly reasonable thing to do. We also considered the merits of the various metrics that are most commonly used and decided that since different authors use different metrics it makes sense to report all of them since that makes it possible to compare the scores for a given classifier with scores <span class="No-Break">published elsewhere.</span></p>
<p>In the next chapter, we will look at labeling, key considerations, and some good practices that can improve the effectiveness and accuracy of the process. We will also explore techniques to improve outcomes and look at a simple architecture and UI for the data <span class="No-Break">labeling task.</span></p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor075"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a lo<a id="_idTextAnchor076"/>ok at the <span class="No-Break">following resources:</span></p>
<ul>
<li>Mohammad, S. M., Bravo-Marquez, F., Salameh, M., and Kiritchenko, S. (2018). <em class="italic">SemEval-2018 Task 1: Affect in Tweets</em>. Proceedings of International Workshop on Semantic <span class="No-Break">Evaluation (SemEval-2018).</span></li>
</ul>
</div>
</div></body></html>