<html><head></head><body>
		<div id="_idContainer011">
			<h1 id="_idParaDest-14" class="chapter-number"><a id="_idTextAnchor013"/>1</h1>
			<h1 id="_idParaDest-15">On Cybersecurity and Machine Learning</h1>
			<p>With the dawn of the Information Age, cybersecurity has become a pressing issue in today’s society and a skill that is much sought after in industry. Businesses, governments, and individual users are all at risk of security attacks and breaches. The fundamental goal of cybersecurity is to keep users and their data safe. Cybersecurity is a multi-faceted problem, ranging from highly technical domains (cryptography and network attacks) to user-facing domains (detecting hate speech or fraudulent credit card transactions). It helps to prevent sensitive information from being corrupted, avoid financial fraud and losses, and safeguard users and their devices from <span class="No-Break">harmful actors.</span></p>
			<p>A large part of cybersecurity analytics, investigations, and detections are now driven by <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>)and “smart” systems. Applying data science and ML to the security space presents a unique set of challenges: the lack of sufficiently labeled data, limitations on powerful models due to the need for explainability, and the need for nearly perfect precision due to high-stakes scenarios. As we progress through the book, you will learn how to handle these critical tasks and apply your ML and data science skills to <span class="No-Break">cybersecurity problems.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>The basics <span class="No-Break">of cybersecurity</span></li>
				<li>An overview of <span class="No-Break">machine learning</span></li>
				<li>Machine learning – cybersecurity versus <span class="No-Break">other domains</span></li>
			</ul>
			<p>By the end of this chapter, you will have built the foundation for <span class="No-Break">further projects.</span></p>
			<h1 id="_idParaDest-16">The basics of cybersecurity</h1>
			<p>This book aims to <a id="_idIndexMarker000"/>marry two important fields of research: cybersecurity and ML. We will present a brief overview of cybersecurity, how it is defined, what the end goals are, and what <span class="No-Break">problems arise.</span></p>
			<h2 id="_idParaDest-17">Traditional principles of cybersecurity</h2>
			<p>The fundamental aim of <a id="_idIndexMarker001"/>cybersecurity is to keep users and data safe. Traditionally, the <a id="_idIndexMarker002"/>goals of cybersecurity were three-fold: <strong class="bold">confidentiality, integrity, and availability</strong>, the <span class="No-Break"><strong class="bold">CIA</strong></span><span class="No-Break"> triad.</span></p>
			<p>Let us now examine each of these <span class="No-Break">in depth.</span></p>
			<h3>Confidentiality</h3>
			<p>The confidentiality goal aims to keep data secret from unauthorized parties. Only authorized entities should have<a id="_idIndexMarker003"/> access <span class="No-Break">to data.</span></p>
			<p>Confidentiality can be achieved by encrypting data. Encryption is a process where plain-text data is coded into a ciphertext using an encryption key. The ciphertext is not human-readable; a corresponding decryption key is needed to decode the data. Encryption of information being sent over networks prevents attackers from reading the contents, even if they intercept the communication. Encrypting data at rest ensures that adversaries will not be able to read your data, even if the physical data storage is compromised (for example, if an attacker breaks into an office and steals <span class="No-Break">hard drives).</span></p>
			<p>Another approach to ensuring confidentiality is access control. Controlling access to information is the first step in preventing unauthorized exposure or sharing of data (whether intentional or not). Access to data should be granted by following the principle of least privilege; an individual or application must have access to the minimum data that it requires to perform its function. For example, only the finance division in a company should have access to revenue and transaction information. Only system administrators should be able to view network and <span class="No-Break">access logs.</span></p>
			<p>ML can help in detecting abnormal access, suspicious behavior patterns, or traffic from malicious sources, thus ensuring that confidentiality is maintained. For example, suppose an administrator suddenly starts accessing confidential/privileged files outside of the regular pattern for themselves or administrators in general. An ML model may flag it as anomalous and set off alarms so that the administrator can <span class="No-Break">be investigated.</span></p>
			<h3>Integrity</h3>
			<p>The integrity goal ensures that data is trustworthy and tamper-free. If the data can be tampered with, there is no <a id="_idIndexMarker004"/>guarantee of its authenticity and accuracy; such data cannot <span class="No-Break">be trusted.</span></p>
			<p>Integrity can be ensured using <a id="_idIndexMarker005"/>hashing and checksums. A <strong class="bold">checksum</strong> is a numerical value computed by applying a hash function to the data. Even if a single bit of the data were to change, the checksum would change. Digital signatures and certificates also facilitate integrity; once an email or code library has been digitally signed by a user, it cannot be changed; any change requires a new <span class="No-Break">digital signature.</span></p>
			<p>Attackers may wish to compromise the integrity of a system or service for their gain. For example, an attacker may intercept incoming requests to a banking server and change the destination account for any money transfer. Malicious browser extensions may redirect users to a site for gaming traffic and advertising statistics; the original destination URL entered by the user was tampered with. By analyzing the patterns in data coupled with other signals, ML models can detect <span class="No-Break">integrity breaches.</span></p>
			<h3>Availability</h3>
			<p>The first two goals ensure that data is kept <a id="_idIndexMarker006"/>secret, tamper-free, and safe from attackers. However, these guarantees are meaningless if authorized users cannot access the data as needed. The availability goal ensures that information is always available to legitimate users of <span class="No-Break">a system.</span></p>
			<p>Attackers may try to compromise <a id="_idIndexMarker007"/>availability by executing a <strong class="bold">denial-of-service</strong> (<strong class="bold">DoS</strong>) attack, where the target service is bombarded with incoming requests from unauthorized or dummy nodes. For example, an attacker may send millions of dummy queries to a database server. While the attacker may not be able to actually extract any useful information, the server is so overwhelmed that legitimate queries by authorized users are never executed. Alternatively, an attacker may also degrade availability by physically <span class="No-Break">destroying data.</span></p>
			<p>Availability can be guaranteed by implementing redundancy in data and services. Regular backup of data ensures that it is still available even if one copy is destroyed or tampered with. If multiple API endpoints to achieve the same functionality are present, legitimate users can switch to another endpoint, and availability will be preserved. Similar to confidentiality and integrity, pattern analysis and classification algorithms can help detect DoS attacks. An emerging<a id="_idIndexMarker008"/> paradigm, graph neural networks, can help detect coordinated bot attacks known as <strong class="bold">distributed denial of </strong><span class="No-Break"><strong class="bold">service</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DDoS</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-18">Modern cybersecurity – a multi-faceted issue</h2>
			<p>The CIA triad focuses solely <a id="_idIndexMarker009"/>on cybersecurity for data. However, cybersecurity today extends far beyond just data and servers. Data stored in servers becomes information <a id="_idIndexMarker010"/>used by organizations, which is transformed into the applications that are ultimately used by humans. Cybersecurity encompasses all these aspects, and there are different problem areas in <span class="No-Break">each aspect.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> shows how varied and multi-faceted the issue of cybersecurity is and the various elements <span class="No-Break">it encompasses:</span></p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B19327_01_01.jpg" alt="Figure 1.1 – Various problem areas in cybersecurity"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Various problem areas in cybersecurity</p>
			<p>In the following sections, we will discuss some common security-related problems and research areas in four broad categories: data security, information security, application security, and <span class="No-Break">user security.</span></p>
			<h3>Data security</h3>
			<p>Data in its raw format is stored on hard drives<a id="_idIndexMarker011"/> in offices or in the cloud, which is eventually stored in physical machines in data centers. At the data level, the role of cybersecurity is to keep the data safe. Thus, the focus is on maintaining confidentiality, integrity, and availability (the three goals of the CIA triad) of the data. Cybersecurity problems at this level focus on novel cryptographic schemes, lightweight encryption algorithms, fault tolerance systems, and complying with regulations for <span class="No-Break">data retention.</span></p>
			<h3>Information security</h3>
			<p>Data from data centers and the cloud is transformed into information, which is used by companies to build various <a id="_idIndexMarker012"/>services. At the information level, the role of cybersecurity is to ensure that the information is being accessed and handled correctly by employees. Problems at this level focus on network security administration, detecting policy violations and insider threats, and ensuring that there is no inadvertent leakage of <span class="No-Break">private data.</span></p>
			<h3>Application security</h3>
			<p>Information is transformed into a<a id="_idIndexMarker013"/> form suitable for consumers using a variety of services. An example of this is information about Facebook users being transformed into a list of top friend recommendations. At the application level, the role of cybersecurity is to ensure that the application cannot be compromised. Problems at this level focus on malware detection, supply chain attack detection, anomaly detection, detecting bot and automated accounts, and flagging phishing emails and <span class="No-Break">malicious URLs.</span></p>
			<h3>User security</h3>
			<p>Finally, applications are used by human<a id="_idIndexMarker014"/> end users, and the role of cybersecurity here is to ensure the safety of these users. Problems at this level include detecting hate speech, content moderation, flagging fraudulent transactions, characterizing abusive behavior, and protecting users from digital crimes (identity theft, scams, and extortion). Note that this aspect goes beyond technical elements of cybersecurity and enters into the realm of humanities, law, and the <span class="No-Break">social sciences.</span></p>
			<p>We will now present a <a id="_idIndexMarker015"/>case study to explain more clearly how security problems arise at each level (data, information, application, <span class="No-Break">and user).</span></p>
			<h3>A study on Twitter</h3>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> shows the threat model in the four broad levels we described. Let us understand this concretely using the<a id="_idIndexMarker016"/> example of Twitter. Twitter is a social media networking platform where users can post short opinions, photos, and videos and interact with the posts <span class="No-Break">of others.</span></p>
			<p>At the data level, all of the data (posts, login credentials, and so on) are stored in the raw form. An adversary may try to physically break into data centers or try to gain access to this data using malicious injection queries. At the information level, Twitter itself is using the data to run analytics and train its predictive models. An adversary may try to harvest employee credentials via a phishing email or poison models with <span class="No-Break">corrupt data.</span></p>
			<p>At the application level, the analytics are being transformed into actionable insights consumable by end users: recommendation lists, news feeds, and top-ranking posts. An adversary may create bot accounts that spread misinformation or malicious extensions that redirect users outside of Twitter. Finally, at the user level, end users actually use the app to tweet. Here, an adversary may try to attack users with hate speech or <span class="No-Break">abusive content.</span></p>
			<p>So far, we have discussed cybersecurity and looked at various cybersecurity problems that occur. We will now turn to a related but slightly different <span class="No-Break">topic: privacy.</span></p>
			<h2 id="_idParaDest-19">Privacy</h2>
			<p>The terms security and privacy are<a id="_idIndexMarker017"/> often confused and used interchangeably. However, the goals of security and privacy are very different. While security aims to secure data and systems, privacy refers to individuals having full control over their data. When it comes to privacy, every individual should know <span class="No-Break">the following:</span></p>
			<ul>
				<li>What data is being collected (location, app usage, web tracking, and <span class="No-Break">health metrics)</span></li>
				<li>How long it will be retained for (deleted immediately or in <span class="No-Break">a week/month/year)</span></li>
				<li>Who can access it (advertisers, research organizations, <span class="No-Break">and governments)</span></li>
				<li>How it can be deleted (how to make a request to <span class="No-Break">the app)</span></li>
			</ul>
			<p>Security and privacy are<a id="_idIndexMarker018"/> interrelated. If an attacker hacks into a hospital or medical database (a breach of security), then they may have access to sensitive patient data (a breach of privacy). There <a id="_idIndexMarker019"/>are numerous laws around the world, such as the <strong class="bold">General Data Protection Regulation </strong>(<strong class="bold">GDPR</strong>) in Europe, that mandate strict security controls in order to ensure <span class="No-Break">user privacy.</span></p>
			<p>Because ML relies on a lot of collected data, there has been a push for privacy-preserving ML. We will be discussing some techniques for this in <span class="No-Break">later chapters.</span></p>
			<p>This completes our discussion of cybersecurity. We started by describing the traditional concepts of security, followed by various cybersecurity problems at multiple levels. We also presented a case study on Twitter that helps put these problems in context. Finally, we looked at privacy, a closely <span class="No-Break">related topic.</span></p>
			<p>Next, we will turn to the second element in the <span class="No-Break">book: ML.</span></p>
			<h1 id="_idParaDest-20">An overview of machine learning</h1>
			<p>In this section, we will <a id="_idIndexMarker020"/>present a brief overview of ML principles and techniques. The traditional computing paradigm defines an algorithm as having three elements: the input, an output, and a process that specifies how to derive the output from the input. For example, in a credit card detection system, a module to flag suspicious transactions may have transaction metadata (location, amount, type) as input and the flag (suspicious or not) as output. The process will define the rule to set the flag based on the input, as shown in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B19327_01_02.jpg" alt="Figure 1.2 – Traditional input-process-output model for fraud detection"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Traditional input-process-output model for fraud detection</p>
			<p>ML is a drastic change to the input-process-output philosophy. The traditional approach defined computing as deriving the output by applying the process to the input. In ML, we are given the input and output, and the task is to derive the process that connects <span class="No-Break">the two.</span></p>
			<p>Continuing our analogy of the credit card fraud detection system, we will now be provided with a dataset that has<a id="_idIndexMarker021"/> the input features and output flags. Our task is to learn how the flag can be computed based on the input. Once we learn the process, we can generalize it to new data that comes our way in the traditional input-process-output way. Most of the security-related problems we will deal with in this book are <em class="italic">classification</em> problems. Classification is a task in which data points are assigned discrete, categorical labels (fraud/non-fraud <span class="No-Break">or low-risk/medium-risk/high-risk).</span></p>
			<p>ML is not a one-step process. There are multiple steps involved, such as cleaning and preparing the data in the right form, training and tuning models, deploying them, and monitoring them. In the next section, we will look at the major steps involved in <span class="No-Break">the workflow.</span></p>
			<h2 id="_idParaDest-21">Machine learning workflow</h2>
			<p>Now, we will see some of the basic steps that go<a id="_idIndexMarker022"/> into end-to-end ML. It begins with preprocessing the data to make it fit for use by ML models and ends with monitoring and tuning<a id="_idIndexMarker023"/> <span class="No-Break">the models.</span></p>
			<h3>Data preprocessing</h3>
			<p>The first step in any ML experiment is to<a id="_idIndexMarker024"/> process the data into a format suitable for ML. Real-world data is often not suitable to be used directly by a model due to two <span class="No-Break">main reasons.</span></p>
			<p>The first reason is variability in format. Data around us is in multiple formats, such as numbers, images, text, audio, and video. All of these need to be converted into numerical representations for a model to consume them. Preprocessing would convert images into matrices, text into word embeddings, and audio streams into time series. Some features of data are discrete; they represent categorical variables. For example, in a dataset about users, the <strong class="source-inline">Country</strong> field would take on string values such as <strong class="source-inline">India</strong>, <strong class="source-inline">Canada</strong>, <strong class="source-inline">China</strong>, and so on. Preprocessing converts such categorical variables into a numerical vector form that can be consumed by <span class="No-Break">a model.</span></p>
			<p>The second reason is noise in real-world data. Measurement inaccuracies, processing errors, and human errors can cause corrupt values to be recorded. For example, a data entry operator might enter your<a id="_idIndexMarker025"/> age as 233 instead of 23 by mistakenly pressing the <em class="italic">3</em> key twice. A web scraper collecting data may face network issues and fail to make a request; some fields in the data will then <span class="No-Break">be missing.</span></p>
			<p>In a nutshell, preprocessing removes noise, handles missing data, and transforms data into a format suitable for consumption by an <span class="No-Break">ML model.</span></p>
			<h3>The training phase</h3>
			<p>After the data has been suitably preprocessed, we train a model to learn from the data. A model expresses the relationship<a id="_idIndexMarker026"/> between the preprocessed features and some target variables. For example, a model to detect phishing emails will produce as output a probability that an email is malicious based on the input features we define. A model that classifies malware as 1 of 10 malware families will output a 10-dimensional vector of probability distribution over the 10 classes. In the training phase, the model will learn the parameters of this relationship so that the error over the training data is minimized (that is, it is able to predict the labels for the training data as closely as possible). During the training phase, a subset of the data is reserved as validation data to examine the error of the trained model over <span class="No-Break">unseen data.</span></p>
			<p>One of the problems that a model can face is overfitting; if a model learns parameters too specific to the training data, it cannot generalize well to newer data. This can be diagnosed by comparing the performance of the model on training and validation data; a model that overfits will show decreasing loss or error over the training data but a non-decreasing loss on validation data. The opposite problem to this also exists – the model can suffer from underfitting where it is simply not able to learn from the <span class="No-Break">training data.</span></p>
			<h3>The inferencing phase</h3>
			<p>Once a model has been trained, we <a id="_idIndexMarker027"/>want to use it to make predictions for new, unseen data. There is no parameter learning in this phase; we simply plug in the features from the data and inspect the prediction made by the model. Inferencing often happens in real time. For example, every time you use your credit card, the parameters of your transaction (amount, location, and category) are used to run inferencing on a fraud detection model. If the model flags the transaction as <a id="_idIndexMarker028"/>suspicious, the transaction <span class="No-Break">is declined.</span></p>
			<h3>The maintenance phase</h3>
			<p>ML models need continuous monitoring and tuning so that their performance does not degrade. A model may become <a id="_idIndexMarker029"/>more error-prone in its predictions as time passes because it has been trained on older data. For example, a model to detect misinformation trained in 2019 would have never been exposed to fake news posts about the COVID-19 pandemic. As a result, it never learned the characteristics of COVID-related fake news and may fail to recognize such articles as misinformation. To avoid this, a model has to be retrained on newer data at a regular cadence so that it learns <span class="No-Break">from it.</span></p>
			<p>Additionally, new features may become available that could help improve the performance of the model. We would then need to train the model with the new feature set and check the performance gains. There may be different slices of data in which different classification thresholds show higher accuracy. The model then has to be tuned to use a different threshold in each slice. Monitoring models is an ongoing process, and there are <a id="_idIndexMarker030"/>automated tools (called MLOps tools) that offer functionality for continuous monitoring, training, tuning, and alerting <span class="No-Break">of models.</span></p>
			<p>Now, we will look at the fundamental ML paradigms: supervised and <span class="No-Break">unsupervised learning.</span></p>
			<h2 id="_idParaDest-22">Supervised learning</h2>
			<p>ML has two major flavors: supervised <a id="_idIndexMarker031"/>and unsupervised. In <strong class="bold">supervised learning</strong>, we have<a id="_idIndexMarker032"/> access to labeled data. From the labeled data, we<a id="_idIndexMarker033"/> can learn the relation between the data and the labels. The most fundamental example of a supervised learning algorithm is <span class="No-Break"><strong class="bold">linear regression</strong></span><span class="No-Break">.</span></p>
			<h3>Linear regression</h3>
			<p>Linear regression assumes that the<a id="_idIndexMarker034"/> target variable can be expressed as a linear function of the features. We <a id="_idIndexMarker035"/>initially start with a linear equation with arbitrary coefficients, and we tune these coefficients as we learn from the data. At a high level and in the simplest form, linear regression<a id="_idIndexMarker036"/> works <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Let <em class="italic">y</em> be the target variable and <em class="italic">x</em><span class="subscript">1</span>, <em class="italic">x</em><span class="subscript">2</span>, and <em class="italic">x</em><span class="subscript">3</span> be the predictor features. Assuming a linear relationship, our model is <em class="italic">y = a</em><span class="subscript">0    </span>+<span class="subscript"> </span>a<span class="subscript">1</span>x<span class="subscript">1   </span> + a<span class="subscript">2</span>x<span class="subscript">2    </span>+<span class="subscript"> </span>a<span class="subscript">3</span>x<span class="subscript">3</span>. Here, <em class="italic">a</em><span class="subscript">0</span>, <em class="italic">a</em><span class="subscript">1</span>, <em class="italic">a</em><span class="subscript">2</span>, and <em class="italic">a</em><span class="subscript">3</span> are parameters initially set <span class="No-Break">to random.</span></li>
				<li>Consider the first data point from the training set. It will have its own set of predictors (<em class="italic">x</em><span class="subscript">1</span>, <em class="italic">x</em><span class="subscript">2</span>, and <em class="italic">x</em><span class="subscript">3 </span>) and the target as ground truth (<em class="italic">y</em>). Calculate a predicted value of the target using the equation defined previously; call this predicted <span class="No-Break">value </span><span class="No-Break"><em class="italic">y’</em></span><span class="No-Break">.</span></li>
				<li>Calculate a <em class="italic">loss</em> that indicates the error of the prediction. Typically, we use the <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) error as the<a id="_idIndexMarker037"/> loss function. It is simply the square of the difference between the actual and predicted value of the target:<span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">y</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break">.</span></li>
				<li>The loss, <em class="italic">L</em>, tells us how far off our prediction is from the actual value. We use the loss to modify the parameters of our model. This part is the one where we <em class="italic">learn</em> from <span class="No-Break">the data.</span></li>
				<li>Repeat <em class="italic">steps 2</em>, <em class="italic">3</em>, and <em class="italic">4</em> over each data point from the training set, and update the parameters as you go. This completes one <em class="italic">epoch</em> <span class="No-Break">of training.</span></li>
				<li>Repeat the<a id="_idIndexMarker038"/> preceding steps for <span class="No-Break">multiple epochs.</span></li>
				<li>In the end, your<a id="_idIndexMarker039"/> parameters will have been tuned to capture the linear relationship between the features and <span class="No-Break">the target.</span></li>
			</ol>
			<p>We will now look at gradient descent, the algorithm that is the heart and soul of linear regression (and many <span class="No-Break">other algorithms).</span></p>
			<h3>Gradient descent</h3>
			<p>The crucial step in the preceding instructions is <em class="italic">step 4</em>; this is the step where we update the parameters based on the loss. This is typically done using an algorithm called <strong class="bold">gradient descent</strong>. Let <em class="italic">θ</em> be the <a id="_idIndexMarker040"/>parameters of the model; we want to<a id="_idIndexMarker041"/> choose the optimal values for <em class="italic">θ</em> such that the loss is as small as possible. We calculate the gradient, which is the derivative of the loss with respect to the parameters. We update the parameter <em class="italic">θ</em> to its new value <em class="italic">θ’</em> based on the gradient <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">η</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">L</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Space"> </span></p>
			<p>The <span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">L</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span> gradient represents the slope of the tangent to the loss curve at that particular value of <em class="italic">θ</em>. The sign of the gradient will tell us the direction in which we need to change θ in order to reach minima on the loss. We always move in the direction of descending gradient to minimize the loss. For a clearer understanding, carefully observe the loss curve plotted against the parameter in the <span class="No-Break">following chart:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B19327_01_03.jpg" alt="Figure 1.3 – Traversing the loss curve using gradient descent"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Traversing the loss curve using gradient descent</p>
			<p>Suppose because of our<a id="_idIndexMarker042"/> random selection, we select the <em class="italic">θ</em><span class="subscript">1</span> parameter. When we<a id="_idIndexMarker043"/> calculate the gradient of the curve (the slope of the tangent to the curve at point A), it will be negative. Applying the previous equation of gradient descent, we will have to calculate the gradient and add it to <em class="italic">θ</em><span class="subscript">1</span> to get the new value (<span class="No-Break">say </span><span class="No-Break"><em class="italic">θ</em></span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">).</span></p>
			<p>We continue this process until we reach point E, where the gradient is very small (nearly 0); this is the minimum loss we can reach, and even if we were to continue the gradient descent process, the updates would be negligible (because of the very small and near-zero value of the gradient). Had we started at point H instead of A, the gradient we calculated would have been positive. In that case, according to the gradient descent equation, we would have had to decrease <em class="italic">θ</em> to reach the minimum loss. Gradient descent ensures that we always move down the curve to reach <span class="No-Break">the minima.</span></p>
			<p>An important element in the equation that we have not discussed so far is <em class="italic">η</em>. This parameter is called the <strong class="bold">learning rate</strong>. The gradient <a id="_idIndexMarker044"/>gives us the direction in which we want to change <em class="italic">θ</em>. The learning rate tells us by how much we want to change <em class="italic">θ</em>. A smaller value of <em class="italic">η</em> means that we are making very small changes and thus taking small steps to reach the minima; it may take a long time to find the optimal values. On the other hand, if we choose a very large value for the learning rate, we may miss <span class="No-Break">the minima.</span></p>
			<p>For example, because of a large learning rate, we might jump directly from point D to F without ever getting to E. At point F, the direction of the gradient changes, and we may jump in the opposite direction<a id="_idIndexMarker045"/> back to D. We will keep oscillating between these two points without reaching <span class="No-Break">the minima.</span></p>
			<p>In the linear regression <a id="_idIndexMarker046"/>algorithm discussed previously, we performed the gradient descent process for every data point in the training data. This is known as <strong class="bold">stochastic gradient descent</strong>. In another, more efficient version of the algorithm, we consider batches of data, aggregate the loss values, and apply gradient descent over the <a id="_idIndexMarker047"/>aggregated loss. This is known as <strong class="bold">batch </strong><span class="No-Break"><strong class="bold">gradient descent</strong></span><span class="No-Break">.</span></p>
			<p>Gradient descent is at the core of most modern ML algorithms. While we have described it in the context of linear regression, it is simply an optimization algorithm and is widely used in other models such as deep neural networks <span class="No-Break">as well.</span></p>
			<h2 id="_idParaDest-23">Unsupervised learning</h2>
			<p>In supervised learning, we<a id="_idIndexMarker048"/> had ground truth data to learn the relationship between the features and labels. The goal of unsupervised learning is to <a id="_idIndexMarker049"/>discover patterns, trends, and relationships within data. Unsupervised learning allows us to make predictions and detect anomalies without having access to labels during training. Let us look at clustering, a popular unsupervised <span class="No-Break">learning problem.</span></p>
			<h3>Clustering</h3>
			<p>As the name suggests, clustering is<a id="_idIndexMarker050"/> the process of grouping data into clusters. It allows us to examine how the data points group together, what the common characteristics in those groups are, and what the hidden trends in the data are. There is no <em class="italic">learning</em> involved in clustering. An ideal clustering is when the intra-cluster similarity is high and inter-cluster similarity is low. This means that points within a cluster are very similar to one another and very different from points in <span class="No-Break">other clusters.</span></p>
			<p>The most fundamental clustering<a id="_idIndexMarker051"/> algorithm is called <strong class="bold">K-means clustering</strong>. It partitions the data points into <em class="italic">k</em> clusters, where <em class="italic">k</em> is a parameter that has to be preset. The general process of the K-means algorithm is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Select <em class="italic">K</em> points randomly from the training data. These points will be the centroids of <span class="No-Break">our clusters.</span></li>
				<li>For each point, calculate the distance to each of the centroids. The distance metric generally <a id="_idIndexMarker052"/>used is <span class="No-Break"><strong class="bold">Euclidean distance</strong></span><span class="No-Break">.</span></li>
				<li>Assign each point to <a id="_idIndexMarker053"/>one of the <em class="italic">K</em> clusters based on the distance. A point will be assigned to the centroid that is closest to it (<span class="No-Break">minimum distance).</span></li>
				<li>Each of the <em class="italic">K</em> centroids will now have a set of points associated with it; this forms a cluster. For each cluster, calculate the updated cluster centroids as a mean of all the points assigned to <span class="No-Break">that cluster.</span></li>
				<li>Repeat <em class="italic">steps 2–4</em> until one of the <span class="No-Break">following occurs:</span><ul><li>The cluster assignment in <em class="italic">step 3</em> does <span class="No-Break">not change</span></li><li>A fixed number of repetitions of the steps <span class="No-Break">have passed</span></li></ul></li>
			</ol>
			<p>The core concept behind this algorithm is to optimize the cluster assignment so that the distance of each point to the centroid of the cluster it belongs to is small; that is, clusters should be as tightly knit <span class="No-Break">as possible.</span></p>
			<p>We have looked at supervised and unsupervised learning. A third variant, semi-supervised learning, is the middle ground between <span class="No-Break">the two.</span></p>
			<h2 id="_idParaDest-24">Semi-supervised learning</h2>
			<p>As the name suggests, semi-supervised <a id="_idIndexMarker054"/>learning is the middle <a id="_idIndexMarker055"/>ground between supervised and unsupervised learning. Often (and especially in the case of critical security applications), labels are not available or are very few in number. Manual labeling is expensive as it requires both time and expert knowledge. Therefore, we have to train a model from only these <span class="No-Break">limited labels.</span></p>
			<p>Semi-supervised learning techniques are generally based on a self-training approach. First, we train a supervised model based on the small subset of labeled data. We then run inferencing on this model to obtain predicted labels on the unlabeled data. We use our original labels, and high-confidence predicted labels together to train the final model. This process can be repeated for a fixed number of iterations or until we reach a point where the model performance does not <span class="No-Break">change significantly.</span></p>
			<p>Another approach to<a id="_idIndexMarker056"/> semi-supervised learning is called <strong class="bold">co-training</strong>, where we jointly train two models with different views (feature sets) of the data. We begin by independently training two models based on different feature sets and available labels. We then apply the models to make predictions for the unlabeled data and obtain pseudo labels. We<a id="_idIndexMarker057"/> add the high-confidence labels from the first model to the training set of the second and vice versa. We repeat the process of training models and obtain pseudo labels for a fixed number of iterations or until we reach a point where the performance of both models does not <span class="No-Break">change significantly.</span></p>
			<p>Now that we have covered the major paradigms in ML, we can turn to evaluate the performance <span class="No-Break">of models.</span></p>
			<h2 id="_idParaDest-25">Evaluation metrics</h2>
			<p>So far, we have mentioned <a id="_idIndexMarker058"/>model performance in passing. In this section, we will <a id="_idIndexMarker059"/>define formal metrics for evaluating the performance of a model. As most of the problems we deal with will be classification-based, we will discuss metrics specific to <span class="No-Break">classification only.</span></p>
			<h3>A confusion matrix</h3>
			<p>Classification involves predicting the label<a id="_idIndexMarker060"/> assigned to a particular data point. Consider<a id="_idIndexMarker061"/> the case of a fraud detection model. As can be seen in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em>, if the actual and predicted label of a data point are both <strong class="bold">Fraud</strong>, then we call that example a <strong class="bold">True Positive (TP)</strong>. If both are <strong class="bold">Non-Fraud</strong>, we call it a <strong class="bold">True Negative (TN)</strong>. If the predicted label is <strong class="bold">Fraud</strong>, but the actual label (expected from ground truth) is <strong class="bold">Non-Fraud</strong>, then we call it a <strong class="bold">False Positive (FP)</strong>. Finally, if the predicted label is <strong class="bold">Non-Fraud</strong>, but the actual label (expected from ground truth) is <strong class="bold">Fraud</strong>, we call it a <strong class="bold">False </strong><span class="No-Break"><strong class="bold">Negative (FN)</strong></span><span class="No-Break">.</span></p>
			<p>Based on the predicted and the<a id="_idIndexMarker062"/> actual label of data, we can construct what is called a <span class="No-Break"><strong class="bold">confusion matrix</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B19327_01_04.jpg" alt="Figure 1.4 – Confusion matrix"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Confusion matrix</p>
			<p>The confusion matrix <a id="_idIndexMarker063"/>provides a quick way to visually inspect the performance of the model. A good model will show the highest true positives and negatives, while<a id="_idIndexMarker064"/> the other two will have smaller values. The confusion matrix allows us to conveniently calculate metrics relevant to classification, namely the accuracy, precision, recall, and F-1 score. Let us learn more about these metrics <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: This is a measure of how accurate the model predictions are across both classes. Simply put, it is the <a id="_idIndexMarker065"/>number of examples for which the model is able to predict the labels correctly, that is, the proportion of true positives and negatives in the data. It can be calculated <span class="No-Break">as follows:</span></li>
			</ul>
			<p><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">_____________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<ul>
				<li><strong class="bold">Precision</strong>: This is a measure of the confidence of the model in the positive predictions. In simple terms, it is the<a id="_idIndexMarker066"/> proportion of the true positives in all the examples predicted as positive. Precision answers the question: Of all the examples predicted as fraud, how many of them are actually fraud? Precision can be calculated as the <span class="No-Break">following formula:</span></li>
			</ul>
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<ul>
				<li><strong class="bold">Recall</strong>: This is a measure of the <a id="_idIndexMarker067"/>completeness of the model in the positive class. Recall answers the question: Of all the examples that were fraud, how many could the model correctly detect as fraud? Recall can be calculated <span class="No-Break">as follows:</span></li>
			</ul>
			<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>When a model has high precision, it means that most of the predicted fraud is actually fraud. When a model has a high recall, it means that out of all the fraud in the data, most of it is<a id="_idIndexMarker068"/> predicted by the model as fraud. Consider a model that predicts <em class="italic">everything</em> as fraud. This model has a high recall; as everything is marked as fraud, naturally, all the actual fraud is also being captured, but at the cost of a large number of false positives (low precision). Alternatively, consider a model that predicts <em class="italic">nothing</em> as fraud. This model has a high precision; as it marks nothing as fraud, there are no false positives, but at the cost of a large number of false negatives (low recall). This trade-off between precision and recall is a classic problem <span class="No-Break">in ML.</span></p>
			<ul>
				<li><strong class="bold">F-1 Score</strong>: This measure captures the <a id="_idIndexMarker069"/>degrees of both precision and recall. High precision comes at the cost of low recall and vice versa. The F-1 score is used to identify the model that has the highest precision and recall together. Mathematically, it is defined as the harmonic mean of the precision and recall, calculated <span class="No-Break">as follows:</span></li>
			</ul>
			<p><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">_____________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<ul>
				<li><strong class="bold">Receiver Operating Characteristic (ROC) curve</strong>: We have seen that classification models assign a label to a data point. In reality, however, the models compute a probability that the<a id="_idIndexMarker070"/> data point belongs to a particular class. The probability is compared with a threshold to <a id="_idIndexMarker071"/>determine whether the example belongs to that class or not. Typically, a threshold of 0.5 is used. So, in our example of the fraud detection model, if the model outputs a value greater than 0.5, we will classify the data point as <strong class="bold">Fraud</strong>. A smaller value will lead to us classifying it <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Non-Fraud</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>The threshold probability is a<a id="_idIndexMarker072"/> parameter, and we can tune this parameter to achieve high precision or recall. If you set a very low threshold, the bar to meet for an example to be fraudulent is very low; for example, at a threshold of 0.1, nearly all examples will be classified as fraud. This means that we will be catching all the fraud out there; we have a <span class="No-Break">high recall.</span></p>
			<p>On the other hand, if you set a very high threshold, the bar to meet will be very high. Not all fraud will be caught, but you can be sure that whatever is marked as fraud is definitely fraud. In other words, you have high precision. This trade-off between precision and recall is captured in the ROC curve, as shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B19327_01_05.jpg" alt="Figure 1.5 – ROC curve"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – ROC curve</p>
			<p>The ROC <a id="_idIndexMarker073"/>curve plots the <strong class="bold">True Positive Rate</strong> (<strong class="bold">TPR</strong>) against the <strong class="bold">False Positive Rate</strong> (<strong class="bold">FPR</strong>) for every value of the threshold<a id="_idIndexMarker074"/> from 0 to 1. It allows us to observe the precision we have to tolerate to achieve a certain recall and vice versa. In the ROC plotted in the preceding diagram, points <strong class="bold">A</strong> and <strong class="bold">B</strong> indicate the true positive and negative rates that we have to tolerate at two different <a id="_idIndexMarker075"/>thresholds. The <strong class="bold">Area under the ROC Curve</strong> (<strong class="bold">AUC</strong>), represented by the shaded area, provides an overall measure of the performance of the model across all threshold <a id="_idIndexMarker076"/>values. The AUC can be interpreted as the probability that a model scores a random positive example (fraud) higher than a random negative one (non-fraud). When comparing multiple models, we choose the one with the highest value of <span class="No-Break">the AUC.</span></p>
			<p>We have looked at the fundamental concepts behind ML, the underlying workflow, and the evaluation approaches. We will now examine why the application of ML in cybersecurity is different from other fields and the novel challenges <span class="No-Break">it poses.</span></p>
			<h1 id="_idParaDest-26">Machine learning – cybersecurity versus other domains</h1>
			<p>ML today is applied to a wide<a id="_idIndexMarker077"/> variety of domains, some of which are detailed in the <span class="No-Break">following list:</span></p>
			<ul>
				<li>In sales and <a id="_idIndexMarker078"/>marketing, to identify the segment of customers likely to buy a <span class="No-Break">particular product</span></li>
				<li>In online advertising, for click prediction and to display <span class="No-Break">ads accordingly</span></li>
				<li>In climate and weather forecasting, to predict trends based on centuries <span class="No-Break">of data</span></li>
				<li>In recommendation systems, to find the best items (movies, songs, posts, and people) relevant to <span class="No-Break">a user</span></li>
			</ul>
			<p>While every sector imaginable applies ML today, the nuances of it being applied to cybersecurity are different from other fields. In the following subsections, we will see some of the reasons why it is much more challenging to apply ML to the cybersecurity domain than to other domains such as sales <span class="No-Break">or advertising.</span></p>
			<h3>High stakes</h3>
			<p>Security problems often involve making crucial decisions that can impact money, resources, and even life. A fraud detection model that performs well has the potential to save millions of dollars in<a id="_idIndexMarker079"/> fraud. A botnet or malware detection model can save critical systems (such as in the military) or sensitive data (such as in healthcare) from being compromised. A model to flag abusive users on social media can potentially save someone’s life. Because the stakes are so high, the precision-recall trade-off becomes even <span class="No-Break">more crucial.</span></p>
			<p>In click fraud detection, we have to operate at high precision (or else we would end up marking genuine clicks as fraud), and this comes at the cost of poor recall. Similarly, in abuse detection, we must operate at high recall (we want all abusers to be caught), which comes at the cost of high false positives (low precision). In use cases such as click prediction or targeting, the worst thing that might happen because of a poor model is a few non-converting clicks or mistargeted advertisements. On the other hand, models for security must be thoroughly tuned and diligently monitored as the stakes are much higher than in <span class="No-Break">other domains.</span></p>
			<h3>Lack of ground truth</h3>
			<p>Most well-studied ML methods are supervised in nature. This means that they depend on ground-truth labels to learn the <a id="_idIndexMarker080"/>model. In security problems, the ground truth is not always available, unlike in other domains. For example, in a task such as customer targeting or weather forecasting, historical information can tell us whether a customer actually purchased a product or whether it actually rained; this can be used as <span class="No-Break">ground truth.</span></p>
			<p>However, such obvious ground truth is not available in security problems such as fraud or botnet detection. We depend on expert annotation and manual investigations to label data, which is a resource-heavy task. In addition, the labels are based on human knowledge and heuristics (i.e., does this seem fraudulent?) and not on absolute truth such as the ones we described for customer targeting or weather prediction. Due to the lack of high-confidence labels, we often have to rely on unsupervised or semi-supervised techniques for <span class="No-Break">security applications.</span></p>
			<h3>The need for user privacy</h3>
			<p>In recent years, there has been a<a id="_idIndexMarker081"/> push to maintain user privacy. As we have discussed previously, with increased privacy comes reduced utility. This proves to be particularly challenging in security tasks. The inability to track users across websites hampers our ability to detect click fraud. A lack of permissions to collect location information will definitely reduce the signals we have access to detect credit card fraud. All of these measures preserve user privacy by avoiding undue tracking and profiling; however, they also degrade the performance of <a id="_idIndexMarker082"/>security models, which are actually for the benefit of the user. The fewer signals available to us, the lower the fidelity in the prediction of <span class="No-Break">our models.</span></p>
			<h3>The need for interpretability</h3>
			<p>Most powerful ML models (neural<a id="_idIndexMarker083"/> networks, transformers, and graph-based learning models) operate as a black box. The predictions made by the model lack interpretability. Because these models learn higher-order features, it is not possible for a human to understand and explain why a particular example is classified the way <span class="No-Break">it is.</span></p>
			<p>In security applications, however, explainability is important. We need to justify each prediction, at least for the positive class. If a transaction is flagged as fraudulent, the bank or credit card company needs to understand what went into the prediction in order to ascertain the truth. If users are to be blocked or banned, the platform may need adequate justification, more convincing than a model prediction. Classical ML algorithms such as decision trees and logistic regression provide some interpretation based on tree structure and coefficients, respectively. This need for interpretability in models is an obstacle to using state-of-the-art deep learning methods <span class="No-Break">in security.</span></p>
			<h3>Data drift</h3>
			<p>The cybersecurity landscape is an ever-changing one, with new attack strategies coming up every day. The nature of attackers<a id="_idIndexMarker084"/> and attack vectors is constantly evolving. As a result, there is also a change in the features that the model expects if data at inference time is significantly different in nature from that which the model was trained on. For example, an entirely new variant of malware may not be detected by a model as no examples of this variant were in the training data. A fake news detection model trained in 2019 may not be able to recognize COVID-19-related misinformation as it was never trained on that data. This data drift makes it challenging to build models that have sustained performance in <span class="No-Break">the wild.</span></p>
			<p>These problems (lack of labels, data drift, and privacy issues) also arise in other applications of ML. However, in the <a id="_idIndexMarker085"/>case of systems for cybersecurity, the stakes are high, and the consequences of an incorrect prediction can be devastating. These issues, therefore, are more challenging to handle <span class="No-Break">in cybersecurity.</span></p>
			<h1 id="_idParaDest-27">Summary</h1>
			<p>This introductory chapter provided a brief overview of cybersecurity and ML. We studied the fundamental goals of traditional cybersecurity and how those goals have now evolved to capture other tasks such as fake news, deep fakes, click spam, and fraud. User privacy, a topic of growing importance in the world, was also introduced. On the ML side, we covered the basics from the ground up: beginning with how ML differs from traditional computing and moving on to the methods, approaches, and common terms used in ML. Finally, we also highlighted the key differences in ML for cybersecurity that make it so much more challenging than other fields. The coming chapters will focus on applying these concepts to designing and implementing ML models for security issues. In the next chapter, we will discuss how to detect anomalies and network attacks <span class="No-Break">using ML.</span></p>
		</div>
	</body></html>