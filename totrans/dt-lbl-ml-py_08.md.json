["```py\nimport cv2\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nnum_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nframe_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \\\n    int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\nprint(\"FPS: \", fps)\nprint(\"Number of frames: \", num_frames)\nprint(\"Frame size: \", frame_size)\n```", "```py\nimport cv2\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\nfor i in range(10):\n    ret, frame = cap.read()\n    if not ret:\n        break\n    cv2.imshow(\"Frame\", frame)\n    cv2.waitKey(0)\ncap.release()\ncv2.destroyAllWindows()\n```", "```py\nimport cv2\nimport matplotlib.pyplot as plt\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\nhistograms = []\n```", "```py\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    histogram = cv2.calcHist([frame], [0, 1, 2], \\\n        None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    histogram = cv2.normalize(histogram, None).flatten()\n    histograms.append(histogram)\ncap.release()\n```", "```py\n# Example of optical flow calculation\nprev_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\nnext_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\nflow = cv2.calcOpticalFlowFarneback(prev_frame, \\\n    next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n```", "```py\n# Example of feature tracking using Shi-Tomasi corner detection and Lucas-Kanade optical flow\ncorners = cv2.goodFeaturesToTrack(prev_frame, \\\n    maxCorners=100, qualityLevel=0.01, minDistance=10)\nnext_corners, status, err = cv2.calcOpticalFlowPyrLK(\\\n    prev_frame, next_frame, corners, None)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Generate sample data\nframe_indices = np.arange(0, 100)\nframe_intensities = np.random.randint(0, 255, size=100)\n```", "```py\n# Frame Visualization\nplt.figure(figsize=(10, 6))\nplt.title(\"Frame Visualization\")\nplt.xlabel(\"Frame Index\")\nplt.ylabel(\"Intensity\")\nplt.plot(frame_indices, frame_intensities)\nplt.show()\n```", "```py\n# Temporal Visualization\ntimestamps = np.linspace(0, 10, 100)\nplt.figure(figsize=(10, 6))\nplt.title(\"Temporal Visualization\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Intensity\")\nplt.plot(timestamps, frame_intensities)\nplt.show()\n```", "```py\n# Motion Visualization\ndx = np.random.randn(100)\ndy = np.random.randn(100)\nplt.figure(figsize=(6, 6))\nplt.title(\"Motion Visualization\")\nplt.quiver(frame_indices, frame_indices, dx, dy)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n```", "```py\n    input_video_dir = \"<your_path>/PacktPublishing/DataLabeling/ch08/kmeans/kmeans_input\"\n    input_video, _ = load_videos_from_directory(input_video_dir)\n    ```", "```py\n    hist_features = extract_histogram_features( \\\n        input_video.reshape(-1, 64, 64, 3))\n    ```", "```py\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(hist_features)\n    ```", "```py\n    kmeans = KMeans(n_clusters=2, random_state=42)\n    predicted_labels = kmeans.fit_predict(scaled_features)\n    print(\"Predicted Labels:\", predicted_labels)\n    ```", "```py\n# Flatten the video_data array to iterate through frames\nflattened_video_data = input_video.reshape(-1, \\\n    input_video.shape[-3], input_video.shape[-2], \\\n    input_video.shape[-1])\n# Create two separate output directories for clusters\noutput_directory_0 = \"/<your_path>/kmeans_output/Cluster_0\"\noutput_directory_1 = \"/<your_path>/kmeans_output/Cluster_1\"\nos.makedirs(output_directory_0, exist_ok=True)\nos.makedirs(output_directory_1, exist_ok=True)\n# Iterate through each frame, save frames in the corresponding cluster folder\nfor idx, (frame, predicted_label) in enumerate( \\\n    zip(flattened_video_data, predicted_labels)):\n    cluster_folder = output_directory_0 if predicted_label == 0 else output_directory_1\n    frame_filename = f\"video_frame_{idx}.png\"\n    frame_path = os.path.join(cluster_folder, frame_filename)\n    cv2.imwrite(frame_path, (frame * 255).astype(np.uint8))\n```", "```py\n# Visualize a few frames from each cluster\nnum_frames_to_visualize = 2\nfor cluster_label in range(2):\n    cluster_folder = os.path.join(\"./kmeans/kmeans_output\", \\\n        f\"Cluster_{cluster_label}\")\n    frame_files = os.listdir(cluster_folder)[:num_frames_to_visualize]\n    for frame_file in frame_files:\n        frame_path = os.path.join(cluster_folder, frame_file)\n        frame = cv2.imread(frame_path)\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n        plt.title(f\"Cluster {cluster_label}\")\n        plt.axis(\"off\")\n        plt.show()\n```", "```py\nimport cv2\nimport numpy as np\n# Read a video file\ncap = cv2.VideoCapture('/<your_path>/CricketBowling.mp4')\n# Initialize Lucas-Kanade optical flow\nlk_params = dict(winSize=(15, 15), maxLevel=2, \\\n    criteria=(cv2.TERM_CRITERIA_EPS |\n    cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n```", "```py\nret, frame1 = cap.read()\nprvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\nprvs_points = cv2.goodFeaturesToTrack(prvs, maxCorners=100, \\\n    qualityLevel=0.3, minDistance=7)\n```", "```py\nwhile True:\n    ret, frame2 = cap.read()\n    if not ret:\n        break\n    next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n    # Calculate optical flow\n    next_points, status, err = cv2.calcOpticalFlowPyrLK( \\\n        prvs, next_frame, prvs_points, None, **lk_params)\n```", "```py\n    # Draw motion vectors on the frame\n    mask = np.zeros_like(frame2)\n    for i, (new, old) in enumerate(zip(next_points, prvs_points)):\n        a, b = new.ravel().astype(int)\n        c, d = old.ravel().astype(int)\n        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n        frame2 = cv2.circle(frame2, (a, b), 5, (0, 0, 255), -1)\n    result = cv2.add(frame2, mask)\n    cv2.imshow('Motion Analysis', result)\n```", "```py\n    # Break the loop on 'Esc' key\n    if cv2.waitKey(30) & 0xFF == 27:\n        break\n```", "```py\ncap.release()\ncv2.destroyAllWindows()\n```", "```py\nimport cv2\n# Create a KCF tracker\ntracker = cv2.TrackerKCF_create()\n```", "```py\n# Read a video file\ncap = cv2.VideoCapture('./PacktPublishing/DataLabeling/ch08/video_dataset/CricketBowling.mp4')\n```", "```py\n# Read the first frame\nret, frame = cap.read()\nbbox = cv2.selectROI('Select Object to Track', frame, False)\n```", "```py\ntracker.init(frame, bbox)\n```", "```py\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n```", "```py\n     # Update the tracker\n    success, bbox = tracker.update(frame)\n```", "```py\n  if success:\n        p1 = (int(bbox[0]), int(bbox[1]))\n        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n```", "```py\n  cv2.imshow('Object Tracking', frame)\n```", "```py\n    # Break the loop on 'Esc' key\n    if cv2.waitKey(30) & 0xFF == 27:\n        break\n```", "```py\ncap.release()\ncv2.destroyAllWindows()\n```", "```py\nfrom deepface import DeepFace\nimport cv2\n# Load a sample image\nimg_path1 = './PacktPublishing/DataLabeling/ch08/data/pic1.jpeg'\nimg_path2 = './PacktPublishing/DataLabeling/ch08/data/pic2.jpeg'\nimg = cv2.imread(img_path)\n# Perform facial recognition\nresult = DeepFace.verify(img1_path=img_path1, img2_path=img_path2)\n# Display the result\nprint(\"Are these faces the same person? \", result[\"verified\"])\n# Additional information\nprint(\"Facial recognition result:\", result)\n```", "```py\nAre these faces the same person? True\nFacial recognition result: {'verified': True, 'distance': 0.20667349278322178, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 74, 'y': 50, 'w': 713, 'h': 713}, 'img2': {'x': 63, 'y': 8, 'w': 386, 'h': 386}}, 'time': 0.48}\n```"]