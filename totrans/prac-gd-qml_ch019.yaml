- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章
- en: Quantum Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络
- en: '*The mind is not a vessel to be filled, but a fire to be kindled.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*心灵不是要被填满的容器，而是一团需要被点燃的火焰。*'
- en: — Plutarch
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ——普鲁塔克
- en: 'In the previous chapter, we explored our first family of quantum machine learning
    models: quantum support vector machines. Now it is time for us to take one step
    further and consider yet another family of models, that of **Quantum** **Neural
    Networks** (**QNNs**).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了我们的第一个量子机器学习模型家族：量子支持向量机。现在是我们进一步探索另一个模型家族的时候了，那就是**量子** **神经网络**（**QNNs**）。
- en: 'In this chapter, you will learn how the notion of a quantum neural network
    can arise naturally from the ideas behind classical neural networks. Of course,
    you will also learn how quantum neural networks work and how they can be trained.
    Then, you will explore how quantum neural networks can actually be implemented,
    run, and trained using the two quantum frameworks that we have been working with
    so far: Qiskit and PennyLane.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习量子神经网络的概念如何自然地从经典神经网络背后的思想中产生。当然，你还将学习量子神经网络的工作原理以及它们的训练方法。然后，你将探索如何使用我们迄今为止一直在使用的两个量子框架——Qiskit和PennyLane——来实现、运行和训练量子神经网络。
- en: 'These are the contents of this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容如下：
- en: Building and training quantum neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练量子神经网络
- en: Quantum neural networks in PennyLane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLane中的量子神经网络
- en: 'Quantum neural networks in Qiskit: a commentary'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskit中的量子神经网络：评论
- en: Quantum support vector machines and quantum neural networks are probably the
    two most popular families of QML models, so, by the end of this chapter, you will
    already have a solid foundation in quantum machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 量子支持向量机和量子神经网络可能是QML模型中最受欢迎的两个家族，所以，到本章结束时，你将已经在量子机器学习方面打下坚实的基础。
- en: To get started, let’s understand how quantum neural networks work and how they
    can be effectively trained. Let’s get to it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们了解量子神经网络是如何工作的，以及它们如何被有效地训练。让我们着手吧！
- en: 10.1 Building and training a quantum neural network
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.1 构建和训练一个量子神经网络
- en: Just like quantum support vector machines, quantum neural networks are what
    we called ”CQ models” back in *Chapter* [*8*](ch017.xhtml#x1-1390008), *What is
    Quantum Machine* *Learning?*, — models with purely classical inputs and outputs
    that use quantum computing at some stage. However, unlike QSVMs, quantum neural
    networks are not a ”particular case” of any classical model, although their behavior
    is inspired by that of classical neural networks. What is more, as we will soon
    see, quantum neural networks are ”purely quantum” models, in the sense that their
    execution will only require classical computing for the preparation of circuits
    and the statistical analysis of measurements. Nevertheless, just like QSVMs, quantum
    neural networks will depend on classical parameters that will be optimized classically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就像量子支持向量机一样，量子神经网络是我们之前在*第8章*[*8*](ch017.xhtml#x1-1390008)，“什么是量子机器学习？”中提到的“CQ模型”——即纯粹使用经典输入和输出，并在某个阶段使用量子计算的模型。然而，与QSVMs不同，量子神经网络不是任何经典模型的“特殊情况”，尽管它们的行为受到经典神经网络行为的启发。更重要的是，正如我们很快就会看到的，量子神经网络是“纯粹量子”模型，这意味着它们的执行将仅需要经典计算来准备电路和进行测量统计分析。尽管如此，就像QSVMs一样，量子神经网络将依赖于经典参数，这些参数将通过经典优化进行优化。
- en: To learn more…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多...
- en: As you surely know by now, (quantum) machine learning is a vast field in which
    terms hardly ever have a unique meaning. The term ”quantum neural network” can,
    in practice, be used to refer to any QML model that is inspired by the behavior
    of a classical neural network. Therefore, you should bear in mind that people
    may also use this name to refer to models different from the ones that we are
    considering to be quantum neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如你此刻所知，(量子)机器学习是一个术语含义几乎不唯一的广阔领域。在实践中，“量子神经网络”这个术语可以用来指代任何受经典神经网络行为启发的QML模型。因此，你应该记住，人们也可能使用这个名称来指代与我们考虑的量子神经网络不同的模型。
- en: That should be enough of an introduction. Let’s now get into the details. What
    actually are quantum neural networks and how do they relate to classical neural
    networks?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该已经足够作为介绍了。现在让我们深入细节。量子神经网络究竟是什么，它们是如何与经典神经网络相关的？
- en: 10.1.1 A journey from classical neural networks to quantum neural networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.1 从经典神经网络到量子神经网络的旅程
- en: 'If we do a small exercise of abstraction, we can think of the action of a classical
    neural network as consisting of the following stages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行一次抽象的小练习，我们可以将经典神经网络的动作看作由以下阶段组成：
- en: '**Data preparation**: This simply amounts to taking some (classical) input
    data and maybe carrying out some (simple) transformations on it. These may include
    normalizing or scaling the input data.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：这仅仅是将一些（经典）输入数据和可能对其进行的某些（简单）转换。这些可能包括对输入数据进行归一化或缩放。'
- en: '**Data processing**: Feeding the data through a sequence of layers that ”transform”
    the data as it flows through them. The behavior of this processing depends on
    some optimizable parameters, which are adjusted in training.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：通过一系列层将数据传递过去，这些层“转换”数据，随着数据流过它们。这种处理的行为取决于一些可优化的参数，这些参数在训练中会被调整。'
- en: '**Data output**: Returning the output through a final layer.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据输出**：通过最终层返回输出。'
- en: Let’s see how we can take this scheme and use it to define an analogous quantum
    model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何可以将这个方案用于定义一个类似的量子模型。
- en: '**Data preparation**: Quantum neural networks are given classical inputs (in
    the form of an array of numbers), but quantum computers don’t work on classical
    data — they work on quantum states! So how can we take these classical inputs
    and embed them into the space of quantum states?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：量子神经网络接收经典输入（以数字数组的形式），但量子计算机并不处理经典数据——它们处理量子状态！那么我们如何将这些经典输入嵌入到量子状态空间中呢？'
- en: That is a problem that we have already dealt with in *Section* [*9.2*](ch018.xhtml#x1-1660009.2).
    In order to encode the classical input of a QNN into a quantum state, we just
    have to use any feature map of our choice. As you know, we may also need to normalize
    or scale the data, of course.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们已经在 *第9.2节* 中处理过的问题。为了将QNN的经典输入编码成量子状态，我们只需要使用我们选择的任何特征映射。正如你所知，我们当然可能还需要对数据进行归一化或缩放。
- en: 'And that is how we actually ”prepare the data” for a quantum neural network:
    feeding it into a feature map.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正是这样，我们实际上为量子神经网络“准备数据”：将其输入到特征映射中。
- en: '**Data processing**: At this point, we have successfully transformed our classical
    input into a ”quantum input,” in the form of a quantum state that encodes our
    classical data according to a certain feature map. Now, we need to figure out
    a way to process this input by drawing some inspiration from the processing in
    a classical neural network.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：在这个阶段，我们已经成功地将我们的经典输入转换成了“量子输入”，即以量子状态的形式编码我们的经典数据，根据某个特征映射。现在，我们需要找出一种方法来处理这个输入，可以从经典神经网络的处理中汲取一些灵感。'
- en: Trying to replicate the full, exact behavior of a classical neural network in
    a quantum neural network might prove not to be ideal given the state of current
    quantum hardware. Instead, we can look at the bigger picture.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在当前量子硬件的状态下，试图在量子神经网络中复制经典神经网络的完整、精确的行为可能并不理想。相反，我们可以从更大的图景来看。
- en: In essence, the processing stage of a classical neural network consists in the
    application of some transformations that depend, exclusively, on some optimizable
    parameters. And that is an idea that we can very easily export to a quantum computer.
    We can simply define the ”processing” stage of a quantum neural network as…the
    application of a circuit that depends on some optimizable parameters! In addition
    to this, as we will see later in this section, this circuit can be structured
    in layers in a way that somewhat reassembles the spirit of a classical neural
    network. This circuit will be said to be a **variational form** — they are just
    like the ones we studied back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE:*
    *Variational Quantum Eigensolver*.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本质上，经典神经网络的处理阶段包括应用一些仅依赖于某些可优化参数的转换。这是一个我们可以非常容易地移植到量子计算机上的想法。我们可以简单地将量子神经网络的“处理”阶段定义为…依赖于某些可优化参数的电路的应用！此外，正如我们将在本节后面看到的那样，这个电路可以被分层结构化，以某种方式重新组合经典神经网络的精髓。这个电路将被称为**变分形式**——它们就像我们在
    *第7章* 中研究的那些一样，*VQE：变分量子本征值求解器*。
- en: '**Data output**: Once we have a processed state, we need to return a classical
    output. And this shall be the result of some measurement operation; this operation
    can be whichever one suits our problem best!'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据输出**：一旦我们有一个处理过的状态，我们需要返回一个经典输出。这将是某些测量操作的结果；这个操作可以是适合我们问题的最佳选择！'
- en: For instance, if we wanted to build a binary classifier with a quantum neural
    network, a natural choice for this measurement operation could be, for example,
    taking the expectation value of the first qubit when measured on the computational
    basis. Remember that the expectation value of a qubit simply corresponds to the
    probability of obtaining ![1](img/file13.png "1") upon measuring the qubit on
    the computational basis.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们想用量子神经网络构建一个二元分类器，这个测量操作的一个自然选择可能是，例如，在计算基上测量第一个量子比特时的期望值。记住，量子比特的期望值简单地对应于在计算基上测量量子比特获得![1](img/file13.png
    "1")的概率。
- en: And those are all the ingredients that make up a quantum neural network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是构成量子神经网络的所有成分。
- en: 'As a matter of fact, feature maps and variational forms are both examples of
    **variational circuits**: quantum circuits that are controlled by some classical
    parameters. The only actual difference between a feature map and a variational
    form is their purpose: feature maps depend on the input data and are used to encode
    it, while variational forms depend on optimizable parameters and are used to transform
    a quantum input state.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，特征图和变分形式都是**变分电路**的例子：受某些经典参数控制的量子电路。特征图和变分形式之间的唯一实际区别是它们的目的：特征图依赖于输入数据，用于对其进行编码，而变分形式依赖于可优化参数，用于将量子输入状态进行转换。
- en: This difference in purpose will materialize in the fact that we will often use
    different circuits for feature maps and variational forms. A good feature map
    need not be a good variational form, and vice versa.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种目的上的差异将体现在我们经常会为特征图和变分形式使用不同的电路。一个好的特征图不一定是一个好的变分形式，反之亦然。
- en: 'You should keep in mind that — like all things QML — the terms ”feature map”
    and ”variational form” are not entirely universal, and different authors may refer
    to them with different expressions. For example, variational forms are commonly
    referred to as **ansatzs**, as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational* *Quantum Eigensolver*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '你应该记住——就像所有量子机器学习（QML）的东西一样——“特征图”和“变分形式”这两个术语并不完全通用，不同的作者可能会用不同的表达来指代它们。例如，变分形式通常被称为**ansatzs**，正如我们在*第7章*[*7*](ch015.xhtml#x1-1190007)
    *VQE: 变分量子本征值求解器*中做的那样。'
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A quantum neural network takes a classical input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") and maps it to a quantum state through a feature map
    ![F](img/file1320.png "F"). The resulting state then goes through a variational
    form ![V](img/file379.png "V"): a variational circuit dependent on some optimizable
    parameters ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}").
    The output of the quantum neural network is the result of a measurement operation
    on the final state. All this can be seen, schematically, in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络将经典输入 ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    通过特征图 ![F](img/file1320.png "F") 映射到量子状态。然后，得到的量子状态通过变分形式 ![V](img/file379.png
    "V")：一个依赖于某些可优化参数 ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}")
    的变分电路。量子神经网络的输出是对最终状态的测量操作的结果。所有这些都可以在以下图中 schematically 看到：
- en: '![ n⃗ |FV0⟩((⃗x𝜃)) ](img/file1322.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![ n⃗ |FV0⟩((⃗x𝜃))] (img/file1322.jpg)'
- en: Thanks to our study of quantum support vector machines, we are already very
    familiar with feature maps, but we have yet to get acquainted with variational
    forms; that is what we will devote the next subsection to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们对量子支持向量机的研究，我们已经非常熟悉特征图，但我们还没有熟悉变分形式；这正是我们将致力于下一小节的内容。
- en: 10.1.2 Variational forms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.2 变分形式
- en: In principle, a variational form could be any variational circuit of your choice,
    but, in general, variational forms for QNNs follow a ”layered structure,” trying
    to mimic the spirit of classical neural networks. We can now make this idea precise.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，变分形式可以是任何你选择的变分电路，但通常，QNN的变分形式遵循“分层结构”，试图模仿经典神经网络的精髓。我们现在可以精确地阐述这个想法。
- en: If we wanted to define a variational form with ![k](img/file317.png "k") layers,
    we could consider ![k](img/file317.png "k") vectors of independent parameters
    ![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}").
    In order to define each layer ![j](img/file258.png "j"), we may take a variational
    circuit ![G_{j}](img/file1324.png "G_{j}") dependent on the parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}"). A common approach is to prepare variational
    forms by stacking these variational circuits consecutively and separating them
    by some circuits ![U_{}](img/file1326.png "U_{}")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要定义一个具有![k](img/file317.png "k")层的变分形式，我们可以考虑![k](img/file317.png "k")个独立参数的向量![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}")。为了定义每一层![j](img/file258.png
    "j")，我们可能需要一个依赖于参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png
    "G_{j}")。一种常见的方法是通过连续堆叠这些变分电路并使用一些电路![U_{}](img/file1326.png "U_{}")来准备变分形式
- en: entˆt![,independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.](img/file1327.png
    ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 纠缠，独立于任何参数，旨在在量子比特之间创建纠缠。正如![图10.1](img/file1327.png ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")所示。
- en: '![Figure 10.1: A variational form with k layers, each defined by a variational
    circuit G_{j} dependent on some parameters {\overset{\rightarrow}{\theta}}_{j}.
    The circuits U_{} entˆtareusedtocreateentanglement,andthestate \left| \psi_{}
    \right. enc\rangle denotes the output of the feature map ](img/file1331.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1：一个具有k层的变分形式，每个层由一个依赖于某些参数{\overset{\rightarrow}{\theta}}_{j}的变分电路G_{j}定义。电路U_{}用于创建纠缠，状态\left|
    \psi_{} \right. enc\rangle表示特征图的输出](img/file1331.jpg)'
- en: '**Figure 10.1**: A variational form with ![k](img/file317.png "k") layers,
    each defined by a variational circuit ![G_{j}](img/file1324.png "G_{j}") dependent
    on some parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}").
    The circuits ![U_{}](img/file1326.png "U_{}") entˆt![areusedtocreateentanglement,andthestate](img/file1328.png
    "areusedtocreateentanglement,andthestate") ![\left| \psi_{} \right.](img/file1329.png
    "\left| \psi_{} \right.") enc![\rangle](img/file1330.png "\rangle") denotes the
    output of the feature map'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.1**：一个具有![k](img/file317.png "k")层的变分形式，每个层由一个依赖于某些参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png "G_{j}")定义。电路![U_{}](img/file1326.png
    "U_{}")用于创建纠缠，状态![\left| \psi_{} \right.](img/file1329.png "\left| \psi_{} \right.")![\rangle](img/file1330.png
    "\rangle")表示特征图的输出'
- en: 'We have now outlined one of the most common structures of variational forms,
    but variational forms are best illustrated by examples. There are lots of variational
    forms out there, and there is no way we could collect them all in this book —
    in truth, there would be no point either. For this reason, we will restrict ourselves
    to presenting just three variational forms, some of which we will use later in
    the book:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了变分形式中最常见的结构之一，但变分形式最好通过例子来说明。变分形式有很多，我们不可能在这本书中收集它们所有——实际上，这样做也没有意义。因此，我们将限制自己只介绍三种变分形式，其中一些我们将在本书的后面部分使用：
- en: '**Two-local**: The **two-local variational form** with ![k](img/file317.png
    "k") repetitions on ![n](img/file244.png "n") qubits relies on ![n \times (k +
    1)](img/file1332.png "n \times (k + 1)") optimizable parameters, which we will
    denote as ![\theta_{rj}](img/file1333.png "\theta_{rj}") with ![r = 0,\ldots,k](img/file1334.png
    "r = 0,\ldots,k") and ![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n"). Its
    circuit is constructed as per the following procedure:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双局部**：在![n](img/file244.png "n")个量子比特上重复![k](img/file317.png "k")次的**双局部变分形式**依赖于![n
    \times (k + 1)](img/file1332.png "n \times (k + 1)")个可优化参数，我们将用![\theta_{rj}](img/file1333.png
    "\theta_{rj}")表示，其中![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k")和![j =
    1,\ldots n](img/file1335.png "j = 1,\ldots n")。其电路的构建按照以下步骤进行：'
- en: '**procedure** TwoLocal(![n,k,\theta](img/file1336.png "n,k,\theta"))'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过程** TwoLocal([![n,k,\theta](img/file1336.png "n,k,\theta"))](img/file1336.png
    "n,k,\theta")'
- en: '**for all** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **do**'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **执行**'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Add the* ![r](img/file1337.png
    "r")*-th* *layer.*     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *添加* ![r](img/file1337.png
    "r")*-层。     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**'
- en: Apply a ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") gate on
    qubit ![j](img/file258.png "j").
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子位 ![j](img/file258.png "j") 上应用 ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})")
    门。
- en: '-'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Create entanglement
    between layers.*     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *在层之间创建纠缠。*    
    * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**if** ![r < k](img/file1340.png "r < k") **then**'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**如果** ![r < k](img/file1340.png "r < k") **则**'
- en: '**for all** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **do**'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **执行**'
- en: Apply a CNOT gate with control on qubit ![t](img/file48.png "t") and target
    on qubit ![t + 1](img/file1342.png "t + 1").
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对控制量子位 ![t](img/file48.png "t") 和目标量子位 ![t + 1](img/file1342.png "t + 1") 应用CNOT门。
- en: -![-](img/file1343.png "-")
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")
- en: -![-](img/file1343.png "-")**** ***In *Figure* [*10.2*](#Figure10.2) we have
    depicted the output of this procedure for ![n = 4](img/file837.png "n = 4") and
    ![k = 3](img/file1344.png "k = 3"). Sound familiar? The two-local variational
    form uses the same circuit as the angle encoding feature map for its layers, and
    then it relies on a cascade of controlled-NOT operations in order to create entanglement.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")**** ***在*图* [*10.2*](#Figure10.2) 中，我们展示了该过程在 ![n
    = 4](img/file837.png "n = 4") 和 ![k = 3](img/file1344.png "k = 3") 时的输出。听起来熟悉吗？两个局部变分形式使用与角度编码特征图相同的电路作为其层，然后它依赖于一系列受控-NOT操作来创建纠缠。
- en: Notice, by the way, how the two-local variational form with ![k](img/file317.png
    "k") repetitions has ![k + 1](img/file1345.png "k + 1") layers, not ![k](img/file317.png
    "k"). This tiny detail can sometimes be misleading.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺便说一下，注意两个局部变分形式重复 ![k](img/file317.png "k") 次时具有 ![k + 1](img/file1345.png
    "k + 1") 层，而不是 ![k](img/file317.png "k") 层。这个小小的细节有时可能会误导。
- en: The two-local variational form is very versatile, and it can be used with any
    measurement operation.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个局部变分形式非常灵活，它可以与任何测量操作一起使用。
- en: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
- en: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
- en: '***   **Tree tensor**: The **tree tensor** variational form with ![k + 1](img/file1345.png
    "k + 1") layers can be applied on ![n = 2^{k}](img/file1347.png "n = 2^{k}") qubits.
    Each layer has half the number of parameters as the previous one, so the variational
    form relies on ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k
    - 1} + \cdots + 1") optimizable parameters of the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **树张量**：具有 ![k + 1](img/file1345.png "k + 1") 层的**树张量**变分形式可以应用于 ![n
    = 2^{k}](img/file1347.png "n = 2^{k}") 量子位。每一层的参数数量是前一层的一半，因此变分形式依赖于 ![2^{k} +
    2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k - 1} + \cdots + 1") 个可优化参数的形式'
- en: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
- en: 'The procedure that defines is somewhat more opaque than that of the two-local
    variational form, and it reads as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义该过程的步骤比两个局部变分形式稍微难以理解，其内容如下：
- en: '**procedure** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过程** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
- en: On each qubit ![j](img/file258.png "j"), apply a rotation ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})").
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个量子位 ![j](img/file258.png "j") 上应用旋转 ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})")。
- en: '**for all** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **do**'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **执行**'
- en: '**for all** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k
    - r} - 1") **do**'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k -
    r} - 1") **执行**'
- en: Apply a CNOT operation with target on qubit ![1 + s2^{r}](img/file1354.png "1
    + s2^{r}") and controlled by qubit ![1 + s2^{r} + 2^{r - 1}](img/file1355.png
    "1 + s2^{r} + 2^{r - 1}").
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对目标在量子位 ![1 + s2^{r}](img/file1354.png "1 + s2^{r}") 上、由量子位 ![1 + s2^{r} + 2^{r
    - 1}](img/file1355.png "1 + s2^{r} + 2^{r - 1}") 控制的CNOT操作进行应用。
- en: Apply a rotation ![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")
    on qubit ![1 + s2^{r}](img/file1354.png "1 + s2^{r}").
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子比特![1 + s2^{r}](img/file1354.png "1 + s2^{r}")上应用一个![R_{Y}(\theta_{r,s})](img/file1356.png
    "R_{Y}(\theta_{r,s})")旋转。
- en: -![-](img/file1343.png "-")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![img/file1343.png "-"]
- en: '-'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: An image is worth a thousand words, so, please, refer to *Figure* [*10.3*](#Figure10.3)
    for a depiction of the output of this procedure for ![k = 3](img/file1344.png
    "k = 3").
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一图胜千言，因此，请参考*图* [*10.3*](#Figure10.3)以了解此过程在![k = 3](img/file1344.png "k = 3")时的输出描述。
- en: The tree tensor variational form fits best in quantum neural networks designed
    to work as binary classifiers. The most natural measurement operation that can
    be used in conjunction with it is the obtention of the expected value of the first
    qubit, as measured in the computational basis.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 树张量变分形式最适合用于作为二分类器的量子神经网络。可以与之结合使用的最自然的测量操作是计算基下第一个量子比特的期望值。
- en: As a curiosity, the name of the tree tensor variational form comes from mathematical
    objects that are used for the simulation of physics systems and also in some machine
    learning models. See the survey paper by Román Orús for model details [[71](ch030.xhtml#Xorus2014practical)].
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为一种好奇，树张量变分形式的名称来源于用于物理系统模拟的数学对象，也用于一些机器学习模型中。有关模型细节，请参阅Román Orús的综述论文 [[71](ch030.xhtml#Xorus2014practical)]。
- en: '![Figure 10.3: Tree tensor variational form on 8 = 2^{3} qubits](img/file1358.png)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图10.3：在8 = 2^{3}量子比特上的树张量变分形式](img/file1358.png)'
- en: '**Figure 10.3**: Tree tensor variational form on ![8 = 2^{3}](img/file1357.png
    "8 = 2^{3}") qubits'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**图10.3**：在![8 = 2^{3}](img/file1357.png "8 = 2^{3}")量子比特上的树张量变分形式'
- en: '**Strongly entangling layers**: The strongly entangling layers variational
    form acts on ![n](img/file244.png "n") qubits and can have any number ![k](img/file317.png
    "k") of layers. Each layer ![l](img/file514.png "l") is given a **range** ![r_{l}](img/file1359.png
    "r_{l}"). In total, the variational form uses ![3nk](img/file1360.png "3nk") parameters
    of the form'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强纠缠层**：强纠缠层变分形式作用于![n](img/file244.png "n")个量子比特，并且可以有任意数量的![k](img/file317.png
    "k")层。每一层![l](img/file514.png "l")都有一个**范围**![r_{l}](img/file1359.png "r_{l}")。总共，变分形式使用了![3nk](img/file1360.png
    "3nk")个参数，形式如下'
- en: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
- en: 'The form is defined by the following algorithm:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 形式由以下算法定义：
- en: '**procedure** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过程** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
- en: '**for all** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **do**'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **执行**'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**'
- en: Apply a rotation ![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")
    on qubit ![j](img/file258.png "j").
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子比特![j](img/file258.png "j")上应用一个![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")旋转。
- en: Apply a rotation ![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")
    on qubit ![j](img/file258.png "j").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子比特![j](img/file258.png "j")上应用一个![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")旋转。
- en: Apply a rotation ![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")
    on qubit ![j](img/file258.png "j").
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子比特![j](img/file258.png "j")上应用一个![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")旋转。
- en: '-'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**'
- en: Apply a CNOT operation controlled by qubit ![j](img/file258.png "j") and with
    target on qubit ![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png
    "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1").
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子比特![j](img/file258.png "j")的控制下应用一个CNOT操作，目标为量子比特![\lbrack(j + r_{l} - 1)\
    \operatorname{mod}\ N\rbrack + 1](img/file1367.png "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\
    N\rbrack + 1")。
- en: -![-](img/file1343.png "-")
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![img/file1343.png "-"]
- en: '-'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: You may find a representation of a sample of this form in *Figure* [*10.4*](#Figure10.4).
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在*图* [*10.4*](#Figure10.4)中找到一个这种形式的样本表示。
- en: '![Figure 10.4: Strongly entangling layers form on four qubits and two layers
    with respective ranges 1 and 2](img/file1368.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图10.4：在四个量子比特上形成强纠缠层，以及两个具有相应范围1和2的层](img/file1368.png)'
- en: '**Figure 10.4**: Strongly entangling layers form on four qubits and two layers
    with respective ranges ![1](img/file13.png "1") and ![2](img/file302.png "2")**'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**图10.4**：在四个量子比特上形成强纠缠层，以及两个具有相应范围![1](img/file13.png "1")和![2](img/file302.png
    "2")的层**'
- en: '**As a final remark, our choice to use mostly ![Y](img/file11.png "Y") rotations
    in the previous examples of variational forms is somewhat arbitrary. We could’ve
    also used ![X](img/file9.png "X") rotations, for example. The same goes for our
    choice to use controlled-![X](img/file9.png "X") operations in the entanglement
    circuits. We could have used a different controlled operation, for instance. In
    addition to this, in the two-local variational form, there are more options for
    the distribution of gates in the entanglement circuit beyond the one that we have
    considered. Our entanglement circuit is said to have a ”linear” arrangement of
    gates, but other possibilities are shown in *Figure* [*10.5*](#Figure10.5).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**作为最后的评论，我们选择在之前的变分形式示例中主要使用![Y](img/file11.png "Y")旋转是有些任意的。我们也可以使用![X](img/file9.png
    "X")旋转，例如。同样，我们选择在纠缠电路中使用受控-![X](img/file9.png "X")操作也是任意的。我们也可以使用不同的受控操作，例如。此外，在两局部变分形式中，在纠缠电路中门的分布还有更多选择，而不仅仅是我们所考虑的那种。我们的纠缠电路被认为具有“线性”的门排列，但其他可能性在*图10.5*中有所展示。'
- en: '![(a) Linear](img/file1369.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![（a）线性](img/file1369.png)'
- en: '**(a)** Linear'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** 线性'
- en: '![(b) Circular](img/file1370.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![（b）圆形](img/file1370.jpg)'
- en: '**(b)** Circular'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** 圆形'
- en: '![(c) Full](img/file1371.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![（c）完整](img/file1371.jpg)'
- en: '**(c)** Full'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**(c)** 完整'
- en: '**Figure 10.5**: Different entanglement circuits'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.5**：不同的纠缠电路'
- en: This is all we need to know, for now, about variational forms. Combined with
    our previous knowledge of feature maps, this ends our analysis of the elements
    of a quantum neural network…almost. We still have to dive deeper into that seemingly
    innocent measurement operation at the end of every quantum neural network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们目前需要了解的所有关于变分形式的内容。结合我们之前对特征图的了解，这结束了我们对量子神经网络元素的解析……几乎。我们仍然需要深入探究每个量子神经网络末尾看似无辜的测量操作。
- en: 10.1.3 A word about measurements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.3 关于测量的说明
- en: 'As we saw back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE: Variational
    Quantum Eigensolver*, any physical observable can be represented by a Hermitian
    operator in such a way that all the possible outcomes of the measurement of the
    observable can be matched to the different eigenvalues of the operator. If you
    haven’t done so already, please, have a look at *Section* [*7.1.1*](ch015.xhtml#x1-1210007.1.1)
    if you are not familiar with this.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第7章*[*7*](ch015.xhtml#x1-1190007)中看到的，“VQE：变分量子本征值求解器”，任何物理可观测量都可以通过一个厄米算符来表示，使得所有可能的测量结果都可以与算符的不同本征值相对应。如果你还不熟悉这一点，请查看*第7.1.1节*[*7.1.1*](ch015.xhtml#x1-1210007.1.1)。
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在计算基下测量单个量子比特时，与相关厄米算符的计算基坐标矩阵可能是以下之一
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
- en: Both of these operators represent the measurement of a qubit, but they differ
    in the eigenvalues that they associate to the distinct outputs. The first operator
    associates the eigenvalues ![1](img/file13.png "1") and ![0](img/file12.png "0")
    to the qubit’s value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个算符都表示对量子比特的测量，但它们在关联不同输出时的本征值上有所不同。第一个算符将本征值![1](img/file13.png "1")和![0](img/file12.png
    "0")分别关联到量子比特的值为![0](img/file12.png "0")和![1](img/file13.png "1")，而第二个可观测量将本征值![1](img/file13.png
    "1")和![- 1](img/file312.png "- 1")关联到这些结果。
- en: Exercise 10.1
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.1
- en: The purpose of this exercise is for you to get more familiar with Dirac notation.
    Show that the two previous Hermitian operators may be written, respectively, as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是让你更熟悉狄拉克符号。证明前两个厄米算符可以分别写成
- en: '| ![1\left&#124; 0 \right\rangle\left\langle 0 \right&#124; + 0\left&#124;
    1 \right\rangle\left\langle 1 \right&#124; = \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;,\quad\left&#124; 0 \right\rangle\left\langle 0 \right&#124; - \left&#124;
    1 \right\rangle\left\langle 1 \right&#124;.](img/file1373.png "1\left&#124; 0
    \right\rangle\left\langle 0 \right&#124; + 0\left&#124; 1 \right\rangle\left\langle
    1 \right&#124; = \left&#124; 1 \right\rangle\left\langle 1 \right&#124;,\quad\left&#124;
    0 \right\rangle\left\langle 0 \right&#124; - \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;.") |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![1\left| 0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle
    1 \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.](img/file1373.png "1\left|
    0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle 1
    \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.") |'
- en: '*Hint*: Remember that the product of a ket (column vector) and a bra (row vector)
    is a matrix. We saw an example of this back in *Section* *[*7.2.1*](ch015.xhtml#x1-1240007.2.1).*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示*：记住，基（列向量）和提（行向量）的乘积是一个矩阵。我们之前在 *第7.2.1节* 中看到了一个例子。'
- en: '*As we will see later on in the chapter, frameworks such as PennyLane allow
    you to work with measurement operations defined by any Hermitian operator. This
    can give you a lot of flexibility when defining the measurement operation of a
    neural network. For instance, in an ![n](img/file244.png "n")-qubit circuit, you
    will be able to instruct PennyLane to compute the expectation value of the observable
    ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M"),
    which has as its coordinate representation in the computational basis the matrix'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*正如我们将在本章后面看到的那样，框架如PennyLane允许你使用由任何厄米算子定义的测量操作。这可以在定义神经网络的测量操作时给你带来很多灵活性。例如，在一个
    ![n](img/file244.png "n")-量子位电路中，你将能够指示PennyLane计算可观测量 ![M \otimes \cdots \otimes
    M](img/file1374.png "M \otimes \cdots \otimes M") 的期望值，其在计算基的坐标表示是矩阵'
- en: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
- en: Alternatively, you may want to consider the observable ![Z \otimes \cdots \otimes
    Z](img/file1376.png "Z \otimes \cdots \otimes Z"). It is easy to see how this
    observable will return ![+ 1](img/file1377.png "+ 1") if an even number of qubits
    are measured as ![0](img/file12.png "0"), and ![- 1](img/file312.png "- 1") otherwise.
    That’s the reason why ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes
    \cdots \otimes Z") is referred to as the **parity** observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可能想考虑可观测量 ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots
    \otimes Z")。很容易看出，如果测量到偶数个量子位为 ![0](img/file12.png "0")，则该可观测量将返回 ![+ 1](img/file1377.png
    "+ 1")，否则返回 ![- 1](img/file312.png "- 1")。这就是为什么 ![Z \otimes \cdots \otimes Z](img/file1376.png
    "Z \otimes \cdots \otimes Z") 被称为**偶数性**可观测量的原因。
- en: Of course, you will also be able to take the measurement operation to be a good
    old expectation value on the first qubit. But, the point is, there’s also a plethora
    of options available to you, should you want to explore them!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你还可以将测量操作视为第一个量子位的经典期望值。但是，重点是，如果你愿意探索，还有很多其他选项可供选择！
- en: As we mentioned before, observables are the final building blocks of every quantum
    neural network architecture. Quantum neural networks accept an input, which usually
    consists of classical data being fed through a feature map. The resulting quantum
    state is then transformed by a variational form and, lastly, some (classical)
    numerical data is obtained through a measurement operation. In this way, we have
    a ”black box” transforming some numerical inputs into outputs, that is, a model
    that — just like any other classical ML model — can be trained.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，可观测量是每个量子神经网络架构的最终构建块。量子神经网络接受一个输入，这通常由通过特征图输入的经典数据组成。然后，通过一个变分形式将得到的量子状态转换，最后通过测量操作获得一些（经典）数值数据。这样，我们就得到了一个将一些数值输入转换为输出的“黑盒”，即一个模型——就像任何其他经典机器学习模型一样——可以被训练。
- en: 'We have now defined what quantum neural networks are and learned how to construct
    them, at least in theory. That means we have a model. But this is quantum machine
    learning, so a model is not enough: we need to train it. And in order to do so,
    we will need, among other things, an optimization algorithm.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了量子神经网络是什么，并且学习了如何构建它们，至少在理论上是如此。这意味着我们有一个模型。但是，这是量子机器学习，所以一个模型是不够的：我们需要对其进行训练。为此，我们将需要，包括但不限于，一个优化算法。
- en: 10.1.4 Gradient computation and the parameter shift rule
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.4 梯度计算和参数平移规则
- en: Although it is not the only option, the optimization algorithms that we shall
    use for quantum neural networks will be gradient descent algorithms; in particular,
    we will use the Adam optimizer. But, as we saw in *Chapter* [*8*](ch017.xhtml#x1-1390008)*,*
    *What is Quantum Machine Learning?*, this algorithm needs to obtain the gradient
    of the expected value of a loss function in terms of the optimizable parameters.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是唯一的选择，但我们将为量子神经网络使用的优化算法将是梯度下降算法；特别是，我们将使用Adam优化器。但是，正如我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*,*
    *什么是量子机器学习？*中看到的，这个算法需要获得损失函数期望值的梯度，相对于可优化参数而言。
- en: 'Since our model uses a quantum circuit, the computation of these gradients
    is not entirely trivial. We shall now go briefly over the three main kinds of
    differentiation methods in which these gradient computations may be carried out:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型使用量子电路，这些梯度的计算并不完全简单。我们现在简要地回顾一下三种主要的微分方法，在这些方法中可以执行这些梯度计算：
- en: '**Numerical approximation**: Of course, we have a method that always works.
    It may not always be the most efficient one, but it’s always there. In order to
    compute gradients, we may just estimate them numerically. In order to do this,
    of course, we will have to run our quantum neural network plenty of times.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值逼近**：当然，我们有一个总是有效的方法。它可能不是最有效的方法，但总是存在的。为了计算梯度，我们可能只需要进行数值估计。为了做到这一点，当然，我们不得不多次运行我们的量子神经网络。'
- en: Just to exemplify this a little bit, if we had a real-valued function taking
    ![n](img/file244.png "n") real inputs ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png
    "\left. f:R^{n}\rightarrow R \right."), we could approximate its partial derivatives
    as
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了稍微举例说明这一点，如果我们有一个接受![n](img/file244.png "n")个实数输入的实值函数 ![\left. f:R^{n}\rightarrow
    R \right.](img/file1378.png "\left. f:R^{n}\rightarrow R \right.")，我们可以近似其偏导数如下
- en: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
- en: for a sufficiently small value of ![h](img/file519.png "h"). That’s, of course,
    the most naive way to numerically approximate a derivative, but hopefully it’s
    enough to give you an intuition of how this works.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于![h](img/file519.png "h")足够小的值。当然，这是数值逼近导数最天真的一种方法，但希望这足以让你对它是如何工作的有一个直观的理解。
- en: '**Automatic differentiation**: Given the current state of real quantum hardware,
    odds are that most of the quantum neural networks that you will train will run
    on simulators. As non-ideal as this may be, it comes with some advantages. Most
    notably, on simulated quantum neural networks, a classical computer may compute
    exact gradients using techniques similar to those employed on classical neural
    networks. If you are interested, the book Aurélien Géron [[104](ch030.xhtml#Xhandsonml),
    Chapter 10] and the one by Shai Shalev-Shwartz and Shai Ben-David [[105](ch030.xhtml#Xunderml),
    §20.6] discuss these techniques for classical neural networks.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动微分**：鉴于当前真实量子硬件的状态，你将训练的大多数量子神经网络很可能会在模拟器上运行。尽管这可能并不理想，但它带来了一些优势。最值得注意的是，在模拟量子神经网络上，经典计算机可以使用类似于在经典神经网络中使用的技巧来计算精确的梯度。如果你感兴趣，Aurélien
    Géron的书籍 [[104](ch030.xhtml#Xhandsonml)，第10章] 和Shai Shalev-Shwartz以及Shai Ben-David的书籍
    [[105](ch030.xhtml#Xunderml)，第20.6节] 讨论了这些经典神经网络的技巧。'
- en: '**The parameter shift rule**: The standard automatic differentiation techniques
    can only be used on simulators. Fortunately, there is still another way to compute
    gradients when executing quantum neural networks on real hardware: using the **parameter
    shift rule**. As the name suggests, this technique enables us to compute gradients
    by using the same circuit in the quantum neural network, yet shifting the values
    of the optimizable parameters. The parameter shift rule can’t always be applied,
    but it works on many common cases and can be used in conjunction with other techniques,
    such as numerical approximation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数平移规则**：标准的自动微分技术只能在模拟器上使用。幸运的是，当在真实硬件上执行量子神经网络时，仍然有另一种方法来计算梯度：使用**参数平移规则**。正如其名所示，这项技术使我们能够通过在量子神经网络中使用相同的电路，同时改变可优化参数的值来计算梯度。参数平移规则并不总是适用，但它适用于许多常见情况，并且可以与其他技术结合使用，例如数值近似。'
- en: We won’t get into the details of how this method works, but you may have a look
    at a research paper by Maria Schuld and others [[109](ch030.xhtml#Xpshift-schuld)]
    for more information. For example, if you had a circuit consisting of a single
    rotation gate ![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)") and the measurement
    of its expectation value ![E(\theta)](img/file1381.png "E(\theta)"), you would
    be able to compute its derivative with respect to ![\theta](img/file89.png "\theta")
    as
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们不会深入探讨这种方法是如何工作的细节，但你可以查看Maria Schuld和其他人发表的研究论文[[109](ch030.xhtml#Xpshift-schuld)]以获取更多信息。例如，如果你有一个由单个旋转门![R_{X}(\theta)](img/file1380.png
    "R_{X}(\theta)")和其期望值![E(\theta)](img/file1381.png "E(\theta)")的测量组成的电路，你将能够计算其对![\theta](img/file89.png
    "\theta")的导数，如下所示：
- en: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
- en: 'This is similar to what happens with some trigonometric functions: for instance,
    you can express the derivative of the sine function in terms of shifted values
    of the same sine function.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与某些三角函数发生的情况类似：例如，你可以用相同正弦函数的平移值来表达正弦函数的导数。
- en: For our purposes, it will suffice to know that it exists and can be used. Of
    course, the parameter shift rule can also be used on simulators!
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，了解它存在并且可以被使用就足够了。当然，参数平移规则也可以用于模拟器！
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When quantum neural networks are run on simulators, gradients can be computed
    using automatic differentiation techniques analogous to those of classical machine
    learning. When they are run on either real hardware or simulators, these gradients
    can also be computed — at least on many cases — using the parameter shift rule.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当量子神经网络在模拟器上运行时，可以使用类似于经典机器学习的自动微分技术来计算梯度。当它们在真实硬件或模拟器上运行时，这些梯度也可以计算——至少在许多情况下可以——使用参数平移规则。
- en: Alternatively, numerical approximation is always an effective way to compute
    gradients.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，数值近似始终是计算梯度的一种有效方法。
- en: As we have mentioned, all of these methods are already fully implemented in
    PennyLane, and we will try them all out in the following section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，所有这些方法已经在PennyLane中完全实现，我们将在下一节尝试所有这些方法。
- en: To learn more…
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: 'Everything looks good and promising, but quantum neural networks also pose
    some challenges when it comes to training them. Most notably, they are known to
    be vulnerable to **barren plateaus**: situations in which the training gradients
    vanish and, thus, the training can no longer progress (see the paper by McClean
    et. al for further explanation [[67](ch030.xhtml#Xmcclean2018barren)]). It is
    also known that the kind of measurement operation used and the depth of the QNN
    play a role in how likely these barren plateaus are to be found. This is studied,
    for instance, in a paper by Cerezo and collaborators [[24](ch030.xhtml#Xcerezo2021cost)].
    In any case, you should be vigilant when training your QNNs, and follow the literature
    for possible solutions should barren plateaus threaten the learning of your models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都很好，很有希望，但量子神经网络在训练时也带来了一些挑战。最值得注意的是，它们已知容易受到**贫瘠平原**的影响：训练梯度消失的情况，因此训练无法再进行（参见McClean等人撰写的论文以获取进一步解释[[67](ch030.xhtml#Xmcclean2018barren)]）。还知道所使用的测量操作类型和QNN的深度会影响这些贫瘠平原出现的可能性。例如，在Cerezo及其合作者的论文中对此进行了研究[[24](ch030.xhtml#Xcerezo2021cost)]。无论如何，你在训练你的QNN时应该保持警惕，如果贫瘠平原威胁到你的模型的学习，应遵循文献中的可能解决方案。
- en: We now have all the ingredients necessary to construct and train quantum neural
    networks. But before we get to do that in practice, we will discuss a few techniques
    and tips that will help you get the most of our brand new quantum machine learning
    models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有了构建和训练量子神经网络所需的所有成分。但在我们实际操作之前，我们将讨论一些技术和技巧，这将帮助你最大限度地发挥我们全新的量子机器学习模型的优势。
- en: 10.1.5 Practical usage of quantum neural networks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.5 量子神经网络的实际应用
- en: 'The following are a collection of ideas that you should keep in mind when designing
    QNN models and training them. You can think of it as a summary of the previous
    sections, with a few highlights from *Chapter* [*8*](ch017.xhtml#x1-1390008)*,
    What is Quantum* *Machine Learning?*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在设计QNN模型和训练它们时你应该记住的一些想法。你可以将其视为前几节的总结，其中包含来自*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子*机器学习？*的一些亮点：
- en: '**Make wise choices**: When you set out to design a QNN, you have three important
    decisions to make: you have to pick a feature map, a variational form, and a measurement
    operation. Be intentional about these choices and consider the problem and the
    data that you are working with. Your decisions can influence how likely you are
    to find barren plateaus, for instance. A good recommendation is to check the literature
    for similar problems and to build up from there.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明智的选择**：当你开始设计一个QNN时，你必须做出三个重要的决定：你必须选择一个特征图、一个变分形式和一个测量操作。对这些选择要有意为之，并考虑你正在处理的问题和数据。你的决定可能会影响你找到贫瘠平原的可能性。一个好的建议是检查文献中类似的问题，并在此基础上构建。'
- en: '**Size matters**: When you use a well-designed variational form, in general,
    the power of the resulting quantum neural network will be directly related to
    the number of optimizable parameters it has. Use too many parameters, and you
    may have a model that overfits. Use very few, and your model may end up underfitting.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小很重要**：当你使用一个设计良好的变分形式时，通常，所得到的量子神经网络的力量将直接与其拥有的可优化参数数量相关。使用过多的参数，你可能会得到一个过拟合的模型。使用非常少的参数，你的模型最终可能欠拟合。'
- en: '**Optimize optimization**: For most problems, the Adam optimizer can be your
    go-to choice for training a quantum neural network. Remember that, as we discussed
    in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum* *Machine Learning?*,
    you will have to pick a learning rate and a batch size when using Adam.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化优化**：对于大多数问题，Adam优化器可以是你训练量子神经网络的默认选择。记住，正如我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子*机器学习？*中讨论的那样，当你使用Adam时，你必须选择一个学习率和批量大小。'
- en: A smaller learning rate will make the algorithm more accurate, but also slower.
    Analogously, a higher batch size should make the optimization more effective,
    to the detriment of execution time.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较小的学习率会使算法更准确，但也会更慢。类似地，较大的批量大小应该会使优化更有效，但会损害执行时间。
- en: '**Feed your QNN properly**: The data that is fed to a quantum neural network
    should be normalized according to the requirements of the feature map in use.
    In addition, depending on the dimensions of the input data, you may want to rely
    on dimensionality reduction techniques.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确喂养你的QNN**：提供给量子神经网络的应该根据所使用的特征图的要求进行归一化。此外，根据输入数据的维度，你可能想要依赖降维技术。'
- en: Of course, the more data you have, the better. Nonetheless, one additional fact
    that you may want to take into account is that, under some conditions, quantum
    neural networks have been shown to need fewer data samples than classical neural
    networks in order to be successfully trained [[112](ch030.xhtml#Xqnn-lowdata)].
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然，你拥有的数据越多，越好。不过，你可能还想考虑的一个额外事实是，在某些条件下，量子神经网络已被证明在成功训练时需要比经典神经网络更少的数据样本 [[112](ch030.xhtml#Xqnn-lowdata)]。
- en: To learn more…
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: If you want to further boost the power of your quantum neural networks, you
    may want to consider the **data reuploading** technique [[110](ch030.xhtml#Xperez2020data)].
    In a vanilla QNN, you have a feature map ![F](img/file1320.png "F") dependent
    on some input data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}"),
    which is then followed by a variational form ![V](img/file379.png "V") dependent
    on some optimizable parameters ![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}"). Data reuploading simply consists in repeating
    this scheme — any number of times you want — before performing the measurement
    operation of the QNN. The feature maps use the same input data in each repetition,
    but each instance of the variational form takes its own, independent, optimizable
    parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要进一步增强你的量子神经网络的能力，你可能需要考虑**数据重上传**技术 [[110](ch030.xhtml#Xperez2020data)]。在传统的QNN中，你有一个依赖于某些输入数据![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}")的特征图![F](img/file1320.png "F")，然后是依赖于某些可优化参数![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}")的变分形式![V](img/file379.png "V")。数据重上传简单地说就是重复这个方案——任何你想要的次数——在执行QNN的测量操作之前。特征图在每个重复中使用相同的输入数据，但每个变分形式的实例都采用它自己的、独立的、可优化的参数。
- en: 'This is represented in the following diagram, which shows data reuploading
    with ![k](img/file317.png "k") repetitions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中表示，它显示了带有![k](img/file317.png "k")次重复的数据重上传：
- en: '![|FVFV0⟩((((n⃗x⃗𝜃⃗x⃗𝜃)1)k)) ... ](img/file1384.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![|FVFV0⟩((((n⃗x⃗𝜃⃗x⃗𝜃)1)k)) ... ](img/file1384.jpg)'
- en: This has been shown, both in practice and in theory [[113](ch030.xhtml#Xdatare-schuld)],
    to offer some advantages over the simpler, standard approach at the cost of increasing
    the depth of the circuits that are used. In any case, it is good to have it in
    mind when implementing your own QNNs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经在实践和理论 [[113](ch030.xhtml#Xdatare-schuld)] 中被证明，与更简单、标准的方法相比，它提供了一些优势，但代价是增加了所使用电路的深度。无论如何，在实现你自己的QNN时，将其牢记在心是好的。
- en: This concludes our theoretical discussion of quantum neural networks. Now it’s
    time for us to get our hands dirty with the actual implementation of all the fancy
    artifacts and techniques that we have discussed. In this regard, we will focus
    mostly on PennyLane. Let’s begin!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对量子神经网络的纯理论讨论。现在是我们动手实现所有我们讨论过的花哨的元素和技术的时候了。在这方面，我们将主要关注PennyLane。让我们开始吧！
- en: 10.2 Quantum neural networks in PennyLane
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.2 PennyLane中的量子神经网络
- en: We are now ready to implement and train our first quantum neural network with
    PennyLane. The PennyLane framework is great for many applications, but it shines
    the most when it comes to the implementation of quantum neural network models.
    This is all due to its flexibility and good integration with classical machine
    learning frameworks. We, in particular, are going to be using PennyLane in conjunction
    with TensorFlow to train a QNN-based binary classifier. All that effort that we
    invested in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine
    Learning?*, is finally going to pay off!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好使用PennyLane实现和训练我们的第一个量子神经网络。PennyLane框架非常适合许多应用，但在实现量子神经网络模型方面最为出色。这都归功于其灵活性和与经典机器学习框架的良好集成。我们将特别使用PennyLane与TensorFlow结合来训练一个基于QNN的二分类器。我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子机器学习？*中投入的所有努力，最终将得到回报！
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember that we are using **version 2.9.1** of the TensorFlow package and **version
    0.26** of PennyLane.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们正在使用TensorFlow包的**版本2.9.1**和PennyLane的**版本0.26**。
- en: 'Let’s begin by importing PennyLane, NumPy, and TensorFlow and setting some
    seeds for these packages, just to make sure that our results are reproducible.
    We can achieve this with the following piece of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入PennyLane、NumPy和TensorFlow以及为这些包设置一些种子开始，以确保我们的结果是可重复的。我们可以通过以下代码片段实现这一点：
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you may still get slightly different results from ours if
    you are using different package versions. However, the results you obtain will
    be fully reproducible in your own machine.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果你使用不同的包版本，你可能会得到与我们略微不同的结果。然而，你获得的结果将在你自己的机器上完全可重复。
- en: 'Before we get to our problem, there’s one last detail that we need to sort
    out. PennyLane works with doubles while TensorFlow uses ordinary floats. This
    isn’t always an issue, but it’s a good idea to ask TensorFlow to work with doubles
    just as PennyLane does. We can accomplish this as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们面对我们的问题之前，还有一个最后的细节需要我们解决。PennyLane使用双精度浮点数，而TensorFlow使用普通的浮点数。这并不总是问题，但最好让TensorFlow像PennyLane一样使用双精度浮点数。我们可以通过以下方式实现这一点：
- en: '[PRE1]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this out of the way, let’s meet our problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题解决完毕之后，让我们来面对我们的问题。
- en: 10.2.1 Preparing data for a QNN
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.1 为QNN准备数据
- en: As we have already mentioned, we are going to train a QNN model to implement
    a binary classifier. Our recurrent use of binary classifiers is no coincidence,
    for binary classifiers are perhaps the simplest machine learning models to train.
    Later in the book, however, we will explore more exciting use cases and architectures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，我们将训练一个QNN模型来实现二元分类器。我们反复使用二元分类器并不是巧合，因为二元分类器可能是训练起来最简单的机器学习模型。然而，在本书的后面，我们将探索更多令人兴奋的使用案例和架构。
- en: 'For our example problem, we are going to use one of the toy datasets provided
    by the scikit-learn package: the ”Breast cancer Wisconsin dataset” [[32](ch030.xhtml#XDua:2019)].
    This dataset has a total of ![569](img/file1385.png "569") samples with ![30](img/file620.png
    "30") numerical variables each. These variables describe features that can be
    used to characterize whether a breast mass is benign or malignant. The label of
    each sample can be either ![0](img/file12.png "0") or ![1](img/file13.png "1"),
    corresponding to malignant or benign, respectively. You may find the documentation
    of this dataset online at [https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)
    (the original documentation of the dataset can also be found at [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例问题，我们将使用scikit-learn包提供的玩具数据集之一：“威斯康星乳腺癌数据集” [[32](ch030.xhtml#XDua:2019)]。这个数据集总共有![569](img/file1385.png
    "569")个样本，每个样本有![30](img/file620.png "30")个数值变量。这些变量描述了可以用来表征乳腺肿块是良性还是恶性的特征。每个样本的标签可以是![0](img/file12.png
    "0")或![1](img/file13.png "1")，分别对应恶性和良性。你可以在网上找到这个数据集的文档[https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)（数据集的原始文档也可以在[https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))找到）。
- en: 'We can get this dataset by calling the `load_breast_cancer` function from `sklearn``.``datasets`,
    setting the optional argument `return_X_y` to true in order to retrieve the labels
    in addition to the samples. For that, we can use the following instructions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从`sklearn.datasets`调用`load_breast_cancer`函数来获取这个数据集，将可选参数`return_X_y`设置为true，以便除了样本外还可以检索标签。为此，我们可以使用以下说明：
- en: '[PRE2]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we trained QSVMs, since we were not going to make any comparisons between
    models, a training and test dataset sufficed. In our case, however, we are going
    to train our models with early stopping on the validation loss. This means — in
    case you don’t remember — that we will be keeping track of the validation loss
    and we will halt the training as soon as it doesn’t improve — according to some
    criteria that we will define. What is more, we will keep the model configuration
    that best minimized the validation loss. Using the test dataset for this purpose
    wouldn’t be good practice, for then the test dataset would have played a role
    in the training and it would not give a good estimate of the true error; that’s
    why we will need a separate validation dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练QSVM时，因为我们不打算在模型之间进行比较，所以一个训练集和测试集就足够了。然而，在我们的情况下，我们将使用验证损失提前停止来训练我们的模型。这意味着——如果你不记得的话——我们将跟踪验证损失，并且一旦它不再改善——根据我们将定义的一些标准，我们将停止训练。更重要的是，我们将保留最佳模型配置，以最小化验证损失。使用测试集来达到这个目的并不是一个好的实践，因为这样测试集就会在训练中发挥作用，它不会给出真实错误的良好估计；这就是为什么我们需要一个单独的验证数据集。
- en: 'We can split our dataset into a training, validation, and test dataset as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的数据集分为训练集、验证集和测试集，如下所示：
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'All the variables in the dataset are non-zero, but they are not normalized.
    In order to use them with any of our feature maps, we shall normalize the training
    data between ![0](img/file12.png "0") and ![1](img/file13.png "1") using `MaxAbsScaler`
    as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的所有变量都不是零，但它们没有归一化。为了使用它们与我们的任何特征映射，我们应该使用`MaxAbsScaler`将训练数据归一化到![0](img/file12.png
    "0")和![1](img/file13.png "1")之间，如下所示：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we then normalize the test and validation datasets in the same proportions
    as the training dataset:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将测试和验证数据集按照训练数据集的比例进行归一化：
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just as we did when we trained a QSVM in the previous chapter!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在上一章训练QSVM时所做的那样！
- en: 'So far, we have simply done some fairly standard data preprocessing, without
    having to think too much about the actual architecture of our future quantum neural
    network. But that changes now. We have a problem to address: our dataset has ![30](img/file620.png
    "30") variables, and that can be a pretty large number for current quantum hardware.
    Since we don’t have access to quantum computers with ![30](img/file620.png "30")
    qubits, we may consider the following choices:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是进行了一些相当标准的预处理，而不必过多考虑我们未来量子神经网络的实际架构。但现在情况改变了。我们有一个问题要解决：我们的数据集有![30](img/file620.png
    "30")个变量，这对于当前的量子硬件来说可能是一个相当大的数字。由于我们没有访问到![30](img/file620.png "30")量子比特的量子计算机，我们可能考虑以下选择：
- en: Use the amplitude encoding feature map on ![5](img/file296.png "5") qubits,
    which can accommodate up to ![2^{5} = 32](img/file1386.png "2^{5} = 32") variables
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在![5](img/file296.png "5")个量子比特上使用幅度编码特征映射，它可以容纳多达![2^{5} = 32](img/file1386.png
    "2^{5} = 32")个变量
- en: Use any of the other feature maps that we have used, but in conjunction with
    a dimensionality reduction technique
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们之前使用过的任何其他特征映射，但与降维技术结合使用
- en: 'We will go for the latter choice. You can try the other possibility on your
    own: it’s fairly straightforward if you use the `qml``.``AmplitudeEmbedding` template
    that we studied back in *Chapter* [*9*](ch018.xhtml#x1-1600009)*, Quantum Support
    Vector* *Machines*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择后者。你可以自己尝试其他可能性：如果你使用我们在*第* [*9*](ch018.xhtml#x1-1600009)*章中学习的`qml`的`AmplitudeEmbedding`模板，它相当直接。
- en: Exercise 10.2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.2
- en: As you follow along this section, try to implement a QNN using all the original
    variables with amplitude encoding on five qubits.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在你跟随本节内容的同时，尝试使用五个量子比特的幅度编码实现一个QNN。
- en: Keep in mind that, when feeding the data to the `qml``.` `AmplitudeEmbedding`
    object through the features argument, instead of using the `inputs` variable,
    you should use `[``a` `for` `a` `in` `inputs``]`. This is needed because of some
    internal type conversions that PennyLane needs to perform.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当通过特征参数将数据喂给`qml`的`AmplitudeEmbedding`对象时，而不是使用`inputs`变量，你应该使用`[``a` `for`
    `a` `in` `inputs``]`。这是因为PennyLane需要执行一些内部类型转换。
- en: Training a quantum neural network on a simulator is a very computationally-intensive
    task. We don’t want anyone’s computer to crash, so, just to make sure everyone
    can run this example smoothly, we will restrict ourselves to using ![4](img/file143.png
    "4")-qubit circuits. Thus, we will use a dimensionality reduction technique to
    shrink the number of variables to ![4](img/file143.png "4"), and then set up a
    QNN with a feature map that will take the resulting ![4](img/file143.png "4")
    input variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟器上训练量子神经网络是一个计算密集型任务。我们不希望任何人的电脑崩溃，所以，为了确保每个人都能顺利运行这个示例，我们将限制自己使用![4](img/file143.png
    "4")-量子比特电路。因此，我们将使用降维技术将变量的数量减少到![4](img/file143.png "4")，然后设置一个具有特征映射的QNN，该映射将接受![4](img/file143.png
    "4")个输入变量。
- en: 'As we did in the previous chapter, we will use principal component analysis
    in order to reduce the number of variables in our dataset to ![4](img/file143.png
    "4"):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中所做的那样，我们将使用主成分分析来减少数据集中变量的数量到![4](img/file143.png "4")：
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have our data fully ready, we need to choose how our quantum neural
    network is going to work. This is exactly the focus of the next subsection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完全准备好了数据，我们需要选择我们的量子神经网络将如何工作。这正是下一小节的重点。
- en: 10.2.2 Building the network
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.2 构建网络
- en: For our case, we will choose the ZZ feature map and the two-local variational
    form. Neither is built into PennyLane, so we have to provide our own implementation
    of these variational circuits. PennyLane includes, however, a version of the two-local
    form with circular entanglement (`qml``.``BasicEntanglerLayers`), in case you
    want to use it in your QNNs. To implement the circuits that we need, we can just
    use the pseudocode that we provided in *Section* *[*10.1.2*](#x1-18400010.1.2)
    and do something like the following:*
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，我们将选择 ZZ 特征图和双局部变分形式。这两个都不是内置在 PennyLane 中的，因此我们必须提供我们自己的变分电路实现。然而，PennyLane
    包含了一个具有环形纠缠的双局部形式版本 (`qml``.``BasicEntanglerLayers`)，以防你想要在 QNNs 中使用它。为了实现我们需要的电路，我们只需使用我们在
    *第 * *10.1.2* *节中提供的伪代码，并执行以下操作：*
- en: '*[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE7]'
- en: Remember that we already implemented the ZZ feature map in PennyLane in the
    previous chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在上一章中已经在 PennyLane 中实现了 ZZ 特征图。
- en: In this chapter, we have talked about observables, and how these are represented
    by Hermitian operators in quantum mechanics. PennyLane allows us to work directly
    with these Hermitian representations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了可观测量，以及这些在量子力学中如何由厄米算子表示。PennyLane 允许我们直接使用这些厄米表示。
- en: Remember how every circuit in PennyLane returns the result of some measurement
    operation? For instance, you may use `return` `qml``.``probs``(``wires` `=` `[0])`
    at the end of the definition of a circuit in order to get the probabilities of
    every possible measurement outcome on the computational basis. Well, it turns
    out that PennyLane offers a few more possibilities. For instance, given any Hermitian
    matrix ![A](img/file183.png "A") (encoded as a numpy array `A`), we may retrieve
    the expectation value of ![A](img/file183.png "A") on an array of wires `w` at
    the end of a circuit simply by calling `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)`. Of course, the dimensions of ![A](img/file183.png "A") must be compatible
    with the length of `w`. This is useful in our case, for in order to get the expectation
    value on the first qubit, we will just have to compute the expectation value of
    the Hermitian
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在 PennyLane 中每个电路都返回某些测量操作的结果吗？例如，你可以在电路定义的末尾使用 `return` `qml``.``probs``(``wires`
    `=` `[0])` 来获取计算基上每个可能测量结果的概率。嗯，结果是 PennyLane 还提供了一些其他可能性。例如，给定任何厄米矩阵 ![A](img/file183.png
    "A")（编码为 numpy 数组 `A`），我们只需在电路末尾调用 `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)` 就可以检索 `w` 上 `A` 的期望值。当然，![A](img/file183.png "A") 的维度必须与 `w` 的长度兼容。这在我们的情况下很有用，因为为了获取第一个量子比特的期望值，我们只需计算厄米矩阵的期望值
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
- en: 'The matrix ![M](img/file704.png "M") can be constructed as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 ![M](img/file704.png "M") 可以按照以下方式构建：
- en: '[PRE8]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this construction, we have used the fact that ![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|"),
    as we discussed in an exercise earlier in this chapter. This will give us, as
    output, a value between ![0](img/file12.png "0") and ![1](img/file13.png "1"),
    which is perfect to construct a classifier: as usual, we will assign class ![1](img/file13.png
    "1") to every data instance with a value of ![0.5](img/file1166.png "0.5") or
    higher, and class ![0](img/file12.png "0") to all the rest.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个构建中，我们使用了之前在本章中讨论的事实，即 ![M = \left| 0 \right\rangle\left\langle 0 \right|](img/file1388.png
    "M = \left| 0 \right\rangle\left\langle 0 \right|")，这将给我们一个介于 ![0](img/file12.png
    "0") 和 ![1](img/file13.png "1") 之间的输出值，这对于构建分类器非常完美：像往常一样，我们将值大于等于 ![0.5](img/file1166.png
    "0.5") 的每个数据实例分配为类别 ![1](img/file13.png "1")，而将所有其他值分配为类别 ![0](img/file12.png
    "0")。
- en: 'Now we have all the pieces gathered in order to implement our quantum neural
    network. We are going to construct it as a quantum node with two arguments: `inputs`
    and `theta`. The first argument is mandatory: in order for PennyLane to be able
    to train a quantum neural network with TensorFlow, its first argument must accept
    an array with all the inputs to the network, and the name of this argument must
    be `inputs`. After this argument, we may add as many as we want. These can correspond
    to any parameters of the circuit, and, of course, they need to include the optimizable
    parameters in the variational form.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经收集了所有必要的组件来实现我们的量子神经网络。我们将将其构建为一个具有两个参数的量子节点：`inputs` 和 `theta`。第一个参数是强制性的：为了让
    PennyLane 能够使用 TensorFlow 训练量子神经网络，它的第一个参数必须接受一个包含网络所有输入的数组，并且这个参数的名称必须是 `inputs`。在这个参数之后，我们可以添加尽可能多的参数。这些可以对应于电路的任何参数，当然，它们需要包括变分形式中的可优化参数。
- en: 'Thus, we may implement our quantum neural network as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以这样实现我们的量子神经网络：
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To keep things simple, we have chosen to use just one repetition of the variational
    form. If your dataset is more complex, you may want to increase this number in
    order to have more trainable parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们选择只使用一次变分形式的重叠。如果你的数据集更复杂，你可能需要增加这个数字，以便有更多的可训练参数。
- en: Notice, by the way, how we have added the argument `interface` `=` `"``tf``"`
    to the quantum node initializer. This is so that the quantum node will work with
    tensors (TensorFlow’s data object) in lieu of with arrays, just to allow PennyLane
    to communicate smoothly with TensorFlow. Had we used the `@qml``.``qnode` decorator,
    we would’ve had to include the argument in its call.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，注意我们是如何在量子节点初始化器中添加了 `interface` `=` `"``tf``"` 参数的。这样做是为了让量子节点能够使用张量（TensorFlow
    的数据对象）而不是数组来工作，以便 PennyLane 能够与 TensorFlow 平滑通信。如果我们使用了 `@qml``.``qnode` 装饰器，我们就需要在它的调用中包含这个参数。
- en: This defines the quantum node that implements our quantum neural network. Now
    we need to figure out a way to train it, and, for that purpose, we will rely on
    TensorFlow. We’ll do exactly that in the next subsection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了实现我们的量子神经网络的量子节点。现在我们需要找出一种方法来训练它，为此我们将依赖 TensorFlow。我们将在下一小节中这样做。
- en: 10.2.3 Using TensorFlow with PennyLane
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.3 使用 TensorFlow 与 PennyLane
- en: In *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine Learning?*,
    we already learned how TensorFlow can be used to train a classical neural network.
    Well, thanks to PennyLane’s great interoperability, we will now be able to train
    our quantum neural network with TensorFlow almost as if it were a classical one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第* [*8*](ch017.xhtml#x1-1390008)*章“什么是量子机器学习？”中，我们已经学习了如何使用 TensorFlow 训练经典神经网络。好吧，多亏了
    PennyLane 的出色互操作性，我们现在几乎可以像训练经典神经网络一样训练我们的量子神经网络。
- en: To learn more…
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: PennyLane can also be integrated with other classical machine learning frameworks
    such as PyTorch. In addition, it provides its own tools to train models based
    on the NumPy package, but these are more limited.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLane 还可以与其他经典机器学习框架集成，例如 PyTorch。此外，它还提供了基于 NumPy 包的模型训练工具，但这些功能更为有限。
- en: 'Remember how we could construct TensorFlow models using Keras layers and joining
    them in sequential models? Look at this:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们是如何使用 Keras 层构建 TensorFlow 模型并将它们组合成顺序模型吗？看看这个例子：
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is how you can create a Keras layer containing our quantum neural network
    — just as if it were any other layer in a classical model! In order to do this,
    we’ve had to call `qml``.``qnn``.``KerasLayer`, and we’ve had to pass a few things
    to it. First, of course, we’ve sent the quantum node with the neural network.
    Then, a dictionary is indexed by the names of all the node arguments that take
    the optimizable parameters, and specifies, for each of these arguments, the number
    of parameters that they take. Since we only have one such argument, `theta`, and
    it should contain ![8](img/file506.png "8") optimizable parameters (that is, it
    will be an array of length ![8](img/file506.png "8")), we have sent in `{``"``theta``:`
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何创建一个包含我们的量子神经网络的 Keras 层——就像它是一个经典模型中的任何其他层一样！为了做到这一点，我们不得不调用 `qml``.``qnn``.``KerasLayer`，并且我们必须向它传递一些东西。首先，当然，我们发送了包含神经网络的量子节点。然后，一个字典通过所有接受可优化参数的节点参数名称索引，并为每个这些参数指定了它们接受的参数数量。由于我们只有一个这样的参数，即
    `theta`，并且它应该包含 ![8](img/file506.png "8") 个可优化参数（即，它将是一个长度为 ![8](img/file506.png
    "8") 的数组），所以我们发送了 `{``"``theta``:`
- en: '`8}`. Lastly, we’ve had to specify the dimension of the output of the quantum
    node; since it only returns a numerical expectation value, this dimension is ![1](img/file13.png
    "1").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`8}`。最后，我们不得不指定量子节点的输出维度；因为它只返回一个数值期望值，所以这个维度是![1](img/file13.png "1")。'
- en: 'Once we have a quantum layer, we can create a Keras model easily:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了量子层，我们就可以轻松地创建一个Keras模型：
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability to integrate quantum nodes into neural networks with such a level
    of flexibility will enable us to easily construct more complex model architectures
    in the following chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 能够以这种程度的灵活性将量子节点集成到神经网络中，将使我们能够轻松地在下一章构建更复杂的模型架构。
- en: 'Having our model ready, we now have to pick an optimizer and a loss function,
    and then we can compile the model just like any classical model. In our case,
    we will use the binary cross entropy loss (because we are training a binary classifier,
    after all) and we will rely on the Adam optimizer with a learning rate of ![0.005](img/file1389.png
    "0.005"). For the remaining parameters of the optimizer, we will trust the default
    values. Our code is, then, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型准备就绪后，我们现在必须选择一个优化器和损失函数，然后我们可以像任何经典模型一样编译模型。在我们的情况下，我们将使用二元交叉熵损失（因为我们毕竟在训练一个二元分类器）并依赖于学习率为![0.005](img/file1389.png
    "0.005")的Adam优化器。对于优化器的其余参数，我们将信任默认值。因此，我们的代码如下：
- en: '[PRE12]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In addition to this, we will use early stopping on the validation loss with
    a patience of two epochs by using the following instructions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用以下指令在验证损失上使用早停机制，耐心设置为两个epoch：
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And we are now ready to send the final instruction to get our model trained.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好发送最终指令来训练我们的模型。
- en: To learn more…
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: You may remember that, at some point in this chapter, we discussed the different
    ways in which gradients involving quantum neural networks could be computed. And
    you might wonder why we haven’t had to deal with that in order to get our model
    trained.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，在本章的某个时候，我们讨论了涉及量子神经网络的梯度可以计算的不同方式。你可能会想知道为什么我们不需要处理这些来训练我们的模型。
- en: It turns out that PennyLane already picks the best differentiation method for
    us in order to compute gradients. Each quantum node can use certain differentiation
    methods — for instance, nodes with devices that act as interfaces to real hardware
    can’t use automatic differentiation methods, but nodes with simulators can, and
    most do.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，PennyLane已经为我们选择了最佳微分方法来计算梯度。每个量子节点可以使用某些微分方法——例如，充当真实硬件接口的设备上的节点不能使用自动微分方法，但具有模拟器的节点可以，而且大多数都可以。
- en: Later in this section, we will discuss in detail all the differentiation methods
    that can be used in PennyLane.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的后面，我们将详细讨论在PennyLane中可以使用的所有微分方法。
- en: 'To train our model, we just have to call the `fit` method. Since we will be
    using early stopping, we will be generous with the number of epochs and set it
    to ![50](img/file1390.png "50"). Also, we will fix a batch size of ![20](img/file588.png
    "20"). For that, we can use the following piece of code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练我们的模型，我们只需调用`fit`方法。由于我们将使用早停机制，我们将慷慨地设置epoch的数量，将其设置为![50](img/file1390.png
    "50")。此外，我们将设置批大小为![20](img/file588.png "20")。为此，我们可以使用以下代码片段：
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output that you will get upon running this instruction will be similar
    to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此指令后，你将得到以下类似的结果：
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To learn more…
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: If you followed all that we’ve done so far without having asked TensorFlow to
    work with doubles, everything would work just fine — although you would get slightly
    different results. Nonetheless, if you try to fit a model using the Lightning
    simulator, you do need to ask TensorFlow to use doubles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你到目前为止一直按照我们的步骤进行，而没有要求TensorFlow使用双精度浮点数，那么一切都会正常工作——尽管你可能会得到略微不同的结果。然而，如果你尝试使用Lightning模拟器拟合模型，你确实需要要求TensorFlow使用双精度浮点数。
- en: Note that we have manually shrunk the progress bar so that the output could
    fit within the width of the page. Also, keep in mind that the execution time may
    vary from device to device, but, in total, the training shouldn’t take more than
    ![20](img/file588.png "20") minutes on an average
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已手动缩小进度条，以便输出能够适应页面宽度。此外，请记住，执行时间可能会因设备而异，但总体而言，训练在平均设备上不应超过![20](img/file588.png
    "20")分钟。
- en: computer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: computer.
- en: Just by looking at the raw output, we can already see that the model is indeed
    learning, because there is a very significant drop in both the training and validation
    losses as the training progresses. It could be argued that there might be a tiny
    amount of overfitting, because the drop in the training loss is slightly greater
    than that in the validation loss. In any case, let’s wait until we have a look
    at the accuracies before coming to any final conclusions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 只需查看原始输出，我们就可以看出模型确实在学习，因为随着训练的进行，训练和验证损失都有非常显著的下降。可以争辩说可能存在一点过拟合，因为训练损失的下降略大于验证损失。无论如何，让我们在查看准确率之前，不要得出任何最终结论。
- en: 'In this case, the training has only run for ![16](img/file619.png "16") epochs,
    so it’s easy to get insights from the output returned by TensorFlow. Nonetheless,
    in the real world, training processes can go on for up to very large numbers of
    epochs, and, needless to say, in those situations the console output isn’t particularly
    informative. In general, it’s always a good practice to plot both the training
    and validation losses against the number of epochs, just to get a better insight
    into the performance of the training process. We can do this with the following
    instructions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练只进行了![16](img/file619.png "16")个epoch，因此很容易从TensorFlow返回的输出中获得洞察。然而，在现实世界中，训练过程可能持续进行到非常大的epoch数量，不用说，在这些情况下，控制台输出并不特别具有信息量。一般来说，始终绘制训练和验证损失与epoch数量的对比图，以便更好地了解训练过程的性能。我们可以使用以下指令来完成：
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’ve decided to define a function just so that we can reuse it in future training
    processes. The resulting plot is shown in *Figure* [*10.6*](#Figure10.6).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定定义一个函数，以便我们可以在未来的训练过程中重用它。生成的图表显示在*图* [*10.6*](#Figure10.6)中。
- en: '![Figure 10.6: Training and validation loss functions for every epoch](img/file1391.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6：每个epoch的训练和验证损失函数](img/file1391.png)'
- en: '**Figure 10.6**: Training and validation loss functions for every epoch'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.6**：每个epoch的训练和验证损失函数'
- en: 'And now it’s time for our final test. Let’s check the accuracy of our model
    on all our datasets to see if its performance is acceptable. This can be done
    with the following piece of code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是我们最终测试的时候了。让我们检查我们的模型在所有数据集上的准确率，看看其性能是否可接受。这可以通过以下代码片段来完成：
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Upon running this, we get a training accuracy of ![71\%](img/file1392.png "71\%"),
    a validation accuracy of ![72\%](img/file1393.png "72\%"), and a test accuracy
    of ![72\%](img/file1393.png "72\%"). These results don’t reflect any kind of overfitting.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们得到训练准确率为![71%](img/file1392.png "71%")，验证准确率为![72%](img/file1393.png
    "72%")，测试准确率为![72%](img/file1393.png "72%")。这些结果并不反映任何过拟合的情况。
- en: Instead of implementing your own variational forms, you may prefer to use one
    of PennyLane’s built-in circuits. For instance, you could use the `StronglyEntanglingLayers`
    class. You should keep in mind, however, that the resulting variational form —
    as opposed to our own implementation of two-local — won’t take a one-dimensional
    array of inputs, but a three dimensional one! In particular, this form on ![n](img/file244.png
    "n") qubits with ![l](img/file514.png "l") layers will take as input a three-dimensional
    array of size ![n \times l \times 3](img/file1394.png "n \times l \times 3").
    Remember how, in this variational form, we need ![3](img/file472.png "3") arguments
    for the rotation gates, and there are ![n](img/file244.png "n") such gates in
    each of the ![l](img/file514.png "l") layers (you can take another look at *Figure*
    * [*10.4*](#Figure10.4)).*
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是实现自己的变分形式，你可能更愿意使用PennyLane内置的电路之一。例如，你可以使用`StronglyEntanglingLayers`类。然而，你应该记住，由此产生的变分形式——与我们的两局部实现相比——不会接受一维输入数组，而是一个三维数组！特别是，这个形式在![n](img/file244.png
    "n")个量子比特和![l](img/file514.png "l")层的情况下将接受一个大小为![n \times l \times 3](img/file1394.png
    "n \times l \times 3")的三维数组作为输入。记得在这个变分形式中，我们需要![3](img/file472.png "3")个参数来旋转门，每个![l](img/file514.png
    "l")层中有![n](img/file244.png "n")个这样的门（你可以再次查看*图* * [*10.4*](#Figure10.4))）。
- en: '*If you are ever in doubt, you may call the `StronglyEntanglingLayers``.``shape`
    function specifying the number of layers and the number of qubits in the respective
    arguments `n_layers` and `n_wires`. This will return a three-tuple with the shape
    that the variational form expects.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你有任何疑问，你可以调用`StronglyEntanglingLayers.shape`函数，指定层数和量子比特数，分别在`n_layers`和`n_wires`参数中。这将返回一个三个元素的元组，表示变分形式期望的形状。'
- en: 'For example, we could redefine our previous QNN to use this variational form
    as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以重新定义我们之前的QNN，使其使用以下变分形式：
- en: '[PRE18]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this piece of code, we have stored in `nreps` the number of repetitions that
    we want in each instance of the variational form, in `weights_dim` the dimensions
    of the input that the variational form expects, and in `nweights` the number of
    inputs that each instance of the variational form will take. The rest is pretty
    self-explanatory. Inside the circuit, we’ve had to reshape the `theta` array of
    parameters to make it fit into the shape that the variational form expects; in
    order to do this, we’ve used the `tf``.``reshape` function, which can reshape
    TensorFlow’s tensors while preserving all their metadata. The `weights_strong`
    dictionary that we defined at the end is the one that we would send to TensorFlow
    when constructing the Keras layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将我们想要在每个变分形式实例中重复的次数存储在`nreps`中，将变分形式期望的输入维度存储在`weights_dim`中，并将每个变分形式实例将接受的输入数量存储在`nweights`中。其余部分相当直观。在电路内部，我们必须将参数的`theta`数组重塑以适应变分形式期望的形状；为了做到这一点，我们使用了`tf.reshape`函数，该函数可以在保留所有元数据的同时重塑TensorFlow的张量。我们定义在最后的`weights_strong`字典是我们构建Keras层时将发送给TensorFlow的字典。
- en: We’ve already learned how you can train a quantum neural network using PennyLane
    and TensorFlow. We shall now discuss a few technical details in depth before bringing
    this section to an end.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何使用PennyLane和TensorFlow训练量子神经网络。在结束本节之前，我们将深入讨论一些技术细节。
- en: 10.2.4 Gradient computation in PennyLane
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.4 PennyLane中的梯度计算
- en: As we have already mentioned, when you train a model with PennyLane, the framework
    itself figures out the best way in which to compute gradients. Different quantum
    nodes may be compatible with different methods of differentiation based on a variety
    of factors, most notably the kind of device they use.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，当你使用PennyLane训练模型时，框架本身会找出计算梯度的最佳方式。不同的量子节点可能基于各种因素与不同的微分方法兼容，最显著的是它们使用的设备类型。
- en: To learn more…
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: For an up-to-date reference of the differentiation methods that the `default``.`
    `qubit` simulator supports, you may check the online documentation at [https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`default` `qubit`模拟器支持的微分方法的最新参考，你可以查看在线文档[https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations)。
- en: You will see that the compatibility of a quantum node with a differentiation
    method not only depends on the device itself but also on the return type of the
    node and the machine learning interface (in our case, the interface was TensorFlow).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现量子节点与微分方法的兼容性不仅取决于设备本身，还取决于节点的返回类型和机器学习接口（在我们的案例中，接口是TensorFlow）。
- en: 'These are the differentiation methods that can be used in PennyLane:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是可以在PennyLane中使用的微分方法：
- en: '**Backpropagation**: Just the good old backpropagation method that is used
    in classical neural networks. Of course, this differentiation method only works
    on simulators that are compatible with automatic differentiation, because that
    is what is needed in order to analytically compute the gradients.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：这只是经典神经网络中使用的良好旧的反向传播方法。当然，这种微分方法仅在兼容自动微分的模拟器上工作，因为这是分析计算梯度的必要条件。'
- en: The name of this method in PennyLane is `"``backprop``"`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在PennyLane中，这种方法的名字是`"backprop"`。
- en: '**Adjoint differentiation**: This is a more efficient version of backpropagation
    that relies on some of the nice computational ”oddities” of quantum computing,
    such as the fact that all the quantum circuits are implemented by unitary matrices,
    which are trivially invertible. Like backpropagation, this method only works on
    the simulators that are compatible with automatic differentiation, but it is more
    restrictive.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伴随微分**：这是依赖量子计算的一些计算“怪异”特性的更有效版本的反向传播，例如，所有量子电路都是由酉矩阵实现的，这些矩阵可以简单地求逆。像反向传播一样，这种方法仅在兼容自动微分的模拟器上工作，但它更为限制性。'
- en: The name of this method in PennyLane is `"``adjoint``"`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在PennyLane中，这种方法的名字是`"adjoint"`。
- en: '**Finite differences**: Ever took a numerical analysis course at college? Then
    this will sound familiar. This method implements the old-school way of computing
    a numerical approximation of a gradient that we discussed in the previous section.
    It works on almost every quantum node.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限差分法**：你在大学里上过数值分析课程吗？那么这听起来会很熟悉。这种方法实现了我们在上一节中讨论的旧式计算梯度数值近似的方法。它几乎适用于每个量子节点。'
- en: The name of this method in PennyLane is `"``finite``-``diff``"`.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在PennyLane中，这种方法的名字是 `"``finite``-``diff``"`.
- en: '**Parameter shift rule**: PennyLane fully implements the parameter-shift rule
    that we introduced previously. It works on most quantum nodes.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数平移规则**：PennyLane完全实现了我们之前介绍过的参数平移规则。它适用于大多数量子节点。'
- en: The name of this method in PennyLane is `"``parameter``-``shift``"`.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在PennyLane中，这种方法的名字是 `"``parameter``-``shift``"`.
- en: '**Device gradient computation**: Some devices provide their own way of computing
    gradients. The name of the corresponding differentiation method is `"``device``"`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备梯度计算**：一些设备提供了自己的计算梯度的方法。相应的微分方法名为 `"``device``"`.'
- en: There are a couple of things that deserve clarification; the first of them is
    how a simulator could not be compatible with automatic differentiation. Oversimplifying
    a little bit, most simulators work by computing the evolution of the quantum state
    of a circuit and returning an output that is differentiable with respect to the
    parameters. The operations required to do all of this are themselves differentiable,
    and hence it’s possible to use automatic differentiation on quantum nodes that
    use that simulator. But simulators may work differently. For instance, a simulator
    could return individual shots in a way that ”breaks” the differentiability of
    the computation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 有几件事情需要澄清；其中之一是模拟器如何与自动微分不兼容。稍微简化一下，大多数模拟器通过计算电路的量子态的演化并返回一个相对于参数可微的输出来工作。执行所有这些操作所需的操作本身是可微的，因此可以在使用该模拟器的量子节点上使用自动微分。但是模拟器可能工作方式不同。例如，一个模拟器可能会以“破坏”计算可微性的方式返回单个射击。
- en: Another thing that may have caught your attention is that the finite difference
    method can be used on ”most” quantum nodes, but not on all of them. That’s because
    some quantum nodes may return outputs that don’t make it possible for the finite
    differences method to work with them. For instance, if a node returns an array
    of samples, the differentiability is broken. If instead, it returned an expectation
    value — even if it were just an empirical approximation obtained from a collection
    of samples — then a gradient would exist and the finite differences method could
    be used to compute it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件可能引起你注意的事情是，有限差分法可以用于“大多数”量子节点，但不是所有。这是因为一些量子节点可能返回的输出使得有限差分法无法与它们一起工作。例如，如果一个节点返回一个样本数组，可微性就会中断。相反，如果它返回一个期望值——即使它只是从样本集合中获得的经验近似——那么就会存在梯度，并且可以使用有限差分法来计算它。
- en: Exercise 10.3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.3
- en: List all the PennyLane differentiation methods that can be used on quantum hardware
    and all the differentiation methods that can be used on simulators.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列出所有可以在量子硬件上使用的PennyLane微分方法以及所有可以在模拟器上使用的微分方法。
- en: The way in which you can ask PennyLane to use a specific differentiation method
    — let’s say one named `"``method``"` — is by passing the optional argument `diff_method`
    `=` `"``method``"` to the quantum node decorator or initializer. That is, if you
    use the QNode decorator, you should write
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将可选参数 `diff_method` `=` `"``method``"` 传递给量子节点装饰器或初始化器来请求PennyLane使用特定的微分方法——比如说一个名为
    `"``method``"` 的方法。也就是说，如果你使用QNode装饰器，你应该写
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Alternatively, if you decided to assemble a circuit `circuit` and a device
    `device` into a quantum node directly, you should call the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你决定直接将电路 `circuit` 和设备 `device` 组装成一个量子节点，你应该调用以下操作：
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By default, `diff_method` is set to `"``best``"`, which, as we’ve said before,
    lets PennyLane choose on our behalf the best differentiation method.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`diff_method` 被设置为 `"``best``"`，正如我们之前所说的，这会让PennyLane代表我们选择最佳微分方法。
- en: In our particular case, PennyLane has been using the backpropagation differentiation
    method all this time — without us even noticing!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特定的案例中，PennyLane一直使用反向传播微分法，而我们甚至没有注意到这一点！
- en: To learn more…
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: 'If you want to know which differentiation method PennyLane uses by default
    on a device `dev` and on a certain interface `inter` (in our case, `"``tensorflow``"`),
    you can just call the following function:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道PennyLane在设备`dev`和特定接口`inter`（在我们的案例中，`"tensorflow"`）上默认使用的微分方法，你可以直接调用以下函数：
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our quantum node is compatible with all the differentiation methods except with
    device differentiation, because `default``.``qubit` doesn’t implement its own
    special way of computing gradients. Thus, just to better understand the differences
    in performance, we can try out all the differentiation methods and see how they
    behave.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的量子节点与所有微分方法兼容，除了设备微分，因为`default` `qubit`没有实现自己的特殊计算梯度方式。因此，为了更好地理解性能差异，我们可以尝试所有微分方法并观察它们的运行情况。
- en: To learn more…
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息...
- en: You may remember that, when using the Lightning simulator, we do need to ask
    TensorFlow to use doubles all across the Keras model instead of floats — it’s
    not an option, but a necessity. The same happens when we use differentiation methods
    other than backpropagation with `default``.` `qubit`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，当使用Lightning模拟器时，我们确实需要要求TensorFlow在整个Keras模型中使用双精度浮点数而不是单精度浮点数——这不是一个选项，而是一个必需品。当我们使用除反向传播之外的微分方法与`default`
    `qubit`时，情况也是如此。
- en: 'Let’s begin with adjoint differentiation. In order to retrain our model with
    this differentiation method, we will rerun all our previous code, but changing
    the quantum node definition to the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从伴随微分开始。为了使用这种微分方法重新训练我们的模型，我们将重新运行所有之前的代码，但将量子节点定义更改为以下内容：
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reasonably enough, instead of rerunning all your code, you may want to add
    the execution of alternative differentiation methods as part of it — particularly
    if you are keeping your code in a notebook. If you want to do so while ensuring
    that the training is done in identical conditions (the same environment and seeds),
    these are the lines that you would have to run:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 足够合理的是，你可能会想在不重新运行所有代码的情况下，将替代微分方法的执行作为其中的一部分——尤其是如果你将代码保存在笔记本中。如果你想确保在相同条件下（相同的环境和种子）完成训练，以下是你必须运行的这些行：
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Upon running this, you will get the exact same training behavior that we got
    with backpropagation — the same evolution of the training and validation losses
    and, of course, the same accuracies. Where there is a noticeable change, however,
    is in training time. In our case, training with backpropagation took a rough average
    of ![21](img/file1395.png "21") seconds per epoch. Using adjoint differentiation,
    in contrast, the training took, on average, ![10](img/file161.png "10") seconds
    per epoch. That’s a big gain!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作后，你将获得与反向传播相同的精确训练行为——相同的训练和验证损失演变，当然，还有相同的准确率。然而，值得注意的是，训练时间有所不同。在我们的案例中，使用反向传播进行训练的平均时间大约为![21](img/file1395.png
    "21")秒每轮。相比之下，使用伴随微分，平均每轮训练时间为![10](img/file161.png "10")秒。这是一个巨大的进步！
- en: Actually, if you wanted to further reduce the training time, you should try
    the Lightning simulator with the adjoint method. Depending on the hardware configuration
    of your computer, it can yield very significant boosts in performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果你想进一步减少训练时间，你应该尝试使用伴随方法与Lightning模拟器。根据你电脑的硬件配置，它可以在性能上带来非常显著的提升。
- en: 'Let’s now train our model with the two remaining differentiation methods, which
    are the hardware-compatible ones: the parameter-shift rule and finite differences.
    In order to do that, we will just have to rerun our code changing the value of
    the differentiation method in the quantum node definition. In order to avoid redundancy,
    we won’t rewrite everything here — we trust these small changes to you!'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用剩下的两种微分方法来训练我们的模型，这两种方法是硬件兼容的：参数移位规则和有限差分。为了做到这一点，我们只需重新运行我们的代码，更改量子节点定义中的微分方法值。为了避免冗余，我们不会在这里重写所有内容——我们相信这些小的变化可以由你来完成！
- en: 'When retraining with these two models, these are the results we obtained:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这两种模型重新训练时，我们得到了以下结果：
- en: Using the parameter shift rule yielded the very same results as the other differentiation
    methods. Regarding training time, each epoch took, on average, ![14](img/file1396.png
    "14") seconds to complete. That’s better than the ![21](img/file1395.png "21")
    seconds that we got with backpropagation, but not as good as the ![10](img/file161.png
    "10") seconds that the adjoint method gave us.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用参数平移规则得到了与其他微分方法相同的结果。关于训练时间，每个epoch平均需要![14](img/file1396.png "14")秒来完成。这比我们用反向传播得到的![21](img/file1395.png
    "21")秒要好，但不如adjoint方法给出的![10](img/file161.png "10")秒好。
- en: When using finite differences differentiation, we got, once again, the same
    results that the other methods yielded. On average, each epoch took ![10](img/file161.png
    "10") seconds to complete, which matches the training time of adjoint differentiation.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用有限差分微分时，我们再次得到了与其他方法相同的结果。平均而言，每个epoch需要![10](img/file161.png "10")秒来完成，这与adjoint微分法的训练时间相匹配。
- en: Keep in mind that this comparison holds for the particular model that we have
    considered. The results may vary as the complexity of the models increases and,
    in particular, hardware-compatible methods may perform more poorly on simulators
    when training complex QNN architectures.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个比较仅适用于我们考虑的特定模型。随着模型复杂性的增加，结果可能会有所不同，特别是与硬件兼容的方法在训练复杂的QNN架构时可能在模拟器上表现得更差。
- en: And that’s probably all you need to know about the differentiation methods that
    are available in PennyLane. Let’s now have a look at what Qiskit has to offer
    in terms of quantum neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您需要了解的关于PennyLane中可用的微分方法的全部内容。现在让我们看看Qiskit在量子神经网络方面能提供什么。
- en: '10.3 Quantum neural networks in Qiskit: a commentary'
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.3 Qiskit中的量子神经网络：评论
- en: In the previous section, we had a chance to explore in great depth the implementation
    and training of quantum neural networks in PennyLane. We won’t do an analogous
    discussion for Qiskit in such a level of detail, but we will at least give you
    a few ideas about how to get started should you ever need to use Qiskit in order
    to work with quantum neural networks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们有机会深入探讨PennyLane中量子神经网络的实现和训练。我们不会以如此详细的程度对Qiskit进行类似的讨论，但至少会给你一些关于如何开始使用Qiskit来处理量子神经网络的想法。
- en: PennyLane provides a very homogeneous and flexible experience. No matter if
    you’re training a simple binary classifier or a complex hybrid architecture like
    the ones we will study in the following chapter, it’s all done in the same way.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLane提供了一个非常统一和灵活的体验。无论您是在训练一个简单的二分类器，还是像我们在下一章将要研究的那样复杂的混合架构，都是用同样的方式完成的。
- en: Qiskit, by contrast, provides a more ”structural” approach. It gives you a suite
    of classes that can be used to train different kinds of neural networks and that
    allow you to define your networks in different ways. It’s difficult to judge whether
    this is a better or worse approach; in the end, it’s just a matter of taste. On
    the one hand, training basic models in Qiskit might be simpler than training them
    in PennyLane because of the ease of use of some of these purpose-built classes.
    On the other hand, having different ways of accomplishing the same thing — one
    could argue — might generate some unnecessary complexity.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Qiskit提供了一种更“结构化”的方法。它提供了一系列可以用来训练不同类型神经网络的类，并允许您以不同的方式定义您的网络。很难判断这是否是一种更好或更差的方法；最终，这只是个人口味的问题。一方面，由于一些专为特定目的设计的类易于使用，在Qiskit中训练基本模型可能比在PennyLane中训练它们要简单。另一方面，有不同方式完成同一件事——有人可能会说——可能会产生一些不必要的复杂性。
- en: 'The classes provided by Qiskit for the implementation of quantum neural networks
    can be imported from `qiskit_machine_learning``.``neural_networks` (please, refer
    to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing the Tools*, for installation
    instructions). These are some of them:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Qiskit为量子神经网络的实现提供的类可以从`qiskit_machine_learning``.``neural_networks`导入（请参阅*附录*
    [*D*](ch027.xhtml#x1-240000D)*，安装工具*，以获取安装说明）。以下是一些例子：
- en: '**Two-layer QNN**: The `TwoLayerQNN` class can be used to implement a quantum
    neural network with a single feature map, a variational form, and an observable.
    It works for any vanilla quantum neural network.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双层QNN**：`TwoLayerQNN`类可以用来实现一个具有单个特征图、变分形式和可观察量的量子神经网络。它适用于任何普通的量子神经网络。'
- en: '**Circuit QNN**: The `CircuitQNN` class allows you to implement a quantum neural
    network from a parametrized circuit. The final state of the circuit will be measured
    on the computational basis, and each measurement result can be mapped to an integer
    label through an interpreter function. This can be useful, for instance, if you
    want to build a classifier.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电路QNN**：`CircuitQNN`类允许你从一个参数化电路中实现一个量子神经网络。电路的最终状态将在计算基上被测量，并且每个测量结果可以通过一个解释函数映射到一个整数标签。这很有用，例如，如果你想构建一个分类器。'
- en: 'By the way, in Qiskit lingo, variational forms are called **ansatzs**. As you
    surely remember, this is also the name that was used in the context of the VQE
    algorithm that we studied in *Chapter* *[*7*](ch015.xhtml#x1-1190007)*, VQE: Variational
    Quantum* *Eigensolver*.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在Qiskit的术语中，变分形式被称为**ansatzs**。正如你肯定记得的，这个名字也用于我们在*第7章*中研究的VQE算法的上下文中，VQE：变分量子*本征值求解器*。
- en: '*If, when designing a neural network in Qiskit, you want to use the ZZ feature
    map or the two-local variational form, there’s no need for you to re-implement
    them; they are bundled with Qiskit. You can get them as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你在Qiskit中设计神经网络时想使用ZZ特征图或两个局部的变分形式，你不需要重新实现它们；它们包含在Qiskit中。你可以如下获取它们：'
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the call to the ZZ feature map class, we have set the number of repetitions
    to ![1](img/file13.png "1") — any other number would yield a feature map with
    that number of repetitions of the ZZ feature map scheme. In the call to the two-local
    class, we have also specified — in addition to the repetitions — the rotation
    gates, the controlled gates, and the entanglement layout that we want to use.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用ZZ特征图类时，我们设置了重复次数为![1](img/file13.png "1")——任何其他数字都会产生具有该数量重复的ZZ特征图方案的特征图。在调用两个局部类时，我们还指定了——除了重复次数之外——我们想要使用的旋转门、受控门和纠缠布局。
- en: 'For the sake of example, we can define a `TwoLayer` quantum neural network
    on three qubits with the ZZ feature map and two-local variational form that we
    have just instantiated. We can do this as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例，我们可以定义一个在三个量子比特上使用ZZ特征图和两个局部变分形式的`TwoLayer`量子神经网络。我们可以这样做：
- en: '[PRE25]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we haven’t specified an observable, the resulting QNN will return the
    expectation value of the ![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes
    Z \otimes Z") observable measured after feeding the execution of the network’s
    circuit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有指定一个可观察量，所以结果QNN将返回在将网络的电路执行后测量的![Z \otimes Z \otimes Z](img/file1397.png
    "Z \otimes Z \otimes Z")可观察量的期望值。
- en: 'We can simulate analytically the network that we have just created on some
    random inputs and optimizable parameters as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在一些随机输入和可优化参数上，如下模拟我们刚刚创建的网络：
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first argument is an array with some (random) classical inputs while the
    second argument is an array with (random) values for the optimizable parameters.
    Notice how we’ve used the `qnum_inputs` and `num_weights` properties of the quantum
    neural network.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是一个包含一些（随机）经典输入的数组，而第二个参数是一个包含（随机）可优化参数值的数组。注意我们是如何使用量子神经网络的`qnum_inputs`和`num_weights`属性的。
- en: All the neural network classes that we have presented are subclasses of a `NeuralNetwork`
    class. For example, should you want to train a neural network as a classifier,
    you could rely on Qiskit’s `NeuralNetworkClassifier` class. This class can be
    initialized with a `NeuralNetwork` object and specifying a loss function and an
    optimizer among other things.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所展示的所有神经网络类都是`NeuralNetwork`类的子类。例如，如果你想将神经网络训练为一个分类器，你可以依赖Qiskit的`NeuralNetworkClassifier`类。这个类可以通过一个`NeuralNetwork`对象以及指定损失函数和优化器等来初始化。
- en: In addition to this, there is a subclass of `NeuralNetworkClassifier` that can
    be used to readily create a trainable neural network classifier directly, providing
    a feature map, a variational form, an optimizer, a loss, and so on.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个`NeuralNetworkClassifier`的子类可以被用来直接创建一个可训练的神经网络分类器，提供特征图、变分形式、优化器、损失函数等。
- en: This subclass is called `VQC` (short for Variational Quantum Classifier) and
    it can also be imported from the Qiskit module `qiskit_machine_learning``.``algorithms``.``classifiers`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这个子类被称为`VQC`（代表变分量子分类器），也可以从Qiskit模块`qiskit_machine_learning``.``algorithms``.``classifiers`中导入。
- en: 'If you wanted to create a neural network classifier object from our previous
    `qnn` object using the default parameters provided by Qiskit, you could run the
    following instructions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用Qiskit提供的默认参数从我们之前的`qnn`对象创建一个神经网络分类器对象，你可以运行以下指令：
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By default, the classifier will use the squared error loss function and rely
    on the SLSQP optimizer [[62](ch030.xhtml#Xkraft1988software)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，分类器将使用平方误差损失函数并依赖于SLSQP优化器[[62](ch030.xhtml#Xkraft1988software)]。
- en: 'Then, if you had some training data `data_train` with labels `labels_train`,
    you could train your newly-created classifier by calling the `fit` method as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果你有一些带有标签`labels_train`的训练数据`data_train`，你可以通过调用`fit`方法来训练你新创建的分类器，如下所示：
- en: '[PRE28]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you then wanted to compute the outcomes of the trained classifier on some
    data `data_test`, you could use the `predict` method like so:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要计算训练好的分类器在某个数据`data_test`上的结果，你可以使用`predict`方法如下：
- en: '[PRE29]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Alternatively, if you wanted to compute the accuracy score of the trained model
    on some test dataset (`data_test` and `labels_test`), you could run the following
    instruction:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想要计算训练模型在某个测试数据集（`data_test`和`labels_test`）上的准确度得分，你可以运行以下指令：
- en: '[PRE30]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Nevertheless, you shouldn’t care too much about the `NeuralNetworkClassifier`
    and `VQC` classes because, as it turns out, there is an alternative — and, in
    our opinion, better — way to train QNNs in Qiskit. We will discuss it in the following
    chapter, and it will involve an interface with an existing machine learning framework,
    PyTorch. What is more, being able to work with this interface will allow us to
    explore Qiskit’s ”Torch Runtime”: a Qiskit utility that will enable us to more
    efficiently train QNNs on IBM’s real quantum hardware. This is the same technique
    that we used in *Chapter* *[*5*](ch013.xhtml#x1-940005)*, QAOA:* *Quantum Approximate
    Optimization Algorithm*, to run QAOA executions on quantum hardware. Exciting,
    isn’t it? Bear with us until the end of the next chapter.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你不必过于关注`NeuralNetworkClassifier`和`VQC`类，因为正如我们所发现的那样，有一个替代方案——并且，在我们看来，这是一个更好的方法来在Qiskit中训练QNNs。我们将在下一章中讨论它，它将涉及与现有机器学习框架PyTorch的接口。更重要的是，能够使用这个接口将使我们能够探索Qiskit的“Torch运行时”：这是一个Qiskit实用工具，将使我们能够更有效地在IBM的真实量子硬件上训练QNNs。这正是我们在*第*
    *[*5*](ch013.xhtml#x1-940005)* *章*，QAOA:* *量子近似优化算法*中使用的相同技术，用于在量子硬件上运行QAOA执行。令人兴奋，不是吗？请耐心等待下一章的结尾。
- en: '*# Summary'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*# 摘要'
- en: This has been a long journey, hasn’t it? In this chapter, we first introduced
    quantum neural networks as quantum analogs of classical neural networks. We have
    seen how the training of a quantum neural network is very similar to that of a
    classical one, and we’ve also explored the differentiation methods that make this
    possible.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经是一段漫长的旅程了，不是吗？在本章中，我们首先介绍了量子神经网络作为经典神经网络的量子模拟。我们看到了量子神经网络的训练过程与经典神经网络非常相似，我们还探讨了使这成为可能的不同化方法。
- en: With the theory out of the way, we got our keyboards ready to do some work.
    We learned how to implement and train a quantum neural network using PennyLane,
    and we also discussed some technicalities about this framework, such as details
    about the differentiation methods that it provides.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 理论部分结束后，我们准备好了键盘开始工作。我们学习了如何使用PennyLane实现和训练量子神经网络，我们还讨论了关于这个框架的一些技术细节，例如它提供的不同化方法。
- en: PennyLane comes with some wonderful simulators, but — as we already mentioned
    in *Chapter* [*2*](ch009.xhtml#x1-400002)*, The Tools of the Trade in Quantum
    Computing* — it’s also integrated with quantum hardware platforms such as Amazon
    Braket and IBM Quantum. Thus, your ability to train quantum neural networks on
    actual quantum computers is at your fingertips!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLane附带一些出色的模拟器，但——正如我们已经在*第* *[*2*](ch009.xhtml#x1-400002)* *章*，量子计算的工具中所提到的——它还与量子硬件平台（如Amazon
    Braket和IBM Quantum）集成。因此，你能够在实际量子计算机上训练量子神经网络的能力就在你的指尖！
- en: We concluded the chapter with a short overview of how to work with quantum neural
    networks in Qiskit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的结尾简要概述了如何在Qiskit中处理量子神经网络。
- en: By now, you have a solid understanding of quantum neural networks. Combined
    with your previous knowledge of quantum support vector machines, this gives you
    a fairly solid foundation in quantum machine learning. In the following chapter
    — which will be very practically-oriented — we will explore more complex model
    architectures based on quantum neural networks.*******
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经对量子神经网络有了扎实的理解。结合你之前对量子支持向量机的知识，这为你打下了量子机器学习相当坚实的基础。在接下来的章节——它将非常注重实践——我们将探讨基于量子神经网络的更复杂模型架构。******
