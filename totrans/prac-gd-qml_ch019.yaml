- en: Chapter 10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantum Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: '*The mind is not a vessel to be filled, but a fire to be kindled.*'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äî Plutarch
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we explored our first family of quantum machine learning
    models: quantum support vector machines. Now it is time for us to take one step
    further and consider yet another family of models, that of **Quantum** **Neural
    Networks** (**QNNs**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how the notion of a quantum neural network
    can arise naturally from the ideas behind classical neural networks. Of course,
    you will also learn how quantum neural networks work and how they can be trained.
    Then, you will explore how quantum neural networks can actually be implemented,
    run, and trained using the two quantum frameworks that we have been working with
    so far: Qiskit and PennyLane.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the contents of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building and training quantum neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum neural networks in PennyLane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantum neural networks in Qiskit: a commentary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum support vector machines and quantum neural networks are probably the
    two most popular families of QML models, so, by the end of this chapter, you will
    already have a solid foundation in quantum machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let‚Äôs understand how quantum neural networks work and how they
    can be effectively trained. Let‚Äôs get to it!
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Building and training a quantum neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like quantum support vector machines, quantum neural networks are what
    we called ‚ÄùCQ models‚Äù back in *Chapter* [*8*](ch017.xhtml#x1-1390008), *What is
    Quantum Machine* *Learning?*, ‚Äî models with purely classical inputs and outputs
    that use quantum computing at some stage. However, unlike QSVMs, quantum neural
    networks are not a ‚Äùparticular case‚Äù of any classical model, although their behavior
    is inspired by that of classical neural networks. What is more, as we will soon
    see, quantum neural networks are ‚Äùpurely quantum‚Äù models, in the sense that their
    execution will only require classical computing for the preparation of circuits
    and the statistical analysis of measurements. Nevertheless, just like QSVMs, quantum
    neural networks will depend on classical parameters that will be optimized classically.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: As you surely know by now, (quantum) machine learning is a vast field in which
    terms hardly ever have a unique meaning. The term ‚Äùquantum neural network‚Äù can,
    in practice, be used to refer to any QML model that is inspired by the behavior
    of a classical neural network. Therefore, you should bear in mind that people
    may also use this name to refer to models different from the ones that we are
    considering to be quantum neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: That should be enough of an introduction. Let‚Äôs now get into the details. What
    actually are quantum neural networks and how do they relate to classical neural
    networks?
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 A journey from classical neural networks to quantum neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we do a small exercise of abstraction, we can think of the action of a classical
    neural network as consisting of the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: This simply amounts to taking some (classical) input
    data and maybe carrying out some (simple) transformations on it. These may include
    normalizing or scaling the input data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data processing**: Feeding the data through a sequence of layers that ‚Äùtransform‚Äù
    the data as it flows through them. The behavior of this processing depends on
    some optimizable parameters, which are adjusted in training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data output**: Returning the output through a final layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let‚Äôs see how we can take this scheme and use it to define an analogous quantum
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: Quantum neural networks are given classical inputs (in
    the form of an array of numbers), but quantum computers don‚Äôt work on classical
    data ‚Äî they work on quantum states! So how can we take these classical inputs
    and embed them into the space of quantum states?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That is a problem that we have already dealt with in *Section* [*9.2*](ch018.xhtml#x1-1660009.2).
    In order to encode the classical input of a QNN into a quantum state, we just
    have to use any feature map of our choice. As you know, we may also need to normalize
    or scale the data, of course.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And that is how we actually ‚Äùprepare the data‚Äù for a quantum neural network:
    feeding it into a feature map.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data processing**: At this point, we have successfully transformed our classical
    input into a ‚Äùquantum input,‚Äù in the form of a quantum state that encodes our
    classical data according to a certain feature map. Now, we need to figure out
    a way to process this input by drawing some inspiration from the processing in
    a classical neural network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trying to replicate the full, exact behavior of a classical neural network in
    a quantum neural network might prove not to be ideal given the state of current
    quantum hardware. Instead, we can look at the bigger picture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In essence, the processing stage of a classical neural network consists in the
    application of some transformations that depend, exclusively, on some optimizable
    parameters. And that is an idea that we can very easily export to a quantum computer.
    We can simply define the ‚Äùprocessing‚Äù stage of a quantum neural network as‚Ä¶the
    application of a circuit that depends on some optimizable parameters! In addition
    to this, as we will see later in this section, this circuit can be structured
    in layers in a way that somewhat reassembles the spirit of a classical neural
    network. This circuit will be said to be a **variational form** ‚Äî they are just
    like the ones we studied back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE:*
    *Variational Quantum Eigensolver*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data output**: Once we have a processed state, we need to return a classical
    output. And this shall be the result of some measurement operation; this operation
    can be whichever one suits our problem best!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For instance, if we wanted to build a binary classifier with a quantum neural
    network, a natural choice for this measurement operation could be, for example,
    taking the expectation value of the first qubit when measured on the computational
    basis. Remember that the expectation value of a qubit simply corresponds to the
    probability of obtaining ![1](img/file13.png "1") upon measuring the qubit on
    the computational basis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: And those are all the ingredients that make up a quantum neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a matter of fact, feature maps and variational forms are both examples of
    **variational circuits**: quantum circuits that are controlled by some classical
    parameters. The only actual difference between a feature map and a variational
    form is their purpose: feature maps depend on the input data and are used to encode
    it, while variational forms depend on optimizable parameters and are used to transform
    a quantum input state.'
  prefs: []
  type: TYPE_NORMAL
- en: This difference in purpose will materialize in the fact that we will often use
    different circuits for feature maps and variational forms. A good feature map
    need not be a good variational form, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should keep in mind that ‚Äî like all things QML ‚Äî the terms ‚Äùfeature map‚Äù
    and ‚Äùvariational form‚Äù are not entirely universal, and different authors may refer
    to them with different expressions. For example, variational forms are commonly
    referred to as **ansatzs**, as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational* *Quantum Eigensolver*.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'A quantum neural network takes a classical input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") and maps it to a quantum state through a feature map
    ![F](img/file1320.png "F"). The resulting state then goes through a variational
    form ![V](img/file379.png "V"): a variational circuit dependent on some optimizable
    parameters ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}").
    The output of the quantum neural network is the result of a measurement operation
    on the final state. All this can be seen, schematically, in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n‚Éó |FV0‚ü©((‚ÉóxùúÉ)) ](img/file1322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thanks to our study of quantum support vector machines, we are already very
    familiar with feature maps, but we have yet to get acquainted with variational
    forms; that is what we will devote the next subsection to.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Variational forms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In principle, a variational form could be any variational circuit of your choice,
    but, in general, variational forms for QNNs follow a ‚Äùlayered structure,‚Äù trying
    to mimic the spirit of classical neural networks. We can now make this idea precise.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to define a variational form with ![k](img/file317.png "k") layers,
    we could consider ![k](img/file317.png "k") vectors of independent parameters
    ![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}").
    In order to define each layer ![j](img/file258.png "j"), we may take a variational
    circuit ![G_{j}](img/file1324.png "G_{j}") dependent on the parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}"). A common approach is to prepare variational
    forms by stacking these variational circuits consecutively and separating them
    by some circuits ![U_{}](img/file1326.png "U_{}")
  prefs: []
  type: TYPE_NORMAL
- en: entÀÜt![,independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.](img/file1327.png
    ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: A variational form with k layers, each defined by a variational
    circuit G_{j} dependent on some parameters {\overset{\rightarrow}{\theta}}_{j}.
    The circuits U_{} entÀÜtareusedtocreateentanglement,andthestate \left| \psi_{}
    \right. enc\rangle denotes the output of the feature map ](img/file1331.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10.1**: A variational form with ![k](img/file317.png "k") layers,
    each defined by a variational circuit ![G_{j}](img/file1324.png "G_{j}") dependent
    on some parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}").
    The circuits ![U_{}](img/file1326.png "U_{}") entÀÜt![areusedtocreateentanglement,andthestate](img/file1328.png
    "areusedtocreateentanglement,andthestate") ![\left| \psi_{} \right.](img/file1329.png
    "\left| \psi_{} \right.") enc![\rangle](img/file1330.png "\rangle") denotes the
    output of the feature map'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now outlined one of the most common structures of variational forms,
    but variational forms are best illustrated by examples. There are lots of variational
    forms out there, and there is no way we could collect them all in this book ‚Äî
    in truth, there would be no point either. For this reason, we will restrict ourselves
    to presenting just three variational forms, some of which we will use later in
    the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-local**: The **two-local variational form** with ![k](img/file317.png
    "k") repetitions on ![n](img/file244.png "n") qubits relies on ![n \times (k +
    1)](img/file1332.png "n \times (k + 1)") optimizable parameters, which we will
    denote as ![\theta_{rj}](img/file1333.png "\theta_{rj}") with ![r = 0,\ldots,k](img/file1334.png
    "r = 0,\ldots,k") and ![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n"). Its
    circuit is constructed as per the following procedure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**procedure** TwoLocal(![n,k,\theta](img/file1336.png "n,k,\theta"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Add the* ![r](img/file1337.png
    "r")*-th* *layer.*¬†¬†¬†¬† * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") gate on
    qubit ![j](img/file258.png "j").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Create entanglement
    between layers.*¬†¬†¬†¬† * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**if** ![r < k](img/file1340.png "r < k") **then**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a CNOT gate with control on qubit ![t](img/file48.png "t") and target
    on qubit ![t + 1](img/file1342.png "t + 1").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")**** ***In *Figure* [*10.2*](#Figure10.2) we have
    depicted the output of this procedure for ![n = 4](img/file837.png "n = 4") and
    ![k = 3](img/file1344.png "k = 3"). Sound familiar? The two-local variational
    form uses the same circuit as the angle encoding feature map for its layers, and
    then it relies on a cascade of controlled-NOT operations in order to create entanglement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice, by the way, how the two-local variational form with ![k](img/file317.png
    "k") repetitions has ![k + 1](img/file1345.png "k + 1") layers, not ![k](img/file317.png
    "k"). This tiny detail can sometimes be misleading.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The two-local variational form is very versatile, and it can be used with any
    measurement operation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***   **Tree tensor**: The **tree tensor** variational form with ![k + 1](img/file1345.png
    "k + 1") layers can be applied on ![n = 2^{k}](img/file1347.png "n = 2^{k}") qubits.
    Each layer has half the number of parameters as the previous one, so the variational
    form relies on ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k
    - 1} + \cdots + 1") optimizable parameters of the form'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The procedure that defines is somewhat more opaque than that of the two-local
    variational form, and it reads as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**procedure** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On each qubit ![j](img/file258.png "j"), apply a rotation ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k
    - r} - 1") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a CNOT operation with target on qubit ![1 + s2^{r}](img/file1354.png "1
    + s2^{r}") and controlled by qubit ![1 + s2^{r} + 2^{r - 1}](img/file1355.png
    "1 + s2^{r} + 2^{r - 1}").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")
    on qubit ![1 + s2^{r}](img/file1354.png "1 + s2^{r}").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An image is worth a thousand words, so, please, refer to *Figure* [*10.3*](#Figure10.3)
    for a depiction of the output of this procedure for ![k = 3](img/file1344.png
    "k = 3").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The tree tensor variational form fits best in quantum neural networks designed
    to work as binary classifiers. The most natural measurement operation that can
    be used in conjunction with it is the obtention of the expected value of the first
    qubit, as measured in the computational basis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a curiosity, the name of the tree tensor variational form comes from mathematical
    objects that are used for the simulation of physics systems and also in some machine
    learning models. See the survey paper by Rom√°n Or√∫s for model details [[71](ch030.xhtml#Xorus2014practical)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3: Tree tensor variational form on 8 = 2^{3} qubits](img/file1358.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.3**: Tree tensor variational form on ![8 = 2^{3}](img/file1357.png
    "8 = 2^{3}") qubits'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Strongly entangling layers**: The strongly entangling layers variational
    form acts on ![n](img/file244.png "n") qubits and can have any number ![k](img/file317.png
    "k") of layers. Each layer ![l](img/file514.png "l") is given a **range** ![r_{l}](img/file1359.png
    "r_{l}"). In total, the variational form uses ![3nk](img/file1360.png "3nk") parameters
    of the form'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The form is defined by the following algorithm:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**procedure** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")
    on qubit ![j](img/file258.png "j").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")
    on qubit ![j](img/file258.png "j").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")
    on qubit ![j](img/file258.png "j").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a CNOT operation controlled by qubit ![j](img/file258.png "j") and with
    target on qubit ![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png
    "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may find a representation of a sample of this form in *Figure* [*10.4*](#Figure10.4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4: Strongly entangling layers form on four qubits and two layers
    with respective ranges 1 and 2](img/file1368.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.4**: Strongly entangling layers form on four qubits and two layers
    with respective ranges ![1](img/file13.png "1") and ![2](img/file302.png "2")**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**As a final remark, our choice to use mostly ![Y](img/file11.png "Y") rotations
    in the previous examples of variational forms is somewhat arbitrary. We could‚Äôve
    also used ![X](img/file9.png "X") rotations, for example. The same goes for our
    choice to use controlled-![X](img/file9.png "X") operations in the entanglement
    circuits. We could have used a different controlled operation, for instance. In
    addition to this, in the two-local variational form, there are more options for
    the distribution of gates in the entanglement circuit beyond the one that we have
    considered. Our entanglement circuit is said to have a ‚Äùlinear‚Äù arrangement of
    gates, but other possibilities are shown in *Figure* [*10.5*](#Figure10.5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![(a) Linear](img/file1369.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(a)** Linear'
  prefs: []
  type: TYPE_NORMAL
- en: '![(b) Circular](img/file1370.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**(b)** Circular'
  prefs: []
  type: TYPE_NORMAL
- en: '![(c) Full](img/file1371.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**(c)** Full'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 10.5**: Different entanglement circuits'
  prefs: []
  type: TYPE_NORMAL
- en: This is all we need to know, for now, about variational forms. Combined with
    our previous knowledge of feature maps, this ends our analysis of the elements
    of a quantum neural network‚Ä¶almost. We still have to dive deeper into that seemingly
    innocent measurement operation at the end of every quantum neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 A word about measurements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE: Variational
    Quantum Eigensolver*, any physical observable can be represented by a Hermitian
    operator in such a way that all the possible outcomes of the measurement of the
    observable can be matched to the different eigenvalues of the operator. If you
    haven‚Äôt done so already, please, have a look at *Section* [*7.1.1*](ch015.xhtml#x1-1210007.1.1)
    if you are not familiar with this.'
  prefs: []
  type: TYPE_NORMAL
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  prefs: []
  type: TYPE_NORMAL
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: Both of these operators represent the measurement of a qubit, but they differ
    in the eigenvalues that they associate to the distinct outputs. The first operator
    associates the eigenvalues ![1](img/file13.png "1") and ![0](img/file12.png "0")
    to the qubit‚Äôs value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.1
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this exercise is for you to get more familiar with Dirac notation.
    Show that the two previous Hermitian operators may be written, respectively, as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![1\left&#124; 0 \right\rangle\left\langle 0 \right&#124; + 0\left&#124;
    1 \right\rangle\left\langle 1 \right&#124; = \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;,\quad\left&#124; 0 \right\rangle\left\langle 0 \right&#124; - \left&#124;
    1 \right\rangle\left\langle 1 \right&#124;.](img/file1373.png "1\left&#124; 0
    \right\rangle\left\langle 0 \right&#124; + 0\left&#124; 1 \right\rangle\left\langle
    1 \right&#124; = \left&#124; 1 \right\rangle\left\langle 1 \right&#124;,\quad\left&#124;
    0 \right\rangle\left\langle 0 \right&#124; - \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;.") |'
  prefs: []
  type: TYPE_TB
- en: '*Hint*: Remember that the product of a ket (column vector) and a bra (row vector)
    is a matrix. We saw an example of this back in *Section* *[*7.2.1*](ch015.xhtml#x1-1240007.2.1).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*As we will see later on in the chapter, frameworks such as PennyLane allow
    you to work with measurement operations defined by any Hermitian operator. This
    can give you a lot of flexibility when defining the measurement operation of a
    neural network. For instance, in an ![n](img/file244.png "n")-qubit circuit, you
    will be able to instruct PennyLane to compute the expectation value of the observable
    ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M"),
    which has as its coordinate representation in the computational basis the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
  prefs: []
  type: TYPE_TB
- en: Alternatively, you may want to consider the observable ![Z \otimes \cdots \otimes
    Z](img/file1376.png "Z \otimes \cdots \otimes Z"). It is easy to see how this
    observable will return ![+ 1](img/file1377.png "+ 1") if an even number of qubits
    are measured as ![0](img/file12.png "0"), and ![- 1](img/file312.png "- 1") otherwise.
    That‚Äôs the reason why ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes
    \cdots \otimes Z") is referred to as the **parity** observable.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you will also be able to take the measurement operation to be a good
    old expectation value on the first qubit. But, the point is, there‚Äôs also a plethora
    of options available to you, should you want to explore them!
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, observables are the final building blocks of every quantum
    neural network architecture. Quantum neural networks accept an input, which usually
    consists of classical data being fed through a feature map. The resulting quantum
    state is then transformed by a variational form and, lastly, some (classical)
    numerical data is obtained through a measurement operation. In this way, we have
    a ‚Äùblack box‚Äù transforming some numerical inputs into outputs, that is, a model
    that ‚Äî just like any other classical ML model ‚Äî can be trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now defined what quantum neural networks are and learned how to construct
    them, at least in theory. That means we have a model. But this is quantum machine
    learning, so a model is not enough: we need to train it. And in order to do so,
    we will need, among other things, an optimization algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 Gradient computation and the parameter shift rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it is not the only option, the optimization algorithms that we shall
    use for quantum neural networks will be gradient descent algorithms; in particular,
    we will use the Adam optimizer. But, as we saw in *Chapter* [*8*](ch017.xhtml#x1-1390008)*,*
    *What is Quantum Machine Learning?*, this algorithm needs to obtain the gradient
    of the expected value of a loss function in terms of the optimizable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our model uses a quantum circuit, the computation of these gradients
    is not entirely trivial. We shall now go briefly over the three main kinds of
    differentiation methods in which these gradient computations may be carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical approximation**: Of course, we have a method that always works.
    It may not always be the most efficient one, but it‚Äôs always there. In order to
    compute gradients, we may just estimate them numerically. In order to do this,
    of course, we will have to run our quantum neural network plenty of times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to exemplify this a little bit, if we had a real-valued function taking
    ![n](img/file244.png "n") real inputs ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png
    "\left. f:R^{n}\rightarrow R \right."), we could approximate its partial derivatives
    as
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: for a sufficiently small value of ![h](img/file519.png "h"). That‚Äôs, of course,
    the most naive way to numerically approximate a derivative, but hopefully it‚Äôs
    enough to give you an intuition of how this works.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Automatic differentiation**: Given the current state of real quantum hardware,
    odds are that most of the quantum neural networks that you will train will run
    on simulators. As non-ideal as this may be, it comes with some advantages. Most
    notably, on simulated quantum neural networks, a classical computer may compute
    exact gradients using techniques similar to those employed on classical neural
    networks. If you are interested, the book Aur√©lien G√©ron [[104](ch030.xhtml#Xhandsonml),
    Chapter 10] and the one by Shai Shalev-Shwartz and Shai Ben-David [[105](ch030.xhtml#Xunderml),
    ¬ß20.6] discuss these techniques for classical neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The parameter shift rule**: The standard automatic differentiation techniques
    can only be used on simulators. Fortunately, there is still another way to compute
    gradients when executing quantum neural networks on real hardware: using the **parameter
    shift rule**. As the name suggests, this technique enables us to compute gradients
    by using the same circuit in the quantum neural network, yet shifting the values
    of the optimizable parameters. The parameter shift rule can‚Äôt always be applied,
    but it works on many common cases and can be used in conjunction with other techniques,
    such as numerical approximation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won‚Äôt get into the details of how this method works, but you may have a look
    at a research paper by Maria Schuld and others [[109](ch030.xhtml#Xpshift-schuld)]
    for more information. For example, if you had a circuit consisting of a single
    rotation gate ![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)") and the measurement
    of its expectation value ![E(\theta)](img/file1381.png "E(\theta)"), you would
    be able to compute its derivative with respect to ![\theta](img/file89.png "\theta")
    as
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'This is similar to what happens with some trigonometric functions: for instance,
    you can express the derivative of the sine function in terms of shifted values
    of the same sine function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For our purposes, it will suffice to know that it exists and can be used. Of
    course, the parameter shift rule can also be used on simulators!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When quantum neural networks are run on simulators, gradients can be computed
    using automatic differentiation techniques analogous to those of classical machine
    learning. When they are run on either real hardware or simulators, these gradients
    can also be computed ‚Äî at least on many cases ‚Äî using the parameter shift rule.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, numerical approximation is always an effective way to compute
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned, all of these methods are already fully implemented in
    PennyLane, and we will try them all out in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything looks good and promising, but quantum neural networks also pose
    some challenges when it comes to training them. Most notably, they are known to
    be vulnerable to **barren plateaus**: situations in which the training gradients
    vanish and, thus, the training can no longer progress (see the paper by McClean
    et. al for further explanation [[67](ch030.xhtml#Xmcclean2018barren)]). It is
    also known that the kind of measurement operation used and the depth of the QNN
    play a role in how likely these barren plateaus are to be found. This is studied,
    for instance, in a paper by Cerezo and collaborators [[24](ch030.xhtml#Xcerezo2021cost)].
    In any case, you should be vigilant when training your QNNs, and follow the literature
    for possible solutions should barren plateaus threaten the learning of your models.'
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the ingredients necessary to construct and train quantum neural
    networks. But before we get to do that in practice, we will discuss a few techniques
    and tips that will help you get the most of our brand new quantum machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.5 Practical usage of quantum neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are a collection of ideas that you should keep in mind when designing
    QNN models and training them. You can think of it as a summary of the previous
    sections, with a few highlights from *Chapter* [*8*](ch017.xhtml#x1-1390008)*,
    What is Quantum* *Machine Learning?*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make wise choices**: When you set out to design a QNN, you have three important
    decisions to make: you have to pick a feature map, a variational form, and a measurement
    operation. Be intentional about these choices and consider the problem and the
    data that you are working with. Your decisions can influence how likely you are
    to find barren plateaus, for instance. A good recommendation is to check the literature
    for similar problems and to build up from there.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size matters**: When you use a well-designed variational form, in general,
    the power of the resulting quantum neural network will be directly related to
    the number of optimizable parameters it has. Use too many parameters, and you
    may have a model that overfits. Use very few, and your model may end up underfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize optimization**: For most problems, the Adam optimizer can be your
    go-to choice for training a quantum neural network. Remember that, as we discussed
    in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum* *Machine Learning?*,
    you will have to pick a learning rate and a batch size when using Adam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smaller learning rate will make the algorithm more accurate, but also slower.
    Analogously, a higher batch size should make the optimization more effective,
    to the detriment of execution time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Feed your QNN properly**: The data that is fed to a quantum neural network
    should be normalized according to the requirements of the feature map in use.
    In addition, depending on the dimensions of the input data, you may want to rely
    on dimensionality reduction techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the more data you have, the better. Nonetheless, one additional fact
    that you may want to take into account is that, under some conditions, quantum
    neural networks have been shown to need fewer data samples than classical neural
    networks in order to be successfully trained [[112](ch030.xhtml#Xqnn-lowdata)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: If you want to further boost the power of your quantum neural networks, you
    may want to consider the **data reuploading** technique [[110](ch030.xhtml#Xperez2020data)].
    In a vanilla QNN, you have a feature map ![F](img/file1320.png "F") dependent
    on some input data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}"),
    which is then followed by a variational form ![V](img/file379.png "V") dependent
    on some optimizable parameters ![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}"). Data reuploading simply consists in repeating
    this scheme ‚Äî any number of times you want ‚Äî before performing the measurement
    operation of the QNN. The feature maps use the same input data in each repetition,
    but each instance of the variational form takes its own, independent, optimizable
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is represented in the following diagram, which shows data reuploading
    with ![k](img/file317.png "k") repetitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![|FVFV0‚ü©((((n‚Éóx‚ÉóùúÉ‚Éóx‚ÉóùúÉ)1)k)) ... ](img/file1384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This has been shown, both in practice and in theory [[113](ch030.xhtml#Xdatare-schuld)],
    to offer some advantages over the simpler, standard approach at the cost of increasing
    the depth of the circuits that are used. In any case, it is good to have it in
    mind when implementing your own QNNs.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our theoretical discussion of quantum neural networks. Now it‚Äôs
    time for us to get our hands dirty with the actual implementation of all the fancy
    artifacts and techniques that we have discussed. In this regard, we will focus
    mostly on PennyLane. Let‚Äôs begin!
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Quantum neural networks in PennyLane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to implement and train our first quantum neural network with
    PennyLane. The PennyLane framework is great for many applications, but it shines
    the most when it comes to the implementation of quantum neural network models.
    This is all due to its flexibility and good integration with classical machine
    learning frameworks. We, in particular, are going to be using PennyLane in conjunction
    with TensorFlow to train a QNN-based binary classifier. All that effort that we
    invested in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine
    Learning?*, is finally going to pay off!
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we are using **version 2.9.1** of the TensorFlow package and **version
    0.26** of PennyLane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs begin by importing PennyLane, NumPy, and TensorFlow and setting some
    seeds for these packages, just to make sure that our results are reproducible.
    We can achieve this with the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that you may still get slightly different results from ours if
    you are using different package versions. However, the results you obtain will
    be fully reproducible in your own machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get to our problem, there‚Äôs one last detail that we need to sort
    out. PennyLane works with doubles while TensorFlow uses ordinary floats. This
    isn‚Äôt always an issue, but it‚Äôs a good idea to ask TensorFlow to work with doubles
    just as PennyLane does. We can accomplish this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With this out of the way, let‚Äôs meet our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Preparing data for a QNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, we are going to train a QNN model to implement
    a binary classifier. Our recurrent use of binary classifiers is no coincidence,
    for binary classifiers are perhaps the simplest machine learning models to train.
    Later in the book, however, we will explore more exciting use cases and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example problem, we are going to use one of the toy datasets provided
    by the scikit-learn package: the ‚ÄùBreast cancer Wisconsin dataset‚Äù [[32](ch030.xhtml#XDua:2019)].
    This dataset has a total of ![569](img/file1385.png "569") samples with ![30](img/file620.png
    "30") numerical variables each. These variables describe features that can be
    used to characterize whether a breast mass is benign or malignant. The label of
    each sample can be either ![0](img/file12.png "0") or ![1](img/file13.png "1"),
    corresponding to malignant or benign, respectively. You may find the documentation
    of this dataset online at [https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)
    (the original documentation of the dataset can also be found at [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get this dataset by calling the `load_breast_cancer` function from `sklearn``.``datasets`,
    setting the optional argument `return_X_y` to true in order to retrieve the labels
    in addition to the samples. For that, we can use the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When we trained QSVMs, since we were not going to make any comparisons between
    models, a training and test dataset sufficed. In our case, however, we are going
    to train our models with early stopping on the validation loss. This means ‚Äî in
    case you don‚Äôt remember ‚Äî that we will be keeping track of the validation loss
    and we will halt the training as soon as it doesn‚Äôt improve ‚Äî according to some
    criteria that we will define. What is more, we will keep the model configuration
    that best minimized the validation loss. Using the test dataset for this purpose
    wouldn‚Äôt be good practice, for then the test dataset would have played a role
    in the training and it would not give a good estimate of the true error; that‚Äôs
    why we will need a separate validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split our dataset into a training, validation, and test dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'All the variables in the dataset are non-zero, but they are not normalized.
    In order to use them with any of our feature maps, we shall normalize the training
    data between ![0](img/file12.png "0") and ![1](img/file13.png "1") using `MaxAbsScaler`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And we then normalize the test and validation datasets in the same proportions
    as the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Just as we did when we trained a QSVM in the previous chapter!
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have simply done some fairly standard data preprocessing, without
    having to think too much about the actual architecture of our future quantum neural
    network. But that changes now. We have a problem to address: our dataset has ![30](img/file620.png
    "30") variables, and that can be a pretty large number for current quantum hardware.
    Since we don‚Äôt have access to quantum computers with ![30](img/file620.png "30")
    qubits, we may consider the following choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the amplitude encoding feature map on ![5](img/file296.png "5") qubits,
    which can accommodate up to ![2^{5} = 32](img/file1386.png "2^{5} = 32") variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use any of the other feature maps that we have used, but in conjunction with
    a dimensionality reduction technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will go for the latter choice. You can try the other possibility on your
    own: it‚Äôs fairly straightforward if you use the `qml``.``AmplitudeEmbedding` template
    that we studied back in *Chapter* [*9*](ch018.xhtml#x1-1600009)*, Quantum Support
    Vector* *Machines*.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.2
  prefs: []
  type: TYPE_NORMAL
- en: As you follow along this section, try to implement a QNN using all the original
    variables with amplitude encoding on five qubits.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, when feeding the data to the `qml``.` `AmplitudeEmbedding`
    object through the features argument, instead of using the `inputs` variable,
    you should use `[``a` `for` `a` `in` `inputs``]`. This is needed because of some
    internal type conversions that PennyLane needs to perform.
  prefs: []
  type: TYPE_NORMAL
- en: Training a quantum neural network on a simulator is a very computationally-intensive
    task. We don‚Äôt want anyone‚Äôs computer to crash, so, just to make sure everyone
    can run this example smoothly, we will restrict ourselves to using ![4](img/file143.png
    "4")-qubit circuits. Thus, we will use a dimensionality reduction technique to
    shrink the number of variables to ![4](img/file143.png "4"), and then set up a
    QNN with a feature map that will take the resulting ![4](img/file143.png "4")
    input variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the previous chapter, we will use principal component analysis
    in order to reduce the number of variables in our dataset to ![4](img/file143.png
    "4"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our data fully ready, we need to choose how our quantum neural
    network is going to work. This is exactly the focus of the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Building the network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our case, we will choose the ZZ feature map and the two-local variational
    form. Neither is built into PennyLane, so we have to provide our own implementation
    of these variational circuits. PennyLane includes, however, a version of the two-local
    form with circular entanglement (`qml``.``BasicEntanglerLayers`), in case you
    want to use it in your QNNs. To implement the circuits that we need, we can just
    use the pseudocode that we provided in *Section* *[*10.1.2*](#x1-18400010.1.2)
    and do something like the following:*
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE7]'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we already implemented the ZZ feature map in PennyLane in the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have talked about observables, and how these are represented
    by Hermitian operators in quantum mechanics. PennyLane allows us to work directly
    with these Hermitian representations.
  prefs: []
  type: TYPE_NORMAL
- en: Remember how every circuit in PennyLane returns the result of some measurement
    operation? For instance, you may use `return` `qml``.``probs``(``wires` `=` `[0])`
    at the end of the definition of a circuit in order to get the probabilities of
    every possible measurement outcome on the computational basis. Well, it turns
    out that PennyLane offers a few more possibilities. For instance, given any Hermitian
    matrix ![A](img/file183.png "A") (encoded as a numpy array `A`), we may retrieve
    the expectation value of ![A](img/file183.png "A") on an array of wires `w` at
    the end of a circuit simply by calling `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)`. Of course, the dimensions of ![A](img/file183.png "A") must be compatible
    with the length of `w`. This is useful in our case, for in order to get the expectation
    value on the first qubit, we will just have to compute the expectation value of
    the Hermitian
  prefs: []
  type: TYPE_NORMAL
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: 'The matrix ![M](img/file704.png "M") can be constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this construction, we have used the fact that ![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|"),
    as we discussed in an exercise earlier in this chapter. This will give us, as
    output, a value between ![0](img/file12.png "0") and ![1](img/file13.png "1"),
    which is perfect to construct a classifier: as usual, we will assign class ![1](img/file13.png
    "1") to every data instance with a value of ![0.5](img/file1166.png "0.5") or
    higher, and class ![0](img/file12.png "0") to all the rest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all the pieces gathered in order to implement our quantum neural
    network. We are going to construct it as a quantum node with two arguments: `inputs`
    and `theta`. The first argument is mandatory: in order for PennyLane to be able
    to train a quantum neural network with TensorFlow, its first argument must accept
    an array with all the inputs to the network, and the name of this argument must
    be `inputs`. After this argument, we may add as many as we want. These can correspond
    to any parameters of the circuit, and, of course, they need to include the optimizable
    parameters in the variational form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we may implement our quantum neural network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To keep things simple, we have chosen to use just one repetition of the variational
    form. If your dataset is more complex, you may want to increase this number in
    order to have more trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, by the way, how we have added the argument `interface` `=` `"``tf``"`
    to the quantum node initializer. This is so that the quantum node will work with
    tensors (TensorFlow‚Äôs data object) in lieu of with arrays, just to allow PennyLane
    to communicate smoothly with TensorFlow. Had we used the `@qml``.``qnode` decorator,
    we would‚Äôve had to include the argument in its call.
  prefs: []
  type: TYPE_NORMAL
- en: This defines the quantum node that implements our quantum neural network. Now
    we need to figure out a way to train it, and, for that purpose, we will rely on
    TensorFlow. We‚Äôll do exactly that in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Using TensorFlow with PennyLane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine Learning?*,
    we already learned how TensorFlow can be used to train a classical neural network.
    Well, thanks to PennyLane‚Äôs great interoperability, we will now be able to train
    our quantum neural network with TensorFlow almost as if it were a classical one.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane can also be integrated with other classical machine learning frameworks
    such as PyTorch. In addition, it provides its own tools to train models based
    on the NumPy package, but these are more limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how we could construct TensorFlow models using Keras layers and joining
    them in sequential models? Look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: That is how you can create a Keras layer containing our quantum neural network
    ‚Äî just as if it were any other layer in a classical model! In order to do this,
    we‚Äôve had to call `qml``.``qnn``.``KerasLayer`, and we‚Äôve had to pass a few things
    to it. First, of course, we‚Äôve sent the quantum node with the neural network.
    Then, a dictionary is indexed by the names of all the node arguments that take
    the optimizable parameters, and specifies, for each of these arguments, the number
    of parameters that they take. Since we only have one such argument, `theta`, and
    it should contain ![8](img/file506.png "8") optimizable parameters (that is, it
    will be an array of length ![8](img/file506.png "8")), we have sent in `{``"``theta``:`
  prefs: []
  type: TYPE_NORMAL
- en: '`8}`. Lastly, we‚Äôve had to specify the dimension of the output of the quantum
    node; since it only returns a numerical expectation value, this dimension is ![1](img/file13.png
    "1").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a quantum layer, we can create a Keras model easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The ability to integrate quantum nodes into neural networks with such a level
    of flexibility will enable us to easily construct more complex model architectures
    in the following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having our model ready, we now have to pick an optimizer and a loss function,
    and then we can compile the model just like any classical model. In our case,
    we will use the binary cross entropy loss (because we are training a binary classifier,
    after all) and we will rely on the Adam optimizer with a learning rate of ![0.005](img/file1389.png
    "0.005"). For the remaining parameters of the optimizer, we will trust the default
    values. Our code is, then, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to this, we will use early stopping on the validation loss with
    a patience of two epochs by using the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And we are now ready to send the final instruction to get our model trained.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, at some point in this chapter, we discussed the different
    ways in which gradients involving quantum neural networks could be computed. And
    you might wonder why we haven‚Äôt had to deal with that in order to get our model
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that PennyLane already picks the best differentiation method for
    us in order to compute gradients. Each quantum node can use certain differentiation
    methods ‚Äî for instance, nodes with devices that act as interfaces to real hardware
    can‚Äôt use automatic differentiation methods, but nodes with simulators can, and
    most do.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this section, we will discuss in detail all the differentiation methods
    that can be used in PennyLane.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train our model, we just have to call the `fit` method. Since we will be
    using early stopping, we will be generous with the number of epochs and set it
    to ![50](img/file1390.png "50"). Also, we will fix a batch size of ![20](img/file588.png
    "20"). For that, we can use the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that you will get upon running this instruction will be similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: If you followed all that we‚Äôve done so far without having asked TensorFlow to
    work with doubles, everything would work just fine ‚Äî although you would get slightly
    different results. Nonetheless, if you try to fit a model using the Lightning
    simulator, you do need to ask TensorFlow to use doubles.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have manually shrunk the progress bar so that the output could
    fit within the width of the page. Also, keep in mind that the execution time may
    vary from device to device, but, in total, the training shouldn‚Äôt take more than
    ![20](img/file588.png "20") minutes on an average
  prefs: []
  type: TYPE_NORMAL
- en: computer.
  prefs: []
  type: TYPE_NORMAL
- en: Just by looking at the raw output, we can already see that the model is indeed
    learning, because there is a very significant drop in both the training and validation
    losses as the training progresses. It could be argued that there might be a tiny
    amount of overfitting, because the drop in the training loss is slightly greater
    than that in the validation loss. In any case, let‚Äôs wait until we have a look
    at the accuracies before coming to any final conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the training has only run for ![16](img/file619.png "16") epochs,
    so it‚Äôs easy to get insights from the output returned by TensorFlow. Nonetheless,
    in the real world, training processes can go on for up to very large numbers of
    epochs, and, needless to say, in those situations the console output isn‚Äôt particularly
    informative. In general, it‚Äôs always a good practice to plot both the training
    and validation losses against the number of epochs, just to get a better insight
    into the performance of the training process. We can do this with the following
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve decided to define a function just so that we can reuse it in future training
    processes. The resulting plot is shown in *Figure* [*10.6*](#Figure10.6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: Training and validation loss functions for every epoch](img/file1391.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10.6**: Training and validation loss functions for every epoch'
  prefs: []
  type: TYPE_NORMAL
- en: 'And now it‚Äôs time for our final test. Let‚Äôs check the accuracy of our model
    on all our datasets to see if its performance is acceptable. This can be done
    with the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Upon running this, we get a training accuracy of ![71\%](img/file1392.png "71\%"),
    a validation accuracy of ![72\%](img/file1393.png "72\%"), and a test accuracy
    of ![72\%](img/file1393.png "72\%"). These results don‚Äôt reflect any kind of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing your own variational forms, you may prefer to use one
    of PennyLane‚Äôs built-in circuits. For instance, you could use the `StronglyEntanglingLayers`
    class. You should keep in mind, however, that the resulting variational form ‚Äî
    as opposed to our own implementation of two-local ‚Äî won‚Äôt take a one-dimensional
    array of inputs, but a three dimensional one! In particular, this form on ![n](img/file244.png
    "n") qubits with ![l](img/file514.png "l") layers will take as input a three-dimensional
    array of size ![n \times l \times 3](img/file1394.png "n \times l \times 3").
    Remember how, in this variational form, we need ![3](img/file472.png "3") arguments
    for the rotation gates, and there are ![n](img/file244.png "n") such gates in
    each of the ![l](img/file514.png "l") layers (you can take another look at *Figure*
    * [*10.4*](#Figure10.4)).*
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are ever in doubt, you may call the `StronglyEntanglingLayers``.``shape`
    function specifying the number of layers and the number of qubits in the respective
    arguments `n_layers` and `n_wires`. This will return a three-tuple with the shape
    that the variational form expects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we could redefine our previous QNN to use this variational form
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this piece of code, we have stored in `nreps` the number of repetitions that
    we want in each instance of the variational form, in `weights_dim` the dimensions
    of the input that the variational form expects, and in `nweights` the number of
    inputs that each instance of the variational form will take. The rest is pretty
    self-explanatory. Inside the circuit, we‚Äôve had to reshape the `theta` array of
    parameters to make it fit into the shape that the variational form expects; in
    order to do this, we‚Äôve used the `tf``.``reshape` function, which can reshape
    TensorFlow‚Äôs tensors while preserving all their metadata. The `weights_strong`
    dictionary that we defined at the end is the one that we would send to TensorFlow
    when constructing the Keras layer.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôve already learned how you can train a quantum neural network using PennyLane
    and TensorFlow. We shall now discuss a few technical details in depth before bringing
    this section to an end.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Gradient computation in PennyLane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, when you train a model with PennyLane, the framework
    itself figures out the best way in which to compute gradients. Different quantum
    nodes may be compatible with different methods of differentiation based on a variety
    of factors, most notably the kind of device they use.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: For an up-to-date reference of the differentiation methods that the `default``.`
    `qubit` simulator supports, you may check the online documentation at [https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations).
  prefs: []
  type: TYPE_NORMAL
- en: You will see that the compatibility of a quantum node with a differentiation
    method not only depends on the device itself but also on the return type of the
    node and the machine learning interface (in our case, the interface was TensorFlow).
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the differentiation methods that can be used in PennyLane:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation**: Just the good old backpropagation method that is used
    in classical neural networks. Of course, this differentiation method only works
    on simulators that are compatible with automatic differentiation, because that
    is what is needed in order to analytically compute the gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``backprop``"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Adjoint differentiation**: This is a more efficient version of backpropagation
    that relies on some of the nice computational ‚Äùoddities‚Äù of quantum computing,
    such as the fact that all the quantum circuits are implemented by unitary matrices,
    which are trivially invertible. Like backpropagation, this method only works on
    the simulators that are compatible with automatic differentiation, but it is more
    restrictive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``adjoint``"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Finite differences**: Ever took a numerical analysis course at college? Then
    this will sound familiar. This method implements the old-school way of computing
    a numerical approximation of a gradient that we discussed in the previous section.
    It works on almost every quantum node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``finite``-``diff``"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parameter shift rule**: PennyLane fully implements the parameter-shift rule
    that we introduced previously. It works on most quantum nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``parameter``-``shift``"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Device gradient computation**: Some devices provide their own way of computing
    gradients. The name of the corresponding differentiation method is `"``device``"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a couple of things that deserve clarification; the first of them is
    how a simulator could not be compatible with automatic differentiation. Oversimplifying
    a little bit, most simulators work by computing the evolution of the quantum state
    of a circuit and returning an output that is differentiable with respect to the
    parameters. The operations required to do all of this are themselves differentiable,
    and hence it‚Äôs possible to use automatic differentiation on quantum nodes that
    use that simulator. But simulators may work differently. For instance, a simulator
    could return individual shots in a way that ‚Äùbreaks‚Äù the differentiability of
    the computation.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that may have caught your attention is that the finite difference
    method can be used on ‚Äùmost‚Äù quantum nodes, but not on all of them. That‚Äôs because
    some quantum nodes may return outputs that don‚Äôt make it possible for the finite
    differences method to work with them. For instance, if a node returns an array
    of samples, the differentiability is broken. If instead, it returned an expectation
    value ‚Äî even if it were just an empirical approximation obtained from a collection
    of samples ‚Äî then a gradient would exist and the finite differences method could
    be used to compute it.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.3
  prefs: []
  type: TYPE_NORMAL
- en: List all the PennyLane differentiation methods that can be used on quantum hardware
    and all the differentiation methods that can be used on simulators.
  prefs: []
  type: TYPE_NORMAL
- en: The way in which you can ask PennyLane to use a specific differentiation method
    ‚Äî let‚Äôs say one named `"``method``"` ‚Äî is by passing the optional argument `diff_method`
    `=` `"``method``"` to the quantum node decorator or initializer. That is, if you
    use the QNode decorator, you should write
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you decided to assemble a circuit `circuit` and a device
    `device` into a quantum node directly, you should call the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By default, `diff_method` is set to `"``best``"`, which, as we‚Äôve said before,
    lets PennyLane choose on our behalf the best differentiation method.
  prefs: []
  type: TYPE_NORMAL
- en: In our particular case, PennyLane has been using the backpropagation differentiation
    method all this time ‚Äî without us even noticing!
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know which differentiation method PennyLane uses by default
    on a device `dev` and on a certain interface `inter` (in our case, `"``tensorflow``"`),
    you can just call the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our quantum node is compatible with all the differentiation methods except with
    device differentiation, because `default``.``qubit` doesn‚Äôt implement its own
    special way of computing gradients. Thus, just to better understand the differences
    in performance, we can try out all the differentiation methods and see how they
    behave.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, when using the Lightning simulator, we do need to ask
    TensorFlow to use doubles all across the Keras model instead of floats ‚Äî it‚Äôs
    not an option, but a necessity. The same happens when we use differentiation methods
    other than backpropagation with `default``.` `qubit`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs begin with adjoint differentiation. In order to retrain our model with
    this differentiation method, we will rerun all our previous code, but changing
    the quantum node definition to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Reasonably enough, instead of rerunning all your code, you may want to add
    the execution of alternative differentiation methods as part of it ‚Äî particularly
    if you are keeping your code in a notebook. If you want to do so while ensuring
    that the training is done in identical conditions (the same environment and seeds),
    these are the lines that you would have to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Upon running this, you will get the exact same training behavior that we got
    with backpropagation ‚Äî the same evolution of the training and validation losses
    and, of course, the same accuracies. Where there is a noticeable change, however,
    is in training time. In our case, training with backpropagation took a rough average
    of ![21](img/file1395.png "21") seconds per epoch. Using adjoint differentiation,
    in contrast, the training took, on average, ![10](img/file161.png "10") seconds
    per epoch. That‚Äôs a big gain!
  prefs: []
  type: TYPE_NORMAL
- en: Actually, if you wanted to further reduce the training time, you should try
    the Lightning simulator with the adjoint method. Depending on the hardware configuration
    of your computer, it can yield very significant boosts in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs now train our model with the two remaining differentiation methods, which
    are the hardware-compatible ones: the parameter-shift rule and finite differences.
    In order to do that, we will just have to rerun our code changing the value of
    the differentiation method in the quantum node definition. In order to avoid redundancy,
    we won‚Äôt rewrite everything here ‚Äî we trust these small changes to you!'
  prefs: []
  type: TYPE_NORMAL
- en: 'When retraining with these two models, these are the results we obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the parameter shift rule yielded the very same results as the other differentiation
    methods. Regarding training time, each epoch took, on average, ![14](img/file1396.png
    "14") seconds to complete. That‚Äôs better than the ![21](img/file1395.png "21")
    seconds that we got with backpropagation, but not as good as the ![10](img/file161.png
    "10") seconds that the adjoint method gave us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using finite differences differentiation, we got, once again, the same
    results that the other methods yielded. On average, each epoch took ![10](img/file161.png
    "10") seconds to complete, which matches the training time of adjoint differentiation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that this comparison holds for the particular model that we have
    considered. The results may vary as the complexity of the models increases and,
    in particular, hardware-compatible methods may perform more poorly on simulators
    when training complex QNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: And that‚Äôs probably all you need to know about the differentiation methods that
    are available in PennyLane. Let‚Äôs now have a look at what Qiskit has to offer
    in terms of quantum neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '10.3 Quantum neural networks in Qiskit: a commentary'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we had a chance to explore in great depth the implementation
    and training of quantum neural networks in PennyLane. We won‚Äôt do an analogous
    discussion for Qiskit in such a level of detail, but we will at least give you
    a few ideas about how to get started should you ever need to use Qiskit in order
    to work with quantum neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane provides a very homogeneous and flexible experience. No matter if
    you‚Äôre training a simple binary classifier or a complex hybrid architecture like
    the ones we will study in the following chapter, it‚Äôs all done in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Qiskit, by contrast, provides a more ‚Äùstructural‚Äù approach. It gives you a suite
    of classes that can be used to train different kinds of neural networks and that
    allow you to define your networks in different ways. It‚Äôs difficult to judge whether
    this is a better or worse approach; in the end, it‚Äôs just a matter of taste. On
    the one hand, training basic models in Qiskit might be simpler than training them
    in PennyLane because of the ease of use of some of these purpose-built classes.
    On the other hand, having different ways of accomplishing the same thing ‚Äî one
    could argue ‚Äî might generate some unnecessary complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classes provided by Qiskit for the implementation of quantum neural networks
    can be imported from `qiskit_machine_learning``.``neural_networks` (please, refer
    to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing the Tools*, for installation
    instructions). These are some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-layer QNN**: The `TwoLayerQNN` class can be used to implement a quantum
    neural network with a single feature map, a variational form, and an observable.
    It works for any vanilla quantum neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circuit QNN**: The `CircuitQNN` class allows you to implement a quantum neural
    network from a parametrized circuit. The final state of the circuit will be measured
    on the computational basis, and each measurement result can be mapped to an integer
    label through an interpreter function. This can be useful, for instance, if you
    want to build a classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the way, in Qiskit lingo, variational forms are called **ansatzs**. As you
    surely remember, this is also the name that was used in the context of the VQE
    algorithm that we studied in *Chapter* *[*7*](ch015.xhtml#x1-1190007)*, VQE: Variational
    Quantum* *Eigensolver*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If, when designing a neural network in Qiskit, you want to use the ZZ feature
    map or the two-local variational form, there‚Äôs no need for you to re-implement
    them; they are bundled with Qiskit. You can get them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the call to the ZZ feature map class, we have set the number of repetitions
    to ![1](img/file13.png "1") ‚Äî any other number would yield a feature map with
    that number of repetitions of the ZZ feature map scheme. In the call to the two-local
    class, we have also specified ‚Äî in addition to the repetitions ‚Äî the rotation
    gates, the controlled gates, and the entanglement layout that we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of example, we can define a `TwoLayer` quantum neural network
    on three qubits with the ZZ feature map and two-local variational form that we
    have just instantiated. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Since we haven‚Äôt specified an observable, the resulting QNN will return the
    expectation value of the ![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes
    Z \otimes Z") observable measured after feeding the execution of the network‚Äôs
    circuit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simulate analytically the network that we have just created on some
    random inputs and optimizable parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is an array with some (random) classical inputs while the
    second argument is an array with (random) values for the optimizable parameters.
    Notice how we‚Äôve used the `qnum_inputs` and `num_weights` properties of the quantum
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: All the neural network classes that we have presented are subclasses of a `NeuralNetwork`
    class. For example, should you want to train a neural network as a classifier,
    you could rely on Qiskit‚Äôs `NeuralNetworkClassifier` class. This class can be
    initialized with a `NeuralNetwork` object and specifying a loss function and an
    optimizer among other things.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, there is a subclass of `NeuralNetworkClassifier` that can
    be used to readily create a trainable neural network classifier directly, providing
    a feature map, a variational form, an optimizer, a loss, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This subclass is called `VQC` (short for Variational Quantum Classifier) and
    it can also be imported from the Qiskit module `qiskit_machine_learning``.``algorithms``.``classifiers`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to create a neural network classifier object from our previous
    `qnn` object using the default parameters provided by Qiskit, you could run the
    following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: By default, the classifier will use the squared error loss function and rely
    on the SLSQP optimizer [[62](ch030.xhtml#Xkraft1988software)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, if you had some training data `data_train` with labels `labels_train`,
    you could train your newly-created classifier by calling the `fit` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you then wanted to compute the outcomes of the trained classifier on some
    data `data_test`, you could use the `predict` method like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you wanted to compute the accuracy score of the trained model
    on some test dataset (`data_test` and `labels_test`), you could run the following
    instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Nevertheless, you shouldn‚Äôt care too much about the `NeuralNetworkClassifier`
    and `VQC` classes because, as it turns out, there is an alternative ‚Äî and, in
    our opinion, better ‚Äî way to train QNNs in Qiskit. We will discuss it in the following
    chapter, and it will involve an interface with an existing machine learning framework,
    PyTorch. What is more, being able to work with this interface will allow us to
    explore Qiskit‚Äôs ‚ÄùTorch Runtime‚Äù: a Qiskit utility that will enable us to more
    efficiently train QNNs on IBM‚Äôs real quantum hardware. This is the same technique
    that we used in *Chapter* *[*5*](ch013.xhtml#x1-940005)*, QAOA:* *Quantum Approximate
    Optimization Algorithm*, to run QAOA executions on quantum hardware. Exciting,
    isn‚Äôt it? Bear with us until the end of the next chapter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*# Summary'
  prefs: []
  type: TYPE_NORMAL
- en: This has been a long journey, hasn‚Äôt it? In this chapter, we first introduced
    quantum neural networks as quantum analogs of classical neural networks. We have
    seen how the training of a quantum neural network is very similar to that of a
    classical one, and we‚Äôve also explored the differentiation methods that make this
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: With the theory out of the way, we got our keyboards ready to do some work.
    We learned how to implement and train a quantum neural network using PennyLane,
    and we also discussed some technicalities about this framework, such as details
    about the differentiation methods that it provides.
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane comes with some wonderful simulators, but ‚Äî as we already mentioned
    in *Chapter* [*2*](ch009.xhtml#x1-400002)*, The Tools of the Trade in Quantum
    Computing* ‚Äî it‚Äôs also integrated with quantum hardware platforms such as Amazon
    Braket and IBM Quantum. Thus, your ability to train quantum neural networks on
    actual quantum computers is at your fingertips!
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter with a short overview of how to work with quantum neural
    networks in Qiskit.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have a solid understanding of quantum neural networks. Combined
    with your previous knowledge of quantum support vector machines, this gives you
    a fairly solid foundation in quantum machine learning. In the following chapter
    ‚Äî which will be very practically-oriented ‚Äî we will explore more complex model
    architectures based on quantum neural networks.*******
  prefs: []
  type: TYPE_NORMAL
