- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬10ç« 
- en: Quantum Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œ
- en: '*The mind is not a vessel to be filled, but a fire to be kindled.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¿ƒçµä¸æ˜¯è¦è¢«å¡«æ»¡çš„å®¹å™¨ï¼Œè€Œæ˜¯ä¸€å›¢éœ€è¦è¢«ç‚¹ç‡ƒçš„ç«ç„°ã€‚*'
- en: â€” Plutarch
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: â€”â€”æ™®é²å¡”å…‹
- en: 'In the previous chapter, we explored our first family of quantum machine learning
    models: quantum support vector machines. Now it is time for us to take one step
    further and consider yet another family of models, that of **Quantum** **Neural
    Networks** (**QNNs**).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé‡å­æœºå™¨å­¦ä¹ æ¨¡å‹å®¶æ—ï¼šé‡å­æ”¯æŒå‘é‡æœºã€‚ç°åœ¨æ˜¯æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢å¦ä¸€ä¸ªæ¨¡å‹å®¶æ—çš„æ—¶å€™äº†ï¼Œé‚£å°±æ˜¯**é‡å­** **ç¥ç»ç½‘ç»œ**ï¼ˆ**QNNs**ï¼‰ã€‚
- en: 'In this chapter, you will learn how the notion of a quantum neural network
    can arise naturally from the ideas behind classical neural networks. Of course,
    you will also learn how quantum neural networks work and how they can be trained.
    Then, you will explore how quantum neural networks can actually be implemented,
    run, and trained using the two quantum frameworks that we have been working with
    so far: Qiskit and PennyLane.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œä½ å°†å­¦ä¹ é‡å­ç¥ç»ç½‘ç»œçš„æ¦‚å¿µå¦‚ä½•è‡ªç„¶åœ°ä»ç»å…¸ç¥ç»ç½‘ç»œèƒŒåçš„æ€æƒ³ä¸­äº§ç”Ÿã€‚å½“ç„¶ï¼Œä½ è¿˜å°†å­¦ä¹ é‡å­ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ä»¥åŠå®ƒä»¬çš„è®­ç»ƒæ–¹æ³•ã€‚ç„¶åï¼Œä½ å°†æ¢ç´¢å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ä¸€ç›´åœ¨ä½¿ç”¨çš„ä¸¤ä¸ªé‡å­æ¡†æ¶â€”â€”Qiskitå’ŒPennyLaneâ€”â€”æ¥å®ç°ã€è¿è¡Œå’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œã€‚
- en: 'These are the contents of this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å†…å®¹å¦‚ä¸‹ï¼š
- en: Building and training quantum neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºå’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œ
- en: Quantum neural networks in PennyLane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLaneä¸­çš„é‡å­ç¥ç»ç½‘ç»œ
- en: 'Quantum neural networks in Qiskit: a commentary'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskitä¸­çš„é‡å­ç¥ç»ç½‘ç»œï¼šè¯„è®º
- en: Quantum support vector machines and quantum neural networks are probably the
    two most popular families of QML models, so, by the end of this chapter, you will
    already have a solid foundation in quantum machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­æ”¯æŒå‘é‡æœºå’Œé‡å­ç¥ç»ç½‘ç»œå¯èƒ½æ˜¯QMLæ¨¡å‹ä¸­æœ€å—æ¬¢è¿çš„ä¸¤ä¸ªå®¶æ—ï¼Œæ‰€ä»¥ï¼Œåˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†å·²ç»åœ¨é‡å­æœºå™¨å­¦ä¹ æ–¹é¢æ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚
- en: To get started, letâ€™s understand how quantum neural networks work and how they
    can be effectively trained. Letâ€™s get to it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ï¼Œè®©æˆ‘ä»¬äº†è§£é‡å­ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•è¢«æœ‰æ•ˆåœ°è®­ç»ƒã€‚è®©æˆ‘ä»¬ç€æ‰‹å§ï¼
- en: 10.1 Building and training a quantum neural network
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.1 æ„å»ºå’Œè®­ç»ƒä¸€ä¸ªé‡å­ç¥ç»ç½‘ç»œ
- en: Just like quantum support vector machines, quantum neural networks are what
    we called â€CQ modelsâ€ back in *Chapter* [*8*](ch017.xhtml#x1-1390008), *What is
    Quantum Machine* *Learning?*, â€” models with purely classical inputs and outputs
    that use quantum computing at some stage. However, unlike QSVMs, quantum neural
    networks are not a â€particular caseâ€ of any classical model, although their behavior
    is inspired by that of classical neural networks. What is more, as we will soon
    see, quantum neural networks are â€purely quantumâ€ models, in the sense that their
    execution will only require classical computing for the preparation of circuits
    and the statistical analysis of measurements. Nevertheless, just like QSVMs, quantum
    neural networks will depend on classical parameters that will be optimized classically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒé‡å­æ”¯æŒå‘é‡æœºä¸€æ ·ï¼Œé‡å­ç¥ç»ç½‘ç»œæ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)ï¼Œâ€œä»€ä¹ˆæ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Ÿâ€ä¸­æåˆ°çš„â€œCQæ¨¡å‹â€â€”â€”å³çº¯ç²¹ä½¿ç”¨ç»å…¸è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶åœ¨æŸä¸ªé˜¶æ®µä½¿ç”¨é‡å­è®¡ç®—çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¸QSVMsä¸åŒï¼Œé‡å­ç¥ç»ç½‘ç»œä¸æ˜¯ä»»ä½•ç»å…¸æ¨¡å‹çš„â€œç‰¹æ®Šæƒ…å†µâ€ï¼Œå°½ç®¡å®ƒä»¬çš„è¡Œä¸ºå—åˆ°ç»å…¸ç¥ç»ç½‘ç»œè¡Œä¸ºçš„å¯å‘ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°çš„ï¼Œé‡å­ç¥ç»ç½‘ç»œæ˜¯â€œçº¯ç²¹é‡å­â€æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„æ‰§è¡Œå°†ä»…éœ€è¦ç»å…¸è®¡ç®—æ¥å‡†å¤‡ç”µè·¯å’Œè¿›è¡Œæµ‹é‡ç»Ÿè®¡åˆ†æã€‚å°½ç®¡å¦‚æ­¤ï¼Œå°±åƒQSVMsä¸€æ ·ï¼Œé‡å­ç¥ç»ç½‘ç»œå°†ä¾èµ–äºç»å…¸å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†é€šè¿‡ç»å…¸ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ã€‚
- en: To learn moreâ€¦
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤š...
- en: As you surely know by now, (quantum) machine learning is a vast field in which
    terms hardly ever have a unique meaning. The term â€quantum neural networkâ€ can,
    in practice, be used to refer to any QML model that is inspired by the behavior
    of a classical neural network. Therefore, you should bear in mind that people
    may also use this name to refer to models different from the ones that we are
    considering to be quantum neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ­¤åˆ»æ‰€çŸ¥ï¼Œ(é‡å­)æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªæœ¯è¯­å«ä¹‰å‡ ä¹ä¸å”¯ä¸€çš„å¹¿é˜”é¢†åŸŸã€‚åœ¨å®è·µä¸­ï¼Œâ€œé‡å­ç¥ç»ç½‘ç»œâ€è¿™ä¸ªæœ¯è¯­å¯ä»¥ç”¨æ¥æŒ‡ä»£ä»»ä½•å—ç»å…¸ç¥ç»ç½‘ç»œè¡Œä¸ºå¯å‘çš„QMLæ¨¡å‹ã€‚å› æ­¤ï¼Œä½ åº”è¯¥è®°ä½ï¼Œäººä»¬ä¹Ÿå¯èƒ½ä½¿ç”¨è¿™ä¸ªåç§°æ¥æŒ‡ä»£ä¸æˆ‘ä»¬è€ƒè™‘çš„é‡å­ç¥ç»ç½‘ç»œä¸åŒçš„æ¨¡å‹ã€‚
- en: That should be enough of an introduction. Letâ€™s now get into the details. What
    actually are quantum neural networks and how do they relate to classical neural
    networks?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥å·²ç»è¶³å¤Ÿä½œä¸ºä»‹ç»äº†ã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç»†èŠ‚ã€‚é‡å­ç¥ç»ç½‘ç»œç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬æ˜¯å¦‚ä½•ä¸ç»å…¸ç¥ç»ç½‘ç»œç›¸å…³çš„ï¼Ÿ
- en: 10.1.1 A journey from classical neural networks to quantum neural networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.1 ä»ç»å…¸ç¥ç»ç½‘ç»œåˆ°é‡å­ç¥ç»ç½‘ç»œçš„æ—…ç¨‹
- en: 'If we do a small exercise of abstraction, we can think of the action of a classical
    neural network as consisting of the following stages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¿›è¡Œä¸€æ¬¡æŠ½è±¡çš„å°ç»ƒä¹ ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç»å…¸ç¥ç»ç½‘ç»œçš„åŠ¨ä½œçœ‹ä½œç”±ä»¥ä¸‹é˜¶æ®µç»„æˆï¼š
- en: '**Data preparation**: This simply amounts to taking some (classical) input
    data and maybe carrying out some (simple) transformations on it. These may include
    normalizing or scaling the input data.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å‡†å¤‡**ï¼šè¿™ä»…ä»…æ˜¯å°†ä¸€äº›ï¼ˆç»å…¸ï¼‰è¾“å…¥æ•°æ®å’Œå¯èƒ½å¯¹å…¶è¿›è¡Œçš„æŸäº›ï¼ˆç®€å•ï¼‰è½¬æ¢ã€‚è¿™äº›å¯èƒ½åŒ…æ‹¬å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–æˆ–ç¼©æ”¾ã€‚'
- en: '**Data processing**: Feeding the data through a sequence of layers that â€transformâ€
    the data as it flows through them. The behavior of this processing depends on
    some optimizable parameters, which are adjusted in training.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¤„ç†**ï¼šé€šè¿‡ä¸€ç³»åˆ—å±‚å°†æ•°æ®ä¼ é€’è¿‡å»ï¼Œè¿™äº›å±‚â€œè½¬æ¢â€æ•°æ®ï¼Œéšç€æ•°æ®æµè¿‡å®ƒä»¬ã€‚è¿™ç§å¤„ç†çš„è¡Œä¸ºå–å†³äºä¸€äº›å¯ä¼˜åŒ–çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨è®­ç»ƒä¸­ä¼šè¢«è°ƒæ•´ã€‚'
- en: '**Data output**: Returning the output through a final layer.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è¾“å‡º**ï¼šé€šè¿‡æœ€ç»ˆå±‚è¿”å›è¾“å‡ºã€‚'
- en: Letâ€™s see how we can take this scheme and use it to define an analogous quantum
    model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•å¯ä»¥å°†è¿™ä¸ªæ–¹æ¡ˆç”¨äºå®šä¹‰ä¸€ä¸ªç±»ä¼¼çš„é‡å­æ¨¡å‹ã€‚
- en: '**Data preparation**: Quantum neural networks are given classical inputs (in
    the form of an array of numbers), but quantum computers donâ€™t work on classical
    data â€” they work on quantum states! So how can we take these classical inputs
    and embed them into the space of quantum states?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å‡†å¤‡**ï¼šé‡å­ç¥ç»ç½‘ç»œæ¥æ”¶ç»å…¸è¾“å…¥ï¼ˆä»¥æ•°å­—æ•°ç»„çš„å½¢å¼ï¼‰ï¼Œä½†é‡å­è®¡ç®—æœºå¹¶ä¸å¤„ç†ç»å…¸æ•°æ®â€”â€”å®ƒä»¬å¤„ç†é‡å­çŠ¶æ€ï¼é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å°†è¿™äº›ç»å…¸è¾“å…¥åµŒå…¥åˆ°é‡å­çŠ¶æ€ç©ºé—´ä¸­å‘¢ï¼Ÿ'
- en: That is a problem that we have already dealt with in *Section* [*9.2*](ch018.xhtml#x1-1660009.2).
    In order to encode the classical input of a QNN into a quantum state, we just
    have to use any feature map of our choice. As you know, we may also need to normalize
    or scale the data, of course.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å·²ç»åœ¨ *ç¬¬9.2èŠ‚* ä¸­å¤„ç†è¿‡çš„é—®é¢˜ã€‚ä¸ºäº†å°†QNNçš„ç»å…¸è¾“å…¥ç¼–ç æˆé‡å­çŠ¶æ€ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æˆ‘ä»¬é€‰æ‹©çš„ä»»ä½•ç‰¹å¾æ˜ å°„ã€‚æ­£å¦‚ä½ æ‰€çŸ¥ï¼Œæˆ‘ä»¬å½“ç„¶å¯èƒ½è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–æˆ–ç¼©æ”¾ã€‚
- en: 'And that is how we actually â€prepare the dataâ€ for a quantum neural network:
    feeding it into a feature map.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ­£æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸ºé‡å­ç¥ç»ç½‘ç»œâ€œå‡†å¤‡æ•°æ®â€ï¼šå°†å…¶è¾“å…¥åˆ°ç‰¹å¾æ˜ å°„ä¸­ã€‚
- en: '**Data processing**: At this point, we have successfully transformed our classical
    input into a â€quantum input,â€ in the form of a quantum state that encodes our
    classical data according to a certain feature map. Now, we need to figure out
    a way to process this input by drawing some inspiration from the processing in
    a classical neural network.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¤„ç†**ï¼šåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸåœ°å°†æˆ‘ä»¬çš„ç»å…¸è¾“å…¥è½¬æ¢æˆäº†â€œé‡å­è¾“å…¥â€ï¼Œå³ä»¥é‡å­çŠ¶æ€çš„å½¢å¼ç¼–ç æˆ‘ä»¬çš„ç»å…¸æ•°æ®ï¼Œæ ¹æ®æŸä¸ªç‰¹å¾æ˜ å°„ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸€ç§æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªè¾“å…¥ï¼Œå¯ä»¥ä»ç»å…¸ç¥ç»ç½‘ç»œçš„å¤„ç†ä¸­æ±²å–ä¸€äº›çµæ„Ÿã€‚'
- en: Trying to replicate the full, exact behavior of a classical neural network in
    a quantum neural network might prove not to be ideal given the state of current
    quantum hardware. Instead, we can look at the bigger picture.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰é‡å­ç¡¬ä»¶çš„çŠ¶æ€ä¸‹ï¼Œè¯•å›¾åœ¨é‡å­ç¥ç»ç½‘ç»œä¸­å¤åˆ¶ç»å…¸ç¥ç»ç½‘ç»œçš„å®Œæ•´ã€ç²¾ç¡®çš„è¡Œä¸ºå¯èƒ½å¹¶ä¸ç†æƒ³ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä»æ›´å¤§çš„å›¾æ™¯æ¥çœ‹ã€‚
- en: In essence, the processing stage of a classical neural network consists in the
    application of some transformations that depend, exclusively, on some optimizable
    parameters. And that is an idea that we can very easily export to a quantum computer.
    We can simply define the â€processingâ€ stage of a quantum neural network asâ€¦the
    application of a circuit that depends on some optimizable parameters! In addition
    to this, as we will see later in this section, this circuit can be structured
    in layers in a way that somewhat reassembles the spirit of a classical neural
    network. This circuit will be said to be a **variational form** â€” they are just
    like the ones we studied back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE:*
    *Variational Quantum Eigensolver*.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æœ¬è´¨ä¸Šï¼Œç»å…¸ç¥ç»ç½‘ç»œçš„å¤„ç†é˜¶æ®µåŒ…æ‹¬åº”ç”¨ä¸€äº›ä»…ä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•°çš„è½¬æ¢ã€‚è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥éå¸¸å®¹æ˜“åœ°ç§»æ¤åˆ°é‡å­è®¡ç®—æœºä¸Šçš„æƒ³æ³•ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†é‡å­ç¥ç»ç½‘ç»œçš„â€œå¤„ç†â€é˜¶æ®µå®šä¹‰ä¸ºâ€¦ä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•°çš„ç”µè·¯çš„åº”ç”¨ï¼æ­¤å¤–ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨æœ¬èŠ‚åé¢çœ‹åˆ°çš„é‚£æ ·ï¼Œè¿™ä¸ªç”µè·¯å¯ä»¥è¢«åˆ†å±‚ç»“æ„åŒ–ï¼Œä»¥æŸç§æ–¹å¼é‡æ–°ç»„åˆç»å…¸ç¥ç»ç½‘ç»œçš„ç²¾é«“ã€‚è¿™ä¸ªç”µè·¯å°†è¢«ç§°ä¸º**å˜åˆ†å½¢å¼**â€”â€”å®ƒä»¬å°±åƒæˆ‘ä»¬åœ¨
    *ç¬¬7ç« * ä¸­ç ”ç©¶çš„é‚£äº›ä¸€æ ·ï¼Œ*VQEï¼šå˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨*ã€‚
- en: '**Data output**: Once we have a processed state, we need to return a classical
    output. And this shall be the result of some measurement operation; this operation
    can be whichever one suits our problem best!'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è¾“å‡º**ï¼šä¸€æ—¦æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤„ç†è¿‡çš„çŠ¶æ€ï¼Œæˆ‘ä»¬éœ€è¦è¿”å›ä¸€ä¸ªç»å…¸è¾“å‡ºã€‚è¿™å°†æ˜¯æŸäº›æµ‹é‡æ“ä½œçš„ç»“æœï¼›è¿™ä¸ªæ“ä½œå¯ä»¥æ˜¯é€‚åˆæˆ‘ä»¬é—®é¢˜çš„æœ€ä½³é€‰æ‹©ï¼'
- en: For instance, if we wanted to build a binary classifier with a quantum neural
    network, a natural choice for this measurement operation could be, for example,
    taking the expectation value of the first qubit when measured on the computational
    basis. Remember that the expectation value of a qubit simply corresponds to the
    probability of obtaining ![1](img/file13.png "1") upon measuring the qubit on
    the computational basis.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ç”¨é‡å­ç¥ç»ç½‘ç»œæ„å»ºä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼Œè¿™ä¸ªæµ‹é‡æ“ä½œçš„ä¸€ä¸ªè‡ªç„¶é€‰æ‹©å¯èƒ½æ˜¯ï¼Œä¾‹å¦‚ï¼Œåœ¨è®¡ç®—åŸºä¸Šæµ‹é‡ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹æ—¶çš„æœŸæœ›å€¼ã€‚è®°ä½ï¼Œé‡å­æ¯”ç‰¹çš„æœŸæœ›å€¼ç®€å•åœ°å¯¹åº”äºåœ¨è®¡ç®—åŸºä¸Šæµ‹é‡é‡å­æ¯”ç‰¹è·å¾—![1](img/file13.png
    "1")çš„æ¦‚ç‡ã€‚
- en: And those are all the ingredients that make up a quantum neural network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å°±æ˜¯æ„æˆé‡å­ç¥ç»ç½‘ç»œçš„æ‰€æœ‰æˆåˆ†ã€‚
- en: 'As a matter of fact, feature maps and variational forms are both examples of
    **variational circuits**: quantum circuits that are controlled by some classical
    parameters. The only actual difference between a feature map and a variational
    form is their purpose: feature maps depend on the input data and are used to encode
    it, while variational forms depend on optimizable parameters and are used to transform
    a quantum input state.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼éƒ½æ˜¯**å˜åˆ†ç”µè·¯**çš„ä¾‹å­ï¼šå—æŸäº›ç»å…¸å‚æ•°æ§åˆ¶çš„é‡å­ç”µè·¯ã€‚ç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼ä¹‹é—´çš„å”¯ä¸€å®é™…åŒºåˆ«æ˜¯å®ƒä»¬çš„ç›®çš„ï¼šç‰¹å¾å›¾ä¾èµ–äºè¾“å…¥æ•°æ®ï¼Œç”¨äºå¯¹å…¶è¿›è¡Œç¼–ç ï¼Œè€Œå˜åˆ†å½¢å¼ä¾èµ–äºå¯ä¼˜åŒ–å‚æ•°ï¼Œç”¨äºå°†é‡å­è¾“å…¥çŠ¶æ€è¿›è¡Œè½¬æ¢ã€‚
- en: This difference in purpose will materialize in the fact that we will often use
    different circuits for feature maps and variational forms. A good feature map
    need not be a good variational form, and vice versa.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç›®çš„ä¸Šçš„å·®å¼‚å°†ä½“ç°åœ¨æˆ‘ä»¬ç»å¸¸ä¼šä¸ºç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼ä½¿ç”¨ä¸åŒçš„ç”µè·¯ã€‚ä¸€ä¸ªå¥½çš„ç‰¹å¾å›¾ä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥½çš„å˜åˆ†å½¢å¼ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: 'You should keep in mind that â€” like all things QML â€” the terms â€feature mapâ€
    and â€variational formâ€ are not entirely universal, and different authors may refer
    to them with different expressions. For example, variational forms are commonly
    referred to as **ansatzs**, as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational* *Quantum Eigensolver*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½ åº”è¯¥è®°ä½â€”â€”å°±åƒæ‰€æœ‰é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰çš„ä¸œè¥¿ä¸€æ ·â€”â€”â€œç‰¹å¾å›¾â€å’Œâ€œå˜åˆ†å½¢å¼â€è¿™ä¸¤ä¸ªæœ¯è¯­å¹¶ä¸å®Œå…¨é€šç”¨ï¼Œä¸åŒçš„ä½œè€…å¯èƒ½ä¼šç”¨ä¸åŒçš„è¡¨è¾¾æ¥æŒ‡ä»£å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå˜åˆ†å½¢å¼é€šå¸¸è¢«ç§°ä¸º**ansatzs**ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬7ç« *[*7*](ch015.xhtml#x1-1190007)
    *VQE: å˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨*ä¸­åšçš„é‚£æ ·ã€‚'
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æç¤º
- en: 'A quantum neural network takes a classical input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") and maps it to a quantum state through a feature map
    ![F](img/file1320.png "F"). The resulting state then goes through a variational
    form ![V](img/file379.png "V"): a variational circuit dependent on some optimizable
    parameters ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}").
    The output of the quantum neural network is the result of a measurement operation
    on the final state. All this can be seen, schematically, in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œå°†ç»å…¸è¾“å…¥ ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    é€šè¿‡ç‰¹å¾å›¾ ![F](img/file1320.png "F") æ˜ å°„åˆ°é‡å­çŠ¶æ€ã€‚ç„¶åï¼Œå¾—åˆ°çš„é‡å­çŠ¶æ€é€šè¿‡å˜åˆ†å½¢å¼ ![V](img/file379.png
    "V")ï¼šä¸€ä¸ªä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•° ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}")
    çš„å˜åˆ†ç”µè·¯ã€‚é‡å­ç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯å¯¹æœ€ç»ˆçŠ¶æ€çš„æµ‹é‡æ“ä½œçš„ç»“æœã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥åœ¨ä»¥ä¸‹å›¾ä¸­ schematically çœ‹åˆ°ï¼š
- en: '![ nâƒ— |FV0âŸ©((âƒ—xğœƒ)) ](img/file1322.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![ nâƒ— |FV0âŸ©((âƒ—xğœƒ))] (img/file1322.jpg)'
- en: Thanks to our study of quantum support vector machines, we are already very
    familiar with feature maps, but we have yet to get acquainted with variational
    forms; that is what we will devote the next subsection to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æˆ‘ä»¬å¯¹é‡å­æ”¯æŒå‘é‡æœºçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å·²ç»éå¸¸ç†Ÿæ‚‰ç‰¹å¾å›¾ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰ç†Ÿæ‚‰å˜åˆ†å½¢å¼ï¼›è¿™æ­£æ˜¯æˆ‘ä»¬å°†è‡´åŠ›äºä¸‹ä¸€å°èŠ‚çš„å†…å®¹ã€‚
- en: 10.1.2 Variational forms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.2 å˜åˆ†å½¢å¼
- en: In principle, a variational form could be any variational circuit of your choice,
    but, in general, variational forms for QNNs follow a â€layered structure,â€ trying
    to mimic the spirit of classical neural networks. We can now make this idea precise.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸåˆ™ä¸Šï¼Œå˜åˆ†å½¢å¼å¯ä»¥æ˜¯ä»»ä½•ä½ é€‰æ‹©çš„å˜åˆ†ç”µè·¯ï¼Œä½†é€šå¸¸ï¼ŒQNNçš„å˜åˆ†å½¢å¼éµå¾ªâ€œåˆ†å±‚ç»“æ„â€ï¼Œè¯•å›¾æ¨¡ä»¿ç»å…¸ç¥ç»ç½‘ç»œçš„ç²¾é«“ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ç²¾ç¡®åœ°é˜è¿°è¿™ä¸ªæƒ³æ³•ã€‚
- en: If we wanted to define a variational form with ![k](img/file317.png "k") layers,
    we could consider ![k](img/file317.png "k") vectors of independent parameters
    ![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}").
    In order to define each layer ![j](img/file258.png "j"), we may take a variational
    circuit ![G_{j}](img/file1324.png "G_{j}") dependent on the parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}"). A common approach is to prepare variational
    forms by stacking these variational circuits consecutively and separating them
    by some circuits ![U_{}](img/file1326.png "U_{}")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¦å®šä¹‰ä¸€ä¸ªå…·æœ‰![k](img/file317.png "k")å±‚çš„å˜åˆ†å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘![k](img/file317.png "k")ä¸ªç‹¬ç«‹å‚æ•°çš„å‘é‡![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}")ã€‚ä¸ºäº†å®šä¹‰æ¯ä¸€å±‚![j](img/file258.png
    "j")ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¸€ä¸ªä¾èµ–äºå‚æ•°![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")çš„å˜åˆ†ç”µè·¯![G_{j}](img/file1324.png
    "G_{j}")ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡è¿ç»­å †å è¿™äº›å˜åˆ†ç”µè·¯å¹¶ä½¿ç”¨ä¸€äº›ç”µè·¯![U_{}](img/file1326.png "U_{}")æ¥å‡†å¤‡å˜åˆ†å½¢å¼
- en: entË†t![,independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.](img/file1327.png
    ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: çº ç¼ ï¼Œç‹¬ç«‹äºä»»ä½•å‚æ•°ï¼Œæ—¨åœ¨åœ¨é‡å­æ¯”ç‰¹ä¹‹é—´åˆ›å»ºçº ç¼ ã€‚æ­£å¦‚![å›¾10.1](img/file1327.png ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")æ‰€ç¤ºã€‚
- en: '![Figure 10.1: A variational form with k layers, each defined by a variational
    circuit G_{j} dependent on some parameters {\overset{\rightarrow}{\theta}}_{j}.
    The circuits U_{} entË†tareusedtocreateentanglement,andthestate \left| \psi_{}
    \right. enc\rangle denotes the output of the feature map ](img/file1331.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.1ï¼šä¸€ä¸ªå…·æœ‰kå±‚çš„å˜åˆ†å½¢å¼ï¼Œæ¯ä¸ªå±‚ç”±ä¸€ä¸ªä¾èµ–äºæŸäº›å‚æ•°{\overset{\rightarrow}{\theta}}_{j}çš„å˜åˆ†ç”µè·¯G_{j}å®šä¹‰ã€‚ç”µè·¯U_{}ç”¨äºåˆ›å»ºçº ç¼ ï¼ŒçŠ¶æ€\left|
    \psi_{} \right. enc\rangleè¡¨ç¤ºç‰¹å¾å›¾çš„è¾“å‡º](img/file1331.jpg)'
- en: '**Figure 10.1**: A variational form with ![k](img/file317.png "k") layers,
    each defined by a variational circuit ![G_{j}](img/file1324.png "G_{j}") dependent
    on some parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}").
    The circuits ![U_{}](img/file1326.png "U_{}") entË†t![areusedtocreateentanglement,andthestate](img/file1328.png
    "areusedtocreateentanglement,andthestate") ![\left| \psi_{} \right.](img/file1329.png
    "\left| \psi_{} \right.") enc![\rangle](img/file1330.png "\rangle") denotes the
    output of the feature map'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10.1**ï¼šä¸€ä¸ªå…·æœ‰![k](img/file317.png "k")å±‚çš„å˜åˆ†å½¢å¼ï¼Œæ¯ä¸ªå±‚ç”±ä¸€ä¸ªä¾èµ–äºæŸäº›å‚æ•°![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}")çš„å˜åˆ†ç”µè·¯![G_{j}](img/file1324.png "G_{j}")å®šä¹‰ã€‚ç”µè·¯![U_{}](img/file1326.png
    "U_{}")ç”¨äºåˆ›å»ºçº ç¼ ï¼ŒçŠ¶æ€![\left| \psi_{} \right.](img/file1329.png "\left| \psi_{} \right.")![\rangle](img/file1330.png
    "\rangle")è¡¨ç¤ºç‰¹å¾å›¾çš„è¾“å‡º'
- en: 'We have now outlined one of the most common structures of variational forms,
    but variational forms are best illustrated by examples. There are lots of variational
    forms out there, and there is no way we could collect them all in this book â€”
    in truth, there would be no point either. For this reason, we will restrict ourselves
    to presenting just three variational forms, some of which we will use later in
    the book:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ¦‚è¿°äº†å˜åˆ†å½¢å¼ä¸­æœ€å¸¸è§çš„ç»“æ„ä¹‹ä¸€ï¼Œä½†å˜åˆ†å½¢å¼æœ€å¥½é€šè¿‡ä¾‹å­æ¥è¯´æ˜ã€‚å˜åˆ†å½¢å¼æœ‰å¾ˆå¤šï¼Œæˆ‘ä»¬ä¸å¯èƒ½åœ¨è¿™æœ¬ä¹¦ä¸­æ”¶é›†å®ƒä»¬æ‰€æœ‰â€”â€”å®é™…ä¸Šï¼Œè¿™æ ·åšä¹Ÿæ²¡æœ‰æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é™åˆ¶è‡ªå·±åªä»‹ç»ä¸‰ç§å˜åˆ†å½¢å¼ï¼Œå…¶ä¸­ä¸€äº›æˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„åé¢éƒ¨åˆ†ä½¿ç”¨ï¼š
- en: '**Two-local**: The **two-local variational form** with ![k](img/file317.png
    "k") repetitions on ![n](img/file244.png "n") qubits relies on ![n \times (k +
    1)](img/file1332.png "n \times (k + 1)") optimizable parameters, which we will
    denote as ![\theta_{rj}](img/file1333.png "\theta_{rj}") with ![r = 0,\ldots,k](img/file1334.png
    "r = 0,\ldots,k") and ![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n"). Its
    circuit is constructed as per the following procedure:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŒå±€éƒ¨**ï¼šåœ¨![n](img/file244.png "n")ä¸ªé‡å­æ¯”ç‰¹ä¸Šé‡å¤![k](img/file317.png "k")æ¬¡çš„**åŒå±€éƒ¨å˜åˆ†å½¢å¼**ä¾èµ–äº![n
    \times (k + 1)](img/file1332.png "n \times (k + 1)")ä¸ªå¯ä¼˜åŒ–å‚æ•°ï¼Œæˆ‘ä»¬å°†ç”¨![\theta_{rj}](img/file1333.png
    "\theta_{rj}")è¡¨ç¤ºï¼Œå…¶ä¸­![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k")å’Œ![j =
    1,\ldots n](img/file1335.png "j = 1,\ldots n")ã€‚å…¶ç”µè·¯çš„æ„å»ºæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š'
- en: '**procedure** TwoLocal(![n,k,\theta](img/file1336.png "n,k,\theta"))'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹** TwoLocal([![n,k,\theta](img/file1336.png "n,k,\theta"))](img/file1336.png
    "n,k,\theta")'
- en: '**for all** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **do**'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **æ‰§è¡Œ**'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Add the* ![r](img/file1337.png
    "r")*-th* *layer.*Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *æ·»åŠ * ![r](img/file1337.png
    "r")*-å±‚ã€‚Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **æ‰§è¡Œ**'
- en: Apply a ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") gate on
    qubit ![j](img/file258.png "j").
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­ä½ ![j](img/file258.png "j") ä¸Šåº”ç”¨ ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})")
    é—¨ã€‚
- en: '-'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Create entanglement
    between layers.*Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *åœ¨å±‚ä¹‹é—´åˆ›å»ºçº ç¼ ã€‚*Â Â Â Â 
    * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**if** ![r < k](img/file1340.png "r < k") **then**'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¦‚æœ** ![r < k](img/file1340.png "r < k") **åˆ™**'
- en: '**for all** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **do**'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **æ‰§è¡Œ**'
- en: Apply a CNOT gate with control on qubit ![t](img/file48.png "t") and target
    on qubit ![t + 1](img/file1342.png "t + 1").
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹æ§åˆ¶é‡å­ä½ ![t](img/file48.png "t") å’Œç›®æ ‡é‡å­ä½ ![t + 1](img/file1342.png "t + 1") åº”ç”¨CNOTé—¨ã€‚
- en: -![-](img/file1343.png "-")
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")
- en: -![-](img/file1343.png "-")**** ***In *Figure* [*10.2*](#Figure10.2) we have
    depicted the output of this procedure for ![n = 4](img/file837.png "n = 4") and
    ![k = 3](img/file1344.png "k = 3"). Sound familiar? The two-local variational
    form uses the same circuit as the angle encoding feature map for its layers, and
    then it relies on a cascade of controlled-NOT operations in order to create entanglement.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")**** ***åœ¨*å›¾* [*10.2*](#Figure10.2) ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥è¿‡ç¨‹åœ¨ ![n
    = 4](img/file837.png "n = 4") å’Œ ![k = 3](img/file1344.png "k = 3") æ—¶çš„è¾“å‡ºã€‚å¬èµ·æ¥ç†Ÿæ‚‰å—ï¼Ÿä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼ä½¿ç”¨ä¸è§’åº¦ç¼–ç ç‰¹å¾å›¾ç›¸åŒçš„ç”µè·¯ä½œä¸ºå…¶å±‚ï¼Œç„¶åå®ƒä¾èµ–äºä¸€ç³»åˆ—å—æ§-NOTæ“ä½œæ¥åˆ›å»ºçº ç¼ ã€‚
- en: Notice, by the way, how the two-local variational form with ![k](img/file317.png
    "k") repetitions has ![k + 1](img/file1345.png "k + 1") layers, not ![k](img/file317.png
    "k"). This tiny detail can sometimes be misleading.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæ³¨æ„ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼é‡å¤ ![k](img/file317.png "k") æ¬¡æ—¶å…·æœ‰ ![k + 1](img/file1345.png
    "k + 1") å±‚ï¼Œè€Œä¸æ˜¯ ![k](img/file317.png "k") å±‚ã€‚è¿™ä¸ªå°å°çš„ç»†èŠ‚æœ‰æ—¶å¯èƒ½ä¼šè¯¯å¯¼ã€‚
- en: The two-local variational form is very versatile, and it can be used with any
    measurement operation.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼éå¸¸çµæ´»ï¼Œå®ƒå¯ä»¥ä¸ä»»ä½•æµ‹é‡æ“ä½œä¸€èµ·ä½¿ç”¨ã€‚
- en: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
- en: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
- en: '***   **Tree tensor**: The **tree tensor** variational form with ![k + 1](img/file1345.png
    "k + 1") layers can be applied on ![n = 2^{k}](img/file1347.png "n = 2^{k}") qubits.
    Each layer has half the number of parameters as the previous one, so the variational
    form relies on ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k
    - 1} + \cdots + 1") optimizable parameters of the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **æ ‘å¼ é‡**ï¼šå…·æœ‰ ![k + 1](img/file1345.png "k + 1") å±‚çš„**æ ‘å¼ é‡**å˜åˆ†å½¢å¼å¯ä»¥åº”ç”¨äº ![n
    = 2^{k}](img/file1347.png "n = 2^{k}") é‡å­ä½ã€‚æ¯ä¸€å±‚çš„å‚æ•°æ•°é‡æ˜¯å‰ä¸€å±‚çš„ä¸€åŠï¼Œå› æ­¤å˜åˆ†å½¢å¼ä¾èµ–äº ![2^{k} +
    2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k - 1} + \cdots + 1") ä¸ªå¯ä¼˜åŒ–å‚æ•°çš„å½¢å¼'
- en: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
- en: 'The procedure that defines is somewhat more opaque than that of the two-local
    variational form, and it reads as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å®šä¹‰è¯¥è¿‡ç¨‹çš„æ­¥éª¤æ¯”ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼ç¨å¾®éš¾ä»¥ç†è§£ï¼Œå…¶å†…å®¹å¦‚ä¸‹ï¼š
- en: '**procedure** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
- en: On each qubit ![j](img/file258.png "j"), apply a rotation ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})").
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªé‡å­ä½ ![j](img/file258.png "j") ä¸Šåº”ç”¨æ—‹è½¬ ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})")ã€‚
- en: '**for all** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **do**'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **æ‰§è¡Œ**'
- en: '**for all** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k
    - r} - 1") **do**'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k -
    r} - 1") **æ‰§è¡Œ**'
- en: Apply a CNOT operation with target on qubit ![1 + s2^{r}](img/file1354.png "1
    + s2^{r}") and controlled by qubit ![1 + s2^{r} + 2^{r - 1}](img/file1355.png
    "1 + s2^{r} + 2^{r - 1}").
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹ç›®æ ‡åœ¨é‡å­ä½ ![1 + s2^{r}](img/file1354.png "1 + s2^{r}") ä¸Šã€ç”±é‡å­ä½ ![1 + s2^{r} + 2^{r
    - 1}](img/file1355.png "1 + s2^{r} + 2^{r - 1}") æ§åˆ¶çš„CNOTæ“ä½œè¿›è¡Œåº”ç”¨ã€‚
- en: Apply a rotation ![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")
    on qubit ![1 + s2^{r}](img/file1354.png "1 + s2^{r}").
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An image is worth a thousand words, so, please, refer to *Figure* [*10.3*](#Figure10.3)
    for a depiction of the output of this procedure for ![k = 3](img/file1344.png
    "k = 3").
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The tree tensor variational form fits best in quantum neural networks designed
    to work as binary classifiers. The most natural measurement operation that can
    be used in conjunction with it is the obtention of the expected value of the first
    qubit, as measured in the computational basis.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a curiosity, the name of the tree tensor variational form comes from mathematical
    objects that are used for the simulation of physics systems and also in some machine
    learning models. See the survey paper by RomÃ¡n OrÃºs for model details [[71](ch030.xhtml#Xorus2014practical)].
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3: Tree tensor variational form on 8 = 2^{3} qubits](img/file1358.png)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.3**: Tree tensor variational form on ![8 = 2^{3}](img/file1357.png
    "8 = 2^{3}") qubits'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Strongly entangling layers**: The strongly entangling layers variational
    form acts on ![n](img/file244.png "n") qubits and can have any number ![k](img/file317.png
    "k") of layers. Each layer ![l](img/file514.png "l") is given a **range** ![r_{l}](img/file1359.png
    "r_{l}"). In total, the variational form uses ![3nk](img/file1360.png "3nk") parameters
    of the form'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The form is defined by the following algorithm:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**procedure** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **do**'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")
    on qubit ![j](img/file258.png "j").
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")
    on qubit ![j](img/file258.png "j").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")
    on qubit ![j](img/file258.png "j").
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a CNOT operation controlled by qubit ![j](img/file258.png "j") and with
    target on qubit ![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png
    "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1").
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may find a representation of a sample of this form in *Figure* [*10.4*](#Figure10.4).
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4: Strongly entangling layers form on four qubits and two layers
    with respective ranges 1 and 2](img/file1368.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.4**: Strongly entangling layers form on four qubits and two layers
    with respective ranges ![1](img/file13.png "1") and ![2](img/file302.png "2")**'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**As a final remark, our choice to use mostly ![Y](img/file11.png "Y") rotations
    in the previous examples of variational forms is somewhat arbitrary. We couldâ€™ve
    also used ![X](img/file9.png "X") rotations, for example. The same goes for our
    choice to use controlled-![X](img/file9.png "X") operations in the entanglement
    circuits. We could have used a different controlled operation, for instance. In
    addition to this, in the two-local variational form, there are more options for
    the distribution of gates in the entanglement circuit beyond the one that we have
    considered. Our entanglement circuit is said to have a â€linearâ€ arrangement of
    gates, but other possibilities are shown in *Figure* [*10.5*](#Figure10.5).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºæœ€åçš„è¯„è®ºï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨ä¹‹å‰çš„å˜åˆ†å½¢å¼ç¤ºä¾‹ä¸­ä¸»è¦ä½¿ç”¨![Y](img/file11.png "Y")æ—‹è½¬æ˜¯æœ‰äº›ä»»æ„çš„ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨![X](img/file9.png
    "X")æ—‹è½¬ï¼Œä¾‹å¦‚ã€‚åŒæ ·ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨çº ç¼ ç”µè·¯ä¸­ä½¿ç”¨å—æ§-![X](img/file9.png "X")æ“ä½œä¹Ÿæ˜¯ä»»æ„çš„ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„å—æ§æ“ä½œï¼Œä¾‹å¦‚ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤å±€éƒ¨å˜åˆ†å½¢å¼ä¸­ï¼Œåœ¨çº ç¼ ç”µè·¯ä¸­é—¨çš„åˆ†å¸ƒè¿˜æœ‰æ›´å¤šé€‰æ‹©ï¼Œè€Œä¸ä»…ä»…æ˜¯æˆ‘ä»¬æ‰€è€ƒè™‘çš„é‚£ç§ã€‚æˆ‘ä»¬çš„çº ç¼ ç”µè·¯è¢«è®¤ä¸ºå…·æœ‰â€œçº¿æ€§â€çš„é—¨æ’åˆ—ï¼Œä½†å…¶ä»–å¯èƒ½æ€§åœ¨*å›¾10.5*ä¸­æœ‰æ‰€å±•ç¤ºã€‚'
- en: '![(a) Linear](img/file1369.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆaï¼‰çº¿æ€§](img/file1369.png)'
- en: '**(a)** Linear'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** çº¿æ€§'
- en: '![(b) Circular](img/file1370.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆbï¼‰åœ†å½¢](img/file1370.jpg)'
- en: '**(b)** Circular'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** åœ†å½¢'
- en: '![(c) Full](img/file1371.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆcï¼‰å®Œæ•´](img/file1371.jpg)'
- en: '**(c)** Full'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**(c)** å®Œæ•´'
- en: '**Figure 10.5**: Different entanglement circuits'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10.5**ï¼šä¸åŒçš„çº ç¼ ç”µè·¯'
- en: This is all we need to know, for now, about variational forms. Combined with
    our previous knowledge of feature maps, this ends our analysis of the elements
    of a quantum neural networkâ€¦almost. We still have to dive deeper into that seemingly
    innocent measurement operation at the end of every quantum neural network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬ç›®å‰éœ€è¦äº†è§£çš„æ‰€æœ‰å…³äºå˜åˆ†å½¢å¼çš„å†…å®¹ã€‚ç»“åˆæˆ‘ä»¬ä¹‹å‰å¯¹ç‰¹å¾å›¾çš„äº†è§£ï¼Œè¿™ç»“æŸäº†æˆ‘ä»¬å¯¹é‡å­ç¥ç»ç½‘ç»œå…ƒç´ çš„è§£æâ€¦â€¦å‡ ä¹ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦æ·±å…¥æ¢ç©¶æ¯ä¸ªé‡å­ç¥ç»ç½‘ç»œæœ«å°¾çœ‹ä¼¼æ— è¾œçš„æµ‹é‡æ“ä½œã€‚
- en: 10.1.3 A word about measurements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.3 å…³äºæµ‹é‡çš„è¯´æ˜
- en: 'As we saw back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE: Variational
    Quantum Eigensolver*, any physical observable can be represented by a Hermitian
    operator in such a way that all the possible outcomes of the measurement of the
    observable can be matched to the different eigenvalues of the operator. If you
    havenâ€™t done so already, please, have a look at *Section* [*7.1.1*](ch015.xhtml#x1-1210007.1.1)
    if you are not familiar with this.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬åœ¨*ç¬¬7ç« *[*7*](ch015.xhtml#x1-1190007)ä¸­çœ‹åˆ°çš„ï¼Œâ€œVQEï¼šå˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨â€ï¼Œä»»ä½•ç‰©ç†å¯è§‚æµ‹é‡éƒ½å¯ä»¥é€šè¿‡ä¸€ä¸ªå„ç±³ç®—ç¬¦æ¥è¡¨ç¤ºï¼Œä½¿å¾—æ‰€æœ‰å¯èƒ½çš„æµ‹é‡ç»“æœéƒ½å¯ä»¥ä¸ç®—ç¬¦çš„ä¸åŒæœ¬å¾å€¼ç›¸å¯¹åº”ã€‚å¦‚æœä½ è¿˜ä¸ç†Ÿæ‚‰è¿™ä¸€ç‚¹ï¼Œè¯·æŸ¥çœ‹*ç¬¬7.1.1èŠ‚*[*7.1.1*](ch015.xhtml#x1-1210007.1.1)ã€‚
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åœ¨è®¡ç®—åŸºä¸‹æµ‹é‡å•ä¸ªé‡å­æ¯”ç‰¹æ—¶ï¼Œä¸ç›¸å…³å„ç±³ç®—ç¬¦çš„è®¡ç®—åŸºåæ ‡çŸ©é˜µå¯èƒ½æ˜¯ä»¥ä¸‹ä¹‹ä¸€
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
- en: Both of these operators represent the measurement of a qubit, but they differ
    in the eigenvalues that they associate to the distinct outputs. The first operator
    associates the eigenvalues ![1](img/file13.png "1") and ![0](img/file12.png "0")
    to the qubitâ€™s value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªç®—ç¬¦éƒ½è¡¨ç¤ºå¯¹é‡å­æ¯”ç‰¹çš„æµ‹é‡ï¼Œä½†å®ƒä»¬åœ¨å…³è”ä¸åŒè¾“å‡ºæ—¶çš„æœ¬å¾å€¼ä¸Šæœ‰æ‰€ä¸åŒã€‚ç¬¬ä¸€ä¸ªç®—ç¬¦å°†æœ¬å¾å€¼![1](img/file13.png "1")å’Œ![0](img/file12.png
    "0")åˆ†åˆ«å…³è”åˆ°é‡å­æ¯”ç‰¹çš„å€¼ä¸º![0](img/file12.png "0")å’Œ![1](img/file13.png "1")ï¼Œè€Œç¬¬äºŒä¸ªå¯è§‚æµ‹é‡å°†æœ¬å¾å€¼![1](img/file13.png
    "1")å’Œ![- 1](img/file312.png "- 1")å…³è”åˆ°è¿™äº›ç»“æœã€‚
- en: Exercise 10.1
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 10.1
- en: The purpose of this exercise is for you to get more familiar with Dirac notation.
    Show that the two previous Hermitian operators may be written, respectively, as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ çš„ç›®çš„æ˜¯è®©ä½ æ›´ç†Ÿæ‚‰ç‹„æ‹‰å…‹ç¬¦å·ã€‚è¯æ˜å‰ä¸¤ä¸ªå„ç±³ç®—ç¬¦å¯ä»¥åˆ†åˆ«å†™æˆ
- en: '| ![1\left&#124; 0 \right\rangle\left\langle 0 \right&#124; + 0\left&#124;
    1 \right\rangle\left\langle 1 \right&#124; = \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;,\quad\left&#124; 0 \right\rangle\left\langle 0 \right&#124; - \left&#124;
    1 \right\rangle\left\langle 1 \right&#124;.](img/file1373.png "1\left&#124; 0
    \right\rangle\left\langle 0 \right&#124; + 0\left&#124; 1 \right\rangle\left\langle
    1 \right&#124; = \left&#124; 1 \right\rangle\left\langle 1 \right&#124;,\quad\left&#124;
    0 \right\rangle\left\langle 0 \right&#124; - \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;.") |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![1\left| 0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle
    1 \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.](img/file1373.png "1\left|
    0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle 1
    \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.") |'
- en: '*Hint*: Remember that the product of a ket (column vector) and a bra (row vector)
    is a matrix. We saw an example of this back in *Section* *[*7.2.1*](ch015.xhtml#x1-1240007.2.1).*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*æç¤º*ï¼šè®°ä½ï¼ŒåŸºï¼ˆåˆ—å‘é‡ï¼‰å’Œæï¼ˆè¡Œå‘é‡ï¼‰çš„ä¹˜ç§¯æ˜¯ä¸€ä¸ªçŸ©é˜µã€‚æˆ‘ä»¬ä¹‹å‰åœ¨ *ç¬¬7.2.1èŠ‚* ä¸­çœ‹åˆ°äº†ä¸€ä¸ªä¾‹å­ã€‚'
- en: '*As we will see later on in the chapter, frameworks such as PennyLane allow
    you to work with measurement operations defined by any Hermitian operator. This
    can give you a lot of flexibility when defining the measurement operation of a
    neural network. For instance, in an ![n](img/file244.png "n")-qubit circuit, you
    will be able to instruct PennyLane to compute the expectation value of the observable
    ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M"),
    which has as its coordinate representation in the computational basis the matrix'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­£å¦‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢çœ‹åˆ°çš„é‚£æ ·ï¼Œæ¡†æ¶å¦‚PennyLaneå…è®¸ä½ ä½¿ç”¨ç”±ä»»ä½•å„ç±³ç®—å­å®šä¹‰çš„æµ‹é‡æ“ä½œã€‚è¿™å¯ä»¥åœ¨å®šä¹‰ç¥ç»ç½‘ç»œçš„æµ‹é‡æ“ä½œæ—¶ç»™ä½ å¸¦æ¥å¾ˆå¤šçµæ´»æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ª
    ![n](img/file244.png "n")-é‡å­ä½ç”µè·¯ä¸­ï¼Œä½ å°†èƒ½å¤ŸæŒ‡ç¤ºPennyLaneè®¡ç®—å¯è§‚æµ‹é‡ ![M \otimes \cdots \otimes
    M](img/file1374.png "M \otimes \cdots \otimes M") çš„æœŸæœ›å€¼ï¼Œå…¶åœ¨è®¡ç®—åŸºçš„åæ ‡è¡¨ç¤ºæ˜¯çŸ©é˜µ'
- en: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
- en: Alternatively, you may want to consider the observable ![Z \otimes \cdots \otimes
    Z](img/file1376.png "Z \otimes \cdots \otimes Z"). It is easy to see how this
    observable will return ![+ 1](img/file1377.png "+ 1") if an even number of qubits
    are measured as ![0](img/file12.png "0"), and ![- 1](img/file312.png "- 1") otherwise.
    Thatâ€™s the reason why ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes
    \cdots \otimes Z") is referred to as the **parity** observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œä½ å¯èƒ½æƒ³è€ƒè™‘å¯è§‚æµ‹é‡ ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots
    \otimes Z")ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œå¦‚æœæµ‹é‡åˆ°å¶æ•°ä¸ªé‡å­ä½ä¸º ![0](img/file12.png "0")ï¼Œåˆ™è¯¥å¯è§‚æµ‹é‡å°†è¿”å› ![+ 1](img/file1377.png
    "+ 1")ï¼Œå¦åˆ™è¿”å› ![- 1](img/file312.png "- 1")ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ ![Z \otimes \cdots \otimes Z](img/file1376.png
    "Z \otimes \cdots \otimes Z") è¢«ç§°ä¸º**å¶æ•°æ€§**å¯è§‚æµ‹é‡çš„åŸå› ã€‚
- en: Of course, you will also be able to take the measurement operation to be a good
    old expectation value on the first qubit. But, the point is, thereâ€™s also a plethora
    of options available to you, should you want to explore them!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ è¿˜å¯ä»¥å°†æµ‹é‡æ“ä½œè§†ä¸ºç¬¬ä¸€ä¸ªé‡å­ä½çš„ç»å…¸æœŸæœ›å€¼ã€‚ä½†æ˜¯ï¼Œé‡ç‚¹æ˜¯ï¼Œå¦‚æœä½ æ„¿æ„æ¢ç´¢ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–é€‰é¡¹å¯ä¾›é€‰æ‹©ï¼
- en: As we mentioned before, observables are the final building blocks of every quantum
    neural network architecture. Quantum neural networks accept an input, which usually
    consists of classical data being fed through a feature map. The resulting quantum
    state is then transformed by a variational form and, lastly, some (classical)
    numerical data is obtained through a measurement operation. In this way, we have
    a â€black boxâ€ transforming some numerical inputs into outputs, that is, a model
    that â€” just like any other classical ML model â€” can be trained.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œå¯è§‚æµ‹é‡æ˜¯æ¯ä¸ªé‡å­ç¥ç»ç½‘ç»œæ¶æ„çš„æœ€ç»ˆæ„å»ºå—ã€‚é‡å­ç¥ç»ç½‘ç»œæ¥å—ä¸€ä¸ªè¾“å…¥ï¼Œè¿™é€šå¸¸ç”±é€šè¿‡ç‰¹å¾å›¾è¾“å…¥çš„ç»å…¸æ•°æ®ç»„æˆã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªå˜åˆ†å½¢å¼å°†å¾—åˆ°çš„é‡å­çŠ¶æ€è½¬æ¢ï¼Œæœ€åé€šè¿‡æµ‹é‡æ“ä½œè·å¾—ä¸€äº›ï¼ˆç»å…¸ï¼‰æ•°å€¼æ•°æ®ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªå°†ä¸€äº›æ•°å€¼è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºçš„â€œé»‘ç›’â€ï¼Œå³ä¸€ä¸ªæ¨¡å‹â€”â€”å°±åƒä»»ä½•å…¶ä»–ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·â€”â€”å¯ä»¥è¢«è®­ç»ƒã€‚
- en: 'We have now defined what quantum neural networks are and learned how to construct
    them, at least in theory. That means we have a model. But this is quantum machine
    learning, so a model is not enough: we need to train it. And in order to do so,
    we will need, among other things, an optimization algorithm.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 Gradient computation and the parameter shift rule
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it is not the only option, the optimization algorithms that we shall
    use for quantum neural networks will be gradient descent algorithms; in particular,
    we will use the Adam optimizer. But, as we saw in *Chapter* [*8*](ch017.xhtml#x1-1390008)*,*
    *What is Quantum Machine Learning?*, this algorithm needs to obtain the gradient
    of the expected value of a loss function in terms of the optimizable parameters.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our model uses a quantum circuit, the computation of these gradients
    is not entirely trivial. We shall now go briefly over the three main kinds of
    differentiation methods in which these gradient computations may be carried out:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical approximation**: Of course, we have a method that always works.
    It may not always be the most efficient one, but itâ€™s always there. In order to
    compute gradients, we may just estimate them numerically. In order to do this,
    of course, we will have to run our quantum neural network plenty of times.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to exemplify this a little bit, if we had a real-valued function taking
    ![n](img/file244.png "n") real inputs ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png
    "\left. f:R^{n}\rightarrow R \right."), we could approximate its partial derivatives
    as
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: for a sufficiently small value of ![h](img/file519.png "h"). Thatâ€™s, of course,
    the most naive way to numerically approximate a derivative, but hopefully itâ€™s
    enough to give you an intuition of how this works.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Automatic differentiation**: Given the current state of real quantum hardware,
    odds are that most of the quantum neural networks that you will train will run
    on simulators. As non-ideal as this may be, it comes with some advantages. Most
    notably, on simulated quantum neural networks, a classical computer may compute
    exact gradients using techniques similar to those employed on classical neural
    networks. If you are interested, the book AurÃ©lien GÃ©ron [[104](ch030.xhtml#Xhandsonml),
    Chapter 10] and the one by Shai Shalev-Shwartz and Shai Ben-David [[105](ch030.xhtml#Xunderml),
    Â§20.6] discuss these techniques for classical neural networks.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The parameter shift rule**: The standard automatic differentiation techniques
    can only be used on simulators. Fortunately, there is still another way to compute
    gradients when executing quantum neural networks on real hardware: using the **parameter
    shift rule**. As the name suggests, this technique enables us to compute gradients
    by using the same circuit in the quantum neural network, yet shifting the values
    of the optimizable parameters. The parameter shift rule canâ€™t always be applied,
    but it works on many common cases and can be used in conjunction with other techniques,
    such as numerical approximation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We wonâ€™t get into the details of how this method works, but you may have a look
    at a research paper by Maria Schuld and others [[109](ch030.xhtml#Xpshift-schuld)]
    for more information. For example, if you had a circuit consisting of a single
    rotation gate ![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)") and the measurement
    of its expectation value ![E(\theta)](img/file1381.png "E(\theta)"), you would
    be able to compute its derivative with respect to ![\theta](img/file89.png "\theta")
    as
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'This is similar to what happens with some trigonometric functions: for instance,
    you can express the derivative of the sine function in terms of shifted values
    of the same sine function.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For our purposes, it will suffice to know that it exists and can be used. Of
    course, the parameter shift rule can also be used on simulators!
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: When quantum neural networks are run on simulators, gradients can be computed
    using automatic differentiation techniques analogous to those of classical machine
    learning. When they are run on either real hardware or simulators, these gradients
    can also be computed â€” at least on many cases â€” using the parameter shift rule.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, numerical approximation is always an effective way to compute
    gradients.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned, all of these methods are already fully implemented in
    PennyLane, and we will try them all out in the following section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything looks good and promising, but quantum neural networks also pose
    some challenges when it comes to training them. Most notably, they are known to
    be vulnerable to **barren plateaus**: situations in which the training gradients
    vanish and, thus, the training can no longer progress (see the paper by McClean
    et. al for further explanation [[67](ch030.xhtml#Xmcclean2018barren)]). It is
    also known that the kind of measurement operation used and the depth of the QNN
    play a role in how likely these barren plateaus are to be found. This is studied,
    for instance, in a paper by Cerezo and collaborators [[24](ch030.xhtml#Xcerezo2021cost)].
    In any case, you should be vigilant when training your QNNs, and follow the literature
    for possible solutions should barren plateaus threaten the learning of your models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the ingredients necessary to construct and train quantum neural
    networks. But before we get to do that in practice, we will discuss a few techniques
    and tips that will help you get the most of our brand new quantum machine learning
    models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.5 Practical usage of quantum neural networks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are a collection of ideas that you should keep in mind when designing
    QNN models and training them. You can think of it as a summary of the previous
    sections, with a few highlights from *Chapter* [*8*](ch017.xhtml#x1-1390008)*,
    What is Quantum* *Machine Learning?*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Make wise choices**: When you set out to design a QNN, you have three important
    decisions to make: you have to pick a feature map, a variational form, and a measurement
    operation. Be intentional about these choices and consider the problem and the
    data that you are working with. Your decisions can influence how likely you are
    to find barren plateaus, for instance. A good recommendation is to check the literature
    for similar problems and to build up from there.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size matters**: When you use a well-designed variational form, in general,
    the power of the resulting quantum neural network will be directly related to
    the number of optimizable parameters it has. Use too many parameters, and you
    may have a model that overfits. Use very few, and your model may end up underfitting.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize optimization**: For most problems, the Adam optimizer can be your
    go-to choice for training a quantum neural network. Remember that, as we discussed
    in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum* *Machine Learning?*,
    you will have to pick a learning rate and a batch size when using Adam.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smaller learning rate will make the algorithm more accurate, but also slower.
    Analogously, a higher batch size should make the optimization more effective,
    to the detriment of execution time.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Feed your QNN properly**: The data that is fed to a quantum neural network
    should be normalized according to the requirements of the feature map in use.
    In addition, depending on the dimensions of the input data, you may want to rely
    on dimensionality reduction techniques.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the more data you have, the better. Nonetheless, one additional fact
    that you may want to take into account is that, under some conditions, quantum
    neural networks have been shown to need fewer data samples than classical neural
    networks in order to be successfully trained [[112](ch030.xhtml#Xqnn-lowdata)].
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: If you want to further boost the power of your quantum neural networks, you
    may want to consider the **data reuploading** technique [[110](ch030.xhtml#Xperez2020data)].
    In a vanilla QNN, you have a feature map ![F](img/file1320.png "F") dependent
    on some input data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}"),
    which is then followed by a variational form ![V](img/file379.png "V") dependent
    on some optimizable parameters ![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}"). Data reuploading simply consists in repeating
    this scheme â€” any number of times you want â€” before performing the measurement
    operation of the QNN. The feature maps use the same input data in each repetition,
    but each instance of the variational form takes its own, independent, optimizable
    parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'This is represented in the following diagram, which shows data reuploading
    with ![k](img/file317.png "k") repetitions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![|FVFV0âŸ©((((nâƒ—xâƒ—ğœƒâƒ—xâƒ—ğœƒ)1)k)) ... ](img/file1384.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: This has been shown, both in practice and in theory [[113](ch030.xhtml#Xdatare-schuld)],
    to offer some advantages over the simpler, standard approach at the cost of increasing
    the depth of the circuits that are used. In any case, it is good to have it in
    mind when implementing your own QNNs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our theoretical discussion of quantum neural networks. Now itâ€™s
    time for us to get our hands dirty with the actual implementation of all the fancy
    artifacts and techniques that we have discussed. In this regard, we will focus
    mostly on PennyLane. Letâ€™s begin!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Quantum neural networks in PennyLane
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to implement and train our first quantum neural network with
    PennyLane. The PennyLane framework is great for many applications, but it shines
    the most when it comes to the implementation of quantum neural network models.
    This is all due to its flexibility and good integration with classical machine
    learning frameworks. We, in particular, are going to be using PennyLane in conjunction
    with TensorFlow to train a QNN-based binary classifier. All that effort that we
    invested in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine
    Learning?*, is finally going to pay off!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we are using **version 2.9.1** of the TensorFlow package and **version
    0.26** of PennyLane.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s begin by importing PennyLane, NumPy, and TensorFlow and setting some
    seeds for these packages, just to make sure that our results are reproducible.
    We can achieve this with the following piece of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you may still get slightly different results from ours if
    you are using different package versions. However, the results you obtain will
    be fully reproducible in your own machine.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get to our problem, thereâ€™s one last detail that we need to sort
    out. PennyLane works with doubles while TensorFlow uses ordinary floats. This
    isnâ€™t always an issue, but itâ€™s a good idea to ask TensorFlow to work with doubles
    just as PennyLane does. We can accomplish this as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this out of the way, letâ€™s meet our problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Preparing data for a QNN
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, we are going to train a QNN model to implement
    a binary classifier. Our recurrent use of binary classifiers is no coincidence,
    for binary classifiers are perhaps the simplest machine learning models to train.
    Later in the book, however, we will explore more exciting use cases and architectures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example problem, we are going to use one of the toy datasets provided
    by the scikit-learn package: the â€Breast cancer Wisconsin datasetâ€ [[32](ch030.xhtml#XDua:2019)].
    This dataset has a total of ![569](img/file1385.png "569") samples with ![30](img/file620.png
    "30") numerical variables each. These variables describe features that can be
    used to characterize whether a breast mass is benign or malignant. The label of
    each sample can be either ![0](img/file12.png "0") or ![1](img/file13.png "1"),
    corresponding to malignant or benign, respectively. You may find the documentation
    of this dataset online at [https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)
    (the original documentation of the dataset can also be found at [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get this dataset by calling the `load_breast_cancer` function from `sklearn``.``datasets`,
    setting the optional argument `return_X_y` to true in order to retrieve the labels
    in addition to the samples. For that, we can use the following instructions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we trained QSVMs, since we were not going to make any comparisons between
    models, a training and test dataset sufficed. In our case, however, we are going
    to train our models with early stopping on the validation loss. This means â€” in
    case you donâ€™t remember â€” that we will be keeping track of the validation loss
    and we will halt the training as soon as it doesnâ€™t improve â€” according to some
    criteria that we will define. What is more, we will keep the model configuration
    that best minimized the validation loss. Using the test dataset for this purpose
    wouldnâ€™t be good practice, for then the test dataset would have played a role
    in the training and it would not give a good estimate of the true error; thatâ€™s
    why we will need a separate validation dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split our dataset into a training, validation, and test dataset as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'All the variables in the dataset are non-zero, but they are not normalized.
    In order to use them with any of our feature maps, we shall normalize the training
    data between ![0](img/file12.png "0") and ![1](img/file13.png "1") using `MaxAbsScaler`
    as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we then normalize the test and validation datasets in the same proportions
    as the training dataset:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just as we did when we trained a QSVM in the previous chapter!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have simply done some fairly standard data preprocessing, without
    having to think too much about the actual architecture of our future quantum neural
    network. But that changes now. We have a problem to address: our dataset has ![30](img/file620.png
    "30") variables, and that can be a pretty large number for current quantum hardware.
    Since we donâ€™t have access to quantum computers with ![30](img/file620.png "30")
    qubits, we may consider the following choices:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Use the amplitude encoding feature map on ![5](img/file296.png "5") qubits,
    which can accommodate up to ![2^{5} = 32](img/file1386.png "2^{5} = 32") variables
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use any of the other feature maps that we have used, but in conjunction with
    a dimensionality reduction technique
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will go for the latter choice. You can try the other possibility on your
    own: itâ€™s fairly straightforward if you use the `qml``.``AmplitudeEmbedding` template
    that we studied back in *Chapter* [*9*](ch018.xhtml#x1-1600009)*, Quantum Support
    Vector* *Machines*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As you follow along this section, try to implement a QNN using all the original
    variables with amplitude encoding on five qubits.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, when feeding the data to the `qml``.` `AmplitudeEmbedding`
    object through the features argument, instead of using the `inputs` variable,
    you should use `[``a` `for` `a` `in` `inputs``]`. This is needed because of some
    internal type conversions that PennyLane needs to perform.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Training a quantum neural network on a simulator is a very computationally-intensive
    task. We donâ€™t want anyoneâ€™s computer to crash, so, just to make sure everyone
    can run this example smoothly, we will restrict ourselves to using ![4](img/file143.png
    "4")-qubit circuits. Thus, we will use a dimensionality reduction technique to
    shrink the number of variables to ![4](img/file143.png "4"), and then set up a
    QNN with a feature map that will take the resulting ![4](img/file143.png "4")
    input variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the previous chapter, we will use principal component analysis
    in order to reduce the number of variables in our dataset to ![4](img/file143.png
    "4"):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have our data fully ready, we need to choose how our quantum neural
    network is going to work. This is exactly the focus of the next subsection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Building the network
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our case, we will choose the ZZ feature map and the two-local variational
    form. Neither is built into PennyLane, so we have to provide our own implementation
    of these variational circuits. PennyLane includes, however, a version of the two-local
    form with circular entanglement (`qml``.``BasicEntanglerLayers`), in case you
    want to use it in your QNNs. To implement the circuits that we need, we can just
    use the pseudocode that we provided in *Section* *[*10.1.2*](#x1-18400010.1.2)
    and do something like the following:*
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we already implemented the ZZ feature map in PennyLane in the
    previous chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have talked about observables, and how these are represented
    by Hermitian operators in quantum mechanics. PennyLane allows us to work directly
    with these Hermitian representations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Remember how every circuit in PennyLane returns the result of some measurement
    operation? For instance, you may use `return` `qml``.``probs``(``wires` `=` `[0])`
    at the end of the definition of a circuit in order to get the probabilities of
    every possible measurement outcome on the computational basis. Well, it turns
    out that PennyLane offers a few more possibilities. For instance, given any Hermitian
    matrix ![A](img/file183.png "A") (encoded as a numpy array `A`), we may retrieve
    the expectation value of ![A](img/file183.png "A") on an array of wires `w` at
    the end of a circuit simply by calling `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)`. Of course, the dimensions of ![A](img/file183.png "A") must be compatible
    with the length of `w`. This is useful in our case, for in order to get the expectation
    value on the first qubit, we will just have to compute the expectation value of
    the Hermitian
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: 'The matrix ![M](img/file704.png "M") can be constructed as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this construction, we have used the fact that ![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|"),
    as we discussed in an exercise earlier in this chapter. This will give us, as
    output, a value between ![0](img/file12.png "0") and ![1](img/file13.png "1"),
    which is perfect to construct a classifier: as usual, we will assign class ![1](img/file13.png
    "1") to every data instance with a value of ![0.5](img/file1166.png "0.5") or
    higher, and class ![0](img/file12.png "0") to all the rest.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all the pieces gathered in order to implement our quantum neural
    network. We are going to construct it as a quantum node with two arguments: `inputs`
    and `theta`. The first argument is mandatory: in order for PennyLane to be able
    to train a quantum neural network with TensorFlow, its first argument must accept
    an array with all the inputs to the network, and the name of this argument must
    be `inputs`. After this argument, we may add as many as we want. These can correspond
    to any parameters of the circuit, and, of course, they need to include the optimizable
    parameters in the variational form.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we may implement our quantum neural network as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To keep things simple, we have chosen to use just one repetition of the variational
    form. If your dataset is more complex, you may want to increase this number in
    order to have more trainable parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Notice, by the way, how we have added the argument `interface` `=` `"``tf``"`
    to the quantum node initializer. This is so that the quantum node will work with
    tensors (TensorFlowâ€™s data object) in lieu of with arrays, just to allow PennyLane
    to communicate smoothly with TensorFlow. Had we used the `@qml``.``qnode` decorator,
    we wouldâ€™ve had to include the argument in its call.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: This defines the quantum node that implements our quantum neural network. Now
    we need to figure out a way to train it, and, for that purpose, we will rely on
    TensorFlow. Weâ€™ll do exactly that in the next subsection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Using TensorFlow with PennyLane
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine Learning?*,
    we already learned how TensorFlow can be used to train a classical neural network.
    Well, thanks to PennyLaneâ€™s great interoperability, we will now be able to train
    our quantum neural network with TensorFlow almost as if it were a classical one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane can also be integrated with other classical machine learning frameworks
    such as PyTorch. In addition, it provides its own tools to train models based
    on the NumPy package, but these are more limited.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how we could construct TensorFlow models using Keras layers and joining
    them in sequential models? Look at this:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is how you can create a Keras layer containing our quantum neural network
    â€” just as if it were any other layer in a classical model! In order to do this,
    weâ€™ve had to call `qml``.``qnn``.``KerasLayer`, and weâ€™ve had to pass a few things
    to it. First, of course, weâ€™ve sent the quantum node with the neural network.
    Then, a dictionary is indexed by the names of all the node arguments that take
    the optimizable parameters, and specifies, for each of these arguments, the number
    of parameters that they take. Since we only have one such argument, `theta`, and
    it should contain ![8](img/file506.png "8") optimizable parameters (that is, it
    will be an array of length ![8](img/file506.png "8")), we have sent in `{``"``theta``:`
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`8}`. Lastly, weâ€™ve had to specify the dimension of the output of the quantum
    node; since it only returns a numerical expectation value, this dimension is ![1](img/file13.png
    "1").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a quantum layer, we can create a Keras model easily:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability to integrate quantum nodes into neural networks with such a level
    of flexibility will enable us to easily construct more complex model architectures
    in the following chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Having our model ready, we now have to pick an optimizer and a loss function,
    and then we can compile the model just like any classical model. In our case,
    we will use the binary cross entropy loss (because we are training a binary classifier,
    after all) and we will rely on the Adam optimizer with a learning rate of ![0.005](img/file1389.png
    "0.005"). For the remaining parameters of the optimizer, we will trust the default
    values. Our code is, then, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In addition to this, we will use early stopping on the validation loss with
    a patience of two epochs by using the following instructions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And we are now ready to send the final instruction to get our model trained.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, at some point in this chapter, we discussed the different
    ways in which gradients involving quantum neural networks could be computed. And
    you might wonder why we havenâ€™t had to deal with that in order to get our model
    trained.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that PennyLane already picks the best differentiation method for
    us in order to compute gradients. Each quantum node can use certain differentiation
    methods â€” for instance, nodes with devices that act as interfaces to real hardware
    canâ€™t use automatic differentiation methods, but nodes with simulators can, and
    most do.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Later in this section, we will discuss in detail all the differentiation methods
    that can be used in PennyLane.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'To train our model, we just have to call the `fit` method. Since we will be
    using early stopping, we will be generous with the number of epochs and set it
    to ![50](img/file1390.png "50"). Also, we will fix a batch size of ![20](img/file588.png
    "20"). For that, we can use the following piece of code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output that you will get upon running this instruction will be similar
    to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To learn moreâ€¦
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: If you followed all that weâ€™ve done so far without having asked TensorFlow to
    work with doubles, everything would work just fine â€” although you would get slightly
    different results. Nonetheless, if you try to fit a model using the Lightning
    simulator, you do need to ask TensorFlow to use doubles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have manually shrunk the progress bar so that the output could
    fit within the width of the page. Also, keep in mind that the execution time may
    vary from device to device, but, in total, the training shouldnâ€™t take more than
    ![20](img/file588.png "20") minutes on an average
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: computer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Just by looking at the raw output, we can already see that the model is indeed
    learning, because there is a very significant drop in both the training and validation
    losses as the training progresses. It could be argued that there might be a tiny
    amount of overfitting, because the drop in the training loss is slightly greater
    than that in the validation loss. In any case, letâ€™s wait until we have a look
    at the accuracies before coming to any final conclusions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the training has only run for ![16](img/file619.png "16") epochs,
    so itâ€™s easy to get insights from the output returned by TensorFlow. Nonetheless,
    in the real world, training processes can go on for up to very large numbers of
    epochs, and, needless to say, in those situations the console output isnâ€™t particularly
    informative. In general, itâ€™s always a good practice to plot both the training
    and validation losses against the number of epochs, just to get a better insight
    into the performance of the training process. We can do this with the following
    instructions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Weâ€™ve decided to define a function just so that we can reuse it in future training
    processes. The resulting plot is shown in *Figure* [*10.6*](#Figure10.6).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: Training and validation loss functions for every epoch](img/file1391.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10.6**: Training and validation loss functions for every epoch'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'And now itâ€™s time for our final test. Letâ€™s check the accuracy of our model
    on all our datasets to see if its performance is acceptable. This can be done
    with the following piece of code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Upon running this, we get a training accuracy of ![71\%](img/file1392.png "71\%"),
    a validation accuracy of ![72\%](img/file1393.png "72\%"), and a test accuracy
    of ![72\%](img/file1393.png "72\%"). These results donâ€™t reflect any kind of overfitting.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing your own variational forms, you may prefer to use one
    of PennyLaneâ€™s built-in circuits. For instance, you could use the `StronglyEntanglingLayers`
    class. You should keep in mind, however, that the resulting variational form â€”
    as opposed to our own implementation of two-local â€” wonâ€™t take a one-dimensional
    array of inputs, but a three dimensional one! In particular, this form on ![n](img/file244.png
    "n") qubits with ![l](img/file514.png "l") layers will take as input a three-dimensional
    array of size ![n \times l \times 3](img/file1394.png "n \times l \times 3").
    Remember how, in this variational form, we need ![3](img/file472.png "3") arguments
    for the rotation gates, and there are ![n](img/file244.png "n") such gates in
    each of the ![l](img/file514.png "l") layers (you can take another look at *Figure*
    * [*10.4*](#Figure10.4)).*
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are ever in doubt, you may call the `StronglyEntanglingLayers``.``shape`
    function specifying the number of layers and the number of qubits in the respective
    arguments `n_layers` and `n_wires`. This will return a three-tuple with the shape
    that the variational form expects.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we could redefine our previous QNN to use this variational form
    as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this piece of code, we have stored in `nreps` the number of repetitions that
    we want in each instance of the variational form, in `weights_dim` the dimensions
    of the input that the variational form expects, and in `nweights` the number of
    inputs that each instance of the variational form will take. The rest is pretty
    self-explanatory. Inside the circuit, weâ€™ve had to reshape the `theta` array of
    parameters to make it fit into the shape that the variational form expects; in
    order to do this, weâ€™ve used the `tf``.``reshape` function, which can reshape
    TensorFlowâ€™s tensors while preserving all their metadata. The `weights_strong`
    dictionary that we defined at the end is the one that we would send to TensorFlow
    when constructing the Keras layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ve already learned how you can train a quantum neural network using PennyLane
    and TensorFlow. We shall now discuss a few technical details in depth before bringing
    this section to an end.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Gradient computation in PennyLane
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, when you train a model with PennyLane, the framework
    itself figures out the best way in which to compute gradients. Different quantum
    nodes may be compatible with different methods of differentiation based on a variety
    of factors, most notably the kind of device they use.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: For an up-to-date reference of the differentiation methods that the `default``.`
    `qubit` simulator supports, you may check the online documentation at [https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: You will see that the compatibility of a quantum node with a differentiation
    method not only depends on the device itself but also on the return type of the
    node and the machine learning interface (in our case, the interface was TensorFlow).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the differentiation methods that can be used in PennyLane:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation**: Just the good old backpropagation method that is used
    in classical neural networks. Of course, this differentiation method only works
    on simulators that are compatible with automatic differentiation, because that
    is what is needed in order to analytically compute the gradients.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``backprop``"`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Adjoint differentiation**: This is a more efficient version of backpropagation
    that relies on some of the nice computational â€odditiesâ€ of quantum computing,
    such as the fact that all the quantum circuits are implemented by unitary matrices,
    which are trivially invertible. Like backpropagation, this method only works on
    the simulators that are compatible with automatic differentiation, but it is more
    restrictive.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``adjoint``"`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Finite differences**: Ever took a numerical analysis course at college? Then
    this will sound familiar. This method implements the old-school way of computing
    a numerical approximation of a gradient that we discussed in the previous section.
    It works on almost every quantum node.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``finite``-``diff``"`.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parameter shift rule**: PennyLane fully implements the parameter-shift rule
    that we introduced previously. It works on most quantum nodes.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``parameter``-``shift``"`.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Device gradient computation**: Some devices provide their own way of computing
    gradients. The name of the corresponding differentiation method is `"``device``"`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a couple of things that deserve clarification; the first of them is
    how a simulator could not be compatible with automatic differentiation. Oversimplifying
    a little bit, most simulators work by computing the evolution of the quantum state
    of a circuit and returning an output that is differentiable with respect to the
    parameters. The operations required to do all of this are themselves differentiable,
    and hence itâ€™s possible to use automatic differentiation on quantum nodes that
    use that simulator. But simulators may work differently. For instance, a simulator
    could return individual shots in a way that â€breaksâ€ the differentiability of
    the computation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that may have caught your attention is that the finite difference
    method can be used on â€mostâ€ quantum nodes, but not on all of them. Thatâ€™s because
    some quantum nodes may return outputs that donâ€™t make it possible for the finite
    differences method to work with them. For instance, if a node returns an array
    of samples, the differentiability is broken. If instead, it returned an expectation
    value â€” even if it were just an empirical approximation obtained from a collection
    of samples â€” then a gradient would exist and the finite differences method could
    be used to compute it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: List all the PennyLane differentiation methods that can be used on quantum hardware
    and all the differentiation methods that can be used on simulators.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The way in which you can ask PennyLane to use a specific differentiation method
    â€” letâ€™s say one named `"``method``"` â€” is by passing the optional argument `diff_method`
    `=` `"``method``"` to the quantum node decorator or initializer. That is, if you
    use the QNode decorator, you should write
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Alternatively, if you decided to assemble a circuit `circuit` and a device
    `device` into a quantum node directly, you should call the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By default, `diff_method` is set to `"``best``"`, which, as weâ€™ve said before,
    lets PennyLane choose on our behalf the best differentiation method.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: In our particular case, PennyLane has been using the backpropagation differentiation
    method all this time â€” without us even noticing!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know which differentiation method PennyLane uses by default
    on a device `dev` and on a certain interface `inter` (in our case, `"``tensorflow``"`),
    you can just call the following function:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our quantum node is compatible with all the differentiation methods except with
    device differentiation, because `default``.``qubit` doesnâ€™t implement its own
    special way of computing gradients. Thus, just to better understand the differences
    in performance, we can try out all the differentiation methods and see how they
    behave.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: To learn moreâ€¦
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, when using the Lightning simulator, we do need to ask
    TensorFlow to use doubles all across the Keras model instead of floats â€” itâ€™s
    not an option, but a necessity. The same happens when we use differentiation methods
    other than backpropagation with `default``.` `qubit`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s begin with adjoint differentiation. In order to retrain our model with
    this differentiation method, we will rerun all our previous code, but changing
    the quantum node definition to the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reasonably enough, instead of rerunning all your code, you may want to add
    the execution of alternative differentiation methods as part of it â€” particularly
    if you are keeping your code in a notebook. If you want to do so while ensuring
    that the training is done in identical conditions (the same environment and seeds),
    these are the lines that you would have to run:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Upon running this, you will get the exact same training behavior that we got
    with backpropagation â€” the same evolution of the training and validation losses
    and, of course, the same accuracies. Where there is a noticeable change, however,
    is in training time. In our case, training with backpropagation took a rough average
    of ![21](img/file1395.png "21") seconds per epoch. Using adjoint differentiation,
    in contrast, the training took, on average, ![10](img/file161.png "10") seconds
    per epoch. Thatâ€™s a big gain!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Actually, if you wanted to further reduce the training time, you should try
    the Lightning simulator with the adjoint method. Depending on the hardware configuration
    of your computer, it can yield very significant boosts in performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s now train our model with the two remaining differentiation methods, which
    are the hardware-compatible ones: the parameter-shift rule and finite differences.
    In order to do that, we will just have to rerun our code changing the value of
    the differentiation method in the quantum node definition. In order to avoid redundancy,
    we wonâ€™t rewrite everything here â€” we trust these small changes to you!'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'When retraining with these two models, these are the results we obtained:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Using the parameter shift rule yielded the very same results as the other differentiation
    methods. Regarding training time, each epoch took, on average, ![14](img/file1396.png
    "14") seconds to complete. Thatâ€™s better than the ![21](img/file1395.png "21")
    seconds that we got with backpropagation, but not as good as the ![10](img/file161.png
    "10") seconds that the adjoint method gave us.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using finite differences differentiation, we got, once again, the same
    results that the other methods yielded. On average, each epoch took ![10](img/file161.png
    "10") seconds to complete, which matches the training time of adjoint differentiation.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that this comparison holds for the particular model that we have
    considered. The results may vary as the complexity of the models increases and,
    in particular, hardware-compatible methods may perform more poorly on simulators
    when training complex QNN architectures.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: And thatâ€™s probably all you need to know about the differentiation methods that
    are available in PennyLane. Letâ€™s now have a look at what Qiskit has to offer
    in terms of quantum neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '10.3 Quantum neural networks in Qiskit: a commentary'
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we had a chance to explore in great depth the implementation
    and training of quantum neural networks in PennyLane. We wonâ€™t do an analogous
    discussion for Qiskit in such a level of detail, but we will at least give you
    a few ideas about how to get started should you ever need to use Qiskit in order
    to work with quantum neural networks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane provides a very homogeneous and flexible experience. No matter if
    youâ€™re training a simple binary classifier or a complex hybrid architecture like
    the ones we will study in the following chapter, itâ€™s all done in the same way.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Qiskit, by contrast, provides a more â€structuralâ€ approach. It gives you a suite
    of classes that can be used to train different kinds of neural networks and that
    allow you to define your networks in different ways. Itâ€™s difficult to judge whether
    this is a better or worse approach; in the end, itâ€™s just a matter of taste. On
    the one hand, training basic models in Qiskit might be simpler than training them
    in PennyLane because of the ease of use of some of these purpose-built classes.
    On the other hand, having different ways of accomplishing the same thing â€” one
    could argue â€” might generate some unnecessary complexity.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The classes provided by Qiskit for the implementation of quantum neural networks
    can be imported from `qiskit_machine_learning``.``neural_networks` (please, refer
    to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing the Tools*, for installation
    instructions). These are some of them:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-layer QNN**: The `TwoLayerQNN` class can be used to implement a quantum
    neural network with a single feature map, a variational form, and an observable.
    It works for any vanilla quantum neural network.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circuit QNN**: The `CircuitQNN` class allows you to implement a quantum neural
    network from a parametrized circuit. The final state of the circuit will be measured
    on the computational basis, and each measurement result can be mapped to an integer
    label through an interpreter function. This can be useful, for instance, if you
    want to build a classifier.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the way, in Qiskit lingo, variational forms are called **ansatzs**. As you
    surely remember, this is also the name that was used in the context of the VQE
    algorithm that we studied in *Chapter* *[*7*](ch015.xhtml#x1-1190007)*, VQE: Variational
    Quantum* *Eigensolver*.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '*If, when designing a neural network in Qiskit, you want to use the ZZ feature
    map or the two-local variational form, thereâ€™s no need for you to re-implement
    them; they are bundled with Qiskit. You can get them as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the call to the ZZ feature map class, we have set the number of repetitions
    to ![1](img/file13.png "1") â€” any other number would yield a feature map with
    that number of repetitions of the ZZ feature map scheme. In the call to the two-local
    class, we have also specified â€” in addition to the repetitions â€” the rotation
    gates, the controlled gates, and the entanglement layout that we want to use.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of example, we can define a `TwoLayer` quantum neural network
    on three qubits with the ZZ feature map and two-local variational form that we
    have just instantiated. We can do this as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we havenâ€™t specified an observable, the resulting QNN will return the
    expectation value of the ![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes
    Z \otimes Z") observable measured after feeding the execution of the networkâ€™s
    circuit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simulate analytically the network that we have just created on some
    random inputs and optimizable parameters as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first argument is an array with some (random) classical inputs while the
    second argument is an array with (random) values for the optimizable parameters.
    Notice how weâ€™ve used the `qnum_inputs` and `num_weights` properties of the quantum
    neural network.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: All the neural network classes that we have presented are subclasses of a `NeuralNetwork`
    class. For example, should you want to train a neural network as a classifier,
    you could rely on Qiskitâ€™s `NeuralNetworkClassifier` class. This class can be
    initialized with a `NeuralNetwork` object and specifying a loss function and an
    optimizer among other things.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, there is a subclass of `NeuralNetworkClassifier` that can
    be used to readily create a trainable neural network classifier directly, providing
    a feature map, a variational form, an optimizer, a loss, and so on.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: This subclass is called `VQC` (short for Variational Quantum Classifier) and
    it can also be imported from the Qiskit module `qiskit_machine_learning``.``algorithms``.``classifiers`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to create a neural network classifier object from our previous
    `qnn` object using the default parameters provided by Qiskit, you could run the
    following instructions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By default, the classifier will use the squared error loss function and rely
    on the SLSQP optimizer [[62](ch030.xhtml#Xkraft1988software)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, if you had some training data `data_train` with labels `labels_train`,
    you could train your newly-created classifier by calling the `fit` method as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you then wanted to compute the outcomes of the trained classifier on some
    data `data_test`, you could use the `predict` method like so:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Alternatively, if you wanted to compute the accuracy score of the trained model
    on some test dataset (`data_test` and `labels_test`), you could run the following
    instruction:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Nevertheless, you shouldnâ€™t care too much about the `NeuralNetworkClassifier`
    and `VQC` classes because, as it turns out, there is an alternative â€” and, in
    our opinion, better â€” way to train QNNs in Qiskit. We will discuss it in the following
    chapter, and it will involve an interface with an existing machine learning framework,
    PyTorch. What is more, being able to work with this interface will allow us to
    explore Qiskitâ€™s â€Torch Runtimeâ€: a Qiskit utility that will enable us to more
    efficiently train QNNs on IBMâ€™s real quantum hardware. This is the same technique
    that we used in *Chapter* *[*5*](ch013.xhtml#x1-940005)*, QAOA:* *Quantum Approximate
    Optimization Algorithm*, to run QAOA executions on quantum hardware. Exciting,
    isnâ€™t it? Bear with us until the end of the next chapter.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '*# Summary'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: This has been a long journey, hasnâ€™t it? In this chapter, we first introduced
    quantum neural networks as quantum analogs of classical neural networks. We have
    seen how the training of a quantum neural network is very similar to that of a
    classical one, and weâ€™ve also explored the differentiation methods that make this
    possible.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: With the theory out of the way, we got our keyboards ready to do some work.
    We learned how to implement and train a quantum neural network using PennyLane,
    and we also discussed some technicalities about this framework, such as details
    about the differentiation methods that it provides.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane comes with some wonderful simulators, but â€” as we already mentioned
    in *Chapter* [*2*](ch009.xhtml#x1-400002)*, The Tools of the Trade in Quantum
    Computing* â€” itâ€™s also integrated with quantum hardware platforms such as Amazon
    Braket and IBM Quantum. Thus, your ability to train quantum neural networks on
    actual quantum computers is at your fingertips!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter with a short overview of how to work with quantum neural
    networks in Qiskit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have a solid understanding of quantum neural networks. Combined
    with your previous knowledge of quantum support vector machines, this gives you
    a fairly solid foundation in quantum machine learning. In the following chapter
    â€” which will be very practically-oriented â€” we will explore more complex model
    architectures based on quantum neural networks.*******
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
