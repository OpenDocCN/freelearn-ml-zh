- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章
- en: Quantum Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络
- en: '*The mind is not a vessel to be filled, but a fire to be kindled.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*心灵不是要被填满的容器，而是一团需要被点燃的火焰。*'
- en: — Plutarch
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ——普鲁塔克
- en: 'In the previous chapter, we explored our first family of quantum machine learning
    models: quantum support vector machines. Now it is time for us to take one step
    further and consider yet another family of models, that of **Quantum** **Neural
    Networks** (**QNNs**).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了我们的第一个量子机器学习模型家族：量子支持向量机。现在是我们进一步探索另一个模型家族的时候了，那就是**量子** **神经网络**（**QNNs**）。
- en: 'In this chapter, you will learn how the notion of a quantum neural network
    can arise naturally from the ideas behind classical neural networks. Of course,
    you will also learn how quantum neural networks work and how they can be trained.
    Then, you will explore how quantum neural networks can actually be implemented,
    run, and trained using the two quantum frameworks that we have been working with
    so far: Qiskit and PennyLane.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习量子神经网络的概念如何自然地从经典神经网络背后的思想中产生。当然，你还将学习量子神经网络的工作原理以及它们的训练方法。然后，你将探索如何使用我们迄今为止一直在使用的两个量子框架——Qiskit和PennyLane——来实现、运行和训练量子神经网络。
- en: 'These are the contents of this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容如下：
- en: Building and training quantum neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练量子神经网络
- en: Quantum neural networks in PennyLane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLane中的量子神经网络
- en: 'Quantum neural networks in Qiskit: a commentary'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskit中的量子神经网络：评论
- en: Quantum support vector machines and quantum neural networks are probably the
    two most popular families of QML models, so, by the end of this chapter, you will
    already have a solid foundation in quantum machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 量子支持向量机和量子神经网络可能是QML模型中最受欢迎的两个家族，所以，到本章结束时，你将已经在量子机器学习方面打下坚实的基础。
- en: To get started, let’s understand how quantum neural networks work and how they
    can be effectively trained. Let’s get to it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们了解量子神经网络是如何工作的，以及它们如何被有效地训练。让我们着手吧！
- en: 10.1 Building and training a quantum neural network
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.1 构建和训练一个量子神经网络
- en: Just like quantum support vector machines, quantum neural networks are what
    we called ”CQ models” back in *Chapter* [*8*](ch017.xhtml#x1-1390008), *What is
    Quantum Machine* *Learning?*, — models with purely classical inputs and outputs
    that use quantum computing at some stage. However, unlike QSVMs, quantum neural
    networks are not a ”particular case” of any classical model, although their behavior
    is inspired by that of classical neural networks. What is more, as we will soon
    see, quantum neural networks are ”purely quantum” models, in the sense that their
    execution will only require classical computing for the preparation of circuits
    and the statistical analysis of measurements. Nevertheless, just like QSVMs, quantum
    neural networks will depend on classical parameters that will be optimized classically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就像量子支持向量机一样，量子神经网络是我们之前在*第8章*[*8*](ch017.xhtml#x1-1390008)，“什么是量子机器学习？”中提到的“CQ模型”——即纯粹使用经典输入和输出，并在某个阶段使用量子计算的模型。然而，与QSVMs不同，量子神经网络不是任何经典模型的“特殊情况”，尽管它们的行为受到经典神经网络行为的启发。更重要的是，正如我们很快就会看到的，量子神经网络是“纯粹量子”模型，这意味着它们的执行将仅需要经典计算来准备电路和进行测量统计分析。尽管如此，就像QSVMs一样，量子神经网络将依赖于经典参数，这些参数将通过经典优化进行优化。
- en: To learn more…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多...
- en: As you surely know by now, (quantum) machine learning is a vast field in which
    terms hardly ever have a unique meaning. The term ”quantum neural network” can,
    in practice, be used to refer to any QML model that is inspired by the behavior
    of a classical neural network. Therefore, you should bear in mind that people
    may also use this name to refer to models different from the ones that we are
    considering to be quantum neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如你此刻所知，(量子)机器学习是一个术语含义几乎不唯一的广阔领域。在实践中，“量子神经网络”这个术语可以用来指代任何受经典神经网络行为启发的QML模型。因此，你应该记住，人们也可能使用这个名称来指代与我们考虑的量子神经网络不同的模型。
- en: That should be enough of an introduction. Let’s now get into the details. What
    actually are quantum neural networks and how do they relate to classical neural
    networks?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该已经足够作为介绍了。现在让我们深入细节。量子神经网络究竟是什么，它们是如何与经典神经网络相关的？
- en: 10.1.1 A journey from classical neural networks to quantum neural networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.1 从经典神经网络到量子神经网络的旅程
- en: 'If we do a small exercise of abstraction, we can think of the action of a classical
    neural network as consisting of the following stages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行一次抽象的小练习，我们可以将经典神经网络的动作看作由以下阶段组成：
- en: '**Data preparation**: This simply amounts to taking some (classical) input
    data and maybe carrying out some (simple) transformations on it. These may include
    normalizing or scaling the input data.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：这仅仅是将一些（经典）输入数据和可能对其进行的某些（简单）转换。这些可能包括对输入数据进行归一化或缩放。'
- en: '**Data processing**: Feeding the data through a sequence of layers that ”transform”
    the data as it flows through them. The behavior of this processing depends on
    some optimizable parameters, which are adjusted in training.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：通过一系列层将数据传递过去，这些层“转换”数据，随着数据流过它们。这种处理的行为取决于一些可优化的参数，这些参数在训练中会被调整。'
- en: '**Data output**: Returning the output through a final layer.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据输出**：通过最终层返回输出。'
- en: Let’s see how we can take this scheme and use it to define an analogous quantum
    model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何可以将这个方案用于定义一个类似的量子模型。
- en: '**Data preparation**: Quantum neural networks are given classical inputs (in
    the form of an array of numbers), but quantum computers don’t work on classical
    data — they work on quantum states! So how can we take these classical inputs
    and embed them into the space of quantum states?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：量子神经网络接收经典输入（以数字数组的形式），但量子计算机并不处理经典数据——它们处理量子状态！那么我们如何将这些经典输入嵌入到量子状态空间中呢？'
- en: That is a problem that we have already dealt with in *Section* [*9.2*](ch018.xhtml#x1-1660009.2).
    In order to encode the classical input of a QNN into a quantum state, we just
    have to use any feature map of our choice. As you know, we may also need to normalize
    or scale the data, of course.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们已经在 *第9.2节* 中处理过的问题。为了将QNN的经典输入编码成量子状态，我们只需要使用我们选择的任何特征映射。正如你所知，我们当然可能还需要对数据进行归一化或缩放。
- en: 'And that is how we actually ”prepare the data” for a quantum neural network:
    feeding it into a feature map.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正是这样，我们实际上为量子神经网络“准备数据”：将其输入到特征映射中。
- en: '**Data processing**: At this point, we have successfully transformed our classical
    input into a ”quantum input,” in the form of a quantum state that encodes our
    classical data according to a certain feature map. Now, we need to figure out
    a way to process this input by drawing some inspiration from the processing in
    a classical neural network.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：在这个阶段，我们已经成功地将我们的经典输入转换成了“量子输入”，即以量子状态的形式编码我们的经典数据，根据某个特征映射。现在，我们需要找出一种方法来处理这个输入，可以从经典神经网络的处理中汲取一些灵感。'
- en: Trying to replicate the full, exact behavior of a classical neural network in
    a quantum neural network might prove not to be ideal given the state of current
    quantum hardware. Instead, we can look at the bigger picture.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在当前量子硬件的状态下，试图在量子神经网络中复制经典神经网络的完整、精确的行为可能并不理想。相反，我们可以从更大的图景来看。
- en: In essence, the processing stage of a classical neural network consists in the
    application of some transformations that depend, exclusively, on some optimizable
    parameters. And that is an idea that we can very easily export to a quantum computer.
    We can simply define the ”processing” stage of a quantum neural network as…the
    application of a circuit that depends on some optimizable parameters! In addition
    to this, as we will see later in this section, this circuit can be structured
    in layers in a way that somewhat reassembles the spirit of a classical neural
    network. This circuit will be said to be a **variational form** — they are just
    like the ones we studied back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE:*
    *Variational Quantum Eigensolver*.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本质上，经典神经网络的处理阶段包括应用一些仅依赖于某些可优化参数的转换。这是一个我们可以非常容易地移植到量子计算机上的想法。我们可以简单地将量子神经网络的“处理”阶段定义为…依赖于某些可优化参数的电路的应用！此外，正如我们将在本节后面看到的那样，这个电路可以被分层结构化，以某种方式重新组合经典神经网络的精髓。这个电路将被称为**变分形式**——它们就像我们在
    *第7章* 中研究的那些一样，*VQE：变分量子本征值求解器*。
- en: '**Data output**: Once we have a processed state, we need to return a classical
    output. And this shall be the result of some measurement operation; this operation
    can be whichever one suits our problem best!'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据输出**：一旦我们有一个处理过的状态，我们需要返回一个经典输出。这将是某些测量操作的结果；这个操作可以是适合我们问题的最佳选择！'
- en: For instance, if we wanted to build a binary classifier with a quantum neural
    network, a natural choice for this measurement operation could be, for example,
    taking the expectation value of the first qubit when measured on the computational
    basis. Remember that the expectation value of a qubit simply corresponds to the
    probability of obtaining ![1](img/file13.png "1") upon measuring the qubit on
    the computational basis.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们想用量子神经网络构建一个二元分类器，这个测量操作的一个自然选择可能是，例如，在计算基上测量第一个量子比特时的期望值。记住，量子比特的期望值简单地对应于在计算基上测量量子比特获得![1](img/file13.png
    "1")的概率。
- en: And those are all the ingredients that make up a quantum neural network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是构成量子神经网络的所有成分。
- en: 'As a matter of fact, feature maps and variational forms are both examples of
    **variational circuits**: quantum circuits that are controlled by some classical
    parameters. The only actual difference between a feature map and a variational
    form is their purpose: feature maps depend on the input data and are used to encode
    it, while variational forms depend on optimizable parameters and are used to transform
    a quantum input state.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，特征图和变分形式都是**变分电路**的例子：受某些经典参数控制的量子电路。特征图和变分形式之间的唯一实际区别是它们的目的：特征图依赖于输入数据，用于对其进行编码，而变分形式依赖于可优化参数，用于将量子输入状态进行转换。
- en: This difference in purpose will materialize in the fact that we will often use
    different circuits for feature maps and variational forms. A good feature map
    need not be a good variational form, and vice versa.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种目的上的差异将体现在我们经常会为特征图和变分形式使用不同的电路。一个好的特征图不一定是一个好的变分形式，反之亦然。
- en: 'You should keep in mind that — like all things QML — the terms ”feature map”
    and ”variational form” are not entirely universal, and different authors may refer
    to them with different expressions. For example, variational forms are commonly
    referred to as **ansatzs**, as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational* *Quantum Eigensolver*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '你应该记住——就像所有量子机器学习（QML）的东西一样——“特征图”和“变分形式”这两个术语并不完全通用，不同的作者可能会用不同的表达来指代它们。例如，变分形式通常被称为**ansatzs**，正如我们在*第7章*[*7*](ch015.xhtml#x1-1190007)
    *VQE: 变分量子本征值求解器*中做的那样。'
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A quantum neural network takes a classical input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") and maps it to a quantum state through a feature map
    ![F](img/file1320.png "F"). The resulting state then goes through a variational
    form ![V](img/file379.png "V"): a variational circuit dependent on some optimizable
    parameters ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}").
    The output of the quantum neural network is the result of a measurement operation
    on the final state. All this can be seen, schematically, in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络将经典输入 ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    通过特征图 ![F](img/file1320.png "F") 映射到量子状态。然后，得到的量子状态通过变分形式 ![V](img/file379.png
    "V")：一个依赖于某些可优化参数 ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}")
    的变分电路。量子神经网络的输出是对最终状态的测量操作的结果。所有这些都可以在以下图中 schematically 看到：
- en: '![ n⃗ |FV0⟩((⃗x𝜃)) ](img/file1322.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![ n⃗ |FV0⟩((⃗x𝜃))] (img/file1322.jpg)'
- en: Thanks to our study of quantum support vector machines, we are already very
    familiar with feature maps, but we have yet to get acquainted with variational
    forms; that is what we will devote the next subsection to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们对量子支持向量机的研究，我们已经非常熟悉特征图，但我们还没有熟悉变分形式；这正是我们将致力于下一小节的内容。
- en: 10.1.2 Variational forms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.2 变分形式
- en: In principle, a variational form could be any variational circuit of your choice,
    but, in general, variational forms for QNNs follow a ”layered structure,” trying
    to mimic the spirit of classical neural networks. We can now make this idea precise.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，变分形式可以是任何你选择的变分电路，但通常，QNN的变分形式遵循“分层结构”，试图模仿经典神经网络的精髓。我们现在可以精确地阐述这个想法。
- en: If we wanted to define a variational form with ![k](img/file317.png "k") layers,
    we could consider ![k](img/file317.png "k") vectors of independent parameters
    ![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}").
    In order to define each layer ![j](img/file258.png "j"), we may take a variational
    circuit ![G_{j}](img/file1324.png "G_{j}") dependent on the parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}"). A common approach is to prepare variational
    forms by stacking these variational circuits consecutively and separating them
    by some circuits ![U_{}](img/file1326.png "U_{}")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要定义一个具有![k](img/file317.png "k")层的变分形式，我们可以考虑![k](img/file317.png "k")个独立参数的向量![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}")。为了定义每一层![j](img/file258.png
    "j")，我们可能需要一个依赖于参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png
    "G_{j}")。一种常见的方法是通过连续堆叠这些变分电路并使用一些电路![U_{}](img/file1326.png "U_{}")来准备变分形式
- en: entˆt![,independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.](img/file1327.png
    ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 纠缠，独立于任何参数，旨在在量子比特之间创建纠缠。正如![图10.1](img/file1327.png ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")所示。
- en: '![Figure 10.1: A variational form with k layers, each defined by a variational
    circuit G_{j} dependent on some parameters {\overset{\rightarrow}{\theta}}_{j}.
    The circuits U_{} entˆtareusedtocreateentanglement,andthestate \left| \psi_{}
    \right. enc\rangle denotes the output of the feature map ](img/file1331.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1：一个具有k层的变分形式，每个层由一个依赖于某些参数{\overset{\rightarrow}{\theta}}_{j}的变分电路G_{j}定义。电路U_{}用于创建纠缠，状态\left|
    \psi_{} \right. enc\rangle表示特征图的输出](img/file1331.jpg)'
- en: '**Figure 10.1**: A variational form with ![k](img/file317.png "k") layers,
    each defined by a variational circuit ![G_{j}](img/file1324.png "G_{j}") dependent
    on some parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}").
    The circuits ![U_{}](img/file1326.png "U_{}") entˆt![areusedtocreateentanglement,andthestate](img/file1328.png
    "areusedtocreateentanglement,andthestate") ![\left| \psi_{} \right.](img/file1329.png
    "\left| \psi_{} \right.") enc![\rangle](img/file1330.png "\rangle") denotes the
    output of the feature map'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.1**：一个具有![k](img/file317.png "k")层的变分形式，每个层由一个依赖于某些参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png "G_{j}")定义。电路![U_{}](img/file1326.png
    "U_{}")用于创建纠缠，状态![\left| \psi_{} \right.](img/file1329.png "\left| \psi_{} \right.")![\rangle](img/file1330.png
    "\rangle")表示特征图的输出'
- en: 'We have now outlined one of the most common structures of variational forms,
    but variational forms are best illustrated by examples. There are lots of variational
    forms out there, and there is no way we could collect them all in this book —
    in truth, there would be no point either. For this reason, we will restrict ourselves
    to presenting just three variational forms, some of which we will use later in
    the book:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了变分形式中最常见的结构之一，但变分形式最好通过例子来说明。变分形式有很多，我们不可能在这本书中收集它们所有——实际上，这样做也没有意义。因此，我们将限制自己只介绍三种变分形式，其中一些我们将在本书的后面部分使用：
- en: '**Two-local**: The **two-local variational form** with ![k](img/file317.png
    "k") repetitions on ![n](img/file244.png "n") qubits relies on ![n \times (k +
    1)](img/file1332.png "n \times (k + 1)") optimizable parameters, which we will
    denote as ![\theta_{rj}](img/file1333.png "\theta_{rj}") with ![r = 0,\ldots,k](img/file1334.png
    "r = 0,\ldots,k") and ![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n"). Its
    circuit is constructed as per the following procedure:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双局部**：在![n](img/file244.png "n")个量子比特上重复![k](img/file317.png "k")次的**双局部变分形式**依赖于![n
    \times (k + 1)](img/file1332.png "n \times (k + 1)")个可优化参数，我们将用![\theta_{rj}](img/file1333.png
    "\theta_{rj}")表示，其中![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k")和![j =
    1,\ldots n](img/file1335.png "j = 1,\ldots n")。其电路的构建按照以下步骤进行：'
- en: '**procedure** TwoLocal(![n,k,\theta](img/file1336.png "n,k,\theta"))'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过程** TwoLocal([![n,k,\theta](img/file1336.png "n,k,\theta"))](img/file1336.png
    "n,k,\theta")'
- en: '**for all** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **do**'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **执行**'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Add the* ![r](img/file1337.png
    "r")*-th* *layer.*     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *添加* ![r](img/file1337.png
    "r")*-层。     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**'
- en: Apply a ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") gate on
    qubit ![j](img/file258.png "j").
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量子位 ![j](img/file258.png "j") 上应用 ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})")
    门。
- en: '-'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Create entanglement
    between layers.*     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *在层之间创建纠缠。*    
    * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**if** ![r < k](img/file1340.png "r < k") **then**'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**如果** ![r < k](img/file1340.png "r < k") **则**'
- en: '**for all** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **do**'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **执行**'
- en: Apply a CNOT gate with control on qubit ![t](img/file48.png "t") and target
    on qubit ![t + 1](img/file1342.png "t + 1").
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对控制量子位 ![t](img/file48.png "t") 和目标量子位 ![t + 1](img/file1342.png "t + 1") 应用CNOT门。
- en: -![-](img/file1343.png "-")
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")
- en: -![-](img/file1343.png "-")**** ***In *Figure* [*10.2*](#Figure10.2) we have
    depicted the output of this procedure for ![n = 4](img/file837.png "n = 4") and
    ![k = 3](img/file1344.png "k = 3"). Sound familiar? The two-local variational
    form uses the same circuit as the angle encoding feature map for its layers, and
    then it relies on a cascade of controlled-NOT operations in order to create entanglement.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")**** ***在*图* [*10.2*](#Figure10.2) 中，我们展示了该过程在 ![n
    = 4](img/file837.png "n = 4") 和 ![k = 3](img/file1344.png "k = 3") 时的输出。听起来熟悉吗？两个局部变分形式使用与角度编码特征图相同的电路作为其层，然后它依赖于一系列受控-NOT操作来创建纠缠。
- en: Notice, by the way, how the two-local variational form with ![k](img/file317.png
    "k") repetitions has ![k + 1](img/file1345.png "k + 1") layers, not ![k](img/file317.png
    "k"). This tiny detail can sometimes be misleading.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺便说一下，注意两个局部变分形式重复 ![k](img/file317.png "k") 次时具有 ![k + 1](img/file1345.png
    "k + 1") 层，而不是 ![k](img/file317.png "k") 层。这个小小的细节有时可能会误导。
- en: The two-local variational form is very versatile, and it can be used with any
    measurement operation.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个局部变分形式非常灵活，它可以与任何测量操作一起使用。
- en: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
- en: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
- en: '***   **Tree tensor**: The **tree tensor** variational form with ![k + 1](img/file1345.png
    "k + 1") layers can be applied on ![n = 2^{k}](img/file1347.png "n = 2^{k}") qubits.
    Each layer has half the number of parameters as the previous one, so the variational
    form relies on ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k
    - 1} + \cdots + 1") optimizable parameters of the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **树张量**：具有 ![k + 1](img/file1345.png "k + 1") 层的**树张量**变分形式可以应用于 ![n
    = 2^{k}](img/file1347.png "n = 2^{k}") 量子位。每一层的参数数量是前一层的一半，因此变分形式依赖于 ![2^{k} +
    2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k - 1} + \cdots + 1") 个可优化参数的形式'
- en: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
- en: 'The procedure that defines is somewhat more opaque than that of the two-local
    variational form, and it reads as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义该过程的步骤比两个局部变分形式稍微难以理解，其内容如下：
- en: '**procedure** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过程** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
- en: On each qubit ![j](img/file258.png "j"), apply a rotation ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})").
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个量子位 ![j](img/file258.png "j") 上应用旋转 ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})")。
- en: '**for all** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **do**'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **执行**'
- en: '**for all** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k
    - r} - 1") **do**'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**对所有** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k -
    r} - 1") **执行**'
- en: Apply a CNOT operation with target on qubit ![1 + s2^{r}](img/file1354.png "1
    + s2^{r}") and controlled by qubit ![1 + s2^{r} + 2^{r - 1}](img/file1355.png
    "1 + s2^{r} + 2^{r - 1}").
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对目标在量子位 ![1 + s2^{r}](img/file1354.png "1 + s2^{r}") 上、由量子位 ![1 + s2^{r} + 2^{r
    - 1}](img/file1355.png "1 + s2^{r} + 2^{r - 1}") 控制的CNOT操作进行应用。
- en: Apply a rotation ![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")
    on qubit ![1 + s2^{r}](img/file1354.png "1 + s2^{r}").
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An image is worth a thousand words, so, please, refer to *Figure* [*10.3*](#Figure10.3)
    for a depiction of the output of this procedure for ![k = 3](img/file1344.png
    "k = 3").
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The tree tensor variational form fits best in quantum neural networks designed
    to work as binary classifiers. The most natural measurement operation that can
    be used in conjunction with it is the obtention of the expected value of the first
    qubit, as measured in the computational basis.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a curiosity, the name of the tree tensor variational form comes from mathematical
    objects that are used for the simulation of physics systems and also in some machine
    learning models. See the survey paper by Román Orús for model details [[71](ch030.xhtml#Xorus2014practical)].
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3: Tree tensor variational form on 8 = 2^{3} qubits](img/file1358.png)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.3**: Tree tensor variational form on ![8 = 2^{3}](img/file1357.png
    "8 = 2^{3}") qubits'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Strongly entangling layers**: The strongly entangling layers variational
    form acts on ![n](img/file244.png "n") qubits and can have any number ![k](img/file317.png
    "k") of layers. Each layer ![l](img/file514.png "l") is given a **range** ![r_{l}](img/file1359.png
    "r_{l}"). In total, the variational form uses ![3nk](img/file1360.png "3nk") parameters
    of the form'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The form is defined by the following algorithm:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**procedure** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **do**'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")
    on qubit ![j](img/file258.png "j").
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")
    on qubit ![j](img/file258.png "j").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a rotation ![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")
    on qubit ![j](img/file258.png "j").
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply a CNOT operation controlled by qubit ![j](img/file258.png "j") and with
    target on qubit ![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png
    "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1").
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -![-](img/file1343.png "-")
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may find a representation of a sample of this form in *Figure* [*10.4*](#Figure10.4).
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4: Strongly entangling layers form on four qubits and two layers
    with respective ranges 1 and 2](img/file1368.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Figure 10.4**: Strongly entangling layers form on four qubits and two layers
    with respective ranges ![1](img/file13.png "1") and ![2](img/file302.png "2")**'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**As a final remark, our choice to use mostly ![Y](img/file11.png "Y") rotations
    in the previous examples of variational forms is somewhat arbitrary. We could’ve
    also used ![X](img/file9.png "X") rotations, for example. The same goes for our
    choice to use controlled-![X](img/file9.png "X") operations in the entanglement
    circuits. We could have used a different controlled operation, for instance. In
    addition to this, in the two-local variational form, there are more options for
    the distribution of gates in the entanglement circuit beyond the one that we have
    considered. Our entanglement circuit is said to have a ”linear” arrangement of
    gates, but other possibilities are shown in *Figure* [*10.5*](#Figure10.5).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**作为最后的评论，我们选择在之前的变分形式示例中主要使用![Y](img/file11.png "Y")旋转是有些任意的。我们也可以使用![X](img/file9.png
    "X")旋转，例如。同样，我们选择在纠缠电路中使用受控-![X](img/file9.png "X")操作也是任意的。我们也可以使用不同的受控操作，例如。此外，在两局部变分形式中，在纠缠电路中门的分布还有更多选择，而不仅仅是我们所考虑的那种。我们的纠缠电路被认为具有“线性”的门排列，但其他可能性在*图10.5*中有所展示。'
- en: '![(a) Linear](img/file1369.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![（a）线性](img/file1369.png)'
- en: '**(a)** Linear'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** 线性'
- en: '![(b) Circular](img/file1370.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![（b）圆形](img/file1370.jpg)'
- en: '**(b)** Circular'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** 圆形'
- en: '![(c) Full](img/file1371.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![（c）完整](img/file1371.jpg)'
- en: '**(c)** Full'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**(c)** 完整'
- en: '**Figure 10.5**: Different entanglement circuits'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10.5**：不同的纠缠电路'
- en: This is all we need to know, for now, about variational forms. Combined with
    our previous knowledge of feature maps, this ends our analysis of the elements
    of a quantum neural network…almost. We still have to dive deeper into that seemingly
    innocent measurement operation at the end of every quantum neural network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们目前需要了解的所有关于变分形式的内容。结合我们之前对特征图的了解，这结束了我们对量子神经网络元素的解析……几乎。我们仍然需要深入探究每个量子神经网络末尾看似无辜的测量操作。
- en: 10.1.3 A word about measurements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.3 关于测量的说明
- en: 'As we saw back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE: Variational
    Quantum Eigensolver*, any physical observable can be represented by a Hermitian
    operator in such a way that all the possible outcomes of the measurement of the
    observable can be matched to the different eigenvalues of the operator. If you
    haven’t done so already, please, have a look at *Section* [*7.1.1*](ch015.xhtml#x1-1210007.1.1)
    if you are not familiar with this.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第7章*[*7*](ch015.xhtml#x1-1190007)中看到的，“VQE：变分量子本征值求解器”，任何物理可观测量都可以通过一个厄米算符来表示，使得所有可能的测量结果都可以与算符的不同本征值相对应。如果你还不熟悉这一点，请查看*第7.1.1节*[*7.1.1*](ch015.xhtml#x1-1210007.1.1)。
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在计算基下测量单个量子比特时，与相关厄米算符的计算基坐标矩阵可能是以下之一
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
- en: Both of these operators represent the measurement of a qubit, but they differ
    in the eigenvalues that they associate to the distinct outputs. The first operator
    associates the eigenvalues ![1](img/file13.png "1") and ![0](img/file12.png "0")
    to the qubit’s value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个算符都表示对量子比特的测量，但它们在关联不同输出时的本征值上有所不同。第一个算符将本征值![1](img/file13.png "1")和![0](img/file12.png
    "0")分别关联到量子比特的值为![0](img/file12.png "0")和![1](img/file13.png "1")，而第二个可观测量将本征值![1](img/file13.png
    "1")和![- 1](img/file312.png "- 1")关联到这些结果。
- en: Exercise 10.1
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.1
- en: The purpose of this exercise is for you to get more familiar with Dirac notation.
    Show that the two previous Hermitian operators may be written, respectively, as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是让你更熟悉狄拉克符号。证明前两个厄米算符可以分别写成
- en: '| ![1\left&#124; 0 \right\rangle\left\langle 0 \right&#124; + 0\left&#124;
    1 \right\rangle\left\langle 1 \right&#124; = \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;,\quad\left&#124; 0 \right\rangle\left\langle 0 \right&#124; - \left&#124;
    1 \right\rangle\left\langle 1 \right&#124;.](img/file1373.png "1\left&#124; 0
    \right\rangle\left\langle 0 \right&#124; + 0\left&#124; 1 \right\rangle\left\langle
    1 \right&#124; = \left&#124; 1 \right\rangle\left\langle 1 \right&#124;,\quad\left&#124;
    0 \right\rangle\left\langle 0 \right&#124; - \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;.") |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![1\left| 0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle
    1 \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.](img/file1373.png "1\left|
    0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle 1
    \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.") |'
- en: '*Hint*: Remember that the product of a ket (column vector) and a bra (row vector)
    is a matrix. We saw an example of this back in *Section* *[*7.2.1*](ch015.xhtml#x1-1240007.2.1).*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示*：记住，基（列向量）和提（行向量）的乘积是一个矩阵。我们之前在 *第7.2.1节* 中看到了一个例子。'
- en: '*As we will see later on in the chapter, frameworks such as PennyLane allow
    you to work with measurement operations defined by any Hermitian operator. This
    can give you a lot of flexibility when defining the measurement operation of a
    neural network. For instance, in an ![n](img/file244.png "n")-qubit circuit, you
    will be able to instruct PennyLane to compute the expectation value of the observable
    ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M"),
    which has as its coordinate representation in the computational basis the matrix'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*正如我们将在本章后面看到的那样，框架如PennyLane允许你使用由任何厄米算子定义的测量操作。这可以在定义神经网络的测量操作时给你带来很多灵活性。例如，在一个
    ![n](img/file244.png "n")-量子位电路中，你将能够指示PennyLane计算可观测量 ![M \otimes \cdots \otimes
    M](img/file1374.png "M \otimes \cdots \otimes M") 的期望值，其在计算基的坐标表示是矩阵'
- en: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
- en: Alternatively, you may want to consider the observable ![Z \otimes \cdots \otimes
    Z](img/file1376.png "Z \otimes \cdots \otimes Z"). It is easy to see how this
    observable will return ![+ 1](img/file1377.png "+ 1") if an even number of qubits
    are measured as ![0](img/file12.png "0"), and ![- 1](img/file312.png "- 1") otherwise.
    That’s the reason why ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes
    \cdots \otimes Z") is referred to as the **parity** observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可能想考虑可观测量 ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots
    \otimes Z")。很容易看出，如果测量到偶数个量子位为 ![0](img/file12.png "0")，则该可观测量将返回 ![+ 1](img/file1377.png
    "+ 1")，否则返回 ![- 1](img/file312.png "- 1")。这就是为什么 ![Z \otimes \cdots \otimes Z](img/file1376.png
    "Z \otimes \cdots \otimes Z") 被称为**偶数性**可观测量的原因。
- en: Of course, you will also be able to take the measurement operation to be a good
    old expectation value on the first qubit. But, the point is, there’s also a plethora
    of options available to you, should you want to explore them!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你还可以将测量操作视为第一个量子位的经典期望值。但是，重点是，如果你愿意探索，还有很多其他选项可供选择！
- en: As we mentioned before, observables are the final building blocks of every quantum
    neural network architecture. Quantum neural networks accept an input, which usually
    consists of classical data being fed through a feature map. The resulting quantum
    state is then transformed by a variational form and, lastly, some (classical)
    numerical data is obtained through a measurement operation. In this way, we have
    a ”black box” transforming some numerical inputs into outputs, that is, a model
    that — just like any other classical ML model — can be trained.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，可观测量是每个量子神经网络架构的最终构建块。量子神经网络接受一个输入，这通常由通过特征图输入的经典数据组成。然后，通过一个变分形式将得到的量子状态转换，最后通过测量操作获得一些（经典）数值数据。这样，我们就得到了一个将一些数值输入转换为输出的“黑盒”，即一个模型——就像任何其他经典机器学习模型一样——可以被训练。
- en: 'We have now defined what quantum neural networks are and learned how to construct
    them, at least in theory. That means we have a model. But this is quantum machine
    learning, so a model is not enough: we need to train it. And in order to do so,
    we will need, among other things, an optimization algorithm.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 Gradient computation and the parameter shift rule
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it is not the only option, the optimization algorithms that we shall
    use for quantum neural networks will be gradient descent algorithms; in particular,
    we will use the Adam optimizer. But, as we saw in *Chapter* [*8*](ch017.xhtml#x1-1390008)*,*
    *What is Quantum Machine Learning?*, this algorithm needs to obtain the gradient
    of the expected value of a loss function in terms of the optimizable parameters.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our model uses a quantum circuit, the computation of these gradients
    is not entirely trivial. We shall now go briefly over the three main kinds of
    differentiation methods in which these gradient computations may be carried out:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical approximation**: Of course, we have a method that always works.
    It may not always be the most efficient one, but it’s always there. In order to
    compute gradients, we may just estimate them numerically. In order to do this,
    of course, we will have to run our quantum neural network plenty of times.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to exemplify this a little bit, if we had a real-valued function taking
    ![n](img/file244.png "n") real inputs ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png
    "\left. f:R^{n}\rightarrow R \right."), we could approximate its partial derivatives
    as
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: for a sufficiently small value of ![h](img/file519.png "h"). That’s, of course,
    the most naive way to numerically approximate a derivative, but hopefully it’s
    enough to give you an intuition of how this works.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Automatic differentiation**: Given the current state of real quantum hardware,
    odds are that most of the quantum neural networks that you will train will run
    on simulators. As non-ideal as this may be, it comes with some advantages. Most
    notably, on simulated quantum neural networks, a classical computer may compute
    exact gradients using techniques similar to those employed on classical neural
    networks. If you are interested, the book Aurélien Géron [[104](ch030.xhtml#Xhandsonml),
    Chapter 10] and the one by Shai Shalev-Shwartz and Shai Ben-David [[105](ch030.xhtml#Xunderml),
    §20.6] discuss these techniques for classical neural networks.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The parameter shift rule**: The standard automatic differentiation techniques
    can only be used on simulators. Fortunately, there is still another way to compute
    gradients when executing quantum neural networks on real hardware: using the **parameter
    shift rule**. As the name suggests, this technique enables us to compute gradients
    by using the same circuit in the quantum neural network, yet shifting the values
    of the optimizable parameters. The parameter shift rule can’t always be applied,
    but it works on many common cases and can be used in conjunction with other techniques,
    such as numerical approximation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won’t get into the details of how this method works, but you may have a look
    at a research paper by Maria Schuld and others [[109](ch030.xhtml#Xpshift-schuld)]
    for more information. For example, if you had a circuit consisting of a single
    rotation gate ![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)") and the measurement
    of its expectation value ![E(\theta)](img/file1381.png "E(\theta)"), you would
    be able to compute its derivative with respect to ![\theta](img/file89.png "\theta")
    as
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'This is similar to what happens with some trigonometric functions: for instance,
    you can express the derivative of the sine function in terms of shifted values
    of the same sine function.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For our purposes, it will suffice to know that it exists and can be used. Of
    course, the parameter shift rule can also be used on simulators!
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: When quantum neural networks are run on simulators, gradients can be computed
    using automatic differentiation techniques analogous to those of classical machine
    learning. When they are run on either real hardware or simulators, these gradients
    can also be computed — at least on many cases — using the parameter shift rule.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, numerical approximation is always an effective way to compute
    gradients.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned, all of these methods are already fully implemented in
    PennyLane, and we will try them all out in the following section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything looks good and promising, but quantum neural networks also pose
    some challenges when it comes to training them. Most notably, they are known to
    be vulnerable to **barren plateaus**: situations in which the training gradients
    vanish and, thus, the training can no longer progress (see the paper by McClean
    et. al for further explanation [[67](ch030.xhtml#Xmcclean2018barren)]). It is
    also known that the kind of measurement operation used and the depth of the QNN
    play a role in how likely these barren plateaus are to be found. This is studied,
    for instance, in a paper by Cerezo and collaborators [[24](ch030.xhtml#Xcerezo2021cost)].
    In any case, you should be vigilant when training your QNNs, and follow the literature
    for possible solutions should barren plateaus threaten the learning of your models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the ingredients necessary to construct and train quantum neural
    networks. But before we get to do that in practice, we will discuss a few techniques
    and tips that will help you get the most of our brand new quantum machine learning
    models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.5 Practical usage of quantum neural networks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are a collection of ideas that you should keep in mind when designing
    QNN models and training them. You can think of it as a summary of the previous
    sections, with a few highlights from *Chapter* [*8*](ch017.xhtml#x1-1390008)*,
    What is Quantum* *Machine Learning?*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Make wise choices**: When you set out to design a QNN, you have three important
    decisions to make: you have to pick a feature map, a variational form, and a measurement
    operation. Be intentional about these choices and consider the problem and the
    data that you are working with. Your decisions can influence how likely you are
    to find barren plateaus, for instance. A good recommendation is to check the literature
    for similar problems and to build up from there.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size matters**: When you use a well-designed variational form, in general,
    the power of the resulting quantum neural network will be directly related to
    the number of optimizable parameters it has. Use too many parameters, and you
    may have a model that overfits. Use very few, and your model may end up underfitting.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize optimization**: For most problems, the Adam optimizer can be your
    go-to choice for training a quantum neural network. Remember that, as we discussed
    in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum* *Machine Learning?*,
    you will have to pick a learning rate and a batch size when using Adam.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smaller learning rate will make the algorithm more accurate, but also slower.
    Analogously, a higher batch size should make the optimization more effective,
    to the detriment of execution time.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Feed your QNN properly**: The data that is fed to a quantum neural network
    should be normalized according to the requirements of the feature map in use.
    In addition, depending on the dimensions of the input data, you may want to rely
    on dimensionality reduction techniques.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the more data you have, the better. Nonetheless, one additional fact
    that you may want to take into account is that, under some conditions, quantum
    neural networks have been shown to need fewer data samples than classical neural
    networks in order to be successfully trained [[112](ch030.xhtml#Xqnn-lowdata)].
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: If you want to further boost the power of your quantum neural networks, you
    may want to consider the **data reuploading** technique [[110](ch030.xhtml#Xperez2020data)].
    In a vanilla QNN, you have a feature map ![F](img/file1320.png "F") dependent
    on some input data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}"),
    which is then followed by a variational form ![V](img/file379.png "V") dependent
    on some optimizable parameters ![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}"). Data reuploading simply consists in repeating
    this scheme — any number of times you want — before performing the measurement
    operation of the QNN. The feature maps use the same input data in each repetition,
    but each instance of the variational form takes its own, independent, optimizable
    parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'This is represented in the following diagram, which shows data reuploading
    with ![k](img/file317.png "k") repetitions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![|FVFV0⟩((((n⃗x⃗𝜃⃗x⃗𝜃)1)k)) ... ](img/file1384.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: This has been shown, both in practice and in theory [[113](ch030.xhtml#Xdatare-schuld)],
    to offer some advantages over the simpler, standard approach at the cost of increasing
    the depth of the circuits that are used. In any case, it is good to have it in
    mind when implementing your own QNNs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our theoretical discussion of quantum neural networks. Now it’s
    time for us to get our hands dirty with the actual implementation of all the fancy
    artifacts and techniques that we have discussed. In this regard, we will focus
    mostly on PennyLane. Let’s begin!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Quantum neural networks in PennyLane
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to implement and train our first quantum neural network with
    PennyLane. The PennyLane framework is great for many applications, but it shines
    the most when it comes to the implementation of quantum neural network models.
    This is all due to its flexibility and good integration with classical machine
    learning frameworks. We, in particular, are going to be using PennyLane in conjunction
    with TensorFlow to train a QNN-based binary classifier. All that effort that we
    invested in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine
    Learning?*, is finally going to pay off!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we are using **version 2.9.1** of the TensorFlow package and **version
    0.26** of PennyLane.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing PennyLane, NumPy, and TensorFlow and setting some
    seeds for these packages, just to make sure that our results are reproducible.
    We can achieve this with the following piece of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you may still get slightly different results from ours if
    you are using different package versions. However, the results you obtain will
    be fully reproducible in your own machine.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get to our problem, there’s one last detail that we need to sort
    out. PennyLane works with doubles while TensorFlow uses ordinary floats. This
    isn’t always an issue, but it’s a good idea to ask TensorFlow to work with doubles
    just as PennyLane does. We can accomplish this as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this out of the way, let’s meet our problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Preparing data for a QNN
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, we are going to train a QNN model to implement
    a binary classifier. Our recurrent use of binary classifiers is no coincidence,
    for binary classifiers are perhaps the simplest machine learning models to train.
    Later in the book, however, we will explore more exciting use cases and architectures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example problem, we are going to use one of the toy datasets provided
    by the scikit-learn package: the ”Breast cancer Wisconsin dataset” [[32](ch030.xhtml#XDua:2019)].
    This dataset has a total of ![569](img/file1385.png "569") samples with ![30](img/file620.png
    "30") numerical variables each. These variables describe features that can be
    used to characterize whether a breast mass is benign or malignant. The label of
    each sample can be either ![0](img/file12.png "0") or ![1](img/file13.png "1"),
    corresponding to malignant or benign, respectively. You may find the documentation
    of this dataset online at [https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)
    (the original documentation of the dataset can also be found at [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get this dataset by calling the `load_breast_cancer` function from `sklearn``.``datasets`,
    setting the optional argument `return_X_y` to true in order to retrieve the labels
    in addition to the samples. For that, we can use the following instructions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we trained QSVMs, since we were not going to make any comparisons between
    models, a training and test dataset sufficed. In our case, however, we are going
    to train our models with early stopping on the validation loss. This means — in
    case you don’t remember — that we will be keeping track of the validation loss
    and we will halt the training as soon as it doesn’t improve — according to some
    criteria that we will define. What is more, we will keep the model configuration
    that best minimized the validation loss. Using the test dataset for this purpose
    wouldn’t be good practice, for then the test dataset would have played a role
    in the training and it would not give a good estimate of the true error; that’s
    why we will need a separate validation dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split our dataset into a training, validation, and test dataset as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'All the variables in the dataset are non-zero, but they are not normalized.
    In order to use them with any of our feature maps, we shall normalize the training
    data between ![0](img/file12.png "0") and ![1](img/file13.png "1") using `MaxAbsScaler`
    as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we then normalize the test and validation datasets in the same proportions
    as the training dataset:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just as we did when we trained a QSVM in the previous chapter!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have simply done some fairly standard data preprocessing, without
    having to think too much about the actual architecture of our future quantum neural
    network. But that changes now. We have a problem to address: our dataset has ![30](img/file620.png
    "30") variables, and that can be a pretty large number for current quantum hardware.
    Since we don’t have access to quantum computers with ![30](img/file620.png "30")
    qubits, we may consider the following choices:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Use the amplitude encoding feature map on ![5](img/file296.png "5") qubits,
    which can accommodate up to ![2^{5} = 32](img/file1386.png "2^{5} = 32") variables
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use any of the other feature maps that we have used, but in conjunction with
    a dimensionality reduction technique
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will go for the latter choice. You can try the other possibility on your
    own: it’s fairly straightforward if you use the `qml``.``AmplitudeEmbedding` template
    that we studied back in *Chapter* [*9*](ch018.xhtml#x1-1600009)*, Quantum Support
    Vector* *Machines*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As you follow along this section, try to implement a QNN using all the original
    variables with amplitude encoding on five qubits.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, when feeding the data to the `qml``.` `AmplitudeEmbedding`
    object through the features argument, instead of using the `inputs` variable,
    you should use `[``a` `for` `a` `in` `inputs``]`. This is needed because of some
    internal type conversions that PennyLane needs to perform.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Training a quantum neural network on a simulator is a very computationally-intensive
    task. We don’t want anyone’s computer to crash, so, just to make sure everyone
    can run this example smoothly, we will restrict ourselves to using ![4](img/file143.png
    "4")-qubit circuits. Thus, we will use a dimensionality reduction technique to
    shrink the number of variables to ![4](img/file143.png "4"), and then set up a
    QNN with a feature map that will take the resulting ![4](img/file143.png "4")
    input variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the previous chapter, we will use principal component analysis
    in order to reduce the number of variables in our dataset to ![4](img/file143.png
    "4"):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have our data fully ready, we need to choose how our quantum neural
    network is going to work. This is exactly the focus of the next subsection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Building the network
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our case, we will choose the ZZ feature map and the two-local variational
    form. Neither is built into PennyLane, so we have to provide our own implementation
    of these variational circuits. PennyLane includes, however, a version of the two-local
    form with circular entanglement (`qml``.``BasicEntanglerLayers`), in case you
    want to use it in your QNNs. To implement the circuits that we need, we can just
    use the pseudocode that we provided in *Section* *[*10.1.2*](#x1-18400010.1.2)
    and do something like the following:*
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we already implemented the ZZ feature map in PennyLane in the
    previous chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have talked about observables, and how these are represented
    by Hermitian operators in quantum mechanics. PennyLane allows us to work directly
    with these Hermitian representations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Remember how every circuit in PennyLane returns the result of some measurement
    operation? For instance, you may use `return` `qml``.``probs``(``wires` `=` `[0])`
    at the end of the definition of a circuit in order to get the probabilities of
    every possible measurement outcome on the computational basis. Well, it turns
    out that PennyLane offers a few more possibilities. For instance, given any Hermitian
    matrix ![A](img/file183.png "A") (encoded as a numpy array `A`), we may retrieve
    the expectation value of ![A](img/file183.png "A") on an array of wires `w` at
    the end of a circuit simply by calling `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)`. Of course, the dimensions of ![A](img/file183.png "A") must be compatible
    with the length of `w`. This is useful in our case, for in order to get the expectation
    value on the first qubit, we will just have to compute the expectation value of
    the Hermitian
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: 'The matrix ![M](img/file704.png "M") can be constructed as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this construction, we have used the fact that ![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|"),
    as we discussed in an exercise earlier in this chapter. This will give us, as
    output, a value between ![0](img/file12.png "0") and ![1](img/file13.png "1"),
    which is perfect to construct a classifier: as usual, we will assign class ![1](img/file13.png
    "1") to every data instance with a value of ![0.5](img/file1166.png "0.5") or
    higher, and class ![0](img/file12.png "0") to all the rest.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all the pieces gathered in order to implement our quantum neural
    network. We are going to construct it as a quantum node with two arguments: `inputs`
    and `theta`. The first argument is mandatory: in order for PennyLane to be able
    to train a quantum neural network with TensorFlow, its first argument must accept
    an array with all the inputs to the network, and the name of this argument must
    be `inputs`. After this argument, we may add as many as we want. These can correspond
    to any parameters of the circuit, and, of course, they need to include the optimizable
    parameters in the variational form.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we may implement our quantum neural network as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To keep things simple, we have chosen to use just one repetition of the variational
    form. If your dataset is more complex, you may want to increase this number in
    order to have more trainable parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Notice, by the way, how we have added the argument `interface` `=` `"``tf``"`
    to the quantum node initializer. This is so that the quantum node will work with
    tensors (TensorFlow’s data object) in lieu of with arrays, just to allow PennyLane
    to communicate smoothly with TensorFlow. Had we used the `@qml``.``qnode` decorator,
    we would’ve had to include the argument in its call.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: This defines the quantum node that implements our quantum neural network. Now
    we need to figure out a way to train it, and, for that purpose, we will rely on
    TensorFlow. We’ll do exactly that in the next subsection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Using TensorFlow with PennyLane
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine Learning?*,
    we already learned how TensorFlow can be used to train a classical neural network.
    Well, thanks to PennyLane’s great interoperability, we will now be able to train
    our quantum neural network with TensorFlow almost as if it were a classical one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane can also be integrated with other classical machine learning frameworks
    such as PyTorch. In addition, it provides its own tools to train models based
    on the NumPy package, but these are more limited.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how we could construct TensorFlow models using Keras layers and joining
    them in sequential models? Look at this:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is how you can create a Keras layer containing our quantum neural network
    — just as if it were any other layer in a classical model! In order to do this,
    we’ve had to call `qml``.``qnn``.``KerasLayer`, and we’ve had to pass a few things
    to it. First, of course, we’ve sent the quantum node with the neural network.
    Then, a dictionary is indexed by the names of all the node arguments that take
    the optimizable parameters, and specifies, for each of these arguments, the number
    of parameters that they take. Since we only have one such argument, `theta`, and
    it should contain ![8](img/file506.png "8") optimizable parameters (that is, it
    will be an array of length ![8](img/file506.png "8")), we have sent in `{``"``theta``:`
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`8}`. Lastly, we’ve had to specify the dimension of the output of the quantum
    node; since it only returns a numerical expectation value, this dimension is ![1](img/file13.png
    "1").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a quantum layer, we can create a Keras model easily:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability to integrate quantum nodes into neural networks with such a level
    of flexibility will enable us to easily construct more complex model architectures
    in the following chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Having our model ready, we now have to pick an optimizer and a loss function,
    and then we can compile the model just like any classical model. In our case,
    we will use the binary cross entropy loss (because we are training a binary classifier,
    after all) and we will rely on the Adam optimizer with a learning rate of ![0.005](img/file1389.png
    "0.005"). For the remaining parameters of the optimizer, we will trust the default
    values. Our code is, then, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In addition to this, we will use early stopping on the validation loss with
    a patience of two epochs by using the following instructions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And we are now ready to send the final instruction to get our model trained.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, at some point in this chapter, we discussed the different
    ways in which gradients involving quantum neural networks could be computed. And
    you might wonder why we haven’t had to deal with that in order to get our model
    trained.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that PennyLane already picks the best differentiation method for
    us in order to compute gradients. Each quantum node can use certain differentiation
    methods — for instance, nodes with devices that act as interfaces to real hardware
    can’t use automatic differentiation methods, but nodes with simulators can, and
    most do.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Later in this section, we will discuss in detail all the differentiation methods
    that can be used in PennyLane.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'To train our model, we just have to call the `fit` method. Since we will be
    using early stopping, we will be generous with the number of epochs and set it
    to ![50](img/file1390.png "50"). Also, we will fix a batch size of ![20](img/file588.png
    "20"). For that, we can use the following piece of code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output that you will get upon running this instruction will be similar
    to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To learn more…
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: If you followed all that we’ve done so far without having asked TensorFlow to
    work with doubles, everything would work just fine — although you would get slightly
    different results. Nonetheless, if you try to fit a model using the Lightning
    simulator, you do need to ask TensorFlow to use doubles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have manually shrunk the progress bar so that the output could
    fit within the width of the page. Also, keep in mind that the execution time may
    vary from device to device, but, in total, the training shouldn’t take more than
    ![20](img/file588.png "20") minutes on an average
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: computer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Just by looking at the raw output, we can already see that the model is indeed
    learning, because there is a very significant drop in both the training and validation
    losses as the training progresses. It could be argued that there might be a tiny
    amount of overfitting, because the drop in the training loss is slightly greater
    than that in the validation loss. In any case, let’s wait until we have a look
    at the accuracies before coming to any final conclusions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the training has only run for ![16](img/file619.png "16") epochs,
    so it’s easy to get insights from the output returned by TensorFlow. Nonetheless,
    in the real world, training processes can go on for up to very large numbers of
    epochs, and, needless to say, in those situations the console output isn’t particularly
    informative. In general, it’s always a good practice to plot both the training
    and validation losses against the number of epochs, just to get a better insight
    into the performance of the training process. We can do this with the following
    instructions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’ve decided to define a function just so that we can reuse it in future training
    processes. The resulting plot is shown in *Figure* [*10.6*](#Figure10.6).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: Training and validation loss functions for every epoch](img/file1391.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10.6**: Training and validation loss functions for every epoch'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'And now it’s time for our final test. Let’s check the accuracy of our model
    on all our datasets to see if its performance is acceptable. This can be done
    with the following piece of code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Upon running this, we get a training accuracy of ![71\%](img/file1392.png "71\%"),
    a validation accuracy of ![72\%](img/file1393.png "72\%"), and a test accuracy
    of ![72\%](img/file1393.png "72\%"). These results don’t reflect any kind of overfitting.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing your own variational forms, you may prefer to use one
    of PennyLane’s built-in circuits. For instance, you could use the `StronglyEntanglingLayers`
    class. You should keep in mind, however, that the resulting variational form —
    as opposed to our own implementation of two-local — won’t take a one-dimensional
    array of inputs, but a three dimensional one! In particular, this form on ![n](img/file244.png
    "n") qubits with ![l](img/file514.png "l") layers will take as input a three-dimensional
    array of size ![n \times l \times 3](img/file1394.png "n \times l \times 3").
    Remember how, in this variational form, we need ![3](img/file472.png "3") arguments
    for the rotation gates, and there are ![n](img/file244.png "n") such gates in
    each of the ![l](img/file514.png "l") layers (you can take another look at *Figure*
    * [*10.4*](#Figure10.4)).*
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are ever in doubt, you may call the `StronglyEntanglingLayers``.``shape`
    function specifying the number of layers and the number of qubits in the respective
    arguments `n_layers` and `n_wires`. This will return a three-tuple with the shape
    that the variational form expects.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we could redefine our previous QNN to use this variational form
    as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this piece of code, we have stored in `nreps` the number of repetitions that
    we want in each instance of the variational form, in `weights_dim` the dimensions
    of the input that the variational form expects, and in `nweights` the number of
    inputs that each instance of the variational form will take. The rest is pretty
    self-explanatory. Inside the circuit, we’ve had to reshape the `theta` array of
    parameters to make it fit into the shape that the variational form expects; in
    order to do this, we’ve used the `tf``.``reshape` function, which can reshape
    TensorFlow’s tensors while preserving all their metadata. The `weights_strong`
    dictionary that we defined at the end is the one that we would send to TensorFlow
    when constructing the Keras layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already learned how you can train a quantum neural network using PennyLane
    and TensorFlow. We shall now discuss a few technical details in depth before bringing
    this section to an end.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Gradient computation in PennyLane
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have already mentioned, when you train a model with PennyLane, the framework
    itself figures out the best way in which to compute gradients. Different quantum
    nodes may be compatible with different methods of differentiation based on a variety
    of factors, most notably the kind of device they use.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: For an up-to-date reference of the differentiation methods that the `default``.`
    `qubit` simulator supports, you may check the online documentation at [https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: You will see that the compatibility of a quantum node with a differentiation
    method not only depends on the device itself but also on the return type of the
    node and the machine learning interface (in our case, the interface was TensorFlow).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the differentiation methods that can be used in PennyLane:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation**: Just the good old backpropagation method that is used
    in classical neural networks. Of course, this differentiation method only works
    on simulators that are compatible with automatic differentiation, because that
    is what is needed in order to analytically compute the gradients.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``backprop``"`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Adjoint differentiation**: This is a more efficient version of backpropagation
    that relies on some of the nice computational ”oddities” of quantum computing,
    such as the fact that all the quantum circuits are implemented by unitary matrices,
    which are trivially invertible. Like backpropagation, this method only works on
    the simulators that are compatible with automatic differentiation, but it is more
    restrictive.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``adjoint``"`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Finite differences**: Ever took a numerical analysis course at college? Then
    this will sound familiar. This method implements the old-school way of computing
    a numerical approximation of a gradient that we discussed in the previous section.
    It works on almost every quantum node.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``finite``-``diff``"`.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parameter shift rule**: PennyLane fully implements the parameter-shift rule
    that we introduced previously. It works on most quantum nodes.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of this method in PennyLane is `"``parameter``-``shift``"`.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Device gradient computation**: Some devices provide their own way of computing
    gradients. The name of the corresponding differentiation method is `"``device``"`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a couple of things that deserve clarification; the first of them is
    how a simulator could not be compatible with automatic differentiation. Oversimplifying
    a little bit, most simulators work by computing the evolution of the quantum state
    of a circuit and returning an output that is differentiable with respect to the
    parameters. The operations required to do all of this are themselves differentiable,
    and hence it’s possible to use automatic differentiation on quantum nodes that
    use that simulator. But simulators may work differently. For instance, a simulator
    could return individual shots in a way that ”breaks” the differentiability of
    the computation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that may have caught your attention is that the finite difference
    method can be used on ”most” quantum nodes, but not on all of them. That’s because
    some quantum nodes may return outputs that don’t make it possible for the finite
    differences method to work with them. For instance, if a node returns an array
    of samples, the differentiability is broken. If instead, it returned an expectation
    value — even if it were just an empirical approximation obtained from a collection
    of samples — then a gradient would exist and the finite differences method could
    be used to compute it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: List all the PennyLane differentiation methods that can be used on quantum hardware
    and all the differentiation methods that can be used on simulators.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The way in which you can ask PennyLane to use a specific differentiation method
    — let’s say one named `"``method``"` — is by passing the optional argument `diff_method`
    `=` `"``method``"` to the quantum node decorator or initializer. That is, if you
    use the QNode decorator, you should write
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Alternatively, if you decided to assemble a circuit `circuit` and a device
    `device` into a quantum node directly, you should call the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By default, `diff_method` is set to `"``best``"`, which, as we’ve said before,
    lets PennyLane choose on our behalf the best differentiation method.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: In our particular case, PennyLane has been using the backpropagation differentiation
    method all this time — without us even noticing!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know which differentiation method PennyLane uses by default
    on a device `dev` and on a certain interface `inter` (in our case, `"``tensorflow``"`),
    you can just call the following function:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our quantum node is compatible with all the differentiation methods except with
    device differentiation, because `default``.``qubit` doesn’t implement its own
    special way of computing gradients. Thus, just to better understand the differences
    in performance, we can try out all the differentiation methods and see how they
    behave.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that, when using the Lightning simulator, we do need to ask
    TensorFlow to use doubles all across the Keras model instead of floats — it’s
    not an option, but a necessity. The same happens when we use differentiation methods
    other than backpropagation with `default``.` `qubit`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with adjoint differentiation. In order to retrain our model with
    this differentiation method, we will rerun all our previous code, but changing
    the quantum node definition to the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reasonably enough, instead of rerunning all your code, you may want to add
    the execution of alternative differentiation methods as part of it — particularly
    if you are keeping your code in a notebook. If you want to do so while ensuring
    that the training is done in identical conditions (the same environment and seeds),
    these are the lines that you would have to run:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Upon running this, you will get the exact same training behavior that we got
    with backpropagation — the same evolution of the training and validation losses
    and, of course, the same accuracies. Where there is a noticeable change, however,
    is in training time. In our case, training with backpropagation took a rough average
    of ![21](img/file1395.png "21") seconds per epoch. Using adjoint differentiation,
    in contrast, the training took, on average, ![10](img/file161.png "10") seconds
    per epoch. That’s a big gain!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Actually, if you wanted to further reduce the training time, you should try
    the Lightning simulator with the adjoint method. Depending on the hardware configuration
    of your computer, it can yield very significant boosts in performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now train our model with the two remaining differentiation methods, which
    are the hardware-compatible ones: the parameter-shift rule and finite differences.
    In order to do that, we will just have to rerun our code changing the value of
    the differentiation method in the quantum node definition. In order to avoid redundancy,
    we won’t rewrite everything here — we trust these small changes to you!'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'When retraining with these two models, these are the results we obtained:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Using the parameter shift rule yielded the very same results as the other differentiation
    methods. Regarding training time, each epoch took, on average, ![14](img/file1396.png
    "14") seconds to complete. That’s better than the ![21](img/file1395.png "21")
    seconds that we got with backpropagation, but not as good as the ![10](img/file161.png
    "10") seconds that the adjoint method gave us.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using finite differences differentiation, we got, once again, the same
    results that the other methods yielded. On average, each epoch took ![10](img/file161.png
    "10") seconds to complete, which matches the training time of adjoint differentiation.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that this comparison holds for the particular model that we have
    considered. The results may vary as the complexity of the models increases and,
    in particular, hardware-compatible methods may perform more poorly on simulators
    when training complex QNN architectures.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: And that’s probably all you need to know about the differentiation methods that
    are available in PennyLane. Let’s now have a look at what Qiskit has to offer
    in terms of quantum neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '10.3 Quantum neural networks in Qiskit: a commentary'
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we had a chance to explore in great depth the implementation
    and training of quantum neural networks in PennyLane. We won’t do an analogous
    discussion for Qiskit in such a level of detail, but we will at least give you
    a few ideas about how to get started should you ever need to use Qiskit in order
    to work with quantum neural networks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane provides a very homogeneous and flexible experience. No matter if
    you’re training a simple binary classifier or a complex hybrid architecture like
    the ones we will study in the following chapter, it’s all done in the same way.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Qiskit, by contrast, provides a more ”structural” approach. It gives you a suite
    of classes that can be used to train different kinds of neural networks and that
    allow you to define your networks in different ways. It’s difficult to judge whether
    this is a better or worse approach; in the end, it’s just a matter of taste. On
    the one hand, training basic models in Qiskit might be simpler than training them
    in PennyLane because of the ease of use of some of these purpose-built classes.
    On the other hand, having different ways of accomplishing the same thing — one
    could argue — might generate some unnecessary complexity.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The classes provided by Qiskit for the implementation of quantum neural networks
    can be imported from `qiskit_machine_learning``.``neural_networks` (please, refer
    to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing the Tools*, for installation
    instructions). These are some of them:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-layer QNN**: The `TwoLayerQNN` class can be used to implement a quantum
    neural network with a single feature map, a variational form, and an observable.
    It works for any vanilla quantum neural network.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circuit QNN**: The `CircuitQNN` class allows you to implement a quantum neural
    network from a parametrized circuit. The final state of the circuit will be measured
    on the computational basis, and each measurement result can be mapped to an integer
    label through an interpreter function. This can be useful, for instance, if you
    want to build a classifier.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the way, in Qiskit lingo, variational forms are called **ansatzs**. As you
    surely remember, this is also the name that was used in the context of the VQE
    algorithm that we studied in *Chapter* *[*7*](ch015.xhtml#x1-1190007)*, VQE: Variational
    Quantum* *Eigensolver*.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '*If, when designing a neural network in Qiskit, you want to use the ZZ feature
    map or the two-local variational form, there’s no need for you to re-implement
    them; they are bundled with Qiskit. You can get them as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the call to the ZZ feature map class, we have set the number of repetitions
    to ![1](img/file13.png "1") — any other number would yield a feature map with
    that number of repetitions of the ZZ feature map scheme. In the call to the two-local
    class, we have also specified — in addition to the repetitions — the rotation
    gates, the controlled gates, and the entanglement layout that we want to use.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of example, we can define a `TwoLayer` quantum neural network
    on three qubits with the ZZ feature map and two-local variational form that we
    have just instantiated. We can do this as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we haven’t specified an observable, the resulting QNN will return the
    expectation value of the ![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes
    Z \otimes Z") observable measured after feeding the execution of the network’s
    circuit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simulate analytically the network that we have just created on some
    random inputs and optimizable parameters as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first argument is an array with some (random) classical inputs while the
    second argument is an array with (random) values for the optimizable parameters.
    Notice how we’ve used the `qnum_inputs` and `num_weights` properties of the quantum
    neural network.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: All the neural network classes that we have presented are subclasses of a `NeuralNetwork`
    class. For example, should you want to train a neural network as a classifier,
    you could rely on Qiskit’s `NeuralNetworkClassifier` class. This class can be
    initialized with a `NeuralNetwork` object and specifying a loss function and an
    optimizer among other things.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, there is a subclass of `NeuralNetworkClassifier` that can
    be used to readily create a trainable neural network classifier directly, providing
    a feature map, a variational form, an optimizer, a loss, and so on.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: This subclass is called `VQC` (short for Variational Quantum Classifier) and
    it can also be imported from the Qiskit module `qiskit_machine_learning``.``algorithms``.``classifiers`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to create a neural network classifier object from our previous
    `qnn` object using the default parameters provided by Qiskit, you could run the
    following instructions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By default, the classifier will use the squared error loss function and rely
    on the SLSQP optimizer [[62](ch030.xhtml#Xkraft1988software)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, if you had some training data `data_train` with labels `labels_train`,
    you could train your newly-created classifier by calling the `fit` method as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you then wanted to compute the outcomes of the trained classifier on some
    data `data_test`, you could use the `predict` method like so:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Alternatively, if you wanted to compute the accuracy score of the trained model
    on some test dataset (`data_test` and `labels_test`), you could run the following
    instruction:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Nevertheless, you shouldn’t care too much about the `NeuralNetworkClassifier`
    and `VQC` classes because, as it turns out, there is an alternative — and, in
    our opinion, better — way to train QNNs in Qiskit. We will discuss it in the following
    chapter, and it will involve an interface with an existing machine learning framework,
    PyTorch. What is more, being able to work with this interface will allow us to
    explore Qiskit’s ”Torch Runtime”: a Qiskit utility that will enable us to more
    efficiently train QNNs on IBM’s real quantum hardware. This is the same technique
    that we used in *Chapter* *[*5*](ch013.xhtml#x1-940005)*, QAOA:* *Quantum Approximate
    Optimization Algorithm*, to run QAOA executions on quantum hardware. Exciting,
    isn’t it? Bear with us until the end of the next chapter.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '*# Summary'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: This has been a long journey, hasn’t it? In this chapter, we first introduced
    quantum neural networks as quantum analogs of classical neural networks. We have
    seen how the training of a quantum neural network is very similar to that of a
    classical one, and we’ve also explored the differentiation methods that make this
    possible.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: With the theory out of the way, we got our keyboards ready to do some work.
    We learned how to implement and train a quantum neural network using PennyLane,
    and we also discussed some technicalities about this framework, such as details
    about the differentiation methods that it provides.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: PennyLane comes with some wonderful simulators, but — as we already mentioned
    in *Chapter* [*2*](ch009.xhtml#x1-400002)*, The Tools of the Trade in Quantum
    Computing* — it’s also integrated with quantum hardware platforms such as Amazon
    Braket and IBM Quantum. Thus, your ability to train quantum neural networks on
    actual quantum computers is at your fingertips!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter with a short overview of how to work with quantum neural
    networks in Qiskit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have a solid understanding of quantum neural networks. Combined
    with your previous knowledge of quantum support vector machines, this gives you
    a fairly solid foundation in quantum machine learning. In the following chapter
    — which will be very practically-oriented — we will explore more complex model
    architectures based on quantum neural networks.*******
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
