<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Time Series and Anomaly Detection</h1>
                </header>
            
            <article>
                
<p class="mce-root">Most of the models that we have discussed up to this point predict a property about something based on other properties related to that something. For example, we predicted the species of a flower based on measurements of the flower. We also tried to predict the progression of the disease diabetes in a patient based on medical attributes about that patient.</p>
<p>The premise of time series modeling is different from these types of property prediction problems. Simply put, time series modeling helps us predict the future based on attributes about the past. For example, we may want to predict future stock prices based on previous values of that stock price, or we may want to predict how many users will be on our website at a certain time based on data about how many users were on our website at previous times. This is sometimes called <strong>forecasting</strong>.</p>
<p>The data utilized in time series modeling is typically different from the data utilized in classification, regression, or clustering. A time series model operates on one or more <strong>time series</strong>, as one might expect. This series is a sequential set of properties, attributes, or other numbers paired with their corresponding date and time or a corresponding proxy for a date and time (measurement index or day number, for example). For stock prices, this series would consist of a bunch of (date and time, stock price) pairings.</p>
<p>This time series data is found everywhere in industry and in academia. It is also becoming increasingly important as we explore and develop the <strong>Internet of Things</strong> (<strong>IoT</strong>). Fitness trackers, <em>smart</em> devices such as refrigerators, thermostats, cameras, drones, and many other new devices, are producing a staggering amount of time series data.</p>
<p>Of course, you are not restricted to predicting the future with this type of data. There are many other useful things that you can do with time series data including anomaly detection, which will be covered later in this chapter. Anomaly detection attempts to detect unexpected or out-of-the-ordinary events in a time series. These events might correspond to catastrophic weather events, infrastructure failures, viral social media behavior, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representing time series data in Go</h1>
                </header>
            
            <article>
                
<p>There are purpose-built systems to store and work with time series data. Some of these are even written in Go, including Prometheus and InfluxDB. However, some of the tooling that we have already utilized in the book is also suitable to handle time series. Specifically, <kbd>github.com/kniren/gota/dataframe</kbd>, <span><kbd>gonum.org/v1/gonum/floats</kbd>, and <kbd>gonum.org/v1/gonum/mat</kbd> can help us as we are working with time series data.</span></p>
<p>Take, for example, a dataset that includes a time series representing the number of international air passengers during the years 1949-1960 (available for download at <a href="https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv">https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv</a>):</p>
<pre><strong>$ head AirPassengers.csv 
time,AirPassengers
1949.0,112
1949.08333333,118
1949.16666667,132
1949.25,129
1949.33333333,121
1949.41666667,135
1949.5,148
1949.58333333,148
1949.66666667,136</strong></pre>
<p>Here, the <kbd>time</kbd> column includes a series of times represented by a year along with a decimal, and the <kbd>AirPassengers</kbd> column includes the number of international air passengers at that <kbd>time</kbd>. In other words, this is a time series with a pairing of (time, number of passengers).</p>
<p>This is just tabular data and we can represent it perfectly well using a dataframe or matrix. Let's utilize a dataframe for simplicity, as shown in the following code:</p>
<pre>// Open the CSV file.
passengersFile, err := os.Open("AirPassengers.csv")
if err != nil {
    log.Fatal(err)
}
defer passengersFile.Close()

// Create a dataframe from the CSV file.
passengersDF := dataframe.ReadCSV(passengersFile)

// As a sanity check, display the records to stdout.
// Gota will format the dataframe for pretty printing.
fmt.Println(passengersDF)</pre>
<p>This will produce the following output:</p>
<pre><strong>$ go build
$ ./myprogram 
[144x2] DataFrame

    time        AirPassengers
 0: 1949.000000 112          
 1: 1949.083333 118          
 2: 1949.166667 132          
 3: 1949.250000 129          
 4: 1949.333333 121          
 5: 1949.416667 135          
 6: 1949.500000 148          
 7: 1949.583333 148          
 8: 1949.666667 136          
 9: 1949.750000 119          
    ...         ...          
    &lt;float&gt;     &lt;int&gt;  </strong> </pre>
<p>We could represent the series similarly with <kbd><span>gonum.org/v1/gonum/mat</span></kbd> <span>and, when/if needed, we can convert the dataframe to slices of floats for use with <kbd>gonum.org/v1/gonum/floats</kbd>. If we wanted to plot the time series, for example, we could convert the columns to floats and produce a plot with <kbd>gonum.org/v1/plot</kbd>, as shown in the following code:</span></p>
<pre>// Open the CSV file.
passengersFile, err := os.Open("AirPassengers.csv")
if err != nil {
    log.Fatal(err)
}
defer passengersFile.Close()

// Create a dataframe from the CSV file.
passengersDF := dataframe.ReadCSV(passengersFile)

// Extract the number of passengers column.
yVals := passengersDF.Col("AirPassengers").Float()

// pts will hold the values for plotting.
pts := make(plotter.XYs, passengersDF.Nrow())

// Fill pts with data.
for i, floatVal := range passengersDF.Col("time").Float() {
    pts[i].X = floatVal
    pts[i].Y = yVals[i]
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "time"
p.Y.Label.Text = "passengers"
p.Add(plotter.NewGrid())

// Add the line plot points for the time series.
l, err := plotter.NewLine(pts)
if err != nil {
    log.Fatal(err)
}
l.LineStyle.Width = vg.Points(1)
l.LineStyle.Color = color.RGBA{B: 255, A: 255}

// Save the plot to a PNG file.
p.Add(l)
if err := p.Save(10*vg.Inch, 4*vg.Inch, "passengers_ts.png"); err != nil {
    log.Fatal(err)
}</pre>
<p>Compiling and running this program produces the following plot of our time series:</p>
<div class="CDPAlignCenter CDPAlign"><img height="181" width="453" class="image-border" src="assets/f8613e7d-2182-4772-93e7-b564d8e2771a.png"/></div>
<p>As expected, the number of international air passengers increases over time as more and more people begin to travel via airplanes. We can also see that there appear to be bumps or spikes that are repeating over time. We will dive more into these features shortly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding time series jargon</h1>
                </header>
            
            <article>
                
<p>You are probably noticing by this time in the book that each set of machine learning techniques has an associated set of jargon, and time series is no different.</p>
<p>Here is an explanation of some of this jargon that will be utilized throughout the rest of the chapter:</p>
<ul>
<li><strong>Time</strong>, <strong>datetime</strong>, or <strong>timestamp</strong>: This property is the temporal element of each pairing in our time series. This could be simply a time or it could be a combination of date and time (sometimes referred to as datetime or timestamp). It might also include time zone.</li>
<li><strong>Observation</strong>, <strong>measurement</strong>, <strong>signal</strong>, or <strong>random variable</strong>: This is the property that we are trying to forecast and/or otherwise analyze as a function of time.</li>
<li><strong>Seasonality</strong>: A time series, such as the time series of air passenger data, may exhibit changes that correspond to seasons (weeks, months, years, and so on). Time series that behave in this manner are said to exhibit some seasonality.</li>
<li><strong>Trends</strong>: Time series that gradually increase or decrease over time (separate from seasonal effects) are said to exhibit a trend.</li>
<li><strong>Stationary</strong>: A time series that exhibits the same patterns over time, without trends or other gradual changes (such as changes in variance or covariance), is said to be stationary.</li>
<li><strong>Time period</strong>: The amount of time between successive observations in the time series, or the difference between one timestamp and the previously occurring timestamp in the series.</li>
<li><strong>Auto-regressive model</strong>: This is a model that tries to model a time series process by one or more delayed, or lagged, versions of the same process. For example, an auto-regressive model of stock prices would try to model stock prices by the value of the stock price at previous time intervals.</li>
<li><strong>Moving average model</strong>: This is a model that tries to model a times series based on the current and various past values of an imperfectly predictable term, commonly referred to as <strong>error</strong>. For example, this imperfectly predictable term may be some white noise in the time series.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistics related to time series</h1>
                </header>
            
            <article>
                
<p>In addition to certain jargon associated with time series, there is an important set of statistics related to time series that we will be relying on as we perform forecasting and anomaly detection. These statistics are mainly related to how values in times series are related to other values in the same time series.</p>
<p>The statistics will help us as we profile our data, which is an important part of any time series modeling project, as it is with all of the other types of modeling that we have covered. Gaining intuition about the behavior of your time series over time, seasonality, and trends is crucial for ensuring that you apply appropriate models and perform mental checks of your results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autocorrelation</h1>
                </header>
            
            <article>
                
<p><strong>Autocorrelation</strong> is a measure of how correlated a signal is with a delayed version of itself. For example, one or more previous observation of a stock price may be correlated (or change together) with the next observation of the stock price. If this was the case, we would say that the stock price was influenced by itself according to some lag or delay. We could then model the future stock price by a delayed version of itself at the specific lags indicated as highly correlated.</p>
<p>To measure the autocorrelation of a variable <em>x<sub>t</sub></em> with a delayed version of itself (or a version with a lag) <em>x<sub>s</sub></em>, we can utilize the <strong>autocorrelation function</strong> (<strong>ACF</strong>), defined as follows:</p>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img height="54" width="620" class="alignnone size-full wp-image-593 image-border" src="assets/7c5965b3-e042-4979-9db1-815be4227eef.png"/></div>
<p class="mce-root">Here, <em>s</em> could represent any lagged version of <em>x</em>. Thus, we can calculate the autocorrelation between <em>x</em> and a version of <em>x</em> that has a lag of a single time period (<em>x<sub>t-1</sub></em>), <span>between</span> <em>x</em> <span>and a version of</span> <em>x</em> <span>that has a lag of two time periods</span> <span>(</span><em>x<sub>t-2</sub></em><span>), and so on. Doing this gives us information about which delayed versions of <em>x</em> are most correlated with <em>x</em></span>, <span>and thus helps us determine which delayed versions of <em>x</em> might be good candidates for use in modeling future versions of <em>x.</em></span></p>
<p>Let's try calculating the first few autocorrelations of our airline passenger time series with itself. To do this, we first need to create a function that will calculate an autocorrelation in our time series for a specific time period lag. Here is an example implementation of this function:</p>
<pre>// acf calculates the autocorrelation for a series
// at the given lag.
func acf(x []float64, lag int) float64 {

    // Shift the series.
    xAdj := x[lag:len(x)]
    xLag := x[0 : len(x)-lag]

    // numerator will hold our accumulated numerator, and
    // denominator will hold our accumulated denominator.
    var numerator float64
    var denominator float64

    // Calculate the mean of our x values, which will be used
    // in each term of the autocorrelation.
    xBar := stat.Mean(x, nil)

    // Calculate the numerator.
    for idx, xVal := range xAdj {
        numerator += ((xVal - xBar) * (xLag[idx] - xBar))
    }

    // Calculate the denominator.
    for _, xVal := range x {
        denominator += math.Pow(xVal-xBar, 2)
    }

    return numerator / denominator
}</pre>
<p>We will then loop over a few lags and utilize the <kbd>acf()</kbd> function to calculate the various autocorrelations. This process is shown in the following code:</p>
<pre>// Open the CSV file.<br/>passengersFile, err := os.Open("AirPassengers.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer passengersFile.Close()<br/><br/>// Create a dataframe from the CSV file.<br/>passengersDF := dataframe.ReadCSV(passengersFile)<br/><br/>// Get the time and passengers as a slice of floats.<br/>passengers := passengersDF.Col("AirPassengers").Float()<br/><br/>// Loop over various values of lag in the series.<br/>fmt.Println("Autocorrelation:")<br/>for i := 1; i &lt; 11; i++ {<br/><br/>    // Shift the series.<br/>    adjusted := passengers[i:len(passengers)]<br/>    lag := passengers[0 : len(passengers)-i]<br/><br/>    // Calculate the autocorrelation.<br/>    ac := stat.Correlation(adjusted, lag, nil)<br/>    fmt.Printf("Lag %d period: %0.2f\n", i, ac)<br/>}</pre>
<p>This produces the following results:</p>
<pre><strong>$ go build</strong><br/><strong>$ ./myprogram </strong><br/><strong>Autocorrelation:</strong><br/><strong>Lag 1 period: 0.95</strong><br/><strong>Lag 2 period: 0.88</strong><br/><strong>Lag 3 period: 0.81</strong><br/><strong>Lag 4 period: 0.75</strong><br/><strong>Lag 5 period: 0.71</strong><br/><strong>Lag 6 period: 0.68</strong><br/><strong>Lag 7 period: 0.66</strong><br/><strong>Lag 8 period: 0.66</strong><br/><strong>Lag 9 period: 0.67</strong><br/><strong>Lag 10 period: 0.70</strong></pre>
<p>As we can see, the autocorrelations with lags further back in the series tend to be smaller (although, this is not the case for every lag). However, this information can be a little bit hard to absorb in its numerical form. Let's plot these values as a function of the lag to better visualize the correlations:</p>
<pre>// Open the CSV file.<br/>passengersFile, err := os.Open("AirPassengers.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer passengersFile.Close()<br/><br/>// Create a dataframe from the CSV file.<br/>passengersDF := dataframe.ReadCSV(passengersFile)<br/><br/>// Get the time and passengers as a slice of floats.<br/>passengers := passengersDF.Col("AirPassengers").Float()<br/><br/>// Create a new plot, to plot our autocorrelations.<br/>p, err := plot.New()<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>p.Title.Text = "Autocorrelations for AirPassengers"<br/>p.X.Label.Text = "Lag"<br/>p.Y.Label.Text = "ACF"<br/>p.Y.Min = 0<br/>p.Y.Max = 1<br/><br/>w := vg.Points(3)<br/><br/>// Create the points for plotting.<br/>numLags := 20<br/>pts := make(plotter.Values, numLags)<br/><br/>// Loop over various values of lag in the series.<br/>for i := 1; i &lt;= numLags; i++ {<br/><br/>    // Calculate the autocorrelation.<br/>    pts[i-1] = acf(passengers, i)<br/>}<br/><br/>// Add the points to the plot.<br/>bars, err := plotter.NewBarChart(pts, w)<br/>if err != nil {<br/>    log.Fatal(err)<br/>} <br/>bars.LineStyle.Width = vg.Length(0)<br/>bars.Color = plotutil.Color(1)<br/><br/>// Save the plot to a PNG file.<br/>p.Add(bars)<br/>if err := p.Save(8*vg.Inch, 4*vg.Inch, "acf.png"); err != nil {<br/>    log.Fatal(err)<br/>}￼</pre>
<p>This code produces the following plot of the ACF:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="540" class="image-border" src="assets/03bf60a0-2e99-416b-a1e9-c104396d33cc.png"/></div>
<p>Notice how the autocorrelations are decreasing generally, but they are staying rather large (well above 0.5) even out to lags of 20 time periods. This is an indication that our time series is not stationary. Indeed, if we look at the previous plot of our time series, it is obviously trending upward. We will deal with this non-stationary behavior later in the chapter, but for now, suffice it to say that the ACF plot is indicating to us that the lagged versions of the number of air passengers are correlated with their non-delayed counterparts.</p>
<div class="packt_tip">More generally, the ACF will allow us to determine which type of time series we are modeling. For a process that could be modeled well by an auto-regressive model, we should see that the <kbd>acf</kbd> function decreases somewhat quickly, but not immediately, as you move to further lags. For a process that could be modeled well by a so-called moving average model, we would see a significant ACF term at the first lag, but then the ACF would die off after that first lag.</div>
<p>For more information on interpreting ACF plots, see <a href="https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/">https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/</a>. This post gives some great details, some of which we are not able to cover here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partial autocorrelation</h1>
                </header>
            
            <article>
                
<p>As you might expect from the name, partial autocorrelation is related to autocorrelation, but there are some subtle differences. Partial means that this is a conditional sort of correlation. In essence, partial autocorrelation measures the correlation of a series with itself at a certain lag after subtracting off any autocorrelations at intermediate lags. You could think of this as the leftover autocorrelation after intermediate correlations have been removed.</p>
<p>The reason that we might want something like this is that we need more than just the ACF to determine the order of our time series model, assuming that it can be modeled by an auto-regressive model. Let's suppose that, using the ACF, we have determined that we can model our series by an auto-regressive model, because the ACF decays exponentially with the lags. How are we to know if we should model this time series by a version of itself lagged by one time period, or both a version of itself lagged by one time period and two time periods, and so on?</p>
<p>By subtracting out intermediate correlations, we are able to quickly determine any leftover correlation that could be modeled using an auto-regressive model with more terms. If the partial autocorrelation dies off after a first lag, we know that we should be able to model our series based on a single lagged version of itself (lagged at one time period). However, if the partial autocorrelation does not die off after a first lag, we know that we will need to employ multiple lagged versions of the time series in our auto-regressive model.</p>
<p>If we imagine linearly modeling a value in our time series (<em>x<sub>t</sub>)</em> by values at successively larger lags in the series (<em>x<sub>t-1</sub></em>, <em>x<sub>t-2</sub>,</em> and so on), our equation would look as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="32" width="335" src="assets/32e2bed0-9723-47dd-9fd2-cfae67fc46e5.jpg"/></div>
<p>The various coefficients <em>m<sub>1</sub></em>, <em>m<sub>2</sub></em>, and so on, are the partial autocorrelations for a lag of one time period, the partial autocorrelation for the lag at two time periods, and so on, respectively. Thus, all we need to do to calculate the partial autocorrelation for a certain lag is to estimate the linear regression formula that will give us the corresponding coefficient. A function that performs this calculation is called the <strong>partial autocorrelation function</strong> (<strong>PACF</strong>).</p>
<p>Using our favorite linear regression package, <kbd>github.com/sajari/regression</kbd>, we can create a Go function that implements the PACF as follows:</p>
<pre>// pacf calculates the partial autocorrelation for a series<br/>// at the given lag.<br/>func pacf(x []float64, lag int) float64 {<br/><br/>    // Create a regresssion.Regression value needed to train<br/>    // a model using github.com/sajari/regression.<br/>    var r regression.Regression<br/>    r.SetObserved("x")<br/><br/>    // Define the current lag and all of the intermediate lags.<br/>    for i := 0; i &lt; lag; i++ {<br/>        r.SetVar(i, "x"+strconv.Itoa(i))<br/>    }<br/><br/>    // Shift the series.<br/>    xAdj := x[lag:len(x)]<br/><br/>    // Loop over the series creating the data set<br/>    // for the regression.<br/>    for i, xVal := range xAdj {<br/><br/>        // Loop over the intermediate lags to build up<br/>        // our independent variables.<br/>        laggedVariables := make([]float64, lag)<br/>        for idx := 1; idx &lt;= lag; idx++ {<br/><br/>            // Get the lagged series variables.<br/>            laggedVariables[idx-1] = x[lag+i-idx]<br/>        }<br/><br/>        // Add these points to the regression value.<br/>        r.Train(regression.DataPoint(xVal, laggedVariables))<br/>    }<br/><br/>    // Fit the regression.<br/>    r.Run()<br/><br/>    return r.Coeff(lag)<br/>}</pre>
<p>Then, we can use this <kbd>pacf</kbd> function to calculate a few values of partial autocorrelation, corresponding to lags for which we previously calculated autocorrelation. This is demonstrated as follows:</p>
<pre>// Open the CSV file.        <br/>passengersFile, err := os.Open("AirPassengers.csv")
if err != nil {
    log.Fatal(err)
}
defer passengersFile.Close()

// Create a dataframe from the CSV file.
passengersDF := dataframe.ReadCSV(passengersFile)

// Get the time and passengers as a slice of floats.
passengers := passengersDF.Col("AirPassengers").Float()

// Loop over various values of lag in the series.
fmt.Println("Partial Autocorrelation:")
for i := 1; i &lt; 11; i++ {

    // Calculate the partial autocorrelation.
    pac := pacf(passengers, i)
    fmt.Printf("Lag %d period: %0.2f\n", i, pac)
}</pre>
<p>Compiling and running this gives the following values of <kbd>Partial Autocorrelation</kbd> in our air passengers times series:</p>
<pre><strong>$ go build
$ ./myprogram 
Partial Autocorrelation:</strong><br/><strong>Lag 1 period: 0.96</strong><br/><strong>Lag 2 period: -0.33</strong><br/><strong>Lag 3 period: 0.20</strong><br/><strong>Lag 4 period: 0.15</strong><br/><strong>Lag 5 period: 0.26</strong><br/><strong>Lag 6 period: -0.03</strong><br/><strong>Lag 7 period: 0.20</strong><br/><strong>Lag 8 period: 0.16</strong><br/><strong>Lag 9 period: 0.57</strong><br/><strong>Lag 10 period: 0.29</strong></pre>
<p>As you can see, the partial autocorrelation dies off quickly after about the second lag. This indicates that there is not much remaining relationship-wise in the time series after factoring in the relationships between the time series and its first and second lags. The partial autocorrelation does not go to <em>0.0</em> exactly, but that is expected due to some noise in the data.</p>
<p>To help us better visualize the PACF, let's create another plot. We can do this the exact same way as we did with the ACF, just substituting the <kbd>pacf()</kbd> function for the <kbd>acf()</kbd> function. The resulting plot is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="540" class="image-border" src="assets/ea6ac50a-56db-457f-be30-be7aee2d3c55.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-regressive models for forecasting</h1>
                </header>
            
            <article>
                
<p>The first category of models that we are going to use to try and forecast our time series are called <strong>auto-regressive</strong> (<strong>AR</strong>) models. As already mentioned, we try to model a data point in our time series based on one or more previous points in the series. We are, thus, modeling the time series using the time series itself. This use of the series itself is what distinguishes AR methods from the more general regression methods discussed in <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml">Chapter 4</a>, <em>Regression</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-regressive model overview</h1>
                </header>
            
            <article>
                
<p>You will often see AR models referred to as AR(1), AR(2), and so on. These numbers correspond to the <strong>order</strong> of the AR model or process you are using to model the time series, and it is this order that you can determine by performing autocorrelation and partial autocorrelation analysis.</p>
<p>An AR(1) model attempts to model an observation in your series based on the observation in the same series at a one time period delay:</p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="22" width="93" src="assets/e12f1bd0-9f16-45a3-8ff5-a0047b6eb370.jpg"/></div>
<p>An AR(2) model would look as follows:</p>
<p class="mce-root"/>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="21" width="139" src="assets/1679ea40-6d0e-4b16-8261-29ccbc3edaf6.jpg"/></div>
<p>An AR(3) model would add another term and so on, all following this pattern.</p>
<div class="packt_infobox"><span>These formulas might remind you of linear regression, and we will actually use many of the same methods here that we used when creating linear regression models. However, the unique aspects of time series modeling should not be ignored here. It is important to have an intuition of the time-related elements of your data (seasonality, trends, autocorrelations, and so on) and how they influence the AR models.</span></div>
<p>The order of AR model that you use to model your time series can be determined best by looking at a graph of the PACF. In the graph, you will see the PACF values decay to and then hover around zero. Look at how many lags it takes for the PACF to start hovering around zero, and then utilize an AR order corresponding to that many lags.</p>
<div class="packt_tip">Note that some packages to plot the PACF and ACF include horizontal lines indicating the statistical significance of the various lagged terms. I have not included these here, but if you want to quantitatively determine the order for your AR models, you might consider calculating these as further discussed here:<br/>
<a href="http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm">http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm</a> and<a href="http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm"><br/></a><a href="http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4463.htm">http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4463.htm</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-regressive model assumptions and pitfalls</h1>
                </header>
            
            <article>
                
<p>The main assumptions of auto-regressive models are as follows:</p>
<ul>
<li><strong>Stationarity</strong>: AR models assume that your time series is stationary. We should not see any trends in the data if we plan on using AR models.</li>
<li><strong>Ergodicity</strong>: This fancy term basically means that the statistical properties of the time series, like mean and variance, should not vary or drift over time.</li>
</ul>
<p>Whatever time series that we are modeling with AR methods should meet these assumptions. However, even when some data (like our air passenger data) that does not meet these assumptions, we can play some differencing tricks to still take advantage of AR models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-regressive model example</h1>
                </header>
            
            <article>
                
<p>We will try to model our air passenger data using an auto-regressive model. Now, we already know that we are breaking one of the assumptions of AR models in that our data is not stationary. However, we can apply a common trick to make our series stationary, called <strong>differencing</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming to a stationary series</h1>
                </header>
            
            <article>
                
<p>To make our time series stationary, we will create a proxy time series where the observation at time period <em>t</em> is the observation at time period <em>t</em> from our original time series minus the previous observation. Let's difference each observation in this manner and then plot the results to see if it gets rid of the trends in our data. We are also going to output this differenced time series to a new <kbd>*.csv</kbd> file, as shown in the following code:</p>
<pre>// as slices of floats.
passengerVals := passengersDF.Col("AirPassengers").Float()
timeVals := passengersDF.Col("time").Float()

// pts will hold the values for plotting.
pts := make(plotter.XYs, passengersDF.Nrow()-1)

// differenced will hold our differenced values
// that will be output to a new CSV file.
var differenced [][]string
differenced = append(differenced, []string{"time", "differenced_passengers"})

// Fill pts with data.
for i := 1; i &lt; len(passengerVals); i++ {
    pts[i-1].X = timeVals[i]
    pts[i-1].Y = passengerVals[i] - passengerVals[i-1]
    differenced = append(differenced, []string{
        strconv.FormatFloat(timeVals[i], 'f', -1, 64),
        strconv.FormatFloat(passengerVals[i]-passengerVals[i-1], 'f', -1, 64),
    })<br/>}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "time"
p.Y.Label.Text = "differenced passengers"
p.Add(plotter.NewGrid())

// Add the line plot points for the time series.
l, err := plotter.NewLine(pts)
if err != nil {
    log.Fatal(err)
}
l.LineStyle.Width = vg.Points(1)
l.LineStyle.Color = color.RGBA{B: 255, A: 255}

// Save the plot to a PNG file.
p.Add(l)
if err := p.Save(10*vg.Inch, 4*vg.Inch, "diff_passengers_ts.png"); err != nil {
    log.Fatal(err)
}

// Save the differenced data out to a new CSV.
f, err := os.Create("diff_series.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

w := csv.NewWriter(f)
w.WriteAll(differenced)

if err := w.Error(); err != nil {
    log.Fatal(err)
}</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="200" width="500" class="image-border" src="assets/8be8954a-2996-4c3b-bbff-648fa7c4c7e1.png"/></div>
<p>Here, we can see that we have basically removed all signs of the upward trend that was in the original time series. However, there still appears to be a problem related to variance. The differenced time series appears to have an increasing variance about the mean as time gets larger, which breaks our ergodicity assumption.</p>
<p>To deal with the increase in variance, we can further transform our time series using a log or power transformation that penalizes the larger values later in the time series. Let's add this log transform, replot the log of the differenced series, and then save the resulting data to a file called <kbd>log_diff_series.csv</kbd>. The code to accomplish this is the same as the previous code snippet, except we use <kbd>math.Log()</kbd> to transform each value, so we will spare the details. The following is the resulting plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="213" width="532" class="image-border" src="assets/99844e43-29c8-4f78-95ec-55d733dbad0c.png"/></div>
<p>Awesome! Now we have what looks to be a stationary series that we can utilize in our AR models. Note, we are looking at this qualitatively for the most part in this example, but there are quantitative tests for stationarity (the Dickey–Fuller test, for example).</p>
<div class="packt_tip">We have transformed our data here with both a difference and a log transformation. This allowed us to fit within the assumptions of the AR model, but it also made our data and our eventual model a little less interpretable. It's harder to think about the log of a differenced time series than the time series itself. We had a justification for this trade-off here, but the trade-off should be noted, and the hope is to avoid such obfuscations where we can.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the ACF and choosing an AR order</h1>
                </header>
            
            <article>
                
<p>Now that we have a stationary series that fits within the assumptions of our model, let's revisit our ACF and PACF plots to see what has changed. We can utilize the same code that we used to plot the ACF and PACF previously, but this time we will use our transformed series.</p>
<p>Here are the resulting ACF plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="540" class="image-border" src="assets/8756cb7f-fb4e-4a2b-a9ce-64eb930f59c3.png"/></div>
<div><span>Here are the resulting PACF plot:</span></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="270" width="540" class="image-border" src="assets/0d3896c7-b4e8-402c-b6b9-6dbc589e7f7a.png"/></div>
<p>The first thing we notice is that the ACF plot no longer has a slow decay from <em>1.0</em> as the lags get larger and larger. The ACF plot decays and fluctuates around <em>0.0</em>. We will come back to the ACF in the next section.</p>
<p>Next, we can see that the PACF also decays down to <em>0.0</em> and fluctuates around 0.0 thereafter. To choose the order of our AR model, we want to examine where the PACF plot appears to cross the zero line for the first time. In our case, this appears to be after the second lag period, and thus, we might want to consider using an AR(2) model to model this time series auto-regressively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting and evaluating an AR(2) model</h1>
                </header>
            
            <article>
                
<p>We have already seen that PACF gives us the coefficients for various orders in our AR model. Taking advantage of this, we can get the coefficients for our first and second lag terms along with the intercept (or error term) in our model using a slightly modified version of the <kbd>pacf()</kbd> function shown in the following code:</p>
<pre>// autoregressive calculates an AR model for a series
// at a given order.
func autoregressive(x []float64, lag int) ([]float64, float64) {

    // Create a regresssion.Regression value needed to train
    // a model using github.com/sajari/regression.
    var r regression.Regression
    r.SetObserved("x")

    // Define the current lag and all of the intermediate lags.
    for i := 0; i &lt; lag; i++ {
        r.SetVar(i, "x"+strconv.Itoa(i))
    }

    // Shift the series.
    xAdj := x[lag:len(x)]

    // Loop over the series creating the data set
    // for the regression.
    for i, xVal := range xAdj {

        // Loop over the intermediate lags to build up
        // our independent variables.
        laggedVariables := make([]float64, lag)
        for idx := 1; idx &lt;= lag; idx++ {

            // Get the lagged series variables.
            laggedVariables[idx-1] = x[lag+i-idx]
         }

         // Add these points to the regression value.
         r.Train(regression.DataPoint(xVal, laggedVariables))
    }

    // Fit the regression.
    r.Run()

    // coeff hold the coefficients for our lags.
    var coeff []float64
    for i := 1; i &lt;= lag; i++ {
        coeff = append(coeff, r.Coeff(i))
    }

    return coeff, r.Coeff(0)
}</pre>
<p>We can then call this on our log differenced series to get our trained AR(2) model coefficients:</p>
<pre>// Open the CSV file.<br/>passengersFile, err := os.Open("log_diff_series.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer passengersFile.Close()<br/>        <br/>// Create a dataframe from the CSV file.
passengersDF := dataframe.ReadCSV(passengersFile)

// Get the time and passengers as a slice of floats.
passengers := passengersDF.Col("log_differenced_passengers").Float()

// Calculate the coefficients for lag 1 and 2 and
// our error.
coeffs, intercept := autoregressive(passengers, 2)

// Output the AR(2) model to stdout.
fmt.Printf("\nlog(x(t)) - log(x(t-1)) = %0.6f + lag1*%0.6f + lag2*%0.6f\n\n", intercept, coeffs[0], coeffs[1])</pre>
<p>Compiling and running this training gives us the following AR(2) formula for the log of the differenced passenger counts:</p>
<pre><strong>$ go build
$ ./myprogram 

log(x(t)) - log(x(t-1)) = 0.008159 + lag1*0.234953 + lag2*-0.173682</strong></pre>
<p>To evaluate this AR(2) model, we can calculate the <strong>Mean Absolute Error</strong> (<strong>MAE</strong>), similar to how we calculated it for linear regression models. Specifically, we will compute predicted passenger count values paired with our observed passenger count values, and then we will calculate the error and accumulate the MAE.</p>
<p>First, let's calculate our transformed (log and differenced) predictions:</p>
<pre>// Open the log differenced dataset file.
transFile, err := os.Open("log_diff_series.csv")
if err != nil {
    log.Fatal(err)
}
defer transFile.Close()

// Create a CSV reader reading from the opened file.
transReader := csv.NewReader(transFile)

// Read in all of the CSV records
transReader.FieldsPerRecord = 2
transData, err := transReader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the data predicting the transformed
// observations.
var transPredictions []float64
for i, _ := range transData {

    // Skip the header and the first two observations
    // (because we need two lags to make a prediction).
    if i == 0 || i == 1 || i == 2 {
        continue
    }

    // Parse the first lag.
    lagOne, err := strconv.ParseFloat(transData[i-1][1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the second lag.
    lagTwo, err := strconv.ParseFloat(transData[i-2][1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict the transformed variable with our trained AR model.
    transPredictions = append(transPredictions, 0.008159+0.234953*lagOne-0.173682*lagTwo)
}</pre>
<p>Now, in order to calculate our MAE, we need to transform these predictions back to normal passenger counts (such that we can compare them directly to our original time series). The reverse transform of our log and differenced data involves calculating cumulative sums in the transformed series, adding them back to the base series values, and then taking an exponential. This reverse transform, the accumulation of the MAE, and the aggregation of points to plot our observations and predictions is as follows:</p>
<pre>// Open the original dataset file.
origFile, err := os.Open("AirPassengers.csv")
if err != nil {
    log.Fatal(err)
}
defer origFile.Close()

// Create a CSV reader reading from the opened file.
origReader := csv.NewReader(origFile)

// Read in all of the CSV records
origReader.FieldsPerRecord = 2
origData, err := origReader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// pts* will hold the values for plotting.
ptsObs := make(plotter.XYs, len(transPredictions))
ptsPred := make(plotter.XYs, len(transPredictions))

// Reverse the transformation and calculate the MAE.
var mAE float64
var cumSum float64
for i := 4; i &lt;= len(origData)-1; i++ {

    // Parse the original observation.
    observed, err := strconv.ParseFloat(origData[i][1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the original date.        
    date, err := strconv.ParseFloat(origData[i][0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Get the cumulative sum up to the index in
    // the transformed predictions.
    cumSum += transPredictions[i-4]

    // Calculate the reverse transformed prediction.
    predicted := math.Exp(math.Log(observed) + cumSum)

    // Accumulate the MAE.
    mAE += math.Abs(observed-predicted) / float64(len(transPredictions))

    // Fill in the points for plotting.
    ptsObs[i-4].X = date
    ptsPred[i-4].X = date
    ptsObs[i-4].Y = observed
    ptsPred[i-4].Y = predicted
}</pre>
<p>Then, let's output the MAE to stand out and save a line plot of the observed and predicted values:</p>
<pre>// Output the MAE to standard out.<br/>fmt.Printf("\nMAE = %0.2f\n\n", mAE)<br/><br/>// Create the plot.<br/>p, err := plot.New()<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>p.X.Label.Text = "time"<br/>p.Y.Label.Text = "passengers"<br/>p.Add(plotter.NewGrid())<br/><br/>// Add the line plot points for the time series.<br/>lObs, err := plotter.NewLine(ptsObs)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>lObs.LineStyle.Width = vg.Points(1)<br/><br/>lPred, err := plotter.NewLine(ptsPred)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>lPred.LineStyle.Width = vg.Points(1)<br/>lPred.LineStyle.Dashes = []vg.Length{vg.Points(5), vg.Points(5)}<br/><br/>// Save the plot to a PNG file.<br/>p.Add(lObs, lPred)<br/>p.Legend.Add("Observed", lObs)<br/>p.Legend.Add("Predicted", lPred)<br/>if err := p.Save(10*vg.Inch, 4*vg.Inch, "passengers_ts.png"); err != nil {<br/>    log.Fatal(err)<br/>}</pre>
<p>Compiling this and running it results in the following <kbd>MAE</kbd>:</p>
<pre><strong>$ go build
$ ./myprogram 

MAE = 355.20</strong></pre>
<p>If you remember from our original visualization of this series, the passenger counts range from just above zero to just above 600. Thus, an MAE of approximately 355 is not super great. To get a more complete view of how our predictions and observations line up, however, let's look at the plot generated by the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="208" width="520" class="image-border" src="assets/770fe545-b8a4-4fa3-8517-a582b320c31e.png"/></div>
<p>As you can see, our model overpredicts the number of air passengers, especially as time goes on. The model does exhibit some of the structure that is seen in the original data and it produces a similar trend. Yet, it appears that we may need a slightly more sophisticated model to more realistically represent this series.</p>
<div class="packt_tip">No model is perfect and we have tried a relatively simple time series model here. It's good that we tried to stick with a simple and interpretable model, but our evaluation results would probably motivate us to refactor our model in a real-world scenario. Refactoring is good! It means that we learned something.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-regressive moving averages and other time series models</h1>
                </header>
            
            <article>
                
<p>The model that we tried earlier was a relatively simple pure auto-regressive model. However, we are not stuck with using auto-regression or pure auto-regression alone in our time series models. As with other classes of machine learning models covered in this book, there is a whole zoo of time series techniques, and we cannot cover them all here. However, we did want to mention a few notable techniques that you could explore as you follow up on this material.</p>
<p>Auto-regressive models are often combined with models called <strong>moving average models</strong>. When these are combined, they are often referred to as <strong>auto-regressive moving average</strong> (<strong>ARMA</strong>) or <strong>auto-regressive integrated moving average</strong> (<strong>ARIMA</strong>) models. The moving average part of ARMA/ARIMA models allows you to capture the effects of things like white noise or other error terms in your time series, which would actually improve our AR(2) model for air passengers.</p>
<p>Unfortunately, at the time of writing this content, no out of the box package exists to perform ARIMA in Go. As mentioned earlier, the auto-regressive part is relatively easy, but the moving average fitting is slightly more complicated. This is another great place to jump in with contributions!</p>
<p>There are also time series models that are outside of the realm of ARIMA models. For example, the <strong>Holt-Winters method</strong> attempts to capture seasonality in time series data via a forecast equation and three smoothing equations. There are preliminary implementations of the Holt-Winters method in <kbd>github.com/datastream/holtwinters</kbd> and <kbd>github.com/dgryski/go-holtwinters</kbd>. These likely need to be further maintained and productionized, but they serve as a starting point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Anomaly detection</h1>
                </header>
            
            <article>
                
<p>As mentioned in the introduction to this chapter, we might not always be interested in forecasting a time series. We might want to detect anomalous behavior in a time series. For example, we might want to know when out of the ordinary bursts of traffic come across our network, or we may want an alert when out of the ordinary numbers of users are attempting certain things inside of our application. These events could be tied to security concerns or may just be used to adjust our infrastructure or application settings.</p>
<p>Thankfully, due to Go's history of usage in monitoring and infrastructure, there are a variety of Go-based options to detect anomalies in time series data. This tooling has been used in production to detect anomalous behavior while monitoring infrastructure and applications and, although there are more tools than can be mentioned here, I will highlight a couple.</p>
<p>First, the InfluxDB (<a href="https://www.influxdata.com/">https://www.influxdata.com/</a>) and Prometheus (<a href="https://prometheus.io/">https://prometheus.io/</a>) ecosystems have a variety of options for anomaly detection. Both InfluxDB and Prometheus offer open source, Go-based time series databases and related tooling. They are useful to monitor infrastructure and applications, and they have widespread use both in the Go community and outside of the Go community. For example, if you are interested in using InfluxDB, you can use <kbd>github.com/nathanielc/morgoth</kbd> for anomaly detection.</p>
<p>This package implements the <strong>Lossy Counting Algorithm</strong> (<strong>LCA</strong>). On the Prometheus side, you could utilize a query-based approach as further discussed on <a href="https://prometheus.io/blog/2015/06/18/practical-anomaly-detection/">https://prometheus.io/blog/2015/06/18/practical-anomaly-detection/</a>.</p>
<p>There are a variety of standalone Go packages for anomaly detection as well, including <kbd>github.com/lytics/anomalyzer</kbd> and <kbd>github.com/sec51/goanomaly</kbd>. More specifically, <kbd>github.com/lytics/anomalyzer</kbd> <span>implements a variety of tests to determine if an observation in your series is anomalous, including tests based on the cumulative distribution functions, bootstrap permutations, permuted rank-sums, relative magnitudes, and more.</span></p>
<p>To detect anomalies with <kbd>github.com/lytics/anomalyzer</kbd><span>, we need to create some configurations and an <kbd>anomalyzer.Anomalyzer</kbd> value. Once we have done this, detecting an anomaly is as simple as calling the <kbd>Push()</kbd> method on the <kbd>anomalyzer.Anomalyzer</kbd> value, as shown in the following code:</span></p>
<pre>// Initialize an AnomalyzerConf value with
// configurations such as which anomaly detection
// methods we want to use.
conf := &amp;anomalyzer.AnomalyzerConf{
    Sensitivity: 0.1,
    UpperBound:  5,
    LowerBound:  anomalyzer.NA, // ignore the lower bound
    ActiveSize:  1,
    NSeasons:    4,
    Methods:     []string{"diff", "fence", "highrank", "lowrank", "magnitude"},
}

// Create a time series of periodic observations
// as a slice of floats.  This could come from a
// database or file, as utilized in earlier examples.
ts := []float64{0.1, 0.2, 0.5, 0.12, 0.38, 0.9, 0.74}

// Create a new anomalyzer based on the existing
// time series values and configuration.
anom, err := anomalyzer.NewAnomalyzer(conf, ts)
if err != nil {
    log.Fatal(err)
}

// Supply a new observed value to the Anomalyzer.
// The Anomalyzer will analyze the value in reference
// to pre-existing values in the series and output
// a probability of the value being anomalous.
prob := anom.Push(15.2)
fmt.Printf("Probability of 15.2 being anomalous: %0.2f\n", prob)
<br/>prob = anom.Push(0.43)
fmt.Printf("Probability of 0.33 being anomalous: %0.2f\n", prob)</pre>
<p>Compiling and running this anomaly detection yields the following:</p>
<pre><strong>$ go build
$ ./myprogram
Probability of 15.2 being anomalous: 0.98
Probability of 0.33 being anomalous: 0.80</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Time series statistics (ACF and PACF):</span></p>
<ul>
<li>How to use ACF: <a href="https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/">https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/</a></li>
<li>Identifying the number of AR or MA terms in an ARIMA model: <a href="https://people.duke.edu/~rnau/411arim3.htm">https://people.duke.edu/~rnau/411arim3.htm</a></li>
</ul>
<p>Auto-regressive models:</p>
<ul>
<li>A more mathematical introduction to AR models: <a href="https://onlinecourses.science.psu.edu/stat501/node/358">https://onlinecourses.science.psu.edu/stat501/node/358</a></li>
<li><kbd>github.com/sajari/regression</kbd> docs: <a href="https://godoc.org/github.com/sajari/regression">https://godoc.org/github.com/sajari/regression</a></li>
</ul>
<p>ARMA/ARIMA models:</p>
<ul>
<li>Introduction to ARIMA: <a href="https://people.duke.edu/~rnau/411arim.htm">https://people.duke.edu/~rnau/411arim.htm</a></li>
</ul>
<p>Anomaly detection:</p>
<ul>
<li>InfluxDB: <a href="https://www.influxdata.com/">https://www.influxdata.com/</a></li>
<li>Prometheus: <a href="https://prometheus.io/">https://prometheus.io/</a></li>
<li><kbd>github.com/lytics/anomalyzer</kbd> docs: <a href="https://godoc.org/github.com/lytics/anomalyzer">https://godoc.org/github.com/lytics/anomalyzer</a></li>
<li><kbd>github.com/sec51/goanomaly</kbd> docs: <a href="https://godoc.org/github.com/sec51/goanomaly">https://godoc.org/github.com/sec51/goanomaly</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Well, that was timely! We now know what time series data is, how to represent it in Go, how to make some forecasts, and how to detect anomalies in our time series data. These skills will come in useful anytime you are working with data that is changing with time, whether its data related to stock prices, or monitoring data related to your infrastructure.</p>
<p>In the next chapter, we will level up our Go-based machine learning by looking at a few advanced techniques, including neural networks and deep learning.</p>


            </article>

            
        </section>
    </body></html>