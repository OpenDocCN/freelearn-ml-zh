- en: Chapter 8. Ensemble Diagnostics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 集合诊断
- en: In earlier chapters, ensemble methods were found to be effective. In the previous
    chapter, we looked at scenarios in which ensemble methods increase the overall
    accuracy of a prediction. It has previously been assumed that different base learners
    are independent of each other. However, unless we have a very large sample and
    the base models are learners that use a distinct set of observations, such an
    assumption is very impractical. Even if we had a large enough sample to believe
    that the partitions are nonoverlapping, each base model is built on a different
    partition, and each partition carries with it the same information as any other
    partition. However, it is difficult to test validations such as this, so we need
    to employ various techniques in order to validate the independence of the base
    models on the same dataset. To do this, we will look at various different methods.
    A brief discussion of the need for ensemble diagnostics will kick off this chapter,
    and the importance of diversity in base models will be covered in the next section.
    For the classification problem, the classifiers can be compared with each other.
    We can then further evaluate the similarity and accuracy of the ensemble. Statistical
    tests that achieve this task will be introduced in the third section. Initially,
    a base learner will be compared with another one, and then we will look at all
    the models of the ensemble in a single step.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们发现集合方法非常有效。在前一章中，我们探讨了集合方法如何提高预测的整体准确性的场景。之前一直假设不同的基学习器之间是相互独立的。然而，除非我们有非常大的样本，并且基模型是使用一组不同观察值的学习者，否则这样的假设是非常不切实际的。即使我们有足够大的样本，相信分区是非重叠的，每个基模型都是建立在不同的分区上，每个分区都携带与任何其他分区相同的信息。然而，测试此类验证是困难的，因此我们需要采用各种技术来验证同一数据集上基模型的独立性。为此，我们将探讨各种不同的方法。本章将简要讨论集合诊断的必要性，并在下一节中介绍基模型多样性的重要性。对于分类问题，可以将分类器相互比较。然后我们可以进一步评估集合的相似性和准确性。在第三部分将介绍实现这一任务的统计测试。最初，将比较一个基学习器与另一个基学习器，然后我们将一次性查看集合中的所有模型。
- en: 'The topics that will be covered in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Ensemble diagnostics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合诊断
- en: Ensemble diversity
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合多样性
- en: Pairwise comparison
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配对比较
- en: Interrater agreement
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分者间一致性
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in the chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用以下库：
- en: '`rpart`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: What is ensemble diagnostics?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是集合诊断？
- en: 'The power of ensemble methods was demonstrated in the preceding chapters. An
    ensemble with decision trees forms a homogeneous ensemble, and this was the main
    topic of [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to [Chapter 6](part0045_split_000.html#1AT9A1-2006c10fab20488594398dc4871637ee
    "Chapter 6. Boosting Refinements"), *Boosting Refinements*. In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and [Chapter 7](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "Chapter 7. The General Ensemble Technique"), *The General Ensemble Technique*,
    we had a peek at stacked ensembles. A central assumption in an ensemble is that
    the models are independent of one another. However, this assumption is seldom
    true, and we know that the same data partition is used over and over again. This
    does not mean that ensembling is bad; we have every reason to use the ensembles
    while previewing the concerns in an ensemble application. Consequently, we need
    to see how close the base models are to each other and overall in their predictions.
    If the predictions are close to each other, then we might need those base models
    in the ensemble. Here, we will build logistic regression, Naïve Bayes, SVM, and
    a decision tree for the German credit dataset as the base models. The analysis
    and program is slightly repetitive here as it is carried over from earlier chapters:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '集成方法的力量在前几章中得到了展示。由决策树组成的集成是一个同质集成，这是[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "第3章。Bagging")*Bagging*到[第6章](part0045_split_000.html#1AT9A1-2006c10fab20488594398dc4871637ee
    "第6章。Boosting Refinements")*Boosting Refinements*的主要内容。在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术介绍")*集成技术介绍*和[第7章](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "第7章。通用集成技术")*通用集成技术*中，我们简要介绍了堆叠集成。集成的一个中心假设是模型之间相互独立。然而，这个假设很少成立，我们知道相同的数据分区被反复使用。这并不意味着集成是坏的；我们有充分的理由在使用集成的同时预览集成应用中的担忧。因此，我们需要了解基础模型在预测上的相似程度以及整体上的相似程度。如果预测彼此很接近，那么我们可能需要在集成中使用这些基础模型。在这里，我们将为德国信用数据集构建逻辑回归、朴素贝叶斯、SVM和决策树作为基础模型。分析和程序在这里略有重复，因为它是从早期章节继承过来的： '
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the next section, we will emphasize the need for diversity in ensembling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将强调在集成中多样性的必要性。
- en: Ensemble diversity
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成多样性
- en: In an ensemble, we have many base models—say *L* number of them. For the classification
    problem, we have base models as classifiers. If we have a regression problem,
    we have the base models as learners. Since the diagnostics are performed on the
    training dataset only, we will drop the convention of train and valid partitions.
    For simplicity, during the rest of the discussion, we will assume that we have
    *N* observations. The *L* number of models implies that we have *L* predictions
    for each of the *N* observations, and thus the number of predictions is ![Ensemble
    diversity](img/00327.jpeg). It is in these predictions that we try to find the
    diversity of the ensemble. The diversity of the ensemble is identified depending
    on the type of problem we are dealing with. First, we will take the regression
    problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个集成中，我们有许多基础模型——比如说有*L*个。对于分类问题，我们有基础模型作为分类器。如果我们有一个回归问题，我们有基础模型作为学习器。由于诊断仅在训练数据集上执行，我们将放弃训练和验证分区这一惯例。为了简单起见，在接下来的讨论中，我们将假设我们有*N*个观测值。*L*个模型意味着对于*N*个观测值中的每一个，我们都有*L*个预测，因此预测的数量是![集成多样性](img/00327.jpeg)。我们正是在这些预测中试图找到集成的多样性。集成的多样性取决于我们处理的问题类型。首先，我们将考虑回归问题。
- en: Numeric prediction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值预测
- en: In the case of regression problems, the predicted values of the observations
    can be compared directly with their actual values. We can easily see which base
    models' predictions are closer to the actual value of the observation and which
    are far away from it. If all the predictions are closer to each other, the base
    models are not diverse. In this case, one of the predictions might suffice all
    the same. If the predictions exhibit some variance, combining them by using the
    average might provide stability. In the assessment of the diversity, it is also
    important to know how close the ensemble prediction is to the true observation
    value.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题的情况下，观测值的预测值可以直接与它们的实际值进行比较。我们可以很容易地看到哪个基础模型的预测值更接近观测的实际值，哪个离它更远。如果所有预测值都彼此接近，则基础模型不具有多样性。在这种情况下，任何一个预测可能就足够了。如果预测值表现出一些变化，通过使用平均值组合它们可能提供稳定性。在评估多样性时，了解集成预测与真实观测值有多接近也是非常重要的。
- en: 'Let''s consider a hypothetical scenario in which we have six observations,
    their actual values, three base learners, predictions by the learners, and the
    ensemble prediction. A sample dataset that will help you to understand the intricacies
    of ensemble diversity is given in the following table:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个假设场景，在这个场景中我们有六个观测值，它们的实际值，三个基础学习器，学习器的预测，以及集成预测。以下表格提供了一个示例数据集，有助于您理解集成多样性的复杂性：
- en: '| Observation Number | Actual | E1 | E2 | E3 | EP |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 观测编号 | 实际值 | E1 | E2 | E3 | EP |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 30 | 15 | 20 | 25 | 20 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 30 | 15 | 20 | 25 | 20 |'
- en: '| 2 | 30 | 40 | 50 | 60 | 50 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 30 | 40 | 50 | 60 | 50 |'
- en: '| 3 | 30 | 25 | 30 | 35 | 30 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 30 | 25 | 30 | 35 | 30 |'
- en: '| 4 | 30 | 28 | 30 | 32 | 30 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 30 | 28 | 30 | 32 | 30 |'
- en: '| 5 | 30 | 20 | 30 | 40 | 30 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 30 | 20 | 30 | 40 | 30 |'
- en: '| 6 | 30 | 10 | 15 | 65 | 30 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 30 | 10 | 15 | 65 | 30 |'
- en: 'Table 1: Six observations, three base learners, and the ensemble'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表1：六个观测值、三个基础学习器和集成
- en: 'For ease of comparison, all the observations'' true values are kept at 30 in
    *Table 1*. The ensemble predictions for the six observations/cases range from
    10–65, while the ensemble prediction—the average of the base learner''s prediction—ranges
    from 20–50\. As a first step to understanding the diversity of the ensemble for
    specific observations and the associated predictions, we will visualize the data
    using the following program block:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于比较，*表1*中所有观测值的真实值都保持在30。六个观测/案例的集成预测范围从10到65，而集成预测——基础学习器预测的平均值——范围从20到50。为了理解特定观测值及其相关预测的集成多样性，我们将使用以下程序块可视化数据：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The program''s explanation is here. The first line of code imports the `Diverse_Numeric.csv`
    data from the code bundle folder. The `windows (X11)` function sets up a new graphical
    device in the Windows (Ubuntu) operating system. The `plot` function then sets
    up an empty plot and the axes'' ranges specification is given by `xlim` and `ylim`.
    Each row of data from *Table 1* is embossed using the plot and the `points` function.
    Choosing `pch` needs further clarification. If we were to choose `pch` at, for
    example, `19`, `1`, and `0`, then this means that we are selecting a filled circle,
    a circle, and a square. The three shapes will denote the actual value, the model
    predictions, and the ensemble prediction respectively. The axis command helps
    us to get the labels in the right display. The result of the preceding R code
    block is the following plot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的解释在这里。代码的第一行从代码包文件夹中导入`Diverse_Numeric.csv`数据。`windows (X11)`函数在Windows（Ubuntu）操作系统中设置一个新的图形设备。然后`plot`函数设置一个空白的绘图，并通过`xlim`和`ylim`指定坐标轴的范围。使用绘图和`points`函数将*表1*中的每一行数据突出显示。选择`pch`需要进一步说明。如果我们选择`pch`为，例如，`19`、`1`和`0`，那么这意味着我们选择了填充的圆圈、圆圈和正方形。这三种形状分别表示实际值、模型预测和集成预测。轴命令帮助我们获得正确的标签显示。前面R代码块的结果是以下绘图：
- en: '![Numeric prediction](img/00328.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![数值预测](img/00328.jpeg)'
- en: 'Figure 1: Understanding ensemble diversity for a regression problem'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：理解回归问题中的集成多样性
- en: We have six observations, each labeled as a **Case**. Consider **Case 1** first.
    The filled circle for each observation is the actual value—**30**, in this case—and
    this is the same across the dataset. For this observation, the ensemble prediction
    is **20**. The empty square and the value predicted by the three base models of
    **15**, **20**, and **25** are depicted in the blank circles. The ensemble forecast—the
    average of the base learner's prediction—is **20** and is denoted by a blank square.
    Now, the three values are less spread out, which is interpreted as indicating
    that the ensemble is less diverse for this observation. Consequently, this is
    an example of low diversity. The estimate of **20** is also far from the actual,
    and we can see that this is a poor estimate. Consequently, this is a *low diversity–poor
    estimate* case.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: In the second case of *Table 1*, the three predictions are well spread out and
    have high diversity. However, the ensemble estimate of **50** is too far from
    the actual value of **30**, and we refer to this as a case of *high diversity–poor
    estimate*. **Case 3** and **Case 4** are thus seen as *low diversity–good estimate*
    since the ensemble prediction matches the actual value and the three ensemble
    predictions are close to each other. **Case 5** makes a fine balance between diversity
    and accuracy, and so we can label this as an example of *high diversity–good estimate*.
    The final case has good accuracy, though the diversity is too high to make the
    ensemble prediction any good. You can refer to Kuncheva (2014) for further details
    on the dilemma of diversity–accuracy of ensemble learners.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: We will consider the diversity–accuracy problem for the classification problem
    next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Class prediction
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The previous section looked at the problem of diversity–accuracy for the regression
    problem. In the case of the classification problem, we can clearly mark whether
    or not the prediction of the classifier matches the actual output/label. Furthermore,
    we only have two potential predictions: 0 or 1\. Consequently, we can compare
    how close two classifiers are with respect to each other over all observations.
    For instance, with two possible outcomes for classifier ![Class prediction](img/00329.jpeg)
    and two possible outcomes for ![Class prediction](img/00330.jpeg), we have four
    possible scenarios for a given observation ![Class prediction](img/00331.jpeg):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Class prediction](img/00332.jpeg) predicts the label as 1; ![Class prediction](img/00330.jpeg)
    predicts it as 1'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00333.jpeg) predicts the label as 1; ![Class prediction](img/00330.jpeg)
    predicts it as 0'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00333.jpeg) predicts the label as 0; ![Class prediction](img/00330.jpeg)
    predicts it as 1'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00329.jpeg) predicts the label as 0; ![Class prediction](img/00330.jpeg)
    predicts it as 0'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In scenarios 1 and 4, the two classifiers *agree* with each other, and in 2
    and 3, they *disagree*. If we have *N* observations, each observation that is
    predicted with the two models will fall into one of the four preceding scenarios.
    Before we consider the formal measures of agreement or disagreement of two or
    more models, we will consider two simpler cases in the forthcoming discussion.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在情景1和4中，两个分类器*同意*彼此，而在2和3中，它们*不同意*。如果我们有*N*个观测，每个观测用两个模型预测将落入上述四种情景之一。在我们考虑两个或更多模型的正式一致性或不一致性度量之前，我们将在接下来的讨论中考虑两个更简单的案例。
- en: 'There is a popular saying that if two people agree with each other all the
    time, one of them is not needed. This is similar to the way in which classifiers
    work. Similarly, say that a pair of geese are known to be very loyal; they stick
    with each other, facing problems together. Now, if we have two models that behave
    in the same way as these geese in all observations, then the diversity is lost
    for good. Consequently, in any given ensemble scenario, we need to eliminate the
    pair of geese and keep only one of them. Suppose then that we have a matrix of
    *L* predictions where the column corresponds to the classifier and the row to
    the *N* observations. In this case, we will define a function named `GP`, and
    an abbreviation for the geese pair, which will tell us which classifiers have
    a geese pair classifier agreeing with them across all observations:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个流行的说法，如果两个人总是同意，其中一个人是不必要的。这与分类器工作的方式相似。同样，假设一对鹅被认为是非常忠诚的；它们彼此相伴，共同面对问题。现在，如果我们有两个模型在所有观测中都以与这些鹅相同的方式表现，那么多样性就会永远丧失。因此，在任何给定的集成情景中，我们需要消除鹅对，只保留其中之一。假设我们有一个*L*预测的矩阵，其中列对应分类器，行对应*N*个观测。在这种情况下，我们将定义一个名为`GP`的函数，以及鹅对的缩写，它将告诉我们哪些分类器有一个鹅对分类器在所有观测中与它们一致：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'How does the geese pair `GP` function work? We give a `matrix` of predictions
    as the input to this function, with the columns for the classifiers and the rows
    for the observations. This function first creates a logical matrix of order ![Class
    prediction](img/00334.jpeg)with default logical values as `TRUE`. Since a classifier
    will obviously agree with itself, we accept the default value. Furthermore, since
    a ![Class prediction](img/00333.jpeg) classifier agrees/disagrees with ![Class
    prediction](img/00330.jpeg) in the same way that ![Class prediction](img/00333.jpeg)
    agrees/disagrees with ![Class prediction](img/00333.jpeg), we make use of this
    fact to compute the lower matrix through a symmetrical relationship. In the two
    nested loops, we compare the predictions of a classifier with every other classifier.
    The `ifelse` function checks whether all the predictions of a classifier match
    with another classifier, and if the condition does not hold even for a single
    observation, we say that the two classifiers under consideration are not the geese
    pair, or that they disagree on at least one occasion:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 鹅对`GP`函数是如何工作的？我们向这个函数输入一个`矩阵`预测作为输入，其中列对应分类器，行对应观测。这个函数首先创建一个逻辑矩阵，其阶数为![类别预测](img/00334.jpeg)，默认逻辑值为`TRUE`。由于一个分类器显然会与自己一致，我们接受默认值。此外，由于![类别预测](img/00333.jpeg)分类器与![类别预测](img/00330.jpeg)在相同的方式上同意/不同意，我们利用这一事实通过对称关系计算下矩阵。在两个嵌套循环中，我们比较一个分类器的预测与另一个分类器的预测。`ifelse`函数检查一个分类器的所有预测是否与另一个分类器匹配，如果对于单个观测条件不成立，我们说正在考虑的两个分类器不是鹅对，或者它们至少在某个场合上不同意：
- en: 'Next, the `GP` function is applied to 500 classifiers that are set up for a
    classification problem. The `CART_Dummy` dataset is taken from the `RSADBE` package.
    The `CART_DUMMY` dataset and related problem description can be found in Chapter
    9 of Tattar (2017). We adapt the code and the resulting output from the same source:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将`GP`函数应用于为分类问题设置的500个分类器。`CART_Dummy`数据集来自`RSADBE`包。`CART_DUMMY`数据集和相关问题描述可以在Tattar（2017）的第9章中找到。我们从这个相同的来源改编了代码和结果输出：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As can be seen from the program, we have three variables here: `X1`, `X2`,
    and `Y`. The variable denoted by `Y` is a binary variable—one class is denoted
    by green and another by red. Using the information provided by the `X1` and `X2`
    variables, the goal is to predict the class of `Y`. The red and green color points
    are intermingled, and so a single linear classifier won''t suffice here to separate
    the reds from the greens. However, if we recursively partition the data space
    by `X1` and `X2`, as shown on the right side of the resulting plots, as shown
    in *Figure 2*, the reds and greens look separable. The previous R code block results
    in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如程序所示，我们这里有三个变量：`X1`、`X2`和`Y`。由`Y`表示的变量是一个二元变量——一个类别用绿色表示，另一个用红色表示。使用`X1`和`X2`变量提供的信息，目标是预测`Y`的类别。红色和绿色点交织在一起，因此单一线性分类器不足以在这里将红色和绿色分开。然而，如果我们通过`X1`和`X2`递归地划分数据空间，如图2右侧的最终图所示，那么红色和绿色看起来是可分离的。前面的R代码块产生以下图表：
- en: '![Class prediction](img/00335.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![类别预测](img/00335.jpeg)'
- en: 'Figure 2: A typical classification problem'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个典型的分类问题
- en: 'A random forest with `500` trees is set up for the `CART_DUMMY` dataset. The
    fixed seed ensures that the output here is reproducible on any execution. Using
    the fitted random forest, we next predict the output of all observations using
    the `500` trees. The options of `type="class"` and `predict.all=TRUE` are central
    to this code block. The `GP` function is then applied to the matrix of predictions
    for the `500` trees. Note that the diagonal elements of the `GP` matrix will always
    be `TRUE`. Consequently, if there is any classifier with which it has perfect
    agreement over all observations, the value of that cell will be `TRUE`. If the
    row sum then exceeds the count by 2, we have a geese classifier for that classifier.
    The following code captures the entire computation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为`CART_DUMMY`数据集设置了包含`500`棵树的随机森林。固定的种子确保了在任何执行中输出的可重复性。使用拟合的随机森林，我们接下来使用`500`棵树预测所有观测值的输出。`type="class"`和`predict.all=TRUE`选项是此代码块的核心。然后，将`GP`函数应用于`500`棵树的预测矩阵。请注意，`GP`矩阵的对角元素始终为`TRUE`。因此，如果有任何分类器与所有观测值完全一致，该单元格的值将是`TRUE`。如果行和超过计数2，则该分类器有鹅分类器。以下代码捕获了整个计算过程：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The reader should note that the bold and larger font of 2 in the preceding
    output is not given by R. It has been modified by the software processing the
    text matter. Consequently, we have a lot of classifiers that have a geese classifier
    matching each of their own predictions. Using the which function, we first find
    all the classifier indexes that meet the criteria, and then, by applying the which
    function for the rows of the `CD_GP` matrix, we get the associated geese classifier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应注意的是，前面输出中2的粗体和较大字体不是由R给出的。这是由处理文本内容的软件修改的。因此，我们有很多具有鹅分类器的分类器，它们与各自的预测相匹配。使用`which`函数，我们首先找到所有满足条件的分类器索引，然后，通过应用`which`函数到`CD_GP`矩阵的行，我们得到相关的鹅分类器：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a result of running the preceding code, we are able to identify the geese
    classifier associated with the classifier. We can choose to remove any one member
    of the geese pair. In the next example, we will apply this method to the German
    credit data. The program tries to identify the geese classifier as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们能够识别与分类器关联的鹅分类器。我们可以选择移除鹅对中的任何一个成员。在下一个示例中，我们将应用此方法到德国信用数据。程序尝试以下方式识别鹅分类器：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since none of the classifiers have a corresponding geese classifier, we don't
    have to eliminate any of the trees.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有分类器有对应的鹅分类器，我们不需要消除任何树。
- en: In Kuncheva (2014), page 112, there is a useful metric known as *oracle output*.
    Next, we formally define the quantity. Remember that we have *L* number of classifiers
    and *N* number of observations. The original/actual values of the label are denoted
    by ![Class prediction](img/00336.jpeg). We will denote the ith predicted value
    using the classifier j by ![Class prediction](img/00337.jpeg).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kuncheva (2014)，第112页，有一个有用的度量标准，称为*神谕输出*。接下来，我们正式定义这个量。记住，我们有*L*个分类器和*N*个观测值。标签的原始/实际值用![类别预测](img/00336.jpeg)表示。我们将使用分类器j表示第i个预测值，用![类别预测](img/00337.jpeg)表示。
- en: '**Oracle output**: The oracle output ![Class prediction](img/00338.jpeg) is
    defined as **1** if the predicted value ![Class prediction](img/00339.jpeg) is
    equal to ![Class prediction](img/00340.jpeg); otherwise it is defined as **0**.
    In mathematical terms, the oracle output is given using the following mathematical
    expression:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Oracle输出**：如果预测值![Class prediction](img/00339.jpeg)等于![Class prediction](img/00340.jpeg)，则Oracle输出![Class
    prediction](img/00338.jpeg)被定义为**1**；否则，它被定义为**0**。用数学术语来说，Oracle输出使用以下数学表达式给出：'
- en: '![Class prediction](img/00341.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Class prediction](img/00341.jpeg)'
- en: So, what is the difference between the oracle output and the predictions? The
    predictions consist of the labels of the data, and the labels might be 1/0, GOOD/BAD,
    +1/-1, YES/NO, or some other binary pair of labels. Besides, in the case of a
    binary label, a prediction of 1 does not necessarily mean that the original value
    is 1; it might be 0 as well. The oracle output takes the value of 1 if 1 is predicted
    as 1, or if 0 is predicted as 0; otherwise, it takes the value of 0\. A consequence
    of using the oracle output is that the proportion of 1s for a classifier will
    give us the accuracy of the classifier.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Oracle输出和预测之间有什么区别？预测包括数据的标签，标签可能是1/0、GOOD/BAD、+1/-1、YES/NO或其他二进制标签对。此外，在二进制标签的情况下，预测为1并不一定意味着原始值是1；它也可能是0。如果预测1为1或0为0，Oracle输出取值为1；否则，它取值为0。使用Oracle输出的一个后果是，分类器中1的比例将给我们提供分类器的准确度。
- en: 'We will now create an R function named `Oracle`, which will give the oracle
    output when it is an input for the prediction matrix and the actual labels. After
    this, we will calculate the accuracy of the classifiers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个名为`Oracle`的R函数，该函数在预测矩阵和实际标签作为输入时将给出Oracle输出。之后，我们将计算分类器的准确度：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The oracle matrix helps us in obtaining the accuracy of the classifiers. In
    the next section, we will discuss some measures that will help us in understanding
    how close the classifiers are to each other.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle矩阵帮助我们获得分类器的准确度。在下一节中，我们将讨论一些有助于我们了解分类器之间距离的度量。
- en: Pairwise measure
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配对度量
- en: 'In this section, we will propose some measures of agreement between two classifiers.
    The intention is to fix the notions of agreement/disagreement for two classifiers
    and then take the concept to the overall classifiers of the ensemble in the next
    section. If ![Pairwise measure](img/00333.jpeg) and ![Pairwise measure](img/00330.jpeg)
    are classifier models with predictions ![Pairwise measure](img/00342.jpeg), we
    can then obtain a table that gives us the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提出一些度量两个分类器之间一致性的方法。目的是固定两个分类器之间的一致性/不一致性概念，然后在下一节中将该概念应用到集成分类器的整体分类器中。如果![配对度量](img/00333.jpeg)和![配对度量](img/00330.jpeg)是具有预测![配对度量](img/00342.jpeg)的分类器模型，那么我们可以获得一个表格，它给出以下内容：
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00343.jpeg)
    as 1; ![Pairwise measure](img/00330.jpeg) predicts it as 1'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![配对度量](img/00333.jpeg)预测![配对度量](img/00343.jpeg)为1；![配对度量](img/00330.jpeg)预测它为1'
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 1; ![Pairwise measure](img/00330.jpeg) predicts it as 0'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![配对度量](img/00333.jpeg)预测![配对度量](img/00344.jpeg)为1；![配对度量](img/00330.jpeg)预测它为0'
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 0; ![Pairwise measure](img/00330.jpeg) predicts it as 1'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![配对度量](img/00333.jpeg)预测![配对度量](img/00344.jpeg)为0；![配对度量](img/00330.jpeg)预测它为1'
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 0; ![Pairwise measure](img/00330.jpeg) predicts it as 0'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![配对度量](img/00333.jpeg)预测![配对度量](img/00344.jpeg)为0；![配对度量](img/00330.jpeg)预测它为0'
- en: 'The information across the *N* observations can be put in a tabular form, as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在*N*个观测值中的信息可以以表格形式表示，如下所示：
- en: '|   | M1 predicts 1 | M1 predicts 0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|   | M1预测1 | M1预测0 |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| M2 predicts 1 | n11 | n10 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| M2预测1 | n11 | n10 |'
- en: '| M2 predicts 0 | n01 | n00 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| M2预测0 | n01 | n00 |'
- en: 'Table 2: Contingency table for two classifiers/raters'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：两个分类器/评分者的列联表
- en: The diagonal elements of the preceding table show the agreement between the
    two models/classifiers, while the off-diagonal elements show the disagreement.
    The models are sometimes referred to as *raters*. The frequency table is also
    known as the **contingency table**. Using this setup, we will now discuss some
    useful measures of *agreement*. The comparisons are called pairwise measures as
    we take only a pair of classifiers into analysis.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个表的对角线元素显示两个模型/分类器之间的一致性，而离对角线元素显示不一致性。这些模型有时被称为*评分者*。频率表也被称为**列联表**。使用这种设置，我们现在将讨论一些有用的*一致性*度量。比较被称为成对度量，因为我们只分析了一对分类器。
- en: Disagreement measure
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不一致性度量
- en: 'The disagreement measure between two classifiers/rates is defined according
    to the following formula:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分类器/评分者之间的一致性度量定义为以下公式：
- en: '![Disagreement measure](img/00345.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![不一致性度量](img/00345.jpeg)'
- en: 'We will now define a `DM` function that is given the predictions for the two
    classifiers. The function will first prepare the contingency table for the predictions.
    The calculation of the disagreement measure is then straightforward, and is given
    in the following code block:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义一个`DM`函数，该函数接受两个分类器的预测。该函数首先为预测准备列联表。不一致性度量的计算是直接的，如下代码块所示：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the first section, we had the predictions for the German credit data based
    on the logistic regression model naïve Bayes, SVM, and a classification tree.
    Now we apply the DM function to these predictions and see how much these classifiers
    disagree with each other:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们根据逻辑回归模型、朴素贝叶斯、SVM和分类树对德国信用数据进行了预测。现在我们将DM函数应用于这些预测，看看这些分类器之间有多大的不一致性：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we had four classifiers, there will be 3 + 2 + 1 = 6 pairwise comparisons.
    The naïve Bayes and classification tree have the maximum disagreement, and the
    least disagreement is between the logistic regression and the naïve Bayes classifiers.
    The DM measure can be used to easily obtain the disagreement of two models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有四个分类器，因此将会有3 + 2 + 1 = 6个成对比较。朴素贝叶斯和分类树之间的不一致性最大，而逻辑回归和朴素贝叶斯分类器之间的一致性最小。可以使用DM度量来轻松地获得两个模型的不一致性。
- en: Yule's or Q-statistic
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Yule或Q统计量
- en: 'The Yule''s coefficient is a measure of agreement, and when its value is nearly
    equal to zero, it will give the disagreement between the two raters. The Yule''s
    measure is given using the following formula:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Yule系数是一致性的度量，当其值接近于零时，它将给出两个评分者之间的一致性。Yule度量使用以下公式给出：
- en: '![Yule''s or Q-statistic](img/00346.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Yule的或Q统计量](img/00346.jpeg)'
- en: 'The Q-statistic takes the value in the range of the correlation coefficient—that
    is, ![Yule''s or Q-statistic](img/00347.jpeg). Consequently, if the Q values are
    closer to 1, this means that the two measures nearly always agree with each other,
    while the value closer to -1 means that the two models predict the opposite of
    each other. When the Q values are closer to 0, it means that there is a very weak
    association between the two raters. A `Yule` function is created and applied to
    the different model predictions in the following code block:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Q统计量的值在相关系数的范围内——即![Yule的或Q统计量](img/00347.jpeg)。因此，如果Q值接近于1，这意味着两个度量几乎总是相互一致，而接近于-1的值意味着两个模型预测的是相反的。当Q值接近于0时，这意味着两个评分者之间有非常弱的相关性。以下代码块创建并应用了一个`Yule`函数，用于不同的模型预测：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The agreement between naïve Bayes predictions and the SVM predictions is highest.
    Note that if we take the complement of the disagreement measure and perform it
    easily using the following code, we get a measure of the following agreement:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯预测和SVM预测之间的一致性最高。请注意，如果我们取不一致性度量的补数并使用以下代码轻松执行，我们得到以下一致性的度量：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: However, this analysis says that the highest agreement is between the logistic
    regression and naïve Bayes raters. Consequently, we note that the output and comparisons
    might lead to different conclusions. The correlation coefficient can also be computed
    for two raters; we will cover this next.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这项分析表明，逻辑回归和朴素贝叶斯评分者之间的一致性最高。因此，我们注意到输出和比较可能会导致不同的结论。也可以计算两个评分者之间的相关系数；我们将在下一部分介绍。
- en: Correlation coefficient measure
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关系数度量
- en: 'The correlation coefficient between two numeric variables is very intuitive,
    and it is also a very useful measure of relationship when there is a linear relationship
    between them. If both variables are categorical in nature, then we can still obtain
    the correlation coefficient between them. For two raters, the correlation coefficient
    is calculated using the following formula:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数值变量之间的相关系数非常直观，当它们之间存在线性关系时，它也是一个非常有用的关系度量。如果两个变量在本质上都是分类的，那么我们仍然可以获取它们之间的相关系数。对于两个评分者，相关系数使用以下公式计算：
- en: '![Correlation coefficient measure](img/00348.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![相关系数度量](img/00348.jpeg)'
- en: 'We will define an `SS_Cor` function that will carry out the necessary computations
    and return the correlation coefficient:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个`SS_Cor`函数，它将执行必要的计算并返回相关系数：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The correlation coefficient function is now applied to the predictions, as
    shown in the previous examples:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将相关系数函数应用于预测，如前例所示：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The results show that the logistic and naïve Bayes predictions are in more agreement
    than any other combination. Correlation tests can be applied for inspecting whether
    the predictions of the classifiers are independent of each other.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，逻辑回归和朴素贝叶斯预测比任何其他组合都更一致。相关性测试可以用来检查分类器的预测是否相互独立。
- en: '**Exercise**: Apply the `chisq.test` to check for the independence of the predictions
    of the various classifiers here.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：应用`chisq.test`来检查各种分类器预测的独立性。'
- en: Cohen's statistic
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科亨统计量
- en: 'The Cohen''s statistic first appeared in 1960\. It is based on the probability
    of the two raters agreeing with each other because of chance or coincidence. The
    probability of two raters agreeing with each other is demonstrated as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 科亨统计量首次出现在1960年。它基于两个评分者因偶然或巧合而达成一致的概率。两个评分者达成一致的概率如下所示：
- en: '![Cohen''s statistic](img/00349.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![科亨统计量](img/00349.jpeg)'
- en: 'However, the probability of agreeing randomly or because of chance is found
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随机或偶然达成一致的概率如下所示：
- en: '![Cohen''s statistic](img/00350.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![科亨统计量](img/00350.jpeg)'
- en: 'Using the definition of ![Cohen''s statistic](img/00351.jpeg) and ![Cohen''s
    statistic](img/00352.jpeg), the Cohen''s statistic is defined by the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用![科亨统计量](img/00351.jpeg)和![科亨统计量](img/00352.jpeg)的定义，科亨统计量如下定义：
- en: '![Cohen''s statistic](img/00353.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![科亨统计量](img/00353.jpeg)'
- en: 'The Cohen''s kappa can take negative values as well. If its value is 1, this
    means that the raters agree with each other completely. The value of 0 means that
    the agreement is only by chance, and a negative value means that the agreement
    is less than the expected number by chance. First, the R function `Kappa` is created
    in the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 科亨的Kappa值也可以是负数。如果其值为1，这意味着评分者完全一致。0的值表示一致仅是偶然的，负值表示偶然的一致性低于预期。首先，在以下代码中创建了R函数`Kappa`：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The coding part is a clear implementation of the formulas, and the choice of
    `a`, `b`, `c`, `d`, `theta1`, and `theta2` has been made to make the code easy
    to interpret and follow. Next, we apply the predictions to the German training
    dataset:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 编码部分是公式的清晰实现，对`a`、`b`、`c`、`d`、`theta1`和`theta2`的选择已经做出，以便代码易于理解和遵循。接下来，我们将预测应用于德语训练数据集：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Again, the agreement between the logistic and naïve Bayes predictions is the
    highest. We now move to the final disagreement measure.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，逻辑回归和朴素贝叶斯预测的一致性是最高的。我们现在转向最终的分歧度量。
- en: Double-fault measure
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双误度量
- en: 'In tennis, a double fault refers to when the serve fails. The server has two
    opportunities to get the right serve, and if they do not, the point is conceded
    to the opponent. The double fault measure occurs when both classifiers get the
    wrong prediction:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在网球中，双误指的是发球失败的情况。发球者有两个机会发球正确，如果他们没有做到，则该分被判给对手。双误度量发生在两个分类器都做出错误预测时：
- en: '![Double-fault measure](img/00354.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![双误度量](img/00354.jpeg)'
- en: 'Clearly, we need the DF to be as low as possible and close to 0\. This function
    is easy to interpret, and so this will be left as an exercise for the reader to
    follow. The R function for the double fault measure and its application is given
    in then following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要DF尽可能低，接近0。这个函数易于理解，因此这将被留给读者作为练习来跟随。以下代码给出了双误度量的R函数及其应用：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The reader should identify the best agreement by using the double fault measure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: In the case of multilabels (more than two categories), the extension
    of the metrics discussed in this section becomes cumbersome. Instead, one can
    use the oracle matrix and repeat these metrics. The reader should apply these
    measures to the oracle output.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The methods discussed thus far apply to only one classifier pair. In the next
    section, we will measure the diversity of all classifiers of an ensemble.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Interrating agreement
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple extension of the measures discussed in the previous section on the
    ensemble classifiers is to compute the measures for all possible pairs of the
    ensemble and then simply average over all those values. This task constitutes
    the next exercise.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: For all possible combinations of ensemble pairs, calculate the
    disagreement measure, Yule''s statistic, correlation coefficient, Cohen''s kappa,
    and the double-fault measure. After doing this, obtain the average of the comparisons
    and report them as the ensemble diversity.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will propose alternative measures of diversity and kick-start the discussion
    with the entropy measure. In all discussions in this section, we will use the
    oracle outputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Entropy measure
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may recall that we denote the oracle outputs according to ![Entropy measure](img/00355.jpeg).
    For a particular instance, the ensemble is most diverse if the number of classifiers
    misclassifying it is ![Entropy measure](img/00356.jpeg). This means that ![Entropy
    measure](img/00357.jpeg) of the ![Entropy measure](img/00358.jpeg)s are 0s, and
    the rest of the ![Entropy measure](img/00359.jpeg), ![Entropy measure](img/00360.jpeg)s
    are 1s. The *entropy measure for the ensemble* is then defined by the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Entropy measure](img/00361.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'The value of the entropy measure E is in the unit interval. If the E value
    is closer to 0, this means that there is no diversity in the ensemble, while a
    value close to 1 means that the diversity is at the highest possible level. Given
    the oracle matrix, we can easily calculate the entropy measure as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By applying the `Entropy_Measure` on the ensemble for the German credit data,
    we can see that the entropy measure value is `0.255`. The random forest ensemble
    exhibits diversity as the entropy measure is not closer to 0\. However, it is
    also far away from 1, which implies diversity. However, there are no critical
    values or tests to interpret whether the diversity is too low, or even too high,
    for that matter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Kohavi-Wolpert measure
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Kohavi–Wolpert measure is based on the variance of the prediction as 1s
    or 0s. It is based on a decomposition formula for the error rate of a classifier.
    For the binary problem, or when using oracle input, the variance is the same as
    the Gini index. This is given according to the following formula:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Kohavi-Wolpert measure](img/00362.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'The Kohavi–Wolpert measure is the average of the variance across all observations.
    By using the prediction probability given by the oracle matrix, or as a side product
    of the fitted objects, we can obtain the variance and then average it across the
    observations. An R function is now created and applied to some of the predictions
    obtained for the German credit data, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kohavi-Wolpert度量是所有观察到的方差的平均值。通过使用Oracle矩阵给出的预测概率，或者作为拟合对象的副产品，我们可以获得方差，然后对观察到的方差进行平均。现在创建了一个R函数，并将其应用于德国信用数据的一些预测，如下所示：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Kohavi–Wolpert measure can also be obtained using the oracle output. We
    define a mathematical entity that will count the number of classifiers that correctly
    classify the observation as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Kohavi-Wolpert度量也可以通过Oracle输出获得。我们定义一个数学实体，用来计算正确分类观察到的分类器的数量如下：
- en: '![Kohavi-Wolpert measure](img/00363.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![Kohavi-Wolpert度量](img/00363.jpeg)'
- en: 'The probability of being correctly predicted is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 被正确预测的概率如下：
- en: '![Kohavi-Wolpert measure](img/00364.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![Kohavi-Wolpert度量](img/00364.jpeg)'
- en: 'Using these probabilities, the variance can be obtained as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些概率，我们可以获得方差如下：
- en: '![Kohavi-Wolpert measure](img/00365.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![Kohavi-Wolpert度量](img/00365.jpeg)'
- en: 'This method is implemented in the following code using the `KW_OM` function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过以下代码使用`KW_OM`函数实现：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From this, we can see that the two methods give us the same result. It is also
    clear that we don't have a great deal of diversity following the construction
    of the random forests.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看出，这两种方法给出了相同的结果。很明显，在随机森林构建之后，我们没有看到太多的多样性。
- en: Disagreement measure for ensemble
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成不一致度量
- en: 'The disagreement measure between two classifiers can be defined using the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分类器之间的不一致度量可以定义为以下：
- en: '![Disagreement measure for ensemble](img/00366.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![集成不一致度量](img/00366.jpeg)'
- en: 'The disagreement measure of the ensemble is given by the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 集成的不一致度量如下所示：
- en: '![Disagreement measure for ensemble](img/00367.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![集成不一致度量](img/00367.jpeg)'
- en: 'The Kohavi–Wolpert and disagreement measure are related as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Kohavi-Wolpert度量与不一致度量之间的关系如下：
- en: '![Disagreement measure for ensemble](img/00368.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![集成不一致度量](img/00368.jpeg)'
- en: 'The next R code block delivers the implementation of the Kohavi–Wolper measure
    using the oracle outputs as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个R代码块展示了使用Oracle输出实现Kohavi-Wolper度量的方法如下：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Again, we don't see much diversity displayed across the ensemble. We will now
    move on to looking at the final measure of an ensemble's diversity.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们没有看到在集成中表现出太多的多样性。现在我们将继续探讨集成多样性的最终度量。
- en: Measurement of interrater agreement
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评分者间一致性度量
- en: 'In the introductory discussion of oracle output, we showed how it can be easily
    used to obtain the accuracy of a classifier. The average of the classifier accuracy
    is defined as the average individual classification accuracy, and it is denoted
    by ![Measurement of interrater agreement](img/00369.jpeg). The measurement of
    the interrater agreement is defined by the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在对Oracle输出介绍的讨论中，我们展示了如何轻松地使用它来获取分类器的准确率。分类器准确率的平均值定义为平均个体分类准确率，并用![评分者间一致性度量](img/00369.jpeg)表示。评分者间一致性的度量由以下定义：
- en: '![Measurement of interrater agreement](img/00370.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![评分者间一致性度量](img/00370.jpeg)'
- en: 'This measure is related to the Kohavi–Wolpert measure as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此度量与Kohavi-Wolpert度量相关，如下所示：
- en: '![Measurement of interrater agreement](img/00371.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![评分者间一致性度量](img/00371.jpeg)'
- en: 'The implementation of the preceding relation can be understood with the help
    of the following code block:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下代码块，我们可以理解前面关系的实现：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This concludes our discussion of the agreement in an ensemble.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对集成一致性的讨论。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Ensemble methods have been found to be very effective for classification, regression,
    and other related problems. Any statistical and machine learning method must always
    be followed up with appropriate diagnostics. The assumption that all base models
    are independent of each other is central to the success of an ensembling method.
    However, this independence condition is rarely satisfied, especially because the
    base models are built on the same dataset. We kicked off the chapter with the
    simplest measure: the geese pair method. With this, we essentially searched for
    the models that agree with each other at all times. If such models are present
    in the ensemble, it is safer to remove one of them. With a large dataset and a
    high number of variables, it is indeed possible that there won''t be any base
    models that speak the same language as another. However, we still need to check
    whether they are equal. With this in mind, we first proposed measures that compare
    only two base models at a time. Different measures can lead to conflicting conclusions.
    However, this is generally not a problem. The concept of pairwise comparison was
    then extended to entire ensemble base models. While we found that our base models
    were not too diverse, it is also important to note here that most of the values
    are a safe distance away from the boundary value of 0\. When we are performing
    the diagnostics on an ensemble and find that the values are equal to zero, it
    is then clear that the base models are not offering any kind of diversity. In
    the next chapter, we will look at the specialized topic of regression data.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法被发现对于分类、回归和其他相关问题是十分有效的。任何统计和机器学习方法都必须始终伴随着适当的诊断。所有基础模型之间相互独立这一假设是集成方法成功的关键。然而，这种独立性条件很少得到满足，尤其是因为基础模型是建立在相同的数据集之上的。我们以最简单的度量方法——鹅对法——开始了这一章节。通过这种方法，我们实际上是在寻找在所有时刻都达成一致的模型。如果这样的模型存在于集成中，那么移除其中之一会更安全。在拥有大量数据集和高数量变量的情况下，确实可能没有任何基础模型与其他模型使用相同的语言。然而，我们仍然需要检查它们是否相等。考虑到这一点，我们首先提出了仅比较两个基础模型的措施。不同的措施可能导致相互矛盾的结论。然而，这通常并不是问题。随后，我们将成对比较的概念扩展到了整个集成基础模型。虽然我们发现我们的基础模型并不太多样化，但在此也要注意，大多数值都远离边界值0。当我们对集成进行诊断并发现值等于零时，那么很明显，基础模型并没有提供任何形式的多样性。在下一章中，我们将探讨回归数据的专门主题。
