- en: Chapter 8. Ensemble Diagnostics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, ensemble methods were found to be effective. In the previous
    chapter, we looked at scenarios in which ensemble methods increase the overall
    accuracy of a prediction. It has previously been assumed that different base learners
    are independent of each other. However, unless we have a very large sample and
    the base models are learners that use a distinct set of observations, such an
    assumption is very impractical. Even if we had a large enough sample to believe
    that the partitions are nonoverlapping, each base model is built on a different
    partition, and each partition carries with it the same information as any other
    partition. However, it is difficult to test validations such as this, so we need
    to employ various techniques in order to validate the independence of the base
    models on the same dataset. To do this, we will look at various different methods.
    A brief discussion of the need for ensemble diagnostics will kick off this chapter,
    and the importance of diversity in base models will be covered in the next section.
    For the classification problem, the classifiers can be compared with each other.
    We can then further evaluate the similarity and accuracy of the ensemble. Statistical
    tests that achieve this task will be introduced in the third section. Initially,
    a base learner will be compared with another one, and then we will look at all
    the models of the ensemble in a single step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble diagnostics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble diversity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pairwise comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrater agreement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following libraries in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ensemble diagnostics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The power of ensemble methods was demonstrated in the preceding chapters. An
    ensemble with decision trees forms a homogeneous ensemble, and this was the main
    topic of [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to [Chapter 6](part0045_split_000.html#1AT9A1-2006c10fab20488594398dc4871637ee
    "Chapter 6. Boosting Refinements"), *Boosting Refinements*. In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and [Chapter 7](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "Chapter 7. The General Ensemble Technique"), *The General Ensemble Technique*,
    we had a peek at stacked ensembles. A central assumption in an ensemble is that
    the models are independent of one another. However, this assumption is seldom
    true, and we know that the same data partition is used over and over again. This
    does not mean that ensembling is bad; we have every reason to use the ensembles
    while previewing the concerns in an ensemble application. Consequently, we need
    to see how close the base models are to each other and overall in their predictions.
    If the predictions are close to each other, then we might need those base models
    in the ensemble. Here, we will build logistic regression, Naïve Bayes, SVM, and
    a decision tree for the German credit dataset as the base models. The analysis
    and program is slightly repetitive here as it is carried over from earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will emphasize the need for diversity in ensembling.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble diversity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an ensemble, we have many base models—say *L* number of them. For the classification
    problem, we have base models as classifiers. If we have a regression problem,
    we have the base models as learners. Since the diagnostics are performed on the
    training dataset only, we will drop the convention of train and valid partitions.
    For simplicity, during the rest of the discussion, we will assume that we have
    *N* observations. The *L* number of models implies that we have *L* predictions
    for each of the *N* observations, and thus the number of predictions is ![Ensemble
    diversity](img/00327.jpeg). It is in these predictions that we try to find the
    diversity of the ensemble. The diversity of the ensemble is identified depending
    on the type of problem we are dealing with. First, we will take the regression
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Numeric prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of regression problems, the predicted values of the observations
    can be compared directly with their actual values. We can easily see which base
    models' predictions are closer to the actual value of the observation and which
    are far away from it. If all the predictions are closer to each other, the base
    models are not diverse. In this case, one of the predictions might suffice all
    the same. If the predictions exhibit some variance, combining them by using the
    average might provide stability. In the assessment of the diversity, it is also
    important to know how close the ensemble prediction is to the true observation
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a hypothetical scenario in which we have six observations,
    their actual values, three base learners, predictions by the learners, and the
    ensemble prediction. A sample dataset that will help you to understand the intricacies
    of ensemble diversity is given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Observation Number | Actual | E1 | E2 | E3 | EP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 30 | 15 | 20 | 25 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 30 | 40 | 50 | 60 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 30 | 25 | 30 | 35 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 30 | 28 | 30 | 32 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 30 | 20 | 30 | 40 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 30 | 10 | 15 | 65 | 30 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Six observations, three base learners, and the ensemble'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For ease of comparison, all the observations'' true values are kept at 30 in
    *Table 1*. The ensemble predictions for the six observations/cases range from
    10–65, while the ensemble prediction—the average of the base learner''s prediction—ranges
    from 20–50\. As a first step to understanding the diversity of the ensemble for
    specific observations and the associated predictions, we will visualize the data
    using the following program block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The program''s explanation is here. The first line of code imports the `Diverse_Numeric.csv`
    data from the code bundle folder. The `windows (X11)` function sets up a new graphical
    device in the Windows (Ubuntu) operating system. The `plot` function then sets
    up an empty plot and the axes'' ranges specification is given by `xlim` and `ylim`.
    Each row of data from *Table 1* is embossed using the plot and the `points` function.
    Choosing `pch` needs further clarification. If we were to choose `pch` at, for
    example, `19`, `1`, and `0`, then this means that we are selecting a filled circle,
    a circle, and a square. The three shapes will denote the actual value, the model
    predictions, and the ensemble prediction respectively. The axis command helps
    us to get the labels in the right display. The result of the preceding R code
    block is the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Numeric prediction](img/00328.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Understanding ensemble diversity for a regression problem'
  prefs: []
  type: TYPE_NORMAL
- en: We have six observations, each labeled as a **Case**. Consider **Case 1** first.
    The filled circle for each observation is the actual value—**30**, in this case—and
    this is the same across the dataset. For this observation, the ensemble prediction
    is **20**. The empty square and the value predicted by the three base models of
    **15**, **20**, and **25** are depicted in the blank circles. The ensemble forecast—the
    average of the base learner's prediction—is **20** and is denoted by a blank square.
    Now, the three values are less spread out, which is interpreted as indicating
    that the ensemble is less diverse for this observation. Consequently, this is
    an example of low diversity. The estimate of **20** is also far from the actual,
    and we can see that this is a poor estimate. Consequently, this is a *low diversity–poor
    estimate* case.
  prefs: []
  type: TYPE_NORMAL
- en: In the second case of *Table 1*, the three predictions are well spread out and
    have high diversity. However, the ensemble estimate of **50** is too far from
    the actual value of **30**, and we refer to this as a case of *high diversity–poor
    estimate*. **Case 3** and **Case 4** are thus seen as *low diversity–good estimate*
    since the ensemble prediction matches the actual value and the three ensemble
    predictions are close to each other. **Case 5** makes a fine balance between diversity
    and accuracy, and so we can label this as an example of *high diversity–good estimate*.
    The final case has good accuracy, though the diversity is too high to make the
    ensemble prediction any good. You can refer to Kuncheva (2014) for further details
    on the dilemma of diversity–accuracy of ensemble learners.
  prefs: []
  type: TYPE_NORMAL
- en: We will consider the diversity–accuracy problem for the classification problem
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Class prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The previous section looked at the problem of diversity–accuracy for the regression
    problem. In the case of the classification problem, we can clearly mark whether
    or not the prediction of the classifier matches the actual output/label. Furthermore,
    we only have two potential predictions: 0 or 1\. Consequently, we can compare
    how close two classifiers are with respect to each other over all observations.
    For instance, with two possible outcomes for classifier ![Class prediction](img/00329.jpeg)
    and two possible outcomes for ![Class prediction](img/00330.jpeg), we have four
    possible scenarios for a given observation ![Class prediction](img/00331.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Class prediction](img/00332.jpeg) predicts the label as 1; ![Class prediction](img/00330.jpeg)
    predicts it as 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00333.jpeg) predicts the label as 1; ![Class prediction](img/00330.jpeg)
    predicts it as 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00333.jpeg) predicts the label as 0; ![Class prediction](img/00330.jpeg)
    predicts it as 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Class prediction](img/00329.jpeg) predicts the label as 0; ![Class prediction](img/00330.jpeg)
    predicts it as 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In scenarios 1 and 4, the two classifiers *agree* with each other, and in 2
    and 3, they *disagree*. If we have *N* observations, each observation that is
    predicted with the two models will fall into one of the four preceding scenarios.
    Before we consider the formal measures of agreement or disagreement of two or
    more models, we will consider two simpler cases in the forthcoming discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a popular saying that if two people agree with each other all the
    time, one of them is not needed. This is similar to the way in which classifiers
    work. Similarly, say that a pair of geese are known to be very loyal; they stick
    with each other, facing problems together. Now, if we have two models that behave
    in the same way as these geese in all observations, then the diversity is lost
    for good. Consequently, in any given ensemble scenario, we need to eliminate the
    pair of geese and keep only one of them. Suppose then that we have a matrix of
    *L* predictions where the column corresponds to the classifier and the row to
    the *N* observations. In this case, we will define a function named `GP`, and
    an abbreviation for the geese pair, which will tell us which classifiers have
    a geese pair classifier agreeing with them across all observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'How does the geese pair `GP` function work? We give a `matrix` of predictions
    as the input to this function, with the columns for the classifiers and the rows
    for the observations. This function first creates a logical matrix of order ![Class
    prediction](img/00334.jpeg)with default logical values as `TRUE`. Since a classifier
    will obviously agree with itself, we accept the default value. Furthermore, since
    a ![Class prediction](img/00333.jpeg) classifier agrees/disagrees with ![Class
    prediction](img/00330.jpeg) in the same way that ![Class prediction](img/00333.jpeg)
    agrees/disagrees with ![Class prediction](img/00333.jpeg), we make use of this
    fact to compute the lower matrix through a symmetrical relationship. In the two
    nested loops, we compare the predictions of a classifier with every other classifier.
    The `ifelse` function checks whether all the predictions of a classifier match
    with another classifier, and if the condition does not hold even for a single
    observation, we say that the two classifiers under consideration are not the geese
    pair, or that they disagree on at least one occasion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the `GP` function is applied to 500 classifiers that are set up for a
    classification problem. The `CART_Dummy` dataset is taken from the `RSADBE` package.
    The `CART_DUMMY` dataset and related problem description can be found in Chapter
    9 of Tattar (2017). We adapt the code and the resulting output from the same source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen from the program, we have three variables here: `X1`, `X2`,
    and `Y`. The variable denoted by `Y` is a binary variable—one class is denoted
    by green and another by red. Using the information provided by the `X1` and `X2`
    variables, the goal is to predict the class of `Y`. The red and green color points
    are intermingled, and so a single linear classifier won''t suffice here to separate
    the reds from the greens. However, if we recursively partition the data space
    by `X1` and `X2`, as shown on the right side of the resulting plots, as shown
    in *Figure 2*, the reds and greens look separable. The previous R code block results
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Class prediction](img/00335.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A typical classification problem'
  prefs: []
  type: TYPE_NORMAL
- en: 'A random forest with `500` trees is set up for the `CART_DUMMY` dataset. The
    fixed seed ensures that the output here is reproducible on any execution. Using
    the fitted random forest, we next predict the output of all observations using
    the `500` trees. The options of `type="class"` and `predict.all=TRUE` are central
    to this code block. The `GP` function is then applied to the matrix of predictions
    for the `500` trees. Note that the diagonal elements of the `GP` matrix will always
    be `TRUE`. Consequently, if there is any classifier with which it has perfect
    agreement over all observations, the value of that cell will be `TRUE`. If the
    row sum then exceeds the count by 2, we have a geese classifier for that classifier.
    The following code captures the entire computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The reader should note that the bold and larger font of 2 in the preceding
    output is not given by R. It has been modified by the software processing the
    text matter. Consequently, we have a lot of classifiers that have a geese classifier
    matching each of their own predictions. Using the which function, we first find
    all the classifier indexes that meet the criteria, and then, by applying the which
    function for the rows of the `CD_GP` matrix, we get the associated geese classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of running the preceding code, we are able to identify the geese
    classifier associated with the classifier. We can choose to remove any one member
    of the geese pair. In the next example, we will apply this method to the German
    credit data. The program tries to identify the geese classifier as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Since none of the classifiers have a corresponding geese classifier, we don't
    have to eliminate any of the trees.
  prefs: []
  type: TYPE_NORMAL
- en: In Kuncheva (2014), page 112, there is a useful metric known as *oracle output*.
    Next, we formally define the quantity. Remember that we have *L* number of classifiers
    and *N* number of observations. The original/actual values of the label are denoted
    by ![Class prediction](img/00336.jpeg). We will denote the ith predicted value
    using the classifier j by ![Class prediction](img/00337.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Oracle output**: The oracle output ![Class prediction](img/00338.jpeg) is
    defined as **1** if the predicted value ![Class prediction](img/00339.jpeg) is
    equal to ![Class prediction](img/00340.jpeg); otherwise it is defined as **0**.
    In mathematical terms, the oracle output is given using the following mathematical
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Class prediction](img/00341.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, what is the difference between the oracle output and the predictions? The
    predictions consist of the labels of the data, and the labels might be 1/0, GOOD/BAD,
    +1/-1, YES/NO, or some other binary pair of labels. Besides, in the case of a
    binary label, a prediction of 1 does not necessarily mean that the original value
    is 1; it might be 0 as well. The oracle output takes the value of 1 if 1 is predicted
    as 1, or if 0 is predicted as 0; otherwise, it takes the value of 0\. A consequence
    of using the oracle output is that the proportion of 1s for a classifier will
    give us the accuracy of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create an R function named `Oracle`, which will give the oracle
    output when it is an input for the prediction matrix and the actual labels. After
    this, we will calculate the accuracy of the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The oracle matrix helps us in obtaining the accuracy of the classifiers. In
    the next section, we will discuss some measures that will help us in understanding
    how close the classifiers are to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will propose some measures of agreement between two classifiers.
    The intention is to fix the notions of agreement/disagreement for two classifiers
    and then take the concept to the overall classifiers of the ensemble in the next
    section. If ![Pairwise measure](img/00333.jpeg) and ![Pairwise measure](img/00330.jpeg)
    are classifier models with predictions ![Pairwise measure](img/00342.jpeg), we
    can then obtain a table that gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00343.jpeg)
    as 1; ![Pairwise measure](img/00330.jpeg) predicts it as 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 1; ![Pairwise measure](img/00330.jpeg) predicts it as 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 0; ![Pairwise measure](img/00330.jpeg) predicts it as 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Pairwise measure](img/00333.jpeg) predicts ![Pairwise measure](img/00344.jpeg)
    as 0; ![Pairwise measure](img/00330.jpeg) predicts it as 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The information across the *N* observations can be put in a tabular form, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | M1 predicts 1 | M1 predicts 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| M2 predicts 1 | n11 | n10 |'
  prefs: []
  type: TYPE_TB
- en: '| M2 predicts 0 | n01 | n00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Contingency table for two classifiers/raters'
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal elements of the preceding table show the agreement between the
    two models/classifiers, while the off-diagonal elements show the disagreement.
    The models are sometimes referred to as *raters*. The frequency table is also
    known as the **contingency table**. Using this setup, we will now discuss some
    useful measures of *agreement*. The comparisons are called pairwise measures as
    we take only a pair of classifiers into analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Disagreement measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The disagreement measure between two classifiers/rates is defined according
    to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Disagreement measure](img/00345.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now define a `DM` function that is given the predictions for the two
    classifiers. The function will first prepare the contingency table for the predictions.
    The calculation of the disagreement measure is then straightforward, and is given
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first section, we had the predictions for the German credit data based
    on the logistic regression model naïve Bayes, SVM, and a classification tree.
    Now we apply the DM function to these predictions and see how much these classifiers
    disagree with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since we had four classifiers, there will be 3 + 2 + 1 = 6 pairwise comparisons.
    The naïve Bayes and classification tree have the maximum disagreement, and the
    least disagreement is between the logistic regression and the naïve Bayes classifiers.
    The DM measure can be used to easily obtain the disagreement of two models.
  prefs: []
  type: TYPE_NORMAL
- en: Yule's or Q-statistic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Yule''s coefficient is a measure of agreement, and when its value is nearly
    equal to zero, it will give the disagreement between the two raters. The Yule''s
    measure is given using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yule''s or Q-statistic](img/00346.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Q-statistic takes the value in the range of the correlation coefficient—that
    is, ![Yule''s or Q-statistic](img/00347.jpeg). Consequently, if the Q values are
    closer to 1, this means that the two measures nearly always agree with each other,
    while the value closer to -1 means that the two models predict the opposite of
    each other. When the Q values are closer to 0, it means that there is a very weak
    association between the two raters. A `Yule` function is created and applied to
    the different model predictions in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The agreement between naïve Bayes predictions and the SVM predictions is highest.
    Note that if we take the complement of the disagreement measure and perform it
    easily using the following code, we get a measure of the following agreement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: However, this analysis says that the highest agreement is between the logistic
    regression and naïve Bayes raters. Consequently, we note that the output and comparisons
    might lead to different conclusions. The correlation coefficient can also be computed
    for two raters; we will cover this next.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficient measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The correlation coefficient between two numeric variables is very intuitive,
    and it is also a very useful measure of relationship when there is a linear relationship
    between them. If both variables are categorical in nature, then we can still obtain
    the correlation coefficient between them. For two raters, the correlation coefficient
    is calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation coefficient measure](img/00348.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will define an `SS_Cor` function that will carry out the necessary computations
    and return the correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The correlation coefficient function is now applied to the predictions, as
    shown in the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The results show that the logistic and naïve Bayes predictions are in more agreement
    than any other combination. Correlation tests can be applied for inspecting whether
    the predictions of the classifiers are independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Apply the `chisq.test` to check for the independence of the predictions
    of the various classifiers here.'
  prefs: []
  type: TYPE_NORMAL
- en: Cohen's statistic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Cohen''s statistic first appeared in 1960\. It is based on the probability
    of the two raters agreeing with each other because of chance or coincidence. The
    probability of two raters agreeing with each other is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cohen''s statistic](img/00349.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the probability of agreeing randomly or because of chance is found
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cohen''s statistic](img/00350.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the definition of ![Cohen''s statistic](img/00351.jpeg) and ![Cohen''s
    statistic](img/00352.jpeg), the Cohen''s statistic is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cohen''s statistic](img/00353.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Cohen''s kappa can take negative values as well. If its value is 1, this
    means that the raters agree with each other completely. The value of 0 means that
    the agreement is only by chance, and a negative value means that the agreement
    is less than the expected number by chance. First, the R function `Kappa` is created
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The coding part is a clear implementation of the formulas, and the choice of
    `a`, `b`, `c`, `d`, `theta1`, and `theta2` has been made to make the code easy
    to interpret and follow. Next, we apply the predictions to the German training
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Again, the agreement between the logistic and naïve Bayes predictions is the
    highest. We now move to the final disagreement measure.
  prefs: []
  type: TYPE_NORMAL
- en: Double-fault measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In tennis, a double fault refers to when the serve fails. The server has two
    opportunities to get the right serve, and if they do not, the point is conceded
    to the opponent. The double fault measure occurs when both classifiers get the
    wrong prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Double-fault measure](img/00354.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, we need the DF to be as low as possible and close to 0\. This function
    is easy to interpret, and so this will be left as an exercise for the reader to
    follow. The R function for the double fault measure and its application is given
    in then following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The reader should identify the best agreement by using the double fault measure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: In the case of multilabels (more than two categories), the extension
    of the metrics discussed in this section becomes cumbersome. Instead, one can
    use the oracle matrix and repeat these metrics. The reader should apply these
    measures to the oracle output.'
  prefs: []
  type: TYPE_NORMAL
- en: The methods discussed thus far apply to only one classifier pair. In the next
    section, we will measure the diversity of all classifiers of an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Interrating agreement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple extension of the measures discussed in the previous section on the
    ensemble classifiers is to compute the measures for all possible pairs of the
    ensemble and then simply average over all those values. This task constitutes
    the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: For all possible combinations of ensemble pairs, calculate the
    disagreement measure, Yule''s statistic, correlation coefficient, Cohen''s kappa,
    and the double-fault measure. After doing this, obtain the average of the comparisons
    and report them as the ensemble diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will propose alternative measures of diversity and kick-start the discussion
    with the entropy measure. In all discussions in this section, we will use the
    oracle outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may recall that we denote the oracle outputs according to ![Entropy measure](img/00355.jpeg).
    For a particular instance, the ensemble is most diverse if the number of classifiers
    misclassifying it is ![Entropy measure](img/00356.jpeg). This means that ![Entropy
    measure](img/00357.jpeg) of the ![Entropy measure](img/00358.jpeg)s are 0s, and
    the rest of the ![Entropy measure](img/00359.jpeg), ![Entropy measure](img/00360.jpeg)s
    are 1s. The *entropy measure for the ensemble* is then defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Entropy measure](img/00361.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The value of the entropy measure E is in the unit interval. If the E value
    is closer to 0, this means that there is no diversity in the ensemble, while a
    value close to 1 means that the diversity is at the highest possible level. Given
    the oracle matrix, we can easily calculate the entropy measure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By applying the `Entropy_Measure` on the ensemble for the German credit data,
    we can see that the entropy measure value is `0.255`. The random forest ensemble
    exhibits diversity as the entropy measure is not closer to 0\. However, it is
    also far away from 1, which implies diversity. However, there are no critical
    values or tests to interpret whether the diversity is too low, or even too high,
    for that matter.
  prefs: []
  type: TYPE_NORMAL
- en: Kohavi-Wolpert measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Kohavi–Wolpert measure is based on the variance of the prediction as 1s
    or 0s. It is based on a decomposition formula for the error rate of a classifier.
    For the binary problem, or when using oracle input, the variance is the same as
    the Gini index. This is given according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kohavi-Wolpert measure](img/00362.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Kohavi–Wolpert measure is the average of the variance across all observations.
    By using the prediction probability given by the oracle matrix, or as a side product
    of the fitted objects, we can obtain the variance and then average it across the
    observations. An R function is now created and applied to some of the predictions
    obtained for the German credit data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kohavi–Wolpert measure can also be obtained using the oracle output. We
    define a mathematical entity that will count the number of classifiers that correctly
    classify the observation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kohavi-Wolpert measure](img/00363.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability of being correctly predicted is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kohavi-Wolpert measure](img/00364.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these probabilities, the variance can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kohavi-Wolpert measure](img/00365.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This method is implemented in the following code using the `KW_OM` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: From this, we can see that the two methods give us the same result. It is also
    clear that we don't have a great deal of diversity following the construction
    of the random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Disagreement measure for ensemble
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The disagreement measure between two classifiers can be defined using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Disagreement measure for ensemble](img/00366.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The disagreement measure of the ensemble is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Disagreement measure for ensemble](img/00367.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Kohavi–Wolpert and disagreement measure are related as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Disagreement measure for ensemble](img/00368.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next R code block delivers the implementation of the Kohavi–Wolper measure
    using the oracle outputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Again, we don't see much diversity displayed across the ensemble. We will now
    move on to looking at the final measure of an ensemble's diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Measurement of interrater agreement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the introductory discussion of oracle output, we showed how it can be easily
    used to obtain the accuracy of a classifier. The average of the classifier accuracy
    is defined as the average individual classification accuracy, and it is denoted
    by ![Measurement of interrater agreement](img/00369.jpeg). The measurement of
    the interrater agreement is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measurement of interrater agreement](img/00370.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This measure is related to the Kohavi–Wolpert measure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measurement of interrater agreement](img/00371.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The implementation of the preceding relation can be understood with the help
    of the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our discussion of the agreement in an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ensemble methods have been found to be very effective for classification, regression,
    and other related problems. Any statistical and machine learning method must always
    be followed up with appropriate diagnostics. The assumption that all base models
    are independent of each other is central to the success of an ensembling method.
    However, this independence condition is rarely satisfied, especially because the
    base models are built on the same dataset. We kicked off the chapter with the
    simplest measure: the geese pair method. With this, we essentially searched for
    the models that agree with each other at all times. If such models are present
    in the ensemble, it is safer to remove one of them. With a large dataset and a
    high number of variables, it is indeed possible that there won''t be any base
    models that speak the same language as another. However, we still need to check
    whether they are equal. With this in mind, we first proposed measures that compare
    only two base models at a time. Different measures can lead to conflicting conclusions.
    However, this is generally not a problem. The concept of pairwise comparison was
    then extended to entire ensemble base models. While we found that our base models
    were not too diverse, it is also important to note here that most of the values
    are a safe distance away from the boundary value of 0\. When we are performing
    the diagnostics on an ensemble and find that the values are equal to zero, it
    is then clear that the base models are not offering any kind of diversity. In
    the next chapter, we will look at the specialized topic of regression data.'
  prefs: []
  type: TYPE_NORMAL
