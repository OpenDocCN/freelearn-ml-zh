- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering Analysis and Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering techniques aim to uncover concealed patterns or groupings within
    a dataset. These algorithms detect groupings without relying on any predefined
    labels. Instead, they select clusters based on the similarity between elements.
    Dimensionality reduction, on the other hand, involves transforming a dataset with
    numerous variables into one with fewer dimensions while preserving relevant information.
    Feature selection methods attempt to identify a subset of the original variables,
    while feature extraction reduces data dimensionality by transforming it into new
    features. This chapter shows us how to divide data into clusters, or groupings
    of similar items. We’ll also learn how to select features that best represent
    the set of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering – basic concepts and methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning-based clustering algorithms with MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping data using the similarity measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering dimensionality reduction techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection and feature extraction using MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. You will also need a working knowledge of the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Minerals.xls`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PeripheralLocations.xls`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YachtHydrodynamics.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SeedsDataset.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding clustering – basic concepts and methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clustering** is a fundamental concept in data analysis, aiming to identify
    meaningful groupings or patterns within a dataset. It involves the partitioning
    of data points into distinct clusters based on their similarity or proximity to
    each other. In both clustering and classification, our goal is to discover the
    underlying rules that enable us to assign observations to the correct class. However,
    clustering differs from classification as it requires identifying a meaningful
    subdivision of classes as well. In classification, we benefit from the target
    variable, which provides the classification information in the training set. In
    contrast, clustering lacks such additional information, necessitating the deduction
    of classes by analyzing the spatial distribution of the data. Dense areas in the
    data correspond to groups of similar observations. If we can identify observations
    that are like each other but distinct from those in another cluster, we can infer
    that these two clusters represent different conditions. At this stage, three crucial
    aspects come into play:'
  prefs: []
  type: TYPE_NORMAL
- en: How to measure similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to find centroids and centers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define a grouping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notion of distance and the definition of a grouping are the fundamental
    components that characterize a clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to measure similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering involves the identification of data groupings based on the measure
    of proximity between elements. Proximity can refer to either similarity or dissimilarity.
    Thus, the definition of a data group relies on how we define similarity or dissimilarity.
    In many approaches, proximity is conceptualized in terms of distance within a
    multidimensional space. The effectiveness of clustering algorithms heavily depends
    on the choice of metric and how distance is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms group elements based on their mutual distances, where
    membership in a particular set is determined by the proximity of an element to
    other members of the same set. Therefore, a set of observations forms a cluster
    when they tend to be closer to each other compared to observations in other sets.
    So, what do we mean by similarity and dissimilarity? **Similarity** refers to
    a numerical measure of how alike two objects are. Higher similarity values indicate
    greater resemblance between objects. Similarities are typically non-negative and
    often range from 0 (no similarity) to 1 (complete similarity). In *Figure 4**.1*,
    the difference between intercluster and intracluster distances is shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Difference between inter cluster and intra cluster distances](img/B21156_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Difference between inter cluster and intra cluster distances
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, **dissimilarity** represents a numerical measure of how different
    two objects are. Lower dissimilarity values indicate greater similarity between
    objects. Dissimilarity is sometimes referred to as distance. Like similarity,
    dissimilarity values may also fall within the interval *[0,1]*, but it is common
    for them to range from 0 to ∞.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dissimilarities between data objects can be quantified using distance metrics.
    Distances possess specific properties and can be employed to measure dissimilarity.
    For example, the **Euclidean distance**, denoted as *d*, can be utilized to measure
    the distance between two points, *x* and *y*, using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: d(x, y) = √ _ ∑ k=1 n (x i − y i) 2
  prefs: []
  type: TYPE_NORMAL
- en: In a two-dimensional plane, the Euclidean distance is the shortest distance
    between two points, represented by a straight line connecting them. This distance
    is calculated by taking the square root of the sum of the squared differences
    between the elements of two vectors, as shown in the earlier formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, distance can be measured using various other metrics, such as the
    Minkowski distance. The **Minkowski distance** metric is a generalization of the
    Euclidean distance and can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: d(x, y) = (∑ k=1 n |x i − y i| r) 1 _ r
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to utilize the **Manhattan distance** metric, which is derived
    from the Minkowski distance metric by setting *r = 1*. Additionally, there is
    the **cosine distance**, which incorporates the dot product scaled by the product
    of the Euclidean distances from the origin. It quantifies the angular distance
    between two vectors while disregarding their scale. Once we have selected a suitable
    metric for our specific case, we can proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an appropriate similarity metric for clustering holds great significance
    as it directly influences the quality and comprehensibility of your clustering
    outcomes. The selection of the metric should be in harmony with both the inherent
    traits of your data and the objectives of your clustering analysis. The default
    distance used in most clustering tools is Euclidean distance. However, this may
    be inappropriate in the presence of very noisy data or with a non-Gaussian distribution.
    In these situations, a robust alternative is the Manhattan distance, although
    it is important to keep in mind that robustness involves a loss of information,
    at least to some extent. In any case, you can choose between different dissimilarity
    measures based on the type of data and research objectives.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have examined different formulas for calculating distances between
    two objects. However, what if the objects under analysis are nominal instead of
    numerical? For nominal data, which can be represented as simple sequences or strings,
    various distance measures can be employed. One possible distance between two strings
    is determined by counting the number of symbols that differ between the strings.
  prefs: []
  type: TYPE_NORMAL
- en: How to find centroids and centers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Centroids and centers are essential concepts in clustering that represent the
    central points or locations within each cluster. The **centroid** is a point within
    the feature space that represents the average position of all the data points
    belonging to the associated cluster. It serves as a type of center of gravity
    for the cluster and, typically, does not coincide with any specific data point
    in the dataset. For example, in **k-means clustering**, a centroid is the mean
    or average position of all the data points within a cluster. It serves as a representative
    point that minimizes the sum of squared distances between the data points and
    the centroid. The centroid is computed by taking the average of the feature values
    across all data points within the cluster. During the iterative process of k-means,
    data points are assigned to the cluster whose centroid they are closest to, and
    the centroids are updated accordingly until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: The term *centers* is often used in other clustering algorithms, such as hierarchical
    clustering or density-based clustering. The centers can represent central points
    within clusters, but the computation method might differ depending on the algorithm.
    In **hierarchical clustering**, centers can be computed as the mean, median, or
    another representative point within each cluster. In **density-based clustering**,
    the center of a cluster can be defined as the data point with the highest density
    or as the medoid, which is the most centrally located point.
  prefs: []
  type: TYPE_NORMAL
- en: Both centroids and centers provide a way to summarize and represent clusters
    in clustering analysis. They offer insights into the characteristic properties
    of each cluster and can be used to classify new data points based on their proximity
    to these central points.
  prefs: []
  type: TYPE_NORMAL
- en: How to define a grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A cluster is a set of objects that are like each other but are dissimilar to
    objects contained in other clusters; a cluster can therefore be defined in terms
    of internal cohesion (homogeneity) and external cohesion (separation). The goal
    is to group – or segment – collections of objects into subsets, called clusters,
    such that objects in the same cluster have similar characteristics to each other,
    unlike elements assigned to different clusters. In this sense, the distance between
    points of the same cluster must be minimized while the distance between points
    belonging to different clusters must be maximized.
  prefs: []
  type: TYPE_NORMAL
- en: In cluster analysis, the definition of a measure that quantifies the degree
    of similarity between objects is crucial (see the measures in *Figure 4**.1*).
    The choice of this measurement greatly influences the formation of one object
    partition over another. When dealing with numeric data, the similarity measure
    is referred to as a distance function. In this case, the goal is to minimize the
    distance function because objects that are closer together exhibit similar characteristics
    and belong to the same cluster. On the other hand, when working with textual data,
    the similarity measure is used instead. In these cases, the aim is to maximize
    similarity since texts that are more similar are closer together and belong to
    the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: A visual representation can help reveal the cohesion properties of clusters
    without explicitly and rigorously defining them. It is important to note that
    there is no universal definition that applies to every situation. Attempts to
    precisely quantify homogeneity and separation using explicit numerical indices
    have resulted in numerous and diverse criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a distance measure has been selected, the next step is to determine how
    to form groups or clusters. There are two main families of clustering algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical clustering**: This approach constructs a hierarchical structure
    or taxonomy of the data. It involves creating a tree-like structure, often referred
    to as a **dendrogram**, which represents the relationships between the data points.
    Hierarchical clustering can be agglomerative, starting with individual data points
    and iteratively merging them into clusters, or divisive, beginning with one large
    cluster and recursively splitting it into smaller clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioning clustering**: In this approach, the data space is divided into
    non-overlapping sub-zones or partitions. Each data point belongs to exactly one
    partition, and the union of all partitions covers the entire space. Partitioning
    algorithms aim to optimize a specific criterion, such as minimizing the within-cluster
    variance or maximizing the separation between clusters. Popular partitioning algorithms
    include k-means, where data points are assigned to the cluster with the nearest
    centroid, and **Gaussian mixture models** (**GMMs**), which represent clusters
    as a mixture of Gaussian distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both hierarchical clustering and partitioning clustering methods have their
    own advantages and applications. The choice between the two depends on the nature
    of the data, the desired interpretation of the results, and the specific goals
    of the clustering analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering is a method of clustering that creates a hierarchy or
    tree-like structure of clusters. It iteratively merges or splits clusters based
    on the similarity or dissimilarity between data points. The resulting structure
    is often represented as a dendrogram, which visualizes the relationships and similarities
    among the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative hierarchical clustering**: This starts with each data point
    considered as an individual cluster and progressively merges similar clusters
    until all data points belong to a single cluster. At the beginning, each data
    point is treated as a separate cluster, and in each iteration, the two most similar
    clusters are merged into a larger cluster. This process continues until all data
    points are in one cluster. The merging process is guided by a distance or similarity
    measure, such as a Euclidean distance or correlation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive hierarchical clustering**: This starts with all data points in a
    single cluster and recursively divides the cluster into smaller subclusters until
    each data point is in its own cluster. In each iteration, a cluster is split into
    two or more subclusters based on a dissimilarity measure. This process continues
    until each data point forms its own cluster or until a stopping criterion is met.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering has several advantages. It does not require specifying
    the number of clusters in advance, as the desired number of clusters can be determined
    by cutting the dendrogram at a particular level. It also provides a visual representation
    of the clustering structure, allowing for interpretation and exploration of the
    data. However, hierarchical clustering can be computationally intensive for large
    datasets, and the choice of distance or similarity measure and the method of merging
    or splitting clusters can influence the results. Overall, hierarchical clustering
    is a flexible and widely used technique for exploring and understanding the underlying
    structure of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, hierarchical clustering is performed using several functions to create
    a cluster tree or dendrogram. The process involves grouping the data and creating
    a multilevel hierarchy where clusters at one level are joined as clusters at the
    next level. The *Statistics and Machine Learning Toolbox* provides the necessary
    functions to perform agglomerative hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following functions are commonly used in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pdist`: This function calculates pairwise distances between data points based
    on a chosen distance metric. It takes the data matrix as input and returns a distance
    matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linkage`: The `linkage` function is used to compute the linkage matrix, which
    represents the distances between clusters. It takes the distance matrix as input
    and performs the linkage calculation based on a specified method, such as `single`,
    `complete`, or `average`. `single` linkage, often referred to as the nearest neighbor
    method, employs the minimum distance between objects in the two clusters. `complete`
    linkage, also known as the farthest neighbor method, utilizes the maximum distance
    between objects in the two clusters. Finally, `average` linkage computes the mean
    distance between all pairs of objects in any two clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster`: The `cluster` function is responsible for forming the clusters based
    on the linkage matrix. It takes the linkage matrix and a cutoff parameter as input
    and returns the cluster assignments for each data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dendrogram`: Finally, the `dendrogram` function is used to visualize the cluster
    tree or dendrogram. It takes the linkage matrix as input and plots the dendrogram,
    showing the merging of clusters at each level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By utilizing these functions in sequence, the `clusterdata` function in MATLAB
    performs agglomerative hierarchical clustering and generates the dendrogram. Understanding
    the sequence of function calls can be beneficial for comprehending the entire
    process and the flow of the hierarchical clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how clustering can be performed in MATLAB by using the provided
    functions. Clustering involves identifying groups within a dataset based on proximity
    measures, which can refer to similarity or dissimilarity. Here’s an example using
    MATLAB. In MATLAB, the `pdist` function is used to compute the distance between
    every pair of objects in a dataset. If the dataset has *k* objects, there are
    *k*(k – 1)/2* pairs in total. The `pdist()` function calculates the Euclidean
    distance by default for a *k-by-p* data matrix, where rows correspond to observations
    and columns correspond to variables. It returns a row vector of length *k(k –
    1)/2*, representing pairs of observations in the source matrix. The distances
    are arranged in the order *(2,1), (3,1), ..., (k,1), (3,2), ..., (k,2), ..., (k,k–1))*.
    To obtain the distance matrix, you can use the `squareform()` function, which
    we’ll cover later.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `pdist()` calculates the Euclidean distance, but there are various
    distance metrics available. You can specify a different metric by selecting one
    of the options provided in the function’s syntax. Some available choices include
    `euclidean`, `squaredeuclidean`, `seuclidean`, `cityblock`, `minkowski`, `chebychev`,
    `mahalanobis`, `cosine`, `correlation`, `spearman`, `hamming`, and `jaccard`.
    Additionally, you can create a custom distance function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's then analyze the MATLAB code line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s consider a matrix with six points in a Cartesian plane,
    represented by pairs of coordinates (*x*, *y*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s apply the `pdist` function to calculate the distances between pairs
    of points. The `pdist` function will return this distance information in a vector,
    where each element represents the distance between a pair of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following matrix was printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is worth noting that the distance matrix is symmetric, indicating that the
    distance between point *i* and point *j* is the same as the distance between point
    *j* and point *i*. In certain cases, it is beneficial to normalize the values
    in the dataset before calculating the distance information. This normalization
    is important because variables in raw data can be measured on different scales.
    These variations in measurement scales can distort proximity calculations. To
    address this, the `zscore()` function can be used to standardize all values in
    the dataset to a common scale.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As previously mentioned, the concepts of distance and how to define groups
    are the key components of a clustering algorithm. After calculating the proximity
    between objects in the dataset, the next step is to determine how these objects
    should be grouped into clusters. This is where the `linkage` function comes into
    play. Utilizing the distance measure generated by the `pdist()` function, the
    `linkage` function links pairs of objects that are close together, forming binary
    clusters. This process continues as the binary clusters are merged with other
    objects, creating larger clusters, until all objects in the original dataset are
    linked together in a hierarchical tree structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the usage of the `linkage` function, we have completed a significant portion
    of the task. This function has identified potential groupings based on the previously
    calculated distance measurements. To gain a deeper understanding of how the function
    operates, let’s examine the results in `GroupsMatrix`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In `GroupsMatrix`, each row represents a newly formed cluster. By analyzing
    this diagram we can understand how the points are identified to group them into
    different groups. The first two columns indicate the points that have been linked,
    particularly in the earlier stages, while the subsequent columns denote the newly
    created clusters. It is important to note that hierarchical clustering, of the
    agglomerative type, begins with small clusters and proceeds to incorporate additional
    elements, generating larger clusters. The third column of the matrix presents
    the distance between these points, providing further insights into the clustering
    process. In *Figure 4**.2*, we can see the dendrogram of the clustering method.
    By analyzing this diagram, we can understand how the points are treated to group
    them into different groups.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `linkage` function is responsible for utilizing the distances calculated
    by the `pdist()` function to determine the order in which clusters are formed.
    Additionally, this function can calculate the distances between newly merged clusters.
    By default, the `linkage` function employs the single `linkage` method. However,
    there are several other methods available for use, including `average`, `centroid`,
    `complete`, `median`, `single`, `ward`, and `weighted`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, to visualize the hierarchical binary cluster tree, we can utilize
    the `dendrogram()` function. This function generates a dendrogram plot, which
    consists of U-shaped lines connecting data points in a hierarchical tree structure.
    The height of each U-shaped line represents the distance between the two connected
    data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In *Figure 4**.2*, we can see a dendrogram of a hierarchical cluster obtained
    from a series of points on the Cartesian plane.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Dendrogram of a hierarchical cluster](img/B21156_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Dendrogram of a hierarchical cluster
  prefs: []
  type: TYPE_NORMAL
- en: A dendrogram is a graphical representation in the form of a branching diagram
    that illustrates the similarities among a group of entities. The horizontal axis
    of the dendrogram represents the elements being analyzed, while the vertical axis
    indicates the level of distance at which the fusion of two elements occurs. The
    strength of the relationships between two elements is depicted by the distance
    between the element’s corresponding vertical lines and the *x* axis.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the relationship between two elements in a dendrogram, we can
    trace a path from one element to another by following the tree diagrams and selecting
    the shortest path. The distance from the starting point to the outermost horizontal
    line crossed by the path reflects the degree of similarity between the two elements.
  prefs: []
  type: TYPE_NORMAL
- en: After building the dendrogram, we make a horizontal cut of the structure. All
    resulting subbranches below the horizontal cut represent a single cluster at the
    top level of the system and determine the corresponding cluster membership for
    each data sample (*Figure 4**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can verify how hierarchical clustering is able to group objects.
    In a dendrogram, the disparity between branches is reflected by their respective
    heights: the taller the branch, the greater the dissimilarity. This vertical measurement
    is referred to as the cophenetic distance, representing the dissimilarity between
    the two objects that form the cluster. To evaluate and compare the cophenetic
    distances between two objects, the `cophenet()` function can be employed. Verifying
    the effectiveness of the dissimilarity measurement can be accomplished by comparing
    the outcomes of clustering the same dataset using various distance calculation
    methods or clustering algorithms. This approach allows us to assess and contrast
    the performance of different techniques. To begin, let’s apply this evaluation
    to the calculations that have just been performed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, utilizing the new distance measure generated by the `pdist()` function
    with the cosine metric, we will form clusters by linking objects that are in close
    proximity. However, this time we will employ a different algorithm for calculating
    the distance between clusters. Specifically, we will use the weighted method,
    which computes the weighted average distance. By doing so, we can establish clusters
    based on the closeness of objects, considering their respective weights:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we can employ the `cophenet()` function to evaluate the clustering
    solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The outcome demonstrates an enhancement in the performance of hierarchical clustering
    by utilizing a different distance metric and linkage method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hierarchical clustering is a versatile clustering method with a wide range of
    applications across various domains. Some of the typical areas where hierarchical
    clustering is applied include **image processing**, **text analysis**, **market
    research** and **customer segmentation**, and **ecology and** **environmental
    science**.
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced the basic concepts of clustering, we can now move on to a
    practical example to discover how to use the tools available in the MATLAB environment
    to correctly perform a partitioning clustering analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning-based clustering algorithms with MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Partitioning-based clustering** is a type of clustering algorithm that aims
    to divide a dataset into distinct groups or partitions. In this approach, each
    data point is assigned to exactly one cluster, and the goal is to minimize the
    intra-cluster distance while maximizing the inter-cluster distance. The most popular
    partitioning-based clustering algorithms include k-medoids, fuzzy c-means, and
    hierarchical k-means. These algorithms vary in their approach and objectives,
    but they all aim to partition the data into well-separated clusters based on some
    distance or similarity measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the k-means algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most well-known partitioning-based clustering algorithms is k-means.
    In k-means clustering, the algorithm attempts to partition the data into *k* clusters,
    where *k* is a predefined number specified by the user. The algorithm iteratively
    assigns data points to the nearest cluster centroid and recalculates the centroid
    positions until convergence is achieved. The result is a set of *k* clusters,
    each represented by its centroid. The process involves iteratively relocating
    data instances by transferring them from one cluster to another, beginning with
    an initial partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm, developed by James MacQueen in 1967 (MacQueen, J. (1967)
    *Some methods for classification and analysis of multivariate observations*, Proceedings
    of the Fifth Berkeley Symposium On Mathematical Statistics and Probabilities,
    1, 281-296.), is a clustering algorithm that partitions a group of objects into
    *k* clusters based on their attributes. It is a variation of the **expectation-maximization**
    (**EM**) algorithm, with the objective of determining *k* data groups generated
    by Gaussian distributions. Unlike EM, k-means calculates the Euclidean distance
    to measure dissimilarity between data items.
  prefs: []
  type: TYPE_NORMAL
- en: In k-means, objects are represented as vectors in a vector space. The algorithm
    aims to minimize the total intra-cluster variance or standard deviation. Each
    cluster is represented by a centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of a set of observations (*x1, x2, …, xn*), where each observation
    is represented as a d-dimensional real vector, the objective of k-means clustering
    is to divide the *n* observations into *k (≤ n)* sets, denoted as *S = {S1, S2,
    …, Sk}*, in a way that minimizes the within-cluster variance. The objective equation
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Min S ∑ i=1 k ∑ x∈S i ‖(x − μ i)‖ 2
  prefs: []
  type: TYPE_NORMAL
- en: In the previous equation, μ i is the mean of the points in S i.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm follows an iterative procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the number of clusters, *k*. Choosing the right value for *k*, , is a
    crucial step as it directly impacts the quality of your clustering outcomes. Numerous
    techniques are available to ascertain the optimal *k*, including the elbow method,
    silhouette score, cross-validation, and hierarchical clustering, among others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *k* partitions and assign each data point randomly or using heuristic
    information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the centroid for each cluster, which is the mean of all the points
    within the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distance between each data point and each cluster centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new partition by assigning each data point to the cluster with the
    closest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recalculate the centroids for the new clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 4* to *6* until the algorithm converges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal is to determine the position of *k* centroids, with each centroid representing
    a cluster. The initial positions of the centroids significantly impact the results,
    and it is beneficial to place them as far apart as possible. Each object is then
    associated with the nearest centroid, resulting in an initial grouping. In subsequent
    iterations, new centroids are recalculated as the cluster barycenter based on
    the previous iteration’s results. Data points are reassigned to the new closest
    centroids. This process continues until the centroids no longer move, indicating
    convergence. We can see how *Figure 4**.4* illustrates the positions of *k* centroids
    in the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In MATLAB, the `kmeans()` function is used to perform k-means clustering. This
    function partitions the data into *k* mutually exclusive clusters and returns
    the index of the cluster to which each object is assigned. The clusters are defined
    by the objects and their centroids. The centroid of each cluster is the point
    that minimizes the sum of distances from all objects in that cluster. The calculation
    of cluster centroids varies depending on the chosen distance measure, aiming to
    minimize the sum of the specified measure. Different distance measures and methods
    for minimizing distances can be specified as input parameters to the `kmeans()`
    function. Here is a list summarizing the available distance measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sqeuclidean`: **Squared Euclidean** distance (default). Each centroid is the
    mean of the points in that cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cityblock`: Sum of absolute differences. Each centroid is the component-wise
    median of the points in that cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cosine`: 1 minus the cosine of the included angle between points. Each centroid
    is the mean of the points in that cluster after normalizing those points to the
    unit of the Euclidean length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`correlation`: 1 minus the sample correlation between points. Each centroid
    is the component-wise mean of the points in that cluster after centering and normalizing
    those points with a mean of 0 and a standard deviation of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hamming`: Suitable for binary data only. This measures the proportion of differing
    bits. Each centroid is the component-wise median of the points in that cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the `kmeans()` function uses the k-means++ algorithm for cluster
    center initialization and the squared Euclidean metric to determine distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means++ enhances the k-means algorithm by introducing a more sophisticated
    approach to initializing cluster centroids. The initialization procedure in k-means++
    comprises the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin by randomly selecting a single data point as the initial centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each subsequent centroid (up to *k*), pick the next centroid from the data
    points with a probability that is proportional to the square of the distance between
    each data point and the nearest existing centroid. This probabilistic selection
    ensures that the initial centroids are distributed in a manner that optimally
    spaces them apart and reduces sensitivity to the initial centroid choices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After initializing the centroids using the k-means++ method, the algorithm proceeds
    in the same fashion as standard k-means. It iteratively assigns data points to
    the closest centroids and updates the centroids until convergence is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K-means++ exhibits a tendency for swifter and more consistent convergence due
    to its enhanced initialization method, which diminishes the likelihood of it becoming
    trapped in local optima. As a consequence, it often yields superior clustering
    quality by virtue of the well-dispersed initial centroids, ultimately resulting
    in more advantageous cluster assignments. Let’s now analyze a practical application
    of this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the usage of the `kmeans()` function, let’s consider a dataset
    containing measurements of the specific weight and hardness (Mohs scale) of minerals
    extracted from different quarries. These measurements are stored in an `xls` file
    named `Minerals.xls`. We can start by importing the data into the MATLAB workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, let’s look at the imported data by plotting a simple scatter plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness
    (Mohs scale) of minerals)](img/B21156_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness
    (Mohs scale) of minerals)
  prefs: []
  type: TYPE_NORMAL
- en: By examining *Figure 4**.3*, it appears that the data points are concentrated
    in four distinct regions, each characterized by different values of the two variables
    (*x,y*). This observation suggests that a cluster analysis with a fixed value
    of *k = 4* should be conducted. To ensure reproducibility and obtain consistent
    results, we can set the following parameters accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the seed to permit the reproducibility of the experiment. The `rgn()`
    function is responsible for controlling the generation of random numbers in MATLAB.
    By setting the seed using `rgn()`, the `rand()`, `randi()`, and `randn()` functions
    will produce a predictable sequence of numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Two variables have been generated: `IdCluster` and `Centroid`. `IdCluster`
    is a vector that stores the predicted cluster indices for each observation in
    the `InputData`. The centroid is a 4-by-2 matrix that represents the final centroid
    locations for the four clusters. By default, the `kmeans()` function utilizes
    the k-means++ algorithm for centroid initialization and employs the squared Euclidean
    distance measure.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have assigned every data point in the original dataset to one of
    the four clusters, we can visualize the clusters in a scatter plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot shown in *Figure 4**.4* is printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 –  Scatter plot of clusters (x is the weight and y is the hardness
    (Mohs scale) of minerals)](img/B21156_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Scatter plot of clusters (x is the weight and y is the hardness
    (Mohs scale) of minerals)
  prefs: []
  type: TYPE_NORMAL
- en: Upon analyzing *Figure 4**.4*, it is evident that the `kmeans()` function has
    successfully identified distinct clusters with clear separation. Each cluster
    is automatically visualized using a different color, making it easy to identify
    them, especially at the boundaries where the data points may overlap. Additionally,
    the use of different markers in the scatter plot aids in distinguishing the data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight the position of each centroid, we have specifically set the marker
    as `x` and adjusted its appearance by setting the line thickness to `4`, marker
    color to `black`, and marker size to `25`. The centroids serve as the center of
    mass for their respective clusters and minimize the sum of squared distances from
    the other data points, as mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Upon further examination of the figure, it becomes apparent that cluster boundaries
    are not always well defined, particularly in this type of chart. Near the boundaries,
    there are data points that merge together, making it challenging to determine
    which cluster they belong to.
  prefs: []
  type: TYPE_NORMAL
- en: To gain a deeper understanding of the separation and boundaries of the clusters,
    we can utilize a `silhouette()` function is specifically designed to generate
    cluster silhouettes based on the provided cluster indices from the `kmeans()`
    function. This function takes a data matrix as input, where each row represents
    a data point and each column represents its coordinates. The cluster indices can
    be categorical variables, numeric vectors, character matrices, or cell arrays
    of character vectors, where each element represents the cluster assignment for
    a data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When using the `silhouette()` function, any `NaN`or empty character vectors
    in the cluster indices are treated as missing values, and the corresponding rows
    in the data matrix are ignored. By default, the `silhouette()` function calculates
    the squared Euclidean distance between the data points in the input matrix. Using
    the silhouette plot, we can gain a better understanding of the separation and
    proximity of the data points within and between clusters, providing valuable insights
    into the quality of the clustering results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following figure shows a silhouette plot for `kmeans` clustering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Silhouette plot for k-means clustering](img/B21156_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Silhouette plot for k-means clustering
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.5*, the *x* axis represents the silhouette values, which range
    from `+1` to `-1`. A silhouette value of `+1` indicates that a data point is significantly
    distant from neighboring clusters, while a value of `0` suggests that the data
    point does not clearly belong to any specific cluster. A silhouette value of `-1`
    implies that the data point is likely assigned to the wrong cluster. The silhouette
    plot returns these values as its first output.
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette plot is useful for assessing the separation distance between
    the resulting clusters. It provides a visual representation of how close each
    data point in one cluster is to the points in neighboring clusters. By examining
    the silhouette plot, we can evaluate the clustering solution and determine whether
    the chosen parameters, such as the number of clusters, are appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: In a successful clustering solution, we expect to see high silhouette values,
    indicating that the points are correctly assigned to their respective clusters
    and have minimal connections to neighboring clusters. If a majority of the data
    points have high silhouette values, it indicates a good clustering solution. However,
    if many points have low or negative silhouette values, it suggests that the clustering
    solution needs to be reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the analysis of *Figure 4**.5*, we can determine whether our assumption
    of choosing *k = 4* for the k-means algorithm has produced favorable results.
    In *Figure 4**.5*, there are no clusters with below-average silhouette scores,
    indicating a good separation between the clusters. Additionally, the silhouette
    plots do not exhibit significant fluctuations in size. The uniform thickness of
    the silhouette plot indicates that the clusters are of similar sizes. These observations
    validate our choice of *k = 4* for the number of clusters and confirm that it
    is an appropriate selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'After analyzing in detail how to easily perform a k-means clustering analysis
    in MATLAB, we can now see a new clustering methodology: the k-medoids algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data using the similarity measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-medoids algorithm is a variation of the k-means algorithm that uses medoids
    (actual data points) as representatives of each cluster instead of centroids.
    Unlike the k-means algorithm, which calculates the mean of the data points within
    each cluster, the k-medoids algorithm selects the most centrally located data
    point within each cluster as the medoid. This makes k-medoids more robust to outliers
    and suitable for data with non-Euclidean distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key differences between k-medoids and k-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representative points**: In k-medoids, the representatives of each cluster
    are actual data points from the dataset (medoids), while in k-means, the representatives
    are the centroids, which are calculated as the mean of the data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance measure**: The distance measure used in k-means is typically the
    Euclidean distance. On the other hand, k-medoids can handle various distance measures,
    including non-Euclidean distances. This flexibility allows k-medoids to work with
    different types of data, such as categorical or ordinal variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness to outliers**: The k-medoids algorithm is generally more robust
    to outliers than k-means. Outliers can significantly affect the centroid calculation
    in k-means, pulling the centroid towards them. In k-medoids, the medoids are actual
    data points, so the impact of outliers is limited to the specific cluster that
    contains them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational complexity**: K-means has a lower computational complexity
    compared to k-medoids. The selection of medoids in k-medoids requires the evaluation
    of pairwise distances between all data points, making it computationally more
    expensive, especially for large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster shape**: The k-means algorithm tends to produce spherical-shaped
    clusters due to the use of Euclidean distance and the calculation of mean centroids.
    K-medoids, on the other hand, can produce clusters of arbitrary shapes, as the
    medoids can be any data point within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When deciding between k-means and k-medoids, consider the nature of your data,
    the desired robustness to outliers, the availability of appropriate distance measures,
    and the computational resources. If your data contains outliers or non-Euclidean
    distances are more appropriate, k-medoids may be a better choice. Otherwise, if
    the data is well behaved and the Euclidean distance is suitable, k-means can provide
    efficient clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: Applying k-medoids in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In MATLAB, the `kmedoids()` function is used to perform k-medoids clustering.
    It partitions the observations in a matrix into *k* clusters and returns a vector
    containing the cluster indices for each observation. The input matrix should have
    rows corresponding to points and columns corresponding to variables. Similar to
    the `kmeans()` function, the `kmedoids()` function uses the squared Euclidean
    distance measure and the k-means++ algorithm for selecting initial cluster medoid
    positions by default.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how the `kmedoids()` method is applied in a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose a large distribution company wants to optimize the positioning of its
    offices to improve the efficiency of transferring goods from sorting hubs to peripheral
    locations. The company already has the geographic coordinates of its peripheral
    locations and wants to determine the optimal positions for the sorting hubs. The
    geographic coordinates have been transformed into relative coordinates for compatibility
    reasons with the `kmedoids()` function. This information is stored in a file named
    `PeripheralLocations.xls`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing this data into the MATLAB workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following plot displays the distribution of the dataset being imported.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Locations of the peripheral offices (geographic coordinates
    of the office: x is the  latitude and y is the longitude)](img/B21156_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6 – Locations of the peripheral offices (geographic coordinates of
    the office: x is the latitude and y is the longitude)'
  prefs: []
  type: TYPE_NORMAL
- en: Based on an initial visual analysis, three main regions can be identified where
    the company’s peripheral offices are located. This implies the need to designate
    three current offices as hubs for distributing goods to these peripheral locations.
    To determine the position of these future hubs and the associated sites for each,
    a clustering analysis will be conducted.
  prefs: []
  type: TYPE_NORMAL
- en: It is evident that each hub’s location should serve as the center of its respective
    cluster. As these positions are concrete values and must correspond to existing
    offices, utilizing the k-medoids method appears to be a logical decision. K-means
    and k-medoids differ in their centroid determination and sensitivity to outliers.
    K-means minimizes the sum of squared distances between data points and cluster
    centroids, using means as centroids and proving efficient for spherical, equally
    sized clusters. However, it is sensitive to outliers and influenced by initialization.
    In contrast, k-medoids minimizes the sum of dissimilarities, using the most centrally
    located data point (medoid) as the cluster center. This makes k-medoids more robust
    to outliers and noise, offering better performance for non-spherical clusters
    or those with uneven sizes. K-medoids is less sensitive to initialization, but
    its computational cost, involving dissimilarity calculations and medoid selection,
    can be higher than k-means, especially for large datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the previous code, the variables have the following meaning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`IdCluster` contains cluster indices of each observation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Kmedoid` contains the *k* cluster medoid locations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SumDist` contains the within-cluster sums of point-to-medoid distances'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dist` contains distances from each point to every medoid'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IdClKm` contains cluster indices of each medoid'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` contains information about the options used by the algorithm when executed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the information returned by the `kmedoids()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The command provides information on the algorithm type, metrics used, and the
    number of iterations to achieve the best performance. The algorithm used is called
    **PAM**, which stands for **partitioning around medoids**, and it is a classic
    approach for solving the k-medoids problem. This algorithm closely resembles k-means;
    however, it employs medoids, which are the data points positioned at the center
    of clusters, as cluster representatives rather than centroids. The primary objective
    of PAM is to minimize the total dissimilarity between data points and their corresponding
    medoids.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second parameter denotes the method used to determine the initial positions
    of the cluster medoids. The chosen metric is the squared Euclidean distance. The
    command also displays the number of iterations and identifies the best iteration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To visualize the result, we can generate plots of the clusters and their respective
    medoids:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The scatter plot displays the clustering results along with the indicated position
    of each medoid. We have specifically adjusted the markers to highlight the medoids’
    locations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic
    coordinates of the office: x is the latitude and y is the longitude)](img/B21156_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic
    coordinates of the office: x is the latitude and y is the longitude)'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing *Figure 4**.7*, it becomes evident that the peripheral sites are
    organized into three clusters as determined by the `kmedoids()` function. Similarly,
    the strategic positioning of the identified hubs is self-explanatory. The distinct
    colors and markers assigned to each cluster facilitate the quick identification
    of each site’s association with its respective cluster.
  prefs: []
  type: TYPE_NORMAL
- en: After having adequately explored the interesting world of clustering, it is
    time to move on to analyze how to reduce the size of data in cases with many features.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering dimensionality reduction techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** is a technique used in machine learning and data
    analysis to reduce the number of variables or features in a dataset. The goal
    of dimensionality reduction is to simplify the data while retaining important
    information, thereby improving the efficiency and effectiveness of subsequent
    analysis tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'High-dimensional datasets can be challenging to work with due to several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Curse of dimensionality**: As the number of features increases, the data
    becomes more sparse, making it difficult to find meaningful patterns or relationships'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational complexity**: Many algorithms and models become computationally
    expensive as the dimensionality of the data increases, requiring more time and
    resources for analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: High-dimensional data is more susceptible to overfitting,
    where a model becomes too specialized to the training data and fails to generalize
    well to new data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction methods aim to address these challenges by reducing
    the number of features while preserving important information. There are two main
    approaches to dimensionality reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: This approach selects a subset of the original features
    based on certain criteria. It aims to identify the most informative features that
    contribute significantly to the prediction or analysis task. Common techniques
    for feature selection include correlation analysis, backward/forward selection,
    and regularization methods such as L1 regularization (Lasso).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: This approach creates new features by combining or
    transforming the original features. It aims to capture the underlying structure
    or patterns in the data. **Principal component analysis** (**PCA**) is a popular
    feature extraction technique that identifies orthogonal axes in the data that
    explain the maximum variance. Other methods, such as **singular value decomposition**
    (**SVD**) and **non-negative matrix factorization** (**NMF**), can also be used
    for feature extraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both feature selection and feature extraction techniques have their advantages
    and disadvantages. The choice of method depends on the specific problem, dataset
    characteristics, and the goals of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the dimensionality of the data, dimensionality reduction methods
    can lead to benefits such as faster computation, improved model performance, better
    visualization, and enhanced interpretability of the data. However, it is important
    to note that dimensionality reduction is not always necessary or beneficial, and
    it should be applied judiciously after considering the specific requirements and
    characteristics of the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing feature selection methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with high-dimensional datasets, it is often beneficial to reduce
    the number of features to focus only on the most relevant ones, discarding the
    rest. This can result in simpler models that generalize more effectively. Feature
    selection involves the process of identifying the most important features while
    disregarding others during processing and analysis. It is crucial for creating
    a functional model that maintains a manageable number of features. In many cases,
    datasets contain redundant or excessive information, while in other cases, they
    may include incorrect information. Therefore, feature selection helps to eliminate
    such issues and improve the overall quality of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature selection enhances the efficiency of model creation by reducing the
    computational load on the CPU and the memory requirements for training algorithms.
    The selection of features serves several purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing clean and interpretable data**: Feature selection helps in selecting
    the most relevant features, resulting in a cleaner and more understandable dataset.
    In MATLAB, cleaning a large dataset involves various data preprocessing steps
    to address missing values, outliers, and inconsistencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplifying models and improving interpretability**: By focusing on a subset
    of important features, models become simpler and easier to interpret, allowing
    for better insights into the relationships between variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing training times**: With fewer features, the training process becomes
    faster, as there is less data to process and analyze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mitigating overfitting**: Overfitting occurs when a model is too complex
    and fits the training data too closely, leading to poor generalization. Feature
    selection helps in reducing overfitting by decreasing the variance in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection involves finding a subset of the original variables, typically
    through an iterative process. By exploring various combinations of variables and
    comparing prediction errors, the subset that produces the minimum error is selected
    as the input for the machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To perform feature selection, appropriate criteria need to be defined beforehand.
    These criteria typically involve minimizing a specific predictive error measure
    for models fitted to different subsets. Feature selection algorithms aim to find
    a subset of predictors that optimally model the measured responses while considering
    constraints, such as required or excluded features and the desired subset size.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is particularly valuable when the goal is to identify an influential
    subset, especially in cases involving categorical features where numerical transformations
    may not be adequate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature selection methodologies can be grouped into three broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter methods**: These methods evaluate characteristics independently of
    the learning model used subsequently. Statistical measures are calculated for
    each feature, such as the correlation with the output or the relative importance
    of the features themselves. Features are selected based on these measures, without
    considering the specific learning model. Examples of filtering methods include
    correlation analysis, chi-square test, information gain, and mutual information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrapper methods**: These methods use a specific learning model to evaluate
    the quality of different combinations of features. Several subsets of features
    are created, and a learning model is trained and evaluated for each subset. The
    goal is to select the subset of features that produce the best performance according
    to the specified metrics, such as accuracy or mean squared error. However, this
    approach can be computationally expensive, requiring repeated training and evaluation
    of many models. Examples of wrapper methods include forward selection, backward
    selection, and bi-directional elimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedded methods**: These methods perform feature selection within the model
    training process itself. The learning algorithms used incorporate feature selection
    mechanisms as part of their optimization process. For example, linear regression
    algorithms with smoothing (such as ridge regression and Lasso regression) tend
    to reduce the importance of less relevant features, helping to automatically select
    the more significant features during training. Examples of embedded methods include
    Lasso, elastic net, and ridge regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each category of feature selection methods has specific advantages and limitations.
    The choice of methodology depends on the nature of the problem, the number of
    features, the availability of data, and the required performance. It is important
    to experiment and compare different methodologies to find the most suitable for
    the specific case.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring feature extraction algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with large datasets, it is often necessary to transform them into
    a reduced representation of features. This transformation process is known as
    feature extraction. As covered briefly earlier, feature extraction involves taking
    an initial set of measured data and creating derivative values that capture the
    essential information while eliminating redundant or unnecessary data. The goal
    is to retain the relevant information contained in the original dataset while
    simplifying it and reducing its dimensionality. By extracting meaningful features,
    the resulting representation can be more manageable and efficient for subsequent
    analysis or modeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: By performing feature extraction, subsequent learning and generalization stages
    are simplified, and in certain cases, it can result in improved interpretations.
    It involves deriving new features from the original ones, aiming to reduce the
    measurement cost, enhance classifier efficiency, and achieve higher classification
    accuracy. When the extracted features are carefully selected, it is anticipated
    that the reduced representation will effectively fulfill the desired task instead
    of using the full-sized input. This enables more efficient and accurate processing
    as the focus shifts to a subset of features that capture the essential information
    required for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extraction algorithms are used to derive new features from the original
    set of features. These algorithms aim to capture the most relevant and informative
    aspects of the data while reducing dimensionality. Here are a few commonly used
    feature extraction algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**: PCA is a widely used technique for feature extraction. It identifies
    the orthogonal axes (principal components) in the data that explain the maximum
    variance. By projecting the data onto these components, it reduces the dimensionality
    while preserving the most important information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis (LDA)**: LDA is primarily used for feature extraction
    in the context of classification tasks. It aims to find a projection of the data
    that maximizes the separation between different classes while minimizing the variance
    within each class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent component analysis (ICA)**: ICA seeks to identify statistically
    independent components from a set of observed signals. It assumes that the observed
    signals are linear combinations of hidden independent components. ICA can be useful
    for extracting meaningful features in signal processing and blind source separation
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NMF**: NMF decomposes a non-negative matrix into two lower-rank matrices,
    where the elements are constrained to be non-negative. It can uncover parts-based
    representations of the data and is often used for feature extraction in image
    and text analysis tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders**: Autoencoders are neural network models that aim to reconstruct
    the input data from a compressed representation (encoding) layer. The encoding
    layer represents the extracted features. By training the autoencoder to minimize
    the reconstruction error, meaningful features can be learned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wavelet transform**: Wavelet transform decomposes the data into different
    frequency bands, allowing the extraction of features at various scales. It is
    commonly used in signal and image processing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of feature extraction algorithms. The choice of
    algorithm depends on the nature of the data, the specific problem, and the desired
    outcome. It is often beneficial to experiment with multiple algorithms and compare
    their performance to select the most suitable one for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extraction works by transforming the original set of features into
    a new representation that captures the essential information while reducing dimensionality.
    The process typically involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preprocessing**: The input data is preprocessed to handle missing values
    and outliers, and normalize the features if necessary. This ensures that the data
    is in a suitable form for feature extraction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: This step aims to reduce the number of features
    while preserving the most important information. Techniques such as PCA, LDA,
    or NMF are commonly used for dimensionality reduction. These methods identify
    a lower-dimensional subspace or combination of features that retain the most significant
    variations in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature construction**: In some cases, new features are constructed from
    the original features. This can involve mathematical operations, transformations,
    or aggregations. The goal is to create features that capture specific patterns
    or relationships in the data that may be more informative for the task at hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature selection**: In addition to dimensionality reduction, feature selection
    methods may be applied to further filter out irrelevant or redundant features.
    This helps to focus on the most informative features and reduce the noise in the
    data. Feature selection can be performed using techniques such as correlation
    analysis, statistical tests, or wrapper methods that evaluate the impact of different
    feature subsets on a specific learning algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature** **representation**: Once the desired set of features is extracted
    and selected, the data is represented using these features. This transformed representation
    is often lower-dimensional than the original data and contains the most relevant
    information for subsequent analysis or modeling tasks. The choice of feature extraction
    techniques depends on the specific problem, the characteristics of the data, and
    the goals of the analysis. In the following list, you’ll find instances across
    computer vision applications where a range of feature extraction techniques are
    put to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In face recognition, we employ PCA and **local binary patterns** (**LBP**) to
    extract facial features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection benefits from the **histogram of oriented gradients (HOG)**,
    which captures object shape and texture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification employs **convolutional neural networks** (**CNNs**) to
    engage in hierarchical feature learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to evaluate the extracted features and assess their impact on
    the performance of the downstream tasks, such as classification or regression.
    Feature extraction is an iterative process, and it may require experimentation
    and fine-tuning to find the most effective combination of techniques for a given
    task.
  prefs: []
  type: TYPE_NORMAL
- en: After having analyzed in detail all the dimensionality reduction techniques,
    the time has come to move on to practice by addressing examples of these technologies
    in the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and feature extraction using MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In MATLAB, there are several built-in functions and toolboxes that can be used
    for dimensionality reduction. In the next section, we will explore some practical
    examples of the dimensionality reduction algorithm in the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: Stepwise regression for feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regression analysis is a valuable approach for understanding the impact of
    independent variables on a dependent variable. It allows us to identify predictors
    that hold greater influence over the model’s response. Stepwise regression is
    a variable selection method used to choose a subset of predictors that exhibit
    the strongest relationship with the dependent variable. There are three common
    variable selection algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward method**: The forward method starts with an empty model, where no
    predictors are initially selected. In the first step, the variable showing the
    most significant association at a statistical level is added. In subsequent steps,
    the remaining variable with the highest statistically significant association
    is added to the model. This process continues until no more variables demonstrate
    statistically significant associations with the dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward method**: The backward method begins with a model that includes
    all variables. It then proceeds step by step to eliminate variables, starting
    with the one with the least significant association with the dependent variable
    on the statistical plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stepwise method**: The stepwise method alternates between the forward and
    backward processes. It involves adding and removing variables that gain or lose
    significance during various model adjustments, including the addition or re-insertion
    of variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These variable selection methods enable the identification of a subset of predictors
    that best explain the relationship with the dependent variable. The choice of
    algorithm depends on the specific context and the researcher’s goals. Each method
    has its own strengths and limitations, and it is essential to interpret the selected
    variables within the context of the overall regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MATLAB, you can create a stepwise regression model using the `stepwiselm()`
    function. This function returns a linear model by performing stepwise regression,
    where predictors are added or removed based on their significance. Here’s how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: The `stepwiselm()` function takes a table or dataset array as input and performs
    stepwise regression on the variables within it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function uses both forward and backward stepwise regression to determine
    the final model. It starts with an initial model specified using the `modelspec`
    attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each step, the function compares the explanatory power of incrementally larger
    and smaller models. It searches for terms to add or remove based on the value
    of the `Criterion` argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function computes the p-value of an F-statistic to test models with and
    without a potential term. If a term is not currently in the model, the null hypothesis
    is that the term would have a zero coefficient if added. If the evidence is strong
    enough to reject the null hypothesis, the term is added to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conversely, if a term is already in the model, the null hypothesis is that the
    term has a zero coefficient. If there is insufficient evidence to reject the null
    hypothesis, the term is removed from the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process continues until no single step improves the model. The function
    terminates when no terms meet the entrance or exit criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `stepwiselm()` function allows you to build different models from the same
    set of potential terms, depending on the initial model and the order in which
    terms are added or removed. By using this function in MATLAB, you can perform
    stepwise regression to select the most significant predictors and determine the
    final model that best explains the relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: To get the data, we utilize the extensive collection of datasets provided by
    the UCI *Machine Learning Repository* ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)).
    This repository serves as a valuable resource for obtaining a wide range of datasets
    for various machine learning and data analysis tasks. By leveraging this repository,
    we can access the necessary dataset required for our analysis or modeling purposes.
    The UCI Machine Learning Repository offers a diverse collection of datasets contributed
    by researchers and practitioners, facilitating the exploration and experimentation
    with different data-driven applications.
  prefs: []
  type: TYPE_NORMAL
- en: The *Yacht Hydrodynamics* dataset is utilized for predicting the hydrodynamic
    performance of sailing yachts based on their dimensions and velocity. This prediction
    plays a crucial role in evaluating the overall ship performance and estimating
    the necessary propulsive power during the initial design phase. The key inputs
    for this prediction include the fundamental hull dimensions and the velocity of
    the yacht. Specifically, the inputs consist of hull geometry coefficients and
    the Froude number, while the output is the residuary resistance per unit weight
    of displacement. By analyzing these inputs, valuable insights can be gained regarding
    the performance and efficiency of sailing yachts, aiding in the design and optimization
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to practically carry out a stepwise regression using MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will import the dataset into MATLAB using the `readtable()` function
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s check the size of the table imported:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The table imported contains 308 records of 7 features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the data now available in the MATLAB workspace in the form of a table,
    we can proceed to perform stepwise regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `stepwiselm()` function in MATLAB creates a linear model using stepwise
    regression to add or remove predictors from a table or dataset array, starting
    from a constant model. The response variable used by `stepwiselm()` is the last
    variable in the table or dataset array. The function employs both forward and
    backward stepwise regression methods to determine the final model. During each
    step, `stepwiselm()` searches for terms to add or remove from the model based
    on the specified criterion value provided through the `Criterion` argument. Terms
    are evaluated for their significance and contribution to the model’s performance.
    The function iteratively adds or removes predictors that improve the model’s fit
    based on the chosen criterion. By leveraging the `stepwiselm()` function in MATLAB,
    you can systematically identify and incorporate the most relevant predictors into
    your linear regression model, ultimately refining and optimizing the model’s predictive
    power.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stepwise regression allows us to systematically select the most relevant predictors
    and build a regression model. By executing the stepwise regression algorithm in
    MATLAB, we can automatically identify the significant predictors that have the
    strongest relationship with the dependent variable. This enables us to create
    an optimized regression model that captures the essential information from the
    dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s print some information about the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We initiated the stepwise regression with a constant model, and the function
    identified the only variable it deemed statistically significant (`x6 = FroudeNumber`)
    to include in the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s now try a different approach by starting with a linear model that includes
    an intercept and linear terms for each predictor. Then, step by step, the function
    will remove terms that are found to be statistically insignificant. This allows
    us to iteratively refine the model, retaining only the predictors that have a
    significant impact on the dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following text was printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this way, we can see the order in which the model has removed the features,
    starting with the ones that are less correlated with the response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can print the summary of the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is the same, but the procedure followed is different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To explore the full range of possibilities, we will now create a full quadratic
    model as the upper bound. We will start with a model that includes an intercept,
    linear terms, interactions, and squared terms for each predictor. This comprehensive
    model allows us to capture more complex relationships between the predictors and
    the dependent variable. However, we will still utilize stepwise regression to
    iteratively remove terms that lack statistical significance. By doing so, we can
    refine the model and focus on the predictors that have a substantial impact on
    the dependent variable, while disregarding those that are not statistically significant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Lots of information about how the variables were removed will be printed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s see the form of the model obtained:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this iteration, the resulting model is more intricate and comprehensive.
    It includes the `FroudeNumber` variable both as a squared term and as part of
    the interaction with `PrismaticCoef`. This increased complexity better captures
    the underlying phenomenon, as indicated by the obtained results: an R-squared
    value of `0.928`, an adjusted R-squared value of `0.927`, and a highly significant
    p-value of `4.93e-172`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These metrics suggest that the model provides a highly representative representation
    of the relationship between the predictors and the dependent variable, explaining
    a significant portion of the variability observed in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how to perform a PCA by analyzing a practical case.
  prefs: []
  type: TYPE_NORMAL
- en: Carrying out PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a statistical technique used for dimensionality reduction and data analysis.
    PCA aims to transform a dataset with a potentially large number of variables into
    a smaller set of uncorrelated variables called principal components. These components
    are linear combinations of the original variables and are ordered in such a way
    that the first component captures the maximum amount of variance in the data,
    the second component captures the next highest amount of variance, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is often used in various fields, such as machine learning, data visualization,
    and exploratory data analysis. It helps in identifying patterns and relationships
    in the data, reducing noise, and simplifying complex datasets. By reducing the
    dimensionality of the data, PCA can aid in visualizing and interpreting high-dimensional
    data, as well as improving computational efficiency in subsequent analyses. Additionally,
    PCA can be used for data preprocessing and feature extraction, allowing for more
    effective modeling and prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major challenges in multivariate statistical analysis is effectively
    displaying datasets with numerous variables. Thankfully, in such datasets, it
    is common for certain variables to be closely interrelated. These variables essentially
    contain the same information as they measure the same underlying quantity that
    influences the system’s behavior. Consequently, these variables are redundant
    and do not contribute anything significant to the model we aim to construct. To
    simplify the problem, we can replace this group of variables with a new variable
    that encapsulates the relevant information. The following figure illustrates redundant
    data in a table.
  prefs: []
  type: TYPE_NORMAL
- en: PCA generates a set of new variables, known as principal components, which are
    uncorrelated with each other. Each principal component is formed as a linear combination
    of the original variables. The orthogonality between the principal components
    ensures that there is no redundant information (see *Figure 4**.8*). Together,
    the principal components form an orthogonal basis for the data space. The primary
    objective of PCA is to explain the maximum variance in the data using the fewest
    number of principal components. In numerous real-world datasets, particularly
    those with high dimensionality, redundancy or noise is frequently present. Not
    all dimensions (features) play an equally vital role in shaping the inherent structure
    or patterns within the data. By pinpointing the directions (principal components)
    along which the data showcases the greatest variance, PCA can trim down the data’s
    dimensionality by opting for a subset of these principal components. This simplification
    streamlines the dataset while conserving its fundamental traits.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – PCA new feature space based on linear combinations of the original
    feature space](img/B21156_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – PCA new feature space based on linear combinations of the original
    feature space
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, you can perform PCA using the `pca()` function. This function takes
    an n-by-p data matrix as input, where each row corresponds to an observation and
    each column corresponds to a variable. The `pca()` function returns the principal
    component coefficients, also known as loadings. The coefficient matrix has dimensions
    p-by-p. Each column of the coefficient matrix contains the coefficients for one
    principal component, arranged in descending order based on the component variance.
    By default, the `pca()` function centers the data and utilizes the SVD algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this study, we utilized a dataset consisting of measurements of the geometric
    properties of kernels from three distinct varieties of wheat. The dataset includes
    kernels from the Kama, Rosa, and Canadian varieties, with a total of 70 samples
    randomly selected for the experiment. The internal structure of the kernels was
    visualized using a soft X-ray technique, resulting in high-quality images captured
    on 13x18 cm X-ray Kodak plates. The wheat grain used in the study was obtained
    from experimental fields and harvested using a combine harvester. The research
    was conducted at the Institute of Agrophysics of the Polish Academy of Sciences
    in Lublin. The data was collected from the UCI Machine Learning Repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we begin by collecting the data to be analyzed. The `SeedsData -dataset`
    is multivariate, consisting of 210 instances. Seven geometric parameters of the
    wheat kernel are used as real-valued attributes organizing an instance. We will
    first import the dataset into the MATLAB workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before applying the `pca()` function to our data, let’s take a preliminary look
    at the `SeedsData` table. The first seven columns of the table represent the measured
    variables, while the eighth column indicates the type of seed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To examine the potential relationships between these variables, we can use
    the `plotmatrix()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `plotmatrix()` function generates a matrix of subaxes, where the diagonal
    subaxes display histogram plots of the data in each respective column. The remaining
    subaxes are scatter plots, representing the relationships between different pairs
    of columns in the matrix. In the following figure, each subplot in the ith row
    and jth column represents a scatter plot of the ith column against the jth column.
    The visualization in *Figure 4**.9* provides insights into the data distribution
    and potential correlations between variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter;
    Compactness; LengthK; WidthK; AsymCoef; LengthKG)](img/B21156_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter;
    Compactness; LengthK; WidthK; AsymCoef; LengthKG)
  prefs: []
  type: TYPE_NORMAL
- en: 'As observed in *Figure 4**.9*, scatter plot matrices provide a useful visual
    tool for identifying potential linear correlations among multiple variables. This
    aids in pinpointing specific variables that may exhibit mutual correlations, indicating
    possible redundancy in the data. Additionally, the diagonal of the matrix displays
    histogram plots, offering insights into the distribution of values for each measured
    variable. The remaining plots represent scatter plots of the matrix columns, with
    each plot appearing twice: once in the corresponding row and again in the corresponding
    column as a mirror image.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon analyzing *Figure 4**.9*, several plots demonstrate a linear relationship
    between variables. For instance, the plot showing the relationship between `Area`
    and `Perimeter`, as well as the one between `Perimeter` and `LengthK`, exhibits
    such a correlation. However, no correlation can be observed for certain variable
    pairs. For example, the `LengthKG` variable appears to have no correlation with
    any other variable, as its data is scattered throughout the plot area.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further validate this initial visual analysis, we can calculate the linear
    correlation coefficients between the measured variables. The `corr()` function
    can be employed for this purpose, which returns a matrix containing the pairwise
    linear correlation coefficient `(r)` between each pair of columns in the user-provided
    matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'One might question the need to analyze both the scatter plots and the correlation
    coefficients. The reason is that there are cases where the scatterplots provide
    information that the correlation coefficients alone cannot convey. If the scatterplot
    does not indicate a linear relationship between variables, the correlation calculation
    becomes less meaningful. In such scenarios, two possibilities arise:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If no relationship exists at all between the variables, calculating the correlation
    is not appropriate because correlation specifically applies to linear relationships.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a strong relationship exists, but it is not linear, the correlation coefficient
    can be misleading. In certain cases, a strong curved relationship may exist, which
    cannot be captured accurately by the correlation coefficient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This emphasizes the critical importance of examining the scatterplots alongside
    the correlation coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With this understanding, it is now appropriate to proceed with the PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code returns the following information:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`coeff`: The principal component coefficients'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score`: The principal component scores'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latent`: The principal component variances'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tsquared`: The Hotelling’s T-squared statistic for each observation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained`: The percentage of the total variance explained by each principal
    component'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mu`: The estimated mean of each variable'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In situations where the variables are measured in different units or the variance
    differs significantly across columns, it is often recommended to scale the data
    or apply weights. The `pca()` function in MATLAB incorporates a default behavior
    of centering the user-provided matrix by subtracting the column means before conducting
    either SVD or eigenvalue decomposition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Additionally, for performing the PCA, the `pca()` function offers three different
    algorithms:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SVD (`svd`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalue decomposition of the covariance matrix (`eig`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternating least squares algorithm (`als`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the `pca()` function utilizes the SVD algorithm for the analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s analyze the results returned by the `pca()` function. The coefficient
    matrix, `coeff`, obtained from the PCA, contains the coefficients for the first
    seven variables present in the `SeedsData` table. The rows of `coeff` represent
    the variables, while the columns correspond to the principal components. The coefficients
    within each column determine the linear combination of the original variables
    that represent the information in the new dimensional space. The columns of `coeff`
    are arranged in descending order based on the variance of each principal component.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A principal component is a linear combination of the original variables, denoted
    as `p`, weighted by a vector, `u`. The first principal component is formed by
    combining the variables with the highest variance. In contrast, the second principal
    component combines variables with a slightly lower variance, while also ensuring
    orthogonality to the previous component. This pattern continues for subsequent
    principal components, each incorporating variables with progressively lower variances
    while maintaining orthogonality to the preceding components. The number of principal
    components is equal to the number of observed variables. Each principal component
    is derived as a linear combination that maximizes the variance while maintaining
    non-correlation with the preceding components.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second output, `score`, comprises the coordinates of the original data in
    the new dimensional space defined by the principal components. It represents how
    each observation aligns with the principal components and provides a representation
    of the data in the transformed space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our goal is to represent the dataset in a new space with reduced dimensions.
    To achieve this, we plot the first two columns of the `score` matrix, which represent
    the coordinates of the original data in the new coordinate system defined by the
    principal components. To make the graph more comprehensible, we group the data
    by class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure is printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The scatter plot for the first two principal components](img/B21156_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The scatter plot for the first two principal components
  prefs: []
  type: TYPE_NORMAL
- en: The previous figure distinctly classifies the seeds into three distinct classes.
    The data points are visibly distributed in separate areas of the plot, with minimal
    uncertainty observed only in the border regions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the results obtained from the `pca()` function. The third output,
    `latent`, is a vector that represents the variance explained by each corresponding
    principal component. The variance of each principal component is reflected in
    the sample variances of the columns in the `score` matrix, which align with the
    corresponding rows in `latent`. As previously mentioned, the columns of `latent`
    are arranged in descending order based on the variance of each principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the analysis, we can visualize both the principal component coefficients
    for each variable and the principal component scores for each observation in a
    single plot. This type of plot is commonly referred to as a biplot. Biplots serve
    as exploration plots that enable the simultaneous display of graphical information
    on both the samples and variables present in a data matrix. In biplots, samples
    are represented as points, while variables are depicted as vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following diagram depicts the principal components coefficients for each
    variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Biplot of the principal component coefficients for each variable
    and principal component scores for each observation](img/B21156_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Biplot of the principal component coefficients for each variable
    and principal component scores for each observation
  prefs: []
  type: TYPE_NORMAL
- en: In the biplot, each of the seven variables is represented by a vector. The direction
    and length of each vector indicate the contribution of that variable to the two
    principal components depicted in the plot.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the first principal component (horizontal axis), four variables
    have positive coefficients, while three variables have a negative coefficient.
    This explains why four vectors are directed toward the right half of the plot,
    while three vectors are directed toward the left half. The largest coefficient
    in the first principal component corresponds to the `AsymCoef` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the second principal component (vertical axis), all the variables
    have positive coefficients. By examining the length of the vectors, we can clearly
    understand the weight of each variable in their respective principal components.
    It is evident that the `AsymCoef` variable holds a significant weight compared
    to the others in the first principal component. Similarly, the `LengthK` variable
    assumes a prominent role in the second principal component.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained knowledge about performing accurate cluster analysis
    in the MATLAB environment. Our exploration began by understanding the measurement
    of similarity, including concepts such as element proximity, similarity, and dissimilarity
    measures. We delved into different methods for grouping objects, namely hierarchical
    clustering, and partitioning clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding partitioning clustering, we focused on the k-means method. We learned
    how to iteratively locate *k* centroids, each representing a cluster. We also
    examined the effectiveness of cluster separation and how to generate a silhouette
    plot using cluster indices obtained from k-means. The silhouette value for each
    data point serves as a measure of its similarity to other points within its own
    cluster, compared to points in other clusters. Furthermore, we delved into k-medoids
    clustering, which involves identifying the centers of clusters using medoids instead
    of centroids. We learned the procedure for locating these medoid centers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explored the process of selecting a feature that best represents a
    given dataset, which is known as dimensionality reduction. We gained an understanding
    of the fundamental concept behind dimensionality reduction and how it can be achieved
    through variable transformation.
  prefs: []
  type: TYPE_NORMAL
- en: To perform feature extraction and dimensionality reduction, we learned how to
    utilize the `stepwiselm()` function. This function enables the creation of a linear
    model and the automatic addition or removal of variables based on their significance.
    We also discovered how to construct small models starting from a constant model
    and large models from models containing numerous terms.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we discussed techniques for handling missing values within a dataset
    and explored various methods for extracting features. Among these methods, we
    specifically analyzed PCA. PCA is a robust quantitative approach that simplifies
    the data by identifying the most informative components.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the fundamental concepts of artificial
    neural networks and their implementation in the MATLAB environment. Our focus
    will be on understanding the basic principles of neural networks and how to apply
    them effectively. We will explore various aspects of neural network analysis,
    including data preparation, fitting, pattern recognition, and clustering analysis,
    all within the MATLAB framework. Additionally, we will delve into the techniques
    of preprocessing, postprocessing, and network visualization, which play a crucial
    role in enhancing training efficiency and evaluating network performance. By the
    end of the chapter, you will have gained practical knowledge on implementing and
    optimizing artificial neural networks using MATLAB, enabling you to tackle a wide
    range of data analysis tasks.
  prefs: []
  type: TYPE_NORMAL
