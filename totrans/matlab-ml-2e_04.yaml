- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Clustering Analysis and Dimensionality Reduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析和降维
- en: Clustering techniques aim to uncover concealed patterns or groupings within
    a dataset. These algorithms detect groupings without relying on any predefined
    labels. Instead, they select clusters based on the similarity between elements.
    Dimensionality reduction, on the other hand, involves transforming a dataset with
    numerous variables into one with fewer dimensions while preserving relevant information.
    Feature selection methods attempt to identify a subset of the original variables,
    while feature extraction reduces data dimensionality by transforming it into new
    features. This chapter shows us how to divide data into clusters, or groupings
    of similar items. We’ll also learn how to select features that best represent
    the set of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类技术旨在揭示数据集中隐藏的模式或分组。这些算法在不需要任何预定义标签的情况下检测分组。相反，它们根据元素之间的相似性选择聚类。另一方面，降维涉及将具有多个变量的数据集转换为具有较少维度的数据集，同时保留相关信息。特征选择方法试图识别原始变量的子集，而特征提取通过将其转换为新的特征来降低数据维度。本章向我们展示了如何将数据划分为聚类，或相似项目的分组。我们还将学习如何选择最能代表数据集的特征。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Understanding clustering – basic concepts and methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类——基本概念和方法
- en: Understanding hierarchical clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解层次聚类
- en: Partitioning-based clustering algorithms with MATLAB
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于划分的聚类算法使用MATLAB
- en: Grouping data using the similarity measures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相似度度量分组数据
- en: Discovering dimensionality reduction techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现降维技术
- en: Feature selection and feature extraction using MATLAB
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MATLAB进行特征选择和特征提取
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will introduce basic concepts relating to machine learning.
    To understand these topics, a basic knowledge of algebra and mathematical modeling
    is needed. You will also need a working knowledge of the MATLAB environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍与机器学习相关的基本概念。要理解这些主题，需要具备代数和数学建模的基本知识。您还需要熟悉MATLAB环境。
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的MATLAB代码，您需要以下文件（可在GitHub上找到：[https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition))：
- en: '`Minerals.xls`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Minerals.xls`'
- en: '`PeripheralLocations.xls`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PeripheralLocations.xls`'
- en: '`YachtHydrodynamics.xlsx`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YachtHydrodynamics.xlsx`'
- en: '`SeedsDataset.xlsx`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SeedsDataset.xlsx`'
- en: Understanding clustering – basic concepts and methods
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类——基本概念和方法
- en: '**Clustering** is a fundamental concept in data analysis, aiming to identify
    meaningful groupings or patterns within a dataset. It involves the partitioning
    of data points into distinct clusters based on their similarity or proximity to
    each other. In both clustering and classification, our goal is to discover the
    underlying rules that enable us to assign observations to the correct class. However,
    clustering differs from classification as it requires identifying a meaningful
    subdivision of classes as well. In classification, we benefit from the target
    variable, which provides the classification information in the training set. In
    contrast, clustering lacks such additional information, necessitating the deduction
    of classes by analyzing the spatial distribution of the data. Dense areas in the
    data correspond to groups of similar observations. If we can identify observations
    that are like each other but distinct from those in another cluster, we can infer
    that these two clusters represent different conditions. At this stage, three crucial
    aspects come into play:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是数据分析中的一个基本概念，旨在识别数据集中有意义的分组或模式。它涉及根据数据点之间的相似性或彼此的邻近性将数据点划分为不同的聚类。在聚类和分类中，我们的目标都是发现使我们可以将观测值分配到正确类别的潜在规则。然而，聚类与分类不同，因为它还需要识别有意义的类别子集。在分类中，我们受益于目标变量，它在训练集中提供了分类信息。相比之下，聚类缺乏此类附加信息，需要通过分析数据的空间分布来推断类别。数据中的密集区域对应于相似观测值的组。如果我们能识别出彼此相似但与另一个聚类中的观测值不同的观测值，我们可以推断这两个聚类代表不同的条件。在这个阶段，有三个关键方面发挥作用：'
- en: How to measure similarity
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何衡量相似度
- en: How to find centroids and centers
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何找到质心和中心
- en: How to define a grouping
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何定义分组
- en: The notion of distance and the definition of a grouping are the fundamental
    components that characterize a clustering algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 距离的概念和分组定义是描述聚类算法的基本组成部分。
- en: How to measure similarity
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何测量相似性
- en: Clustering involves the identification of data groupings based on the measure
    of proximity between elements. Proximity can refer to either similarity or dissimilarity.
    Thus, the definition of a data group relies on how we define similarity or dissimilarity.
    In many approaches, proximity is conceptualized in terms of distance within a
    multidimensional space. The effectiveness of clustering algorithms heavily depends
    on the choice of metric and how distance is calculated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类涉及根据元素之间的邻近度度量来识别数据分组。邻近度可以指相似性或差异性。因此，数据分组的定义依赖于我们如何定义相似性或差异性。在许多方法中，邻近度是在多维空间内的距离概念化的。聚类算法的有效性在很大程度上取决于指标的选择以及距离的计算方式。
- en: Clustering algorithms group elements based on their mutual distances, where
    membership in a particular set is determined by the proximity of an element to
    other members of the same set. Therefore, a set of observations forms a cluster
    when they tend to be closer to each other compared to observations in other sets.
    So, what do we mean by similarity and dissimilarity? **Similarity** refers to
    a numerical measure of how alike two objects are. Higher similarity values indicate
    greater resemblance between objects. Similarities are typically non-negative and
    often range from 0 (no similarity) to 1 (complete similarity). In *Figure 4**.1*,
    the difference between intercluster and intracluster distances is shown.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法根据元素之间的相互距离来分组，一个特定集合的成员资格由一个元素与其他同一集合成员的邻近度决定。因此，当一组观测值相对于其他集合中的观测值更接近时，它们形成了一个簇。那么，我们所说的相似性和差异性是什么意思呢？**相似性**是指两个对象之间相似程度的数值度量。较高的相似性值表示对象之间有更大的相似性。相似性通常是正的，通常从
    0（没有相似性）到 1（完全相似性）的范围。在 *图 4*.1 中，展示了聚类间和聚类内距离的差异。
- en: '![Figure 4.1 – Difference between inter cluster and intra cluster distances](img/B21156_04_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 聚类间和聚类内距离的差异](img/B21156_04_01.jpg)'
- en: Figure 4.1 – Difference between inter cluster and intra cluster distances
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 聚类间和聚类内距离的差异
- en: On the contrary, **dissimilarity** represents a numerical measure of how different
    two objects are. Lower dissimilarity values indicate greater similarity between
    objects. Dissimilarity is sometimes referred to as distance. Like similarity,
    dissimilarity values may also fall within the interval *[0,1]*, but it is common
    for them to range from 0 to ∞.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**差异性**表示两个对象之间差异的数值度量。较低的差异性值表示对象之间有更大的相似性。差异性有时也被称为距离。与相似性一样，差异性值也可能落在区间
    *[0,1]* 内，但它们通常从 0 到 ∞ 范围内。
- en: 'Dissimilarities between data objects can be quantified using distance metrics.
    Distances possess specific properties and can be employed to measure dissimilarity.
    For example, the **Euclidean distance**, denoted as *d*, can be utilized to measure
    the distance between two points, *x* and *y*, using the following formula:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用距离指标来量化数据对象之间的差异性。距离具有特定的属性，可以用来测量差异性。例如，**欧几里得距离**，用 *d* 表示，可以使用以下公式来测量两点
    *x* 和 *y* 之间的距离：
- en: d(x, y) = √ _ ∑ k=1 n (x i − y i) 2
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: d(x, y) = √(∑(k=1 to n) (x_i - y_i)^2)
- en: In a two-dimensional plane, the Euclidean distance is the shortest distance
    between two points, represented by a straight line connecting them. This distance
    is calculated by taking the square root of the sum of the squared differences
    between the elements of two vectors, as shown in the earlier formula.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维平面上，欧几里得距离是两点之间的最短距离，由连接它们的直线表示。这个距离是通过取两个向量元素平方差的和的平方根来计算的，如前面的公式所示。
- en: 'However, distance can be measured using various other metrics, such as the
    Minkowski distance. The **Minkowski distance** metric is a generalization of the
    Euclidean distance and can be expressed as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以使用各种其他指标来测量距离，例如闵可夫斯基距离。**闵可夫斯基距离**指标是欧几里得距离的推广，可以表示如下：
- en: d(x, y) = (∑ k=1 n |x i − y i| r) 1 _ r
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: d(x, y) = (∑(k=1 to n) |x_i - y_i|^r)^(1/r)
- en: Another option is to utilize the **Manhattan distance** metric, which is derived
    from the Minkowski distance metric by setting *r = 1*. Additionally, there is
    the **cosine distance**, which incorporates the dot product scaled by the product
    of the Euclidean distances from the origin. It quantifies the angular distance
    between two vectors while disregarding their scale. Once we have selected a suitable
    metric for our specific case, we can proceed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是利用**曼哈顿距离**度量，它是通过将**r = 1**设置在**闵可夫斯基距离**度量上得到的。此外，还有**余弦距离**，它结合了由原点到欧几里得距离的乘积缩放的点积。它量化了两个向量之间的角度距离，同时不考虑它们的规模。一旦我们为我们的特定情况选择了一个合适的度量，我们就可以继续进行。
- en: Choosing an appropriate similarity metric for clustering holds great significance
    as it directly influences the quality and comprehensibility of your clustering
    outcomes. The selection of the metric should be in harmony with both the inherent
    traits of your data and the objectives of your clustering analysis. The default
    distance used in most clustering tools is Euclidean distance. However, this may
    be inappropriate in the presence of very noisy data or with a non-Gaussian distribution.
    In these situations, a robust alternative is the Manhattan distance, although
    it is important to keep in mind that robustness involves a loss of information,
    at least to some extent. In any case, you can choose between different dissimilarity
    measures based on the type of data and research objectives.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的相似性度量对于聚类具有重要意义，因为它直接影响到聚类结果的质量和可理解性。度量选择应与数据的固有特性和聚类分析的目标相协调。大多数聚类工具中使用的默认距离是欧几里得距离。然而，在非常嘈杂的数据或非高斯分布的情况下，这可能是不可取的。在这些情况下，一个稳健的替代方案是曼哈顿距离，尽管需要记住，稳健性涉及至少在一定程度上损失信息。在任何情况下，您可以根据数据类型和研究目标选择不同的相似性度量。
- en: So far, we have examined different formulas for calculating distances between
    two objects. However, what if the objects under analysis are nominal instead of
    numerical? For nominal data, which can be represented as simple sequences or strings,
    various distance measures can be employed. One possible distance between two strings
    is determined by counting the number of symbols that differ between the strings.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了计算两个对象之间距离的不同公式。然而，如果分析的对象是名义的而不是数值的，会怎样呢？对于名义数据，它可以表示为简单的序列或字符串，可以采用各种距离度量。两个字符串之间的一种可能的距离是通过计算字符串之间不同的符号数量来确定的。
- en: How to find centroids and centers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何找到质心和中心
- en: Centroids and centers are essential concepts in clustering that represent the
    central points or locations within each cluster. The **centroid** is a point within
    the feature space that represents the average position of all the data points
    belonging to the associated cluster. It serves as a type of center of gravity
    for the cluster and, typically, does not coincide with any specific data point
    in the dataset. For example, in **k-means clustering**, a centroid is the mean
    or average position of all the data points within a cluster. It serves as a representative
    point that minimizes the sum of squared distances between the data points and
    the centroid. The centroid is computed by taking the average of the feature values
    across all data points within the cluster. During the iterative process of k-means,
    data points are assigned to the cluster whose centroid they are closest to, and
    the centroids are updated accordingly until convergence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 质心和中心是聚类中的基本概念，它们代表每个簇中的中心点或位置。**质心**是特征空间中的一个点，代表属于相关簇的所有数据点的平均位置。它作为簇的重心，通常不与数据集中的任何特定数据点重合。例如，在**k-means聚类**中，质心是簇内所有数据点的平均值或平均位置。它作为一个代表点，最小化了数据点与质心之间的平方距离之和。质心是通过计算簇内所有数据点的特征值的平均值来计算的。在k-means的迭代过程中，数据点被分配到它们最近的质心所在的簇，并且相应地更新质心，直到收敛。
- en: The term *centers* is often used in other clustering algorithms, such as hierarchical
    clustering or density-based clustering. The centers can represent central points
    within clusters, but the computation method might differ depending on the algorithm.
    In **hierarchical clustering**, centers can be computed as the mean, median, or
    another representative point within each cluster. In **density-based clustering**,
    the center of a cluster can be defined as the data point with the highest density
    or as the medoid, which is the most centrally located point.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*中心*常用于其他聚类算法中，例如层次聚类或基于密度的聚类。中心可以表示簇内的中心点，但计算方法可能因算法而异。在**层次聚类**中，中心可以计算为每个簇的均值、中位数或其他代表性点。在**基于密度的聚类**中，簇的中心可以定义为密度最高的数据点或中位数，即最中心的位置点。
- en: Both centroids and centers provide a way to summarize and represent clusters
    in clustering analysis. They offer insights into the characteristic properties
    of each cluster and can be used to classify new data points based on their proximity
    to these central points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 中心点和中心都为聚类分析中总结和表示簇提供了一种方法。它们揭示了每个簇的特征属性，并可以根据它们与这些中心点的接近程度来对新数据点进行分类。
- en: How to define a grouping
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何定义分组
- en: A cluster is a set of objects that are like each other but are dissimilar to
    objects contained in other clusters; a cluster can therefore be defined in terms
    of internal cohesion (homogeneity) and external cohesion (separation). The goal
    is to group – or segment – collections of objects into subsets, called clusters,
    such that objects in the same cluster have similar characteristics to each other,
    unlike elements assigned to different clusters. In this sense, the distance between
    points of the same cluster must be minimized while the distance between points
    belonging to different clusters must be maximized.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 簇是一组彼此相似但与其他簇中的对象不相似的对象；因此，簇可以定义为内部凝聚力（同质性）和外部凝聚力（分离）。目标是把一组对象分成子集，称为簇，使得同一簇中的对象彼此具有相似的特征，而分配到不同簇的元素则不同。从这个意义上说，必须最小化同一簇中点的距离，同时最大化属于不同簇的点的距离。
- en: In cluster analysis, the definition of a measure that quantifies the degree
    of similarity between objects is crucial (see the measures in *Figure 4**.1*).
    The choice of this measurement greatly influences the formation of one object
    partition over another. When dealing with numeric data, the similarity measure
    is referred to as a distance function. In this case, the goal is to minimize the
    distance function because objects that are closer together exhibit similar characteristics
    and belong to the same cluster. On the other hand, when working with textual data,
    the similarity measure is used instead. In these cases, the aim is to maximize
    similarity since texts that are more similar are closer together and belong to
    the same cluster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类分析中，定义一个量化对象之间相似度程度的度量标准至关重要（参见*图4**.1*中的度量标准）。这个测量标准的选择极大地影响了对象分区形成的选择。当处理数值数据时，相似度度量被称为距离函数。在这种情况下，目标是使距离函数最小化，因为彼此更接近的对象表现出相似的特征，属于同一簇。另一方面，当处理文本数据时，使用相似度度量。在这些情况下，目标是最大化相似度，因为更相似的文字更接近，属于同一簇。
- en: A visual representation can help reveal the cohesion properties of clusters
    without explicitly and rigorously defining them. It is important to note that
    there is no universal definition that applies to every situation. Attempts to
    precisely quantify homogeneity and separation using explicit numerical indices
    have resulted in numerous and diverse criteria.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可视表示可以帮助揭示簇的凝聚力属性，而无需明确和严格地定义它们。重要的是要注意，没有适用于每种情况的通用定义。试图使用显式的数值指标精确量化同质性和分离的努力导致了众多和多样化的标准。
- en: 'Once a distance measure has been selected, the next step is to determine how
    to form groups or clusters. There are two main families of clustering algorithms:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了距离度量，下一步就是确定如何形成组或簇。主要有两种聚类算法家族：
- en: '**Hierarchical clustering**: This approach constructs a hierarchical structure
    or taxonomy of the data. It involves creating a tree-like structure, often referred
    to as a **dendrogram**, which represents the relationships between the data points.
    Hierarchical clustering can be agglomerative, starting with individual data points
    and iteratively merging them into clusters, or divisive, beginning with one large
    cluster and recursively splitting it into smaller clusters.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：这种方法构建数据的层次结构或分类体系。它涉及创建一个树状结构，通常称为**树状图**，它表示数据点之间的关系。层次聚类可以是聚合的，从单个数据点开始，迭代地将它们合并到聚类中，或者可以是分裂的，从一个大聚类开始，递归地将其分割成更小的聚类。'
- en: '**Partitioning clustering**: In this approach, the data space is divided into
    non-overlapping sub-zones or partitions. Each data point belongs to exactly one
    partition, and the union of all partitions covers the entire space. Partitioning
    algorithms aim to optimize a specific criterion, such as minimizing the within-cluster
    variance or maximizing the separation between clusters. Popular partitioning algorithms
    include k-means, where data points are assigned to the cluster with the nearest
    centroid, and **Gaussian mixture models** (**GMMs**), which represent clusters
    as a mixture of Gaussian distributions.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**划分聚类**：在这种方法中，数据空间被划分为非重叠的子区域或分区。每个数据点属于恰好一个分区，所有分区的并集覆盖整个空间。划分算法旨在优化特定的标准，例如最小化聚类内的方差或最大化聚类之间的分离。流行的划分算法包括k-means，其中数据点被分配到最近的质心所在的聚类，以及**高斯混合模型**（**GMMs**），它将聚类表示为高斯分布的混合。'
- en: Both hierarchical clustering and partitioning clustering methods have their
    own advantages and applications. The choice between the two depends on the nature
    of the data, the desired interpretation of the results, and the specific goals
    of the clustering analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类和划分聚类方法都有它们自己的优点和应用。选择哪一种取决于数据的性质、期望的结果解释以及聚类分析的具体目标。
- en: Understanding hierarchical clustering
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解层次聚类
- en: Hierarchical clustering is a method of clustering that creates a hierarchy or
    tree-like structure of clusters. It iteratively merges or splits clusters based
    on the similarity or dissimilarity between data points. The resulting structure
    is often represented as a dendrogram, which visualizes the relationships and similarities
    among the data points.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种创建聚类层次或树状结构的聚类方法。它根据数据点之间的相似性或不相似性迭代地合并或分割聚类。结果结构通常表示为树状图，它可视化数据点之间的关系和相似性。
- en: 'There are two main types of hierarchical clustering:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类主要有两种类型：
- en: '**Agglomerative hierarchical clustering**: This starts with each data point
    considered as an individual cluster and progressively merges similar clusters
    until all data points belong to a single cluster. At the beginning, each data
    point is treated as a separate cluster, and in each iteration, the two most similar
    clusters are merged into a larger cluster. This process continues until all data
    points are in one cluster. The merging process is guided by a distance or similarity
    measure, such as a Euclidean distance or correlation.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合层次聚类**：这种方法从每个数据点被视为一个单独的聚类开始，并逐步合并相似的聚类，直到所有数据点都属于一个单一的聚类。一开始，每个数据点被视为一个单独的聚类，在每次迭代中，两个最相似的聚类被合并成一个更大的聚类。这个过程一直持续到所有数据点都在一个聚类中。合并过程由距离或相似度度量指导，例如欧几里得距离或相关性。'
- en: '**Divisive hierarchical clustering**: This starts with all data points in a
    single cluster and recursively divides the cluster into smaller subclusters until
    each data point is in its own cluster. In each iteration, a cluster is split into
    two or more subclusters based on a dissimilarity measure. This process continues
    until each data point forms its own cluster or until a stopping criterion is met.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂层次聚类**：这种方法从所有数据点在一个单一的聚类开始，递归地将聚类分割成更小的子聚类，直到每个数据点都在自己的聚类中。在每次迭代中，一个聚类根据差异度度量被分割成两个或更多的子聚类。这个过程一直持续到每个数据点形成自己的聚类或直到满足停止标准。'
- en: Hierarchical clustering has several advantages. It does not require specifying
    the number of clusters in advance, as the desired number of clusters can be determined
    by cutting the dendrogram at a particular level. It also provides a visual representation
    of the clustering structure, allowing for interpretation and exploration of the
    data. However, hierarchical clustering can be computationally intensive for large
    datasets, and the choice of distance or similarity measure and the method of merging
    or splitting clusters can influence the results. Overall, hierarchical clustering
    is a flexible and widely used technique for exploring and understanding the underlying
    structure of a dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类有几个优点。它不需要事先指定聚类数量，因为所需的聚类数量可以通过在特定级别切割树状图来确定。它还提供了聚类结构的可视化表示，允许对数据进行解释和探索。然而，对于大型数据集，层次聚类可能计算密集，距离或相似度量的选择以及合并或拆分聚类的策略可能会影响结果。总的来说，层次聚类是一种灵活且广泛使用的探索和理解数据集潜在结构的技术。
- en: In MATLAB, hierarchical clustering is performed using several functions to create
    a cluster tree or dendrogram. The process involves grouping the data and creating
    a multilevel hierarchy where clusters at one level are joined as clusters at the
    next level. The *Statistics and Machine Learning Toolbox* provides the necessary
    functions to perform agglomerative hierarchical clustering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，层次聚类是通过使用几个函数来创建聚类树或树状图来执行的。该过程涉及对数据进行分组并创建一个多级层次结构，其中某一层的聚类在下一层作为聚类连接。*统计与机器学习工具箱*提供了执行凝聚层次聚类的必要函数。
- en: 'The following functions are commonly used in the process:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在此过程中常用的函数：
- en: '`pdist`: This function calculates pairwise distances between data points based
    on a chosen distance metric. It takes the data matrix as input and returns a distance
    matrix.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pdist`: 此函数根据选择的距离度量计算数据点之间的成对距离。它接受数据矩阵作为输入，并返回距离矩阵。'
- en: '`linkage`: The `linkage` function is used to compute the linkage matrix, which
    represents the distances between clusters. It takes the distance matrix as input
    and performs the linkage calculation based on a specified method, such as `single`,
    `complete`, or `average`. `single` linkage, often referred to as the nearest neighbor
    method, employs the minimum distance between objects in the two clusters. `complete`
    linkage, also known as the farthest neighbor method, utilizes the maximum distance
    between objects in the two clusters. Finally, `average` linkage computes the mean
    distance between all pairs of objects in any two clusters.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linkage`: `linkage`函数用于计算连接矩阵，该矩阵表示聚类之间的距离。它接受距离矩阵作为输入，并根据指定的方法（如`single`、`complete`或`average`）执行连接计算。`single`连接，通常称为最近邻方法，使用两个聚类中对象之间的最小距离。`complete`连接，也称为最远邻方法，使用两个聚类中对象之间的最大距离。最后，`average`连接计算任意两个聚类中所有成对对象之间的平均距离。'
- en: '`cluster`: The `cluster` function is responsible for forming the clusters based
    on the linkage matrix. It takes the linkage matrix and a cutoff parameter as input
    and returns the cluster assignments for each data point.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster`: `cluster`函数负责根据连接矩阵形成聚类。它接受连接矩阵和截止参数作为输入，并返回每个数据点的聚类分配。'
- en: '`dendrogram`: Finally, the `dendrogram` function is used to visualize the cluster
    tree or dendrogram. It takes the linkage matrix as input and plots the dendrogram,
    showing the merging of clusters at each level.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dendrogram`: 最后，使用`dendrogram`函数来可视化聚类树或树状图。它接受连接矩阵作为输入，并绘制树状图，显示每个级别的聚类合并。'
- en: By utilizing these functions in sequence, the `clusterdata` function in MATLAB
    performs agglomerative hierarchical clustering and generates the dendrogram. Understanding
    the sequence of function calls can be beneficial for comprehending the entire
    process and the flow of the hierarchical clustering algorithm.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依次利用这些函数，MATLAB中的`clusterdata`函数执行层次聚类并生成树状图。理解函数调用的顺序对于理解整个过程和层次聚类算法的流程是有益的。
- en: Let’s explore how clustering can be performed in MATLAB by using the provided
    functions. Clustering involves identifying groups within a dataset based on proximity
    measures, which can refer to similarity or dissimilarity. Here’s an example using
    MATLAB. In MATLAB, the `pdist` function is used to compute the distance between
    every pair of objects in a dataset. If the dataset has *k* objects, there are
    *k*(k – 1)/2* pairs in total. The `pdist()` function calculates the Euclidean
    distance by default for a *k-by-p* data matrix, where rows correspond to observations
    and columns correspond to variables. It returns a row vector of length *k(k –
    1)/2*, representing pairs of observations in the source matrix. The distances
    are arranged in the order *(2,1), (3,1), ..., (k,1), (3,2), ..., (k,2), ..., (k,k–1))*.
    To obtain the distance matrix, you can use the `squareform()` function, which
    we’ll cover later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用提供的函数来探索如何在 MATLAB 中执行聚类。聚类涉及根据邻近度度量识别数据集中的组，这些度量可以指相似性或差异性。以下是一个使用 MATLAB
    的示例。在 MATLAB 中，`pdist` 函数用于计算数据集中每对对象之间的距离。如果数据集有 *k* 个对象，则总共有 *k*(k – 1)/2* 对。`pdist()`
    函数默认计算 *k-by-p* 数据矩阵的欧几里得距离，其中行对应于观测值，列对应于变量。它返回一个长度为 *k(k – 1)/2* 的行向量，表示源矩阵中的观测值对。距离按顺序排列为
    *(2,1), (3,1), ..., (k,1), (3,2), ..., (k,2), ..., (k,k–1))*. 要获取距离矩阵，您可以使用我们稍后将要介绍的
    `squareform()` 函数。
- en: By default, `pdist()` calculates the Euclidean distance, but there are various
    distance metrics available. You can specify a different metric by selecting one
    of the options provided in the function’s syntax. Some available choices include
    `euclidean`, `squaredeuclidean`, `seuclidean`, `cityblock`, `minkowski`, `chebychev`,
    `mahalanobis`, `cosine`, `correlation`, `spearman`, `hamming`, and `jaccard`.
    Additionally, you can create a custom distance function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`pdist()` 计算欧几里得距离，但有多种距离度量可供选择。您可以通过选择函数语法中提供的选项之一来指定不同的度量。一些可用的选择包括
    `euclidean`、`squaredeuclidean`、`seuclidean`、`cityblock`、`minkowski`、`chebychev`、`mahalanobis`、`cosine`、`correlation`、`spearman`、`hamming`
    和 `jaccard`。此外，您还可以创建一个自定义的距离函数。
- en: Let's then analyze the MATLAB code line by line.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们逐行分析 MATLAB 代码。
- en: 'As an example, let’s consider a matrix with six points in a Cartesian plane,
    represented by pairs of coordinates (*x*, *y*):'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为例子，让我们考虑一个在笛卡尔平面上有六个点的矩阵，这些点由坐标对 (*x*, *y*) 表示：
- en: '[PRE0]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s apply the `pdist` function to calculate the distances between pairs
    of points. The `pdist` function will return this distance information in a vector,
    where each element represents the distance between a pair of points:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们应用 `pdist` 函数来计算点对之间的距离。`pdist` 函数将返回一个向量，其中每个元素代表一对点之间的距离：
- en: '[PRE1]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following matrix was printed:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的矩阵被打印出来：
- en: '[PRE2]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It is worth noting that the distance matrix is symmetric, indicating that the
    distance between point *i* and point *j* is the same as the distance between point
    *j* and point *i*. In certain cases, it is beneficial to normalize the values
    in the dataset before calculating the distance information. This normalization
    is important because variables in raw data can be measured on different scales.
    These variations in measurement scales can distort proximity calculations. To
    address this, the `zscore()` function can be used to standardize all values in
    the dataset to a common scale.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值得注意的是，距离矩阵是对称的，表示点 *i* 和点 *j* 之间的距离与点 *j* 和点 *i* 之间的距离相同。在某些情况下，在计算距离信息之前对数据集中的值进行归一化是有益的。这种归一化很重要，因为原始数据中的变量可能是在不同的尺度上测量的。这些测量尺度的变化可能会扭曲邻近度计算。为了解决这个问题，可以使用
    `zscore()` 函数将数据集中的所有值标准化到公共尺度。
- en: 'As previously mentioned, the concepts of distance and how to define groups
    are the key components of a clustering algorithm. After calculating the proximity
    between objects in the dataset, the next step is to determine how these objects
    should be grouped into clusters. This is where the `linkage` function comes into
    play. Utilizing the distance measure generated by the `pdist()` function, the
    `linkage` function links pairs of objects that are close together, forming binary
    clusters. This process continues as the binary clusters are merged with other
    objects, creating larger clusters, until all objects in the original dataset are
    linked together in a hierarchical tree structure:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，距离的概念以及如何定义组是聚类算法的关键组成部分。在计算数据集中对象之间的邻近度之后，下一步是确定如何将这些对象分组到聚类中。这就是 `linkage`
    函数发挥作用的地方。利用 `pdist()` 函数生成的距离度量，`linkage` 函数将邻近的对象配对，形成二元聚类。随着二元聚类与其他对象合并，形成更大的聚类，这个过程持续进行，直到原始数据集中的所有对象都在层次树结构中相互连接：
- en: '[PRE3]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With the usage of the `linkage` function, we have completed a significant portion
    of the task. This function has identified potential groupings based on the previously
    calculated distance measurements. To gain a deeper understanding of how the function
    operates, let’s examine the results in `GroupsMatrix`.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `linkage` 函数，我们已经完成了任务的重要部分。此函数基于先前计算的距离度量识别了潜在的分组。为了更深入地了解函数的工作原理，让我们检查
    `GroupsMatrix` 中的结果。
- en: In `GroupsMatrix`, each row represents a newly formed cluster. By analyzing
    this diagram we can understand how the points are identified to group them into
    different groups. The first two columns indicate the points that have been linked,
    particularly in the earlier stages, while the subsequent columns denote the newly
    created clusters. It is important to note that hierarchical clustering, of the
    agglomerative type, begins with small clusters and proceeds to incorporate additional
    elements, generating larger clusters. The third column of the matrix presents
    the distance between these points, providing further insights into the clustering
    process. In *Figure 4**.2*, we can see the dendrogram of the clustering method.
    By analyzing this diagram, we can understand how the points are treated to group
    them into different groups.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `GroupsMatrix` 中，每一行代表一个新形成的聚类。通过分析此图，我们可以了解如何识别点并将它们分组到不同的组中。前两列表示已连接的点，尤其是在早期阶段，而后续的列表示新创建的聚类。需要注意的是，层次聚类（聚合类型）从小的聚类开始，然后逐步合并额外的元素，生成更大的聚类。矩阵的第三列展示了这些点之间的距离，为聚类过程提供了更深入的见解。在
    *图 4**.2* 中，我们可以看到聚类方法的树状图。通过分析此图，我们可以了解如何处理点以将它们分组到不同的组中。
- en: The `linkage` function is responsible for utilizing the distances calculated
    by the `pdist()` function to determine the order in which clusters are formed.
    Additionally, this function can calculate the distances between newly merged clusters.
    By default, the `linkage` function employs the single `linkage` method. However,
    there are several other methods available for use, including `average`, `centroid`,
    `complete`, `median`, `single`, `ward`, and `weighted`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`linkage` 函数负责利用 `pdist()` 函数计算的距离来确定聚类形成的顺序。此外，此函数还可以计算新合并的聚类之间的距离。默认情况下，`linkage`
    函数采用单链接方法。然而，还有其他几种方法可供使用，包括 `average`、`centroid`、`complete`、`median`、`single`、`ward`
    和 `weighted`。'
- en: 'Finally, to visualize the hierarchical binary cluster tree, we can utilize
    the `dendrogram()` function. This function generates a dendrogram plot, which
    consists of U-shaped lines connecting data points in a hierarchical tree structure.
    The height of each U-shaped line represents the distance between the two connected
    data points:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了可视化层次二元聚类树，我们可以使用 `dendrogram()` 函数。此函数生成一个树状图，它由连接层次树结构中数据点的 U 形线组成。每条
    U 形线的高度代表两个连接数据点之间的距离：
- en: '[PRE4]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In *Figure 4**.2*, we can see a dendrogram of a hierarchical cluster obtained
    from a series of points on the Cartesian plane.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 *图 4**.2* 中，我们可以看到一个从笛卡尔平面上的一系列点获得的层次聚类的树状图。
- en: '![Figure 4.2 – Dendrogram of a hierarchical cluster](img/B21156_04_02.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 层次聚类树状图](img/B21156_04_02.jpg)'
- en: Figure 4.2 – Dendrogram of a hierarchical cluster
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 层次聚类树状图
- en: A dendrogram is a graphical representation in the form of a branching diagram
    that illustrates the similarities among a group of entities. The horizontal axis
    of the dendrogram represents the elements being analyzed, while the vertical axis
    indicates the level of distance at which the fusion of two elements occurs. The
    strength of the relationships between two elements is depicted by the distance
    between the element’s corresponding vertical lines and the *x* axis.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图是一种以分支图形式表示的图形表示，它说明了实体组之间的相似性。树状图的水平轴表示正在分析的项目，而垂直轴表示两个元素融合发生的距离级别。两个元素之间关系的强度通过元素对应垂直线与*x*轴之间的距离来表示。
- en: To understand the relationship between two elements in a dendrogram, we can
    trace a path from one element to another by following the tree diagrams and selecting
    the shortest path. The distance from the starting point to the outermost horizontal
    line crossed by the path reflects the degree of similarity between the two elements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解树状图中两个元素之间的关系，我们可以通过遵循树状图并选择最短路径，从一个元素追踪到另一个元素。路径从起点到路径交叉的最外侧水平线的距离反映了两个元素之间的相似程度。
- en: After building the dendrogram, we make a horizontal cut of the structure. All
    resulting subbranches below the horizontal cut represent a single cluster at the
    top level of the system and determine the corresponding cluster membership for
    each data sample (*Figure 4**.2*).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建树状图之后，我们对结构进行水平切割。所有位于水平切割下方的子分支代表系统顶层的一个单独的聚类，并确定每个数据样本的对应聚类成员资格（*图4.2*）。
- en: 'Finally, we can verify how hierarchical clustering is able to group objects.
    In a dendrogram, the disparity between branches is reflected by their respective
    heights: the taller the branch, the greater the dissimilarity. This vertical measurement
    is referred to as the cophenetic distance, representing the dissimilarity between
    the two objects that form the cluster. To evaluate and compare the cophenetic
    distances between two objects, the `cophenet()` function can be employed. Verifying
    the effectiveness of the dissimilarity measurement can be accomplished by comparing
    the outcomes of clustering the same dataset using various distance calculation
    methods or clustering algorithms. This approach allows us to assess and contrast
    the performance of different techniques. To begin, let’s apply this evaluation
    to the calculations that have just been performed:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以验证层次聚类如何将对象分组。在树状图中，分支之间的差异通过它们各自的高度来反映：分支越高，差异越大。这种垂直测量被称为邻接距离，表示形成聚类的两个对象之间的差异。为了评估和比较两个对象之间的邻接距离，可以使用`cophenet()`函数。通过比较使用不同的距离计算方法或聚类算法对同一数据集进行聚类的结果，可以验证差异度量的有效性。这种方法使我们能够评估和对比不同技术的性能。首先，让我们将这种评估应用于刚刚进行的计算：
- en: '[PRE5]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following results are returned:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE6]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, utilizing the new distance measure generated by the `pdist()` function
    with the cosine metric, we will form clusters by linking objects that are in close
    proximity. However, this time we will employ a different algorithm for calculating
    the distance between clusters. Specifically, we will use the weighted method,
    which computes the weighted average distance. By doing so, we can establish clusters
    based on the closeness of objects, considering their respective weights:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，利用`pdist()`函数生成的由余弦度量生成的新距离度量，我们将通过连接接近的对象来形成聚类。然而，这次我们将使用不同的算法来计算聚类之间的距离。具体来说，我们将使用加权方法，该方法计算加权平均距离。通过这样做，我们可以根据对象的接近程度以及它们各自的权重来建立聚类：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, we can employ the `cophenet()` function to evaluate the clustering
    solution:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`cophenet()`函数来评估聚类解决方案：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following results are printed:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The outcome demonstrates an enhancement in the performance of hierarchical clustering
    by utilizing a different distance metric and linkage method.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果表明，通过使用不同的距离指标和链接方法，层次聚类的性能得到了提升。
- en: Hierarchical clustering is a versatile clustering method with a wide range of
    applications across various domains. Some of the typical areas where hierarchical
    clustering is applied include **image processing**, **text analysis**, **market
    research** and **customer segmentation**, and **ecology and** **environmental
    science**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种多用途的聚类方法，在各个领域都有广泛的应用。层次聚类被应用于一些典型领域，包括**图像处理**、**文本分析**、**市场研究**、**客户细分**以及**生态和环境科学**。
- en: Having introduced the basic concepts of clustering, we can now move on to a
    practical example to discover how to use the tools available in the MATLAB environment
    to correctly perform a partitioning clustering analysis.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了聚类的基礎概念之后，我们现在可以继续到一个实际例子，以了解如何使用MATLAB环境中的工具正确执行分区聚类分析。
- en: Partitioning-based clustering algorithms with MATLAB
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分区聚类的MATLAB算法
- en: '**Partitioning-based clustering** is a type of clustering algorithm that aims
    to divide a dataset into distinct groups or partitions. In this approach, each
    data point is assigned to exactly one cluster, and the goal is to minimize the
    intra-cluster distance while maximizing the inter-cluster distance. The most popular
    partitioning-based clustering algorithms include k-medoids, fuzzy c-means, and
    hierarchical k-means. These algorithms vary in their approach and objectives,
    but they all aim to partition the data into well-separated clusters based on some
    distance or similarity measure.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于分区聚类**是一种旨在将数据集划分为不同组或分区的聚类算法。在此方法中，每个数据点被分配到恰好一个簇，目标是最小化簇内距离同时最大化簇间距离。最流行的基于分区聚类算法包括k-medoids、模糊c-means和层次k-means。这些算法在方法和目标上有所不同，但它们都旨在根据某些距离或相似度度量将数据划分为分离良好的簇。'
- en: Introducing the k-means algorithm
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍k-means算法
- en: One of the most well-known partitioning-based clustering algorithms is k-means.
    In k-means clustering, the algorithm attempts to partition the data into *k* clusters,
    where *k* is a predefined number specified by the user. The algorithm iteratively
    assigns data points to the nearest cluster centroid and recalculates the centroid
    positions until convergence is achieved. The result is a set of *k* clusters,
    each represented by its centroid. The process involves iteratively relocating
    data instances by transferring them from one cluster to another, beginning with
    an initial partitioning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的基于分区聚类算法之一是k-means。在k-means聚类中，算法试图将数据划分为*k*个聚类，其中*k*是由用户预先指定的一个数。算法通过迭代地将数据点分配到最近的簇质心并重新计算质心位置，直到达到收敛。结果是*k*个簇，每个簇由其质心表示。这个过程涉及通过将数据实例从一个簇转移到另一个簇来迭代地重新定位数据实例，从初始分区开始。
- en: The k-means algorithm, developed by James MacQueen in 1967 (MacQueen, J. (1967)
    *Some methods for classification and analysis of multivariate observations*, Proceedings
    of the Fifth Berkeley Symposium On Mathematical Statistics and Probabilities,
    1, 281-296.), is a clustering algorithm that partitions a group of objects into
    *k* clusters based on their attributes. It is a variation of the **expectation-maximization**
    (**EM**) algorithm, with the objective of determining *k* data groups generated
    by Gaussian distributions. Unlike EM, k-means calculates the Euclidean distance
    to measure dissimilarity between data items.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1967年由詹姆斯·麦克昆（James MacQueen）提出的k-means算法（MacQueen, J. (1967) *Some methods
    for classification and analysis of multivariate observations*, Proceedings of
    the Fifth Berkeley Symposium On Mathematical Statistics and Probabilities, 1,
    281-296.），是一种基于属性将一组对象划分为*k*个聚类的聚类算法。它是**期望最大化**（**EM**）算法的一种变体，目标是确定由高斯分布生成的*k*个数据组。与EM不同，k-means通过计算欧几里得距离来衡量数据项之间的相似度。
- en: In k-means, objects are represented as vectors in a vector space. The algorithm
    aims to minimize the total intra-cluster variance or standard deviation. Each
    cluster is represented by a centroid.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means中，对象被表示为向量空间中的向量。该算法旨在最小化总簇内方差或标准差。每个簇由一个质心表示。
- en: 'In the context of a set of observations (*x1, x2, …, xn*), where each observation
    is represented as a d-dimensional real vector, the objective of k-means clustering
    is to divide the *n* observations into *k (≤ n)* sets, denoted as *S = {S1, S2,
    …, Sk}*, in a way that minimizes the within-cluster variance. The objective equation
    is shown here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在一组观测值（*x1, x2, …, xn*）的背景下，其中每个观测值表示为一个d维实向量，k-means聚类的目标是将*n*个观测值划分为*k (≤
    n)*个集合，表示为*S = {S1, S2, …, Sk}*，以最小化簇内方差。目标方程如下：
- en: Min S ∑ i=1 k ∑ x∈S i ‖(x − μ i)‖ 2
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Min S ∑ i=1 k ∑ x∈S i ‖(x − μ i)‖ 2
- en: In the previous equation, μ i is the mean of the points in S i.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，μ i 是S i 中点的平均值。
- en: 'The algorithm follows an iterative procedure:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 算法遵循迭代过程：
- en: Choose the number of clusters, *k*. Choosing the right value for *k*, , is a
    crucial step as it directly impacts the quality of your clustering outcomes. Numerous
    techniques are available to ascertain the optimal *k*, including the elbow method,
    silhouette score, cross-validation, and hierarchical clustering, among others.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择聚类数量，*k*。选择正确的*k*值是一个关键步骤，因为它直接影响到聚类结果的质量。有许多技术可以用来确定最优的*k*，包括肘部方法、轮廓分数、交叉验证和层次聚类等。
- en: Initialize *k* partitions and assign each data point randomly or using heuristic
    information.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化*k*个分区，并随机或使用启发式信息将每个数据点分配给一个分区。
- en: Calculate the centroid for each cluster, which is the mean of all the points
    within the cluster.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个聚类的质心，它是聚类内所有点的平均值。
- en: Calculate the distance between each data point and each cluster centroid.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个数据点与每个聚类质心之间的距离。
- en: Create a new partition by assigning each data point to the cluster with the
    closest centroid.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将每个数据点分配给最近的质心来创建一个新的分区。
- en: Recalculate the centroids for the new clusters.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算新聚类的质心。
- en: Repeat *steps 4* to *6* until the algorithm converges.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤4*到*6*，直到算法收敛。
- en: The goal is to determine the position of *k* centroids, with each centroid representing
    a cluster. The initial positions of the centroids significantly impact the results,
    and it is beneficial to place them as far apart as possible. Each object is then
    associated with the nearest centroid, resulting in an initial grouping. In subsequent
    iterations, new centroids are recalculated as the cluster barycenter based on
    the previous iteration’s results. Data points are reassigned to the new closest
    centroids. This process continues until the centroids no longer move, indicating
    convergence. We can see how *Figure 4**.4* illustrates the positions of *k* centroids
    in the data distribution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是确定*k*个质心的位置，每个质心代表一个聚类。质心的初始位置对结果有显著影响，将它们放置得尽可能远是有益的。然后，每个对象都与最近的质心关联，从而形成一个初始分组。在后续迭代中，根据前一次迭代的结果重新计算新的质心作为聚类质心。数据点被重新分配到新的最近质心。这个过程一直持续到质心不再移动，表明收敛。我们可以看到*图4**.4*如何说明了*k*个质心在数据分布中的位置。
- en: Using k-means in MATLAB
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在MATLAB中使用k-means
- en: 'In MATLAB, the `kmeans()` function is used to perform k-means clustering. This
    function partitions the data into *k* mutually exclusive clusters and returns
    the index of the cluster to which each object is assigned. The clusters are defined
    by the objects and their centroids. The centroid of each cluster is the point
    that minimizes the sum of distances from all objects in that cluster. The calculation
    of cluster centroids varies depending on the chosen distance measure, aiming to
    minimize the sum of the specified measure. Different distance measures and methods
    for minimizing distances can be specified as input parameters to the `kmeans()`
    function. Here is a list summarizing the available distance measures:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，使用`kmeans()`函数进行k均值聚类。此函数将数据划分为*k*个互斥的聚类，并返回每个对象分配到的聚类索引。聚类由对象及其质心定义。每个聚类的质心是使该聚类中所有对象到质心的距离之和最小的点。聚类质心的计算取决于所选的距离度量，目的是最小化指定度量的总和。可以将不同的距离度量以及最小化距离的方法作为输入参数指定给`kmeans()`函数。以下是一个总结可用距离度量的列表：
- en: '`sqeuclidean`: **Squared Euclidean** distance (default). Each centroid is the
    mean of the points in that cluster.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sqeuclidean`：**平方欧几里得**距离（默认）。每个质心是该聚类中点的平均值。'
- en: '`cityblock`: Sum of absolute differences. Each centroid is the component-wise
    median of the points in that cluster.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cityblock`：绝对差分的总和。每个质心是该聚类中点的分位数中值。'
- en: '`cosine`: 1 minus the cosine of the included angle between points. Each centroid
    is the mean of the points in that cluster after normalizing those points to the
    unit of the Euclidean length.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cosine`：1减去点之间包含角度的余弦值。每个质心是在将点归一化到欧几里得长度单位后，该聚类中点的平均值。'
- en: '`correlation`: 1 minus the sample correlation between points. Each centroid
    is the component-wise mean of the points in that cluster after centering and normalizing
    those points with a mean of 0 and a standard deviation of 1.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`相关系数`：1减去点之间的样本相关系数。每个质心是中心化并使用均值为0和标准差为1对点进行归一化后，该簇中点的分量均值。'
- en: '`hamming`: Suitable for binary data only. This measures the proportion of differing
    bits. Each centroid is the component-wise median of the points in that cluster.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`汉明距离`：仅适用于二进制数据。这衡量了不同位的比例。每个质心是该簇中点的分量中位数。'
- en: By default, the `kmeans()` function uses the k-means++ algorithm for cluster
    center initialization and the squared Euclidean metric to determine distances.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`kmeans()`函数使用k-means++算法进行聚类中心初始化，并使用平方欧几里得距离来确定距离。
- en: 'K-means++ enhances the k-means algorithm by introducing a more sophisticated
    approach to initializing cluster centroids. The initialization procedure in k-means++
    comprises the following steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: K-means++通过引入初始化簇质心的更复杂方法来增强k-means算法。k-means++的初始化过程包括以下步骤：
- en: Begin by randomly selecting a single data point as the initial centroid.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先随机选择一个数据点作为初始质心。
- en: For each subsequent centroid (up to *k*), pick the next centroid from the data
    points with a probability that is proportional to the square of the distance between
    each data point and the nearest existing centroid. This probabilistic selection
    ensures that the initial centroids are distributed in a manner that optimally
    spaces them apart and reduces sensitivity to the initial centroid choices.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个后续质心（最多*k*个），从数据点中选择下一个质心，其概率与每个数据点与最近现有质心之间的距离的平方成正比。这种概率选择确保初始质心以最佳方式分布，从而减少对初始质心选择的敏感性。
- en: After initializing the centroids using the k-means++ method, the algorithm proceeds
    in the same fashion as standard k-means. It iteratively assigns data points to
    the closest centroids and updates the centroids until convergence is achieved.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-means++方法初始化质心后，算法的执行方式与标准k-means相同。它通过迭代地将数据点分配给最近的质心，并更新质心，直到达到收敛。
- en: 'K-means++ exhibits a tendency for swifter and more consistent convergence due
    to its enhanced initialization method, which diminishes the likelihood of it becoming
    trapped in local optima. As a consequence, it often yields superior clustering
    quality by virtue of the well-dispersed initial centroids, ultimately resulting
    in more advantageous cluster assignments. Let’s now analyze a practical application
    of this algorithm:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其改进的初始化方法，K-means++倾向于更快和更一致的收敛，这减少了陷入局部最优的可能性。因此，它通常通过分散的初始质心，从而产生更高质量的聚类，最终导致更有利的聚类分配。现在让我们分析这个算法的实际应用：
- en: 'To illustrate the usage of the `kmeans()` function, let’s consider a dataset
    containing measurements of the specific weight and hardness (Mohs scale) of minerals
    extracted from different quarries. These measurements are stored in an `xls` file
    named `Minerals.xls`. We can start by importing the data into the MATLAB workspace:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了说明`kmeans()`函数的用法，让我们考虑一个包含从不同采石场提取的矿物的特定重量和硬度（莫氏硬度）测量的数据集。这些测量值存储在一个名为`Minerals.xls`的`xls`文件中。我们可以首先将数据导入MATLAB工作空间：
- en: '[PRE10]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, let’s look at the imported data by plotting a simple scatter plot:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，让我们通过绘制简单的散点图来查看导入的数据：
- en: '![Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness
    (Mohs scale) of minerals)](img/B21156_04_03.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 数据集分布（x是重量，y是矿物的硬度（莫氏硬度））](img/B21156_04_03.jpg)'
- en: Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness
    (Mohs scale) of minerals)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 数据集分布（x是重量，y是矿物的硬度（莫氏硬度））
- en: By examining *Figure 4**.3*, it appears that the data points are concentrated
    in four distinct regions, each characterized by different values of the two variables
    (*x,y*). This observation suggests that a cluster analysis with a fixed value
    of *k = 4* should be conducted. To ensure reproducibility and obtain consistent
    results, we can set the following parameters accordingly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查*图4**.3*，可以看出数据点集中在四个不同的区域，每个区域由两个变量（*x,y*）的不同值所表征。这一观察表明，应该进行具有固定值*k =
    4*的聚类分析。为了确保可重复性和获得一致的结果，我们可以相应地设置以下参数。
- en: 'We can set the seed to permit the reproducibility of the experiment. The `rgn()`
    function is responsible for controlling the generation of random numbers in MATLAB.
    By setting the seed using `rgn()`, the `rand()`, `randi()`, and `randn()` functions
    will produce a predictable sequence of numbers:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以设置种子以允许实验的可重复性。`rgn()` 函数负责控制 MATLAB 中随机数的生成。通过使用 `rgn()` 设置种子，`rand()`、`randi()`
    和 `randn()` 函数将产生可预测的数字序列：
- en: '[PRE11]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Two variables have been generated: `IdCluster` and `Centroid`. `IdCluster`
    is a vector that stores the predicted cluster indices for each observation in
    the `InputData`. The centroid is a 4-by-2 matrix that represents the final centroid
    locations for the four clusters. By default, the `kmeans()` function utilizes
    the k-means++ algorithm for centroid initialization and employs the squared Euclidean
    distance measure.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已生成两个变量：`IdCluster` 和 `Centroid`。`IdCluster` 是一个向量，用于存储 `InputData` 中每个观察值的预测簇索引。质心是一个
    4 行 2 列的矩阵，表示四个簇的最终质心位置。默认情况下，`kmeans()` 函数使用 k-means++ 算法进行质心初始化，并采用平方欧几里得距离度量。
- en: 'Now that we have assigned every data point in the original dataset to one of
    the four clusters, we can visualize the clusters in a scatter plot:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经将原始数据集中的每个数据点分配到四个簇中的某一个，我们可以在散点图中可视化这些簇：
- en: '[PRE12]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The plot shown in *Figure 4**.4* is printed:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如 *图 4.4* 所示的图表打印如下：
- en: '![Figure 4.4 –  Scatter plot of clusters (x is the weight and y is the hardness
    (Mohs scale) of minerals)](img/B21156_04_04.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 矿物簇的散点图（x 是重量，y 是硬度（莫氏硬度））](img/B21156_04_04.jpg)'
- en: Figure 4.4 – Scatter plot of clusters (x is the weight and y is the hardness
    (Mohs scale) of minerals)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 矿物簇的散点图（x 是重量，y 是硬度（莫氏硬度））
- en: Upon analyzing *Figure 4**.4*, it is evident that the `kmeans()` function has
    successfully identified distinct clusters with clear separation. Each cluster
    is automatically visualized using a different color, making it easy to identify
    them, especially at the boundaries where the data points may overlap. Additionally,
    the use of different markers in the scatter plot aids in distinguishing the data
    points.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 分析 *图 4.4* 后，可以明显看出 `kmeans()` 函数已成功识别出具有清晰分离的独立簇。每个簇都自动使用不同的颜色进行可视化，这使得识别它们变得容易，尤其是在数据点可能重叠的边界处。此外，散点图中使用不同的标记有助于区分数据点。
- en: To highlight the position of each centroid, we have specifically set the marker
    as `x` and adjusted its appearance by setting the line thickness to `4`, marker
    color to `black`, and marker size to `25`. The centroids serve as the center of
    mass for their respective clusters and minimize the sum of squared distances from
    the other data points, as mentioned earlier.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出每个质心的位置，我们特别将标记设置为 `x`，并通过将线粗细设置为 `4`、标记颜色设置为 `black` 和标记大小设置为 `25` 来调整其外观。质心是其各自簇的质量中心，并最小化与其他数据点之间的平方距离，如前所述。
- en: Upon further examination of the figure, it becomes apparent that cluster boundaries
    are not always well defined, particularly in this type of chart. Near the boundaries,
    there are data points that merge together, making it challenging to determine
    which cluster they belong to.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步观察图表后，可以明显看出簇边界并不总是定义得很好，尤其是在这种类型的图表中。在边界附近，有一些数据点合并在一起，这使得确定它们属于哪个簇变得具有挑战性。
- en: To gain a deeper understanding of the separation and boundaries of the clusters,
    we can utilize a `silhouette()` function is specifically designed to generate
    cluster silhouettes based on the provided cluster indices from the `kmeans()`
    function. This function takes a data matrix as input, where each row represents
    a data point and each column represents its coordinates. The cluster indices can
    be categorical variables, numeric vectors, character matrices, or cell arrays
    of character vectors, where each element represents the cluster assignment for
    a data point.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更深入地了解簇的分离和边界，我们可以使用专门设计用于根据 `kmeans()` 函数提供的簇索引生成簇轮廓的 `silhouette()` 函数。此函数接受一个数据矩阵作为输入，其中每一行代表一个数据点，每一列代表其坐标。簇索引可以是分类变量、数值向量、字符矩阵或字符向量的单元数组，其中每个元素代表数据点的簇分配。
- en: 'When using the `silhouette()` function, any `NaN`or empty character vectors
    in the cluster indices are treated as missing values, and the corresponding rows
    in the data matrix are ignored. By default, the `silhouette()` function calculates
    the squared Euclidean distance between the data points in the input matrix. Using
    the silhouette plot, we can gain a better understanding of the separation and
    proximity of the data points within and between clusters, providing valuable insights
    into the quality of the clustering results:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当使用`silhouette()`函数时，任何在簇索引中的`NaN`或空字符向量都被视为缺失值，并且数据矩阵中的对应行将被忽略。默认情况下，`silhouette()`函数计算输入矩阵中数据点之间的平方欧几里得距离。使用轮廓图，我们可以更好地理解数据点在簇内和簇之间的分离和邻近程度，从而为聚类结果的质量提供有价值的见解：
- en: '[PRE13]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The following figure shows a silhouette plot for `kmeans` clustering.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了k-means聚类的轮廓图。
- en: '![Figure 4.5 – Silhouette plot for k-means clustering](img/B21156_04_05.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – k-means聚类的轮廓图](img/B21156_04_05.jpg)'
- en: Figure 4.5 – Silhouette plot for k-means clustering
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – k-means聚类的轮廓图
- en: In *Figure 4**.5*, the *x* axis represents the silhouette values, which range
    from `+1` to `-1`. A silhouette value of `+1` indicates that a data point is significantly
    distant from neighboring clusters, while a value of `0` suggests that the data
    point does not clearly belong to any specific cluster. A silhouette value of `-1`
    implies that the data point is likely assigned to the wrong cluster. The silhouette
    plot returns these values as its first output.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图4**.5中，*x*轴表示轮廓值，范围从`+1`到`-1`。轮廓值为`+1`表示数据点与邻近簇显著距离，而值为`0`则表明数据点不属于任何特定的簇。轮廓值为`-1`意味着数据点可能被分配到了错误的簇。轮廓图将这些值作为其第一个输出返回。
- en: The silhouette plot is useful for assessing the separation distance between
    the resulting clusters. It provides a visual representation of how close each
    data point in one cluster is to the points in neighboring clusters. By examining
    the silhouette plot, we can evaluate the clustering solution and determine whether
    the chosen parameters, such as the number of clusters, are appropriate.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图对于评估结果簇之间的分离距离很有用。它提供了每个簇中的每个数据点与邻近簇中的点的接近程度的视觉表示。通过检查轮廓图，我们可以评估聚类解决方案，并确定选择的参数，如簇数量，是否合适。
- en: In a successful clustering solution, we expect to see high silhouette values,
    indicating that the points are correctly assigned to their respective clusters
    and have minimal connections to neighboring clusters. If a majority of the data
    points have high silhouette values, it indicates a good clustering solution. However,
    if many points have low or negative silhouette values, it suggests that the clustering
    solution needs to be reviewed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个成功的聚类解决方案中，我们期望看到高轮廓值，这表明点被正确地分配到各自的簇中，并且与邻近簇的连接最小。如果大多数数据点具有高轮廓值，则表明聚类解决方案良好。然而，如果许多点具有低或负的轮廓值，则表明聚类解决方案需要重新审视。
- en: Based on the analysis of *Figure 4**.5*, we can determine whether our assumption
    of choosing *k = 4* for the k-means algorithm has produced favorable results.
    In *Figure 4**.5*, there are no clusters with below-average silhouette scores,
    indicating a good separation between the clusters. Additionally, the silhouette
    plots do not exhibit significant fluctuations in size. The uniform thickness of
    the silhouette plot indicates that the clusters are of similar sizes. These observations
    validate our choice of *k = 4* for the number of clusters and confirm that it
    is an appropriate selection.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**图4**.5的分析，我们可以确定选择**k = 4**作为k-means算法的参数是否产生了理想的结果。在**图4**.5中，没有出现低于平均轮廓分数的簇，这表明簇之间有良好的分离。此外，轮廓图在大小上没有显著波动。轮廓图均匀的厚度表明簇的大小相似。这些观察结果验证了我们选择**k
    = 4**作为簇数量的决定，并确认这是一个合适的选择。
- en: 'After analyzing in detail how to easily perform a k-means clustering analysis
    in MATLAB, we can now see a new clustering methodology: the k-medoids algorithm.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细分析了如何在MATLAB中轻松进行k-means聚类分析之后，我们现在可以看到一种新的聚类方法：k-medoids算法。
- en: Grouping data using the similarity measures
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用相似性度量对数据进行分组
- en: The k-medoids algorithm is a variation of the k-means algorithm that uses medoids
    (actual data points) as representatives of each cluster instead of centroids.
    Unlike the k-means algorithm, which calculates the mean of the data points within
    each cluster, the k-medoids algorithm selects the most centrally located data
    point within each cluster as the medoid. This makes k-medoids more robust to outliers
    and suitable for data with non-Euclidean distances.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: k-medoids算法是k-means算法的一种变体，它使用medoids（实际数据点）作为每个簇的代表性，而不是质心。与计算每个簇内数据点平均值的不同，k-medoids算法选择每个簇中位置最中央的数据点作为medoid。这使得k-medoids对异常值更鲁棒，适用于具有非欧几里得距离的数据。
- en: 'Here are some key differences between k-medoids and k-means:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是k-medoids和k-means之间的一些关键区别：
- en: '**Representative points**: In k-medoids, the representatives of each cluster
    are actual data points from the dataset (medoids), while in k-means, the representatives
    are the centroids, which are calculated as the mean of the data points.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代表性点**：在k-medoids中，每个簇的代表性是数据集（medoids）中的实际数据点，而在k-means中，代表性是质心，它是通过计算数据点的平均值得到的。'
- en: '**Distance measure**: The distance measure used in k-means is typically the
    Euclidean distance. On the other hand, k-medoids can handle various distance measures,
    including non-Euclidean distances. This flexibility allows k-medoids to work with
    different types of data, such as categorical or ordinal variables.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离度量**：k-means中使用的距离度量通常是欧几里得距离。另一方面，k-medoids可以处理各种距离度量，包括非欧几里得距离。这种灵活性使得k-medoids能够处理不同类型的数据，例如分类或有序变量。'
- en: '**Robustness to outliers**: The k-medoids algorithm is generally more robust
    to outliers than k-means. Outliers can significantly affect the centroid calculation
    in k-means, pulling the centroid towards them. In k-medoids, the medoids are actual
    data points, so the impact of outliers is limited to the specific cluster that
    contains them.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对异常值的鲁棒性**：k-medoids算法通常比k-means算法对异常值更鲁棒。异常值可以显著影响k-means中的质心计算，将质心拉向它们。在k-medoids中，medoids是实际数据点，因此异常值的影响仅限于包含它们的特定簇。'
- en: '**Computational complexity**: K-means has a lower computational complexity
    compared to k-medoids. The selection of medoids in k-medoids requires the evaluation
    of pairwise distances between all data points, making it computationally more
    expensive, especially for large datasets.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：与k-medoids相比，k-means的计算复杂度较低。k-medoids中medoids的选择需要评估所有数据点之间的成对距离，这使得它计算上更昂贵，尤其是在大型数据集上。'
- en: '**Cluster shape**: The k-means algorithm tends to produce spherical-shaped
    clusters due to the use of Euclidean distance and the calculation of mean centroids.
    K-medoids, on the other hand, can produce clusters of arbitrary shapes, as the
    medoids can be any data point within the cluster.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇形状**：由于使用了欧几里得距离和计算平均质心，k-means算法倾向于产生球形簇。另一方面，k-medoids可以产生任意形状的簇，因为medoids可以是簇内的任何数据点。'
- en: When deciding between k-means and k-medoids, consider the nature of your data,
    the desired robustness to outliers, the availability of appropriate distance measures,
    and the computational resources. If your data contains outliers or non-Euclidean
    distances are more appropriate, k-medoids may be a better choice. Otherwise, if
    the data is well behaved and the Euclidean distance is suitable, k-means can provide
    efficient clustering results.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定选择k-means还是k-medoids时，请考虑数据的性质、对异常值的期望鲁棒性、适当的距离度量的可用性以及计算资源。如果数据包含异常值或非欧几里得距离更合适，k-medoids可能是一个更好的选择。否则，如果数据表现良好且欧几里得距离是合适的，k-means可以提供高效的聚类结果。
- en: Applying k-medoids in MATLAB
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在MATLAB中应用k-medoids
- en: In MATLAB, the `kmedoids()` function is used to perform k-medoids clustering.
    It partitions the observations in a matrix into *k* clusters and returns a vector
    containing the cluster indices for each observation. The input matrix should have
    rows corresponding to points and columns corresponding to variables. Similar to
    the `kmeans()` function, the `kmedoids()` function uses the squared Euclidean
    distance measure and the k-means++ algorithm for selecting initial cluster medoid
    positions by default.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，使用`kmedoids()`函数执行k-medoids聚类。它将矩阵中的观测值划分为*k*个簇，并返回一个包含每个观测值的簇索引的向量。输入矩阵的行应对应于点，列应对应于变量。类似于`kmeans()`函数，`kmedoids()`函数默认使用平方欧几里得距离度量，并使用k-means++算法选择初始簇medoid位置。
- en: Now, let’s see how the `kmedoids()` method is applied in a practical example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `kmedoids()` 方法在实际示例中的应用。
- en: 'Suppose a large distribution company wants to optimize the positioning of its
    offices to improve the efficiency of transferring goods from sorting hubs to peripheral
    locations. The company already has the geographic coordinates of its peripheral
    locations and wants to determine the optimal positions for the sorting hubs. The
    geographic coordinates have been transformed into relative coordinates for compatibility
    reasons with the `kmedoids()` function. This information is stored in a file named
    `PeripheralLocations.xls`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家大型分销公司希望优化其办公室的位置，以提高从分拣中心到边缘位置的货物转移效率。该公司已经拥有其边缘位置的地理坐标，并希望确定分拣中心的最佳位置。出于与
    `kmedoids()` 函数兼容的原因，地理坐标已被转换为相对坐标。这些信息存储在一个名为 `PeripheralLocations.xls` 的文件中：
- en: 'Let’s start by importing this data into the MATLAB workspace:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从将此数据导入 MATLAB 工作空间开始：
- en: '[PRE14]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The following plot displays the distribution of the dataset being imported.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的图显示了正在导入的数据集的分布。
- en: '![Figure 4.6 – Locations of the peripheral offices (geographic coordinates
    of the office: x is the  latitude and y is the longitude)](img/B21156_04_06.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 边缘办公室的位置（办公室的地理坐标：x 是纬度，y 是经度）](img/B21156_04_06.jpg)'
- en: 'Figure 4.6 – Locations of the peripheral offices (geographic coordinates of
    the office: x is the latitude and y is the longitude)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 边缘办公室的位置（办公室的地理坐标：x 是纬度，y 是经度）
- en: Based on an initial visual analysis, three main regions can be identified where
    the company’s peripheral offices are located. This implies the need to designate
    three current offices as hubs for distributing goods to these peripheral locations.
    To determine the position of these future hubs and the associated sites for each,
    a clustering analysis will be conducted.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 根据初步的视觉分析，可以识别出三个主要区域，其中公司的边缘办公室位于这些区域。这意味着需要指定三个现有的办公室作为向这些边缘位置分配货物的中心。为了确定这些未来中心的位置以及每个中心相关的站点，将进行聚类分析。
- en: It is evident that each hub’s location should serve as the center of its respective
    cluster. As these positions are concrete values and must correspond to existing
    offices, utilizing the k-medoids method appears to be a logical decision. K-means
    and k-medoids differ in their centroid determination and sensitivity to outliers.
    K-means minimizes the sum of squared distances between data points and cluster
    centroids, using means as centroids and proving efficient for spherical, equally
    sized clusters. However, it is sensitive to outliers and influenced by initialization.
    In contrast, k-medoids minimizes the sum of dissimilarities, using the most centrally
    located data point (medoid) as the cluster center. This makes k-medoids more robust
    to outliers and noise, offering better performance for non-spherical clusters
    or those with uneven sizes. K-medoids is less sensitive to initialization, but
    its computational cost, involving dissimilarity calculations and medoid selection,
    can be higher than k-means, especially for large datasets.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显然，每个中心的位置应作为其相应聚类的中心。由于这些位置是具体数值，并且必须对应于现有的办公室，因此使用 k-medoids 方法似乎是一个合理的决定。K-means
    和 k-medoids 在质心确定和对异常值敏感度方面有所不同。K-means 通过使用均值作为质心，最小化数据点与聚类质心之间的平方距离之和，对于球形、大小相等的聚类来说效率很高。然而，它对异常值敏感，并且受初始化影响。相比之下，k-medoids
    通过使用最中心位置的数据点（原型）作为聚类中心，最小化差异之和。这使得 k-medoids 对异常值和噪声更稳健，对于非球形聚类或大小不均匀的聚类提供了更好的性能。k-medoids
    对初始化不太敏感，但其计算成本，包括差异计算和原型选择，可能高于 k-means，尤其是对于大型数据集。
- en: '[PRE15]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the previous code, the variables have the following meaning:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，变量具有以下含义：
- en: '`IdCluster` contains cluster indices of each observation'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IdCluster` 包含每个观测的聚类索引'
- en: '`Kmedoid` contains the *k* cluster medoid locations'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kmedoid` 包含 *k* 个聚类原型的位置'
- en: '`SumDist` contains the within-cluster sums of point-to-medoid distances'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SumDist` 包含点到原型距离的聚类内和距离之和'
- en: '`Dist` contains distances from each point to every medoid'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dist` 包含每个点到每个原型的距离'
- en: '`IdClKm` contains cluster indices of each medoid'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IdClKm` 包含每个原型的聚类索引'
- en: '`info` contains information about the options used by the algorithm when executed'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` 包含关于算法执行时使用的选项的信息'
- en: 'Let’s take a look at the information returned by the `kmedoids()` function:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看 `kmedoids()` 函数返回的信息：
- en: '[PRE16]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The command provides information on the algorithm type, metrics used, and the
    number of iterations to achieve the best performance. The algorithm used is called
    **PAM**, which stands for **partitioning around medoids**, and it is a classic
    approach for solving the k-medoids problem. This algorithm closely resembles k-means;
    however, it employs medoids, which are the data points positioned at the center
    of clusters, as cluster representatives rather than centroids. The primary objective
    of PAM is to minimize the total dissimilarity between data points and their corresponding
    medoids.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该命令提供了有关算法类型、使用的指标以及达到最佳性能所需的迭代次数的信息。所使用的算法称为 **PAM**，代表 **partitioning around
    medoids**（围绕类核的划分），这是一种解决 k-medoids 问题的经典方法。该算法与 k-means 非常相似；然而，它使用类核（位于簇中心的点）作为簇的代表，而不是质心。PAM
    的主要目标是使数据点与其对应类核之间的总差异最小化。
- en: The second parameter denotes the method used to determine the initial positions
    of the cluster medoids. The chosen metric is the squared Euclidean distance. The
    command also displays the number of iterations and identifies the best iteration.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个参数表示用于确定簇类核初始位置的算法。所选指标是平方欧几里得距离。命令还显示了迭代次数并标识了最佳迭代。
- en: 'To visualize the result, we can generate plots of the clusters and their respective
    medoids:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了可视化结果，我们可以生成聚类及其相应类核的图表：
- en: '[PRE17]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The scatter plot displays the clustering results along with the indicated position
    of each medoid. We have specifically adjusted the markers to highlight the medoids’
    locations.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 散点图显示了聚类结果以及每个类核的指示位置。我们特别调整了标记以突出显示类核的位置。
- en: '![Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic
    coordinates of the office: x is the latitude and y is the longitude)](img/B21156_04_07.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 聚类散点图和每个类核的位置（办公室的地理坐标：x 是纬度，y 是经度）](img/B21156_04_07.jpg)'
- en: 'Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic
    coordinates of the office: x is the latitude and y is the longitude)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 聚类散点图和每个类核的位置（办公室的地理坐标：x 是纬度，y 是经度）
- en: By analyzing *Figure 4**.7*, it becomes evident that the peripheral sites are
    organized into three clusters as determined by the `kmedoids()` function. Similarly,
    the strategic positioning of the identified hubs is self-explanatory. The distinct
    colors and markers assigned to each cluster facilitate the quick identification
    of each site’s association with its respective cluster.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析 *图 4**.7*，可以明显看出外围站点被 `kmedoids()` 函数组织成三个簇。同样，识别出的枢纽的战略位置也是不言而喻的。为每个簇分配的特定颜色和标记有助于快速识别每个站点与其相应簇的关联。
- en: After having adequately explored the interesting world of clustering, it is
    time to move on to analyze how to reduce the size of data in cases with many features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在充分探索了有趣的聚类世界之后，是时候继续分析如何在具有许多特征的情况下减小数据的大小了。
- en: Discovering dimensionality reduction techniques
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现降维技术
- en: '**Dimensionality reduction** is a technique used in machine learning and data
    analysis to reduce the number of variables or features in a dataset. The goal
    of dimensionality reduction is to simplify the data while retaining important
    information, thereby improving the efficiency and effectiveness of subsequent
    analysis tasks.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**是机器学习和数据分析中用于减少数据集中变量或特征数量的技术。降维的目标是在保留重要信息的同时简化数据，从而提高后续分析任务的效率和效果。'
- en: 'High-dimensional datasets can be challenging to work with due to several reasons:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几个原因，高维数据集可能难以处理：
- en: '**Curse of dimensionality**: As the number of features increases, the data
    becomes more sparse, making it difficult to find meaningful patterns or relationships'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度灾难**：随着特征数量的增加，数据变得更加稀疏，难以找到有意义的模式或关系。'
- en: '**Computational complexity**: Many algorithms and models become computationally
    expensive as the dimensionality of the data increases, requiring more time and
    resources for analysis'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：随着数据维度的增加，许多算法和模型变得计算成本高昂，需要更多的时间和资源进行分析。'
- en: '**Overfitting**: High-dimensional data is more susceptible to overfitting,
    where a model becomes too specialized to the training data and fails to generalize
    well to new data'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：高维数据更容易过拟合，此时模型对训练数据过于专门化，无法很好地推广到新数据。'
- en: 'Dimensionality reduction methods aim to address these challenges by reducing
    the number of features while preserving important information. There are two main
    approaches to dimensionality reduction:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 降维方法旨在通过减少特征数量同时保留重要信息来解决这些挑战。降维主要有两种方法：
- en: '**Feature selection**: This approach selects a subset of the original features
    based on certain criteria. It aims to identify the most informative features that
    contribute significantly to the prediction or analysis task. Common techniques
    for feature selection include correlation analysis, backward/forward selection,
    and regularization methods such as L1 regularization (Lasso).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：这种方法根据某些标准选择原始特征的一个子集。它旨在识别对预测或分析任务有显著贡献的最具信息量的特征。特征选择的常见技术包括相关分析、正向/反向选择和正则化方法，如L1正则化（Lasso）。'
- en: '**Feature extraction**: This approach creates new features by combining or
    transforming the original features. It aims to capture the underlying structure
    or patterns in the data. **Principal component analysis** (**PCA**) is a popular
    feature extraction technique that identifies orthogonal axes in the data that
    explain the maximum variance. Other methods, such as **singular value decomposition**
    (**SVD**) and **non-negative matrix factorization** (**NMF**), can also be used
    for feature extraction.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这种方法通过组合或变换原始特征来创建新特征。它旨在捕捉数据中的潜在结构或模式。"主成分分析"（PCA）是一种流行的特征提取技术，它识别数据中的正交轴，这些轴解释了最大的方差。其他方法，如"奇异值分解"（SVD）和"非负矩阵分解"（NMF），也可以用于特征提取。'
- en: Both feature selection and feature extraction techniques have their advantages
    and disadvantages. The choice of method depends on the specific problem, dataset
    characteristics, and the goals of the analysis.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择和特征提取技术都有其优点和缺点。方法的选择取决于具体问题、数据集特征和分析目标。
- en: By reducing the dimensionality of the data, dimensionality reduction methods
    can lead to benefits such as faster computation, improved model performance, better
    visualization, and enhanced interpretability of the data. However, it is important
    to note that dimensionality reduction is not always necessary or beneficial, and
    it should be applied judiciously after considering the specific requirements and
    characteristics of the problem at hand.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少数据的维度，降维方法可以带来诸如计算更快、模型性能改进、更好的可视化和数据解释性增强等好处。然而，需要注意的是，降维并不总是必要或有益的，并且应在考虑具体问题和特性的基础上谨慎应用。
- en: Introducing feature selection methods
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍特征选择方法
- en: When dealing with high-dimensional datasets, it is often beneficial to reduce
    the number of features to focus only on the most relevant ones, discarding the
    rest. This can result in simpler models that generalize more effectively. Feature
    selection involves the process of identifying the most important features while
    disregarding others during processing and analysis. It is crucial for creating
    a functional model that maintains a manageable number of features. In many cases,
    datasets contain redundant or excessive information, while in other cases, they
    may include incorrect information. Therefore, feature selection helps to eliminate
    such issues and improve the overall quality of the model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理高维数据集时，通常有益于减少特征数量，仅关注最相关的特征，丢弃其余部分。这可能导致更简单的模型，并且能够更有效地泛化。特征选择涉及在处理和分析过程中识别最重要的特征，同时忽略其他特征的过程。对于创建一个保持可管理特征数量的功能模型至关重要。在许多情况下，数据集包含冗余或过多的信息，而在其他情况下，它们可能包含错误信息。因此，特征选择有助于消除这些问题，并提高模型的总体质量。
- en: 'Feature selection enhances the efficiency of model creation by reducing the
    computational load on the CPU and the memory requirements for training algorithms.
    The selection of features serves several purposes:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择通过减少CPU的计算负载和训练算法的内存需求，提高了模型创建的效率。特征选择的目的包括：
- en: '**Preparing clean and interpretable data**: Feature selection helps in selecting
    the most relevant features, resulting in a cleaner and more understandable dataset.
    In MATLAB, cleaning a large dataset involves various data preprocessing steps
    to address missing values, outliers, and inconsistencies.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备干净且可解释的数据**：特征选择有助于选择最相关的特征，从而得到更干净、更易于理解的数据库。在MATLAB中，清理大型数据集涉及各种数据预处理步骤，以解决缺失值、异常值和不一致性。'
- en: '**Simplifying models and improving interpretability**: By focusing on a subset
    of important features, models become simpler and easier to interpret, allowing
    for better insights into the relationships between variables.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化模型并提高可解释性**：通过关注重要特征子集，模型变得简单且易于解释，从而更好地了解变量之间的关系。'
- en: '**Reducing training times**: With fewer features, the training process becomes
    faster, as there is less data to process and analyze.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少训练时间**：由于特征数量减少，训练过程变得更快，因为需要处理和分析的数据更少。'
- en: '**Mitigating overfitting**: Overfitting occurs when a model is too complex
    and fits the training data too closely, leading to poor generalization. Feature
    selection helps in reducing overfitting by decreasing the variance in the model.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减轻过拟合**：当模型过于复杂且与训练数据拟合得太紧密时，就会发生过拟合，导致泛化能力差。特征选择通过减少模型中的方差来帮助减轻过拟合。'
- en: Feature selection involves finding a subset of the original variables, typically
    through an iterative process. By exploring various combinations of variables and
    comparing prediction errors, the subset that produces the minimum error is selected
    as the input for the machine learning algorithm.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择涉及从原始变量中找到子集，通常通过迭代过程实现。通过探索各种变量的组合并比较预测误差，选择产生最小误差的子集作为机器学习算法的输入。
- en: To perform feature selection, appropriate criteria need to be defined beforehand.
    These criteria typically involve minimizing a specific predictive error measure
    for models fitted to different subsets. Feature selection algorithms aim to find
    a subset of predictors that optimally model the measured responses while considering
    constraints, such as required or excluded features and the desired subset size.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行特征选择，需要事先定义适当的准则。这些准则通常涉及最小化针对不同子集拟合的模型的具体预测误差度量。特征选择算法旨在找到一组预测因子，这些因子在考虑约束条件（如所需或排除的特征以及所需的子集大小）的同时，最优地模拟测量的响应。
- en: Feature selection is particularly valuable when the goal is to identify an influential
    subset, especially in cases involving categorical features where numerical transformations
    may not be adequate.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是识别一个有影响力的子集时，特征选择尤其有价值，尤其是在涉及分类特征且数值转换可能不足的情况下。
- en: 'Feature selection methodologies can be grouped into three broad categories:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法可以分为三大类：
- en: '**Filter methods**: These methods evaluate characteristics independently of
    the learning model used subsequently. Statistical measures are calculated for
    each feature, such as the correlation with the output or the relative importance
    of the features themselves. Features are selected based on these measures, without
    considering the specific learning model. Examples of filtering methods include
    correlation analysis, chi-square test, information gain, and mutual information.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤方法**：这些方法独立于随后使用的学习模型评估特征。为每个特征计算统计度量，例如与输出的相关性或特征本身的相对重要性。基于这些度量选择特征，而不考虑特定的学习模型。过滤方法的例子包括相关分析、卡方检验、信息增益和互信息。'
- en: '**Wrapper methods**: These methods use a specific learning model to evaluate
    the quality of different combinations of features. Several subsets of features
    are created, and a learning model is trained and evaluated for each subset. The
    goal is to select the subset of features that produce the best performance according
    to the specified metrics, such as accuracy or mean squared error. However, this
    approach can be computationally expensive, requiring repeated training and evaluation
    of many models. Examples of wrapper methods include forward selection, backward
    selection, and bi-directional elimination.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包装方法**：这些方法使用特定的学习模型来评估不同特征组合的质量。创建了几个特征子集，并为每个子集训练和评估一个学习模型。目标是选择根据指定指标（如准确度或均方误差）产生最佳性能的特征子集。然而，这种方法可能计算成本高昂，需要重复训练和评估许多模型。包装方法的例子包括前向选择、后向选择和双向消除。'
- en: '**Embedded methods**: These methods perform feature selection within the model
    training process itself. The learning algorithms used incorporate feature selection
    mechanisms as part of their optimization process. For example, linear regression
    algorithms with smoothing (such as ridge regression and Lasso regression) tend
    to reduce the importance of less relevant features, helping to automatically select
    the more significant features during training. Examples of embedded methods include
    Lasso, elastic net, and ridge regression.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入式方法**：这些方法在模型训练过程中本身执行特征选择。使用的学习算法将特征选择机制作为其优化过程的一部分。例如，具有平滑的线性回归算法（如岭回归和Lasso回归）倾向于降低不太相关特征的重要性，有助于在训练过程中自动选择更重要的特征。嵌入式方法的例子包括Lasso、弹性网络和岭回归。'
- en: Each category of feature selection methods has specific advantages and limitations.
    The choice of methodology depends on the nature of the problem, the number of
    features, the availability of data, and the required performance. It is important
    to experiment and compare different methodologies to find the most suitable for
    the specific case.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每种特征选择方法类别都有其特定的优点和局限性。方法的选择取决于问题的性质、特征的数量、数据的可用性和所需性能。重要的是要实验和比较不同的方法，以找到最适合特定情况的方案。
- en: Exploring feature extraction algorithms
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索特征提取算法
- en: When dealing with large datasets, it is often necessary to transform them into
    a reduced representation of features. This transformation process is known as
    feature extraction. As covered briefly earlier, feature extraction involves taking
    an initial set of measured data and creating derivative values that capture the
    essential information while eliminating redundant or unnecessary data. The goal
    is to retain the relevant information contained in the original dataset while
    simplifying it and reducing its dimensionality. By extracting meaningful features,
    the resulting representation can be more manageable and efficient for subsequent
    analysis or modeling tasks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量数据集时，通常需要将它们转换成特征的低维表示。这个过程被称为特征提取。如前所述，特征提取包括从一组初始测量数据中提取衍生值，以捕捉关键信息同时消除冗余或不必要的数据。目标是保留原始数据集中包含的相关信息，同时简化数据并降低其维度。通过提取有意义的特征，结果表示可以更易于管理和高效地用于后续的分析或建模任务。
- en: By performing feature extraction, subsequent learning and generalization stages
    are simplified, and in certain cases, it can result in improved interpretations.
    It involves deriving new features from the original ones, aiming to reduce the
    measurement cost, enhance classifier efficiency, and achieve higher classification
    accuracy. When the extracted features are carefully selected, it is anticipated
    that the reduced representation will effectively fulfill the desired task instead
    of using the full-sized input. This enables more efficient and accurate processing
    as the focus shifts to a subset of features that capture the essential information
    required for the task at hand.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行特征提取，后续的学习和泛化阶段得以简化，在某些情况下，它还可以导致更好的解释。这涉及到从原始特征中推导出新的特征，旨在降低测量成本、提高分类器效率并实现更高的分类准确率。当提取的特征被仔细选择时，预计简化后的表示将有效地完成所需的任务，而不是使用全尺寸的输入。这使处理更加高效和准确，因为重点转向了捕获任务所需的关键信息的特征子集。
- en: 'Feature extraction algorithms are used to derive new features from the original
    set of features. These algorithms aim to capture the most relevant and informative
    aspects of the data while reducing dimensionality. Here are a few commonly used
    feature extraction algorithms:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取算法用于从原始特征集中推导出新的特征。这些算法旨在捕捉数据中最相关和最有信息量的方面，同时降低维度。以下是一些常用的特征提取算法：
- en: '**PCA**: PCA is a widely used technique for feature extraction. It identifies
    the orthogonal axes (principal components) in the data that explain the maximum
    variance. By projecting the data onto these components, it reduces the dimensionality
    while preserving the most important information.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCA**：PCA是一种广泛使用的特征提取技术。它识别数据中的正交轴（主成分），这些轴解释了最大的方差。通过将这些数据投影到这些成分上，它降低了维度同时保留了最重要的信息。'
- en: '**Linear discriminant analysis (LDA)**: LDA is primarily used for feature extraction
    in the context of classification tasks. It aims to find a projection of the data
    that maximizes the separation between different classes while minimizing the variance
    within each class.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析（LDA）**: LDA主要用于分类任务中的特征提取。它的目标是找到一个数据投影，使得不同类之间的分离最大化，同时最小化每个类内的方差。'
- en: '**Independent component analysis (ICA)**: ICA seeks to identify statistically
    independent components from a set of observed signals. It assumes that the observed
    signals are linear combinations of hidden independent components. ICA can be useful
    for extracting meaningful features in signal processing and blind source separation
    problems.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析（ICA）**: ICA旨在从一组观察到的信号中识别出统计上独立的成分。它假设观察到的信号是隐藏独立成分的线性组合。ICA在信号处理和盲源分离问题中提取有意义的特征时可能很有用。'
- en: '**NMF**: NMF decomposes a non-negative matrix into two lower-rank matrices,
    where the elements are constrained to be non-negative. It can uncover parts-based
    representations of the data and is often used for feature extraction in image
    and text analysis tasks.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NMF**: NMF将非负矩阵分解为两个低秩矩阵，其中元素被限制为非负。它可以揭示数据的部分表示，通常用于图像和文本分析任务中的特征提取。'
- en: '**Autoencoders**: Autoencoders are neural network models that aim to reconstruct
    the input data from a compressed representation (encoding) layer. The encoding
    layer represents the extracted features. By training the autoencoder to minimize
    the reconstruction error, meaningful features can be learned.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**: 自编码器是神经网络模型，旨在从压缩表示（编码）层重建输入数据。编码层表示提取的特征。通过训练自编码器以最小化重建误差，可以学习到有意义的特征。'
- en: '**Wavelet transform**: Wavelet transform decomposes the data into different
    frequency bands, allowing the extraction of features at various scales. It is
    commonly used in signal and image processing tasks.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小波变换**: 小波变换将数据分解为不同的频带，允许在不同尺度上提取特征。它在信号和图像处理任务中常用。'
- en: These are just a few examples of feature extraction algorithms. The choice of
    algorithm depends on the nature of the data, the specific problem, and the desired
    outcome. It is often beneficial to experiment with multiple algorithms and compare
    their performance to select the most suitable one for a given task.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是特征提取算法的几个例子。算法的选择取决于数据的性质、具体问题和期望的结果。通常，尝试多种算法并比较它们的性能，以选择给定任务中最合适的一个是有益的。
- en: 'Feature extraction works by transforming the original set of features into
    a new representation that captures the essential information while reducing dimensionality.
    The process typically involves the following steps:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取通过将原始特征集转换为一种新的表示，这种表示捕捉了基本信息同时降低了维度。这个过程通常涉及以下步骤：
- en: '**Data preprocessing**: The input data is preprocessed to handle missing values
    and outliers, and normalize the features if necessary. This ensures that the data
    is in a suitable form for feature extraction.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据预处理**: 输入数据经过预处理以处理缺失值和异常值，并在必要时对特征进行归一化。这确保了数据处于适合特征提取的合适形式。'
- en: '**Dimensionality reduction**: This step aims to reduce the number of features
    while preserving the most important information. Techniques such as PCA, LDA,
    or NMF are commonly used for dimensionality reduction. These methods identify
    a lower-dimensional subspace or combination of features that retain the most significant
    variations in the data.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**降维**: 此步骤旨在减少特征的数量，同时保留最重要的信息。PCA、LDA或NMF等技术常用于降维。这些方法识别出一个低维子空间或特征组合，它保留了数据中最显著的变化。'
- en: '**Feature construction**: In some cases, new features are constructed from
    the original features. This can involve mathematical operations, transformations,
    or aggregations. The goal is to create features that capture specific patterns
    or relationships in the data that may be more informative for the task at hand.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征构造**: 在某些情况下，新特征是从原始特征构造出来的。这可能涉及数学运算、转换或聚合。目标是创建能够捕捉数据中特定模式或关系的特征，这些特征可能对当前任务更有信息量。'
- en: '**Feature selection**: In addition to dimensionality reduction, feature selection
    methods may be applied to further filter out irrelevant or redundant features.
    This helps to focus on the most informative features and reduce the noise in the
    data. Feature selection can be performed using techniques such as correlation
    analysis, statistical tests, or wrapper methods that evaluate the impact of different
    feature subsets on a specific learning algorithm.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征选择**：除了降维之外，特征选择方法还可以进一步过滤掉无关或冗余的特征。这有助于关注最有信息量的特征并减少数据中的噪声。特征选择可以使用相关分析、统计测试或评估不同特征子集对特定学习算法影响的外包装方法等技术进行。'
- en: '**Feature** **representation**: Once the desired set of features is extracted
    and selected, the data is represented using these features. This transformed representation
    is often lower-dimensional than the original data and contains the most relevant
    information for subsequent analysis or modeling tasks. The choice of feature extraction
    techniques depends on the specific problem, the characteristics of the data, and
    the goals of the analysis. In the following list, you’ll find instances across
    computer vision applications where a range of feature extraction techniques are
    put to use:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征表示**：一旦提取并选择了所需的特征集，数据就使用这些特征进行表示。这种转换后的表示通常比原始数据维度更低，并包含后续分析或建模任务中最相关的信息。特征提取技术的选择取决于具体问题、数据的特征和分析目标。以下列表中，你将找到计算机视觉应用中各种特征提取技术的实例：'
- en: In face recognition, we employ PCA and **local binary patterns** (**LBP**) to
    extract facial features
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在人脸识别中，我们使用PCA和**局部二值模式（LBP**）来提取面部特征。
- en: Object detection benefits from the **histogram of oriented gradients (HOG)**,
    which captures object shape and texture
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测受益于**方向梯度直方图（HOG**），它能捕捉到物体的形状和纹理。
- en: Image classification employs **convolutional neural networks** (**CNNs**) to
    engage in hierarchical feature learning
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类使用**卷积神经网络（CNNs**）进行层次特征学习。
- en: It is important to evaluate the extracted features and assess their impact on
    the performance of the downstream tasks, such as classification or regression.
    Feature extraction is an iterative process, and it may require experimentation
    and fine-tuning to find the most effective combination of techniques for a given
    task.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 评估提取的特征并评估它们对下游任务（如分类或回归）性能的影响是很重要的。特征提取是一个迭代过程，可能需要实验和微调以找到给定任务中最有效的技术组合。
- en: After having analyzed in detail all the dimensionality reduction techniques,
    the time has come to move on to practice by addressing examples of these technologies
    in the MATLAB environment.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细分析了所有降维技术之后，现在是时候通过解决MATLAB环境中的这些技术示例来转向实践了。
- en: Feature selection and feature extraction using MATLAB
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MATLAB进行特征选择和特征提取
- en: In MATLAB, there are several built-in functions and toolboxes that can be used
    for dimensionality reduction. In the next section, we will explore some practical
    examples of the dimensionality reduction algorithm in the MATLAB environment.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，有多个内置函数和工具箱可用于降维。在下一节中，我们将探讨MATLAB环境中降维算法的一些实际示例。
- en: Stepwise regression for feature selection
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于特征选择的逐步回归
- en: 'Regression analysis is a valuable approach for understanding the impact of
    independent variables on a dependent variable. It allows us to identify predictors
    that hold greater influence over the model’s response. Stepwise regression is
    a variable selection method used to choose a subset of predictors that exhibit
    the strongest relationship with the dependent variable. There are three common
    variable selection algorithms:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析是一种理解独立变量对因变量影响的有价值方法。它允许我们识别对模型响应有更大影响的预测因子。逐步回归是一种变量选择方法，用于选择与因变量关系最强的预测因子子集。有三种常见的变量选择算法：
- en: '**Forward method**: The forward method starts with an empty model, where no
    predictors are initially selected. In the first step, the variable showing the
    most significant association at a statistical level is added. In subsequent steps,
    the remaining variable with the highest statistically significant association
    is added to the model. This process continues until no more variables demonstrate
    statistically significant associations with the dependent variable.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向法**：正向法从一个空模型开始，其中最初没有选择任何预测变量。在第一步中，添加显示在统计水平上最显著关联的变量。在后续步骤中，将具有最高统计显著关联的剩余变量添加到模型中。这个过程会持续进行，直到没有更多变量显示出与因变量有统计显著关联。'
- en: '**Backward method**: The backward method begins with a model that includes
    all variables. It then proceeds step by step to eliminate variables, starting
    with the one with the least significant association with the dependent variable
    on the statistical plane.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向法**：反向法从包含所有变量的模型开始。然后逐步消除变量，从与因变量在统计平面上关联最不显著的变量开始。'
- en: '**Stepwise method**: The stepwise method alternates between the forward and
    backward processes. It involves adding and removing variables that gain or lose
    significance during various model adjustments, including the addition or re-insertion
    of variables.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步法**：逐步法在正向和反向过程之间交替进行。它涉及在模型调整过程中添加和删除变量，包括变量的添加或重新插入。'
- en: These variable selection methods enable the identification of a subset of predictors
    that best explain the relationship with the dependent variable. The choice of
    algorithm depends on the specific context and the researcher’s goals. Each method
    has its own strengths and limitations, and it is essential to interpret the selected
    variables within the context of the overall regression analysis.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量选择方法能够识别出最能解释与因变量之间关系的预测变量子集。算法的选择取决于具体情境和研究人员的目标。每种方法都有其自身的优势和局限性，并且必须将选定的变量放在整体回归分析的情境中进行解释。
- en: 'In MATLAB, you can create a stepwise regression model using the `stepwiselm()`
    function. This function returns a linear model by performing stepwise regression,
    where predictors are added or removed based on their significance. Here’s how
    it works:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，您可以使用`stepwiselm()`函数创建逐步回归模型。此函数通过执行逐步回归来返回一个线性模型，其中预测变量根据其显著性被添加或删除。以下是其工作原理：
- en: The `stepwiselm()` function takes a table or dataset array as input and performs
    stepwise regression on the variables within it.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`stepwiselm()`函数接受一个表或数据集数组作为输入，并在其中的变量上执行逐步回归。'
- en: The function uses both forward and backward stepwise regression to determine
    the final model. It starts with an initial model specified using the `modelspec`
    attribute.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数使用正向和反向逐步回归来确定最终模型。它从使用`modelspec`属性指定的初始模型开始。
- en: At each step, the function compares the explanatory power of incrementally larger
    and smaller models. It searches for terms to add or remove based on the value
    of the `Criterion` argument.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每一步中，该函数比较了增量更大和更小的模型的解释力。它根据`Criterion`参数的值搜索要添加或删除的项。
- en: The function computes the p-value of an F-statistic to test models with and
    without a potential term. If a term is not currently in the model, the null hypothesis
    is that the term would have a zero coefficient if added. If the evidence is strong
    enough to reject the null hypothesis, the term is added to the model.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数计算F统计量的p值，以测试包含和不含潜在项的模型。如果一个项目前不在模型中，则零假设是该项如果添加将会有一个零系数。如果证据足够强大以拒绝零假设，则该项将被添加到模型中。
- en: Conversely, if a term is already in the model, the null hypothesis is that the
    term has a zero coefficient. If there is insufficient evidence to reject the null
    hypothesis, the term is removed from the model.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，如果一个项已经在模型中，则零假设是该项的系数为零。如果没有足够的证据拒绝零假设，则该项将从模型中删除。
- en: The process continues until no single step improves the model. The function
    terminates when no terms meet the entrance or exit criteria.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程会持续进行，直到没有单个步骤能改进模型。当没有项满足进入或退出标准时，函数终止。
- en: The `stepwiselm()` function allows you to build different models from the same
    set of potential terms, depending on the initial model and the order in which
    terms are added or removed. By using this function in MATLAB, you can perform
    stepwise regression to select the most significant predictors and determine the
    final model that best explains the relationship between the variables.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`stepwiselm()`函数允许您根据初始模型和术语添加或删除的顺序，从同一组潜在术语中构建不同的模型。通过在MATLAB中使用此函数，您可以执行逐步回归以选择最显著的预测因子，并确定最佳解释变量之间关系的最终模型。'
- en: To get the data, we utilize the extensive collection of datasets provided by
    the UCI *Machine Learning Repository* ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)).
    This repository serves as a valuable resource for obtaining a wide range of datasets
    for various machine learning and data analysis tasks. By leveraging this repository,
    we can access the necessary dataset required for our analysis or modeling purposes.
    The UCI Machine Learning Repository offers a diverse collection of datasets contributed
    by researchers and practitioners, facilitating the exploration and experimentation
    with different data-driven applications.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取数据，我们利用UCI *机器学习仓库*提供的广泛数据集集合([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/))。这个仓库作为获取各种机器学习和数据分析任务所需的大量数据集的有价值资源。通过利用这个仓库，我们可以访问分析或建模所需的数据集。UCI机器学习仓库提供了由研究人员和实践者贡献的多样化数据集，促进了不同数据驱动应用的探索和实验。
- en: The *Yacht Hydrodynamics* dataset is utilized for predicting the hydrodynamic
    performance of sailing yachts based on their dimensions and velocity. This prediction
    plays a crucial role in evaluating the overall ship performance and estimating
    the necessary propulsive power during the initial design phase. The key inputs
    for this prediction include the fundamental hull dimensions and the velocity of
    the yacht. Specifically, the inputs consist of hull geometry coefficients and
    the Froude number, while the output is the residuary resistance per unit weight
    of displacement. By analyzing these inputs, valuable insights can be gained regarding
    the performance and efficiency of sailing yachts, aiding in the design and optimization
    processes.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*游艇流体动力学*数据集用于根据游艇的尺寸和速度预测其流体动力学性能。这种预测在评估整体船舶性能和估算初始设计阶段所需的推进功率方面起着至关重要的作用。此预测的关键输入包括基本船体尺寸和游艇的速度。具体来说，输入包括船体几何系数和弗劳德数，而输出是单位重量位移的残余阻力。通过分析这些输入，可以获得有关帆船性能和效率的宝贵见解，有助于设计和优化过程。'
- en: Let's see how to practically carry out a stepwise regression using MATLAB.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实际使用MATLAB进行逐步回归。
- en: 'To start, we will import the dataset into MATLAB using the `readtable()` function
    as follows:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用`readtable()`函数将数据集导入MATLAB，如下所示：
- en: '[PRE18]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s check the size of the table imported:'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们检查导入的表的大小：
- en: '[PRE19]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The table imported contains 308 records of 7 features.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入的表包含7个特征的308条记录。
- en: 'With the data now available in the MATLAB workspace in the form of a table,
    we can proceed to perform stepwise regression:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据以表的形式存在于MATLAB工作区中，我们可以继续进行逐步回归：
- en: '[PRE20]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `stepwiselm()` function in MATLAB creates a linear model using stepwise
    regression to add or remove predictors from a table or dataset array, starting
    from a constant model. The response variable used by `stepwiselm()` is the last
    variable in the table or dataset array. The function employs both forward and
    backward stepwise regression methods to determine the final model. During each
    step, `stepwiselm()` searches for terms to add or remove from the model based
    on the specified criterion value provided through the `Criterion` argument. Terms
    are evaluated for their significance and contribution to the model’s performance.
    The function iteratively adds or removes predictors that improve the model’s fit
    based on the chosen criterion. By leveraging the `stepwiselm()` function in MATLAB,
    you can systematically identify and incorporate the most relevant predictors into
    your linear regression model, ultimately refining and optimizing the model’s predictive
    power.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MATLAB中的`stepwiselm()`函数通过逐步回归创建线性模型，从常数模型开始，向表中或数据集数组中添加或删除预测变量。`stepwiselm()`使用的响应变量是表或数据集数组中的最后一个变量。该函数采用前向和后向逐步回归方法来确定最终模型。在每一步中，`stepwiselm()`根据通过`Criterion`参数提供的指定标准值搜索要添加或从模型中删除的项。项根据其对模型性能的显著性和贡献进行评估。该函数迭代地添加或删除预测变量，以根据所选标准改进模型的拟合度。通过利用MATLAB中的`stepwiselm()`函数，您可以系统地识别并将最相关的预测变量纳入您的线性回归模型，从而最终完善和优化模型的预测能力。
- en: Stepwise regression allows us to systematically select the most relevant predictors
    and build a regression model. By executing the stepwise regression algorithm in
    MATLAB, we can automatically identify the significant predictors that have the
    strongest relationship with the dependent variable. This enables us to create
    an optimized regression model that captures the essential information from the
    dataset.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逐步回归使我们能够系统地选择最相关的预测变量并构建回归模型。通过在MATLAB中执行逐步回归算法，我们可以自动识别与因变量关系最强的显著预测变量。这使我们能够创建一个优化的回归模型，该模型能够从数据集中捕获关键信息。
- en: 'Let’s print some information about the model:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们打印一些关于模型的信息：
- en: '[PRE21]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We initiated the stepwise regression with a constant model, and the function
    identified the only variable it deemed statistically significant (`x6 = FroudeNumber`)
    to include in the model.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们以常数模型开始逐步回归，函数识别出它认为统计上显著的唯一变量（`x6 = FroudeNumber`），并将其包含在模型中。
- en: 'Let’s now try a different approach by starting with a linear model that includes
    an intercept and linear terms for each predictor. Then, step by step, the function
    will remove terms that are found to be statistically insignificant. This allows
    us to iteratively refine the model, retaining only the predictors that have a
    significant impact on the dependent variable:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一种不同的方法，从包含截距和每个预测变量的线性项的线性模型开始。然后，逐步地，该函数将删除发现统计上不显著的项。这使我们能够迭代地完善模型，仅保留对因变量有显著影响的预测变量：
- en: '[PRE22]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following text was printed:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印的以下文本：
- en: '[PRE23]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this way, we can see the order in which the model has removed the features,
    starting with the ones that are less correlated with the response.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以这种方式，我们可以看到模型删除特征的顺序，从与响应变量相关性较低的项开始。
- en: 'Now, we can print the summary of the model:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以打印模型的摘要：
- en: '[PRE24]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The result is the same, but the procedure followed is different.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果相同，但遵循的程序不同。
- en: 'To explore the full range of possibilities, we will now create a full quadratic
    model as the upper bound. We will start with a model that includes an intercept,
    linear terms, interactions, and squared terms for each predictor. This comprehensive
    model allows us to capture more complex relationships between the predictors and
    the dependent variable. However, we will still utilize stepwise regression to
    iteratively remove terms that lack statistical significance. By doing so, we can
    refine the model and focus on the predictors that have a substantial impact on
    the dependent variable, while disregarding those that are not statistically significant:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了探索所有可能性，我们现在将创建一个完整的二次模型作为上限。我们将从一个包含截距、线性项、交互项和每个预测变量的平方项的模型开始。这个综合模型使我们能够捕捉预测变量和因变量之间的更复杂关系。然而，我们仍然将利用逐步回归迭代地删除缺乏统计显著性的项。通过这样做，我们可以完善模型，关注对因变量有重大影响的预测变量，同时忽略那些统计上不显著的变量：
- en: '[PRE25]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Lots of information about how the variables were removed will be printed.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将打印出大量有关如何删除变量的信息。
- en: 'Let’s see the form of the model obtained:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看得到的模型形式：
- en: '[PRE26]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this iteration, the resulting model is more intricate and comprehensive.
    It includes the `FroudeNumber` variable both as a squared term and as part of
    the interaction with `PrismaticCoef`. This increased complexity better captures
    the underlying phenomenon, as indicated by the obtained results: an R-squared
    value of `0.928`, an adjusted R-squared value of `0.927`, and a highly significant
    p-value of `4.93e-172`.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这次迭代中，得到的模型更加复杂和全面。它不仅将`FroudeNumber`变量作为平方项，还将其作为与`PrismaticCoef`交互的一部分。这种增加的复杂性更好地捕捉了底层现象，如获得的结果所示：R平方值为`0.928`，调整后的R平方值为`0.927`，以及高度显著的p值为`4.93e-172`。
- en: These metrics suggest that the model provides a highly representative representation
    of the relationship between the predictors and the dependent variable, explaining
    a significant portion of the variability observed in the data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标表明，该模型提供了预测变量和因变量之间关系的非常代表性的表示，解释了数据中观察到的显著部分变异性。
- en: Let’s now see how to perform a PCA by analyzing a practical case.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何通过分析一个实际案例来执行PCA。
- en: Carrying out PCA
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行PCA
- en: It is a statistical technique used for dimensionality reduction and data analysis.
    PCA aims to transform a dataset with a potentially large number of variables into
    a smaller set of uncorrelated variables called principal components. These components
    are linear combinations of the original variables and are ordered in such a way
    that the first component captures the maximum amount of variance in the data,
    the second component captures the next highest amount of variance, and so on.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一种用于降维和数据分析的统计技术。PCA旨在将具有可能大量变量的数据集转换成一组较小的、不相关的变量，称为主成分。这些成分是原始变量的线性组合，并按顺序排列，使得第一个成分捕捉数据中的最大方差，第二个成分捕捉下一个最高的方差，依此类推。
- en: PCA is often used in various fields, such as machine learning, data visualization,
    and exploratory data analysis. It helps in identifying patterns and relationships
    in the data, reducing noise, and simplifying complex datasets. By reducing the
    dimensionality of the data, PCA can aid in visualizing and interpreting high-dimensional
    data, as well as improving computational efficiency in subsequent analyses. Additionally,
    PCA can be used for data preprocessing and feature extraction, allowing for more
    effective modeling and prediction tasks.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: PCA常用于各个领域，如机器学习、数据可视化和探索性数据分析。它有助于识别数据中的模式和关系，减少噪声，简化复杂的数据集。通过降低数据的维度，PCA可以帮助可视化并解释高维数据，以及在后续分析中提高计算效率。此外，PCA可用于数据预处理和特征提取，从而更有效地进行建模和预测任务。
- en: One of the major challenges in multivariate statistical analysis is effectively
    displaying datasets with numerous variables. Thankfully, in such datasets, it
    is common for certain variables to be closely interrelated. These variables essentially
    contain the same information as they measure the same underlying quantity that
    influences the system’s behavior. Consequently, these variables are redundant
    and do not contribute anything significant to the model we aim to construct. To
    simplify the problem, we can replace this group of variables with a new variable
    that encapsulates the relevant information. The following figure illustrates redundant
    data in a table.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量统计分析中的一个主要挑战是有效地显示具有许多变量的数据集。幸运的是，在这样的数据集中，某些变量通常是紧密相关的。这些变量本质上包含相同的信息，因为它们测量的是影响系统行为的相同基础量。因此，这些变量是冗余的，对我们旨在构建的模型没有显著的贡献。为了简化问题，我们可以用一个新的变量替换这一组变量，该变量封装了相关信息。以下图示展示了表格中的冗余数据。
- en: PCA generates a set of new variables, known as principal components, which are
    uncorrelated with each other. Each principal component is formed as a linear combination
    of the original variables. The orthogonality between the principal components
    ensures that there is no redundant information (see *Figure 4**.8*). Together,
    the principal components form an orthogonal basis for the data space. The primary
    objective of PCA is to explain the maximum variance in the data using the fewest
    number of principal components. In numerous real-world datasets, particularly
    those with high dimensionality, redundancy or noise is frequently present. Not
    all dimensions (features) play an equally vital role in shaping the inherent structure
    or patterns within the data. By pinpointing the directions (principal components)
    along which the data showcases the greatest variance, PCA can trim down the data’s
    dimensionality by opting for a subset of these principal components. This simplification
    streamlines the dataset while conserving its fundamental traits.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: PCA生成了一组新的变量，称为主成分，它们彼此之间不相关。每个主成分都是原始变量的线性组合。主成分之间的正交性确保没有冗余信息（见*图4*.8）。共同地，主成分构成了数据空间的正交基。PCA的主要目标是使用尽可能少的几个主成分来解释数据中的最大方差。在许多现实世界的数据集中，尤其是那些具有高维度的数据集中，冗余或噪声经常存在。并非所有维度（特征）在塑造数据内在结构或模式方面都起着同等重要的作用。通过确定数据展示最大方差的方向（主成分），PCA可以通过选择这些主成分的子集来降低数据的维度。这种简化简化了数据集，同时保留了其基本特征。
- en: '![Figure 4.8 – PCA new feature space based on linear combinations of the original
    feature space](img/B21156_04_08.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – 基于原始特征空间线性组合的PCA新特征空间](img/B21156_04_08.jpg)'
- en: Figure 4.8 – PCA new feature space based on linear combinations of the original
    feature space
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 基于原始特征空间线性组合的PCA新特征空间
- en: In MATLAB, you can perform PCA using the `pca()` function. This function takes
    an n-by-p data matrix as input, where each row corresponds to an observation and
    each column corresponds to a variable. The `pca()` function returns the principal
    component coefficients, also known as loadings. The coefficient matrix has dimensions
    p-by-p. Each column of the coefficient matrix contains the coefficients for one
    principal component, arranged in descending order based on the component variance.
    By default, the `pca()` function centers the data and utilizes the SVD algorithm.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，您可以使用`pca()`函数执行PCA。该函数接受一个n-by-p的数据矩阵作为输入，其中每一行对应一个观察值，每一列对应一个变量。`pca()`函数返回主成分系数，也称为载荷。系数矩阵的维度为p-by-p。系数矩阵的每一列包含一个主成分的系数，按成分方差降序排列。默认情况下，`pca()`函数对数据进行中心化并使用SVD算法。
- en: 'In this study, we utilized a dataset consisting of measurements of the geometric
    properties of kernels from three distinct varieties of wheat. The dataset includes
    kernels from the Kama, Rosa, and Canadian varieties, with a total of 70 samples
    randomly selected for the experiment. The internal structure of the kernels was
    visualized using a soft X-ray technique, resulting in high-quality images captured
    on 13x18 cm X-ray Kodak plates. The wheat grain used in the study was obtained
    from experimental fields and harvested using a combine harvester. The research
    was conducted at the Institute of Agrophysics of the Polish Academy of Sciences
    in Lublin. The data was collected from the UCI Machine Learning Repository:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们使用了一个包含三个不同小麦品种的籽粒几何属性测量的数据集。该数据集包括Kama、Rosa和加拿大品种的籽粒，总共随机选择了70个样本用于实验。使用软X射线技术可视化了籽粒的内部结构，从而在13x18厘米的X射线柯达板上捕获了高质量的图像。研究中使用的小麦是从实验田中收获的联合收割机收获的。研究在波兰科学院卢布林农业物理学研究所进行。数据是从UCI机器学习仓库收集的：
- en: 'As always, we begin by collecting the data to be analyzed. The `SeedsData -dataset`
    is multivariate, consisting of 210 instances. Seven geometric parameters of the
    wheat kernel are used as real-valued attributes organizing an instance. We will
    first import the dataset into the MATLAB workspace:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先收集要分析的数据。`SeedsData -数据集`是多变量的，包含210个实例。我们使用小麦籽粒的七个几何参数作为实值属性来组织一个实例。我们首先将数据集导入MATLAB工作空间：
- en: '[PRE27]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Before applying the `pca()` function to our data, let’s take a preliminary look
    at the `SeedsData` table. The first seven columns of the table represent the measured
    variables, while the eighth column indicates the type of seed.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在应用`pca()`函数到我们的数据之前，让我们先初步查看`SeedsData`表。表的前七列代表测量变量，而第八列表示种子类型。
- en: 'To examine the potential relationships between these variables, we can use
    the `plotmatrix()` function:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查这些变量之间的潜在关系，我们可以使用`plotmatrix()`函数：
- en: '[PRE28]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `plotmatrix()` function generates a matrix of subaxes, where the diagonal
    subaxes display histogram plots of the data in each respective column. The remaining
    subaxes are scatter plots, representing the relationships between different pairs
    of columns in the matrix. In the following figure, each subplot in the ith row
    and jth column represents a scatter plot of the ith column against the jth column.
    The visualization in *Figure 4**.9* provides insights into the data distribution
    and potential correlations between variables.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`plotmatrix()`函数生成一个子轴矩阵，其中对角线子轴显示每列数据的直方图。其余子轴是散点图，表示矩阵中不同列对之间的关系。在下面的图中，第i行第j列的每个子图代表第i列与第j列之间的散点图。*图4.9*中的可视化提供了数据分布和变量之间潜在相关性的见解。'
- en: '![Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter;
    Compactness; LengthK; WidthK; AsymCoef; LengthKG)](img/B21156_04_09.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 测量变量的散点图矩阵（面积；周长；紧凑度；LengthK；WidthK；不对称系数；LengthKG）](img/B21156_04_09.jpg)'
- en: Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter;
    Compactness; LengthK; WidthK; AsymCoef; LengthKG)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 测量变量的散点图矩阵（面积；周长；紧凑度；LengthK；WidthK；不对称系数；LengthKG）
- en: 'As observed in *Figure 4**.9*, scatter plot matrices provide a useful visual
    tool for identifying potential linear correlations among multiple variables. This
    aids in pinpointing specific variables that may exhibit mutual correlations, indicating
    possible redundancy in the data. Additionally, the diagonal of the matrix displays
    histogram plots, offering insights into the distribution of values for each measured
    variable. The remaining plots represent scatter plots of the matrix columns, with
    each plot appearing twice: once in the corresponding row and again in the corresponding
    column as a mirror image.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.9*所示，散点图矩阵为识别多个变量之间潜在的线性相关性提供了一个有用的视觉工具。这有助于确定可能存在相互相关性的特定变量，表明数据中可能存在冗余。此外，矩阵的对角线显示直方图，为每个测量变量的值分布提供了见解。其余的图表代表矩阵列的散点图，每个图表出现两次：一次在相应的行中，再次在相应的列中作为镜像图像。
- en: Upon analyzing *Figure 4**.9*, several plots demonstrate a linear relationship
    between variables. For instance, the plot showing the relationship between `Area`
    and `Perimeter`, as well as the one between `Perimeter` and `LengthK`, exhibits
    such a correlation. However, no correlation can be observed for certain variable
    pairs. For example, the `LengthKG` variable appears to have no correlation with
    any other variable, as its data is scattered throughout the plot area.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析**图4.9**后，几个图表展示了变量之间的线性关系。例如，展示“面积”与“周长”之间关系以及“周长”与“LengthK”之间关系的图表显示出这种相关性。然而，某些变量对之间没有观察到相关性。例如，“LengthKG”变量似乎与任何其他变量都没有相关性，因为其数据在图表区域内散布。
- en: 'To further validate this initial visual analysis, we can calculate the linear
    correlation coefficients between the measured variables. The `corr()` function
    can be employed for this purpose, which returns a matrix containing the pairwise
    linear correlation coefficient `(r)` between each pair of columns in the user-provided
    matrix:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进一步验证这种初步的视觉分析，我们可以计算测量变量之间的线性相关系数。可以采用`corr()`函数来完成此目的，该函数返回一个矩阵，包含用户提供的矩阵中每对列之间的成对线性相关系数（r）：
- en: '[PRE29]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'One might question the need to analyze both the scatter plots and the correlation
    coefficients. The reason is that there are cases where the scatterplots provide
    information that the correlation coefficients alone cannot convey. If the scatterplot
    does not indicate a linear relationship between variables, the correlation calculation
    becomes less meaningful. In such scenarios, two possibilities arise:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有些人可能会质疑分析散点图和相关性系数的必要性。原因是有些情况下，散点图提供的信息是相关性系数本身无法传达的。如果散点图没有表明变量之间存在线性关系，那么相关性计算就变得不那么有意义。在这种情况下，有两种可能性：
- en: If no relationship exists at all between the variables, calculating the correlation
    is not appropriate because correlation specifically applies to linear relationships.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果变量之间根本不存在任何关系，计算相关性是不恰当的，因为相关性专门适用于线性关系。
- en: If a strong relationship exists, but it is not linear, the correlation coefficient
    can be misleading. In certain cases, a strong curved relationship may exist, which
    cannot be captured accurately by the correlation coefficient.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在强烈的关系，但不是线性的，相关系数可能会误导。在某些情况下，可能存在强烈的曲线关系，这无法被相关系数准确捕捉。
- en: This emphasizes the critical importance of examining the scatterplots alongside
    the correlation coefficients.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这强调了检查散点图与相关系数一起的重要性。
- en: 'With this understanding, it is now appropriate to proceed with the PCA:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这种理解，现在进行PCA分析是合适的：
- en: '[PRE30]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This code returns the following information:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码返回以下信息：
- en: '`coeff`: The principal component coefficients'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coeff`: 主成分系数'
- en: '`score`: The principal component scores'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score`: 主成分得分'
- en: '`latent`: The principal component variances'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent`: 主成分方差'
- en: '`tsquared`: The Hotelling’s T-squared statistic for each observation'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tsquared`: 每个观察值的Hotelling T平方统计量'
- en: '`explained`: The percentage of the total variance explained by each principal
    component'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explained`: 每个主成分解释的总方差的百分比'
- en: '`mu`: The estimated mean of each variable'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mu`: 每个变量的估计均值'
- en: In situations where the variables are measured in different units or the variance
    differs significantly across columns, it is often recommended to scale the data
    or apply weights. The `pca()` function in MATLAB incorporates a default behavior
    of centering the user-provided matrix by subtracting the column means before conducting
    either SVD or eigenvalue decomposition.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变量以不同单位测量或列间的方差差异显著的情况下，通常建议对数据进行缩放或应用权重。MATLAB中的`pca()`函数在执行SVD或特征值分解之前，通过减去列均值来对用户提供的矩阵进行中心化，这是其默认行为。
- en: 'Additionally, for performing the PCA, the `pca()` function offers three different
    algorithms:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，为了执行PCA，`pca()`函数提供了三种不同的算法：
- en: SVD (`svd`)
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD (`svd`)
- en: Eigenvalue decomposition of the covariance matrix (`eig`)
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差矩阵的特征值分解 (`eig`)
- en: Alternating least squares algorithm (`als`)
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交替最小二乘算法 (`als`)
- en: By default, the `pca()` function utilizes the SVD algorithm for the analysis.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认情况下，`pca()`函数使用SVD算法进行分析。
- en: Let’s analyze the results returned by the `pca()` function. The coefficient
    matrix, `coeff`, obtained from the PCA, contains the coefficients for the first
    seven variables present in the `SeedsData` table. The rows of `coeff` represent
    the variables, while the columns correspond to the principal components. The coefficients
    within each column determine the linear combination of the original variables
    that represent the information in the new dimensional space. The columns of `coeff`
    are arranged in descending order based on the variance of each principal component.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们分析`pca()`函数返回的结果。从PCA中获得的特征矩阵`coeff`包含了`SeedsData`表中前七个变量的系数。`coeff`的行代表变量，而列对应主成分。每个列内的系数决定了原始变量在新的维度空间中的线性组合，以表示信息。`coeff`的列根据每个主成分的方差降序排列。
- en: A principal component is a linear combination of the original variables, denoted
    as `p`, weighted by a vector, `u`. The first principal component is formed by
    combining the variables with the highest variance. In contrast, the second principal
    component combines variables with a slightly lower variance, while also ensuring
    orthogonality to the previous component. This pattern continues for subsequent
    principal components, each incorporating variables with progressively lower variances
    while maintaining orthogonality to the preceding components. The number of principal
    components is equal to the number of observed variables. Each principal component
    is derived as a linear combination that maximizes the variance while maintaining
    non-correlation with the preceding components.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主成分是原始变量的线性组合，表示为`p`，并乘以一个向量`u`。第一个主成分是通过组合具有最高方差的变量形成的。相反，第二个主成分结合了方差略低的变量，同时确保与前一个成分的正交性。这种模式继续应用于后续的主成分，每个主成分都包含具有递减方差的变量，同时保持与前一个成分的正交性。主成分的数量等于观察到的变量数量。每个主成分都是通过线性组合得到的，以最大化方差，同时保持与前一个成分的非相关性。
- en: The second output, `score`, comprises the coordinates of the original data in
    the new dimensional space defined by the principal components. It represents how
    each observation aligns with the principal components and provides a representation
    of the data in the transformed space.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个输出，`score`，包含了原始数据在新维度空间中的坐标，该空间由主成分定义。它表示每个观察与主成分的对齐情况，并在变换空间中提供了数据的表示。
- en: 'Our goal is to represent the dataset in a new space with reduced dimensions.
    To achieve this, we plot the first two columns of the `score` matrix, which represent
    the coordinates of the original data in the new coordinate system defined by the
    principal components. To make the graph more comprehensible, we group the data
    by class:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的目标是在一个新空间中以降低的维度表示数据集。为了实现这一点，我们绘制了`score`矩阵的前两列，它们代表了原始数据在新坐标系中的坐标，该坐标系由主成分定义。为了使图表更易于理解，我们按类别分组数据：
- en: '[PRE31]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following figure is printed:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的图被打印出来：
- en: '![Figure 4.10 – The scatter plot for the first two principal components](img/B21156_04_10.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – 前两个主成分的散点图](img/B21156_04_10.jpg)'
- en: Figure 4.10 – The scatter plot for the first two principal components
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 前两个主成分的散点图
- en: The previous figure distinctly classifies the seeds into three distinct classes.
    The data points are visibly distributed in separate areas of the plot, with minimal
    uncertainty observed only in the border regions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图明显地将种子分为三个不同的类别。数据点在图的单独区域中可见地分布，只在边界区域观察到最小的不确定性。
- en: Let’s revisit the results obtained from the `pca()` function. The third output,
    `latent`, is a vector that represents the variance explained by each corresponding
    principal component. The variance of each principal component is reflected in
    the sample variances of the columns in the `score` matrix, which align with the
    corresponding rows in `latent`. As previously mentioned, the columns of `latent`
    are arranged in descending order based on the variance of each principal component.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下从`pca()`函数获得的结果。第三个输出，`latent`，是一个向量，它代表了每个对应主成分所解释的方差。每个主成分的方差反映在`score`矩阵中列的样本方差中，这与`latent`中的对应行相匹配。正如之前提到的，`latent`的列是根据每个主成分的方差降序排列的。
- en: 'To complete the analysis, we can visualize both the principal component coefficients
    for each variable and the principal component scores for each observation in a
    single plot. This type of plot is commonly referred to as a biplot. Biplots serve
    as exploration plots that enable the simultaneous display of graphical information
    on both the samples and variables present in a data matrix. In biplots, samples
    are represented as points, while variables are depicted as vectors:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完成分析，我们可以在一个图表中可视化每个变量的主成分系数和每个观察的主成分得分。这种类型的图表通常被称为双图。双图作为探索图，允许同时显示数据矩阵中存在的样本和变量的图形信息。在双图中，样本以点表示，而变量则以向量表示：
- en: '[PRE32]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following diagram depicts the principal components coefficients for each
    variable:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的图展示了每个变量的主成分系数：
- en: '![Figure 4.11 – Biplot of the principal component coefficients for each variable
    and principal component scores for each observation](img/B21156_04_11.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 每个变量的主成分系数和每个观察的主成分得分的双图](img/B21156_04_11.jpg)'
- en: Figure 4.11 – Biplot of the principal component coefficients for each variable
    and principal component scores for each observation
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 每个变量的主成分系数和每个观察的主成分得分的双图
- en: In the biplot, each of the seven variables is represented by a vector. The direction
    and length of each vector indicate the contribution of that variable to the two
    principal components depicted in the plot.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在双图中，每个变量都由一个向量表示。每个向量的方向和长度表示该变量对图中描绘的两个主成分的贡献。
- en: For example, in the first principal component (horizontal axis), four variables
    have positive coefficients, while three variables have a negative coefficient.
    This explains why four vectors are directed toward the right half of the plot,
    while three vectors are directed toward the left half. The largest coefficient
    in the first principal component corresponds to the `AsymCoef` variable.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在第一个主成分（水平轴）中，有四个变量的系数为正，有三个变量的系数为负。这解释了为什么有四个向量指向图的右半部分，而三个向量指向左半部分。第一个主成分中最大的系数对应于`AsymCoef`变量。
- en: Similarly, in the second principal component (vertical axis), all the variables
    have positive coefficients. By examining the length of the vectors, we can clearly
    understand the weight of each variable in their respective principal components.
    It is evident that the `AsymCoef` variable holds a significant weight compared
    to the others in the first principal component. Similarly, the `LengthK` variable
    assumes a prominent role in the second principal component.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在第二个主成分（垂直轴）中，所有变量都具有正系数。通过检查向量的长度，我们可以清楚地理解每个变量在其各自的主成分中的权重。很明显，`AsymCoef`变量在第一个主成分中相对于其他变量具有显著权重。同样，`LengthK`变量在第二个主成分中扮演着突出的角色。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gained knowledge about performing accurate cluster analysis
    in the MATLAB environment. Our exploration began by understanding the measurement
    of similarity, including concepts such as element proximity, similarity, and dissimilarity
    measures. We delved into different methods for grouping objects, namely hierarchical
    clustering, and partitioning clustering.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们获得了在MATLAB环境中执行精确聚类分析的知识。我们的探索从理解相似度的测量开始，包括元素邻近度、相似度和不相似度等概念。我们深入研究了分组对象的不同方法，即层次聚类和划分聚类。
- en: Regarding partitioning clustering, we focused on the k-means method. We learned
    how to iteratively locate *k* centroids, each representing a cluster. We also
    examined the effectiveness of cluster separation and how to generate a silhouette
    plot using cluster indices obtained from k-means. The silhouette value for each
    data point serves as a measure of its similarity to other points within its own
    cluster, compared to points in other clusters. Furthermore, we delved into k-medoids
    clustering, which involves identifying the centers of clusters using medoids instead
    of centroids. We learned the procedure for locating these medoid centers.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 关于划分聚类，我们专注于k-means方法。我们学习了如何迭代地定位*k*个质心，每个质心代表一个簇。我们还考察了簇分离的有效性以及如何使用从k-means获得的簇索引生成轮廓图。每个数据点的轮廓值是其与其簇内其他点的相似性的度量，与簇外点的相似性相比。此外，我们还深入研究了k-medoids聚类，它涉及使用中位数而不是质心来识别簇的中心。我们学习了定位这些中位数中心的过程。
- en: Next, we explored the process of selecting a feature that best represents a
    given dataset, which is known as dimensionality reduction. We gained an understanding
    of the fundamental concept behind dimensionality reduction and how it can be achieved
    through variable transformation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探索了选择最能代表给定数据集的特征的过程，这被称为降维。我们理解了降维背后的基本概念以及它是如何通过变量转换来实现的。
- en: To perform feature extraction and dimensionality reduction, we learned how to
    utilize the `stepwiselm()` function. This function enables the creation of a linear
    model and the automatic addition or removal of variables based on their significance.
    We also discovered how to construct small models starting from a constant model
    and large models from models containing numerous terms.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行特征提取和降维，我们学习了如何使用`stepwiselm()`函数。这个函数能够创建一个线性模型，并基于变量的显著性自动添加或删除变量。我们还发现了如何从常数模型开始构建小型模型，以及从包含许多项的模型开始构建大型模型。
- en: In addition, we discussed techniques for handling missing values within a dataset
    and explored various methods for extracting features. Among these methods, we
    specifically analyzed PCA. PCA is a robust quantitative approach that simplifies
    the data by identifying the most informative components.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了处理数据集中缺失值的技术，并探索了提取特征的各种方法。在这些方法中，我们特别分析了主成分分析（PCA）。PCA是一种稳健的定量方法，通过识别最有信息量的成分来简化数据。
- en: In the next chapter, we will delve into the fundamental concepts of artificial
    neural networks and their implementation in the MATLAB environment. Our focus
    will be on understanding the basic principles of neural networks and how to apply
    them effectively. We will explore various aspects of neural network analysis,
    including data preparation, fitting, pattern recognition, and clustering analysis,
    all within the MATLAB framework. Additionally, we will delve into the techniques
    of preprocessing, postprocessing, and network visualization, which play a crucial
    role in enhancing training efficiency and evaluating network performance. By the
    end of the chapter, you will have gained practical knowledge on implementing and
    optimizing artificial neural networks using MATLAB, enabling you to tackle a wide
    range of data analysis tasks.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨人工神经网络的基本概念及其在MATLAB环境中的实现。我们的重点是理解神经网络的基本原理以及如何有效地应用它们。我们将在MATLAB框架内探讨神经网络分析的各个方面，包括数据准备、拟合、模式识别和聚类分析。此外，我们还将深入研究预处理、后处理和网络可视化的技术，这些技术在提高训练效率和评估网络性能方面发挥着至关重要的作用。到本章结束时，你将获得使用MATLAB实现和优化人工神经网络的实用知识，这将使你能够应对各种数据分析任务。
