<html><head></head><body>
  <div id="_idContainer244" class="Basic-Text-Frame">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-230" class="chapterTitle">Interpretation Methods for Multivariate Forecasting and Sensitivity Analysis</h1>
    <p class="normal">Throughout this book, we have learned about various methods we can use to interpret supervised learning models. They can be quite effective at assessing models while also uncovering their most influential predictors and their hidden interactions. But as the term supervised learning suggests, these methods can only leverage known samples and permutations based on these known samples’ distributions. However, when these samples represent the past, things can get tricky! As the Nobel laureate in physics Niels Bohr famously quipped, “Prediction is very difficult, especially if it’s about the future.”</p>
    <p class="normal">Indeed, when you see data points fluctuating in a time series, they may appear to be rhythmically dancing in a predictable pattern – at least in the best-case scenarios. Like a dancer moving to a beat, every repetitive movement (or frequency) can be attributed to seasonal patterns, while a gradual change in volume (or amplitude) is attributed to an equally predictable trend. The dance is inevitably misleading because there are always missing pieces of the puzzle that slightly shift the data points, such as a delay in a supplier’s supply chain causing an unexpected dent in today’s sales figures. To make matters worse, there are also unforeseen catastrophic once-in-a-decade, once-in-a-generation, or simply once-ever events that can radically make the somewhat understood movement of a time series unrecognizable, similar to a ballroom dancer having a seizure. For instance, in 2020, sales forecasts everywhere, either for better or worse, were rendered useless by COVID-19!</p>
    <p class="normal">We could call this an extreme outlier event, but we must recognize that models weren’t built to predict these momentous events because they were trained on almost entirely likely occurrences. Not predicting these unlikely yet most consequential events is why we shouldn’t place so much trust in forecasting models to begin with, especially without discussing certainty or confidence bounds.</p>
    <p class="normal">This chapter will examine a multivariate forecasting problem <a id="_idIndexMarker892"/>with <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) models. We will first assess the models with traditional interpretation methods, followed by the <strong class="keyWord">Integrated Gradient</strong> method <a id="_idIndexMarker893"/>we learned about in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, to generate our model’s local attributions. </p>
    <p class="normal">But more importantly, we will understand the LSTM’s learning process and limitations better. We will then employ a prediction approximator method and SHAP’s <code class="inlineCode">KernelExplainer</code> for both global and local interpretation. Lastly, <em class="italic">forecasting and uncertainty are intrinsically linked</em>, and <em class="italic">sensitivity analysis</em> is a family of methods designed to measure the uncertainty of the model’s output in relation to its input, so it’s very useful in forecasting scenarios. We will also study two such methods: <strong class="keyWord">Morris</strong> for <em class="italic">factor prioritization</em> and <strong class="keyWord">Sobol</strong> for <em class="italic">factor fixing</em>, which involves cost sensitivity.</p>
    <p class="normal">The following are the main topics we are going to cover:</p>
    <ul>
      <li class="bulletList">Assessing time series models with traditional interpretation methods</li>
      <li class="bulletList">Generating LSTM attributions with integrated gradients</li>
      <li class="bulletList">Computing global and local attributions with SHAP’s <code class="inlineCode">KernelExplainer</code></li>
      <li class="bulletList">Identifying influential features with factor prioritization</li>
      <li class="bulletList">Quantifying uncertainty and cost sensitivity with factor fixing</li>
    </ul>
    <p class="normal">Let’s begin!</p>
    <h1 id="_idParaDest-231" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">tensorflow</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">alibi</code>, <code class="inlineCode">distython</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">SALib</code> libraries. Instructions on how to install all these libraries can be found in this book’s preface. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/b6118"><span class="url">https://packt.link/b6118</span></a>.</p>
    </div>
    <h1 id="_idParaDest-232" class="heading-1">The mission</h1>
    <p class="normal">Highway<a id="_idIndexMarker894"/> traffic congestion is a problem that’s affecting cities across the world. As the number of vehicles per capita steadily increases across the developing world with not enough road and parking infrastructure to keep up with it, congestion has been increasing at alarming levels. In the United States, the vehicle per capita statistic is among the highest in the world (838 per 1,000 people in 2019). For this reason, US cities represent 62 out of the 381 cities worldwide with at least a 15% congestion level.</p>
    <p class="normal">Minneapolis is one such city (see <em class="italic">Figure 9.1</em>) where that threshold was recently surpassed and keeps rising. To put this metropolitan area into context, congestion levels are extremely severe at above 50%, but moderate-level congestion (15-25%) is already a warning sign of bad congestion to come. It’s challenging to reverse congestion once it reaches 25% because any infrastructure improvement will be costly to implement without disrupting traffic even further. One of the worst congestion points is between the twin cities of Minneapolis and St. Paul throughout the Interstate 94 (I-94) highway, which congests alternate routes as commuters try to cut travel time. Knowing this, the mayors of both cities have obtained some federal funding to expand the highway:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_01.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.1: TomTom’s 2019 traffic index for Minneapolis</p>
    <p class="normal">The mayors want to be able to tout a completed expansion as a joint accomplishment to get reelected for a second term. However, they are well aware that a noisy, dirty, and obstructive expansion can be a big nuisance for commuters, so the construction project could backfire politically if it’s not made nearly invisible. Therefore, they have stipulated that the construction company prefabricates as much as possible elsewhere and assembles only during low-volume hours. These hours have less than 1,500 vehicles per hour. They can also only work on one direction of the highway at a time and only block no more than half of its lanes when they are working on it. To ensure compliance with these stipulations, they will fine the company if they are blocking more than a quarter of the highway any time that volume is above this threshold, at a rate of $15 per vehicle.</p>
    <p class="normal">In addition<a id="_idIndexMarker895"/> to that, if the construction crew are on-site blocking half the highway while traffic is over 1,500 vehicles per hour, it will cost them $5,000 a day. To put this into perspective, blocking during a typical peak hour could cost the construction company $67,000 per hour, plus the $5,000 daily fee! The local authorities will <a id="_idIndexMarker896"/>use <strong class="keyWord">Automated Traffic Recorder</strong> (<strong class="keyWord">ATR</strong>) stations along the route to monitor traffic volume, as well as local traffic police to register when lanes are getting blocked for construction.</p>
    <p class="normal">The project has been planned as a 2-year construction project; the first year will expand the westbound lanes on the I-94 route, while the second will expand the eastbound lanes. The on-site portion of the construction will only occur from May through October because snow is less likely to delay construction during these months. Throughout the rest of the year, they will focus on pre-fabrication. They will attempt to work weekdays only because the workers union negotiated generous overtime pay for weekends. Therefore, weekend construction will happen only if there are significant delays. However, the union agreed to work holidays May through October for the same rate.</p>
    <p class="normal">The construction company doesn’t want to take any risks! Therefore, they need a model to predict traffic for the I-94 route and, more importantly, to understand what factors create uncertainty and possibly increase costs. They have hired a machine learning expert to do this: you!</p>
    <p class="normal">The ATR data provided by the construction company includes hourly traffic volumes up to September 2018, as well as weather data at the same timescale. It only consists of the westbound lanes because that expansion will come first.</p>
    <h1 id="_idParaDest-233" class="heading-1">The approach</h1>
    <p class="normal">You have<a id="_idIndexMarker897"/> trained a <a id="_idIndexMarker898"/>stateful <strong class="keyWord">Bidirectional LSTM</strong> model with almost four years’ worth of data (October 2012 – September 2016). You reserved the last year for testing (September 2017–2018) and the prior year to that for validation (September 2016 –2017). This made sense because the combined testing and validation datasets align well with the highway expansion project’s expected conditions (March – November). You wondered about using other splitting schemes that leveraged only the data representative of these conditions, but you didn’t want to reduce the training data so drastically, and maybe they might need it for winter predictions after all. A <a id="_idIndexMarker899"/>look-back window defines how much past data a time series model has access to. You chose 168 hours (1 week) as the look-back window size. Given the stateful nature of the model, as the model moves forward in the training data, it can learn daily and weekly seasonality, as well as some trends and patterns that can only be observed across several weeks. You also trained another two models. You have outlined the following steps to meet the client’s expectations:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">With <em class="italic">RMSE</em>, <em class="italic">regression plots</em>, <em class="italic">confusion matrices</em>, and much more, you will access the models’ predictive performance and, more importantly, how the error is distributed.</li>
      <li class="numberedList">With <em class="italic">integrated gradients</em>, you will understand if you took the best modeling strategy since it can help you visualize each of the model’s pathways to a decision, and help you choose a model based on that.</li>
      <li class="numberedList">With <em class="italic">SHAP’s</em> <code class="inlineCode">KernelExplainer</code> and a prediction approximation method, you will derive both a global and local understanding of what features matter to the chosen model.</li>
      <li class="numberedList">With <em class="italic">Morris sensitivity analysis</em>, you will identify <em class="italic">factor prioritization</em>, which ranks factors (in other words, features) by how much they can drive output variability.</li>
      <li class="numberedList">With <em class="italic">Sobol sensitivity analysis</em>, you will compute <em class="italic">factor fixing</em>, which helps determine what factors aren’t influential. It does this by quantifying the input factors’ contributions and interactions to the output’s variability. With this, you can understand what factors may have the most effect on potential fines and costs, thus producing a variance-based cost-sensitivity analysis.</li>
    </ol>
    <h1 id="_idParaDest-234" class="heading-1">The preparation</h1>
    <p class="normal">You can find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb</span></a>.</p>
    <h2 id="_idParaDest-235" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run<a id="_idIndexMarker900"/> this example, you will need to install the following<a id="_idIndexMarker901"/> libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate the dataset</li>
      <li class="bulletList"><code class="inlineCode">tensorflow</code> to load the model</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn), <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">alibi</code>, <code class="inlineCode">distython</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">SALib</code> to create and visualize the interpretations</li>
    </ul>
    <p class="normal">You should load all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="hljs-keyword">import</span> \ TimeseriesGenerator
<span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> get_file
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> TwoSlopeNorm
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> alibi.explainers <span class="hljs-keyword">import</span> IntegratedGradients
<span class="hljs-keyword">from</span> distython <span class="hljs-keyword">import</span> HEOM
<span class="hljs-keyword">import</span> shap
<span class="hljs-keyword">from</span> SALib.sample <span class="hljs-keyword">import</span> morris <span class="hljs-keyword">as</span> ms
<span class="hljs-keyword">from</span> SALib.analyze <span class="hljs-keyword">import</span> morris <span class="hljs-keyword">as</span> ma
<span class="hljs-keyword">from</span> SALib.plotting <span class="hljs-keyword">import</span> morris <span class="hljs-keyword">as</span> mp
<span class="hljs-keyword">from</span> SALib.sample.saltelli <span class="hljs-keyword">import</span> sample <span class="hljs-keyword">as</span> ss
<span class="hljs-keyword">from</span> SALib.analyze.sobol <span class="hljs-keyword">import</span> analyze <span class="hljs-keyword">as</span> sa
<span class="hljs-keyword">from</span> SALib.plotting.bar <span class="hljs-keyword">import</span> plot <span class="hljs-keyword">as</span> barplot
</code></pre>
    <p class="normal">Let’s check that TensorFlow has loaded the right version by using the <code class="inlineCode">print(tf.__version__)</code> command. It should be 2.0 or above.</p>
    <h2 id="_idParaDest-236" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">In the following snippet, we <a id="_idIndexMarker902"/>are loading the data into a DataFrame called <code class="inlineCode">traffic_df</code>. Please note that the <code class="inlineCode">prepare=True</code> parameter is important because it performs necessary tasks such as subsetting the DataFrame to the required timeframe, since October 2015, some interpolation, correcting holidays, and performing one-hot encoding:</p>
    <pre class="programlisting code"><code class="hljs-code">traffic_df = mldatasets.load(<span class="hljs-string">"traffic-volume-v2"</span>, prepare=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">There should be over 52,000 records and 16 columns. We can verify this with <code class="inlineCode">traffic_df.info()</code>. The output should check out. All the features are numerical and have no missing values, and the categorical features have already been one-hot encoded for us.</p>
    <h3 id="_idParaDest-237" class="heading-3">The data dictionary</h3>
    <p class="normal">There are <a id="_idIndexMarker903"/>only nine features, but they become 16 columns because of categorical encoding:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">dow</code>: Ordinal; day of the week starting with Monday (between 0 and 6)</li>
      <li class="bulletList"><code class="inlineCode">hr</code>: Ordinal; hour of the day (between 0 and 23)</li>
      <li class="bulletList"><code class="inlineCode">temp</code>: Continuous; average temperature in Celsius (between-30 and 37)</li>
      <li class="bulletList"><code class="inlineCode">rain_1h</code>: Continuous; mm of rainfall occurred in the hour (between 0 and 21)</li>
      <li class="bulletList"><code class="inlineCode">snow_1h</code>: Continuous; cm of snow (when converted to liquid form) occurred in the hour (between 0 and 2.5)</li>
      <li class="bulletList"><code class="inlineCode">cloud_coverage</code>: Continuous; percentage of cloud coverage (between 0 and 100)</li>
      <li class="bulletList"><code class="inlineCode">is_holiday</code>: Binary; is the day a national or state holiday when it occurs Monday to Friday (1 for yes, 0 for no)?</li>
      <li class="bulletList"><code class="inlineCode">traffic_volume</code>: Continuous; the target feature capturing traffic volume</li>
      <li class="bulletList"><code class="inlineCode">weather</code>: Categorical; a short description of the weather during that hour (Clear | Clouds | Haze | Mist | Rain | Snow | Unknown | Other)</li>
    </ul>
    <h3 id="_idParaDest-238" class="heading-3">Understanding the data</h3>
    <p class="normal">The first step in understanding a time series problem is understanding the target variable. This is because it determines how you approach everything else, from data preparation to modeling. The target variable is likely to have a special relationship with time, such as a seasonal movement or a trend.</p>
    <h4 class="heading-4">Understanding weeks</h4>
    <p class="normal">First, we can <a id="_idIndexMarker904"/>sample one 168-hour period from every season to understand the variance a bit better between days of the week, and then get an idea of how they could vary across seasons and holidays:</p>
    <pre class="programlisting code"><code class="hljs-code">lb = <span class="hljs-number">168</span>
fig, (ax0,ax1,ax2,ax3) = plt.subplots(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">8</span>))
plt.subplots_adjust(top = <span class="hljs-number">0.99</span>, bottom=<span class="hljs-number">0.01</span>, hspace=<span class="hljs-number">0.4</span>)
traffic_df[(lb*<span class="hljs-number">160</span>):(lb*<span class="hljs-number">161</span>)].traffic_volume.plot(ax=ax0)
traffic_df[(lb*<span class="hljs-number">173</span>):(lb*<span class="hljs-number">174</span>)].traffic_volume.plot(ax=ax1)
traffic_df[(lb*<span class="hljs-number">186</span>):(lb*<span class="hljs-number">187</span>)].traffic_volume.plot(ax=ax2)
traffic_df[(lb*<span class="hljs-number">199</span>):(lb*<span class="hljs-number">200</span>)].traffic_volume.plot(ax=ax3)
</code></pre>
    <p class="normal">The preceding code generates the plots shown in <em class="italic">Figure 9.2</em>. If you read them from left to right, you’ll see that they all start with Wednesday and end with Tuesday of the following week. Every day of the week starts and ends at a low point, with a high point in between. Weekdays tend to have two peaks corresponding to morning and afternoon rush hour, while weekends only have one mid-afternoon bump:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_02.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 9.2: Several sample weekly periods for traffic_volume representing each season</p>
    <p class="normal">There are<a id="_idIndexMarker905"/> some major outliers, such as Saturday October 31, which is basically Halloween and is not an official holiday. Also, February 2 (a Tuesday) was the beginning of a severe snowstorm, and the period in the late summer is much more chaotic than the other sample weeks. It turns out that in that year, the state fair occurred. Like Halloween, it’s not a federal or a regional holiday, but it’s important to note that the fairgrounds are located halfway between Minneapolis and St. Paul. You’ll also notice that on Friday July 29, there’s a midnight bump in traffic, which can be attributed to this being a big day for Minneapolis concerts.</p>
    <p class="normal">Trying to explain these inconsistencies while comparing periods in your time series is a good exercise as it helps you figure out what variables to add to your model, or at least know what is missing. In our case, we know our <code class="inlineCode">is_holiday</code> variable doesn’t include days such as Halloween or the entire state fair week, nor do we have a variable for big music or sporting events. To produce a more robust model, it would be advisable to look for reliable external data sources and add more features that cover all these possibilities, not to mention validate the existing variables. For now, we will work with what we’ve got.</p>
    <h4 class="heading-4">Understanding days</h4>
    <p class="normal">It is crucial <a id="_idIndexMarker906"/>for the highway expansion project to understand what traffic looks like for the average workday. The construction crew will be working on weekdays only (Monday to Friday) unless they experience delays, in which case they will also work weekends. We must also make a distinction between holidays and other weekdays because these are likely to be different.</p>
    <p class="normal">To this end, we will create a DataFrame (<code class="inlineCode">weekend_df</code>) and engineer a new column (<code class="inlineCode">type_of_day</code>) that codes hours as being part of a “Holiday,” “Weekday,” or “Weekend.” Then, we can group by this column and the <code class="inlineCode">hr</code> column, and aggregate with <code class="inlineCode">mean</code> and standard deviation (<code class="inlineCode">std</code>). We can then <code class="inlineCode">pivot</code> so that we have one column with the average and standard deviations traffic volumes for every <code class="inlineCode">type_of_day</code> category, where the rows represent the hours of the day (<code class="inlineCode">hr</code>). Then, we can plot the resulting DataFrame. We can create intervals with the standard deviations:</p>
    <pre class="programlisting code"><code class="hljs-code">weekend_df = traffic_df[
    [<span class="hljs-string">'hr'</span>, <span class="hljs-string">'dow'</span>, <span class="hljs-string">'is_holiday'</span>, <span class="hljs-string">'traffic_volume'</span>]].copy()
weekend_df[<span class="hljs-string">'type_of_day'</span>] = np.where(
    weekend_df.is_holiday == <span class="hljs-number">1</span>,
    <span class="hljs-string">'Holiday'</span>,
    np.where(weekend_df.dow &gt;= <span class="hljs-number">5</span>, <span class="hljs-string">'Weekend'</span>, <span class="hljs-string">'Weekday'</span>)
)
weekend_df = weekend_df.groupby(
[<span class="hljs-string">'type_of_day'</span>,<span class="hljs-string">'hr'</span>]) [<span class="hljs-string">'traffic_volume'</span>]
    .agg([<span class="hljs-string">'mean'</span>,<span class="hljs-string">'std'</span>])
    .reset_index()
    .pivot(index=<span class="hljs-string">'hr'</span>, columns=<span class="hljs-string">'type_of_day'</span>, values=[<span class="hljs-string">'mean'</span>, <span class="hljs-string">'std'</span>]
)
weekend_df.columns = [
    <span class="hljs-string">''</span>.join(col).strip().replace(<span class="hljs-string">'mean'</span>,<span class="hljs-string">''</span>)\
    <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> weekend_df.columns.values
]
fig, ax = plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">8</span>))
weekend_df[[<span class="hljs-string">'Holiday'</span>,<span class="hljs-string">'Weekday'</span>,<span class="hljs-string">'Weekend'</span>]].plot(ax=ax)
plt.fill_between(
    weekend_df.index,
    np.maximum(weekend_df.Weekday - <span class="hljs-number">2</span> * weekend_df.std_Weekday, <span class="hljs-number">0</span>),
    weekend_df.Weekday + <span class="hljs-number">2</span> * weekend_df.std_Weekday,
    color=<span class="hljs-string">'darkorange'</span>,
    alpha=<span class="hljs-number">0.2</span>
)
plt.fill_between(
    weekend_df.index,\
    np.maximum(weekend_df.Weekend - <span class="hljs-number">2</span> * weekend_df.std_Weekend, <span class="hljs-number">0</span>),
    weekend_df.Weekend + <span class="hljs-number">2</span> * weekend_df.std_Weekend,
    color=<span class="hljs-string">'green'</span>,
    alpha=<span class="hljs-number">0.1</span>
)
plt.fill_between(
    weekend_df.index,\
    np.maximum(weekend_df.Holiday - <span class="hljs-number">2</span> * weekend_df.std_Holiday, <span class="hljs-number">0</span>),
    weekend_df.Holiday + <span class="hljs-number">2</span> * weekend_df.std_Holiday,
    color=<span class="hljs-string">'cornflowerblue'</span>,
    alpha=<span class="hljs-number">0.1</span>
)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker907"/> preceding snippet results in the following plot. It represents the hourly average, but there’s quite a bit of variation, which is why the construction company is proceeding with caution. There are horizontal lines that have been plotted representing each of the thresholds:</p>
    <ul>
      <li class="bulletList">5,300 for full capacity.</li>
      <li class="bulletList">2,650 for half-capacity, after which the construction company will get fined the daily amount specified.</li>
      <li class="bulletList">1,500 is the no-construction threshold, after which the construction company will get fined the hourly amount specified.</li>
    </ul>
    <p class="normal">They only want to work Monday to Friday during the hours that are typically below the 1,500 threshold. These five hours would be 11 p.m. (the day before) to 5 a.m. If they had to work weekends, this schedule would typically be delayed until 1 a.m. and end at 6 a.m. There’s considerably less variance during weekdays, so it’s understandable why the construction company is adamant about only working weekdays. During these hours, holidays appear to be similar to weekends, but holidays tend to vary even more than weekends, which is potentially even more problematic:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_03.png" alt="Chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.3: The average hourly traffic volume for holidays, weekdays, and weekends, with intervals</p>
    <p class="normal">Usually, for <a id="_idIndexMarker908"/>a project like this, you would explore the predictor variables to the extent we have done with the target. This book is about model interpretation, so we will learn about the predictors by interpreting the models. But before we get to the models, we must prepare the data for them.</p>
    <h3 id="_idParaDest-239" class="heading-3">Data preparation</h3>
    <p class="normal">The first <a id="_idIndexMarker909"/>data preparation step is to split it into train, validation, and test sets. Please note that the test dataset comprises the last 52 weeks (<code class="inlineCode">2184</code> hours), while<a id="_idIndexMarker910"/> the validation dataset comprises the 52 weeks before that, so it starts at <code class="inlineCode">4368</code> and ends <code class="inlineCode">2184</code> hours before the last row of the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">train = traffic_df[:-<span class="hljs-number">4368</span>]
valid = traffic_df[-<span class="hljs-number">4368</span>:-<span class="hljs-number">2184</span>]
test = traffic_df[-<span class="hljs-number">2184</span>:]
</code></pre>
    <p class="normal">Now that the DataFrame has been split, we can plot it to ensure that its parts are split as intended. We can do so with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(train.index.values, train.traffic_volume.values,
          label=<span class="hljs-string">'</span><span class="hljs-string">train'</span>)
plt.plot(valid.index.values, valid.traffic_volume.values,
           label=<span class="hljs-string">'validation'</span>)
plt.plot(test.index.values, test.traffic_volume.values,
          label=<span class="hljs-string">'test'</span>)
plt.ylabel(<span class="hljs-string">'Traffic Volume'</span>)
plt.legend()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker911"/>preceding code produces <em class="italic">Figure 9.4</em>. It shows that almost 4 years of data was allocated for the training dataset, and a year to validate and test each. We won’t reference the validation dataset from this point on during this exercise because it was only instrumental during training to assess the model’s predictive performance after every epoch.</p>
    <p class="packt_figref"><img src="../Images/B18406_09_04.png" alt="Chart  Description automatically generated with low confidence"/></p>
    <p class="packt_figref">Figure 9.4: Time series split into train, validation, and test sets</p>
    <p class="normal">The next <a id="_idIndexMarker912"/>step is to min-max normalize the data. We are doing this because larger values lead to slower learning for all neural networks in general and LSTMs are very prone to <strong class="keyWord">exploding and vanishing gradients</strong>. Relatively uniform and small numbers can help counter these problems. We will discuss this later in this chapter, but basically, the network becomes either numerically unstable or ineffective at reaching a global minimum.</p>
    <p class="normal">We can min-max normalize with <code class="inlineCode">MinMaxScaler</code> from the <code class="inlineCode">scikit</code> package. For now, all we will do is <code class="inlineCode">fit</code> the scaler so that we can use them whenever we need them. We will create a scaler for our target (<code class="inlineCode">traffic_volume</code>) called <code class="inlineCode">y_scaler</code> and another for the rest of the variables (<code class="inlineCode">X_scaler</code>) with the entire dataset, so that transformations are consistent no matter what part you are using, be it <code class="inlineCode">train</code>, <code class="inlineCode">valid</code>, or <code class="inlineCode">test</code>. All the <code class="inlineCode">fit</code> process does is save the formula to make each variable fit between zero and one:</p>
    <pre class="programlisting code"><code class="hljs-code">y_scaler = MinMaxScaler()
y_scaler.fit(traffic_df[[<span class="hljs-string">'traffic_volume'</span>]])
X_scaler = MinMaxScaler()
X_scaler.fit(traffic_df.drop([<span class="hljs-string">'traffic_volume'</span>], axis=<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">Now, we will <code class="inlineCode">transform</code> both our train and test datasets with our scaler, creating <em class="italic">y</em> and <em class="italic">X</em> pairs for each:</p>
    <pre class="programlisting code"><code class="hljs-code">y_train = y_scaler.transform(train[['traffic_volume']])
X_train = X_scaler.transform(train.drop(['traffic_volume'], axis=1))
y_test = y_scaler.transform(test[['traffic_volume']])
X_test = X_scaler.transform(test.drop(['traffic_volume'], axis=1))
</code></pre>
    <p class="normal">However, for<a id="_idIndexMarker913"/> a time series model, the <em class="italic">y</em> and <em class="italic">X</em> pairs we <a id="_idIndexMarker914"/>created aren’t useful because each observation is a timestep. And each timestep is more than the features that occur for that timestep, but to a certain extent what happens before it, called lags. For instance, say if we predict traffic based on 168 lagged observations, for every label, we will need the previous 168 hours of each feature. Therefore, you have to generate an array for every timestep, as well as its lags. Fortunately, <code class="inlineCode">keras</code> has a function called <code class="inlineCode">TimeseriesGenerator</code> that takes your <em class="italic">X</em> and <em class="italic">y</em> and produces a generator that feeds the data to your model. You must<a id="_idIndexMarker915"/> specify a certain <code class="inlineCode">length</code>, which is the number of lagged observations (also known as the <strong class="keyWord">lookback window</strong>). The default <code class="inlineCode">batch_size</code> is one, but we are using 24 because the client prefers to get forecasts 24 hours at a time, and also training and inference are much faster with a larger batch size.</p>
    <p class="normal">Naturally, when you need to forecast tomorrow, you will need tomorrow’s weather, but you can complete the timesteps with weather forecasts:</p>
    <pre class="programlisting code"><code class="hljs-code">gen_train = TimeseriesGenerator(
    X_train,
    y_train,
    length=lb,
    batch_size=<span class="hljs-number">24</span>
)
gen_test = TimeseriesGenerator(
    X_test,
    y_test,
    length=lb,
    batch_size=<span class="hljs-number">24</span>
)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">"gen_train:%s×%s→%s"</span> % (<span class="hljs-built_in">len</span>(gen_train),
    gen_train[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape, gen_train[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>].shape)
)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">"gen_test:%s×%s→%s"</span> % (<span class="hljs-built_in">len</span>(gen_test),
    gen_test[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape, gen_test[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>].shape)
)
</code></pre>
    <p class="normal">The preceding snippet outputs the dimensions of the training generator (<code class="inlineCode">gen_train</code>) and the testing generator (<code class="inlineCode">gen_test</code>), which use a length of 168 hours and a batch size of 24:</p>
    <pre class="programlisting con"><code class="hljs-con">gen_train:  1454 ×   (24, 168, 15)   →   (24, 1)
gen_test:   357  ×   (24, 168, 15)   →   (24, 1)
</code></pre>
    <p class="normal">Any model <a id="_idIndexMarker916"/>that was trained with a 1-week look-back window and 24-hour batch size will need this generator. Each generator is a list of tuples <a id="_idIndexMarker917"/>corresponding to each batch. Index 0 of this tuple is the <em class="italic">X</em> feature array, while index 1 is the <em class="italic">y</em> label array. Therefore, the first number output is the length of the list, which is the number of batches. The dimensions of the <em class="italic">X</em> and <em class="italic">y</em> array follow. </p>
    <p class="normal">For instance, <code class="inlineCode">gen_train</code> has 1,454 batches, and each batch has 24 timesteps, with a length of 168 and 15 features. The shape of the predicted labels expected from these 24 timesteps is <code class="inlineCode">(24,1)</code>.</p>
    <p class="normal">Lastly, before moving forward with handling models and stochastic interpretation methods, let’s attempt to make things more reproducible by initializing our random seeds:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>]=<span class="hljs-built_in">str</span>(rand)
tf.random.set_seed(rand)
np.random.seed(rand)
</code></pre>
    <h3 id="_idParaDest-240" class="heading-3">Loading the LSTM model</h3>
    <p class="normal">We can quickly <a id="_idIndexMarker918"/>load the model and output its summary like this:</p>
    <pre class="programlisting code"><code class="hljs-code">model_name = <span class="hljs-string">'LSTM_traffic_168_compact1.hdf5'</span>
model_path = get_file(
    model_name,
    <span class="hljs-string">'https://github.com/PacktPublishing/Interpretable-\ </span>
    <span class="hljs-string">Machine-Learning-with-Python-2E/blob/main/models/{}?raw=true'</span>
    .<span class="hljs-built_in">format</span>(model_name)
)
lstm_traffic_mdl = keras.models.load_model(model_path)
lstm_traffic_mdl.summary()
</code></pre>
    <p class="normal">As you<a id="_idIndexMarker919"/> can tell by the summary that’s produced by the preceding snippet, the model starts with a <strong class="keyWord">bidirectional LSTM</strong> layer<a id="_idIndexMarker920"/> with an output of <code class="inlineCode">(24, 168)</code>. 24 corresponds to the batch size, while 168 means that there’s not one but two 84-unit LSTMs going in opposite directions and meeting in the middle. It has a <code class="inlineCode">dropout</code> of 10%, and then a <code class="inlineCode">dense</code> layer with a single ReLu-activated unit. The ReLu ensures that all the predictions are over zero since negative traffic volume makes no sense:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "LSTM_traffic_168_compact1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Bidir_LSTM (Bidirectional)   (24, 168)                 67200     
_________________________________________________________________
Dropout (Dropout)            (24, 168)                 0         
_________________________________________________________________
Dense (Dense)                (24, 1)                   169       
=================================================================
Total params: 67,369
Trainable params: 67,369
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">Now, let’s assess the <code class="inlineCode">LSTM_traffic_168_compact1</code> model using traditional interpretation methods.</p>
    <h1 id="_idParaDest-241" class="heading-1">Assessing time series models with traditional interpretation methods</h1>
    <p class="normal">A<a id="_idIndexMarker921"/> time series regressor model can be evaluated as you would evaluate any regression model; that is, using <a id="_idIndexMarker922"/>metrics derived<a id="_idIndexMarker923"/> from the <strong class="keyWord">mean squared error</strong> or the <strong class="keyWord">R-squared</strong> score. There are, of course, cases in which you will need to use a metric with medians, logs, deviances, or absolute values. These models don’t require any of this.</p>
    <h2 id="_idParaDest-242" class="heading-2">Using standard regression metrics</h2>
    <p class="normal">The <code class="inlineCode">evaluate_reg_mdl</code> function<a id="_idIndexMarker924"/> can evaluate the model, output some standard regression metrics, and plot them. The parameters for this model are the fitted model (<code class="inlineCode">lstm_traffic_mdl</code>), <code class="inlineCode">X_train</code> (<code class="inlineCode">gen_train</code>), <code class="inlineCode">X_test</code> (<code class="inlineCode">gen_test</code>), <code class="inlineCode">y_train</code>, and <code class="inlineCode">y_test</code>.</p>
    <p class="normal">Optionally, we<a id="_idIndexMarker925"/> can specify a <code class="inlineCode">y_scaler</code> so that the model is evaluated with the labels’ inverse transformed, which<a id="_idIndexMarker926"/> makes the plot and <strong class="keyWord">root mean square error</strong> (<strong class="keyWord">RMSE</strong>) much easier to interpret. Another optional parameter that is very much necessary, in this case, is <code class="inlineCode">y_truncate=True</code> because our <code class="inlineCode">y_train</code> and <code class="inlineCode">y_test</code> are of larger dimensions than the predicted labels. This discrepancy happens because the first prediction occurs several timesteps after the first timestep in the dataset due to the look-back window. Therefore, we would need to deduct these timesteps from <code class="inlineCode">y_train</code> in order to match the length of <code class="inlineCode">gen_train</code>.</p>
    <p class="normal">We will now evaluate both models with the following code. To observe the prediction’s progress as it happens, we will use <code class="inlineCode">predopts={"verbose":1}</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">y_train_pred, y_test_pred, y_train, y_test =\
    mldatasets.evaluate_reg_mdl(lstm_traffic_mdl,
        gen_train,
        gen_test,
        y_train,
        y_test,
        scaler=y_scaler,
        y_truncate=<span class="hljs-literal">True</span>,
        predopts={<span class="hljs-string">"verbose"</span>:<span class="hljs-number">1</span>}
)
</code></pre>
    <p class="normal">The preceding snippet produced the plot and metrics shown in <em class="italic">Figure 9.5</em>. The <em class="italic">regression plot</em> is, essentially, a scatter plot of the observed versus predicted traffic volumes, fitted to a linear regression model to show how well they match. These plots show that the model tends to predict zero traffic when it’s substantially higher. Besides that, there are a number of extreme outliers, but it fits relatively well with a test RMSE of 430 and only a slightly better train RMSE:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_05.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 9.5: Predictive performance evaluations for the “LSTM_traffic_168_compact1” model</p>
    <p class="normal">We can also<a id="_idIndexMarker927"/> evaluate the model by comparing observed versus predicted traffic. It would be helpful to <a id="_idIndexMarker928"/>break down the error by the hour and type of day too. To this end, we can create DataFrames with these values – one for each model. But first, we must truncate the DataFrame (<code class="inlineCode">-y_test_pred.shape[0]</code>) so that it matches the length of the predictions array, and we won’t need all the columns, so we are providing indexes for only those we are interested in: <code class="inlineCode">traffic_volume</code> is #7 but we also will want <code class="inlineCode">dow</code> (#0), <code class="inlineCode">hr</code> (#1), and <code class="inlineCode">is_holiday</code> (#6). We will rename <code class="inlineCode">traffic_volume</code> to <code class="inlineCode">actual_traffic</code> and create a new column called <code class="inlineCode">predicted_traffic</code> with our predictions. Then, we will engineer a <code class="inlineCode">type_of_day</code> column, as we<a id="_idIndexMarker929"/> did previously, which tells us if it’s a holiday, weekday, or weekend. Finally, we can drop the <code class="inlineCode">dow</code> and <code class="inlineCode">is_holiday</code> columns since we won’t need them:</p>
    <pre class="programlisting code"><code class="hljs-code">evaluate_df = test.iloc[-y_test_pred.shape[<span class="hljs-number">0</span>]:,[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]]
    .rename(columns={<span class="hljs-string">'traffic_volume'</span>:<span class="hljs-string">'actual_traffic'</span>}
)
evaluate_df[<span class="hljs-string">'predicted_traffic'</span>] = y_test_pred
evaluate_df[<span class="hljs-string">'type_of_day'</span>] = np.where(
    evaluate_df.is_holiday == <span class="hljs-number">1</span>,
    <span class="hljs-string">'Holiday'</span>,
    np.where(evaluate_df.dow &gt;= <span class="hljs-number">5</span>,
    <span class="hljs-string">'Weekend'</span>, <span class="hljs-string">'Weekday'</span>)
)
evaluate_df.drop([<span class="hljs-string">'dow'</span>,<span class="hljs-string">'is_holiday'</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">You can <a id="_idIndexMarker930"/>quickly review the contents of the DataFrames by simply running a cell with <code class="inlineCode">evaluate_df</code>. It should have 4 columns.</p>
    <h3 id="_idParaDest-243" class="heading-3">Predictive error aggregations</h3>
    <p class="normal">It may be that some days <a id="_idIndexMarker931"/>and times of day are more prone to predictive errors. To get a better sense of how these errors are distributed across time, we can plot RMSE on an hourly basis segmented by <code class="inlineCode">type_of_day</code>. To do this, we must first define an <code class="inlineCode">rmse</code> function and then group each of the models’ evaluated DataFrames by <code class="inlineCode">type_of_day</code> and <code class="inlineCode">hr</code> and use the <code class="inlineCode">apply</code> function to aggregate using the <code class="inlineCode">rmse</code> function. We can then pivot to ensure that each <code class="inlineCode">type_of_day</code> has a column with the RMSEs on an hourly basis. We can then average these columns and store them in a series:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">rmse</span>(<span class="hljs-params">g</span>):
    rmse = np.sqrt(
    metrics.mean_squared_error(g[<span class="hljs-string">'actual_traffic'</span>],
                               g[<span class="hljs-string">'predicted_traffic'</span>])
    )
    <span class="hljs-keyword">return</span> pd.Series({<span class="hljs-string">'rmse'</span>: rmse})
evaluate_by_hr_df = evaluate_df.groupby([<span class="hljs-string">'type_of_day'</span>, <span class="hljs-string">'hr'</span>])
    .apply(rmse).reset_index()
    .pivot(index=<span class="hljs-string">'hr'</span>, columns=<span class="hljs-string">'type_of_day'</span>, values=<span class="hljs-string">'</span><span class="hljs-string">rmse'</span>)
mean_by_daytype_s = evaluate_by_hr_df.mean(axis=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Now that <a id="_idIndexMarker932"/>we have DataFrames with the hourly RMSEs for holidays, weekdays, and weekends, as well as the average for these “types” of day, we can plot them using the <code class="inlineCode">evaluate_by_hr</code> DataFrame. We will also create dotted horizontal lines with the averages for each <code class="inlineCode">type_of_day</code> from the <code class="inlineCode">mean_by_daytype</code> <code class="inlineCode">pandas</code> series:</p>
    <pre class="programlisting code"><code class="hljs-code">evaluate_by_hr_df.plot()
ax = plt.gca()
ax.set_title(<span class="hljs-string">'Hourly RMSE distribution'</span>, fontsize=<span class="hljs-number">16</span>)
ax.set_ylim([<span class="hljs-number">0</span>,<span class="hljs-number">2500</span>])
ax.axhline(
    y=mean_by_daytype_s.Holiday,
    linewidth=<span class="hljs-number">2</span>,
    color=<span class="hljs-string">'cornflowerblue'</span>,
    dashes=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
)
ax.axhline(
    y=mean_by_daytype_s.Weekday,
    linewidth=<span class="hljs-number">2</span>,
    color=<span class="hljs-string">'darkorange'</span>,
    dashes=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
)
ax.axhline(
    y=mean_by_daytype_s.Weekend,
    linewidth=<span class="hljs-number">2</span>,
    color=<span class="hljs-string">'</span><span class="hljs-string">green'</span>,
    dashes=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
)
</code></pre>
    <p class="normal">The preceding code generated the plot shown in <em class="italic">Figure 9.6</em>. As we can see, the model has a high RMSE for holidays. However, the model could be overestimating the traffic volume, and overestimating is not as bad as underestimating in this particular use case because underestimating can lead to annoying commuters with traffic delays and additional costs from fines:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_06.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <figure class="mediaobject">Figure 9.6: Hourly RMSE segmented by type_of_day for the “LSTM_traffic_168_compact1” model</figure>
    <h3 id="_idParaDest-244" class="heading-3">Evaluating the model like a classification problem</h3>
    <p class="normal">Indeed, just like <a id="_idIndexMarker933"/>classification problems can have false positives and false negatives and one is more costly than the other, you can frame any regression problem with concepts such as underestimation and overestimation. This framing is especially useful when one is more costly than the other. If you have clearly defined thresholds, as we have for this project, you can evaluate any regression problem as you would a classification one. We will assess it with a confusion matrix with half-capacity and no-construction thresholds. To accomplish this, we can use <code class="inlineCode">np.where</code> to get binary arrays for when the actuals and predictions surpass each threshold. We can then use the <code class="inlineCode">compare_confusion_matrices</code> function to compare the confusion matrices for the model:</p>
    <pre class="programlisting code"><code class="hljs-code">actual_over_half_cap = np.where(evaluate_df[<span class="hljs-string">'</span><span class="hljs-string">actual_traffic'</span>] &gt;\
                                <span class="hljs-number">2650</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
pred_over_half_cap = np.where(evaluate_df[<span class="hljs-string">'predicted_traffic'</span>] &gt;\
                              <span class="hljs-number">2650</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
actual_over_nc_thresh = np.where(evaluate_df[<span class="hljs-string">'actual_traffic'</span>] &gt;\
                                 <span class="hljs-number">1500</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
pred_over_nc_thresh = np.where(evaluate_df[<span class="hljs-string">'predicted_traffic'</span>] &gt;\
                               <span class="hljs-number">1500</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
mldatasets.compare_confusion_matrices(
    actual_over_half_cap,
    pred_over_half_cap,
    actual_over_nc_thresh,
    pred_over_nc_thresh,
    <span class="hljs-string">'Over Half-Capacity'</span>,
    <span class="hljs-string">'Over No-Construction Threshold'</span>
)
</code></pre>
    <p class="normal">The preceding snippet produced the confusion matrices shown in <em class="italic">Figure 9.7</em>.</p>
    <p class="packt_figref"><img src="../Images/B18406_09_07.png" alt="Chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.7: Confusion matrices for going over half and the no-construction threshold for the “LSTM_traffic_168_compact1” model</p>
    <p class="normal">We <a id="_idIndexMarker934"/>are most interested in the percentage of false negatives (bottom-left quadrant) because predicting no traffic beyond the threshold when, in fact, it did rise above it, will lead to a steep fine. On the other hand, the cost of false positives is in preemptively leaving the construction site when traffic didn’t rise above the threshold after all. It’s better to be safe than sorry, though! If you compare false negatives for the “no-construction” threshold (0.85%), it’s less than a third of that of the half-capacity threshold (3.08%). Ultimately, what matters most is the no-construction threshold because the idea is to stop construction before it gets close to half-capacity.</p>
    <p class="normal">Now that we have leveraged traditional methods to understand the model’s decisions, let’s move on to some more advanced model-agnostic methods.</p>
    <h1 id="_idParaDest-245" class="heading-1">Generating LSTM attributions with integrated gradients</h1>
    <p class="normal">We first <a id="_idIndexMarker935"/>learned about <strong class="keyWord">integrated gradients</strong> (<strong class="keyWord">IG</strong>) in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>. Unlike the <a id="_idIndexMarker936"/>other gradient-based attribution methods studied in that<a id="_idIndexMarker937"/> chapter, path-integrated gradients is not contingent on convolutional layers, nor is it limited to classification problems. </p>
    <p class="normal">In fact, since it computes the gradients of the output concerning the inputs averaged along the path, the input and output could be anything! It is common to use integrated gradients <a id="_idIndexMarker938"/>with <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>) and <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>), like the one we <a id="_idIndexMarker939"/>are interpreting in this chapter. Frankly, when you see an IG LSTM example online, it has an embedding layer and is an NLP classifier, but IG could be used very effectively for LSTMs that even process sounds or genetic data!</p>
    <p class="normal">The integrated gradient explainer and the explainers that we will use moving forward can access any part of the traffic dataset. First, let’s create a generator for all of it:</p>
    <pre class="programlisting code"><code class="hljs-code">y_all = y_scaler.transform(traffic_df[[<span class="hljs-string">'traffic_volume'</span>]])
X_all = X_scaler.transform(
    traffic_df.drop([<span class="hljs-string">'traffic_volume'</span>], axis=<span class="hljs-number">1</span>)
)
gen_all = TimeseriesGenerator(
    X_all, y_all, length=lb, batch_size=<span class="hljs-number">24</span>
)
</code></pre>
    <p class="normal">Integrated gradients is a local interpretation method. So, let’s get a few sample “instances of interest” we can interpret. We know holidays may require specialized logic, so let’s see if our model picks up on the importance of <code class="inlineCode">is_holiday</code> for one example (<code class="inlineCode">holiday_afternoon_s</code>). Also, mornings are a concern, especially mornings with a larger than average rush hour because of weather conditions, so we have one example for that (<code class="inlineCode">peak_morning_s</code>). Lastly, a hot day might have more traffic, especially on a weekend (<code class="inlineCode">hot_Saturday_s</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">X_df = traffic_df.drop([<span class="hljs-string">'traffic_volume'</span>], axis=<span class="hljs-number">1</span> \
    ).reset_index(drop=<span class="hljs-literal">True</span>)
holiday_afternoon_s = X_df[
    (X_df.index &gt;= <span class="hljs-number">43800</span>) &amp; (X_df.dow==<span class="hljs-number">0</span>) &amp;\
    (X_df.hr==<span class="hljs-number">16</span>) &amp;(X_df.is_holiday==<span class="hljs-number">1</span>)
].tail(<span class="hljs-number">1</span>)
peak_morning_s = X_df[
    (X_df.index &gt;= <span class="hljs-number">43800</span>) &amp; (X_df.dow==<span class="hljs-number">2</span>) &amp;\
    (X_df.hr==<span class="hljs-number">8</span>) &amp; (X_df.weather_Clouds==<span class="hljs-number">1</span>) &amp; (X_df.temp&lt;<span class="hljs-number">20</span>)
].tail(<span class="hljs-number">1</span>)
hot_Saturday_s = X_df[
    (X_df.index &gt;= <span class="hljs-number">43800</span>) &amp; (X_df.dow==<span class="hljs-number">5</span>) &amp;\
    (X_df.hr==<span class="hljs-number">12</span>) &amp; (X_df.temp&gt;<span class="hljs-number">29</span>) &amp; (X_df.weather_Clear==<span class="hljs-number">1</span>)
].tail(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now that<a id="_idIndexMarker940"/> we have created some<a id="_idIndexMarker941"/> instances, let’s instantiate our explainers. <code class="inlineCode">IntegratedGradients</code> from the <code class="inlineCode">alibi</code> package only requires a deep learning model, but it is recommended to set a number of steps (<code class="inlineCode">n_steps</code>) for the integral approximation and <code class="inlineCode">internal_batch_size</code>. We will instantiate an explainer for our model:</p>
    <pre class="programlisting code"><code class="hljs-code">ig = IntegratedGradients(
    lstm_traffic_mdl, n_steps=<span class="hljs-number">25</span>, internal_batch_size=<span class="hljs-number">24</span>
)
</code></pre>
    <p class="normal">Before we iterate our samples and the explainers, it is important to realize how we need to input the sample to the explainer because it will need a batch of 24. To this end, we will have to get the index of the sample once we’ve deducted the lookback window (<code class="inlineCode">nidx</code>). Then, you can obtain the batch for this sample from the generator (<code class="inlineCode">gen_all</code>). Each batch includes 24 timesteps, so you floor <code class="inlineCode">nidx</code> by 24 (<code class="inlineCode">nidx//24</code>) to get the batch’s position for that sample. Once you’ve got the batch for the sample (<code class="inlineCode">batch_X</code>) and printed the shape <code class="inlineCode">(24, 168, 15)</code>, it shouldn’t surprise you that the first number is 24. Of course, we will need to get the index of the sample within the batch (<code class="inlineCode">nidx%24</code>) to obtain the data for that sample:</p>
    <pre class="programlisting code"><code class="hljs-code">nidx = holiday_afternoon_s.index.tolist()[<span class="hljs-number">0</span>] – lb
batch_X = gen_all[nidx//<span class="hljs-number">24</span>][<span class="hljs-number">0</span>]
<span class="hljs-built_in">print</span>(batch_X.shape)
</code></pre>
    <p class="normal">The <code class="inlineCode">for</code> loop will use the previously explained method to locate the batch for the sample <code class="inlineCode">(batch_X</code>). This <code class="inlineCode">batch_X</code> is inputted into the <code class="inlineCode">explain</code> function. This is because this is a regression problem and there’s no target class; that is, <code class="inlineCode">target=None</code>. Once the explanation is produced, the <code class="inlineCode">attributions</code> property will have the attributions for the entire batch. We can only obtain this for the sample and <code class="inlineCode">transpose</code> it to produce an image that<a id="_idIndexMarker942"/> has this shape: <code class="inlineCode">(15, lb)</code>. The rest of the code in the <code class="inlineCode">for</code> loop simply obtains the labels to use in the tick marks and<a id="_idIndexMarker943"/> then plots an image stretched out to fit the dimensions of our <code class="inlineCode">figure</code>, along with its labels:</p>
    <pre class="programlisting code"><code class="hljs-code">samples = [holiday_afternoon_s, peak_morning_s, hot_saturday_s]
sample_names = [<span class="hljs-string">'Holiday Afternoon'</span>, <span class="hljs-string">'Peak Morning'</span> , <span class="hljs-string">'Hot Saturday'</span>]
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(samples)):
    nidx = samples[s].index.tolist()[<span class="hljs-number">0</span>] - lb
    batch_X = gen_all[nidx//<span class="hljs-number">24</span>][<span class="hljs-number">0</span>]
    explanation = ig.explain(batch_X, target=<span class="hljs-literal">None</span>)
    attributions = explanation.attributions[<span class="hljs-number">0</span>]
    attribution_img = np.transpose(attributions[nidx%<span class="hljs-number">24</span>,:,:])
    end_date = traffic_df.iloc[samples[s].index
        ].index.to_pydatetime()[<span class="hljs-number">0</span>]
    date_range = pd.date_range(
        end=end_date, periods=<span class="hljs-number">8</span>, freq=<span class="hljs-string">'1D'</span>).to_pydatetime().tolist()
    columns = samples[s].columns.tolist()  
    plt.title(
        <span class="hljs-string">'Integrated Gradient Attribution Map for "{}"'</span>.\
        <span class="hljs-built_in">format</span>(sample_names[s], lb), fontsize=<span class="hljs-number">16</span>
    )
    divnorm = TwoSlopeNorm(
        vmin=attribution_img.<span class="hljs-built_in">min</span>(),
        vcenter=<span class="hljs-number">0</span>,
        vmax=attribution_img.<span class="hljs-built_in">max</span>()
    )
    plt.imshow(
        attribution_img,
        interpolation=<span class="hljs-string">'nearest'</span> ,
        aspect=<span class="hljs-string">'auto'</span>,
        cmap=<span class="hljs-string">'coolwarm_r'</span>,
        norm=divnorm
)
    plt.xticks(np.linspace(<span class="hljs-number">0</span>,lb,<span class="hljs-number">8</span>).astype(<span class="hljs-built_in">int</span>), labels=date_range)
    plt.yticks([*<span class="hljs-built_in">range</span>(<span class="hljs-number">15</span>)], labels=columns)
    plt.colorbar(pad=<span class="hljs-number">0.01</span>,fraction=<span class="hljs-number">0.02</span>,anchor=(<span class="hljs-number">1.0</span>,<span class="hljs-number">0.0</span>))
    plt.show()
</code></pre>
    <p class="normal">The preceding code will generate the plots shown in <em class="italic">Figure 9.8</em>. On the <em class="italic">y</em>-axis, you can see the variable names, while on the <em class="italic">x</em>-axis, you can see the dates corresponding to the lookback window for the sample in question. The rightmost part of the <em class="italic">x</em>-axis is the sample’s <a id="_idIndexMarker944"/>date, and as you move left, you<a id="_idIndexMarker945"/> go backward in time. For instance, the holiday afternoon sample was 4 p.m. September 3 and there is one week’s worth of lookback, so each tick mark backward is a day before that date.</p>
    <p class="packt_figref"><img src="../Images/B18406_09_08.png" alt="Graphical user interface, application, table  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.8: Annotated integrated gradients attribution map for all samples for the “LSTM_traffic_168_compact1” model</p>
    <p class="normal">You can <a id="_idIndexMarker946"/>tell by the intensity in the attribution maps in <em class="italic">Figure 9.8</em> which hour/variables mattered for the prediction. The<a id="_idIndexMarker947"/> color bar to the right of each attribution map can serve as a key. Negative numbers in red correspond to a negative correlation, while positive numbers in blue correspond to a positive correlation. However, something that is pretty evident is the tendency for intensities to fade as each map goes backward in time. Since it’s bidirectional, this happens from both ends. What is surprising is how fast this happens.</p>
    <p class="normal">Let’s start from the bottom. For “Hot Saturday,” day of the week, hour, temperature, and clear weather play an important role in this prediction increasingly as you get closer to the predicted time (midday Saturday). The day started cooler, which explains how there’s a patch of red before the blue in the temperature feature.</p>
    <p class="normal">For “Peak Morning,” attributions make sense since it was clear after it had been previously rainy and cloudy, which caused the rush hour to peak quickly rather than increase slowly. To a certain degree, the LSTM has learned that only recent weather matters – no more than two or three days’ worth. However, that is not the only reason the integrated gradients fade. They also fade because<a id="_idIndexMarker948"/> of the <strong class="keyWord">vanishing gradient problem</strong>. This problem occurs during backpropagation because the gradient values are multiplied by the weight matrices in each step, so gradients can exponentially decrease to zero.</p>
    <p class="normal">LSTMs are organized in a very long sequence, making the network ever more ineffective at capturing dependencies in the long term. Fortunately, these LSTMs <a id="_idIndexMarker949"/>are <strong class="keyWord">stateful</strong>, which means they string batches in a sequence by leveraging states from the previous batch. <strong class="keyWord">Statefulness</strong> ensures<a id="_idIndexMarker950"/> learning from a long sequence, despite vanishing gradients. This is why when we observe the attribution map for “Holiday Afternoon,” there are negative attributions for <code class="inlineCode">is_holiday</code>, which makes sense to anticipate no rush hour. It turns out September 3 (Labor Day) is nearly two months after the previous holiday (Independence Day), which is a more festive holiday. Is it possible that the model is picking up on these patterns?</p>
    <p class="normal">We could try subcategorizing holidays by their traffic patterns to see if that helps the model identify them. We could also make rolling aggregations of previous weather conditions to make it easier for the model to pick up on recent weather patterns. Weather patterns span hours, so it is intuitive to aggregate, not to mention easier to interpret. Interpretation methods can point us in the right direction as to how to improve models, and there’s certainly a lot of room for improvement.</p>
    <p class="normal">Next, we will take a stab at a permutation-based method!</p>
    <h1 id="_idParaDest-246" class="heading-1">Computing global and local attributions with SHAP’s KernelExplainer</h1>
    <p class="normal">Permutation methods<a id="_idIndexMarker951"/> make changes to the input to assess how much difference they will make to a model’s output. We first discussed this in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic interpretation methods</em>, but if you recall, there’s a coalitional framework to perform these permutations that will produce the average marginal contribution for each feature across different coalitions of features. This process’s outcome<a id="_idIndexMarker952"/> is <strong class="keyWord">Shapley</strong><strong class="keyWord"> values</strong>, which have essential mathematical properties such as additivity and symmetry. Unfortunately, Shapley values are costly to compute for datasets that aren’t small, so the SHAP library has approximation methods. One of these methods is <code class="inlineCode">KernelExplainer</code>, which we also explained in <em class="chapterRef">Chapter 4</em> and used in <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>. It approximates the Shapley values with a weighted local linear regression, just like LIME does.</p>
    <h2 id="_idParaDest-247" class="heading-2">Why use KernelExplainer?</h2>
    <p class="normal">We have<a id="_idIndexMarker953"/> a deep learning model, so why aren’t we using SHAP’s <code class="inlineCode">DeepExplainer</code> as we did with the CNN in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>? DeepExplainer adapted the DeepLIFT algorithm to approximate the Shapley values. It works very well with any feedforward network that’s used for tabular data, CNNs, and RNNs with an embedding layer, such as those used for an NLP classifier, or even to detect genomic sequences. It gets trickier for multivariate time series because DeepExplainer doesn’t know what to do with the input’s three-dimensional array. Even if it did, it includes data for previous timesteps, so you cannot permute one timestep without considering the previous ones. For instance, if the permutation dictates that the temperature is five degrees lower, shouldn’t that affect all the previous timestep’s temperatures up to a certain number of hours? And what if it’s 20 degrees lower? Doesn’t that mean it’s likely in a different season with entirely different weather – perhaps more clouds and snow as well?</p>
    <p class="normal">SHAP’s <code class="inlineCode">KernelExplainer</code> can receive any arbitrary black box <code class="inlineCode">predict </code>function. It also makes assumptions about the input dimensions. Fortunately, we can change the input data before it <a id="_idIndexMarker954"/>permutes it, making it seem to the <code class="inlineCode">KernelExplainer</code> like it’s dealing with a tabular dataset. The arbitrary <code class="inlineCode">predict</code> function doesn’t have to simply call the model’s <code class="inlineCode">predict </code>function – it can change data both on the way in and on the way out!</p>
    <h2 id="_idParaDest-248" class="heading-2">Defining a strategy to get it to work with a multivariate time series model</h2>
    <p class="normal">To mimic likely past <a id="_idIndexMarker955"/>weather patterns based on the permutated input data, we could create a generative model or something to that effect. This strategy will help us to generate a variety of past timesteps that fit the permutated timestep, as well as to generate images for a specific class. Although this would likely lead to more accurate predictions, we won’t use this strategy because it’s incredibly time-consuming.</p>
    <p class="normal">Instead, we will find the time series data that best suits the permutated input with existing examples from our <code class="inlineCode">gen_all</code> generator. There are distance metrics we can use to find the one that is closest to the permutated input. However, we must place some guardrails because if the permutation is for a Saturday at 5 a.m. with a temperature of 27 degrees Celsius and 90 percent cloud coverage, the closest observation to this one could be on a Friday at 7 a.m., but regardless of the weather traffic, it would be completely different. Therefore, we can implement a filter function that ensures that it only finds the closest observations for the same <code class="inlineCode">dow</code>, <code class="inlineCode">is_holiday</code>, and <code class="inlineCode">hr</code>. The filter function can also clean up the permutated sample to remove or modify anything nonsensical for the model, such as a continuous value for a categorical feature:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_09.png" alt="" role="presentation"/></p>
    <p class="packt_figref">Figure 9.9: Permutation approximation strategy</p>
    <p class="normal"><em class="italic">Figure 9.9</em> depicts the rest<a id="_idIndexMarker956"/> of the process where it uses a distance function to find the closest observation to the modified permutated sample. This function returns the closest observation index, but the model can’t predict on singular observations (or timesteps), so it requires its past hourly history up to the lookback window. For this reason, it retrieves the right batch from the generator and makes a prediction on that, but the predictions will be on a different scale, so they need to be inverse transformed with <code class="inlineCode">y_scaler</code>. Once the <code class="inlineCode">predict</code> function has iterated through all the samples and made predictions for them and rescaled them, it sends them back to the <code class="inlineCode">KernelExplainer</code>, which outputs their SHAP values.</p>
    <h2 id="_idParaDest-249" class="heading-2">Laying the groundwork for the permutation approximation strategy</h2>
    <p class="normal">You can <a id="_idIndexMarker957"/>define a custom filter function (<code class="inlineCode">filt_fn</code>). It takes a <code class="inlineCode">pandas</code> DataFrame with the entire dataset (<code class="inlineCode">X_df</code>) you want to filter from, as well as the permutated sample (<code class="inlineCode">x</code>) for filtering and the length of the <code class="inlineCode">lookback</code> window. </p>
    <p class="normal">The function can also modify the permutated sample. In this case, we have to do this because so many features of the model are discrete, but the permutation process makes them continuous. As we mentioned previously, all the filtering does is protect the distance <a id="_idIndexMarker958"/>function from finding a nonsensical closest sample to the permutated sample by limiting the options:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">filt_fn</span>(<span class="hljs-params">X_df, x, lookback</span>):
    x_ = x.copy()
    x_[<span class="hljs-number">0</span>] = <span class="hljs-built_in">round</span>(x_[<span class="hljs-number">0</span>]) <span class="hljs-comment">#round dow</span>
    x_[<span class="hljs-number">1</span>] = <span class="hljs-built_in">round</span>(x_[<span class="hljs-number">1</span>]) <span class="hljs-comment">#round hr</span>
    x_[<span class="hljs-number">6</span>] = <span class="hljs-built_in">round</span>(x_[<span class="hljs-number">6</span>]) <span class="hljs-comment">#round is_holiday</span>
    <span class="hljs-keyword">if</span> x_[<span class="hljs-number">1</span>] &lt; <span class="hljs-number">0</span>:<span class="hljs-comment">#if hr &lt; 0</span>
        x_[<span class="hljs-number">1</span>] = <span class="hljs-number">24</span> + x_[<span class="hljs-number">1</span>]
        x_[<span class="hljs-number">0</span>] = x_[<span class="hljs-number">0</span>] – <span class="hljs-number">1</span>  <span class="hljs-comment">#make it previous day</span>
    <span class="hljs-keyword">if</span> x_[<span class="hljs-number">0</span>] &lt; <span class="hljs-number">0</span>:<span class="hljs-comment">#if dow &lt; 0</span>
        x_[<span class="hljs-number">0</span>] = <span class="hljs-number">7</span> + x_[<span class="hljs-number">0</span>] <span class="hljs-comment">#make it previous week</span>
        X_filt_df = X_df[
            (X_df.index &gt;= lookback) &amp; (X_df.dow==x_[<span class="hljs-number">0</span>]) &amp;\
            (X_df.hr==x_[<span class="hljs-number">1</span>]) &amp; (X_df.is_holiday==x_[<span class="hljs-number">6</span>]) &amp;\
            (X_df.temp-<span class="hljs-number">5</span>&lt;=x_[<span class="hljs-number">2</span>]) &amp; (X_df.temp+<span class="hljs-number">5</span>&gt;=x_[<span class="hljs-number">2</span>])
        ]
    <span class="hljs-keyword">return</span> X_filt_df, x_
</code></pre>
    <p class="normal">If you refer to <em class="italic">Figure 9.9</em>, after the filter function, the next thing we ought to define is the distance function. We could use any standard distance function accepted by <code class="inlineCode">scipy.spatial.distance.cdist</code>, such as “Euclidean,” “cosine,” or “Hamming.” The problem with these standard distance functions is that they either work well with continuous or discrete variables but not both. We have both in this dataset!</p>
    <p class="normal">Fortunately, some alternatives exist that <a id="_idIndexMarker959"/>can handle both, such as <strong class="keyWord">Heterogeneous Euclidean-Overlap Metric</strong> (<strong class="keyWord">HEOM</strong>) and <strong class="keyWord">Heterogeneous Value Difference Metric</strong> (<strong class="keyWord">HVDM</strong>). Both methods apply different distance<a id="_idIndexMarker960"/> metrics, depending on the nature of the variable. HEOM uses a normalized Euclidean <img src="../Images/B18406_09_001.png" alt="" role="presentation"/> for continuous and , for discrete, “overlap” distance; that is, a distance of zero if the same and one otherwise.</p>
    <p class="normal">HVDM is more complicated because, for continuous variables, it’s the absolute distance between both values, divided by the standard deviation of the feature in question times four <img src="../Images/B18406_09_002.png" alt="" role="presentation"/>), which is a great distance metric for handling outliers. For discrete variables, it uses a normalized <strong class="keyWord">value difference metric</strong>, which is based on the difference between the conditional probability of both values.</p>
    <p class="normal">Even <a id="_idIndexMarker961"/>though HVDM is better than HEOM for datasets with many continuous values, it is overkill in this case. Once the dataset has been filtered by day of the week (<code class="inlineCode">dow</code>) and hour (<code class="inlineCode">hr</code>), the remaining discrete features are all binary, so “overlap” distance is ideal, and for the three remaining continuous features (<code class="inlineCode">temp</code>, <code class="inlineCode">rain_1h</code>, <code class="inlineCode">snow_1h</code>, and <code class="inlineCode">cloud_coverage</code>), Euclidean distance should suffice. <code class="inlineCode">distython</code> has an <code class="inlineCode">HEOM</code> distance method, and all it requires is a background dataset (<code class="inlineCode">X_df.values</code>) and the indexes of the categorical features (<code class="inlineCode">cat_idxs</code>). We can programmatically identify these features with an <code class="inlineCode">np.where</code> command. </p>
    <p class="normal">If you want to verify that these are the right ones, run <code class="inlineCode">print(cat_idxs)</code> in a cell. Only indexes 2, 3, 4, and 5 should be omitted:</p>
    <pre class="programlisting code"><code class="hljs-code">cat_idxs = np.where(traffic_df.drop([<span class="hljs-string">'traffic_volume'</span>],\
                                    axis=<span class="hljs-number">1</span>).dtypes != np.float64)[<span class="hljs-number">0</span>]
heom_dist = HEOM(X_df.values, cat_idxs)
<span class="hljs-built_in">print</span>(cat_idxs)
</code></pre>
    <p class="normal">Now, we can create a <code class="inlineCode">lambda</code> function that puts everything depicted in <em class="italic">Figure 9.9</em> together. It leverages a function called <code class="inlineCode">approx_predict_ts</code> that takes care of the entire pipeline. It takes our filter function (<code class="inlineCode">filt_fn</code>), distance function (<code class="inlineCode">heom_dist.heom</code>), generator (<code class="inlineCode">gen_all</code>), and fitted model (<code class="inlineCode">lstm_traffic_mdl</code>), and chains them together, as described in <em class="italic">Figure 9.9</em>. It also scales the data with our scalers (<code class="inlineCode">X_scaler</code> and <code class="inlineCode">y_scaler</code>). Distance is computed on transformed features for higher accuracy, and the predictions are reverse transformed on the way out:</p>
    <pre class="programlisting code"><code class="hljs-code">predict_fn = <span class="hljs-keyword">lambda</span> X: mldatasets.approx_predict_ts(
    X, X_df,
    gen_all,
    lstm_traffic_mdl,
    dist_metric=heom_dist.heom,
    lookback=lookback,
    filt_fn=filt_fn,
    X_scaler=X_scaler,
    y_scaler=y_scaler
)
</code></pre>
    <p class="normal">We can now use the prediction function with <code class="inlineCode">KernelExplainer</code>, but it should be done on samples that<a id="_idIndexMarker962"/> are most representative of the construction crew’s expected working conditions; that is, they plan to work March through November only, preferably on weekdays and in low-traffic hours. To this end, let’s create a DataFrame (<code class="inlineCode">working_season_df</code>) that only includes these months and initializes a <code class="inlineCode">KernelExplainer</code> with <code class="inlineCode">predict_fn</code> and the k-means of the DataFrame as background data:</p>
    <pre class="programlisting code"><code class="hljs-code">working_season_df =\
    traffic_df[lookback:].drop([<span class="hljs-string">'traffic_volume'</span>], axis=<span class="hljs-number">1</span>).copy()
working_season_df =\
    working_season_df[(working_season_df.index.month &gt;= <span class="hljs-number">3</span>) &amp;\
                      (working_season_df.index.month &lt;= <span class="hljs-number">11</span>)]
explainer = shap.KernelExplainer(
    predict_fn, shap.kmeans(working_season_df.values, <span class="hljs-number">24</span>)
)
</code></pre>
    <p class="normal">We can now produce SHAP values for a random set of observations of the <code class="inlineCode">working_season_df</code> DataFrame.</p>
    <h2 id="_idParaDest-250" class="heading-2">Computing the SHAP values</h2>
    <p class="normal">We will <a id="_idIndexMarker963"/>sample 48 observations from it. <code class="inlineCode">KernelExplainer</code> is rather slow, especially when it’s using our approximation method. To get an optimal global interpretation, it is best to use a high number of observations but also a high <code class="inlineCode">nsamples</code>, which is the number of times we need to reevaluate the model when explaining each prediction. Unfortunately, having 50 of each would cause the explainer to take many hours to run, depending on your available compute, so we will use <code class="inlineCode">nsamples=10</code>. You can look at SHAP’s progress bar and adjust it accordingly. Once it’s done, it will produce a feature importance <code class="inlineCode">summary_plot</code> containing the SHAP values:</p>
    <pre class="programlisting code"><code class="hljs-code">X_samp_df = working_season_df.sample(<span class="hljs-number">80</span>, random_state=rand)
shap_values = explainer.shap_values(X_samp_df, nsamples=<span class="hljs-number">10</span>)
shap.summary_plot(shap_values, X_samp_df)
</code></pre>
    <p class="normal">The preceding code plots the summary shown in the following graph. Not surprisingly, <code class="inlineCode">hr</code> and <code class="inlineCode">dow</code> are the most important features, followed by some weather features. Strangely enough, temperature and rain don’t seem to weigh in on the predictions, but late spring through fall may not be a significant factor. Or maybe more observations and a higher <code class="inlineCode">nsample</code> will yield a better global interpretation:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_10.png" alt="A picture containing graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.10: SHAP summary plot based on the SHAP values produced by 48 sampled observations</p>
    <p class="normal">We can <a id="_idIndexMarker964"/>do the same with the instances of interest we chose in the previous section for local interpretations. Let’s iterate through all these data points. Then, we can produce a single <code class="inlineCode">shap_values</code>, but this time with <code class="inlineCode">nsamples=80</code>, and then generate a <code class="inlineCode">force_plot</code> for each one:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(samples)):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Local Force Plot for "{}"'</span>.<span class="hljs-built_in">format</span>(sample_names[s]))
    shap_values_single = explainer.shap_values(
        datapoints[i], nsamples=<span class="hljs-number">80</span>)
    shap.force_plot(
    explainer.expected_value,
    shap_values_single[<span class="hljs-number">0</span>],
    samples[s],
    matplotlib=<span class="hljs-literal">True</span>
)
    plt.show()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker965"/>preceding code generates the plots shown in <em class="italic">Figure 9.11</em>. “Holiday afternoon” has the hour (<code class="inlineCode">hr=16</code>) pushing toward a higher prediction, while the fact that it’s a Monday (<code class="inlineCode">dow=0</code>) and a holiday (<code class="inlineCode">is_holiday=1</code>) is a driving force in the opposite direction. On the other hand, “Peak Morning” is mostly peak due to the hour (<code class="inlineCode">hr=8.0</code>), but it has a high <code class="inlineCode">cloud_coverage</code>, affirmative <code class="inlineCode">weather_Clouds</code>, and yet no rain (<code class="inlineCode">rain_1h=0.0</code>). Lastly, “Hot Saturday” has the day of the week (<code class="inlineCode">dow=5</code>) pushing for a lower value, but the abnormally high value is mostly due to it being midday with no rain and clouds. Strangely, higher than normal temperature is not one of the factors:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_11.png" alt="Timeline  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.11: Force plots generated with SHAP values using nsamples=80 for a Holiday Afternoon, Peak Morning, and Hot Saturday</p>
    <p class="normal">With <a id="_idIndexMarker966"/>SHAP’s game theory-based approach, we can gauge how many permutations for the existing observations marginally vary the predicted outcome across many possible coalitions of features. However, this approach can be very limiting because our background data’s existing variance shapes our understanding of outcome variance.</p>
    <p class="normal">In the real world, <em class="italic">variability is often determined by what is NOT represented in your data – but infinitesimally plausible</em>. For instance, reaching 25°C (77°F) before 5 a.m. in a Minneapolis summer is not a common occurrence, but with global warming, it could become frequent, so we would want to simulate how it could impact traffic patterns. Forecasting models are particularly prone to risk, so simulating is a crucial interpretation component to assess this uncertainty. A better understanding of uncertainty can yield more robust models and directly inform decisions. Next, we will discuss how we can produce simulations with sensitivity analysis methods.</p>
    <h1 id="_idParaDest-251" class="heading-1">Identifying influential features with factor prioritization</h1>
    <p class="normal">The <strong class="keyWord">Morris method</strong> is<a id="_idIndexMarker967"/> one of several global sensitivity analysis methods<a id="_idIndexMarker968"/> that range from <a id="_idIndexMarker969"/>simple <strong class="keyWord">Fractional factorial</strong> to complicated <strong class="keyWord">Monte Carlo filtering</strong>. Morris is somewhere on this spectrum, falling into two <a id="_idIndexMarker970"/>categories. It uses <strong class="keyWord">one-at-a-time sampling</strong>, which means <a id="_idIndexMarker971"/>that only one value changes between consecutive simulations. It’s also<a id="_idIndexMarker972"/> an <strong class="keyWord">Elementary Effects</strong> (<strong class="keyWord">EE</strong>) method, which means that it doesn’t quantify the exact effect of a factor in a model but rather gauges its importance and relationship with other factors. By the way, <strong class="keyWord">factor</strong> is just another word for a feature or variable that’s commonly used in applied statistics. To be consistent with the related theory, we will use this word in this and the next section.</p>
    <p class="normal">Another property of Morris is that it’s less computationally expensive than the variance-based methods we will study next. It can provide more insights than simpler and less costly methods such as regression-, derivative-, or factorial-based ones. It can’t quantify effects precisely but can identify those with negligible or interaction effects, making it an ideal method for screening factors <a id="_idIndexMarker973"/>when the number of factors is low. Screening is also known as <strong class="keyWord">factor prioritization</strong> because it can prioritize your factors by how they are classified.</p>
    <h2 id="_idParaDest-252" class="heading-2">Computing Morris sensitivity indices</h2>
    <p class="normal">The Morris method derives <a id="_idIndexMarker974"/>a distribution of elementary effects that it associates with an individual factor. Each EE distribution has a mean (<em class="italic">µ</em>) and a standard deviation (<em class="italic">σ</em>). These two statistics are what help map the factors into different classifications. The mean could be negative when the model is non-monotonic, so a Morris method variation adjusts for this with absolute values (<em class="italic">µ</em><sup class="superscript">*</sup>) so that it is more manageable to interpret. We will use this variation here.</p>
    <p class="normal">Now, let’s limit the scope of this problem to make it more manageable. The traffic uncertainties the construction crew will face will be ongoing from May to October, Monday to Friday, from 11 p.m. to 5 a.m. Therefore, we can take the <code class="inlineCode">working_season_df</code> DataFrame and subset it further to produce a working hours one (<code class="inlineCode">working_hrs_df</code>) that we can <code class="inlineCode">describe</code>. We will include the 1%, 50%, and 99% percentiles to understand where the median and outliers lie:</p>
    <pre class="programlisting code"><code class="hljs-code">working_hrs_df = working_season_df[
    (working_season_df.dow &lt; <span class="hljs-number">5</span>)
    &amp; ((working_season_df.hr &lt; <span class="hljs-number">5</span>) | (working_season_df.hr &gt; <span class="hljs-number">22</span>))
]
working_hrs_df.describe(percentiles=[<span class="hljs-number">.01</span>,<span class="hljs-number">.5</span>,<span class="hljs-number">.99</span>]).transpose()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker975"/>preceding code produced the table in <em class="italic">Figure 9.12</em>. We can use this table to extract the ranges we will use for our features in the simulation. Typically, we would use plausible values that have exceeded the existing maximums or minimums. For most models, any feature value can be increased or decreased beyond its known limits, and since the model learned a monotonic relationship, it can infer a realistic outcome. For instance, it might learn that rain beyond a certain point will increasingly diminish traffic. Then, say you want to simulate a severe flood with, say, 30 mm of rain per hour; it can accurately predict no traffic:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_12.png" alt="" role="presentation"/></p>
    <p class="packt_figref">Figure 9.12: Summary statistics for the period that the construction crew plans to work through</p>
    <p class="normal">However, because we are using a prediction approximation method that samples from historical values, we are limited to how far we can push the boundaries outside of the known. For this reason, we will use the 1% and 99% percentile values as our limits. We should note that this is an important caveat for any findings, especially for features that could plausibly extend beyond these limits, such as <code class="inlineCode">temp</code>, <code class="inlineCode">rain_1h</code>, and <code class="inlineCode">snow_1h</code>.</p>
    <p class="normal">Another<a id="_idIndexMarker976"/> thing to note from the summary of <em class="italic">Figure 9.12</em> is that many weather-related binary features are very sparse. You can tell by their extremely low mean. Each factor that’s added to the sensitivity analysis simulation slows it down, so we will only take the top three; that is, <code class="inlineCode">weather_Clear</code>, <code class="inlineCode">weather_Clouds</code>, and <code class="inlineCode">weather_Rain</code>. These factors are specified along with the other six factors in a “problem” dictionary (<code class="inlineCode">morris_problem</code>), which has their corresponding <code class="inlineCode">names</code>, <code class="inlineCode">bounds</code>, and <code class="inlineCode">groups</code>. Now, <code class="inlineCode">bounds</code> is critical because it denotes what ranges of values will be simulated for each factor. We will use [0,4] (Monday to Friday) for <code class="inlineCode">dow</code> and [-1,4] (11 p.m. to 4 a.m.) for <code class="inlineCode">hr</code>. The filter function automatically translates negative hours into hours from the day before so that -1 on a Tuesday is equivalent to 23 on a Monday. The rest of the bounds were informed by the percentiles. Note that <code class="inlineCode">groups</code> all have factors in the same group, except for the three weather ones:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_problem = {
    <span class="hljs-comment"># There are nine variables</span>
    <span class="hljs-string">'num_vars'</span>: <span class="hljs-number">10</span>,
    <span class="hljs-comment"># These are their names</span>
    <span class="hljs-string">'names'</span>: [<span class="hljs-string">'dow'</span>, <span class="hljs-string">'hr'</span>, <span class="hljs-string">'temp'</span>, <span class="hljs-string">'rain_1h'</span>, <span class="hljs-string">'snow_1h'</span>,\
              <span class="hljs-string">'cloud_coverage'</span>, <span class="hljs-string">'is_holiday'</span>, <span class="hljs-string">'weather_Clear'</span>,\
              <span class="hljs-string">'weather_Clouds'</span>, <span class="hljs-string">'weather_Rain'</span>],
    <span class="hljs-comment"># Plausible ranges over which we'll move the variables</span>
    <span class="hljs-string">'bounds'</span>: [
        [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>], <span class="hljs-comment"># dow Monday - Firday</span>
        [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>], <span class="hljs-comment"># hr</span>
        [-<span class="hljs-number">12</span>, <span class="hljs-number">25.</span>], <span class="hljs-comment"># temp (C)</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">3.1</span>], <span class="hljs-comment"># rain_1h</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">.3</span>], <span class="hljs-comment"># snow_1h</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">100.</span>], <span class="hljs-comment"># cloud_coverage</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-comment"># is_holiday</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-comment"># weather_Clear</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-comment"># weather_Clouds</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] <span class="hljs-comment"># weather_Rain</span>
    ],
    <span class="hljs-comment"># Only weather is grouped together</span>
    <span class="hljs-string">'groups'</span>: [<span class="hljs-string">'dow'</span>, <span class="hljs-string">'hr'</span>, <span class="hljs-string">'temp'</span>, <span class="hljs-string">'rain_1h'</span>, <span class="hljs-string">'snow_1h'</span>,\
                <span class="hljs-string">'cloud_coverage'</span>, <span class="hljs-string">'is_holiday'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'weather'</span>,\
                <span class="hljs-string">'weather'</span>]
}
</code></pre>
    <p class="normal">Once the dictionary has been defined, we can generate Morris method samples with <code class="inlineCode">SALib's</code> <code class="inlineCode">sample</code> method. In addition to the dictionary, it takes a number of trajectories (<code class="inlineCode">256</code>) and levels (<code class="inlineCode">num_levels=4</code>). The method uses a grid with factors and levels to construct the trajectories for which inputs are randomly moved <strong class="keyWord">one at a time</strong> (<strong class="keyWord">OAT</strong>). What is<a id="_idIndexMarker977"/> important to heed here is that more levels add more resolution to this grid, potentially making for a better analysis. However, this can be very time-consuming. It’s better to start with a ratio between the number of trajectories and levels of 25:1 or higher. </p>
    <p class="normal">Then, you can decrease this ratio progressively. In other words, if you have enough compute, you can make <code class="inlineCode">num_levels</code> match the number of trajectories, but if you have this much compute available, you could try <code class="inlineCode">optimal_trajectories=True</code>. However, given that we have groups, <code class="inlineCode">local_optimization</code> would have to be <code class="inlineCode">False</code>. The output of <code class="inlineCode">sample</code> is an array that is one column for each factor and (<em class="italic">G</em> + 1) × <em class="italic">T</em> rows (where <em class="italic">G</em> is the number of groups and <em class="italic">T</em> is the number of trajectories). We have eight groups and 256 trajectories, so <code class="inlineCode">print</code> should output a shape of 2,304 rows and 10 columns:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_sample = ms.sample(morris_problem, <span class="hljs-number">256</span>,\
                          num_levels=<span class="hljs-number">4</span>, seed=rand)
<span class="hljs-built_in">print</span>(morris_sample.shape)
</code></pre>
    <p class="normal">Given that the <code class="inlineCode">predict</code> function will only work with 15 factors, we should modify the samples to fill the remaining five factors with zeroes. We use zeroes because that is the median value for these features. Medians are least likely to increase traffic, but you ought to tailor your default values on a case-by-case basis. If you recall our <strong class="keyWord">Cardiovascular Disease</strong> (<strong class="keyWord">CVD</strong>) example from <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, the feature value that would increase CVD risk was sometimes the minimum or maximum.</p>
    <p class="normal">The <code class="inlineCode">np.hstack</code> function can concatenate the array horizontally so that three zero factors follow the samples for the first eight factors. Then, there’s a lonely ninth sample factor corresponding to <code class="inlineCode">weather_Rain</code>, followed by two zero factors. The resulting array should have the same number of rows as before but 15 columns:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_sample_mod = np.hstack(
    (
        morris_sample[:,<span class="hljs-number">0</span>:<span class="hljs-number">9</span>],
        np.zeros((morris_sample.shape[<span class="hljs-number">0</span>],<span class="hljs-number">3</span>)),
        morris_sample[:,<span class="hljs-number">9</span>:<span class="hljs-number">10</span>],
        np.zeros((morris_sample.shape[<span class="hljs-number">0</span>],<span class="hljs-number">2</span>))
    )
)
<span class="hljs-built_in">print</span>(morris_sample_mod.shape)
</code></pre>
    <p class="normal">The <code class="inlineCode">numpy</code> array known as <code class="inlineCode">morris_sample_mod</code> now has the Morris samples in a shape that can be understood by our <code class="inlineCode">predict</code> function. If this was a model that had been trained on a<a id="_idIndexMarker978"/> tabular dataset, we could just leverage the model’s <code class="inlineCode">predict</code> function. However, just as we did with SHAP, we have to use the approximation method. This time, we won’t use <code class="inlineCode">predict_fn</code> because we want to set one additional option, <code class="inlineCode">progress_bar=True</code>, in <code class="inlineCode">approx_predict_ts</code>. Everything else will remain the same. The progress bar will come in handy because this should take a while. Run the cell and take a coffee break:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_preds = mldatasets.approx_predict_ts(
    morris_sample_mod,
    X_df,
    gen_all,
    lstm_traffic_mdl,
    filt_fn=filt_fn,
    dist_metric=heom_dist.heom,
    lookback=lookback,
    X_scaler=X_scaler,
    y_scaler=y_scaler,
    progress_bar=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">To produce a sensitivity analysis with <code class="inlineCode">SALib's</code> <code class="inlineCode">analyze</code> function, all you need is your problem dictionary (<code class="inlineCode">morris_problem</code>), the original Morris samples (<code class="inlineCode">morris_sample</code>), and the predictions we just produced with those samples (<code class="inlineCode">morris_preds</code>). There’s an optional confidence interval level argument (<code class="inlineCode">conf_level</code>), but the default of 0.95 is good. It uses resamples to compute this confidence level, which is 1,000 by default. This setting can also be changed with an optional <code class="inlineCode">num_resamples</code> argument:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_sensitivities = ma.analyze(
    morris_problem, morris_sample, morris_preds,\
    print_to_console=<span class="hljs-literal">False</span>
)
</code></pre>
    <h2 id="_idParaDest-253" class="heading-2">Analyzing the elementary effects</h2>
    <p class="normal"><code class="inlineCode">analyze</code> will return a <a id="_idIndexMarker979"/>dictionary with the Morris sensitivity indices, including the mean (<em class="italic">µ</em>) and standard deviation (<em class="italic">σ</em>) elementary effect, as well as the absolute value of the mean (<em class="italic">µ</em><sup class="superscript">*</sup>). It’s easier to appreciate these values in a tabular format so that we can place them into a DataFrame and sort and color-code them according to <em class="italic">µ</em><sup class="superscript">*</sup>, which<a id="_idIndexMarker980"/> can be interpreted as the overall importance of the factor. <em class="italic">σ</em>, on the other hand, is how much the factor interacts with other ones:</p>
    <pre class="programlisting code"><code class="hljs-code">morris_df = pd.DataFrame(
    {
        <span class="hljs-string">'features'</span>:morris_sensitivities[<span class="hljs-string">'names'</span>],
        <span class="hljs-string">'μ'</span>:morris_sensitivities[<span class="hljs-string">'mu'</span>],
        <span class="hljs-string">'μ*'</span>:morris_sensitivities[<span class="hljs-string">'mu_star'</span>],
        <span class="hljs-string">'σ'</span>:morris_sensitivities[<span class="hljs-string">'sigma'</span>]
    }
)
morris_df.sort_values(<span class="hljs-string">'μ*'</span>, ascending=<span class="hljs-literal">False</span>).style\
    .background_gradient(cmap=<span class="hljs-string">'plasma'</span>, subset=[<span class="hljs-string">'μ*'</span>])
</code></pre>
    <p class="normal">The preceding code outputs the DataFrame depicted in <em class="italic">Figure 9.13</em>. You can tell that <code class="inlineCode">is_holiday</code> is one of the most important factors, at least during the bounds specified in the problem definition (<code class="inlineCode">morris_problem</code>). Another thing to note is that weather does have an absolute mean elementary effect but inconclusive interaction effects. Groups are challenging to assess, especially when they are sparse binary factors:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_13.png" alt="Timeline  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.13: The elementary effects decomposition of the factors</p>
    <p class="normal">The DataFrame<a id="_idIndexMarker981"/> in the preceding figure is not the best way to visualize the elementary effects. When there are not too many factors, it’s easier to plot them. <code class="inlineCode">SALib</code> comes with two plotting methods. The horizontal bar plot (<code class="inlineCode">horizontal_bar_plot</code>) and covariance plot (<code class="inlineCode">covariance_plot</code>) can be placed side by side. The covariance plot is excellent, but it doesn’t annotate the areas it delineates. We will learn about these next. So, solely for instructional purposes, we will use <code class="inlineCode">text</code> to place the annotations:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, (ax0, ax1) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
mp.horizontal_bar_plot(ax0, morris_sensitivities, {})
mp.covariance_plot(ax1, morris_sensitivities, {})
ax1.text(
    ax1.get_xlim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.45</span>, ax1.get_ylim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.75</span>,\
    <span class="hljs-string">'Non-linear and/or-monotonic'</span>, color=<span class="hljs-string">'gray'</span>,\
    horizontalalignment=<span class="hljs-string">'center'</span>
)
ax1.text(ax1.get_xlim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.75</span>, ax1.get_ylim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.5</span>,\
    <span class="hljs-string">'Almost Monotonic'</span>, color=<span class="hljs-string">'gray'</span>, horizontalalignment=<span class="hljs-string">'center'</span>)
ax1.text(ax1.get_xlim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.83</span>, ax1.get_ylim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.2</span>,\
    <span class="hljs-string">'Monotonic'</span>, color=<span class="hljs-string">'gray'</span>, horizontalalignment=<span class="hljs-string">'center'</span>)
ax1.text(ax1.get_xlim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.9</span>, ax1.get_ylim()[<span class="hljs-number">1</span>] * <span class="hljs-number">0.025</span>,
    <span class="hljs-string">'Linear'</span>, color=<span class="hljs-string">'gray'</span>, horizontalalignment=<span class="hljs-string">'</span><span class="hljs-string">center'</span>)
</code></pre>
    <p class="normal">The preceding code produces the plots shown in <em class="italic">Figure 9.14</em>. The bar plot on the left ranks the factors by <em class="italic">µ</em><sup class="superscript">*</sup>, while the lines sticking out of each bar signify their corresponding confidence bands. The covariance plot to the right is a scatter plot with <em class="italic">µ</em><sup class="superscript">*</sup> on the <em class="italic">x</em>-axis and <em class="italic">σ</em> on the <em class="italic">y</em>-axis. Therefore, the farther right the point is, the more important it is, while the further up it is in the plot, the more it interacts with other factors and becomes increasingly less monotonic. Naturally, this means that factors that don’t interact much and are mostly monotonic ones comply with linear regression assumptions, such as linearity and multicollinearity. However, the spectrum between linear and non-linear or non-monotonic is determined diagonally by the ratio of <em class="italic">σ</em> and <em class="italic">µ</em><sup class="superscript">*</sup>:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_14.png" alt="Chart, scatter chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.14: A bar and covariance plot depicting the elementary effects</p>
    <p class="normal">You can<a id="_idIndexMarker982"/> tell by the preceding covariance plot that all the factors are non-linear or non-monotonic. <code class="inlineCode">hr</code> is by far the most important, with the following two (<code class="inlineCode">dow</code> and <code class="inlineCode">temp</code>) clustered relatively nearby, followed by <code class="inlineCode">weather</code> and <code class="inlineCode">is_holiday</code>. The <code class="inlineCode">weather</code> group is not on the plot because interactivity was inconclusive, yet <code class="inlineCode">cloud_coverage</code>, <code class="inlineCode">rain_1h</code>, and <code class="inlineCode">snow_1h</code> are considerably more interactive than important on their own.</p>
    <p class="normal">Elementary effects help us understand how to classify our factors in accordance with their effects on model outcomes. However, it’s not a robust method to properly quantify their effects or those derived from factor interactions. For that, we would have to turn to a variance-based global method that uses a probabilistic framework to decompose the output’s variance and trace <a id="_idIndexMarker983"/>it back to the inputs. Those <a id="_idIndexMarker984"/>methods include <strong class="keyWord">Fourier Amplitude Sensitivity Test</strong> (<strong class="keyWord">FAST</strong>) and <strong class="keyWord">Sobol</strong>. We will study the latter approach next.</p>
    <h1 id="_idParaDest-254" class="heading-1">Quantifying uncertainty and cost sensitivity with factor fixing</h1>
    <p class="normal">With<a id="_idIndexMarker985"/> the Morris indices, it became evident that all the factors are non-linear or non-monotonic. There’s a high degree of interactivity between them – as expected! It should be no surprise that climate factors (<code class="inlineCode">temp</code>, <code class="inlineCode">rain_1h</code>, <code class="inlineCode">snow_1h</code>, and <code class="inlineCode">cloud_coverage</code>) are likely multicollinear with <code class="inlineCode">hr</code>. There are also patterns to be found between <code class="inlineCode">hr</code>, <code class="inlineCode">is_holiday</code>, and <code class="inlineCode">dow</code> and the target. Many of these factors most definitely don’t have a monotonic relationship with the target. We know this already. For instance, traffic doesn’t consistently increase as hours increase throughout the day. That’s not the case for days of the week either!</p>
    <p class="normal">However, we didn’t know to what degree <code class="inlineCode">is_holiday</code> and <code class="inlineCode">temp</code> impacted the model, particularly during the crew’s working hours, which was an important insight. That being said, factor prioritization with Morris indices is usually to be taken as a starting point or “first setting” because once you ascertain that there are interaction effects, it’s best if you disentangle them. To this end, there’s a “second setting” called <strong class="keyWord">factor fixing</strong>. We can quantify the <a id="_idIndexMarker986"/>variance and, by doing so, the uncertainty brought on by all the factors.</p>
    <p class="normal">Only <strong class="keyWord">variance-based methods</strong> can<a id="_idIndexMarker987"/> quantify these effects in a statistically rigorous fashion. <strong class="keyWord">Sobol sensitivity analysis</strong> is<a id="_idIndexMarker988"/> one of these methods, which means that it decomposes the model’s output variance into percentages and attributes it to the model’s inputs and interactions. Like Morris, it has a sampling step, as well as a sensitivity index estimation step.</p>
    <p class="normal">Unlike Morris, the sampling doesn’t follow a series of levels but the input data’s distribution. It <a id="_idIndexMarker989"/>uses a <strong class="keyWord">quasi-Monte Carlo method</strong>, where it samples points in hyperspace that follow the inputs’ probability distributions. <strong class="keyWord">Monte Carlo</strong> methods <a id="_idIndexMarker990"/>are a family of algorithms that perform random sampling, often for optimization or simulation. They seek shortcuts on problems that would be impossible to solve with brute force or entirely deterministic approaches. Monte Carlo methods are common in sensitivity analysis precisely for this reason. Quasi-Monte Carlo methods have the same goal. However, they converge faster because they use a deterministic low-discrepancy sequence instead of using a pseudorandom one. The Sobol method uses <a id="_idIndexMarker991"/>the <strong class="keyWord">Sobol sequence</strong>, devised by the same mathematician. We will use another sampling scheme derived from Sobol’s, called Saltelli’s.</p>
    <p class="normal">Once the samples have been produced, Monte Carlo estimators compute the variance-based sensitivity indices. These indices are capable of quantifying non-linear non-additive effects <a id="_idIndexMarker992"/>and second-order indices, which relate to the interaction between two factors. Morris can reveal interactivity in your model, but not precisely how it is manifested. Sobol can tell you what factors are interacting and to what degree.</p>
    <h2 id="_idParaDest-255" class="heading-2">Generating and predicting on Saltelli samples</h2>
    <p class="normal">To begin a <a id="_idIndexMarker993"/>Sobol sensitivity analysis with <code class="inlineCode">SALib</code>, we must first define a problem. We’ll do the same as we did with Morris. This<a id="_idIndexMarker994"/> time, we will reduce the factors because we realized that the <code class="inlineCode">weather</code> grouping led to inconclusive results. We should include the least sparse of all the weather factors; that is, <code class="inlineCode">weather_Clear</code>. And since Sobol uses a probabilistic framework, there’s no harm in expanding the bounds to their minimum and maximum values for <code class="inlineCode">temp</code>, <code class="inlineCode">rain_1h</code>, and <code class="inlineCode">cloud_coverage</code>, as seen in <em class="italic">Figure 9.12</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">sobol_problem = {
    <span class="hljs-string">'num_vars'</span>: <span class="hljs-number">8</span>,
    <span class="hljs-string">'names'</span>: [<span class="hljs-string">'dow'</span>, <span class="hljs-string">'hr'</span>, <span class="hljs-string">'temp'</span>, <span class="hljs-string">'rain_1h'</span>, <span class="hljs-string">'snow_1h'</span>,
              <span class="hljs-string">'cloud_coverage'</span>, <span class="hljs-string">'is_holiday'</span>, <span class="hljs-string">'weather_Clear'</span>],
    <span class="hljs-string">'</span><span class="hljs-string">bounds'</span>: [
        [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>], <span class="hljs-comment"># dow Monday through Friday</span>
        [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>], <span class="hljs-comment"># hr</span>
        [-<span class="hljs-number">3.</span>, <span class="hljs-number">31.</span>], <span class="hljs-comment"># temp (C)</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">21.</span>], <span class="hljs-comment"># rain_1h</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">1.6</span>], <span class="hljs-comment"># snow_1h</span>
        [<span class="hljs-number">0.</span>, <span class="hljs-number">100.</span>], <span class="hljs-comment"># cloud_coverage</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-comment"># is_holiday</span>
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] <span class="hljs-comment"># weather_Clear</span>
      ],
    <span class="hljs-string">'groups'</span>: <span class="hljs-literal">None</span>
}
</code></pre>
    <p class="normal">Generating the samples should look familiar too. The Saltelli <code class="inlineCode">sample</code> function requires the following:</p>
    <ul>
      <li class="bulletList">A problem statement (<code class="inlineCode">sobol_problem</code>)</li>
      <li class="bulletList">A number of samples to produce per factor (<code class="inlineCode">300</code>)</li>
      <li class="bulletList">Second-order indices to compute (<code class="inlineCode">calc_second_order=True</code>)</li>
    </ul>
    <p class="normal">Given that we want the interactions, the output of <code class="inlineCode">sample</code> is an array that has one column for each factor and <img src="../Images/B18406_09_003.png" alt="" role="presentation"/> rows (where <em class="italic">N</em> is the number of samples and <em class="italic">F</em> is the number of factors). We have eight factors and 256 samples per factor, so <code class="inlineCode">print</code> should output <a id="_idIndexMarker995"/>a shape of 4,608 rows and 8 columns. First, we will modify it, as we did previously, with <code class="inlineCode">hstack</code> to add the 7 empty factors <a id="_idIndexMarker996"/>needed to make the predictions, resulting in 15 columns instead:</p>
    <pre class="programlisting code"><code class="hljs-code">saltelli_sample = ss.sample(
    sobol_problem, <span class="hljs-number">256</span>, calc_second_order=<span class="hljs-literal">True</span>, seed=rand
)
saltelli_sample_mod = np.hstack(
    (saltelli_sample, np.zeros((saltelli_sample.shape[<span class="hljs-number">0</span>],<span class="hljs-number">7</span>)))
)
<span class="hljs-built_in">print</span>(saltelli_sample_mod.shape)
</code></pre>
    <p class="normal">Now, let’s predict on these samples. This should take a while, so it’s coffee time once more:</p>
    <pre class="programlisting code"><code class="hljs-code">saltelli_preds = mldatasets.pprox._predict_ts(
    saltelli_sample_mod,
    X_df,
    gen_all,
    lstm_traffic_mdl,
    filt_fn=filt_fn,
    dist_metric=heom_dist.heom,
    lookback=lookback,
    X_scaler=X_scaler,
    y_scaler=y_scaler,
    progress_bar=<span class="hljs-literal">True</span>
)
</code></pre>
    <h2 id="_idParaDest-256" class="heading-2">Performing Sobol sensitivity analysis</h2>
    <p class="normal">For <a id="_idIndexMarker997"/>Sobol sensitivity analysis (<code class="inlineCode">analyze</code>), all you need is a problem statement (<code class="inlineCode">sobol_problem</code>) and the model outputs (<code class="inlineCode">saltelli_preds</code>). But the predictions don’t tell the story of uncertainty. Sure, there’s variance in the predicted traffic, but that traffic is only a problem once it exceeds 1,500. Uncertainty is something you want to relate to risk or reward, costs or revenue, loss or profit – something tangible you can connect to your problem.</p>
    <p class="normal">First, we must <a id="_idIndexMarker998"/>assess if there’s any risk at all. To get an idea of whether the predicted traffic in the samples exceeded the no-construction threshold during working hours, we can use <code class="inlineCode">print(max(saltelli_preds[:,0]))</code>. The maximum traffic level should be somewhere in the neighborhood of 1,800-1,900, which means that there’s at least some risk that the construction company will pay a fine. Instead of using the predictions (<code class="inlineCode">saltelli_preds</code>) as the model’s output, we can create a simple binary array with ones when it exceeded 1,500 and zero otherwise. We will call this <code class="inlineCode">costs</code>, and then run the <code class="inlineCode">analyze</code> function with it. Note that <code class="inlineCode">calc_second_order=True</code> is also set here. It will throw an error if <code class="inlineCode">sample</code> and <code class="inlineCode">analyze</code> don’t have a consistent setting. Like with Morris, there’s an optional confidence interval level argument (<code class="inlineCode">conf_level</code>), but the default of 0.95 is good:</p>
    <pre class="programlisting code"><code class="hljs-code">costs = np.where(saltelli_preds &gt; <span class="hljs-number">1500</span>, <span class="hljs-number">1</span>,<span class="hljs-number">0</span>)[:,<span class="hljs-number">0</span>]
factor_fixing_sa = sa.analyze(
    sobol_problem,
    costs,
    calc_second_order=<span class="hljs-literal">True</span>,
    print_to_console=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal"><code class="inlineCode">analyze</code> will return a dictionary with the Sobol sensitivity indices, including the first-order (<code class="inlineCode">S1</code>), second-order (<code class="inlineCode">S2</code>), and total-order (<code class="inlineCode">ST</code>) indices, as well as the total confidence bounds (<code class="inlineCode">ST_conf</code>). The indices correspond to percentages, but the totals won’t necessarily add up unless the model is additive. It’s easier to appreciate these values in a tabular format so that we can place them into a DataFrame and sort and color-code them according to the total, which can be interpreted as the overall importance of the factor. However, we will leave the second-order indices out because they are two-dimensional and akin to a correlation plot:</p>
    <pre class="programlisting code"><code class="hljs-code">sobol_df = pd.DataFrame(
    {
        <span class="hljs-string">'features'</span>:sobol_problem[<span class="hljs-string">'names'</span>],
        <span class="hljs-string">'1st'</span>:factor_fixing_sa[<span class="hljs-string">'S1'</span>],
        <span class="hljs-string">'Total'</span>:factor_fixing_sa[<span class="hljs-string">'ST'</span>],
        <span class="hljs-string">'Total Conf'</span>:factor_fixing_sa[<span class="hljs-string">'ST_conf'</span>],
        <span class="hljs-string">'Mean of Input'</span>:saltelli_sample.mean(axis=<span class="hljs-number">0</span>)[:<span class="hljs-number">8</span>]
    }
)
sobol_df.sort_values(<span class="hljs-string">'Total'</span>, ascending=<span class="hljs-literal">False</span>).style
    .background_gradient(cmap=<span class="hljs-string">'plasma'</span>, subset=[<span class="hljs-string">'Total'</span>])
</code></pre>
    <p class="normal">The preceding<a id="_idIndexMarker999"/> code outputs the DataFrame depicted in <em class="italic">Figure 9.15</em>. You can tell that <code class="inlineCode">temp</code> and <code class="inlineCode">is_holiday</code> are in the top four, at least during the bounds specified in the problem definition (<code class="inlineCode">sobol_problem</code>). Another thing to note is that <code class="inlineCode">weather_Clear</code> does have more of an effect on its own, but <code class="inlineCode">rain_1h</code> and <code class="inlineCode">cloud_coverage</code> seem to have no effect on the potential cost because they have zero total first-order indices:</p>
    <figure class="mediaobject"><img src="../Images/B18406_09_15.png" alt="Timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.15: Sobol global sensitivity indices for the eight factors</p>
    <p class="normal">Something interesting about the first-order values is how low they are, suggesting that interactions account for most of the model output variance. We can easily produce a heatmap with second-order indices to corroborate this. It’s the combination of these indices and the first-order ones that add up to the totals:</p>
    <pre class="programlisting code"><code class="hljs-code">S2 = factor_fixing_sa[<span class="hljs-string">'S2'</span>]
divnorm = TwoSlopeNorm(vmin=S2.<span class="hljs-built_in">min</span>(), vcenter=<span class="hljs-number">0</span>, vmax=S2.<span class="hljs-built_in">max</span>())
sns.heatmap(S2, center=<span class="hljs-number">0.00</span>, norm=divnorm, cmap=<span class="hljs-string">'coolwarm_r'</span>,\
            annot=<span class="hljs-literal">True</span>, fmt =<span class="hljs-string">'.2f'</span>,\
            xticklabels=sobol_problem[<span class="hljs-string">'names'</span>],\
            yticklabels=sobol_problem[<span class="hljs-string">'names'</span>])
</code></pre>
    <p class="normal">The preceding code outputs the heatmap in <em class="italic">Figure 9.16</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_16.png" alt="Chart, waterfall chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.16: Sobol second-order indices for the eight factors</p>
    <p class="normal">Here, you<a id="_idIndexMarker1000"/> can tell that <code class="inlineCode">is_holiday</code> and <code class="inlineCode">weather_Clear</code> are the two factors that contribute the most to the output variance with the highest absolute value of 0.26. <code class="inlineCode">dow</code> and <code class="inlineCode">hr</code> have sizable interactions with all the factors.</p>
    <h2 id="_idParaDest-257" class="heading-2">Incorporating a realistic cost function</h2>
    <p class="normal">Now, we <a id="_idIndexMarker1001"/>can create a cost function that takes our inputs (<code class="inlineCode">saltelli_sample</code>) and outputs (<code class="inlineCode">saltelli_preds</code>) and computes how much the twin cities would fine the construction company, plus any additional costs the additional traffic could produce. </p>
    <p class="normal">It is better to do this if both the input and outputs are in the same array because we will need details from both to calculate the costs. We can use <code class="inlineCode">hstack</code> to join the samples and their corresponding predictions, producing an array with eight columns (<code class="inlineCode">saltelli_sample_preds</code>). We can then define a cost function that can compute the costs (<code class="inlineCode">cost_fn</code>), given an array with these nine columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Join input and outputs into a sample+prediction array</span>
saltelli_sample_preds = np.hstack((saltelli_sample, saltelli_preds))
</code></pre>
    <p class="normal">We know that <a id="_idIndexMarker1002"/>the half-capacity threshold wasn’t exceeded for any sample predictions, so we won’t even bother to include the daily penalty in the function. Besides that, the fines are $15 per vehicle that exceeds the hourly no-construction threshold. In addition to these fines, to be able to leave on time, the construction company estimates additional costs: $1,500 in extra wages if the threshold is exceeded at 4 a.m. and $4,500 more on Fridays to speed up the moving of their equipment because it can’t stay on the highway shoulder during weekends. Once we have the cost function, we can iterate through the combined array (<code class="inlineCode">saltelli_sample_preds</code>), calculating costs for each sample. List comprehension can do this efficiently:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Define cost function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">cost_fn</span>(<span class="hljs-params">x</span>):
    cost = <span class="hljs-number">0</span>
    <span class="hljs-keyword">if</span> x[<span class="hljs-number">8</span>] &gt; <span class="hljs-number">1500</span>:
        cost = (x[<span class="hljs-number">8</span>] - <span class="hljs-number">1500</span>) * <span class="hljs-number">15</span>
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(x[<span class="hljs-number">1</span>]) == <span class="hljs-number">4</span>:
        cost = cost + <span class="hljs-number">1500</span>
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(x[<span class="hljs-number">0</span>]) == <span class="hljs-number">4</span>:
            cost = cost + <span class="hljs-number">4500</span>
    <span class="hljs-keyword">return</span> cost
<span class="hljs-comment">#Use list comprehension to compute costs for sample+prediction array</span>
costs2 = np.array([cost_fn(xi) <span class="hljs-keyword">for</span> xi <span class="hljs-keyword">in</span> saltelli_sample_preds])
<span class="hljs-comment">#Print total fines for entire sample predictions</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">'</span><span class="hljs-string">Total Fines: $%s'</span> % <span class="hljs-string">'{:,.2f}'</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">sum</span>(costs2)))
</code></pre>
    <p class="normal">The <code class="inlineCode">print</code> statement should output a cost somewhere between $170,000 and $200,000. But not to worry! The construction crew only plans to work about 195 days on-site per year and 5 hours each day, for a total of 975 hours. However, there are 4,608 samples, which means that there are almost 5 years’ worth of predicted costs due to excess traffic. In any case, the point of calculating these costs is to figure out how they relate to the model’s inputs. More years’ worth of samples means tighter confidence intervals:</p>
    <pre class="programlisting code"><code class="hljs-code">factor_fixing2_sa = sa.analyze(
    sobol_problem, costs2, calc_second_order=<span class="hljs-literal">True</span>,
    print_to_console=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">We can now <a id="_idIndexMarker1003"/>perform the analysis again but with <code class="inlineCode">costs2</code>, and we can save the analysis into a <code class="inlineCode">factor_fixing2_sa</code> dictionary. Lastly, we can produce a new sorted and color-coded DataFrame with this dictionary’s values, as we did previously for <em class="italic">Figure 9.15</em>, which generates the output shown in <em class="italic">Figure 9.17</em>.</p>
    <p class="normal">As you can tell by <em class="italic">Figure 9.17</em> once the actual costs have been factored in, <code class="inlineCode">dow</code>, <code class="inlineCode">hr</code>, and <code class="inlineCode">is_holiday</code> become riskier factors, while <code class="inlineCode">snow_1h</code> and <code class="inlineCode">temp</code> become less relevant when compared to <em class="italic">Figure 9.15</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_17.png" alt="Timeline  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.17: Sobol global sensitivity indices for the eight factors using the realistic cost function</p>
    <p class="normal">One thing that is hard to appreciate with a table is the confidence intervals of the sensitivity indices. For that, we can use a bar plot, but first, we must convert the entire dictionary<a id="_idIndexMarker1004"/> into a DataFrame so that <code class="inlineCode">SALib's</code> plotting function can plot it:</p>
    <pre class="programlisting code"><code class="hljs-code">factor_fixing2_df = factor_fixing2_sa.to_df()
fig, (ax) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">7</span>))
sp.plot(factor_fixing2_df[<span class="hljs-number">0</span>], ax=ax)
</code></pre>
    <p class="normal">The preceding code generates the bar plot in <em class="italic">Figure 9.18</em>. The 95% confidence interval for <code class="inlineCode">dow</code> is much larger than for other important factors, which shouldn’t be surprising considering how much variance there is between days of the week. Another interesting insight is how <code class="inlineCode">weather_Clear</code> has negative first-order effects, so the positive total-order indices are entirely attributed to second-order ones, which expand the confidence interval:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_18.png" alt="Chart, scatter chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.18: Bar plot with the Sobol sensitivity total-order indices and their confidence intervals using a realistic cost function</p>
    <p class="normal">To understand how, let’s plot the heatmap shown in <em class="italic">Figure 9.16</em> again but this time using <code class="inlineCode">factor_fixing2_sa</code> instead of <code class="inlineCode">factor_fixing_sa</code>. The heatmap in <em class="italic">Figure 9.19</em> should depict how the realistic costs reflect the interactions in the model:</p>
    <p class="packt_figref"><img src="../Images/B18406_09_19.png" alt="Chart, waterfall chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 9.19: Sobol second-order indices for seven factors while factoring a more realistic cost function</p>
    <p class="normal">The<a id="_idIndexMarker1005"/> preceding heatmap shows similar salient interactions to those in <em class="italic">Figure 9.16</em> but they’re much more nuanced since there are more shades. It becomes evident that <code class="inlineCode">weather_Clear</code> has a magnifying effect when combined with <code class="inlineCode">is_holiday</code>, and a tempering effect for <code class="inlineCode">dow</code> and <code class="inlineCode">hr</code>.</p>
    <h1 id="_idParaDest-258" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission was to train a traffic prediction model and understand what factors create uncertainty and possibly increase costs for the construction company. We can conclude a significant portion of the potential $35,000/year in fines can be attributed to the <code class="inlineCode">is_holiday</code> factor. Therefore, the construction company should rethink working holidays. There are only seven or eight holidays between March and November, and they could cost more because of the fines than working on a few Sundays instead. With this caveat, the mission was successful, but there’s still a lot of room for improvement.</p>
    <p class="normal">Of course, these conclusions are for the <code class="inlineCode">LSTM_traffic_168_compact1</code> model – which we can compare with other models. Try replacing the <code class="inlineCode">model_name</code> at the beginning of the notebook with <code class="inlineCode">LSTM_traffic_168_compact2</code>, an equally small but significantly more robust model, or <code class="inlineCode">LSTM_traffic_168_optimal</code>, a larger slightly better-performing model, and re-running the notebook. Or glance at the notebooks named <code class="inlineCode">Traffic_compact2</code> and <code class="inlineCode">Traffic_optimal</code>, which already have been re-run with these corresponding models. You will find that it is possible to train and select models that manage uncertain inputs much better. That being said, improvement doesn’t always come by simply selecting a better model.</p>
    <p class="normal">For instance, one thing that could be covered in further depth is the true impact of <code class="inlineCode">temp</code>, <code class="inlineCode">rain_1h</code>, and <code class="inlineCode">snow_1h</code>. Our prediction approximation method precluded Sobol from testing the effect of extreme weather events. If we modified the model to train on aggregated weather features at single timesteps and built in some guardrails, we could simulate weather extremes with Sobol. And the “third setting” of sensitivity analysis, known as factor mapping, could help pinpoint how exactly some factor values affect the predicted outcome, leading to a sturdier cost-benefit analysis, but we won’t cover that in this chapter.</p>
    <p class="normal">Throughout <em class="italic">Part Two</em> of this book, we explored an ecosystem of interpretation methods: global and local; model-specific and model-agnostic; permutation-based and sensitivity-based. There’s no shortage of interpretation methods to choose from for any machine learning use case. However, it cannot be stressed enough that <em class="italic">NO method is perfect</em>. Still, they can complement each other to approximate a better understanding of your machine learning solution and the problem it aims to solve.</p>
    <p class="normal">This chapter’s focus on certainty in forecasting was designed to shed light on a particular problem in the machine learning community: overconfidence. <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, Explainability; and Why Does It All Matter?</em>, in the <em class="italic">A business case of interpretability</em> section, described the many biases that infest human decision-making. These biases are often fueled by overconfidence in domain knowledge or our models’ impressive results. And these impressive results cloud us from grasping the limitations of our models as the public distrust of AI increases.</p>
    <p class="normal">As we discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, Explainability; and Why Does It All Matter?</em>, machine learning is only meant to tackle <em class="italic">incomplete problems</em>. Otherwise, we might as well use deterministic and procedural programming like those found in closed-loop systems. The best we can do to solve an incomplete problem is an incomplete solution, which should be optimized to solve as much of it as possible. Whether through gradient descent, least-squares estimation, or splitting and pruning a decision tree, machine learning doesn’t produce a model that generalizes perfectly. That lack of completeness in machine learning is precisely why we need interpretation methods. In a nutshell: models learn from our data, and we can learn a lot from our models, but only if we interpret them!</p>
    <p class="normal">Interpretability doesn’t stop there, though. Model interpretations can drive decisions and help us understand model strengths and weaknesses. However, often, there are problems in the data or models themselves that can make them less interpretable. In <em class="italic">Part Three</em> of this book, we’ll learn how to tune models and the training data for interpretability by reducing complexity, mitigating bias, placing guardrails, and enhancing reliability.</p>
    <p class="normal">Statistician George E.P. Box famously quipped that “<em class="italic">all models are wrong, but some are useful</em>.” Perhaps they aren’t always wrong, but humility is required from machine learning practitioners to accept that even high-performance models should be subject to scrutiny and our assumptions about them. Uncertainty with machine learning models is expected and shouldn’t be a source of shame or embarrassment. This leads us to another takeaway from this chapter: that uncertainty comes with ramifications, be it costs or profit lift, and that we can gauge these with sensitivity analysis.</p>
    <h1 id="_idParaDest-259" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should understand how to assess a time series model’s predictive performance, know how to perform local interpretations for them with integrated gradients, and know how to produce both local and global attributions with SHAP. You should also know how to leverage sensitivity analysis factor prioritization and factor fixing for any model.</p>
    <p class="normal">In the next chapter, we will learn how to reduce the complexity of a model and make it more interpretable with feature selection and engineering.</p>
    <h1 id="_idParaDest-260" class="heading-1">Dataset and image sources</h1>
    <ul>
      <li class="bulletList">TomTom, 2019, Traffic Index: <a href="https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf"><span class="url">https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf</span></a></li>
      <li class="bulletList">UCI Machine Learning Repository, 2019, Metro Interstate Traffic Volume Data Set: <a href="https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume"><span class="url">https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume</span></a></li>
    </ul>
    <h1 id="_idParaDest-261" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Wilson, D.R., and Martinez, T., 1997, <em class="italic">Improved Heterogeneous Distance Functions</em>. J. Artif. Int. Res. 6-1. pp.1-34: <a href="https://arxiv.org/abs/cs/9701101"><span class="url">https://arxiv.org/abs/cs/9701101</span></a></li>
      <li class="bulletList">Morris, M., 1991, <em class="italic">Factorial sampling plans for preliminary computational experiments</em>. Quality Engineering, 37, 307-310: <a href="https://doi.org/10.2307%2F1269043"><span class="url">https://doi.org/10.2307%2F1269043</span></a></li>
      <li class="bulletList">Saltelli, A., Tarantola, S., Campolongo, F., and Ratto, M., 2007, <em class="italic">Sensitivity analysis in practice: A guide to assessing scientific models</em>. Chichester: John Wiley &amp; Sons.</li>
      <li class="bulletList">Sobol, I.M., 2001, <em class="italic">Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates</em>. MATH COMPUT SIMULAT,55(1–3),271-280: <a href="https://doi.org/10.1016/S0378-4754(00)00270-6"><span class="url">https://doi.org/10.1016/S0378-4754(00)00270-6</span></a></li>
      <li class="bulletList">Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and S. Tarantola, 2010, <em class="italic">Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index</em>. Computer Physics Communications, 181(2):259-270: <a href="https://doi.org/10.1016/j.cpc.2009.09.018 "><span class="url">https://doi.org/10.1016/j.cpc.2009.09.018</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_9.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>