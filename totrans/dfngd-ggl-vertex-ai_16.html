<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer163">
			<h1 id="_idParaDest-233" class="chapter-number"><a id="_idTextAnchor233"/>16</h1>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor234"/>Vision-Based Defect Detection System – Machines Can See Now!</h1>
			<p><strong class="bold">Computer Vision</strong> (<strong class="bold">CV</strong>) is a field of artificial intelligence concerned with giving machines the ability to analyze and extract meaningful information from digital images, videos, and other visual input, as well as take actions or make recommendations based on the extracted information. Decades of research in the field of CV have led to the development of powerful <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>)-based vision algorithms that are capable of classifying images into some pre-defined categories, detecting objects from images, understanding written content from digital images, and detecting actions being performed in videos. Such vision algorithms have given businesses and organizations the ability to analyze large amounts of digital content (images and videos) and also automate processes to make <span class="No-Break">instant decisions.</span></p>
			<p>CV-based algorithms have changed the way we interact with smart devices in our day-to-day life – for example, we can now unlock smartphones by just showing our face, and photo editing apps today can make us look younger or older. Another important use case of applying CV-based ML algorithms is defect detection. ML algorithms can be leveraged to analyze visual input and detect defects in product images, which can be quite useful for <span class="No-Break">manufacturing industries.</span></p>
			<p>In this chapter, we will develop a real-world defect detection solution using deep learning on Google Cloud. We will also see how to deploy our vision-based defect detection model as a Vertex AI endpoint so that it can be utilized for online prediction. </p>
			<p>This chapter covers the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Vision-based <span class="No-Break">defect detection</span></li>
				<li>Deploying a vision model to a Vertex <span class="No-Break">AI endpoint</span></li>
				<li>Getting online predictions from a <span class="No-Break">vision model</span></li>
			</ul>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor235"/>Technical requirements</h1>
			<p>The code samples used in this chapter can be found at the following GitHub <span class="No-Break">address: </span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor236"/>Vision-based defect detection</h1>
			<p>CV is capable <a id="_idIndexMarker1063"/>nowadays of detecting visual defects on object surfaces or inconsistencies in their designs (such as dents or scratches on a car body), by just analyzing their digital photographs or videos. The manufacturing industry can leverage CV algorithms to automatically detect and remove low-quality and defected products from being packed and reaching customers. There are many possible ways to detect defects within digital content using CV-based algorithms. One simple idea is to solve defect detection as a classification problem, where a vision model can be trained to classify images such as <em class="italic">good</em> or <em class="italic">defected</em>. A more complex defect detection system will also locate the exact area of an image with a defect. The problem of identifying and locating visual defects can be solved using <span class="No-Break">object-detection algorithms.</span></p>
			<p>In this section, we will build and train a simple defect detection system step by step. In this example, we will use ML classification as a mechanism to detect visually defected products. Let’s explore <span class="No-Break">the example.</span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor237"/>Dataset</h2>
			<p>For this experiment, we <a id="_idIndexMarker1064"/>have downloaded an open source dataset from Kaggle. The dataset has over a thousand colored images of glass bangles. These images contain bangles of different sizes and colors and can be classified into three major categories, based on their manufacturing quality and damages – good, defected, and broken. Defected bangles may have a manufacturing defect such as invariable width or improper circular shape, while broken bangles would have some piece of the circle missing. In this experiment, we will utilize some of these images to train an ML classification model and test it on a few of the unseen samples. Images are already separated into the aforementioned categories, so there’s no need for manual data annotation. The dataset used in this experiment can be downloaded from the Kaggle link provided in the Jupyter Notebook corresponding to this experiment. The GitHub location of the code samples is presented at the beginning of this chapter in the <em class="italic">Technical </em><span class="No-Break"><em class="italic">requirements</em></span><span class="No-Break"> section.</span></p>
			<p>We have already downloaded and extracted the dataset into the same directory as our Jupyter Notebook. Now, we can start looking at some of the image samples. Let’s get into the coding <span class="No-Break">part now.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor238"/>Importing useful libraries</h2>
			<p>The first step is to <a id="_idIndexMarker1065"/>import useful Python packages for our experiment. Here, <strong class="source-inline">cv2</strong> refers to the OpenCV library, which has lots of prebuilt functionalities for dealing with images and other <span class="No-Break">CV tasks:</span></p>
			<pre class="source-code">
import numpy as np
import glob
import matplotlib.pyplot as plt
import cv2
from tqdm import tqdm_notebook
import tensorflow
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
%matplotlib inl<a id="_idTextAnchor239"/><a id="_idTextAnchor240"/>ine</pre>			<p>Next, let’s look at <span class="No-Break">the dataset.</span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor241"/>Loading and verifying data</h2>
			<p>Now, let’s load <a id="_idIndexMarker1066"/>all the image file paths into three separate lists, one for each category – good, defected, and broken. Keeping three separate lists will make it easier to keep track of image labels. Let’s also print the exact number of images within <span class="No-Break">each category:</span></p>
			<pre class="source-code">
good_bangle_paths = glob.glob("dataset/good/*.jpg")
defected_bangle_paths = glob.glob("dataset/defect/*.jpg")
broken_bangle_paths = glob.glob("dataset/broken/*.jpg")
print(len(good_bangle_paths), len(defected_bangle_paths), \
    len(broken_bangle_paths))</pre>			<p>Here is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
520 244 316</pre>			<p>We have 520 good-quality images, 244 defected images, and 316 broken bangle images in total. Next, let’s verify<a id="_idIndexMarker1067"/> a few samples from each category by plotting them <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">matplot<a id="_idTextAnchor242"/><a id="_idTextAnchor243"/>lib</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor244"/>Checking few samples</h2>
			<p>In this step, we <a id="_idIndexMarker1068"/>will randomly choose a few image paths from each of the previously discussed lists and plot them with their category name as <span class="No-Break">their title.</span></p>
			<p>Let’s plot a few good <span class="No-Break">bangle images:</span></p>
			<pre class="source-code">
plt.figure(figsize=(10, 10))
for ix, img_path in enumerate( \
    np.random.choice(good_bangle_paths, size=5)
):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(550 + 1 + ix)
    plt.imshow(img)
    plt.axis('off')
    plt.title('Good Bangle!')
plt.show()
print("-"*101)</pre>			<p>Similarly, we will plot a few random defective <span class="No-Break">bangle pieces:</span></p>
			<pre class="source-code">
plt.figure(figsize=(10, 10))
for ix, img_path in enumerate( \
    np.random.choice(defected_bangle_paths, size=5)
):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(550 + 1 + ix)
    plt.imshow(img)
    plt.axis('off')
    plt.title('Defected Bangle!')
plt.show()
print("-"*101)</pre>			<p>Finally, we will also<a id="_idIndexMarker1069"/> plot some broken bangle images in a similar way so that we can see all three categories visually and learn more about <span class="No-Break">the data:</span></p>
			<pre class="source-code">
plt.figure(figsize=(10, 10))
for ix, img_path in enumerate( \
    np.random.choice(broken_bangle_paths, size=5)
):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(550 + 1 + ix)
    plt.imshow(img)
    plt.axis('off')
    plt.title('Broken Bangle!')
plt.show()</pre>			<p>The output of the preceding scripts is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.1</em> where each row represents a few random samples from each of the aforementioned categories. The category name is present in the title <span class="No-Break">of images.</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17792_16_1.jpg" alt="Figure 16.1 – Bangle image samples from each category – good, defected, and broken" width="1169" height="821"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.1 – Bangle image samples from each category – good, defected, and broken</p>
			<p>As shown in <a id="_idIndexMarker1070"/>the preceding figure, good bangles are the ones that look like a perfect circle with an even width, defected bangles may have some uneven width or surface, while broken bangles are easily distinguishable, as they are missing some part of their circular shape. So, differentiating between good and defected bangles can be a bit challenging, but differentiating both of these categories from the broken bangles should be easier for the algorithm. Let’s see how it goes in the next <span class="No-Break">few se<a id="_idTextAnchor245"/><a id="_idTextAnchor246"/>ctions.</span></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor247"/>Data preparation</h2>
			<p>In this step, we <a id="_idIndexMarker1071"/>will prepare data for our <strong class="bold">TensorFlow</strong> (<strong class="bold">TF</strong>)-based <a id="_idIndexMarker1072"/>deep learning model. A TF-based model requires all the inputs to have the same shape, so the first step would be to make all the input images the same shape. We will also downgrade the quality of images a bit so that they are more memory-efficient while reading them and performing calculations over them during training. We have to keep one thing in mind – we can’t degrade the data quality significantly such that it becomes hard for the model to find any visual clues to detect defected surfaces. For this experiment, we will resize each image to a 200x200 resolution. Secondly, as we are only concerned about finding defects, the color of the image is not important to us. So, we will convert all the colored images into grayscale images, as it reduces the images’ channels from 3 to 1; thus, the image size becomes a third of its original size. Finally, we will convert them into NumPy arrays, as TF models require the input tensors to be <span class="No-Break">NumP<a id="_idTextAnchor248"/><a id="_idTextAnchor249"/>y arrays.</span></p>
			<p>The following are the pre-processing steps that we will follow for each image in <span class="No-Break">the dataset:</span></p>
			<ul>
				<li>Reading the colored image using the <span class="No-Break">OpenCV library</span></li>
				<li>Resizing an image to a fixed size (200 <span class="No-Break">x 200)</span></li>
				<li>Converting image to grayscale (black <span class="No-Break">and white)</span></li>
				<li>Converting the list of images into <span class="No-Break">NumPy arrays</span></li>
				<li>Adding one channel dimensions, as required by <span class="No-Break">convolutional layers</span></li>
			</ul>
			<p>Let’s first follow these steps for good <span class="No-Break">bangle images:</span></p>
			<pre class="source-code">
good_bangles = []
defected_bangles = []
broken_bangles = []
for img_path in tqdm_notebook(good_bangle_paths):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (200, 200))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    good_bangles.append(img)
good_bangles = np.array(good_bangles)
good_bangles = np.expand_dims(good_bangles, axis=-1)</pre>			<p>Similarly, we will pre-process the defected <span class="No-Break">bangle images:</span></p>
			<pre class="source-code">
for img_path in tqdm_notebook(defected_bangle_paths):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (200, 200))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    defected_bangles.append(img)
defected_bangles = np.array(defected_bangles)
defected_bangles = np.expand_dims(defected_bangles, axis=-1)</pre>			<p>Finally, we will <a id="_idIndexMarker1073"/>follow the same pre-processing steps for the broken bangle images. We will also print the final shapes of NumPy arrays for each category <span class="No-Break">of images:</span></p>
			<pre class="source-code">
for img_path in tqdm_notebook(broken_bangle_paths):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (200, 200))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    broken_bangles.append(img)
broken_bangles = np.array(broken_bangles)
broken_bangles = np.expand_dims(broken_bangles, axis=-1)
print(good_bangles.shape, defected_bangles.shape, \
    broken_bangles.shape)</pre>			<p>Each image array now will have the shape of 200x200x1, and the first dimension will represent the total number of images. The following is the output of the <span class="No-Break">previous script:</span></p>
			<pre class="source-code">
(520, 200, 200, 1) (244, 200, 200, 1) (316, 20<a id="_idTextAnchor250"/><a id="_idTextAnchor251"/>0, 200, 1)</pre>			<p>Now, let’s <a id="_idIndexMarker1074"/>split the data into training and <span class="No-Break">test partitions.</span></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor252"/>Splitting data into train and test</h2>
			<p>As our data is now <a id="_idIndexMarker1075"/>ready and compatible with TF-model format, we just need to perform one last important step of splitting the data into two partitions – train and test. The <em class="italic">train</em> partition will be shown to the model for learning purposes during training of the model, and the <em class="italic">test</em> partition will be kept separate and will not contribute in the model parameters’ update. Once the model is trained, we will check how well it performs on the unseen test partition <span class="No-Break">of data.</span></p>
			<p>As our test data should also have a significant number of samples from each category, we will divide each of the image categories into train and test partitions and, finally, merge all the train and test partitions together. We will utilize the first 75% of the images from each category array for training and the rest of the 25% images for testing purposes, as shown in the <span class="No-Break">followi<a id="_idTextAnchor253"/>ng snippet:</span></p>
			<pre class="source-code">
good_bangles_train = good_bangles[:int( \
    len(good_bangles)*0.75),]
good_bangles_test = good_bangles[int( \
    len(good_bangles)*0.75):,]
defected_bangles_train = defected_bangles[:int( \
    len(defected_bangles)*0.75),]
defected_bangles_test = defected_bangles[int( \
    len(defected_bangles)*0.75):,]
broken_bangles_train = broken_bangles[:int( \
    len(broken_bangles)*0.75),]
broken_bangles_test = broken_bangles[int( \
    len(broken_bangles)*0.75):,]
print(good_bangles_train.shape, good_bangles_test.shape)</pre>			<p>The previously defined code also prints the shape of the train and test partitions just for data verification purposes. The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
(390, 200, 200, 1) (130, 200, 200, 1)</pre>			<p>Our training and test<a id="_idIndexMarker1076"/> data is almost ready now; we just need corresponding labels for model training. In the next step, we will create label arrays for <span class="No-Break">both partitions.</span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor254"/>Final preparation of training and testing data</h2>
			<p>Now that we have <a id="_idIndexMarker1077"/>three pairs of train and test partitions (one for each category), let’s combine them into a single pair of train and test partitions for model training and testing purpose. After concatenating these NumPy arrays, we will also perform reshuffling so that images from each of the categories are well-mixed. This is important, as during training we will only send small batches to the model, so for smooth training of the model, each batch should have samples from all the classes. As test data is kept separate, there is no need to <span class="No-Break">shuffle it.</span></p>
			<p>Additionally, we also need to create corresponding label arrays as well. As ML algorithms only support numeric data, we need to encode our output categories into some numeric values. We can represent our three categories with three numbers – 0, 1, <span class="No-Break">and 2.</span></p>
			<p>We will use the following label mapping rule to encode <span class="No-Break">our categories:</span></p>
			<ul>
				<li><strong class="bold">Good –  0</strong></li>
				<li><strong class="bold">Defected –  1</strong></li>
				<li><strong class="bold">Broken –</strong><strong class="bold">  2</strong></li>
			</ul>
			<p>The following code snippet concatenates all the training partitions into a single train partition and also creates a label array, using the aforementioned <span class="No-Break">mapping rule:</span></p>
			<pre class="source-code">
all_train_images = np.concatenate((good_bangles_train, \
    defected_bangles_train, broken_bangles_train), axis=0)
all_train_labels = np.concatenate((
    np.array([0]*len(good_bangles_train)),
    np.array([1]*len(defected_bangles_train)),
    np.array([2]*len(broken_bangles_train))),
    axis=0
)</pre>			<p>Similarly, we will concatenate all the test partitions into a single test partition. We will also create a label array for our test partition, which will help us to check the accuracy metrics of our trained model. Here, we also print the final shapes of the train and test partitions, as <a id="_idIndexMarker1078"/>shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
all_test_images = np.concatenate((good_bangles_test, \
    defected_bangles_test, broken_bangles_test), axis=0)
all_test_labels = np.concatenate((
    np.array([0]*len(good_bangles_test)),
    np.array([1]*len(defected_bangles_test)),
    np.array([2]*len(broken_bangles_test))),
    axis=0
)
print(all_train_images.shape, all_train_labels.shape)
print(all_test_images.shape, all_test_labels.shape)</pre>			<p>The output of this script shows that we have 810 training images and 270 test images in total, as shown in the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
(810, 200, 200, 1) (810,)
(270, 200, 200, 1) (270,)</pre>			<p>As discussed before, it is very important to shuffle our training partition, ensuring that images from each category are well-mixed and each batch will have a good variety. The important thing to keep in mind while shuffling is that we also need to shuffle the label array accordingly to avoid any data label mismatches. For this purpose, we have defined a Python function that shuffles two given arr<a id="_idTextAnchor255"/>ays <span class="No-Break">in unison:</span></p>
			<pre class="source-code">
def unison_shuffled_copies(a, b):
    assert len(a) == len(b)
    p = np.random.permutation(len(a))
    return a[p], b[p]
all_train_images, all_train_labels = unison_shuffled_copies( \
    all_train_images, all_train_labels)
all_test_images, all_test_labels = unison_shuffled_copies( \
    all_test_images, all_test_labels)</pre>			<p>Our training and<a id="_idIndexMarker1079"/> testing dataset is now all set. We can now move to the <span class="No-Break">model architecture.</span></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor256"/>TF model architecture</h2>
			<p>In this section, we<a id="_idIndexMarker1080"/> will define a model<a id="_idIndexMarker1081"/> architecture for our TF-based deep learning classification model. As we are dealing with images in this experiment, we will utilize <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) layers to learn and extract important features from<a id="_idIndexMarker1082"/> training images. CNNs have proved to be quite useful in the field of CV. In general, a few CNN layers are stacked on top of each other to extract low-level (minor details) and high-level (big shape-related) feature information. We will also create a CNN-based feature extraction architecture in a similar way, combining it with a few fully connected layers. The final fully connected layer should have three neurons to generate output for each category, and a <em class="italic">sigmoid</em> activation layer to have that output in the form of a <span class="No-Break">probability distribution.</span></p>
			<h3>A convolutional block</h3>
			<p>We will now<a id="_idIndexMarker1083"/> define a reusable convolutional block that we can use repeatedly to create our final model architecture. In the convolutional block, we will have each convolutional layer followed by the layers of <strong class="bold">Batch Normalization</strong> (<strong class="bold">BN</strong>), <strong class="bold">ReLU</strong> activation, a max-pooling layer, and a dropout layer. Here, the<a id="_idIndexMarker1084"/> layers<a id="_idIndexMarker1085"/> of BN and dropout are for regularization purposes to ensure the smooth learning of our TF model. The following Python snippet defines our<a id="_idTextAnchor257"/> <span class="No-Break">convolutional block:</span></p>
			<pre class="source-code">
def convolution_block(data, filters, kernel, strides):
    data = tensorflow.keras.layers.Conv2D(
        filters=filters,
        kernel_size=kernel,
        strides=strides,
    )(data)
    data = tensorflow.keras.layers.BatchNormalization()(data)
    data = tensorflow.keras.layers.Activation('relu')(data)
    data = tensorflow.keras.layers.MaxPooling2D(strides=strides)(data)
    data = tensorflow.keras.layers.Dropout(0.2)(data)
    return data</pre>			<p>Now, let’s define our <a id="_idIndexMarker1086"/>complete model architecture by making use of this <span class="No-Break">convolutional block.</span></p>
			<h3>TF model definition</h3>
			<p>We can now define our<a id="_idIndexMarker1087"/> final TF model architecture, which can utilize a few iterations of the convolutional block defined in the previous step. We will first define an input layer to tell the model about the size of the input images that it will be expecting during training. As discussed earlier, each image in our dataset has a size of 200x200x1. The following Python code defines the convolution-based feature extraction part of <span class="No-Break">our network:</span></p>
			<pre class="source-code">
input_data = tensorflow.keras.layers.Input(shape=(200, 200, 1))
data = input_data
data = convolution_block(data, filters=64, kernel=2, strides=2)
data = convolution_block(data, filters=128, kernel=2, strides=2)
data = convolution_block(data, filters=256, kernel=2, strides=2)
data = convolution_block(data, filters=256, kernel=2, strides=1)</pre>			<p>Next, we will use a <strong class="source-inline">Flatten()</strong> layer to bring all the features into a single dimension and apply fully connected layers to refine these features. Finally, we use another fully connected layer with three neurons, followed by a <strong class="source-inline">softmax</strong> activation, to generate probabilistic output for three classes. We then define our model object with input and output layers and print <a id="_idIndexMarker1088"/>out a summary of the model <span class="No-Break">for reference:</span></p>
			<pre class="source-code">
data = tensorflow.keras.layers.Flatten()(data)
data = tensorflow.keras.layers.Dense(64)(data)
data = tensorflow.keras.layers.Activation('relu')(data)
data = tensorflow.keras.layers.Dense(3)(data)
output_data = tensorflow.keras.layers.Activation('softmax')(data)
model = tensorflow.keras.models.Model(inputs=input_data, \
    outputs=output_data)
model.summary()</pre>			<p>This snippet prints the model summary, which looks something similar to what is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.2</em> (for a complete summary, check out the <span class="No-Break">Jupyter Notebook):</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B17792_16_2.jpg" alt="Figure 16.2: The model Summary for the TF-based defect detection architecture" width="979" height="1240"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.2: The model Summary for the TF-based defect detection architecture</p>
			<p> Our TF model <a id="_idIndexMarker1089"/>graph is now ready, so we can now compile and fit <span class="No-Break">our model.</span></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor258"/>Compiling the model</h2>
			<p>In this step, we can<a id="_idIndexMarker1090"/> define the appropriate loss function, optimization algorithm, and metrics. As we have a multi-label classification problem, we will utilize <em class="italic">categorical cross-entropy</em> as a loss function. We will utilize the <em class="italic">Adam</em> optimizer with its default values a<a id="_idTextAnchor259"/>nd <strong class="source-inline">'accuracy'</strong> as <span class="No-Break">a metric:</span></p>
			<pre class="source-code">
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)</pre>			<p>Now, we are all<a id="_idIndexMarker1091"/> set to start training our model on the previously <span class="No-Break">curated dataset.</span></p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor260"/>Training the model</h2>
			<p>Now, we are all set <a id="_idIndexMarker1092"/>to launch the training of our model, as our data and model object are both set. We plan to train our model for 50 epochs with a batch size of 64. After each epoch, we will keep checking the model’s loss and accuracy on training and <span class="No-Break">testing partitions:</span></p>
			<pre class="source-code">
history = model.fit(
    x=all_train_images,
    y=all_train_labels,
    batch_size=64,
    epochs=50,
    validation_data=(all_test_images, all_test_labels),
)</pre>			<p>The training logs with loss and accuracy values will look similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
Epoch 1/50
13/13 [==============================] - 6s 311ms/step - loss: 1.1004 - accuracy: 0.5000 - val_loss: 7.9670 - val_accuracy: 0.2259
Epoch 2/50
13/13 [==============================] - 3s 260ms/step - loss: 0.8947 - accuracy: 0.5938 - val_loss: 11.9169 - val_accuracy: 0.2259
-     -    -    -
Epoch 50/50
13/13 [==============================] - 3s 225ms/step - loss: 0.1369 - accuracy: 0.9481 - val_loss: 0.9686 - val_accuracy: 0.6741</pre>			<p>Once the training is <a id="_idIndexMarker1093"/>complete, we can start verifying the results of <span class="No-Break">our model.</span></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor261"/>Plotting the training progress</h2>
			<p>In this step, we <a id="_idIndexMarker1094"/>will utilize the <strong class="source-inline">history</strong> variable defined in the previous step to plot the progress of the training loss, test loss, train accuracy, and test accuracy with progressing epochs. These graphs can help us understand whether our model training is going in the right direction or not. Also, we can check the ideal number of epochs required to get reasonable accuracy on <span class="No-Break">test sets.</span></p>
			<p>Let’s first plot the training and validation loss of the model with an epoch number on the <em class="italic">X</em> axis and a loss value on the <span class="No-Break"><em class="italic">Y</em></span><span class="No-Break"> axis:</span></p>
			<pre class="source-code">
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()</pre>			<p>The output of this snippet is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.3</em>. We can see in the figure that training and validation loss decrease as the training progresses, which tells us that our model training is going in the right direction and our model <span class="No-Break">is learning.</span></p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B17792_16_3.jpg" alt="Figure 16.3 – Training and validation loss" width="978" height="777"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.3 – Training and validation loss</p>
			<p>Next, we can also<a id="_idIndexMarker1095"/> plot the accuracy of the model on training and test partitions with the progress of training, as shown in <span class="No-Break">following code:</span></p>
			<pre class="source-code">
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('TF model accuracy trend.')
plt.ylabel('TF model accuracy')
plt.xlabel('epoch number')
plt.legend(['train', 'val'], loc='upper left')
plt.show()</pre>			<p>The resulting plot can be seen in <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.4</em>. We can see that training accuracy keeps increasing and reaches close to 100% as the model starts overfitting, while the validation accuracy increases to around 70% and keeps fluctuating around that. It means that our current setup is capable of achieving around 70% accuracy on the test set. This accuracy value can be improved further by either increasing the capacity of our network (by adding a few more layers), or by improving the way we extract features from the images. For our experiment, this accuracy is satisfactory, and we will move forward <span class="No-Break">with it.</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B17792_16_4.jpg" alt="Figure 16.4 – Training and validation accuracy with the progress of training epochs" width="1035" height="819"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.4 – Training and validation accuracy with the progress of training epochs</p>
			<p>As our model training is<a id="_idIndexMarker1096"/> now complete, we can start making predictions on unseen data. Let’s first check the results on our <span class="No-Break">test set.</span></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor262"/>Results</h2>
			<p>Here comes the<a id="_idIndexMarker1097"/> interesting part where we check the TF-model results on our unseen test set. Before checking the numbers, let’s first plot a few test images along with their true labels and <span class="No-Break">model outputs.</span></p>
			<h3>Checking the results on a few random test images</h3>
			<p>Let’s first visually<a id="_idIndexMarker1098"/> verify the results of the model by choosing a few random images from our test set, making predictions on them, and plotting them with model predictions along with actual labels. We will put the label and prediction information in the image titles, as shown in the following <span class="No-Break">Python snippet:</span></p>
			<pre class="source-code">
for iteration in range(2):
    plt.figure(figsize=(12, 12))
    for idx, img_idx in enumerate(np.random.permutation( \
        len(all_test_images))[:4]
    ):
        img = all_test_images[img_idx]
        label = all_test_labels[img_idx]
        pred = model.predict(np.array([img]), verbose=0)
        model_output = np.argmax(pred)
        plt.subplot(440 + 1 + idx)
        plt.imshow(img, cmap='gray')
        plt.title(f'Label: {label}, Model_output: \
            {model_output}')
        plt.axis('off')
    plt.show()</pre>			<p>The output of this snippet is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.4</em>. We can see that the model is slightly confused between class 0 and class 1 but easily identifies class 2, due to big clues on the shape. As a reminder, class 0 here represents good bangles, class 1 represents defected bangles, and class 2 represents <span class="No-Break">broken bangles.</span></p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B17792_16_5.jpg" alt="Figure 16.5 – The model results for the classification of good, defected, and broken bangles" width="1051" height="556"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.5 – The model results for the classification of good, defected, and broken bangles</p>
			<p>As per <span class="No-Break"><em class="italic">Figure 16</em></span><em class="italic">.5</em>, our model is doing a good job of identifying class 2 but is slightly confused<a id="_idIndexMarker1099"/> between class 0 and class 1, due to very tiny visual clues. Next, let’s check the metrics on the entire test set to get a sense of the quality of <span class="No-Break">our classifier.</span></p>
			<h3>Classification report</h3>
			<p>To generate the<a id="_idIndexMarker1100"/> classification report on our entire test set, we first need to generate model outputs for the entire test set. We will choose the class with maximum probability as the model prediction (note that choosing the output class like that may not be the best option when we have highly imbalanced datasets). The following is the Python code that generates the model output on the entire <span class="No-Break">test set:</span></p>
			<pre class="source-code">
test_pred = model.predict(all_test_images)
test_outputs = [np.argmax(pred) for pred in test_pred]</pre>			<p>We can now print the classification report for <span class="No-Break">our model:</span></p>
			<pre class="source-code">
print(
    classification_report(all_test_labels,
                      test_outputs,
                      target_names=['Good', 'Defected', 'Broken'],
    )
)</pre>			<p>The following is the output of the <span class="No-Break">classification report:</span></p>
			<pre class="source-code">
              precision    recall  f1-score   support
        Good       0.88      0.52      0.65       130
    Defected       0.62      0.69      0.65        61
      Broken       0.58      0.92      0.71        79
    accuracy                           0.67       270
   macro avg       0.69      0.71      0.67       270
weighted avg       0.73      0.67      0.67       270</pre>			<p>The classification report indicates that our model has a F1 score of around 0.65 for class 0 and class 1, and 0.71 for class 2. As suspected, the model is doing a better job at identifying broken <a id="_idIndexMarker1101"/>bangle images. Thus, the recall of the model for the <strong class="source-inline">Broken</strong> class is very good, around 92%. Overall, our model is doing a decent job but has potential <span class="No-Break">for improvement.</span></p>
			<p>Improving the accuracy of this model can be a good exercise for you. The following are some hints that may help to increase the overall accuracy of <span class="No-Break">the model:</span></p>
			<ul>
				<li>Work with better resolution (something better <span class="No-Break">than 200x200)</span></li>
				<li>A deeper model (more and bigger CNN layers to get <span class="No-Break">better features)</span></li>
				<li>Data augmentation (make the model <span class="No-Break">more robust)</span></li>
				<li>A better network (better feature extraction with an attention mechanism or any other feature <span class="No-Break">extraction strategy)</span></li>
			</ul>
			<p>Finally, let’s print the <span class="No-Break">confusion matrix:</span></p>
			<pre class="source-code">
confusion_matrix(all_test_labels, test_outputs,)</pre>			<p>Here is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
array([[67, 21, 42],
       [ 8, 42, 11],
       [ 1,  5, 73]])</pre>			<p>A confusion matrix can help to determine what kind of mistakes our model makes. In other words, when it classifies class 0 incorrectly, which class does our model confuse <span class="No-Break">it with?</span></p>
			<p>With this, our exercise of training a custom TF-based model for the task of defect detection is <a id="_idIndexMarker1102"/>complete. We now have an average-performing trained TF model. Now, let’s see how can we deploy this model on Google <span class="No-Break">Vertex AI.</span></p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor263"/>Deploying a vision model to a Vertex AI endpoint</h1>
			<p>In the previous<a id="_idIndexMarker1103"/> section, we completed our <a id="_idIndexMarker1104"/>experiment of training a TF-based vision model to identify detects from product images. We now have a trained model that can identify defected or broken bangle images. To make this model usable in downstream applications, we need to deploy it to an endpoint so that we can query that endpoint, getting outputs for new input images on demand. There are certain things that are important to consider while deploying a model, such as expected traffic, expected latency, and expected cost. Based on these factors, we can choose the best infrastructure to deploy our models. If there are strict low-latency requirements, we can deploy our model to machines with accelerators (such as <strong class="bold">Graphical Processing Units</strong> (<strong class="bold">GPUs</strong>) or <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>)). Conversely, we don’t have the <a id="_idIndexMarker1105"/>necessity<a id="_idIndexMarker1106"/> of online or on-demand predictions, so we don’t need to deploy our model to an endpoint. Offline batch-prediction requests can be handled without even deploying the model to <span class="No-Break">an endpoint.</span></p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor264"/>Saving model to Google Cloud Storage (GCS)</h2>
			<p>In our case, we are <a id="_idIndexMarker1107"/>interested in<a id="_idIndexMarker1108"/> deploying our model to a Vertex AI endpoint, just to see how it works. The very first step is to save our trained TF model <span class="No-Break">into GCS:</span></p>
			<pre class="source-code">
model.save(
    filepath='gs://my-training-artifacts/tf_model/',
    overwrite=True,
)</pre>			<p>Once the model is saved, the next step would be to upload this model artifact to the Vertex AI Model Registry. Once our model is uploaded to the Model Registry, it can be deployed easily, either by using the Google Cloud console UI or <span class="No-Break">Python scripts.</span></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor265"/>Uploading the TF model to the Vertex Model Registry</h2>
			<p>In this step, we <a id="_idIndexMarker1109"/>will utilize the Vertex AI SDK to<a id="_idIndexMarker1110"/> upload our TF model to the Model Registry. Alternatively, we can also use the console UI to do the same. For this purpose, we will be required to provide a model display name, a region to upload the model to, the URI of the saved model artifact, and a serving container image. Note that the serving container images must have dependencies installed for appropriate versions of frameworks, such as TF. Google Cloud provides a list of prebuilt serving containers that can be readily used to serve the models. In our case, we will also use a prebuilt container that supports TF 2.11 <span class="No-Break">for <a id="_idTextAnchor266"/>serving.</span></p>
			<p>Let’s set up <span class="No-Break">some configurations:</span></p>
			<pre class="source-code">
PROJECT_ID='417812395597'
REGION='us-central1'
ARTIFACT_URI='gs://my-training-artifacts/tf_model/'
MODEL_DISPLAY_NAME='tf-bangle-defect-detector-v1'
SERVING_IMAGE='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest'</pre>			<p>Now, we can go ahead and upload the model artifact to the Model Registry, as shown in the following <span class="No-Break">Python code:</span></p>
			<pre class="source-code">
aiplatform.init(project=PROJECT_ID, location=REGION)
model = aiplatform.Model.upload(
    display_name=MODEL_DISPLAY_NAME,
    artifact_uri=ARTIFACT_URI,
    serving_container_image_uri=SERVING_IMAGE,
    sync=True,
)
model.wait()
print("Model Display Name: ", model.display_name)
print("Model Resource Name: ", model.resource_name)</pre>			<p>Here is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Model Display Name:  <strong class="source-inline">tf-bangle-defect-detector-v1</strong>
Model Resource Name:  <strong class="source-inline">projects/417812395597/locations/us-central1/models/3991356951198957568</strong></pre>			<p>We have now successfully uploaded our TF model to the Registry. We can now locate our model, using <a id="_idIndexMarker1111"/>either <a id="_idIndexMarker1112"/>the model display name or the model resource ID, as per the previous output. Next, we need an endpoint to deploy <span class="No-Break">our model.</span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor267"/>Creating a Vertex AI endpoint</h2>
			<p>In this step, we will <a id="_idIndexMarker1113"/>create a Vertex AI endpoint that will be used to serve our model prediction requests. Again, the endpoints can also be created using the Google Cloud console UI, but here, we will do it programmatically with the Vertex <span class="No-Break">AI SDK.</span></p>
			<p>Here is the sample function<a id="_idTextAnchor268"/> that can be leveraged to create <span class="No-Break">an endpoint:</span></p>
			<pre class="source-code">
def create_vertex_endpoint(
    project_id: str,
    display_name: str,
    location: str,
):
    aiplatform.init(project=project_id, location=location)
    endpoint = aiplatform.Endpoint.create(
        display_name=display_name,
        project=project_id,
        location=location,
    )
    print("Endpoint Display Name: ", endpoint.display_name)
    print("Endpoint Resource Name: ", endpoint.resource_name)
    return endpoint</pre>			<p>Let’s use this <a id="_idTextAnchor269"/>function <a id="_idIndexMarker1114"/>to create an endpoint for <span class="No-Break">our model:</span></p>
			<pre class="source-code">
ENDPOINT_DISPLAY_NAME='tf-bangle-defect-detector-endpoint'
vertex_endpoint = create_vertex_endpoint(
    project_id=PROJECT_ID,
    display_name=ENDPOINT_DISPLAY_NAME,
    location=REGION,
)</pre>			<p>This snippet gives us the following output, and it confirms that the endpoint has been created (note that this endpoint is currently empty without <span class="No-Break">any model):</span></p>
			<pre class="source-code">
Endpoint Display Name:  tf-bangle-defect-detector-endpoint
Endpoint Resource Name:  projects/417812395597/locations/us-central1/endpoints/4516901519043330048</pre>			<p>We can use the following command to list all the endpoints within a region to verify whether it has been <span class="No-Break">successfully created:</span></p>
			<pre class="source-code">
gcloud ai endpoints list --region={REGION}</pre>			<p>This command gives the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
ENDPOINT_ID          DISPLAY_NAME
4516901519043330048  tf-bangle-defect-detector-endpoint</pre>			<p>Our endpoint is <a id="_idIndexMarker1115"/>now set, so we can go ahead and deploy a <span class="No-Break">model now.</span></p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor270"/>Deploying a model to the Vertex AI endpoint</h2>
			<p>We now have our<a id="_idIndexMarker1116"/> TF model in the Model Registry, and <a id="_idIndexMarker1117"/>we have also created a Vertex AI endpoint. We are now all set to deploy our model to this endpoint. This action can also be performed using the Google Cloud console UI, but here, we will do it programmatically using the Vertex AI SDK. First, let’<a id="_idTextAnchor271"/>s get the details of our model from <span class="No-Break">the Registry:</span></p>
			<pre class="source-code">
## List Model versions
models = aiplatform.Model.list( \
    filter=f"display_name={MODEL_DISPLAY_NAME}")
print("Number of models:", len(models))
print("Version ID:", models[0].version_id)
Output:
Number of models: 1
Version <a id="_idTextAnchor272"/>ID: 1</pre>			<p>Now, let’s deploy our model to <span class="No-Break">the endpoint:</span></p>
			<pre class="source-code">
MODEL = models[0]
DEPLOYED_MODEL_DISPLAY_NAME='tf-bangle-defect-detector-deployed-v1'
MACHINE_TYPE='n1-standard-16'
#Deploy the model to the Vertex AI endpoint
response = vertex_endpoint.deploy(
    model=MODEL,
    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,
    machine_type=MACHINE_TYPE,
)</pre>			<p>As soon as the execution of this snippet is complete without any errors, our model should be ready to accept requests for online predictions. This deployed model information should be visible in the Google Cloud console UI as well. We can also verify th<a id="_idTextAnchor273"/>e deployed model by using the <span class="No-Break">following function:</span></p>
			<pre class="source-code">
vertex_endpoint.gca_resource.deployed_models[0]</pre>			<p>This should print <a id="_idIndexMarker1118"/>the<a id="_idIndexMarker1119"/> resources related to the deployed model. The output will look something similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
id: "4885783272115666944"
model: "projects/417812395597/locations/us-central1/models/3991356951198957568"
display_name: "tf-bangle-defect-detector-deployed-v1"
create_time {
    seconds: 1694242166
    nanos: 923666000
}
dedicated_resources {
    machine_spec {
        machine_type: "n1-standard-16"
    }
    min_replica_count: 1
    max_replica_count: 1
}
model_version_id: "1"</pre>			<p>We have now verified that our TF model is successfully deployed. Now, we are all set to call this endpoint for predictions. In the next section, we will see how an endpoint with a deployed model <a id="_idIndexMarker1120"/>can <a id="_idIndexMarker1121"/>serve <span class="No-Break">online predictions.</span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor274"/>Getting online predictions from a vision model</h1>
			<p>In the previous<a id="_idIndexMarker1122"/> section, we deployed our <a id="_idIndexMarker1123"/>custom TF-based model to a Vertex AI Endpoint so that we could embed it into any downstream application, querying it for on-demand or online predictions. In this section, we will see how we can call this endpoint for online predictions programmatically using Python. However, the prediction requests can also be made by using a <strong class="source-inline">curl</strong> command and sending a JSON file with <span class="No-Break">input data.</span></p>
			<p>There are a few things to consider while making prediction requests; the most important part is pre-processing the input data accordingly. In the first section, when we trained our model, we did some pre-processing on our image dataset to make it compatible with the model. Similarly, while requesting the predictions, we should follow the exact same data preparation steps. Otherwise, either the model request will fail, due to an incompatible input format, or it will give bad results, due to training-serving skew. We already have pre-processed test images, so we can pass them directly within the <span class="No-Break">prediction requests.</span></p>
			<p>As we will pass the input image to the request JSON, we will need to encode it into a JSON serializable format. So, we will send our input image to the model encode with the <strong class="source-inline">base64</strong> format. Check out the following snippet<a id="_idTextAnchor275"/> for an exampl<a id="_idTextAnchor276"/>e payload creation of a single <span class="No-Break">test image:</span></p>
			<pre class="source-code">
import base64
Instances = [
    {
      "input_1": {
        "b64": base64.b64encode(all_test_images[0]).decode("utf-8")
      }
    <a id="_idTextAnchor277"/>}
]</pre>			<p>Let’s set some configurations to call <span class="No-Break">the endpoint:</span></p>
			<pre class="source-code">
from google.protobuf import <a id="_idTextAnchor278"/>json_format
from google.protobuf.s<a id="_idTextAnchor279"/>truct_pb2 import Value
ENDPOINT_ID="4516901519043330048"
client_options = {"api_endpoint": "us-central1-aiplatform.googleapis.com"}
client = aiplatform.gapic.PredictionServiceClient( \
    client_options=client_options)
instances = [
    json_format.ParseDict(instance_dict, Value()) for \
        instance_dict in Instances
]
parameters_dict = {}
parameters = json_format.ParseDict(parameters_dict, Value())</pre>			<p>We can now go ahead <a id="_idIndexMarker1124"/>and<a id="_idIndexMarker1125"/> make a prediction request to our Vertex AI endpoint. Note that our parameters dictionary is empty, which means that we will get the raw model predictions as a result. In a more customized setting, we can also pass some parameters, such as <em class="italic">thresholds</em>, to perform minor post-processing on the model predictions accordingly. Ch<a id="_idTextAnchor280"/>eck out the following Python code to <span class="No-Break">request predictions:</span></p>
			<pre class="source-code">
endpoint = client.endpoint_path(
    project=PROJECT_ID, location=REGION, endpoint=ENDPOINT_ID
)
response = client.predict(
    endpoint=endpoint, instances=instances, \
    parameters=parameters
)
print("response")
print(" deployed_model_id:", response.deployed_model_id)
predictions = response.predictions</pre>			<p>Similarly, we can <a id="_idIndexMarker1126"/>pass <a id="_idIndexMarker1127"/>a number of images in the <strong class="source-inline">instances</strong> variable and get an on-demand or online prediction result from <span class="No-Break">an endpoint.</span></p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor281"/>Summary</h1>
			<p>In this chapter, we created an end-to-end vision-based solution to detect visual defects from images. We saw how CNN-based deep learning architectures can be used to extract useful features from images and then use those features for tasks such as classification. After training and testing our model, we went ahead and deployed it to a Vertex AI endpoint, allowing it to serve online or on-demand prediction requests for any number of <span class="No-Break">downstream applications.</span></p>
			<p>After completing this chapter, you should be confident about how to approach vision-based problems and how to utilize ML to solve them. You should now be able to train your own vision-based classification models to solve real-world business problems. After completing the second section on deploying a custom model to a Vertex AI endpoint and the third section on getting online prediction from a Vertex endpoint, you should now be able to make your custom vision models usable for any downstream business application, by deploying them to Google Vertex AI. We hope this chapter was a good learning experience, with a hands-on real-world example. The next chapter will also present a hands-on example of a real-world, NLP-related <span class="No-Break">use case.</span></p>
		</div>
	</div>
</div>
</body></html>