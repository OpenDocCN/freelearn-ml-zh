<html><head></head><body>
<div id="_idContainer063">
<h1 class="chapter-number" id="_idParaDest-104"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-105"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.2.1">Improving the Performance of Machine Learning Models</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, you learned about different techniques for properly validating and assessing the performance of your machine learning models. </span><span class="koboSpan" id="kobo.3.2">The next step is to extend your knowledge of those techniques for improving the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">your models.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, you will learn about techniques to improve the performance and generalizability of your models by working on the data or algorithm you select for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">learning modeling.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Options for improving </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">model performance</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Synthetic </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">data generation</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Improving pre-training </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">data processing</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Regularization to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">model generalizability</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.17.1">By the end of this chapter, you will be familiar with different techniques to improve the performance and generalizability of your models and you will know how you can benefit from Python in implementing them for </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">your projects.</span></span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">The following requirements are needed for this chapter as they help you better understand the concepts and enable you to use them in your projects and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.22.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">sklearn</span></strong><span class="koboSpan" id="kobo.25.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">ray</span></strong><span class="koboSpan" id="kobo.28.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">2.3.1</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">tune_sklearn</span></strong><span class="koboSpan" id="kobo.31.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">0.4.5</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">bayesian_optimization</span></strong><span class="koboSpan" id="kobo.34.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">1.4.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">numpy</span></strong><span class="koboSpan" id="kobo.37.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">1.22.4</span></span></li><li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">imblearn</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.40.1">matplotlib</span></strong><span class="koboSpan" id="kobo.41.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">3.7.1</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.43.1">Knowledge of machine learning validation techniques such as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.44.1">k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">-fold cross-validation</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.46.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05"><span class="No-Break"><span class="koboSpan" id="kobo.48.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.49.1">.</span></span></p>
<h1 id="_idParaDest-107"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.50.1">Options for improving model performance</span></h1>
<p><span class="koboSpan" id="kobo.51.1">The changes we can make to improve the performance of our models could be related to the algorithms we use or the data we feed them to train our models (see </span><em class="italic"><span class="koboSpan" id="kobo.52.1">Table 5.1</span></em><span class="koboSpan" id="kobo.53.1">). </span><span class="koboSpan" id="kobo.53.2">Adding more data </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.54.1">points could reduce the variance of the model, for example, by adding data close to the decision boundaries of classification models to increase confidence in the identified boundaries and reduce overfitting. </span><span class="koboSpan" id="kobo.54.2">Removing outliers could reduce both bias and variance by eliminating the effect of distant data points. </span><span class="koboSpan" id="kobo.54.3">Adding more features could help the model to become better at the training stage (that is, lower model bias), but it might result in higher variance. </span><span class="koboSpan" id="kobo.54.4">There could also be features that cause overfitting and their removal could help to increase </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">model generalizability.</span></span></p>
<table class="No-Table-Style" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.56.1">Change</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.57.1">Potential effect</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.58.1">Description</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.59.1">Adding more training </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">data points</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.61.1">Reducing variance</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.62.1">We could add new data points randomly or try to add data points with specific feature values, output values, </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">or labels.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.64.1">Removing outliers with </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">lower stringency</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.66.1">Reducing bias </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">and variance</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.68.1">Removing outliers could reduce errors in the training set but it could also help in training a more generalizable model (that is, a model with </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">lower variance).</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.70.1">Adding </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">more features</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.72.1">Reducing bias</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.73.1">We could add features that provide unknown information to the model. </span><span class="koboSpan" id="kobo.73.2">For example, adding the crime rate of a neighborhood for house price prediction could improve model performance if that info is not already captured by </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">existing features.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.75.1">Removing features</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.76.1">Reducing variance</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.77.1">Each feature could have a positive effect on training performance but it might add information that is not generalizable to new data points and result in </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">higher variance.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.79.1">Running optimization process for </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">more iterations</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.81.1">Reducing bias</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.82.1">Optimizing for more iteration reduces training error but might result </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">in overfitting.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.84.1">Using more </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">complex models</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.86.1">Reducing bias</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.87.1">Increasing the depth of decision trees is an example of an increase in model complexity that could result in lower model bias but potentially a higher chance </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">of overfitting.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.89.1">Table 5.1 – Some of the options to reduce the bias and/or variance of machine learning models</span></p>
<p><span class="koboSpan" id="kobo.90.1">Increasing model complexity could help to reduce bias, as we discussed in the previous chapter, but a model can have many hyperparameters that affect its complexity or result in improving or lowering model bias and generalizability. </span><span class="koboSpan" id="kobo.90.2">Some of the hyperparameters that you could start with in the optimization process for the widely used supervised </span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.91.1">and unsupervised learning methods are provided in </span><em class="italic"><span class="koboSpan" id="kobo.92.1">Table 5.2</span></em><span class="koboSpan" id="kobo.93.1">. </span><span class="koboSpan" id="kobo.93.2">These hyperparameters should help you to improve the performance of your models, but you don’t need to write new functions or classes for </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">hyperparameter optimization.</span></span></p>
<table class="No-Table-Style" id="table002-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.95.1">Method</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.96.1">Hyperparameter</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.97.1">Logistic regression</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.98.1">sklearn.linear_model.LogisticRegression()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">penalty</span></strong><span class="koboSpan" id="kobo.100.1">: Choosing regularization between </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">l1</span></strong><span class="koboSpan" id="kobo.102.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">l2</span></strong><span class="koboSpan" id="kobo.104.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">elasticnet</span></strong><span class="koboSpan" id="kobo.106.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">None</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.109.1">class_weight</span></strong><span class="koboSpan" id="kobo.110.1">: Associating weights </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">to classes</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">l1_ratio</span></strong><span class="koboSpan" id="kobo.113.1">: The Elastic-Net </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">mixing parameter</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.115.1">K-Nearest Neighbors</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">sklearn.neighbors.KNeighborsClassifier()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">n_neighbors</span></strong><span class="koboSpan" id="kobo.118.1">: Number </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">of neighbors</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">weights</span></strong><span class="koboSpan" id="kobo.121.1">: Choosing between </span><strong class="source-inline"><span class="koboSpan" id="kobo.122.1">uniform</span></strong><span class="koboSpan" id="kobo.123.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.124.1">distance</span></strong><span class="koboSpan" id="kobo.125.1"> to use neighbors equally or assign weights to them based on </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">their distance</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.127.1">Support Vector Machine</span></strong><span class="koboSpan" id="kobo.128.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.129.1">SVM</span></strong><span class="koboSpan" id="kobo.130.1">) classifier </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">or regressor</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">sklearn.svm.SVC()</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">sklearn.svm.SVR()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">C</span></strong><span class="koboSpan" id="kobo.135.1">: Inverse strength of regularization with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">l2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1"> penalty</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">kernel</span></strong><span class="koboSpan" id="kobo.139.1">: An SVM kernel with prebuilt kernels including </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">linear</span></strong><span class="koboSpan" id="kobo.141.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">poly</span></strong><span class="koboSpan" id="kobo.143.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">rbf</span></strong><span class="koboSpan" id="kobo.145.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">sigmoid</span></strong><span class="koboSpan" id="kobo.147.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.149.1">precomputed</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">degree</span></strong><span class="koboSpan" id="kobo.151.1"> (degree of polynomial): Degree of the polynomial kernel </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">function (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">poly</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">)</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">class_weight</span></strong><span class="koboSpan" id="kobo.156.1"> (only for classification): Associating weights </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">with classes</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.158.1">Random forest classifier </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">or regressor</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">sklearn.ensemble.RandomForestClassifier()</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.161.1">sklearn.ensemble.RandomForestRegressor()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.162.1">n_estimators</span></strong><span class="koboSpan" id="kobo.163.1">: Number of trees in </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the forest</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">max_depth</span></strong><span class="koboSpan" id="kobo.166.1">: Maximum depth of </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">the trees</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">class_weight</span></strong><span class="koboSpan" id="kobo.169.1">: Associating weights </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">with classes</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.171.1">min_samples_split</span></strong><span class="koboSpan" id="kobo.172.1">: The minimum number of samples required to be at a </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">leaf node</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.174.1">XGBoost classifier </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">or regressor</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">xgboost.XGBClassifier()</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">xgboost.XGBRegressor()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">booster</span></strong><span class="koboSpan" id="kobo.179.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">gbtree</span></strong><span class="koboSpan" id="kobo.181.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">gblinear</span></strong><span class="koboSpan" id="kobo.183.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">dart</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.187.1">For a </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">tree booster:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">eta</span></strong><span class="koboSpan" id="kobo.190.1">: Step size shrinkage to </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">prevent overfitting.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">max_depth</span></strong><span class="koboSpan" id="kobo.193.1">: Maximum depth of </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">the trees</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">min_child_weight</span></strong><span class="koboSpan" id="kobo.196.1">: The minimum sum of data point weights needed to </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">continue partitioning</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">lambda</span></strong><span class="koboSpan" id="kobo.199.1">: L2 </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">regularization factor</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">alpha</span></strong><span class="koboSpan" id="kobo.202.1">: L1 </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">regularization factor</span></span></li></ul></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.204.1">LightGBM classifier </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">or regressor</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">Lightgbm.LGBMClassifier()</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">Lightgbm.LGBMRegressor()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">boosting_type</span></strong><span class="koboSpan" id="kobo.209.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">gbdt</span></strong><span class="koboSpan" id="kobo.211.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">dart</span></strong><span class="koboSpan" id="kobo.213.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">rf</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">)</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">num_leaves</span></strong><span class="koboSpan" id="kobo.218.1">: Maximum </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">tree leaves</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">max_depth</span></strong><span class="koboSpan" id="kobo.221.1">: Maximum </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">tree depth</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">n_estimators</span></strong><span class="koboSpan" id="kobo.224.1">: Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">boosted trees</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">reg_alpha</span></strong><span class="koboSpan" id="kobo.227.1">: L1 regularization term </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">on weights</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">reg_lambda</span></strong><span class="koboSpan" id="kobo.230.1">: L2 regularization term </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">on weights</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.232.1">K-Means clustering</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">sklearn.cluster.KMeans()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">n_clusters</span></strong><span class="koboSpan" id="kobo.235.1">: Number </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">of clusters</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.237.1">Agglomerative clustering</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">sklearn.cluster.AgglomerativeClustering()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">n_clusters</span></strong><span class="koboSpan" id="kobo.240.1">: Number </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">of clusters</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">metric</span></strong><span class="koboSpan" id="kobo.243.1">: Distance measures with prebuilt measures including </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">euclidean</span></strong><span class="koboSpan" id="kobo.245.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">l1</span></strong><span class="koboSpan" id="kobo.247.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">l2</span></strong><span class="koboSpan" id="kobo.249.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">manhattan</span></strong><span class="koboSpan" id="kobo.251.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">cosine</span></strong><span class="koboSpan" id="kobo.253.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">precomputed</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">linkage</span></strong><span class="koboSpan" id="kobo.257.1">: Linkage criterion with prebuilt methods including </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">ward</span></strong><span class="koboSpan" id="kobo.259.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">complete</span></strong><span class="koboSpan" id="kobo.261.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.262.1">average</span></strong><span class="koboSpan" id="kobo.263.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">single</span></strong></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.266.1">DBSCAN clustering</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">sklearn.cluster.DBSCAN()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">eps</span></strong><span class="koboSpan" id="kobo.269.1">: Maximum allowed distance between data points for them to be </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">considered neighbors</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">min_samples</span></strong><span class="koboSpan" id="kobo.272.1">: The minimum number of neighbors a data point needs to be considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">core point</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.274.1">UMAP</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">umap.UMAP()</span></strong></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">n_neighbors</span></strong><span class="koboSpan" id="kobo.277.1">: Constraining the size of the local neighborhood for the learning </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">data structure</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">min_dist</span></strong><span class="koboSpan" id="kobo.280.1">: Controlling the compactness of groups in </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">low-dimensional space</span></span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.282.1">Table 5.2 – Some of the most important hyperparameters of widely used supervised and unsupervised machine learning methods to start hyperparameter optimization with</span></p>
<p><span class="koboSpan" id="kobo.283.1">The Python libraries listed in </span><em class="italic"><span class="koboSpan" id="kobo.284.1">Table 5.3</span></em><span class="koboSpan" id="kobo.285.1"> have modules dedicated to different hyperparameter </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.286.1">optimization techniques such as </span><em class="italic"><span class="koboSpan" id="kobo.287.1">grid search</span></em><span class="koboSpan" id="kobo.288.1">, </span><em class="italic"><span class="koboSpan" id="kobo.289.1">random search</span></em><span class="koboSpan" id="kobo.290.1">, </span><em class="italic"><span class="koboSpan" id="kobo.291.1">Bayesian search</span></em><span class="koboSpan" id="kobo.292.1">, and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.293.1">successive halving</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">.</span></span></p>
<table class="No-Table-Style" id="table003-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.295.1">Library</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.296.1">URL</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">scikit-optimize</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/scikit-optimize/"><span class="No-Break"><span class="koboSpan" id="kobo.298.1">https://pypi.org/project/scikit-optimize/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">Optuna</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/optuna/"><span class="No-Break"><span class="koboSpan" id="kobo.300.1">https://pypi.org/project/optuna/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">GpyOpt</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/GPyOpt/"><span class="No-Break"><span class="koboSpan" id="kobo.302.1">https://pypi.org/project/GPyOpt/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">Hyperopt</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://hyperopt.github.io/hyperopt/"><span class="No-Break"><span class="koboSpan" id="kobo.304.1">https://hyperopt.github.io/hyperopt/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">ray.tune</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://docs.ray.io/en/latest/tune/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.306.1">https://docs.ray.io/en/latest/tune/index.html</span></span></a></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.307.1">Table 5.3 – Commonly used Python libraries for hyperparameter optimization</span></p>
<p><span class="koboSpan" id="kobo.308.1">Let’s talk about each method </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">in detail.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.310.1">Grid search</span></h2>
<p><span class="koboSpan" id="kobo.311.1">This method is about determining a series of hyperparameter nomination sets to be tested one by </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.312.1">one to find the optimum combination. </span><span class="koboSpan" id="kobo.312.2">The cost of grid-searching to find an optimal combination is high. </span><span class="koboSpan" id="kobo.312.3">Also, considering there would be a specific set of hyperparameters that mattered for each problem, grid search with a predetermined set of hyperparameter combinations for all problems is not an </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">effective approach.</span></span></p>
<p><span class="koboSpan" id="kobo.314.1">Here is an example of grid search hyperparameter optimization using </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">sklearn.model_selection.GridSearchCV()</span></strong><span class="koboSpan" id="kobo.316.1"> for a random forest classifier model. </span><span class="koboSpan" id="kobo.316.2">80% of the data is used for hyperparameter optimization and the performance of the model is </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.317.1">assessed using stratified </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">5-fold CV:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.319.1">
# determining random state for data split and model initializationrandom_state = 42
# loading and splitting digit data to train and test sets
digits = datasets.load_digits()
x = digits.data
y = digits.target
x_train, x_test, y_train, y_test = train_test_split(
    x, y, random_state= random_state, test_size=0.2)
# list of hyperparameters to use for tuning
parameter_grid = {"max_depth": [2, 5, 10, 15, 20],
    "min_samples_split": [2, 5, 7]}
# validating using stratified k-fold (k=5) cross-validation
stratified_kfold_cv = StratifiedKFold(
    n_splits = 5, shuffle=True, random_state=random_state)
# generating the grid search
start_time = time.time()
sklearn_gridsearch = GridSearchCV(
    estimator = RFC(n_estimators = 10,
        random_state = random_state),
    param_grid = parameter_grid, cv = stratified_kfold_cv,
    n_jobs=-1)
# fitting the grid search cross-validation
sklearn_gridsearch.fit(x_train, y_train)</span></pre>
<p><span class="koboSpan" id="kobo.320.1">In this code, 10 estimators are used and different </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">min_samples_split</span></strong><span class="koboSpan" id="kobo.322.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">max_depth</span></strong><span class="koboSpan" id="kobo.324.1"> values are considered for the hyperparameter optimization process. </span><span class="koboSpan" id="kobo.324.2">You can specify different performance metrics according to what you learned in the previous chapter using the scoring parameter, as one of the parameters of </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">sklearn.model_selection.GridSearchCV()</span></strong><span class="koboSpan" id="kobo.326.1">. </span><span class="koboSpan" id="kobo.326.2">A combination of </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">max_depth</span></strong><span class="koboSpan" id="kobo.328.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">10</span></strong><span class="koboSpan" id="kobo.330.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">min_samples_split</span></strong><span class="koboSpan" id="kobo.332.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">7</span></strong><span class="koboSpan" id="kobo.334.1"> was identified as the best hyperparameter set in this case, which </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.335.1">resulted in 0.948 accuracy using the stratified 5-fold CV. </span><span class="koboSpan" id="kobo.335.2">We can extract the best hyperparameter and corresponding score using </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">sklearn_gridsearch.best_params_</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.337.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">sklearn_gridsearch.best_score_</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.340.1">Random search</span></h2>
<p><span class="koboSpan" id="kobo.341.1">This method is an alternative to grid search. </span><span class="koboSpan" id="kobo.341.2">It randomly tries different combinations of hyperparameter </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.342.1">values. </span><span class="koboSpan" id="kobo.342.2">For the same high enough computational budget, it is shown that random search can achieve a higher performance model compared to grid search, as it can search a larger space (Bergstra and </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">Bengio, 2012).</span></span></p>
<p><span class="koboSpan" id="kobo.344.1">Here is an example of random search hyperparameter optimization using </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">sklearn.model_selection.RandomizedSearchCV()</span></strong><span class="koboSpan" id="kobo.346.1"> for the same model and data used in the </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">previous code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.348.1">
# generating the grid searchstart_time = time.time()
sklearn_randomsearch = RandomizedSearchCV(
    estimator = RFC(n_estimators = 10,
        random_state = random_state),
    param_distributions = parameter_grid,
    cv = stratified_kfold_cv, random_state = random_state,
    n_iter = 5, n_jobs=-1)
# fitting the grid search cross-validation
sklearn_randomsearch.fit(x_train, y_train)</span></pre>
<p><span class="koboSpan" id="kobo.349.1">With only five iterations, this random search resulted in 0.942 CV accuracy with less than one-third of running time, which could depend on your local or cloud system configuration. </span><span class="koboSpan" id="kobo.349.2">In this case, a combination of </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">max_depth</span></strong><span class="koboSpan" id="kobo.351.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">15</span></strong><span class="koboSpan" id="kobo.353.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">min_samples_split</span></strong><span class="koboSpan" id="kobo.355.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">7</span></strong><span class="koboSpan" id="kobo.357.1"> was identified as the best hyperparameter set. </span><span class="koboSpan" id="kobo.357.2">Comparing the results of grid search </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.358.1">and random search, we can conclude that models with different </span><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">max_depth</span></strong><span class="koboSpan" id="kobo.360.1"> values could result in similar CV accuracies for this specific case of random forest modeling with 10 estimators using the digit dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">.</span></span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.364.1">Bayesian search</span></h2>
<p><span class="koboSpan" id="kobo.365.1">In Bayesian optimization, instead of randomly selecting hyperparameter combinations without checking the </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.366.1">value of previous combination sets, each combination of hyperparameter sets gets selected in an iteration based on the history of previously tested hyperparameter sets. </span><span class="koboSpan" id="kobo.366.2">This process helps to reduce the computational cost compared to grid search but it doesn’t always beat random search. </span><span class="koboSpan" id="kobo.366.3">We want to use Ray Tune (</span><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">ray.tune</span></strong><span class="koboSpan" id="kobo.368.1">) here for this approach. </span><span class="koboSpan" id="kobo.368.2">You can read more about different functionalities available in Ray Tune such as </span><em class="italic"><span class="koboSpan" id="kobo.369.1">logging tune runs</span></em><span class="koboSpan" id="kobo.370.1">, </span><em class="italic"><span class="koboSpan" id="kobo.371.1">how to stop and resume</span></em><span class="koboSpan" id="kobo.372.1">, </span><em class="italic"><span class="koboSpan" id="kobo.373.1">analyzing tune experiment results</span></em><span class="koboSpan" id="kobo.374.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.375.1">deploying tune in the cloud</span></em><span class="koboSpan" id="kobo.376.1"> on the tutorial </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">page: </span></span><a href="https://docs.ray.io/en/latest/tune/tutorials/overview.html"><span class="No-Break"><span class="koboSpan" id="kobo.378.1">https://docs.ray.io/en/latest/tune/tutorials/overview.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.379.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">The following implementation of Bayesian hyperparameter optimization using </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">ray.tune.sklearn.TuneSearchCV()</span></strong><span class="koboSpan" id="kobo.382.1"> for the same random forest model, as explained previously, achieves 0.942 </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">CV accuracy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.384.1">
start_time = time.time()tune_bayessearch = TuneSearchCV(
    RFC(n_estimators = 10, random_state = random_state),
    parameter_grid,
    search_optimization="bayesian",
    cv = stratified_kfold_cv,
    n_trials=3, # number of sampled parameter settings
    early_stopping=True,
    max_iters=10,
    random_state = random_state)
tune_bayessearch.fit(x_train, y_train)</span></pre>
<h2 id="_idParaDest-111"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.385.1">Successive halving</span></h2>
<p><span class="koboSpan" id="kobo.386.1">The idea behind successive having is to not invest in all hyperparameters equally. </span><span class="koboSpan" id="kobo.386.2">Candidate hyperparameter sets get evaluated using limited resources, for example, using only a fraction </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.387.1">of training data or a limited number of trees in a random forest model in an iteration, and some of them pass to the next iteration. </span><span class="koboSpan" id="kobo.387.2">In later iterations, more resources get used until the last iteration in which all resources, for example, all training data, get used to evaluate the remaining hyperparameter sets. </span><span class="koboSpan" id="kobo.387.3">You can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">HalvingGridSearchCV()</span></strong><span class="koboSpan" id="kobo.389.1">and </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">HalvingRandomSearchCV()</span></strong><span class="koboSpan" id="kobo.391.1">as part of </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">sklearn.model_selection</span></strong><span class="koboSpan" id="kobo.393.1"> to try out successive halving. </span><span class="koboSpan" id="kobo.393.2">You </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.394.1">can read more about these two Python modules </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">at </span></span><a href="https://scikit-learn.org/stable/modules/grid_search.html#id3"><span class="No-Break"><span class="koboSpan" id="kobo.396.1">https://scikit-learn.org/stable/modules/grid_search.html#id3</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.397.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.398.1">There are other hyperparameter </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.399.1">optimization techniques, such as </span><strong class="bold"><span class="koboSpan" id="kobo.400.1">Hyperband</span></strong><span class="koboSpan" id="kobo.401.1"> (Li et al., 2017) and </span><strong class="bold"><span class="koboSpan" id="kobo.402.1">BOHB</span></strong><span class="koboSpan" id="kobo.403.1"> (Falkner et al., 2018) that you can try out, but the general idea behind most </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.404.1">advancements in hyperparameter optimization is to minimize the computational resources necessary to achieve an optimum hyperparameter set. </span><span class="koboSpan" id="kobo.404.2">There are also techniques and libraries for hyperparameter optimization in deep learning, which we will cover in </span><a href="B16369_12.xhtml#_idTextAnchor320"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.405.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.406.1">, </span><em class="italic"><span class="koboSpan" id="kobo.407.1">Going Beyond ML Debugging with Deep Learning</span></em><span class="koboSpan" id="kobo.408.1">, and </span><a href="B16369_13.xhtml#_idTextAnchor342"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.409.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.410.1">, </span><em class="italic"><span class="koboSpan" id="kobo.411.1">Advanced Deep Learning Techniques</span></em><span class="koboSpan" id="kobo.412.1">. </span><span class="koboSpan" id="kobo.412.2">Although hyperparameter optimization helps us to get better models, using the provided data for model training and the selected machine learning method, we can improve model performance with other approaches, such as generating synthetic data for model training, which is our </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">next topic.</span></span></p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.414.1">Synthetic data generation</span></h1>
<p><span class="koboSpan" id="kobo.415.1">The data we have access </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.416.1">to for training and evaluating our machine learning models may be limited. </span><span class="koboSpan" id="kobo.416.2">For example, in the case of classification models, we might have classes with a limited number of data points, resulting in lower performance of our models for unseen data points of the same classes. </span><span class="koboSpan" id="kobo.416.3">We will go through a few methods </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.417.1">here to help you improve the performance of your models in </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">these situations.</span></span></p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.419.1">Oversampling for imbalanced data</span></h2>
<p><span class="koboSpan" id="kobo.420.1">Imbalanced data classification is challenging due to the dominating effect of majority classes during training </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.421.1">as well as in model performance reporting. </span><span class="koboSpan" id="kobo.421.2">For model performance reporting, we discussed different </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.422.1">performance metrics in the previous chapter and how you can select a reliable metric even in the case of imbalanced data classification. </span><span class="koboSpan" id="kobo.422.2">Here, we want to talk about the concept of oversampling to help you improve the performance of your models by synthetically improving your training data. </span><span class="koboSpan" id="kobo.422.3">The concept of oversampling is to increase the number of data points in your minority classes using the real data points you have in your dataset. </span><span class="koboSpan" id="kobo.422.4">The simplest way of thinking about it is to duplicate some of the data points in minority classes, which is not a good approach as they will not provide complementary information to real data in the </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.423.1">training process. </span><span class="koboSpan" id="kobo.423.2">There are techniques designed for oversampling processes, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.424.1">Synthetic Minority Oversampling Technique</span></strong><span class="koboSpan" id="kobo.425.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.426.1">SMOTE</span></strong><span class="koboSpan" id="kobo.427.1">) and its variations for tabular data, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">present here.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.429.1">Undersampling</span></p>
<p class="callout"><span class="koboSpan" id="kobo.430.1">In classifying imbalanced data, an alternative to oversampling is to decrease the imbalance by sampling the </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.431.1">majority class. </span><span class="koboSpan" id="kobo.431.2">This process reduces the ratio of the majority-class to minority-class data points. </span><span class="koboSpan" id="kobo.431.3">As not all the data points get included in one set of sampling, multiple models can be built by sampling different subsets of majority-class data points and the output of those models can be combined, for example, through majority voting between the models. </span><span class="koboSpan" id="kobo.431.4">This process is called </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">undersampling</span></strong><span class="koboSpan" id="kobo.433.1">. </span><span class="koboSpan" id="kobo.433.2">Oversampling usually </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.434.1">results in higher performance improvement compared </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">to undersampling.</span></span></p>
<h3><span class="koboSpan" id="kobo.436.1">SMOTE</span></h3>
<p><span class="koboSpan" id="kobo.437.1">SMOTE is an old yet widely used approach to oversampling the minority class, for continuous feature </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.438.1">sets, using the distribution of neighboring data points (Chawla et al., 2022; see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.440.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.442.1"><img alt="Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE, Borderline-SMOTE, and ADASYN" src="image/B16369_05_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.443.1">Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE, Borderline-SMOTE, and ADASYN</span></p>
<p><span class="koboSpan" id="kobo.444.1">The steps in generating any synthetic data point using SMOTE can be summarized </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.446.1">Choose a random data point from a </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">minority class.</span></span></li>
<li><span class="koboSpan" id="kobo.448.1">Identify the K-Nearest Neighbors for that </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">data point.</span></span></li>
<li><span class="koboSpan" id="kobo.450.1">Choose one of the </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">neighbors randomly.</span></span></li>
<li><span class="koboSpan" id="kobo.452.1">Generate a synthetic data point at a randomly selected point between the two data points in the </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">feature space.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.454.1">SMOTE and two of its variations, </span><strong class="bold"><span class="koboSpan" id="kobo.455.1">Borderline-SMOTE</span></strong><span class="koboSpan" id="kobo.456.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">Adaptive synthetic</span></strong><span class="koboSpan" id="kobo.458.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.459.1">ADASYN</span></strong><span class="koboSpan" id="kobo.460.1">), are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.461.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.462.1">.1</span></em><span class="koboSpan" id="kobo.463.1">. </span><em class="italic"><span class="koboSpan" id="kobo.464.1">Steps 2</span></em><span class="koboSpan" id="kobo.465.1"> to</span><em class="italic"><span class="koboSpan" id="kobo.466.1"> 4</span></em><span class="koboSpan" id="kobo.467.1"> of SMOTE, Borderline-SMOTE, and ADASYN are similar. </span><span class="koboSpan" id="kobo.467.2">However, Borderline-SMOTE focuses </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.468.1">on the real data points that </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.469.1">divide the classes and </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.470.1">ADASYN focuses on </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.471.1">the data points of the minority class in regions of the feature space dominated </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.472.1">by the majority classes. </span><span class="koboSpan" id="kobo.472.2">In this way, Borderline-SMOTE increases the confidence in decision boundary identification to avoid overfitting and ADASYN improves generalizability for minority-class prediction in the parts of the space dominated by </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">majority classes.</span></span></p>
<p><span class="koboSpan" id="kobo.474.1">You can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">imblearn</span></strong><span class="koboSpan" id="kobo.476.1"> Python library for synthetic data generation using SMOTE, Borderline-SMOTE, and ADASYN. </span><span class="koboSpan" id="kobo.476.2">However, before getting into using these functionalities, we need to write a plotting function for later use to show the data before and after the </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">oversampling process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.478.1">
def plot_fun(x_plot: list, y_plot: list, title: str):    """
    Plotting a binary classification dataset
    :param x_plot: list of x coordinates (i.e. </span><span class="koboSpan" id="kobo.478.2">dimension 1)
    :param y_plot: list of y coordinates (i.e. </span><span class="koboSpan" id="kobo.478.3">dimension 2)
    :param title: title of plot
    """
    cmap, norm = mcolors.from_levels_and_colors([0, 1, 2],
        ['black', 'red'])
    plt.scatter([x_plot[iter][0] for iter in range(
        0, len(x_plot))],
        [x_plot[iter][1] for iter in range(
            0, len(x_plot))],
        c=y_plot, cmap=cmap, norm=norm)
    plt.xticks(fontsize = 12)
    plt.yticks(fontsize = 12)
    plt.xlabel('1st dimension', fontsize = 12)
    plt.ylabel('2nd dimension', fontsize = 12)
    plt.title(title)
    plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.479.1">Then, we generate a synthetic dataset with two classes and only two features (that is, two-dimensional data) and consider it our real dataset. </span><span class="koboSpan" id="kobo.479.2">We consider 100 data points in one of the classes </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.480.1">as the majority class, and 10 data points in another class as the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">minority class:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.482.1">
np.random.seed(12)minority_sample_size = 10
majority_sample_size = 100
# generating random set of x coordinates
group_1_X1 = np.repeat(2,majority_sample_size)+\
np.random.normal(loc=0, scale=1,size=majority_sample_size)
group_1_X2 = np.repeat(2,majority_sample_size)+\
np.random.normal(loc=0, scale=1,size=majority_sample_size)
# generating random set of x coordinates
group_2_X1 = np.repeat(4,minority_sample_size)+\
np.random.normal(loc=0, scale=1,size=minority_sample_size)
group_2_X2 = np.repeat(4,minority_sample_size)+\
np.random.normal(loc=0, scale=1,size=minority_sample_size)
X_all = [[group_1_X1[iter], group_1_X2[iter]] for\
            iter in range(0, len(group_1_X1))]+\
            [[group_2_X1[iter], group_2_X2[iter]]\
              for iter in range(0, len(group_2_X1))]
y_all = [0]*majority_sample_size+[1]*minority_sample_size
# plotting the randomly generated data
plot_fun(x_plot = X_all, y_plot = y_all,
    title = 'Original')</span></pre>
<p><span class="koboSpan" id="kobo.483.1">The resulting data points are shown in the following scatter plot with red and black data points representing the </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.484.1">minority and majority classes, respectively. </span><span class="koboSpan" id="kobo.484.2">We are using this synthetic data instead of a real dataset to visually show you how different synthetic data generation </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">methods work.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.486.1"><img alt="Figure 5.2 – Example dataset with two features (that is, dimensions), generated synthetically, to use for practicing with SMOTE and its alternatives" src="image/B16369_05_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.487.1">Figure 5.2 – Example dataset with two features (that is, dimensions), generated synthetically, to use for practicing with SMOTE and its alternatives</span></p>
<p><span class="koboSpan" id="kobo.488.1">We now use SMOTE via </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">imblearn.over_sampling.SMOTE()</span></strong><span class="koboSpan" id="kobo.490.1">, as shown in the following code snippet, to generate synthetic data points for the minority </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">class only:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.492.1">
k_neighbors = 5# initializing smote
# using 'auto', equivalent to 'not majority',
# sampling_strategy that enforces resampling all classes but the majority class
smote = SMOTE(sampling_strategy='auto',
                    k_neighbors=k_neighbors)
# fitting smote to oversample the minority class
x_smote, y_smote = smote.fit_resample(X_all, y_all)
# plotting the resulted oversampled data
plot_fun(x_plot = x_smote, y_plot = y_smote,
    title = 'SMOTE')</span></pre>
<p><span class="koboSpan" id="kobo.493.1">As you can see in the following figure, the new oversampled data points will be within the gaps between the </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.494.1">original data points of the minority class (that is, red data points). </span><span class="koboSpan" id="kobo.494.2">However, many of these new data points don’t help to identify a reliable decision boundary as they are grouped in the very top-right corner, far from the black data points and potential </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">decision boundaries.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.496.1"><img alt="Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing SMOTE" src="image/B16369_05_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.497.1">Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing SMOTE</span></p>
<p><span class="koboSpan" id="kobo.498.1">We use </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.499.1">Borderline-SMOTE instead via </span><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">imblearn.over_sampling.BorderlineSMOTE()</span></strong><span class="koboSpan" id="kobo.501.1"> as follows for synthetic </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">data generation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.503.1">
k_neighbors = 5# using 5 neighbors to determine if a minority sample is in "danger"
m_neighbors = 10
# initializing borderline smote
# using 'auto', equivalent to 'not majority', sampling_strategy that enforces resampling all classes but the majority class
borderline_smote = BorderlineSMOTE(
    sampling_strategy='auto',
    k_neighbors=k_neighbors,
    m_neighbors=m_neighbors)
# fitting borderline smote to oversample the minority class
x_bordersmote,y_bordersmote =borderline_smote.fit_resample(
    X_all, y_all)
# plotting the resulted oversampled data
plot_fun(x_plot = x_bordersmote, y_plot = y_bordersmote,
    title = 'Borderline-SMOTE')</span></pre>
<p><span class="koboSpan" id="kobo.504.1">We can see that the new synthetically generated data points are closer to the black data points of the majority </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.505.1">class, which helps with identifying a generalizable </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">decision boundary:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.507.1"><img alt="Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing Borderline-SMOTE" src="image/B16369_05_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.508.1">Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing Borderline-SMOTE</span></p>
<p><span class="koboSpan" id="kobo.509.1">We can also use ADASYN via </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">imblearn.over_sampling.ADASYN()</span></strong><span class="koboSpan" id="kobo.511.1">, which also generates more of the new synthetic data close to the black data points as it focuses on the regions with more </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">majority-class samples:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.513.1">
# using 5 neighbors for each datapoint in the oversampling process by SMOTEn_neighbors = 5
# initializing ADASYN
# using 'auto', equivalent to 'not majority', sampling_strategy that enforces resampling all classes but the majority class
adasyn_smote = ADASYN(sampling_strategy = 'auto',n_neighbors                                         = n_neighbors)
# fitting ADASYN to oversample the minority class
x_adasyn_smote, y_adasyn_smote = adasyn_smote.fit_resample(X_all, y_all)
# plotting the resulted oversampled data
plot_fun(x_plot = x_adasyn_smote, y_plot = y_adasyn_smote,
    title = "ADASYN")</span></pre>
<p><span class="koboSpan" id="kobo.514.1">The data including </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.515.1">original and synthetically generated data points using ADASYN are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.516.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.517.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.519.1"><img alt="Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing ADASYN" src="image/B16369_05_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.520.1">Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing ADASYN</span></p>
<p><span class="koboSpan" id="kobo.521.1">There have been more recent methods built upon SMOTE for synthetic data generation such as </span><strong class="bold"><span class="koboSpan" id="kobo.522.1">density-based synthetic minority over-sampling technique</span></strong><span class="koboSpan" id="kobo.523.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.524.1">DSMOTE</span></strong><span class="koboSpan" id="kobo.525.1">) (Xiaolong et al., 2019) and </span><strong class="bold"><span class="koboSpan" id="kobo.526.1">k-means SMOTE</span></strong><span class="koboSpan" id="kobo.527.1"> (Felix et al., 2017). </span><span class="koboSpan" id="kobo.527.2">Both of these methods </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.528.1">try to capture </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.529.1">groupings of data points either within the target minority class or the whole dataset. </span><span class="koboSpan" id="kobo.529.2">In DSMOTE, </span><strong class="bold"><span class="koboSpan" id="kobo.530.1">Density-based spatial clustering of applications with noise</span></strong><span class="koboSpan" id="kobo.531.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.532.1">DBSCAN</span></strong><span class="koboSpan" id="kobo.533.1">) is used to divide data points of the minority </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.534.1">class into three groups of core samples, borderline samples, and noise (i.e., outlying) samples, and then the core and borderline samples only get used for oversampling. </span><span class="koboSpan" id="kobo.534.2">This approach is shown to </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.535.1">work better than SMOTE and Borderline-SMOTE (Xiaolong et al., 2019). </span><span class="koboSpan" id="kobo.535.2">K-means SMOTE is another recent alternative to SMOTE (Last et al., 2017) that relies on clustering of the whole dataset using a k-means clustering algorithm before oversampling (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.536.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.537.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.539.1"><img alt="Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE (Last et al. 2017))" src="image/B16369_05_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.540.1">Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE (Last et al. </span><span class="koboSpan" id="kobo.540.2">2017))</span></p>
<p><span class="koboSpan" id="kobo.541.1">Here are the steps in </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.542.1">the k-means SMOTE method for data generation, which you can use via the </span><strong class="source-inline"><span class="koboSpan" id="kobo.543.1">kmeans-smote</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.544.1">Python library:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.545.1">Identify the decision boundary based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">original data.</span></span></li>
<li><span class="koboSpan" id="kobo.547.1">Cluster data points into k clusters using </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">k-means clustering.</span></span></li>
<li><span class="koboSpan" id="kobo.549.1">Oversample </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.550.1">using SMOTE for clusters with an </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">Imbalance Ratio</span></strong><span class="koboSpan" id="kobo.552.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.553.1">IR</span></strong><span class="koboSpan" id="kobo.554.1">) greater </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.555.1">than the </span><strong class="bold"><span class="koboSpan" id="kobo.556.1">Imbalance Ratio </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.557.1">Threshold</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.558.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.559.1">IRT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.561.1">Repeat the decision boundary identification process. </span><span class="koboSpan" id="kobo.561.2">(Note: IRT can be chosen by the user or optimized like </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">a hyperparameter.)</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.563.1">You can practice with </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.564.1">different variations of SMOTE and find out which one works best for your datasets, but Borderline-SMOTE and K-means SMOTE could be good </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">starting points.</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">Next, you will learn about techniques that help you in improving the quality of your data before getting into </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">model training.</span></span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.568.1">Improving pre-training data processing</span></h1>
<p><span class="koboSpan" id="kobo.569.1">Data processing in the early stages of a machine learning life cycle, before model training and evaluation, </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.570.1">determines the quality of the data we feed into the training, validation, and testing process, and consequently our success in achieving a high-performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">reliable model.</span></span></p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.572.1">Anomaly detection and outlier removal</span></h2>
<p><span class="koboSpan" id="kobo.573.1">Anomalies and </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.574.1">outliers in your data could decrease the </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.575.1">performance and reliability of your models in production. </span><span class="koboSpan" id="kobo.575.2">The existence of outliers in training data, the data you use for model evaluation, and unseen data in production could have </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">different impacts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.577.1">Outliers in model training</span></strong><span class="koboSpan" id="kobo.578.1">: The existence of outliers in the training data for supervised learning models could result in lower model generalizability. </span><span class="koboSpan" id="kobo.578.2">It could cause unnecessarily complex decision boundaries in classification or unnecessary nonlinearity in </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">regression models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.580.1">Outliers in model evaluation</span></strong><span class="koboSpan" id="kobo.581.1">: Outliers in validation and test data could lower the model performance. </span><span class="koboSpan" id="kobo.581.2">As the models are not necessarily designed for outlying data points, they cause the model performance assessment to be unreliable by impacting the performance of the models, which cannot predict their labels or continuous values properly. </span><span class="koboSpan" id="kobo.581.3">This issue could make the process of model </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">selection unreliable.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.583.1">Outliers in production</span></strong><span class="koboSpan" id="kobo.584.1">: Unseen data points in production could be far from the distribution of training or even test data. </span><span class="koboSpan" id="kobo.584.2">Our model may have been designed to identify those anomalies, as in the case of fraud detection, but if that is not the objective, then we should tag those data points as samples, which our model is not confident doing or designed for. </span><span class="koboSpan" id="kobo.584.3">For example, if we designed a model to suggest drugs to cancer patients based on the genetic information of their tumors, our model should report low confidence for patients that </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.585.1">need to be considered as </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.586.1">outlier samples, as wrong medication could have </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">life-threatening consequences.</span></span></li>
</ul>
<p><em class="italic"><span class="koboSpan" id="kobo.588.1">Table 5.4</span></em><span class="koboSpan" id="kobo.589.1"> provides a summary of some of the anomaly detection methods you can use to identify anomalies in your data and remove outliers </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">if necessary:</span></span></p>
<table class="No-Table-Style" id="table004-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.591.1">Method</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.592.1">Article </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.593.1">and URL</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.594.1">Isolation </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.595.1">Forest</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.596.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.597.1">iForest</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.599.1">Liu et </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">al. </span><span class="koboSpan" id="kobo.600.2">2008</span></span></p>
<p><a href="https://ieeexplore.ieee.org/abstract/document/4781136"><span class="No-Break"><span class="koboSpan" id="kobo.601.1">https://ieeexplore.ieee.org/abstract/document/4781136</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.602.1">Lightweight on-line detector of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.603.1">anomalies</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.604.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.605.1">Loda</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.607.1">Penvy, 2016</span></span></p>
<p><a href="https://link.springer.com/article/10.1007/s10994-015-5521-0"><span class="No-Break"><span class="koboSpan" id="kobo.608.1">https://link.springer.com/article/10.1007/s10994-015-5521-0</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.609.1">Local outlier </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.610.1">factor</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.611.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.612.1">LOF</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.614.1">Breunig et </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">al., 2000</span></span></p>
<p><a href="https://dl.acm.org/doi/abs/10.1145/342009.335388"><span class="No-Break"><span class="koboSpan" id="kobo.616.1">https://dl.acm.org/doi/abs/10.1145/342009.335388</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.617.1">Angle-Based Outlier </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.618.1">Detection </span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">(</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.620.1">ABOD</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.622.1">Kriegel et </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">al., 2008</span></span></p>
<p><a href="https://dl.acm.org/doi/abs/10.1145/1401890.1401946"><span class="No-Break"><span class="koboSpan" id="kobo.624.1">https://dl.acm.org/doi/abs/10.1145/1401890.1401946</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.625.1">Robust kernel density </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.626.1">estimation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.627.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.628.1">RKDE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.630.1">Kim and </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">Scott, 2008</span></span></p>
<p><a href="https://ieeexplore.ieee.org/document/4518376"><span class="No-Break"><span class="koboSpan" id="kobo.632.1">https://ieeexplore.ieee.org/document/4518376</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.633.1">Support Vector Method for </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">Novelty Detection</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.635.1">Schölkopf et </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">al., 1999</span></span></p>
<p><a href="https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html"><span class="No-Break"><span class="koboSpan" id="kobo.637.1">https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html</span></span></a></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.638.1">Table 5.4 – Widely used anomaly detection techniques (Emmott et al., 2013 and 2015)</span></p>
<p><span class="koboSpan" id="kobo.639.1">One of the effective methods for anomaly detection is </span><strong class="bold"><span class="koboSpan" id="kobo.640.1">iForest</span></strong><span class="koboSpan" id="kobo.641.1"> (Emmott et al., 2013 and 2015; Liu et al. </span><span class="koboSpan" id="kobo.641.2">2008), which </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.642.1">is available as one of the </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.643.1">functionalities of </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.645.1">. </span><span class="koboSpan" id="kobo.645.2">To try it out, we first generate a synthetic training dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.647.1">
n_samples, n_outliers = 100, 20rng = np.random.RandomState(12)
# Generating two synthetic clusters of datapoints sampled from a univariate "normal" (Gaussian) distribution of mean 0 and variance 1
cluster_1 = 0.2 * rng.randn(n_samples, 2) + np.array(
    [1, 1])
cluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array(
    [5, 5])
# Generating synthetic outliers
outliers = rng.uniform(low=2, high=4, size=(n_outliers, 2))
X = np.concatenate([cluster_1, cluster_2, outliers])
y = np.concatenate(
    [np.ones((2 * n_samples), dtype=int),
        -np.ones((n_outliers), dtype=int)])</span></pre>
<p><span class="koboSpan" id="kobo.648.1">Then, we </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.649.1">use </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">IsolationForest()</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.651.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.654.1">
# initializing iForestclf = IsolationForest(n_estimators = 10, random_state=10)
# fitting iForest using training data
clf.fit(X)
# plotting the results
scatter = plt.scatter(X[:, 0], X[:, 1])
handles, labels = scatter.legend_elements()
disp = DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    plot_method = "contour",
    response_method="predict",
    alpha=1
)
disp.ax_.scatter(X[:, 0], X[:, 1], s = 10)
disp.ax_.set_title("Binary decision boundary of iForest (
    n_estimators = 10)")
plt.xlabel('Dimension 1', fontsize = 12)
plt.ylabel('Dimension 2', fontsize = 12)
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.655.1">We used 10 decision trees in the previous code using </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">n_estimator = 10</span></strong><span class="koboSpan" id="kobo.657.1"> when initializing </span><strong class="source-inline"><span class="koboSpan" id="kobo.658.1">IsolationForest()</span></strong><span class="koboSpan" id="kobo.659.1">. </span><span class="koboSpan" id="kobo.659.2">This is one of the hyperparameters of iForest and we can play with it to </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.660.1">get better results. </span><span class="koboSpan" id="kobo.660.2">You can see the </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.661.1">resulting boundaries for </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">n_estimator = 10</span></strong><span class="koboSpan" id="kobo.663.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">n_estimator = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.665.1">100</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.666.1"> next.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.667.1"><img alt="Figure 5.7 – Decision boundaries of iForest using different numbers of estimators" src="image/B16369_05_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.668.1">Figure 5.7 – Decision boundaries of iForest using different numbers of estimators</span></p>
<p><span class="koboSpan" id="kobo.669.1">If you accept the result of an anomaly detection method such as iForest without further investigation, you might decide to use only the data within the shown boundaries. </span><span class="koboSpan" id="kobo.669.2">However, there could </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.670.1">be issues with these techniques, as </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.671.1">with any other machine method. </span><span class="koboSpan" id="kobo.671.2">Although iForest is not a supervised learning method, the boundaries for identifying anomalies could be prone to overfitting and not generalizable for further evaluation or use on unseen data in production. </span><span class="koboSpan" id="kobo.671.3">Also, the choice of hyperparameters could result in considering a large fraction of the data points as </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">outliers mistakenly.</span></span></p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.673.1">Benefitting from data of lower quality or relevance</span></h2>
<p><span class="koboSpan" id="kobo.674.1">When doing supervised machine learning, we would like to ideally have access to a large quantity of high-quality data. </span><span class="koboSpan" id="kobo.674.2">However, features or output values do not have the same level of certainty </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.675.1">across the data points we have access to. </span><span class="koboSpan" id="kobo.675.2">For example, in the case of classification, labels might not all have the same level of validity. </span><span class="koboSpan" id="kobo.675.3">In other words, our confidence in the labels of different data points could be different. </span><span class="koboSpan" id="kobo.675.4">Some of the commonly used labeling processes for data points are conducted by averaging experimental measurements (for example, as in biological or chemical contexts), or by using the annotations of multiple experts (</span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">or non-experts).</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">You could also have a problem such as predicting the response of breast cancer patients to specific drugs where you have access to data on patients’ response to the same or similar drugs in other cancer types. </span><span class="koboSpan" id="kobo.677.2">Then, part of your data has a lower level of relevance to the objective of breast cancer patients’ responses to </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">the drug.</span></span></p>
<p><span class="koboSpan" id="kobo.679.1">We preferably want to rely on high-quality data, or highly confident annotations and labels in these cases. </span><span class="koboSpan" id="kobo.679.2">However, we might have access to large quantities of data points that are either of lower quality, or lower relevance to the objective we have in mind. </span><span class="koboSpan" id="kobo.679.3">There are a few methods we could use to benefit from these data points of lower quality or relevance, although they are not successful all </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">the time:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.681.1">Assigning weight during optimization</span></strong><span class="koboSpan" id="kobo.682.1">: You can assign a weight to each data point when training machine learning models. </span><span class="koboSpan" id="kobo.682.2">For example, in </span><strong class="source-inline"><span class="koboSpan" id="kobo.683.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.684.1">, after initializing a random forest model such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.685.1">rf_model = RandomForestClassifier(random_state = 42)</span></strong><span class="koboSpan" id="kobo.686.1">, you can specify the weight of each datapoint in the fitting step as </span><strong class="source-inline"><span class="koboSpan" id="kobo.687.1">rf_model.fit(X_train,y_train, sample_weight = weights_array)</span></strong><span class="koboSpan" id="kobo.688.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">weights_array</span></strong><span class="koboSpan" id="kobo.690.1"> is an array of weights for each data point in the training set. </span><span class="koboSpan" id="kobo.690.2">These weights could be the confidence scores you have for each data point according to their relevance to the objective in mind or their quality. </span><span class="koboSpan" id="kobo.690.3">For example, if you use 10 different expert annotators for assigning labels to a series of data points, you can use a fraction of them to agree on a class label as the weight of each data point. </span><span class="koboSpan" id="kobo.690.4">If there is a data point with a class of 1 but only 7 out of 10 annotators agreed on this class, it will receive a lower weight compared to another class-1 data point for which all 10 annotators agreed on </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">its label.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.692.1">Ensemble learning</span></strong><span class="koboSpan" id="kobo.693.1">: If you consider a distribution of the quality of or confidence score for each data point, then you can build different models using data points of each part of this distribution and then combine the predictions of the models, for example, using their weighted average (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.694.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.695.1">.8</span></em><span class="koboSpan" id="kobo.696.1">). </span><span class="koboSpan" id="kobo.696.2">The weights assigned to each model could be a number, representative of the quality of the data points used for </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">its training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.698.1">Transfer learning</span></strong><span class="koboSpan" id="kobo.699.1">: In transfer learning, we can train a model on a reference task, typically </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.700.1">with many more data points, and then fine-tune it on a smaller task to come up with the task-specific predictions (Weiss et al., 2016, Madani Tonekaboni et al., 2020). </span><span class="koboSpan" id="kobo.700.2">This method can be used on data with different levels of confidence (Madani Tonekaboni et al., 2020). </span><span class="koboSpan" id="kobo.700.3">You can train a model on a large dataset with different levels of label confidence (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.701.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.702.1">.8</span></em><span class="koboSpan" id="kobo.703.1">), excluding very low-confidence data and then fine-tune it on the very high-confidence part of </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">your dataset.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.705.1"><img alt="Figure 5.8 – Techniques for using data points of different quality or relevance to the target problem in training machine learning models" src="image/B16369_05_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.706.1">Figure 5.8 – Techniques for using data points of different quality or relevance to the target problem in training machine learning models</span></p>
<p><span class="koboSpan" id="kobo.707.1">These methods could help you reduce the need to generate more high-quality data. </span><span class="koboSpan" id="kobo.707.2">However, having </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.708.1">more high-quality and highly relevant data is preferred, </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">if possible.</span></span></p>
<p><span class="koboSpan" id="kobo.710.1">As the final approach we want to go through in this chapter, we will talk about regularization as a technique to control overfitting and help in generating models with a higher chance </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">of generalizability.</span></span></p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.712.1">Regularization to improve model generalizability</span></h1>
<p><span class="koboSpan" id="kobo.713.1">We learned in the </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.714.1">previous chapter </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.715.1">that high model complexity could cause overfitting. </span><span class="koboSpan" id="kobo.715.2">One of the approaches to controlling the model complexity and reducing the effect of features that affect model</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.716.1"> generalizability is </span><strong class="bold"><span class="koboSpan" id="kobo.717.1">regularization</span></strong><span class="koboSpan" id="kobo.718.1">. </span><span class="koboSpan" id="kobo.718.2">In the regularization process, we consider a regularization or penalty term in the loss function to be optimized during the training process. </span><span class="koboSpan" id="kobo.718.3">Regularization, in the simple case of linear modeling, can be added as follows to the loss function during the </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">optimization process:</span></span></p>
<p/>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.720.1"><img alt="" role="presentation" src="image/01.jpg"/></span>
</div>
</div>
<p><span class="koboSpan" id="kobo.721.1">where the first term is the loss and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.722.1">Ω</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.723.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.724.1">W</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.725.1">)</span></span><span class="koboSpan" id="kobo.726.1"> is the regularization term as a function of model weights, or </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.727.1">parameters, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.728.1">W</span></span><span class="koboSpan" id="kobo.729.1">. </span><span class="koboSpan" id="kobo.729.2">However, regularization </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.730.1">could be used with different machine learning methods such as SVMs or </span><strong class="bold"><span class="koboSpan" id="kobo.731.1">LightGBM</span></strong><span class="koboSpan" id="kobo.732.1"> (refer to </span><em class="italic"><span class="koboSpan" id="kobo.733.1">Table 5.2</span></em><span class="koboSpan" id="kobo.734.1">). </span><span class="koboSpan" id="kobo.734.2">Three of the common </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.735.1">regularization terms are </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.736.1">shown in the following </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.737.1">table including </span><strong class="bold"><span class="koboSpan" id="kobo.738.1">L1 regularization</span></strong><span class="koboSpan" id="kobo.739.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.740.1">L2 regularization</span></strong><span class="koboSpan" id="kobo.741.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">their combination.</span></span></p>
<table class="No-Table-Style" id="table005-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.743.1">Method</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.744.1">Regularization term</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.745.1">Parameters</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.746.1">L2 regularization</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.747.1">Ω</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.748.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.749.1">W</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.750.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.751.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.752.1">λ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.753.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.754.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.755.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.756.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.757.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.758.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.759.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.760.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.761.1">w</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.762.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.763.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.764.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.765.1">2</span></span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.766.1">λ</span></span><span class="koboSpan" id="kobo.767.1">: The regularization parameter that determines the strength </span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">of regularization</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.769.1">L1 regularization</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.770.1">Ω</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.771.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.772.1">W</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.773.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.774.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.775.1">λ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.776.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.777.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.778.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.779.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.780.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.781.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.782.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.783.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.784.1">|</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.785.1">w</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.786.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.787.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.788.1">|</span></span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.789.1">λ</span></span><span class="koboSpan" id="kobo.790.1">: As in </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">L2 regularization</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.792.1">L2 </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">and L1</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.794.1">Ω</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.795.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.796.1">W</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.797.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.798.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.799.1">λ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.800.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.801.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.802.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.803.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.804.1">α</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.805.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.806.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.807.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.808.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.809.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.810.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.811.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.812.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.813.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.814.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.815.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.816.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.817.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.818.1">w</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.819.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.820.1">j</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.821.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.822.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.823.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.824.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.825.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.826.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.827.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.828.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.829.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.830.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.831.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.832.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.833.1">|</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.834.1">w</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.835.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.836.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.837.1">|</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.838.1">)</span></span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.839.1">λ</span></span><span class="koboSpan" id="kobo.840.1">: As in L1 and </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">L2 regularization</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.842.1">α</span></span><span class="koboSpan" id="kobo.843.1">: A missing parameter to determine the effect of L1 versus L2 in the </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">regularization process</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.845.1">Table 5.5 – Commonly used regularization methods for machine learning modeling</span></p>
<p><span class="koboSpan" id="kobo.846.1">We can consider the process of optimization with regularization as the process of getting as close as </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.847.1">possible to the optimal </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.848.1">parameter set </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.849.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.850.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.851.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.852.1">β</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.853.1"> </span></span><span class="koboSpan" id="kobo.854.1"> while keeping the parameters bound to a constrained region, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.855.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.856.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.858.1"><img alt="Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for controlling overfitting in a two-dimensional feature space" src="image/B16369_05_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.859.1">Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for controlling overfitting in a two-dimensional feature space</span></p>
<p><span class="koboSpan" id="kobo.860.1">Corners in parameter-constrained regions of L1 regularization result in the elimination of some of the parameters, or making their associated weights zero. </span><span class="koboSpan" id="kobo.860.2">However, the convexity of the constrained parameter region for L2 regularization only results in lowering the effect of parameters by decreasing their weights. </span><span class="koboSpan" id="kobo.860.3">This difference usually results in a higher robustness of L1 regularization </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">to outliers.</span></span><a id="_idTextAnchor197"/></p>
<p><span class="koboSpan" id="kobo.862.1">Linear classification models </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.863.1">with L1 regularization and L2 regularization </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.864.1">are called </span><strong class="bold"><span class="koboSpan" id="kobo.865.1">Lasso</span></strong><span class="koboSpan" id="kobo.866.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.867.1">Ridge</span></strong><span class="koboSpan" id="kobo.868.1"> regression, respectively (Tibshirani, 1996). </span><span class="koboSpan" id="kobo.868.2">Elastic-Net was proposed later using a combination of both L1 regularization and L2 regularization terms (Zou et al., 2005). </span><span class="koboSpan" id="kobo.868.3">Here, we want to practice using these three methods, but you can use regularization hyperparameters with other methods, such as an SVM or XGBoost classifier (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.869.1">Table 5.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.871.1">We first import </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.872.1">the necessary libraries </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.873.1">and design a plotting function to visually show the effect of the regularization parameter values. </span><span class="koboSpan" id="kobo.873.2">We also load the digit dataset from </span><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.875.1"> to use for model training </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">and evaluation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.877.1">
random_state = 42# loading and splitting digit data to train and test sets
digits = datasets.load_digits()
x = digits.data
y = digits.target
# using stratified k-fold (k=5) cross-validation
stratified_kfold_cv = StratifiedKFold(n_splits = 5,
    shuffle=True, random_state=random_state)
# function for plotting the CV score across different hyperparameter values
def reg_search_plot(search_fit, parameter: str):
    """
    :param search_fit: hyperparameter search object after model fitting
    :param parameter: hyperparameter name
    """
    parameters = [search_fit.cv_results_[
        'params'][iter][parameter] for iter in range(
            0,len(search_fit.cv_results_['params']))]
    mean_test_score = search_fit.cv_results_[
        'mean_test_score']
    plt.scatter(parameters, mean_test_score)
    plt.xticks(fontsize = 12)
    plt.yticks(fontsize = 12)
    plt.xlabel(parameter, fontsize = 12)
    plt.ylabel('accuracy', fontsize = 12)
    plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.878.1">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.879.1">GridSearchCV()</span></strong><span class="koboSpan" id="kobo.880.1"> function to assess the effect of different regularization parameter values in the following models. </span><span class="koboSpan" id="kobo.880.2">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.881.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.882.1">, the regularization </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.883.1">parameter is </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.884.1">usually named </span><strong class="source-inline"><span class="koboSpan" id="kobo.885.1">alpha</span></strong><span class="koboSpan" id="kobo.886.1"> instead of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.887.1">λ</span></span><span class="koboSpan" id="kobo.888.1">, and the mixing parameter is called </span><strong class="source-inline"><span class="koboSpan" id="kobo.889.1">l1_ratio</span></strong><span class="koboSpan" id="kobo.890.1"> instead of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.891.1">α</span></span><span class="koboSpan" id="kobo.892.1">. </span><span class="koboSpan" id="kobo.892.2">Here, we first assess the effect of different </span><strong class="source-inline"><span class="koboSpan" id="kobo.893.1">alpha</span></strong><span class="koboSpan" id="kobo.894.1"> values on Lasso models, with L1 regularization, trained and evaluated using a </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">digit dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.896.1">
# Defining hyperparameter gridparameter_grid = {"alpha": [0, 0.1, 0.2, 0.3, 0.4, 0.5]}
# generating the grid search
lasso_search = GridSearchCV(Lasso(
    random_state = random_state),
    parameter_grid,cv = stratified_kfold_cv,n_jobs=-1)
# fitting the grid search cross-validation
lasso_search.fit(x, y)
reg_search_plot(search_fit = lasso_search,
    parameter = 'alpha')</span></pre>
<p><span class="koboSpan" id="kobo.897.1">The optimum </span><strong class="source-inline"><span class="koboSpan" id="kobo.898.1">alpha</span></strong><span class="koboSpan" id="kobo.899.1"> is identified to be 0.1, as shown in the following plot, which results in the highest accuracy across the considered values. </span><span class="koboSpan" id="kobo.899.2">This means that increasing the effect of regularization after an </span><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">alpha</span></strong><span class="koboSpan" id="kobo.901.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.902.1">0.1</span></strong><span class="koboSpan" id="kobo.903.1"> increases the model bias, resulting in a model with low performance </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">in training.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.905.1"><img alt="Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso model" src="image/B16369_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.906.1">Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso model</span></p>
<p><span class="koboSpan" id="kobo.907.1">If we assess the </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.908.1">effect of different </span><strong class="source-inline"><span class="koboSpan" id="kobo.909.1">alpha</span></strong><span class="koboSpan" id="kobo.910.1"> values </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.911.1">in a ridge model, with L2 regularization, we can see that the performance increases as we increase the strength of regularization (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.912.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.913.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.914.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.915.1"><img alt="Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge model" src="image/B16369_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.916.1">Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge model</span></p>
<p><span class="koboSpan" id="kobo.917.1">An alternative to </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.918.1">these two methods is </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.919.1">Elastic-Net, which combines the effect of L1 and L2 regularizations. </span><span class="koboSpan" id="kobo.919.2">In this case, the trend of the effect of </span><strong class="source-inline"><span class="koboSpan" id="kobo.920.1">alpha</span></strong><span class="koboSpan" id="kobo.921.1"> on the model performance is more similar to Lasso; however, the range of accuracy values is narrower in comparison with Lasso, which only relies on L1 regularization (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.923.1">.12</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.924.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.925.1"><img alt="Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net model" src="image/B16369_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.926.1">Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net model</span></p>
<p><span class="koboSpan" id="kobo.927.1">If your dataset is not very small, more complex models help you to achieve higher performance. </span><span class="koboSpan" id="kobo.927.2">It would be only in rare cases that you would consider a linear model your ultimate model. </span><span class="koboSpan" id="kobo.927.3">To </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.928.1">assess the effect of </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.929.1">regularization on more complex models, we chose the SVM classifier and examined the effect of different values of </span><strong class="source-inline"><span class="koboSpan" id="kobo.930.1">C</span></strong><span class="koboSpan" id="kobo.931.1"> as the regularization parameter </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.933.1">sklearn.svm.SVC()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.935.1">
# Defining hyperparameter gridparameter_grid = {"C": [0.01, 0.2, 0.4, 0.6, 0.8, 1]}
# generating the grid search
svc_search = GridSearchCV(SVC(kernel = 'poly',
    random_state = random_state),parameter_grid,
    cv = stratified_kfold_cv,n_jobs=-1)
# fitting the grid search cross-validation
svc_search.fit(x, y)
reg_search_plot(search_fit = svc_search, parameter = 'C')</span></pre>
<p><span class="koboSpan" id="kobo.936.1">As shown next, the range of accuracy for the models is higher, between 0.92 and 0.99, compared to linear models with an accuracy of lower than 0.6, but higher regularization controls overfitting and achieves </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">better performance.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.938.1"><img alt="Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification model" src="image/B16369_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.939.1">Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification model</span></p>
<p><span class="koboSpan" id="kobo.940.1">In </span><a href="B16369_12.xhtml#_idTextAnchor320"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.941.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.942.1">, </span><em class="italic"><span class="koboSpan" id="kobo.943.1">Going Beyond ML Debugging with Deep Learning</span></em><span class="koboSpan" id="kobo.944.1">, you </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.945.1">will also learn about regularization techniques in deep neural </span><a id="_idIndexMarker369"/><span class="No-Break"><span class="koboSpan" id="kobo.946.1">network models.</span></span></p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.947.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.948.1">In this chapter, you learned about techniques to improve the performance of your models and reduce their bias and variance. </span><span class="koboSpan" id="kobo.948.2">You learned about the different hyperparameters of widely used machine learning methods, other than deep learning, which will be covered later in the book, and Python libraries to help you in identifying the optimal hyperparameter sets. </span><span class="koboSpan" id="kobo.948.3">You learned about regularization as another technique to help you in training generalizable machine learning models. </span><span class="koboSpan" id="kobo.948.4">You also learned how to increase the quality of the data to be fed into the training process by methods such as synthetic data generation and </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">outlier detection.</span></span></p>
<p><span class="koboSpan" id="kobo.950.1">In the next chapter, you will learn about interpretability and explainability in machine learning modeling and how you can use the related techniques and Python tools to identify opportunities for improving </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">your models.</span></span></p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.952.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.953.1">Does adding more features and training data points reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">model variance?</span></span></li>
<li><span class="koboSpan" id="kobo.955.1">Could you provide examples of methods to use to combine data with different confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">class labels?</span></span></li>
<li><span class="koboSpan" id="kobo.957.1">How could oversampling improve the generalizability of your supervised machine </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">learning models?</span></span></li>
<li><span class="koboSpan" id="kobo.959.1">What is the difference between DSMOTE </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">and Borderline-SMOTE?</span></span></li>
<li><span class="koboSpan" id="kobo.961.1">Do you need to check the effect of every single value of every hyperparameter of a model during </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">hyperparameter optimization?</span></span></li>
<li><span class="koboSpan" id="kobo.963.1">Could L1 regularization eliminate the contribution of some of the features to supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">model predictions?</span></span></li>
<li><span class="koboSpan" id="kobo.965.1">Could Lasso and Ridge regression models result in the same performance on the same test data if trained using the same </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">training data?</span></span></li>
</ol>
<h1 id="_idParaDest-120"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.967.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.968.1">Bergstra, James, and Yoshua Bengio. </span><span class="koboSpan" id="kobo.968.2">“</span><em class="italic"><span class="koboSpan" id="kobo.969.1">Random search for hyper-parameter optimization</span></em><span class="koboSpan" id="kobo.970.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.971.1">Journal of machine learning research</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.972.1">13.2 (2012).</span></span></li>
<li><span class="koboSpan" id="kobo.973.1">Bergstra, James, et al. </span><span class="koboSpan" id="kobo.973.2">“</span><em class="italic"><span class="koboSpan" id="kobo.974.1">Algorithms for hyper-parameter optimization</span></em><span class="koboSpan" id="kobo.975.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.976.1">Advances in neural information processing systems</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.977.1">24 (2011).</span></span></li>
<li><span class="koboSpan" id="kobo.978.1">Nguyen, Vu. </span><span class="koboSpan" id="kobo.978.2">“</span><em class="italic"><span class="koboSpan" id="kobo.979.1">Bayesian optimization for accelerating hyper-parameter tuning</span></em><span class="koboSpan" id="kobo.980.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.981.1">2019 IEEE second international conference on artificial intelligence and knowledge engineering (AIKE)</span></em><span class="koboSpan" id="kobo.982.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">IEEE (2019).</span></span></li>
<li><span class="koboSpan" id="kobo.984.1">Li, Lisha, et al. </span><span class="koboSpan" id="kobo.984.2">“</span><em class="italic"><span class="koboSpan" id="kobo.985.1">Hyperband: A novel bandit-based approach to hyperparameter optimization</span></em><span class="koboSpan" id="kobo.986.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.987.1">Journal of Machine Learning Research</span></em><span class="koboSpan" id="kobo.988.1"> 18.1 (2017): </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">pp. </span><span class="koboSpan" id="kobo.989.2">6765-6816.</span></span></li>
<li><span class="koboSpan" id="kobo.990.1">Falkner, Stefan, Aaron Klein, and Frank Hutter. </span><span class="koboSpan" id="kobo.990.2">“</span><em class="italic"><span class="koboSpan" id="kobo.991.1">BOHB: Robust and efficient hyperparameter optimization at scale</span></em><span class="koboSpan" id="kobo.992.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.993.1">International Conference on Machine Learning</span></em><span class="koboSpan" id="kobo.994.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">PMLR (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.996.1">Ng, Andrew, Stanford CS229: Machine Learning Course, </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">Autumn 2018.</span></span></li>
<li><span class="koboSpan" id="kobo.998.1">Wong, Sebastien C., et al. </span><span class="koboSpan" id="kobo.998.2">“</span><em class="italic"><span class="koboSpan" id="kobo.999.1">Understanding data augmentation for classification: when to warp?</span></em><span class="koboSpan" id="kobo.1000.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1001.1">2016 international conference on digital image computing: techniques and applications (DICTA)</span></em><span class="koboSpan" id="kobo.1002.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">IEEE (2016).</span></span></li>
<li><span class="koboSpan" id="kobo.1004.1">Mikołajczyk, Agnieszka, and Michał Grochowski. </span><span class="koboSpan" id="kobo.1004.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1005.1">Data augmentation for improving deep learning in image classification problem</span></em><span class="koboSpan" id="kobo.1006.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1007.1">2018 international interdisciplinary PhD workshop (IIPhDW)</span></em><span class="koboSpan" id="kobo.1008.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">IEEE (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.1010.1">Shorten, Connor, and Taghi M. </span><span class="koboSpan" id="kobo.1010.2">Khoshgoftaar. </span><span class="koboSpan" id="kobo.1010.3">“</span><em class="italic"><span class="koboSpan" id="kobo.1011.1">A survey on image data augmentation for deep learning</span></em><span class="koboSpan" id="kobo.1012.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1013.1">Journal of big data</span></em><span class="koboSpan" id="kobo.1014.1"> 6.1 (2019): </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">pp. </span><span class="koboSpan" id="kobo.1015.2">1-48.</span></span></li>
<li><span class="koboSpan" id="kobo.1016.1">Taylor, Luke, and Geoff Nitschke. </span><span class="koboSpan" id="kobo.1016.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1017.1">Improving deep learning with generic data augmentation</span></em><span class="koboSpan" id="kobo.1018.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1019.1">2018 IEEE Symposium Series on Computational Intelligence (SSCI)</span></em><span class="koboSpan" id="kobo.1020.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">IEEE (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.1022.1">Shorten, Connor, Taghi M. </span><span class="koboSpan" id="kobo.1022.2">Khoshgoftaar, and Borko Furht. </span><span class="koboSpan" id="kobo.1022.3">“</span><em class="italic"><span class="koboSpan" id="kobo.1023.1">Text data augmentation for deep learning</span></em><span class="koboSpan" id="kobo.1024.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1025.1">Journal of big Data</span></em><span class="koboSpan" id="kobo.1026.1"> 8.1 (2021): </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">pp. </span><span class="koboSpan" id="kobo.1027.2">1-34.</span></span></li>
<li><span class="koboSpan" id="kobo.1028.1">Perez, Luis, and Jason Wang. </span><span class="koboSpan" id="kobo.1028.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1029.1">The effectiveness of data augmentation in image classification using deep learning</span></em><span class="koboSpan" id="kobo.1030.1">.” </span><span class="koboSpan" id="kobo.1030.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">arXiv:1712.04621 (2017).</span></span></li>
<li><span class="koboSpan" id="kobo.1032.1">Ashrapov, Insaf. </span><span class="koboSpan" id="kobo.1032.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1033.1">Tabular GANs for uneven distribution</span></em><span class="koboSpan" id="kobo.1034.1">.” </span><span class="koboSpan" id="kobo.1034.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.1035.1">arXiv:2010.00638 (2020).</span></span></li>
<li><span class="koboSpan" id="kobo.1036.1">Xu, Lei, et al. </span><span class="koboSpan" id="kobo.1036.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1037.1">Modeling tabular data using conditional gan</span></em><span class="koboSpan" id="kobo.1038.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1039.1">Advances in Neural Information Processing Systems</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1040.1">32 (2019).</span></span></li>
<li><span class="koboSpan" id="kobo.1041.1">Chawla, Nitesh V., et al. </span><span class="koboSpan" id="kobo.1041.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1042.1">SMOTE: synthetic minority over-sampling technique</span></em><span class="koboSpan" id="kobo.1043.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1044.1">Journal of artificial intelligence research</span></em><span class="koboSpan" id="kobo.1045.1"> 16 (2002): </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">pp. </span><span class="koboSpan" id="kobo.1046.2">321-357.</span></span></li>
<li><span class="koboSpan" id="kobo.1047.1">Han, H., Wang, WY., Mao, BH. </span><span class="koboSpan" id="kobo.1047.2">(2005). </span><span class="koboSpan" id="kobo.1047.3">“</span><em class="italic"><span class="koboSpan" id="kobo.1048.1">Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning</span></em><span class="koboSpan" id="kobo.1049.1">.” </span><span class="koboSpan" id="kobo.1049.2">In: Huang, DS., Zhang, XP., Huang, GB. </span><span class="koboSpan" id="kobo.1049.3">(eds) </span><em class="italic"><span class="koboSpan" id="kobo.1050.1">Advances in Intelligent Computing</span></em><span class="koboSpan" id="kobo.1051.1">. </span><span class="koboSpan" id="kobo.1051.2">ICIC 2005. </span><em class="italic"><span class="koboSpan" id="kobo.1052.1">Lecture Notes in Computer Science</span></em><span class="koboSpan" id="kobo.1053.1">, vol. </span><span class="koboSpan" id="kobo.1053.2">3644. </span><span class="koboSpan" id="kobo.1053.3">Springer, </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">Berlin, Heidelberg.</span></span></li>
<li><span class="koboSpan" id="kobo.1055.1">He, Haibo, Yang Bai, E. </span><span class="koboSpan" id="kobo.1055.2">A. </span><span class="koboSpan" id="kobo.1055.3">Garcia, and Shutao Li, “</span><em class="italic"><span class="koboSpan" id="kobo.1056.1">ADASYN: Adaptive synthetic sampling approach for imbalanced learning</span></em><span class="koboSpan" id="kobo.1057.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1058.1">2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)</span></em><span class="koboSpan" id="kobo.1059.1"> (2008): pp. </span><span class="koboSpan" id="kobo.1059.2">1322-1328, </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">doi: 10.1109/IJCNN.2008.4633969.</span></span></li>
<li><span class="koboSpan" id="kobo.1061.1">X. </span><span class="koboSpan" id="kobo.1061.2">Xiaolong, C. </span><span class="koboSpan" id="kobo.1061.3">Wen, and S. </span><span class="koboSpan" id="kobo.1061.4">Yanfei, “</span><em class="italic"><span class="koboSpan" id="kobo.1062.1">Over-sampling algorithm for imbalanced data classification</span></em><span class="koboSpan" id="kobo.1063.1">,” in </span><em class="italic"><span class="koboSpan" id="kobo.1064.1">Journal of Systems Engineering and Electronics</span></em><span class="koboSpan" id="kobo.1065.1">, vol. </span><span class="koboSpan" id="kobo.1065.2">30, no. </span><span class="koboSpan" id="kobo.1065.3">6, pp. </span><span class="koboSpan" id="kobo.1065.4">1182-1191, Dec. </span><span class="koboSpan" id="kobo.1065.5">2019, </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">doi: 10.21629/JSEE.2019.06.12.</span></span></li>
<li><span class="koboSpan" id="kobo.1067.1">Last, Felix, Georgios Douzas, and Fernando Bacao. </span><span class="koboSpan" id="kobo.1067.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1068.1">Oversampling for imbalanced learning based on k-means and smote</span></em><span class="koboSpan" id="kobo.1069.1">.” </span><span class="koboSpan" id="kobo.1069.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.1070.1">arXiv:1711.00837 (2017).</span></span></li>
<li><span class="koboSpan" id="kobo.1071.1">Emmott, Andrew F., et al. </span><span class="koboSpan" id="kobo.1071.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1072.1">Systematic construction of anomaly detection benchmarks from real data</span></em><span class="koboSpan" id="kobo.1073.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1074.1">Proceedings of the ACM SIGKDD workshop on outlier detection and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1075.1">description</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">. </span><span class="koboSpan" id="kobo.1076.2">2013.</span></span></li>
<li><span class="koboSpan" id="kobo.1077.1">Emmott, Andrew, et al. </span><span class="koboSpan" id="kobo.1077.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1078.1">A meta-analysis of the anomaly detection problem</span></em><span class="koboSpan" id="kobo.1079.1">.” </span><span class="koboSpan" id="kobo.1079.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">arXiv:1503.01158 (2015).</span></span></li>
<li><span class="koboSpan" id="kobo.1081.1">Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. </span><span class="koboSpan" id="kobo.1081.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1082.1">Isolation forest</span></em><span class="koboSpan" id="kobo.1083.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1084.1">2008 eighth IEEE international conference on data mining</span></em><span class="koboSpan" id="kobo.1085.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">IEEE (2008).</span></span></li>
<li><span class="koboSpan" id="kobo.1087.1">Pevný, Tomáš. </span><span class="koboSpan" id="kobo.1087.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1088.1">Loda: Lightweight on-line detector of anomalies</span></em><span class="koboSpan" id="kobo.1089.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1090.1">Machine Learning 102</span></em><span class="koboSpan" id="kobo.1091.1"> (2016): </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">pp. </span><span class="koboSpan" id="kobo.1092.2">275-304.</span></span></li>
<li><span class="koboSpan" id="kobo.1093.1">Breunig, Markus M., et al. </span><span class="koboSpan" id="kobo.1093.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1094.1">LOF: identifying density-based local outliers</span></em><span class="koboSpan" id="kobo.1095.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1096.1">Proceedings of the 2000 ACM SIGMOD international conference on Management of </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1097.1">data</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1"> (2000).</span></span></li>
<li><span class="koboSpan" id="kobo.1099.1">Kriegel, Hans-Peter, Matthias Schubert, and Arthur Zimek. </span><span class="koboSpan" id="kobo.1099.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1100.1">Angle-based outlier detection in high-dimensional data</span></em><span class="koboSpan" id="kobo.1101.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1102.1">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1103.1">mining</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1"> (2008).</span></span></li>
<li><span class="koboSpan" id="kobo.1105.1">Joo Seuk Kim and C. </span><span class="koboSpan" id="kobo.1105.2">Scott, “</span><em class="italic"><span class="koboSpan" id="kobo.1106.1">Robust kernel density estimation</span></em><span class="koboSpan" id="kobo.1107.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1108.1">2008 IEEE International Conference on Acoustics, Speech and Signal Processing</span></em><span class="koboSpan" id="kobo.1109.1">, Las Vegas, NV, USA (2008): pp. </span><span class="koboSpan" id="kobo.1109.2">3381-3384, </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">doi: 10.1109/ICASSP.2008.4518376.</span></span></li>
<li><span class="koboSpan" id="kobo.1111.1">Schölkopf, Bernhard, et al. </span><span class="koboSpan" id="kobo.1111.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1112.1">Support vector method for novelty detection</span></em><span class="koboSpan" id="kobo.1113.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1114.1">Advances in neural information processing systems</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1115.1">12 (1999).</span></span></li>
<li><span class="koboSpan" id="kobo.1116.1">Weiss, Karl, Taghi M. </span><span class="koboSpan" id="kobo.1116.2">Khoshgoftaar, and DingDing Wang. </span><span class="koboSpan" id="kobo.1116.3">“</span><em class="italic"><span class="koboSpan" id="kobo.1117.1">A survey of transfer learning</span></em><span class="koboSpan" id="kobo.1118.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1119.1">Journal of Big data</span></em><span class="koboSpan" id="kobo.1120.1"> 3.1 (2016): </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">pp. </span><span class="koboSpan" id="kobo.1121.2">1-40.</span></span></li>
<li><span class="koboSpan" id="kobo.1122.1">Tonekaboni, Seyed Ali Madani, et al. </span><span class="koboSpan" id="kobo.1122.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1123.1">Learning across label confidence distributions using Filtered Transfer Learning</span></em><span class="koboSpan" id="kobo.1124.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1125.1">2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)</span></em><span class="koboSpan" id="kobo.1126.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">IEEE (2020).</span></span></li>
<li><span class="koboSpan" id="kobo.1128.1">Tibshirani, Robert. </span><span class="koboSpan" id="kobo.1128.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1129.1">Regression shrinkage and selection via the lasso</span></em><span class="koboSpan" id="kobo.1130.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1131.1">Journal of the Royal Statistical Society: Series B (Methodological)</span></em><span class="koboSpan" id="kobo.1132.1"> 58.1 (1996): </span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1">pp. </span><span class="koboSpan" id="kobo.1133.2">267-288.</span></span></li>
<li><span class="koboSpan" id="kobo.1134.1">Hastie, Trevor, et al. </span><em class="italic"><span class="koboSpan" id="kobo.1135.1">The elements of statistical learning: data mining, inference, and prediction</span></em><span class="koboSpan" id="kobo.1136.1">. </span><span class="koboSpan" id="kobo.1136.2">vol. </span><span class="koboSpan" id="kobo.1136.3">2. </span><span class="koboSpan" id="kobo.1136.4">New York: </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">Springer, 2009.</span></span></li>
<li><span class="koboSpan" id="kobo.1138.1">Zou, Hui, and Trevor Hastie. </span><span class="koboSpan" id="kobo.1138.2">“</span><em class="italic"><span class="koboSpan" id="kobo.1139.1">Regularization and variable selection via the elastic net</span></em><span class="koboSpan" id="kobo.1140.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.1141.1">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</span></em><span class="koboSpan" id="kobo.1142.1"> 67.2 (2005): </span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">pp. </span><span class="koboSpan" id="kobo.1143.2">301-320.</span></span></li>
</ul>
</div>
</body></html>