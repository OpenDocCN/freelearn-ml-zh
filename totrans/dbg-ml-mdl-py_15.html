<html><head></head><body>
<div id="_idContainer150">
<h1 class="chapter-number" id="_idParaDest-244"><a id="_idTextAnchor406"/><span class="koboSpan" id="kobo.1.1">15</span></h1>
<h1 id="_idParaDest-245"><a id="_idTextAnchor407"/><span class="koboSpan" id="kobo.2.1">Correlation versus Causality</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In previous chapters of this book, you learned how to train, evaluate, and build high-performance and low-bias machine learning models. </span><span class="koboSpan" id="kobo.3.2">However, the algorithms and example methods we used to practice the concepts that were introduced in this book do not necessarily provide you with a causal relationship between features and output variables in a supervised learning setting. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will discuss how causal inference and modeling could help you increase the reliability of your models </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">in production.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Correlation as part of machine </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">learning models</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Causal modeling to reduce risks and </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">improve performance</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Assessing causation in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">learning models</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Causal modeling </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">using Python</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">By the end of this chapter, you will have learned about the benefits of causal modeling and inference compared to correlative modeling and practice with available Python functionalities to identify the causal relationship between features and </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">output variables.</span></span></p>
<h1 id="_idParaDest-246"><a id="_idTextAnchor408"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">You need the following for this chapter as they will help you better understand the concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">dowhy</span></strong><span class="koboSpan" id="kobo.23.1"> == </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">0.5.1</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.25.1">bnlearn</span></strong><span class="koboSpan" id="kobo.26.1"> == </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">0.7.16</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">sklearn</span></strong><span class="koboSpan" id="kobo.29.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">d3blocks</span></strong><span class="koboSpan" id="kobo.32.1"> == </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">1.3.0</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.34.1">You will also require basic knowledge of machine learning model training, validation, </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">and testing</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.36.1">The code files for this chapter are available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter15"><span class="No-Break"><span class="koboSpan" id="kobo.38.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter15</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.39.1">.</span></span></p>
<h1 id="_idParaDest-247"><a id="_idTextAnchor409"/><span class="koboSpan" id="kobo.40.1">Correlation as part of machine learning models</span></h1>
<p><span class="koboSpan" id="kobo.41.1">The majority of machine learning modeling and data analysis projects result in correlative relationships between features and output variables in supervised learning settings and statistical </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.42.1">modeling. </span><span class="koboSpan" id="kobo.42.2">Although these relationships are not causal, identifying causal relationships is of high value, even if it’s not a necessity in most problems we try to solve. </span><span class="koboSpan" id="kobo.42.3">For example, we can define medical diagnosis as “</span><em class="italic"><span class="koboSpan" id="kobo.43.1">The identification of the diseases that are most likely to be causing the patient’s symptoms, given their medical history.</span></em><span class="koboSpan" id="kobo.44.1">” (Richens et </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">al., 2020).</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">Identifying causal relationships resolves issues in identifying misleading relationships between variables. </span><span class="koboSpan" id="kobo.46.2">Relying solely on correlations rather than causality could result in spurious and bizarre associations such as the following (</span><a href="https://www.tylervigen.com/spurious-correlations"><span class="No-Break"><span class="koboSpan" id="kobo.47.1">https://www.tylervigen.com/spurious-correlations</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.48.1">; </span></span><a href="https://www.buzzfeednews.com/article/kjh2110/the-10-most-bizarre-correlations"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://www.buzzfeednews.com/article/kjh2110/the-10-most-bizarre-correlations</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">):</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.51.1">US spending on science, space, and technology correlates with suicides by hanging, strangulation, </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">and suffocation</span></span></li>
<li><span class="koboSpan" id="kobo.53.1">Total revenue generated by arcades correlates with computer science doctorates awarded in </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">the US</span></span></li>
<li><span class="koboSpan" id="kobo.55.1">US crude oil imports from Norway correlates with drivers killed in collisions with </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">railway trains</span></span></li>
<li><span class="koboSpan" id="kobo.57.1">Eating organic food correlates </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">with autism</span></span></li>
<li><span class="koboSpan" id="kobo.59.1">Obesity correlates with the </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">debt bubble</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.61.1">You can find more of these spurious correlations in the sources for </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">these examples.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">Relying on correlations versus causation could decrease the reliability of different aspects of technology</span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.64.1"> development and improvement processes such as AB testing. </span><span class="koboSpan" id="kobo.64.2">For example, understanding “if we get more visitors to search, we’ll see an increase in purchases and revenue” (</span><a href="https://conversionsciences.com/correlation-causation-impact-ab-testing/"><span class="koboSpan" id="kobo.65.1">https://conversionsciences.com/correlation-causation-impact-ab-testing/</span></a><span class="koboSpan" id="kobo.66.1">) helps in proper decision-making and investment in </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">technology development.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">Now that you understand the problems with relying solely on correlative relationships, let’s discuss what causal modeling means in a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">learning setting</span><a id="_idTextAnchor410"/><span class="koboSpan" id="kobo.70.1">.</span></span></p>
<h1 id="_idParaDest-248"><a id="_idTextAnchor411"/><span class="koboSpan" id="kobo.71.1">Causal modeling to reduce risks and improve performance</span></h1>
<p><span class="koboSpan" id="kobo.72.1">Causal modeling helps in eliminating unreliable correlative relationships between variables. </span><span class="koboSpan" id="kobo.72.2">Eliminating </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.73.1">such unreliable relationships reduces </span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.74.1">the risks of wrong decision-making across different domains of applications for machine learning, such as healthcare. </span><span class="koboSpan" id="kobo.74.2">Decisions in healthcare, such as diagnosing diseases and assigning effective treatment regimens to patients, have a direct effect on quality of life and survival. </span><span class="koboSpan" id="kobo.74.3">Hence, decisions need to be based on reliable models and relationships in which causal modeling and inference could help us (Richens et al., 2020; Prosperi et al., 2020; Sanchez et </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">al., 2022).</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">Causal modeling techniques help in eliminating bias, such as confounding and collider bias, in our models (Prosperi et al., 2020) (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.77.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.78.1">.1</span></em><span class="koboSpan" id="kobo.79.1">). </span><span class="koboSpan" id="kobo.79.2">An example of such bias is smoking as a confounder of the relationship between yellow fingers and lung cancer (Prosperi et al., 2020). </span><span class="koboSpan" id="kobo.79.3">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.80.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.81.1">.1</span></em><span class="koboSpan" id="kobo.82.1">, the existence of collider variables results in correlative, but biased and unreal, associations between some of the input variables and outcome. </span><span class="koboSpan" id="kobo.82.2">Also, not having some of the variables that could be confounding in our modeling could result in us concluding other variables are associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">the outcome:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<span class="koboSpan" id="kobo.84.1"><img alt="Figure 15.1 – Schematic representation of confounding and collider bi﻿as" src="image/B16369_15_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.85.1">Figure 15.1 – Schematic representation of confounding and collider bi</span><a id="_idTextAnchor412"/><span class="koboSpan" id="kobo.86.1">as</span></p>
<p><span class="koboSpan" id="kobo.87.1">Next, we will </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.88.1">mention some concepts and techniques in causal </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.89.1">modeling such as causal inference and how to test causation in a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">learning model.</span></span></p>
<h1 id="_idParaDest-249"><a id="_idTextAnchor413"/><span class="koboSpan" id="kobo.91.1">Assessing causation in machine learning models</span></h1>
<p><span class="koboSpan" id="kobo.92.1">Calculating the correlation between features and outcomes in machine learning modeling has been </span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.93.1">a common approach in many fields and industries. </span><span class="koboSpan" id="kobo.93.2">For example, we can simply calculate the Pearson correlation coefficient to identify correlative features with the target variable. </span><span class="koboSpan" id="kobo.93.3">There are also features in many of our machine learning models that contribute to the prediction of outcomes not as causal but rather as correlative predictors. </span><span class="koboSpan" id="kobo.93.4">There are several ways to differentiate between such correlative and causal features with the available functionalities in Python. </span><span class="koboSpan" id="kobo.93.5">Here are a </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">few examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.95.1">Experimental design</span></strong><span class="koboSpan" id="kobo.96.1">: One way to establish causality is to conduct experiments where we measure the effect of changes in the causal feature on the target variable. </span><span class="koboSpan" id="kobo.96.2">However, such experimental studies may not always be feasible </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">or ethical.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.98.1">Feature importance</span></strong><span class="koboSpan" id="kobo.99.1">: We can use explainability techniques, as presented in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.100.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.101.1">, </span><em class="italic"><span class="koboSpan" id="kobo.102.1">Interpretability and Explainability in Machine Learning Modeling</span></em><span class="koboSpan" id="kobo.103.1">, to identify feature importance and use such information to discriminate between correlation </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">and causality.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.105.1">Causal inference</span></strong><span class="koboSpan" id="kobo.106.1">: Causal inference methods aim to identify the causal relationship </span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.107.1">between variables. </span><span class="koboSpan" id="kobo.107.2">You can use causal inference to determine whether a change in one variable causes a change in </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">another variable.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.109.1">We discussed different explainability techniques such as SHAP, LIME, and counterfactual explanations in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.111.1">, </span><em class="italic"><span class="koboSpan" id="kobo.112.1">Interpretability and Explainability in Machine Learning Modeling</span></em><span class="koboSpan" id="kobo.113.1">. </span><span class="koboSpan" id="kobo.113.2">You can use these techniques to identify features that are not causal in your models. </span><span class="koboSpan" id="kobo.113.3">For example, features with low SHAP values most probably are not causal in the model under </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.114.1">investigation. </span><span class="koboSpan" id="kobo.114.2">If there is a feature with low importance in the local approximation, according to LIME, then it is likely to not be causal regarding the output of your model. </span><span class="koboSpan" id="kobo.114.3">Or if changing a feature has little or no effect on the output of your model, through counterfactual analysis, then it is likely not a </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">causal feature.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">We can also </span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.117.1">use another technique, called </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">permutation feature importance</span></strong><span class="koboSpan" id="kobo.119.1">, which is also considered </span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.120.1">under the umbrella of explainability techniques to identify features with a low chance of being causal. </span><span class="koboSpan" id="kobo.120.2">In this approach, we change the values of a feature and measure the effect of change on the model’s performance. </span><span class="koboSpan" id="kobo.120.3">Then we can identify features with low effects that are likely to not </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">be causal.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">We already practiced explainability techniques in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.123.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.124.1">, </span><em class="italic"><span class="koboSpan" id="kobo.125.1">Interpretability and Explainability in Machine Learning Modeling</span></em><span class="koboSpan" id="kobo.126.1">. </span><span class="koboSpan" id="kobo.126.2">We will focus on causal inference for the remainder of </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">this</span><a id="_idTextAnchor414"/><span class="koboSpan" id="kobo.128.1"> chapter.</span></span></p>
<h2 id="_idParaDest-250"><a id="_idTextAnchor415"/><span class="koboSpan" id="kobo.129.1">Causal inference</span></h2>
<p><span class="koboSpan" id="kobo.130.1">In causal inference, we </span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.131.1">aim to identify and understand the causal relationship between variables in a dataset or model. </span><span class="koboSpan" id="kobo.131.2">In this process, we might rely on different statistical and machine learning techniques to analyze data and infer causal relationships </span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.132.1">between variables. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.133.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.134.1">.2</span></em><span class="koboSpan" id="kobo.135.1"> shows five such methods: </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">experimental design</span></strong><span class="koboSpan" id="kobo.137.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">observational studies</span></strong><span class="koboSpan" id="kobo.139.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.140.1">propensity score matching</span></strong><span class="koboSpan" id="kobo.141.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.142.1">instrumental variables</span></strong><span class="koboSpan" id="kobo.143.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.144.1">machine </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.145.1">learning-based methods</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<span class="koboSpan" id="kobo.147.1"><img alt="Figure 15.2 – Five causal inference techniques" src="image/B16369_15_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.148.1">Figure 15.2 – Five causal inference techniques</span></p>
<p><span class="koboSpan" id="kobo.149.1">In </span><strong class="bold"><span class="koboSpan" id="kobo.150.1">experimental design</span></strong><span class="koboSpan" id="kobo.151.1">, you design </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.152.1">experiments to compare outcome variables for samples with differences in a treatment variable, or different conditions based on a specific feature or characteristics. </span><span class="koboSpan" id="kobo.152.2">Examples of treatment and outcome </span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.153.1">variables are provided in </span><em class="italic"><span class="koboSpan" id="kobo.154.1">Table 15.1</span></em><span class="koboSpan" id="kobo.155.1"> to help you </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.156.1">understand the difference between these </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">two terms:</span></span></p>
<table class="No-Table-Style" id="table001-11">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.158.1">Treatment Variable</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.159.1">Outcome Variable</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.160.1">Education level</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.161.1">Income level</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.162.1">Smoking</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.163.1">Lung cancer</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.164.1">Physical activity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.165.1">Cardiovascular health</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.166.1">Family income</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.167.1">Academic performance</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.168.1">Table 15.1 – Examples of treatment and outcome variables in causal modeling</span></p>
<p><span class="koboSpan" id="kobo.169.1">In </span><strong class="bold"><span class="koboSpan" id="kobo.170.1">observational studies</span></strong><span class="koboSpan" id="kobo.171.1">, we use </span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.172.1">observational data, instead of controlled experiments, and try to identify causal relationships by controlling confounding variables. </span><strong class="bold"><span class="koboSpan" id="kobo.173.1">Propensity score matching</span></strong><span class="koboSpan" id="kobo.174.1"> matches treatment and control groups based on </span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.175.1">the probability of receiving the treatment given the observed variables. </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">Instrumental variables</span></strong><span class="koboSpan" id="kobo.177.1"> is used to overcome a common problem </span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.178.1">in observational studies where the treatment and outcome variables are jointly determined by other variables, or confounders, that are not included in the model. </span><span class="koboSpan" id="kobo.178.2">This approach starts with identifying an instrument that is correlated with the treatment variable and uncorrelated with the outcome variable, except through </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.179.1">its effect on the treatment variable. </span><strong class="bold"><span class="koboSpan" id="kobo.180.1">Machine learning-based methods</span></strong><span class="koboSpan" id="kobo.181.1"> are other categories of techniques where machine learning methods such as Bayesian networks and decision trees are used to identify causal relationships between variables </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">and outcom</span><a id="_idTextAnchor416"/><span class="koboSpan" id="kobo.183.1">es.</span></span></p>
<h3><span class="koboSpan" id="kobo.184.1">Bayesian networks</span></h3>
<p><span class="koboSpan" id="kobo.185.1">You can benefit from Bayesian networks in causal modeling and identifying causal relationships </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.186.1">between variables. </span><span class="koboSpan" id="kobo.186.2">Bayesian networks are graphical </span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.187.1">models that show the relationship between variables through </span><strong class="bold"><span class="koboSpan" id="kobo.188.1">directed acyclic graphs</span></strong><span class="koboSpan" id="kobo.189.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.190.1">DAGs</span></strong><span class="koboSpan" id="kobo.191.1">), where each variable, including the </span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.192.1">input features and outputs, is a node and directions show the relationship between variables (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.193.1">Figure 15</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.194.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer147">
<span class="koboSpan" id="kobo.196.1"><img alt="Figure 15.3 – Illustrating an example Bayesian network" src="image/B16369_15_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.197.1">Figure 15.3 – Illustrating an example Bayesian network</span></p>
<p><span class="koboSpan" id="kobo.198.1">What this network tells us is that higher values of </span><strong class="bold"><span class="koboSpan" id="kobo.199.1">Feature A</span></strong><span class="koboSpan" id="kobo.200.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.201.1">Feature B</span></strong><span class="koboSpan" id="kobo.202.1"> make it more likely for the outcome to occur. </span><span class="koboSpan" id="kobo.202.2">Note that the features could be numerical or categorical. </span><span class="koboSpan" id="kobo.202.3">Although the directions, such as from Feature A to the outcome (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.203.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.204.1">.3</span></em><span class="koboSpan" id="kobo.205.1">), don’t necessarily mean causality, Bayesian networks can be used for estimating the causal effects of variables on the outcome while controlling the </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">confounding variables.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">From a probabilistic </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.208.1">perspective, the network can be used to </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.209.1">simplify the joint probability of all the variables, including the features and outcome, </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">as follows:</span></span></p>
<p> <span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.211.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.212.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.213.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.214.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.215.1">A</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.216.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.218.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.219.1">B</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.220.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.221.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.222.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">C</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.224.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.225.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.226.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.227.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.228.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.229.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.230.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.231.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.232.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.233.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.235.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.236.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.237.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.238.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.240.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.242.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.243.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.244.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.245.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.246.1">A</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.247.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.248.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.249.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.250.1">B</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.251.1">)</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.252.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.253.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.254.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.255.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.256.1">B</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.257.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.258.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.259.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.260.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.261.1">)</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.262.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.263.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.264.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.265.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.266.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.267.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.268.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.269.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.270.1">A</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.271.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.272.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.273.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.274.1">F</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.275.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.276.1">A</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.277.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.279.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.280.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.283.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.284.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.285.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.286.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.287.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.288.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.289.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.290.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.291.1">A</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.292.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.293.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.294.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.295.1">B</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.296.1">)</span></span><span class="koboSpan" id="kobo.297.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.298.1">conditional probability distribution</span></strong><span class="koboSpan" id="kobo.299.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.300.1">CPD</span></strong><span class="koboSpan" id="kobo.301.1">) of the outcome given </span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.302.1">the values of Features A and B, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.303.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.304.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.305.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.306.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.307.1">B</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.308.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.309.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.310.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.311.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.312.1">)</span></span><span class="koboSpan" id="kobo.313.1"> is the CPD of Feature B given Feature C, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.314.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.315.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.316.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.317.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.318.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.319.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.320.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.321.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.322.1">A</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.323.1">)</span></span><span class="koboSpan" id="kobo.324.1"> is the CPD of Feature C given Feature A, and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.325.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.326.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.327.1">F</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.328.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.329.1">A</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.330.1">)</span></span><span class="koboSpan" id="kobo.331.1"> is the probability of Feature A that is not conditional to other features as no edge is directed toward it in the graph. </span><span class="koboSpan" id="kobo.331.2">These CPDs can help us estimate the effect of change one feature value has on another. </span><span class="koboSpan" id="kobo.331.3">It tells us about the likelihood of the occurrence of one variable given the occurrence of one or more variables. </span><span class="koboSpan" id="kobo.331.4">You will learn how to make a Bayesian network in a data-driven way </span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.332.1">for a given dataset and how to identify the </span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.333.1">CPDs of the network using Python by the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">There are several methods available in Python for causal inference. </span><span class="koboSpan" id="kobo.335.2">We’ll cover </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">t</span><a id="_idTextAnchor417"/><span class="koboSpan" id="kobo.337.1">hese next.</span></span></p>
<h1 id="_idParaDest-251"><a id="_idTextAnchor418"/><span class="koboSpan" id="kobo.338.1">Causal modeling using Python</span></h1>
<p><span class="koboSpan" id="kobo.339.1">Several Python </span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.340.1">libraries provide you with easy-to-use functionalities </span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.341.1">for using causal methods and </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.342.1">conducting causal </span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.343.1">inference. </span><span class="koboSpan" id="kobo.343.2">Some </span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.344.1">of these </span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.345.1">are </span><a id="_idIndexMarker855"/><span class="No-Break"><span class="koboSpan" id="kobo.346.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">dowhy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.348.1"> (</span></span><a href="https://pypi.org/project/dowhy/"><span class="No-Break"><span class="koboSpan" id="kobo.349.1">https://pypi.org/project/dowhy/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.350.1">)</span></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">pycausalimpact</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.352.1"> (</span></span><a href="https://pypi.org/project/pycausalimpact/"><span class="No-Break"><span class="koboSpan" id="kobo.353.1">https://pypi.org/project/pycausalimpact/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.354.1">)</span></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">causalnex</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.356.1"> (</span></span><a href="https://pypi.org/project/causalnex/"><span class="No-Break"><span class="koboSpan" id="kobo.357.1">https://pypi.org/project/causalnex/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.358.1">)</span></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">econml</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.360.1"> (</span></span><a href="https://pypi.org/project/econml/"><span class="No-Break"><span class="koboSpan" id="kobo.361.1">https://pypi.org/project/econml/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.362.1">)</span></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">bnlearn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.364.1"> (</span></span><a href="https://pypi.org/project/bnlearn/"><span class="No-Break"><span class="koboSpan" id="kobo.365.1">https://pypi.org/project/bnlearn/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.366.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.367.1">In the next few subsections, we will review </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">d</span><a id="_idTextAnchor419"/><span class="koboSpan" id="kobo.369.1">owhy</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.370.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">bnlearn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">.</span></span></p>
<h2 id="_idParaDest-252"><a id="_idTextAnchor420"/><span class="koboSpan" id="kobo.373.1">Using dowhy for causal effect estimation</span></h2>
<p><span class="koboSpan" id="kobo.374.1">First, we want to </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.375.1">practice with a propensity score </span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.376.1">matching approach that is useful when you have a treatment variable in mind – for example, when you want to identify the effect of a drug on patients and have other variables in the model, such as their diet, age, sex, and so on. </span><span class="koboSpan" id="kobo.376.2">Here, we will use the breast cancer dataset of </span><strong class="source-inline"><span class="koboSpan" id="kobo.377.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.378.1">, where the target variable is a binary outcome telling us about the cells, from masses of breast cancer patients, as being from malignant or benign masses. </span><span class="koboSpan" id="kobo.378.2">Here, we will use the mean </span><em class="italic"><span class="koboSpan" id="kobo.379.1">radius</span></em><span class="koboSpan" id="kobo.380.1"> feature – the mean distance from the center to points on the perimeter – as the </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">treatment variable.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">First, we must import the required libraries and modules </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.384.1">
import pandas as pdimport numpy as np
from sklearn.datasets import load_breast_cancer
import dowhy
from dowhy import CausalModel</span></pre>
<p><span class="koboSpan" id="kobo.385.1">Then, we must load the breast cancer dataset and convert it into </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">a DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.387.1">
breast_cancer = load_breast_cancer()data = pd.DataFrame(breast_cancer.data,
    columns=breast_cancer.feature_names)
data['target'] = breast_cancer.target</span></pre>
<p><span class="koboSpan" id="kobo.388.1">Now, we need to </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.389.1">convert the numerical values </span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.390.1">of the treatment variable, the mean radius, into a binary as propensity scoring matching only accepts binary </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">treatment variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.392.1">
data['mean radius'] = data['mean radius'].gt(data[    'mean radius'].values.mean()).astype(int)
data=data.astype({'mean radius':'bool'}, copy=False)</span></pre>
<p><span class="koboSpan" id="kobo.393.1">We also need to make a list of common causes, which in this case we consider as being all the other attributes in </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.395.1">
common_causes_list = data.columns.values.tolist()common_causes_list.remove('mean radius')
common_causes_list.remove('target')</span></pre>
<p><span class="koboSpan" id="kobo.396.1">Now, we can build a model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">CausalModel()</span></strong><span class="koboSpan" id="kobo.398.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">dowhy</span></strong><span class="koboSpan" id="kobo.400.1"> by specifying the data, treatment, outcome variable, and common causes. </span><span class="koboSpan" id="kobo.400.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">CausalModel()</span></strong><span class="koboSpan" id="kobo.402.1"> object helps us estimate the causal effect of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">treatment</span></strong><span class="koboSpan" id="kobo.404.1"> variable (mean radius) on the outcome </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">variable (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">target</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.408.1">
model = CausalModel(    data=data,
    treatment='mean radius',
    outcome='target',
    common_causes=common_causes_list
)</span></pre>
<p><span class="koboSpan" id="kobo.409.1">Now, we can estimate the causal effect of the specified treatment variable, the mean radius, on the target variable. </span><span class="koboSpan" id="kobo.409.2">Note that propensity score matching, which we’re using here, is applicable only for discrete </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">treatment variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.411.1">
identified_est = model.identify_effect()estimate = model.estimate_effect(identified_est,
    method_name='backdoor.propensity_score_matching')</span></pre>
<p><span class="koboSpan" id="kobo.412.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">estimate</span></strong><span class="koboSpan" id="kobo.414.1"> value is -0.279, which means that the probability of the outcome is decreased by ~28% with the high mean radius as the treatment variable. </span><span class="koboSpan" id="kobo.414.2">This propensity score is the conditional </span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.415.1">probability of receiving the </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.416.1">treatment (high mean radius) given a set of observed covariates. </span><span class="koboSpan" id="kobo.416.2">The backdoor adjustment controls the confounding variables, which are associated with both the treatment and </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">outcome variables.</span></span></p>
<p><span class="koboSpan" id="kobo.418.1">We can also use </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">refute_estimate()</span></strong><span class="koboSpan" id="kobo.420.1"> to assess the validity of our hypothesis regarding the causal variables and their data-driven estimated effects on the outcome. </span><span class="koboSpan" id="kobo.420.2">For example, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">'placebo_treatment_refuter'</span></strong><span class="koboSpan" id="kobo.422.1"> method, which replaces the specified treatment variable with an independent random variable. </span><span class="koboSpan" id="kobo.422.2">If our assumption of causality between the treatment and outcome is correct, then the new estimate goes close to zero. </span><span class="koboSpan" id="kobo.422.3">Here is the code to check the validity of our assumption </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">'placebo_treatment_refuter'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.426.1">
refute_results = model.refute_estimate(identified_estimand,    estimate, method_name='placebo_treatment_refuter',
    placebo_type='permute', num_simulations=40)</span></pre>
<p><span class="koboSpan" id="kobo.427.1">This results in the new effect of 0.0014, which is an assurance about the validity of our assumption. </span><span class="koboSpan" id="kobo.427.2">However, the </span><em class="italic"><span class="koboSpan" id="kobo.428.1">p</span></em><span class="koboSpan" id="kobo.429.1">-value estimate, which is another output of this command, is 0.48, which shows the level of </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">statistical confidence.</span></span></p>
<p><span class="koboSpan" id="kobo.431.1">A low </span><em class="italic"><span class="koboSpan" id="kobo.432.1">p</span></em><span class="koboSpan" id="kobo.433.1">-value from </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">refute_estimate()</span></strong><span class="koboSpan" id="kobo.435.1"> does not mean that the treatment variable is not causal. </span><span class="koboSpan" id="kobo.435.2">A low </span><em class="italic"><span class="koboSpan" id="kobo.436.1">p</span></em><span class="koboSpan" id="kobo.437.1">-value shows </span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.438.1">the sensitivity of the estimated causal </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.439.1">effect to the specific assumption being tested. </span><span class="koboSpan" id="kobo.439.2">The significance of the refutation result does not imply the absence of a causal relationship between the treatment variable </span><a id="_idTextAnchor421"/><span class="koboSpan" id="kobo.440.1">and the </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">outcome variable.</span></span></p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor422"/><span class="koboSpan" id="kobo.442.1">Using bnlearn for causal inference through Bayesian networks</span></h2>
<p><span class="koboSpan" id="kobo.443.1">One of the </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.444.1">libraries that </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.445.1">exists in both the Python </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.446.1">and R programming languages for Bayesian network learning and inference is </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">bnlearn</span></strong><span class="koboSpan" id="kobo.448.1">. </span><span class="koboSpan" id="kobo.448.2">We can learn a Bayesian network for a given dataset using this library and then use the learned graph to infer </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">causal relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">To practice with </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">bnlearn</span></strong><span class="koboSpan" id="kobo.452.1">, we must install and then import this library and load the Sprinkler dataset that exists as part </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">of it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.454.1">
import bnlearn as bndf = bn.import_example('sprinkler')</span></pre>
<p><span class="koboSpan" id="kobo.455.1">Next, we must fit a </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">structure_learning()</span></strong><span class="koboSpan" id="kobo.457.1"> model to generate a Bayesian network or </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">a DAG:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.459.1">
DAG = bn.structure_learning.fit(df)</span></pre> <p><span class="koboSpan" id="kobo.460.1">Then, we must define the properties of the nodes and visualize the DAG, </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.462.1">
# Set some colors to the edges and nodesnode_properties = bn.get_node_properties(DAG)
node_properties['Sprinkler']['node_color']='#00FFFF'
node_properties['Wet_Grass']['node_color']='#FF0000'
node_properties['Rain']['node_color']='#A9A9A9'
node_properties['Cloudy']['node_color']='#A9A9A9'
# Plotting the Bayesian Network
bn.plot(DAG,
    node_properties=node_properties,
    interactive=True,
    params_interactive={'notebook':True,
        'cdn_resources': 'remote'})</span></pre>
<p><span class="koboSpan" id="kobo.463.1">This results in the network shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.464.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.465.1">.4</span></em><span class="koboSpan" id="kobo.466.1">. </span><span class="koboSpan" id="kobo.466.2">As shown in this DAG, </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">'Sprinkler'</span></strong><span class="koboSpan" id="kobo.468.1"> could </span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.469.1">be a causal variable for both cloudy weather and wet grass. </span><span class="koboSpan" id="kobo.469.2">And wet grass could be </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.470.1">potentially caused by rain </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.471.1">and sprinklers. </span><span class="koboSpan" id="kobo.471.2">But there are functionalities to quantify </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">these dependencies:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<span class="koboSpan" id="kobo.473.1"><img alt="Figure 15.4 – Learned DAG using bnlearn for the Sprinkler dataset" src="image/B16369_15_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.474.1">Figure 15.4 – Learned DAG using bnlearn for the Sprinkler dataset</span></p>
<p><span class="koboSpan" id="kobo.475.1">You can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">independence_test()</span></strong><span class="koboSpan" id="kobo.477.1"> as follows to test the dependency of </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">the variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.479.1">
bn.independence_test(DAG, df, test = 'chi_square',    prune = True)</span></pre>
<p><em class="italic"><span class="koboSpan" id="kobo.480.1">Table 15.2</span></em><span class="koboSpan" id="kobo.481.1"> includes a summary of the output of the previous command, clearly showing the significance of the dependency of the paired variables in </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">the DAG:</span></span></p>
<table class="No-Table-Style" id="table002-7">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.483.1">Source</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.484.1">Target</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.485.1">p-value (from </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.486.1">chi_sqare test)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.487.1">chi-square</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.488.1">Cloudy</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.489.1">Rain</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.490.1">1.080606e-87</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.491.1">394.061629</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.492.1">Sprinkler</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.493.1">Wet_Grass</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.494.1">1.196919e-23</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.495.1">100.478455</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.496.1">Sprinkler</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.497.1">Cloudy</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.498.1">8.383708e-53</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.499.1">233.906474</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.500.1">Rain</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.501.1">Wet_Grass</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.502.1">3.886511e-64</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.503.1">285.901702</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.504.1">Table 15.2 – Summary of using bnlearn.independence_test() on the Sprinkler dataset</span></p>
<p><span class="koboSpan" id="kobo.505.1">You can </span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.506.1">also use </span><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">bnlearn.parameter_learning.fit()</span></strong><span class="koboSpan" id="kobo.508.1"> to learn about the CPDs, </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.510.1">
model_mle = bn.parameter_learning.fit(DAG, df,    methodtype='maximumlikelihood')
# Printing the learned Conditional Probability Distribution (CPDs)
bn.print_CPD(model_mle)</span></pre>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.511.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.512.1">.5</span></em><span class="koboSpan" id="kobo.513.1"> shows </span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.514.1">the CPDs of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">Cloudy</span></strong><span class="koboSpan" id="kobo.516.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.517.1">Rain</span></strong><span class="koboSpan" id="kobo.518.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">Sprinkler</span></strong><span class="koboSpan" id="kobo.520.1"> variables. </span><span class="koboSpan" id="kobo.520.2">These CPDs, in combination with </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.521.1">the identified DAG (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.522.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.523.1">.4</span></em><span class="koboSpan" id="kobo.524.1">), provide the required information to not only identify potentially causal relationships between the variables but also do a quantitative assessment </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">of them:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer149">
<span class="koboSpan" id="kobo.526.1"><img alt="Figure 15.5 – Examples of CPDs identified by bnlearn for the Sprinkler dataset" src="image/B16369_15_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.527.1">Figure 15.5 – Examples of CPDs identified by bnlearn for the Sprinkler dataset</span></p>
<p><span class="koboSpan" id="kobo.528.1">In this chapter, you </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.529.1">practiced </span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.530.1">using causal modeling, but </span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.531.1">there is much more to this topic. </span><span class="koboSpan" id="kobo.531.2">This is one of the most important topics in machine learning and you will benefit from learning more about </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">this subj</span><a id="_idTextAnchor423"/><span class="koboSpan" id="kobo.533.1">ect.</span></span></p>
<h1 id="_idParaDest-254"><a id="_idTextAnchor424"/><span class="koboSpan" id="kobo.534.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.535.1">In this chapter, you learned about the difference between correlative and causal relationships, the importance of causal modeling, and techniques such as Bayesian networks for causal inference. </span><span class="koboSpan" id="kobo.535.2">Later, we went through Python practices to help you start working with causal modeling and inference in your projects so that you can identify more reliable relationships between variables in your datasets and design </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">reliable models.</span></span></p>
<p><span class="koboSpan" id="kobo.537.1">In the next chapter, you will learn techniques for preserving privacy and ensuring security while maximizing the benefits of using private and proprietary data in building reliable machine </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">learning mod</span><a id="_idTextAnchor425"/><span class="koboSpan" id="kobo.539.1">els.</span></span></p>
<h1 id="_idParaDest-255"><a id="_idTextAnchor426"/><span class="koboSpan" id="kobo.540.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.541.1">Could you have a feature that is highly correlated with the output but not causal in a supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">learning model?</span></span></li>
<li><span class="koboSpan" id="kobo.543.1">What is the difference between experimental design and observation studies for </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">causal inference?</span></span></li>
<li><span class="koboSpan" id="kobo.545.1">What are the requirements for using instrumental variables for </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">causal inference?</span></span></li>
<li><span class="koboSpan" id="kobo.547.1">Could relationships in a Bayesian network necessarily be </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">considered cau</span><a id="_idTextAnchor427"/><span class="koboSpan" id="kobo.549.1">sal?</span></span></li>
</ol>
<h1 id="_idParaDest-256"><a id="_idTextAnchor428"/><span class="koboSpan" id="kobo.550.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.551.1">Schölkopf, Bernhard. </span><em class="italic"><span class="koboSpan" id="kobo.552.1">Causality for machine learning</span></em><span class="koboSpan" id="kobo.553.1">. </span><span class="koboSpan" id="kobo.553.2">Probabilistic and Causal Inference: The Works of Judea Pearl. </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">2022. </span><span class="koboSpan" id="kobo.554.2">765-804.</span></span></li>
<li><span class="koboSpan" id="kobo.555.1">Kaddour, Jean, et al. </span><em class="italic"><span class="koboSpan" id="kobo.556.1">Causal machine learning: A survey and open problems</span></em><span class="koboSpan" id="kobo.557.1">. </span><span class="koboSpan" id="kobo.557.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">arXiv:2206.15475 (2022).</span></span></li>
<li><span class="koboSpan" id="kobo.559.1">Pearl, Judea. </span><em class="italic"><span class="koboSpan" id="kobo.560.1">Bayesian </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.561.1">networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">. </span><span class="koboSpan" id="kobo.562.2">(2011).</span></span></li>
<li><span class="koboSpan" id="kobo.563.1">Richens, Jonathan G., Ciarán M. </span><span class="koboSpan" id="kobo.563.2">Lee, and Saurabh Johri. </span><em class="italic"><span class="koboSpan" id="kobo.564.1">Improving the accuracy of medical diagnosis with causal machine learning</span></em><span class="koboSpan" id="kobo.565.1">. </span><span class="koboSpan" id="kobo.565.2">Nature communications 11.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">2020): 3923.</span></span></li>
<li><span class="koboSpan" id="kobo.567.1">Prosperi, Mattia, et al. </span><em class="italic"><span class="koboSpan" id="kobo.568.1">Causal inference and counterfactual prediction in machine learning for actionable healthcare</span></em><span class="koboSpan" id="kobo.569.1">. </span><span class="koboSpan" id="kobo.569.2">Nature Machine Intelligence 2.7 (</span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">2020): 369-375.</span></span></li>
<li><span class="koboSpan" id="kobo.571.1">Sanchez, Pedro, et al. </span><em class="italic"><span class="koboSpan" id="kobo.572.1">Causal machine learning for healthcare and precision medicine</span></em><span class="koboSpan" id="kobo.573.1">. </span><span class="koboSpan" id="kobo.573.2">Royal Society Open Science 9.8 (</span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">2022): 220638.</span></span></li>
</ul>
</div>
</body></html>