- en: Chapter 11. Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we covered a few supervised, semi-supervised, unsupervised, and reinforcement
    learning techniques and algorithms. In this chapter, we will cover neural networks
    and its relationship with the deep learning practices. The traditional learning
    approach was about writing programs that tell the computer what to do, but neural
    networks are about learning and finding solutions using observational data that
    forms a primary source of input. This technique's success depends on how the neural
    networks are trained (that is, the quality of the observational data). Deep learning
    refers to methods of learning the previously referenced neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The advancement in technology has taken these techniques to new heights where
    these techniques demonstrate superior performance, and are used to solve some
    key non-trivial requirements in computer vision, speech recognition, and **Natural
    Language Processing** (**NLP**). Large companies such as Facebook and Google,
    among many others, have adopted deep learning practices on a substantial basis.
  prefs: []
  type: TYPE_NORMAL
- en: The primary aim of this chapter is to enforce mastering the neural networks
    and related deep learning techniques conceptually. With the aid of a complex pattern
    recognition problem, this chapter covers the procedure to develop a typical neural
    network, which you will be able to use to solve a problem of a similar complexity.
    The following representation shows all the learning methods covered in this book,
    highlighting the primary subject of learning in this chapter—*Deep learning*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning](img/B03980_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The chapter covers the following topics in depth:'
  prefs: []
  type: TYPE_NORMAL
- en: A quick revisit of the purpose of Machine learning, types of learning, and the
    context of deep learning with details on a particular problem that it solves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An overview of neural networks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human brain as the primary inspiration for neural networks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of neural network architectures and some basic models of neurons
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple learning example (digit recognition)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of perceptrons, the first generation of neural networks and what
    they are capable of doing and what they are not capable of doing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of linear and logistic output neurons. An introduction to back the
    propagation algorithm and applying the derivatives of back propagation algorithm
    for solving some real-world problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of cognitive science, the softmax output function, and handling
    multi-output scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying convolution nets and the problem of object or digit recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNN**) and Gradient descent method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal processing as the principle of component analysis and autoencoders; the
    types of autoencoders which are deep and shallow autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A hands-on implementation of exercises using Apache Mahout, R, Julia, Python
    (scikit-learn), and Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first recap the premise of Machine learning and reinforce the purpose
    and context of learning methods. As we learned, Machine learning is about training
    machines by building models using observational data, against directly writing
    specific instructions that define the model for the data to address a particular
    classification or a prediction problem. The word *model* is nothing but a *system*
    in this context.
  prefs: []
  type: TYPE_NORMAL
- en: The program or system is built using data and hence, looks as though it's very
    different from a hand-written one. If the data changes, the program also adapts
    to it for the next level of training on the new data. So all it needs is the ability
    to process large-scale as opposed to getting a skilled programmer to write for
    all the conditions that could still prove to be heavily erroneous.
  prefs: []
  type: TYPE_NORMAL
- en: We have an example of a Machine learning system called spam detector. The primary
    purpose of this system is to identify which mail is spam and which is not. In
    this case, the spam detector is not coded to handle every type of mail; instead,
    it learns from the data. Hence, it is always true that the precision of these
    models depends on how good the observational data is. In other words, the features
    extracted from the raw data should typically cover all the states of data for
    the model to be accurate. Feature extractors are built to extract standard features
    from the given sample of data that the classifier or a predictor uses.
  prefs: []
  type: TYPE_NORMAL
- en: Some more examples include recognizing patterns such as speech recognition,
    object recognition, face detection, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a type of Machine learning that attempts to learn prominent
    features from the given data, and thus tries to reduce the task of building a
    feature extractor for every category of data (for example, image, voice, and so
    on.). For a face detection requirement, a deep learning algorithm records or learns
    features such as the length of the nose, the distance between the eyes, the color
    of the eyeballs, and so on. This data is used to address a classification or a
    prediction problem and is evidently very different from the traditional **shallow
    learning algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: The human brain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human brain is known to be one of the most implausible organs in the human
    body. The brain is essentially what makes us, humans, intelligent. It is responsible
    for building our perceptions based on what we experience regarding our senses
    of touch, smell, sight, vision, and sound. These experiences are collected and
    stored as memories and emotions. Inherently, the brain is what makes us intelligent
    without which, we probably are just primitive organisms in the world.
  prefs: []
  type: TYPE_NORMAL
- en: The brain of a newborn infant is capable of solving problems that any complex
    and powerful machine cannot solve. In fact, just within a few days of birth, the
    baby starts recognizing the face and voice of his/her parents and starts showing
    the expressions of longing to see them when they are not around. Over a period,
    they begin associating sounds with objects and can even recognize an object given
    a sight. Now, how do they do this? If they come across a dog, how do they recognize
    it to be a dog; also, do they associate a barking sound with it and mimic the
    same sound?
  prefs: []
  type: TYPE_NORMAL
- en: It is simple. Every time the infant comes across a dog, his/her parents qualify
    it to be a dog, and this reinforces the child's model. In case they qualify the
    child to be wrong, the child's model would incorporate this information. So, a
    dog has long ears, long nose, four legs, a long tail, and can be of different
    colors such as black, white or brown, making a barking sound. These characteristics
    are recognized through sight and sound that an infant's brain records. The observational
    data thus collected drives the recognition of any new object henceforth.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's say the infant sees a wolf for the first time; he/she would identify
    a wolf to be a dog by looking at the similarity of its characteristics s. Now,
    if the parent feeds in the definite differences on the first sighting, for example,
    a difference in the sound that it makes, then it becomes a new experience and
    is stored in memory, which is applied to the next sighting. With the assimilation
    of more and more such examples, the child's model becomes more and more accurate;
    this process is very subconscious.
  prefs: []
  type: TYPE_NORMAL
- en: For several years, we have been working toward building machines that can be
    intelligent with brains as those of humans. We are talking about robots that can
    behave as humans do and can perform a particular job with similar efficiency to
    humans beings, such as driving a car, cleaning a house, and so on. Now, what does
    it take to build machines as robots? We probably need to build some super-complex
    computational systems that solve the problems our brain can solve in no time.
    This field that works on building artificially intelligent systems is called deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some formal definitions of deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: According to Wikipedia, Deep learning is a set of algorithms for machine learning
    that attempts to model high-level abstractions in data by using model architectures
    composed of multiple non-linear transformations.
  prefs: []
  type: TYPE_NORMAL
- en: According to [http://deeplearning.net/](http://deeplearning.net/), Deep learning
    is the new area of Machine learning research that has been introduced with the
    objective of moving Machine learning closer to one of its original goals—Artificial
    Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'This subject has evolved over several years; the following table lists research
    areas across the years:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research Area | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Neural networks | 1960 |'
  prefs: []
  type: TYPE_TB
- en: '| Multilayer Perceptrons | 1985 |'
  prefs: []
  type: TYPE_TB
- en: '| Restricted Boltzmann Machine | 1986 |'
  prefs: []
  type: TYPE_TB
- en: '| Support Vector Machine | 1995 |'
  prefs: []
  type: TYPE_TB
- en: '| Hinton presents the **Deep Belief Network** (**DBN**)New interests in deep
    learning and RBMState of the art MNIST | 2005 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Recurrent Neural Network | 2009 |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional DBN | 2010 |'
  prefs: []
  type: TYPE_TB
- en: '| Max-Pooling CDBN | 2011 |'
  prefs: []
  type: TYPE_TB
- en: Among many others, some key contributors to this field are Geoffrey Hinton,
    Yann LeCun, Honglak Lee, Andrew Y. Ng, and Yoshua Bengio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following concept model covers different areas of Deep learning and the
    scope of topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The human brain](img/B03980_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at a simple problem on hand; the requirement is to recognize the
    digits from the handwritten script given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The human brain](img/B03980_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a human brain, this is very simple as we can recognize the digits as 287635\.
    The simplicity with which our brain interprets the digits is perceptive that it
    undermines the complexity involved in this process. Our brain is trained to intercept
    different visuals progressively due to the presence of visual cortices, with each
    cortex containing more than 140 million neurons that have billions of connections
    between them. In short, our brain is no less than a supercomputer that has evolved
    over several millions of years and is known to adapt well to the visual world.
  prefs: []
  type: TYPE_NORMAL
- en: If a computer program has to crack the recognition of the digits, what should
    be the rules to identify and differentiate a digit from another?
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are one such field being researched for several years and is
    known to address the need for multilayered learning. The overall idea is to feed
    a large number of handwritten digits; an example of this data (training) is shown
    in the following image, and that can learn from these examples. This means the
    rules are automatically inferred from the provided training data. So, the larger
    the training dataset, the more accurate would be the prediction. If we are posed
    with a problem to differentiate the digit 1 from the digit 7 or the digit 6 from
    the digit 0, some minor differences will need to be learned. For a zero, the distance
    between the starting and ending point is minimal or nothing.
  prefs: []
  type: TYPE_NORMAL
- en: '![The human brain](img/B03980_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The difference is basically because these learning methods have been targeted
    to mimic a human brain. Let's see what makes this a difficult problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, with deep learning being a subset of Machine learning, we know that
    this involves the technique of feeding examples and a model that can evaluate
    the pattern to evolve it in case it makes a mistake. Thus, over a period of time,
    this model would solve the problem with the best possible accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: If this needs to be represented mathematically, let's define our model to be
    a function *f(x,θ)*.
  prefs: []
  type: TYPE_NORMAL
- en: Here, *x* is the input that is provided as a vector of values and *θ* is a reference
    vector that the model uses to predict or classify *x*. So, it is *θ* that we need
    to expose to a maximum set of examples in order to improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example; if we were to predict whether a visitor to a restaurant
    would come back based on two factors—one is the amount of bill (*x[1]*) and the
    other is his/her age(*x[2]*). When we collect data for a specific duration of
    time and analyze it for an output value that can be 1(in case the visitor came
    back) or -1(if the visitor has not come back). The data, when plotted, can take
    any form—from a linear relationship or any other complex form, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The human brain](img/B03980_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Something like a linear relationship looks straight forward and more complex
    relationships complicate the dynamics of the model. Can parameter *θ* have an
    optimal value at all? We might have to apply optimization techniques and in the
    next sections to follow, we will cover these techniques such as perceptrons and
    gradient descent methods among others. If we want to develop a program to do this,
    we need to know what our brain does to recognize these digits, and even if we
    knew, these programs might be very complex in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural computations have been a primary interest of the study to understand
    how parallel computations work in neurons (the concept of flexible connections)
    and solve practical problems like a human brain does. Let''s now look at the core
    fundamental unit of the human brain, the *neuron*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural networks](img/B03980_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The human brain is all about neurons and connections. A neuron is the smallest
    part of the brain, and if we take a small rice grain sized piece of the brain,
    it is known to contain at least 10000 neurons. Every neuron on an average has
    around 6000 connections with other neurons. If we look at the general structure
    of a neuron, it looks as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every feeling that we humans go through, be it thought or emotion, is because
    of these millions of cells in our brain called neurons. As a result of these neurons
    communicating with each other by passing messages, humans feel, act, and form
    perceptions. The diagram here depicts the biological neural structure and its
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuron](img/B03980_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Every neuron has a central cell body; as any cell, in general, it has an axon
    and a dendritic tree that are responsible for sending and receiving messages respectively
    with other neurons. The place where axons connect to the dendritic tree is called
    a synapse. The synapses themselves have an interesting structure. They contain
    transmitter molecules that trigger transmission, which can either be positive
    or negative in nature.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to the neurons are aggregated, and when they exceed the threshold,
    an electrical spike is transmitted to the next neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Synapses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following diagram depicts the model of a synapse depicting the flow of messages
    from axon to dendrite. The job of the synapse is not just the transmission of
    messages, but in fact, adapt themselves to the flow of signals and have the ability
    to learn from past activities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Synapses](img/B03980_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As an analogy in the field of Machine learning, the strength of the incoming
    connection is determined on the basis of how often it is used, and thus its impact
    on the neuron output is determined. This is how new concepts are learned by humans
    subconsciously.
  prefs: []
  type: TYPE_NORMAL
- en: There can additionally be external factors such as medication or body chemistry
    that might impact this learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will finally summarize how the learning happens inside the brain with
    the help of the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: Neurons communicate with other neurons or sometimes receptors. The cortical
    neurons use spikes for communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strengths of connections between neurons can change. They can take positive
    or negative values by either establishing and removing connections between neurons
    or by strengthening the connection based on the influence that a neuron can have
    over the other. A process called **long-term potentiation** (**LTP**) occurs that
    results in this long-term impact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are about 1011 neurons having weights that make the computations that
    the human brain can do more efficiently than a workstation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the brain is modular; different parts of the cortex are responsible
    for doing different things. Some tasks infuse more blood flow in some regions
    over the other and thus, ensuring different results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before schematizing the neuron model into the **artificial neural network**
    (**ANN**), let us first look at different types, categories, or aspects of neurons,
    and in specific the Artificial neuron or Perceptron, the deep learning equivalent
    of a biological neuron. This approach is known to have produced extremely efficient
    results in some of the use cases we listed in the previous section. ANNs are also
    called feed-forward neural networks, **Multi-Layer Perceptrons** (**MLP**), and,
    recently, deep networks or learning. One of the important characteristics has
    been the need for feature engineering, whereas deep learning represents applications
    that require minimum feature engineering, where learning happens through multiple
    learned layers of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons or perceptrons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is obvious that artificial neurons draw inspiration from biological neurons,
    as represented previously. The features of an artificial neuron are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a set of inputs received from other neurons that activate the neuron
    in context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an output transmitter that transfers signals or an activation of the
    other neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the core processing unit is responsible for producing output activations
    from the input activations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Idealizing for a neuron is a process that is applied to building models. In
    short, it is a simplification process. Once simplified, it is possible to apply
    mathematics and relate analogies. To this case, we can easily add complexities
    and make the model robust under identified conditions. Necessary care needs to
    be taken in ensuring that none of the significantly contributing aspects are removed
    as a part of the simplification process.
  prefs: []
  type: TYPE_NORMAL
- en: Linear neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear neurons are the simplest form of neurons; they can be represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear neurons](img/B03980_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output *y* is a summation of the product of the input *x[i]* and its weight
    *w[i]*. This is mathematically represented as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear neurons](img/B03980_11_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *b* is the bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph representation of the previous equation is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear neurons](img/B03980_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Rectified linear neurons / linear threshold neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rectified linear neurons are similar to linear neurons, as explained in the
    preceding section, with a minor difference where the output parameter value is
    set to zero in cases where it is less than (<) zero (0), and in case the output
    value is greater than (>) zero (0), it continues to remain as the linear weighted
    sum of the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rectified linear neurons / linear threshold neurons](img/B03980_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Binary threshold neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The binary threshold neurons were introduced by McCulloch and Pitts in 1943\.
    This class of neurons first have the weighted sum of the inputs computed, similar
    to the linear neurons. If this value exceeds a defined threshold, a fixed size
    spike to activity is sent out. This spike is called as the *truth value of a proposition*.
    Another important point is the output. The output at any given point in time is
    binary (0 or 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation that demonstrates this behavior is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary threshold neurons](img/B03980_11_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 1* if *z ≥ θ*,'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 0* otherwise'
  prefs: []
  type: TYPE_NORMAL
- en: here *θ = -b (bias)*
  prefs: []
  type: TYPE_NORMAL
- en: (OR)
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary threshold neurons](img/B03980_11_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 1* if *z ≥ 0*,'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 0* otherwise'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, a graphical representation of the previous equation is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary threshold neurons](img/B03980_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sigmoid neurons are highly adopted in artificial neural networks. These neurons
    are known to provide the output that is smooth, real-valued, and therefore a bounded
    function of all the inputs. Unlike the types of the neurons that we have seen
    until now, these neurons use the logistic function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic function is known to have an easy-to-calculate derivative that
    makes learning easy. This derivative value is used in computing the weights. Following
    is the equation for the sigmoid neuron output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sigmoid neurons](img/B03980_11_44.jpg)![Sigmoid neurons](img/B03980_11_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The diagrammatic/graphical representation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sigmoid neurons](img/B03980_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic binary neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stochastic binary neurons use the same equation as logistic units, with one
    important difference that the output is measured for a probabilistic value, which
    measures the probability of producing a spike in a short window of time. So, the
    equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic binary neurons](img/B03980_11_46.jpg)![Stochastic binary neurons](img/B03980_11_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, the graphical representation of this equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic binary neurons](img/B03980_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Overall, what we can observe is that each neuron takes in a weighted sum of
    a bunch of inputs on which a non-linear activation function is applied. Rectified
    linear function is typically applied for solving regression problems and for classification
    problems, logistic functions are applied. A generic representation of this can
    be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic binary neurons](img/B03980_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, these inputs can be fed into a series of layers of neurons. Let''s look
    at what happens next and how this happens. The input layer pushes input values;
    the hidden layers of neurons then take the values as input. It is possible to
    have multiple layers within these hidden layers, where the output from one layer
    feeds as the input to the subsequent layer. Each of these layers can be responsible
    for the specialized learning. Moreover, finally, the last in the hidden layer
    feeds into the final output layer. This typical structure of an ANN is illustrated
    next. Every circle in the next diagram represents a neuron. The concept of the
    **Credit Assignment Path** (**CAP**) refers to the path from input to output.
    In the feed-forward networks, the length of the path is the total number of hidden
    layers along with the output layer. The following diagram shows a feed-forward
    neural network with a single hidden layer and connections between layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic binary neurons](img/B03980_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The case of two hidden layers are shown in here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic binary neurons](img/B03980_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neural Network size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computing the number of neurons or parameters is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the single layer network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of neurons = 4 + 2 = 6 (inputs are not counted)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Total weights = [3 x 4] + [4 x 2] = 20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Total bias = 4 + 2 = 6, for 26 learnable parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the two layer network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of neurons = 4 + 4 + 1 = 9 (inputs are not counted)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Total weights = [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Total bias = 4 + 4 + 1 = 9 for 41 learnable parameters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, what is the optimal size of neural networks? It is important to identify
    the possible number of hidden layers along with the size of each layer. These
    decisions determine the capacity of the network. A higher value helps to support
    a higher capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example where we will try three different sizes of the hidden
    layer by obtaining the following classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural Network size](img/B03980_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, with more neurons, functions with higher complexity can be expressed,
    which is good, but we need to watch out for the over-fitting case. So, a smaller-sized
    network works well for simpler data. With the increasing data complexity, the
    need for a bigger size arises. The trade-off is always between handling the complexity
    of the model versus. over-fitting. Deep learning addresses this problem as it
    applies complex models to extremely complex problems and handles over-fitting
    by taking additional measures.
  prefs: []
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A face recognition case using the multi-layered perceptron approach is shown
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/B03980_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple layers take this image as input and finally, a classifier definition
    is created and stored.
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/B03980_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given a photograph, each layer focuses on learning a specific part of the photograph
    and finally stores the output pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key notes on the weights and error measures are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The training data is the source of learning the weights of neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error measure or the cost function is different from the regression and
    classification problems. For classification, log functions are applied, and for
    regression, least square measures are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods help to keep these error measures in check by updating the weights
    using convex optimization techniques such as decent gradient methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will cover some key types of neural networks. The following
    concept map lists a few principal types of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network types](img/B03980_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multilayer fully connected feedforward networks or Multilayer Perceptrons (MLP)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As covered in the introductory sections about neural networks, an MLP has multiple
    layers where the output of one layer feeds as an input to a subsequent layer.
    A multilayer perceptron is represented as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multilayer fully connected feedforward networks or Multilayer Perceptrons
    (MLP)](img/B03980_11_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Jordan networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Jordan networks are partially recurrent networks. These networks are the current
    feedforward networks with a difference of having additional context neurons inside
    the input layer. These context neurons are self-imposed and created using the
    direct feedback from input neurons. In Jordon networks, the number of context
    neurons is always equal to the input neurons. The following diagram depicts the
    difference in the input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jordan networks](img/B03980_11_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Elman networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Elman networks, just as Jordon networks, are partially recurrent feedforward
    networks. These networks also have context neurons, but in this case, the main
    difference is that the context neurons receive the feed from the output neurons,
    and not from the hidden layers. There is no direct correlation between the number
    of context neurons and input neurons; rather, the number of context neurons is
    the same as the number of hidden neurons. This, in turn, makes this model more
    flexible, just as the number of hidden neurons do on a case-by-case basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Elman networks](img/B03980_11_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Radial Bias Function (RBF) networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Radial Bias Function networks are also feed-forward neural networks. These networks
    have a special hidden layer of special neurons called radially symmetric neurons.
    These neurons are for converting the distance value between the input vector and
    the center using a Gaussian measure. The advantage of this additional layer is
    that it gives an additional capability to determine the number of layers required
    without a manual intervention. The choice of the linear function determines the
    optimal output layer. Therefore, the learning happens relatively faster in these
    networks even in comparison to back propagation.
  prefs: []
  type: TYPE_NORMAL
- en: The only downside of this method is its ability to handle large input vectors.
    The diagram below depicts the hidden layer of radially symmetric neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '![Radial Bias Function (RBF) networks](img/B03980_11_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hopfield networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hopfield networks work around a concept called *energy of network*. This is
    nothing but an optimal local minima of the network that defines an equilibrium
    state for the functionality. Hopfield networks target the state of achieving this
    equilibrium state. An equilibrium state is when the output of one layer becomes
    equal to the output of the previous layer. The following diagram depicts how the
    input and output states are checked and managed in the Hopfield network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hopfield networks](img/B03980_11_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dynamic Learning Vector Quantization (DLVQ) networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **Dynamic Learning Vector Quantization** (**DLVQ**) network model is another
    variation of neural networks that starts with a smaller number hidden layers and
    dynamically generates these hidden layers. It is important to have similarities
    in patterns that belong to the same class; hence, this algorithm best suits classification
    problems, such as recognition of patterns, digits, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we will look at one of the most popular ways of optimizing
    the neural network, minimizing the cost function, minimizing errors, and improving
    the accuracy of the neural network: the Gradient descent method. The graph here
    shows the actual versus. The predicted value along with the cases of inaccuracy
    in the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient descent method](img/B03980_11_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking forward the topic of training the networks, the Gradient descent algorithm
    helps neural networks to learn the weights and biases. Moreover, to compute the
    gradient of the cost function, we use an algorithm called backpropagation. Backpropagation
    was first discussed in the 1970s and became more prominent regarding its application
    only in the 1980s. It was proven that neural network learning was much faster
    when backpropagation algorithm was employed.
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier sections of this chapter, we saw how a matrix-based algorithm
    works; a similar notation is used for the backpropagations algorithm. For a given
    weight *w* and bias *b*, the cost function C has two partial derivatives which
    are *∂C/∂w* and *∂C/∂b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key assumptions regarding the cost function for backpropagation are stated
    here. Let''s assume that the cost function is defined by the equation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where, *n* = number of training examples
  prefs: []
  type: TYPE_NORMAL
- en: '*x* = sum across the individual training sets'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *y(x)* is the expected output'
  prefs: []
  type: TYPE_NORMAL
- en: '*L* = total number of layers in the neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '*a^L* = *a^L(x)* is the output activation vector'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 1: The overall cost function can be an average of the individual
    cost functions. For *x* individual training sets, the cost function can now be
    stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, the cost function for an individual training set can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With this assumption, since we can compute the partial derivatives for each
    training set *x* as ∂[x]C/∂w and ∂[x]C/∂b, the overall partial derivative functions
    ∂C/∂w and ∂C/∂b can be an average of the partial derivatives for each training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assumption 2: This hypothesis is about the cost function *C* that *C* can be
    the function of outputs from the neural networks as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Extending the previous equation of the cost function, the quadratic cost function
    for each training example set *x* can now be written as follows. We can see how
    this acts as a function of the output activations as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Back propagation is about the impact the weights and bias have on the overall
    cost function value.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the error in the *j^(th)* neuron in the *l^(th)* layer, *δ^l[j]*,
    and then use this value to calculate the partial derivatives that relate to this
    error *δ^l[j]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The error function *δ^l[j]* of the *j^(th)* neuron in the *l^(th)* layer can
    be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the error for the layer *L* *δ^L* can be computed as well. This, in turn,
    helps to compute the gradient of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equations are used by the back propagation algorithm in sequence,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 1: The computing error in the layer *L*, *δ^L*, given the neuron at
    the position *j*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2: The computing error in the layer *L*, *δ^L*, given the error in
    the next layer *δ^(L+1)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Hadamard product is a matrix multiplication technique that is used for
    element-wise matrix multiplication as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The notation ![Backpropagation algorithm](img/B03980_11_70.jpg) is used to represent
    this method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 3: This equation measures the impact on the cost and gives a change
    in the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, we will get the following from equations 1 and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is because the error value is the same as the rate of change of the partial
    derivative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 4: This equation is used to compute the rate of change of the cost
    as a relationship to the weight.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation algorithm](img/B03980_11_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At every stage of these algorithms, there is some kind of learning that impacts
    the overall output from the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final backpropagation algorithm as compiled is explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer *x*, and for *x =1* Set the activation as *a¹*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of the other layers *L = 2, 3, 4 …. L*, compute the activations as:![Backpropagation
    algorithm](img/B03980_11_37.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the error *δ^L* using equations 1 and 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error for *l = L-1, L-2, … 2*, 1 using the equation 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, compute the gradient of the cost function using equation 4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we observe the algorithm, the error vectors *δ^l* are calculated backwards,
    starting from the output layer. This is the fact that the cost is a function of
    outputs from the network. To understand the impact of earlier weights on the cost,
    a chain rule needs to be applied that works backwards through all the layers.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax regression technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Softmax regression is also known as multinomial logistic regression. This section
    does not cover the concept of logistic regression in depth as it is covered in
    the chapter related to a regression in this book. Instead, we will specifically
    look at understanding how this technique is employed in digit recognition-related
    problems in deep learning use cases.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is a special case of the logistic regression that works for multiple
    classes. As we learned, the result of logistic regression is a binary value *{0,1}*.
    Softmax regression facilitates handling *y(i)<--{1,…,n}*, where *n* is the number
    of classes against the binary classification. In the MNIST digit recognition case,
    the value *n* is 10, representing 10 different classes. For example, in the MNIST
    digit recognition task, we would have *K=10* different classes.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of its ability to process multiple classes, this technique is used
    actively in neural network-based, problem-solving areas.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning taxonomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The feature learning taxonomy for deep learning cases is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning taxonomy](img/B03980_11_63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some of the frameworks that are used to implement neural network applications
    are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Theano is a Python library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch a Lua programming language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning4J is an open, source Java-based framework that works with Spark
    and Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe is a C++ based framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNN/ConvNets)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNN, also known as convolution nets (ConvNets), are a variation of the regular
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Let us recap the function of the regular neural network. Regular neural networks
    have a single vector-based input that is transformed through a series of hidden
    layers where the neurons in each layer are connected with the neurons in its neighboring
    layers. The last layer in this series provides the output. This layer is called
    the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: When the input to the neural network is an image and does not just fit into
    a single vector structure, the complexity grows. CNN have this slight variation
    where the input is assumed as a three-dimensional vector having depth (D), height
    (H) and width (W). This assumption changes the way the neural network is organized
    and the way it functions. The following diagram compares the standard three layers
    neural network with the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional neural networks (CNN/ConvNets)](img/B03980_11_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we see, the convolutional net shown previously arranges neurons in a three-dimensional
    way; every layer in the network transforms this into a 3D output of neuron activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolution network architecture comprises a fixed set of layers designated
    for specialized functions. The most critical layers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layer** (**CONV**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling layer** (**POOL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full-connected** (**FC**) layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, the activation function is written as another layer (RELU); a
    distinct normalization layer for FC layer conversion may exist.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer (CONV)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The convolutional layer forms the core of convolution nets. This layer is responsible
    for holding the neurons in a three-dimensional format and is therefore responsible
    for a three-dimensional output. The following is an example of an input volume
    with the dimensions 32 x 32 x 3\. As shown, each neuron is connected to a particular
    input region. Along the depth, there can be many neurons; we can see five neurons
    in the example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layer (CONV)](img/B03980_11_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The diagram here shows how the net convolution function works in the neuron
    function representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layer (CONV)](img/B03980_11_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That said, the core function of the neuron remains unchanged and is responsible
    for computing the product of weights and the inputs followed by an observation
    of non-linear behavior. The only difference is the restrictions on the connectivity
    to the local regions.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer (POOL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There can be multiple convolution layers and, between these convolution layers,
    there can be a pooling layer. The pooling layer is responsible for reducing the
    chances of over-fitting by reducing the spatial size of the input volume. The
    reduction of the spatial size implies reducing the number of parameters or the
    amount of computations in the network. The MAX functions contribute to reducing
    the spatial size. The pooling layers use the MAX functions and apply it on every
    slice in the three-dimensional representation, sliced depth-wise. Usually, the
    pooling layers apply filters of size 2 X 2 applied along both width and height.
    This can discard around 75% of the activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the pooling layer has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Always considers a volume size of W1×H1×D1 as an input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applies stride S and spatial extent F and generates the W2×H2×D2 output where:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W2=(W1−F)/S+1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: H2=(H1−F)/S+1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D2=D1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fully connected layer (FC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fully connected layer is very similar to the regular or traditional neural
    networks, responsible for establishing extensive connections to the previous layer
    activations. The connection activations are computed using matrix multiplication
    techniques. More details on this can be found upon referring to the earlier sections
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RNNs are a special case of neural networks that are known to be very efficient
    in remembering information, because the hidden state is stored in a distributed
    manner, and so it can hold more information about the experiences. These networks
    also apply non-linear functions to update the hidden state. The following diagram
    depicts how the hidden states link in RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Neural Networks (RNNs)](img/B03980_11_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In most of the real world examples, the inputs and outputs are not independent
    of each other. For example, if we had to predict the next word, it would be important
    for us to know the words that came before it. As the word suggests, "Recurrent"
    Neural Networks execute the same task over and over again, where the input of
    one execution is the output of the previous execution. Usually, RNNs are known
    to go back only a few steps in the past and not always through all the iterations.
    The following diagram depicts how RNNs work; it shows how RNNs unfold across iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Neural Networks (RNNs)](img/B03980_11_65.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous example, the requirement was to predict the next word, and if
    there were five words in the input, then RNN unfolds upto five layers.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines (RBMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RBMs came into existence to solve the difficulty in training RNNs. The rise
    of restricted recurrent models to handle these training difficulties simplified
    the problem context, and additionally, learning algorithms are applied to solve
    the problem. The Hopfield Neural Network is an example of a restricted model that
    addresses the previously described problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, Boltzmann machines came into existence. These models were
    a special case of Hopfield Neural Networks with a stochastic element. In this
    case, the neurons were of two categories: the ones that resulted in visible states
    and the others in hidden states. This was also similar to the Hidden Markov''s
    model. A RBM is again a special case of Boltzmann machine, where the difference
    is primarily to do with the absence of connections between the neurons in the
    same layer. So, for the given states of the neurons of one group, the states of
    the neurons in the other group are independent. The following diagram depicts
    a typical RBN structure and the previous definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann Machines (RBMs)](img/B03980_11_66.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Taking this definition further for a deeper interpretation, some visible states
    of neurons are observable, and there are hidden states of neurons that are not
    visible or cannot directly be seen. There are a few probabilistic conclusions
    made on the hidden states based on the available visible states, and this is how
    the training model is formed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an RBM, the connectivity is restricted, and this, in turn, eases the inferencing
    and learning. It typically takes only one step to reach an equilibrium state with
    the visible states clamped. The following formula shows how the probability of
    the hidden state is computed, given that the information about the visible states
    is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann Machines (RBMs)](img/B03980_11_67.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Deep Boltzmann Machines (DBMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DBMs are a special case of conventional Boltzmann machines with a lot of missing
    connections, and, unlike the sequential stochastic updates, parallel updates are
    allowed for ensuring efficiency in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'DBMs restrict the connections between hidden variables and primarily use unlabeled
    data for training the models. Labeled data is used for fine-tuning the model.
    The following diagram depicts the general structure of a three-layered DBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Boltzmann Machines (DBMs)](img/B03980_11_68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we understand autoencoders, let's first learn about **autoassociators**
    (**AAs**). The goal of AAs is to receive an input to the maximum possible precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of an AA is to receive the output as an image of the input as precisely
    as possible. There are two categories of AAs: one is generating AAs, and the second
    is synthesizing AAs. RBMs covered in the previous section are categorized as generating
    AAs, and autoencoders synthesize AAs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An autoencoder is a type of neural network that has a single open layer. Applying
    backpropagation and unsupervised learning techniques, autoencoders start with
    an assumption that the target value is equal to the input value, *y = x*. The
    following diagram depicts an autoencoder that learns the function *h[W,b] (x)
    ≈ x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autoencoders](img/B03980_11_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The layer in the middle is open, and as depicted in the previous diagram, for
    optimal output, it is essential for this layer to have lesser number of neurons
    than that of the input layer. The goal of this model is to learn an approximation
    to the identity function in such a way that the values of **Layer L[3]** are equal
    to values in **Layer L[1]**.
  prefs: []
  type: TYPE_NORMAL
- en: The data is compressed when it passes through the input to output layers. When
    an image of certain pixels, say 100 pixels (10 X 10 pixels), is input to the model
    for a hidden layer with 50 neurons, the expectation is that the network tries
    to compress the image by keeping the pixel configuration intact. This kind of
    compression is possible only if there are hidden interconnections and other characteristic
    correlations that can reduce the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Another variation of an autoencoder is **denoising autoencoder** (**DA**). The
    difference in this variation of autoencoder is its additional capability to recover
    and restore the state impacted by the corrupt input data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ANNs and Deep learning methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing artificial
    neural networks and other deep learning methods covered in this chapters (source
    code path `.../chapter11/...` under each of the folders for the technologies).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter11/annexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter11/dlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter11/annexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter11/dlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter11/annexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter11/dlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (Scikit-learn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter11/annexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter11/dlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter11/annexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter11/dlexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the model of a biological neuron and how an artificial
    neuron is related to its function. You learned the core concepts of neural networks,
    and how fully connected layers work. We have also explored some key activation
    functions that are used in conjunction with matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
