- en: Chapter 11. Deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 深度学习
- en: Until now, we covered a few supervised, semi-supervised, unsupervised, and reinforcement
    learning techniques and algorithms. In this chapter, we will cover neural networks
    and its relationship with the deep learning practices. The traditional learning
    approach was about writing programs that tell the computer what to do, but neural
    networks are about learning and finding solutions using observational data that
    forms a primary source of input. This technique's success depends on how the neural
    networks are trained (that is, the quality of the observational data). Deep learning
    refers to methods of learning the previously referenced neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍了一些监督学习、半监督学习、无监督学习和强化学习的技术和算法。在本章中，我们将介绍神经网络及其与深度学习实践的关系。传统的学习方法涉及编写告诉计算机做什么的程序，但神经网络是关于学习和使用观察数据来寻找解决方案，这些数据是输入的主要来源。这种技术的成功取决于神经网络是如何训练的（即观察数据的质量）。深度学习指的是学习先前提到的神经网络的方法。
- en: The advancement in technology has taken these techniques to new heights where
    these techniques demonstrate superior performance, and are used to solve some
    key non-trivial requirements in computer vision, speech recognition, and **Natural
    Language Processing** (**NLP**). Large companies such as Facebook and Google,
    among many others, have adopted deep learning practices on a substantial basis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的进步将这些技术提升到了新的高度，在这些技术中，它们展示了卓越的性能，并被用于解决计算机视觉、语音识别和**自然语言处理**（**NLP**）中的一些关键非平凡需求。Facebook和Google等大型公司以及其他许多公司已经在很大程度上采用了深度学习实践。
- en: The primary aim of this chapter is to enforce mastering the neural networks
    and related deep learning techniques conceptually. With the aid of a complex pattern
    recognition problem, this chapter covers the procedure to develop a typical neural
    network, which you will be able to use to solve a problem of a similar complexity.
    The following representation shows all the learning methods covered in this book,
    highlighting the primary subject of learning in this chapter—*Deep learning*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目的是在概念上掌握神经网络和相关深度学习技术。借助一个复杂的模式识别问题，本章涵盖了开发典型神经网络的步骤，这将使你能够使用它来解决类似复杂性的问题。以下表示展示了本书涵盖的所有学习方法，突出了本章学习的主要主题——*深度学习*。
- en: '![Deep learning](img/B03980_11_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习](img/B03980_11_01.jpg)'
- en: 'The chapter covers the following topics in depth:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了以下主题：
- en: A quick revisit of the purpose of Machine learning, types of learning, and the
    context of deep learning with details on a particular problem that it solves.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速回顾机器学习的目的、学习类型以及深度学习的背景，特别是它解决的一个特定问题。
- en: 'An overview of neural networks:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的概述：
- en: Human brain as the primary inspiration for neural networks
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类大脑作为神经网络的主要灵感来源
- en: The types of neural network architectures and some basic models of neurons
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构的类型和一些神经元的基本模型
- en: A simple learning example (digit recognition)
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的学习示例（数字识别）
- en: An overview of perceptrons, the first generation of neural networks and what
    they are capable of doing and what they are not capable of doing
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器概述，神经网络的第一代及其所能做到的和不能做到的事情
- en: An overview of linear and logistic output neurons. An introduction to back the
    propagation algorithm and applying the derivatives of back propagation algorithm
    for solving some real-world problems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性和逻辑输出神经元的概述。介绍反向传播算法以及应用反向传播算法的导数来解决一些实际问题
- en: The concepts of cognitive science, the softmax output function, and handling
    multi-output scenarios
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认知科学的概念、softmax输出函数以及处理多输出场景
- en: Applying convolution nets and the problem of object or digit recognition
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用卷积网络和对象或数字识别的问题
- en: '**Recurrent neural networks** (**RNN**) and Gradient descent method'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）和梯度下降法'
- en: Signal processing as the principle of component analysis and autoencoders; the
    types of autoencoders which are deep and shallow autoencoders
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号处理作为成分分析和自编码器的原理；深度和浅度自编码器的类型
- en: A hands-on implementation of exercises using Apache Mahout, R, Julia, Python
    (scikit-learn), and Apache Spark
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Mahout、R、Julia、Python（scikit-learn）和Apache Spark进行练习的动手实现
- en: Background
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Let's first recap the premise of Machine learning and reinforce the purpose
    and context of learning methods. As we learned, Machine learning is about training
    machines by building models using observational data, against directly writing
    specific instructions that define the model for the data to address a particular
    classification or a prediction problem. The word *model* is nothing but a *system*
    in this context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回顾一下机器学习的先决条件，并加强学习方法的宗旨和背景。正如我们所学的，机器学习是通过构建模型来训练机器，使用观察数据，而不是直接编写定义数据模型的特定指令，以解决特定的分类或预测问题。在这个背景下，“模型”这个词不过是一个“系统”。
- en: The program or system is built using data and hence, looks as though it's very
    different from a hand-written one. If the data changes, the program also adapts
    to it for the next level of training on the new data. So all it needs is the ability
    to process large-scale as opposed to getting a skilled programmer to write for
    all the conditions that could still prove to be heavily erroneous.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 程序或系统是使用数据构建的，因此看起来与手写的程序非常不同。如果数据发生变化，程序也会适应它以进行新数据的下一级训练。所以它所需要的只是处理大规模数据的能力，而不是让熟练的程序员为所有可能仍然证明是严重错误的条件编写代码。
- en: We have an example of a Machine learning system called spam detector. The primary
    purpose of this system is to identify which mail is spam and which is not. In
    this case, the spam detector is not coded to handle every type of mail; instead,
    it learns from the data. Hence, it is always true that the precision of these
    models depends on how good the observational data is. In other words, the features
    extracted from the raw data should typically cover all the states of data for
    the model to be accurate. Feature extractors are built to extract standard features
    from the given sample of data that the classifier or a predictor uses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个机器学习系统的例子，称为垃圾邮件检测器。这个系统的首要目的是识别哪些邮件是垃圾邮件，哪些不是。在这种情况下，垃圾邮件检测器并没有被编码来处理所有类型的邮件；相反，它是从数据中学习的。因此，这些模型的精确度始终取决于观察数据的好坏。换句话说，从原始数据中提取的特征通常应该覆盖数据模型的所有状态，以便模型准确。特征提取器被构建出来，用于从给定的数据样本中提取标准特征，这些特征被分类器或预测器使用。
- en: Some more examples include recognizing patterns such as speech recognition,
    object recognition, face detection, and more.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他例子包括识别模式，如语音识别、物体识别、人脸检测等。
- en: Deep learning is a type of Machine learning that attempts to learn prominent
    features from the given data, and thus tries to reduce the task of building a
    feature extractor for every category of data (for example, image, voice, and so
    on.). For a face detection requirement, a deep learning algorithm records or learns
    features such as the length of the nose, the distance between the eyes, the color
    of the eyeballs, and so on. This data is used to address a classification or a
    prediction problem and is evidently very different from the traditional **shallow
    learning algorithm**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种机器学习方法，它试图从给定数据中学习显著特征，因此试图减少为每个数据类别（例如，图像、声音等）构建特征提取器的任务。对于人脸检测的需求，深度学习算法记录或学习特征，如鼻子的长度、眼睛之间的距离、眼球的颜色等。这些数据被用来解决分类或预测问题，显然与传统的**浅层学习算法**非常不同。
- en: The human brain
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类大脑
- en: The human brain is known to be one of the most implausible organs in the human
    body. The brain is essentially what makes us, humans, intelligent. It is responsible
    for building our perceptions based on what we experience regarding our senses
    of touch, smell, sight, vision, and sound. These experiences are collected and
    stored as memories and emotions. Inherently, the brain is what makes us intelligent
    without which, we probably are just primitive organisms in the world.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑被认为是人体中最不可思议的器官之一。大脑本质上是我们人类智能的源泉。它负责根据我们关于触觉、嗅觉、视觉、听觉等方面的体验来构建我们的感知。这些体验被收集并存储为记忆和情感。本质上，大脑是我们智能的来源，没有它，我们可能只是世界上的原始生物。
- en: The brain of a newborn infant is capable of solving problems that any complex
    and powerful machine cannot solve. In fact, just within a few days of birth, the
    baby starts recognizing the face and voice of his/her parents and starts showing
    the expressions of longing to see them when they are not around. Over a period,
    they begin associating sounds with objects and can even recognize an object given
    a sight. Now, how do they do this? If they come across a dog, how do they recognize
    it to be a dog; also, do they associate a barking sound with it and mimic the
    same sound?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 新生儿的脑部能够解决任何复杂且强大的机器都无法解决的问题。实际上，就在出生后的几天内，婴儿就开始识别他/她父母的脸和声音，并在他们不在时表现出渴望见到他们的表情。在一段时间内，他们开始将声音与物体联系起来，甚至可以在看到物体时识别它。那么他们是如何做到这一点的呢？如果他们遇到一只狗，他们是如何识别它为狗的；他们是否将吠声与之联系起来并模仿同样的声音？
- en: It is simple. Every time the infant comes across a dog, his/her parents qualify
    it to be a dog, and this reinforces the child's model. In case they qualify the
    child to be wrong, the child's model would incorporate this information. So, a
    dog has long ears, long nose, four legs, a long tail, and can be of different
    colors such as black, white or brown, making a barking sound. These characteristics
    are recognized through sight and sound that an infant's brain records. The observational
    data thus collected drives the recognition of any new object henceforth.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它很简单。每次婴儿遇到一只狗时，他/她的父母都会将其定义为狗，这加强了孩子的模型。如果他们判定孩子是错误的，孩子的模型就会吸收这些信息。因此，狗有长长的耳朵、长长的鼻子、四条腿、长长的尾巴，可以是黑色、白色或棕色等不同颜色，并发出吠声。这些特征是通过婴儿大脑记录的视觉和声音来识别的。因此收集到的观察数据驱动了此后对新物体的识别。
- en: Now, let's say the infant sees a wolf for the first time; he/she would identify
    a wolf to be a dog by looking at the similarity of its characteristics s. Now,
    if the parent feeds in the definite differences on the first sighting, for example,
    a difference in the sound that it makes, then it becomes a new experience and
    is stored in memory, which is applied to the next sighting. With the assimilation
    of more and more such examples, the child's model becomes more and more accurate;
    this process is very subconscious.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设婴儿第一次看到狼；他/她会通过观察其特征相似性将狼识别为狗。现在，如果父母在第一次看到时就指出明显的差异，例如，狼发出的声音不同，那么这将成为一种新的体验，并存储在记忆中，用于下一次的观察。随着越来越多此类例子的同化，孩子的模型变得越来越准确；这个过程是非常无意识的。
- en: For several years, we have been working toward building machines that can be
    intelligent with brains as those of humans. We are talking about robots that can
    behave as humans do and can perform a particular job with similar efficiency to
    humans beings, such as driving a car, cleaning a house, and so on. Now, what does
    it take to build machines as robots? We probably need to build some super-complex
    computational systems that solve the problems our brain can solve in no time.
    This field that works on building artificially intelligent systems is called deep
    learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，我们一直在努力构建具有人类大脑智能的机器。我们谈论的是可以像人类一样行为并能够以类似人类的效率完成特定工作的机器人，例如开车、打扫房子等等。现在，要构建这样的机器人需要什么？我们可能需要构建一些超级复杂的计算系统，这些系统能够迅速解决我们大脑可以解决的问题。这个致力于构建人工智能系统的领域被称为深度学习。
- en: 'Following are some formal definitions of deep learning:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些深度学习的正式定义：
- en: According to Wikipedia, Deep learning is a set of algorithms for machine learning
    that attempts to model high-level abstractions in data by using model architectures
    composed of multiple non-linear transformations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维基百科，深度学习是一组机器学习算法，它试图通过使用由多个非线性变换组成的模型架构来模拟数据中的高级抽象。
- en: According to [http://deeplearning.net/](http://deeplearning.net/), Deep learning
    is the new area of Machine learning research that has been introduced with the
    objective of moving Machine learning closer to one of its original goals—Artificial
    Intelligence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[http://deeplearning.net/](http://deeplearning.net/)，深度学习是机器学习研究的新领域，其目标是使机器学习更接近其最初目标之一——人工智能。
- en: 'This subject has evolved over several years; the following table lists research
    areas across the years:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题已经演变了数年；以下表格列出了这些年的研究领域：
- en: '| Research Area | Year |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 研究领域 | 年份 |'
- en: '| --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Neural networks | 1960 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | 1960 |'
- en: '| Multilayer Perceptrons | 1985 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 | 1985 |'
- en: '| Restricted Boltzmann Machine | 1986 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 限制性玻尔兹曼机 | 1986 |'
- en: '| Support Vector Machine | 1995 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 | 1995 |'
- en: '| Hinton presents the **Deep Belief Network** (**DBN**)New interests in deep
    learning and RBMState of the art MNIST | 2005 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 辛顿提出**深度信念网络**（**DBN**）对深度学习和RBM的新兴趣，以及最先进的MNIST | 2005 |'
- en: '| Deep Recurrent Neural Network | 2009 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 深度循环神经网络 | 2009 |'
- en: '| Convolutional DBN | 2010 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 卷积深度信念网络 | 2010 |'
- en: '| Max-Pooling CDBN | 2011 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 最大池化卷积深度信念网络 | 2011 |'
- en: Among many others, some key contributors to this field are Geoffrey Hinton,
    Yann LeCun, Honglak Lee, Andrew Y. Ng, and Yoshua Bengio.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多贡献者中，这个领域的几位关键贡献者包括杰弗里·辛顿（Geoffrey Hinton）、杨立昆（Yann LeCun）、洪乐克·李（Honglak
    Lee）、安德鲁·杨（Andrew Y. Ng）和约书亚·本吉奥（Yoshua Bengio）。
- en: 'The following concept model covers different areas of Deep learning and the
    scope of topics covered in this chapter:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的概念模型涵盖了深度学习的不同领域，以及本章涵盖的主题范围：
- en: '![The human brain](img/B03980_11_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![人类大脑](img/B03980_11_02.jpg)'
- en: 'Let''s look at a simple problem on hand; the requirement is to recognize the
    digits from the handwritten script given here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的问题；要求是识别这里给出的手写脚本中的数字：
- en: '![The human brain](img/B03980_11_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![人类大脑](img/B03980_11_03.jpg)'
- en: For a human brain, this is very simple as we can recognize the digits as 287635\.
    The simplicity with which our brain interprets the digits is perceptive that it
    undermines the complexity involved in this process. Our brain is trained to intercept
    different visuals progressively due to the presence of visual cortices, with each
    cortex containing more than 140 million neurons that have billions of connections
    between them. In short, our brain is no less than a supercomputer that has evolved
    over several millions of years and is known to adapt well to the visual world.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类大脑来说，这非常简单，因为我们能将数字识别为287635。我们大脑解释数字的简单性是感知的，它削弱了这一过程中涉及的复杂性。由于视觉皮层的存在，我们的大脑被训练成逐步拦截不同的视觉，每个皮层包含超过1.4亿个神经元，它们之间有数十亿个连接。简而言之，我们的大脑不亚于一个经过数百万年演化的超级计算机，并且已知能够很好地适应视觉世界。
- en: If a computer program has to crack the recognition of the digits, what should
    be the rules to identify and differentiate a digit from another?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个计算机程序需要破解数字识别，应该遵循什么规则来识别和区分一个数字与另一个数字？
- en: Neural networks are one such field being researched for several years and is
    known to address the need for multilayered learning. The overall idea is to feed
    a large number of handwritten digits; an example of this data (training) is shown
    in the following image, and that can learn from these examples. This means the
    rules are automatically inferred from the provided training data. So, the larger
    the training dataset, the more accurate would be the prediction. If we are posed
    with a problem to differentiate the digit 1 from the digit 7 or the digit 6 from
    the digit 0, some minor differences will need to be learned. For a zero, the distance
    between the starting and ending point is minimal or nothing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是多年来一直在研究的一个领域，并且已知可以解决多层学习的需求。整体的想法是输入大量手写的数字；以下图像展示了这些数据（训练）的一个例子，并且可以从这些例子中学习。这意味着规则是从提供的训练数据中自动推断出来的。因此，训练数据集越大，预测的准确性就越高。如果我们面临一个问题，需要区分数字1和数字7，或者数字6和数字0，就需要学习一些细微的差异。对于一个零，起点和终点之间的距离是最小或没有。
- en: '![The human brain](img/B03980_11_04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![人类大脑](img/B03980_11_04.jpg)'
- en: The difference is basically because these learning methods have been targeted
    to mimic a human brain. Let's see what makes this a difficult problem to solve.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 差异基本上是因为这些学习方法的目标是模仿人类大脑。让我们看看是什么使得这个问题难以解决。
- en: In summary, with deep learning being a subset of Machine learning, we know that
    this involves the technique of feeding examples and a model that can evaluate
    the pattern to evolve it in case it makes a mistake. Thus, over a period of time,
    this model would solve the problem with the best possible accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，深度学习作为机器学习的一个子集，我们知道这涉及到提供示例和评估模式以在出错时进行演化的模型。因此，在一段时间内，这个模型将以最佳可能的准确性解决问题。
- en: If this needs to be represented mathematically, let's define our model to be
    a function *f(x,θ)*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这需要用数学来表示，让我们定义我们的模型为一个函数 *f(x,θ)*。
- en: Here, *x* is the input that is provided as a vector of values and *θ* is a reference
    vector that the model uses to predict or classify *x*. So, it is *θ* that we need
    to expose to a maximum set of examples in order to improve the accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是作为值向量提供的输入，而 *θ* 是模型用来预测或分类 *x* 的参考向量。因此，我们需要将 *θ* 暴露给最大数量的示例，以提高准确性。
- en: 'Let''s take an example; if we were to predict whether a visitor to a restaurant
    would come back based on two factors—one is the amount of bill (*x[1]*) and the
    other is his/her age(*x[2]*). When we collect data for a specific duration of
    time and analyze it for an output value that can be 1(in case the visitor came
    back) or -1(if the visitor has not come back). The data, when plotted, can take
    any form—from a linear relationship or any other complex form, as shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子；如果我们想根据两个因素预测一家餐厅的访客是否会再次光临——一个是账单金额（*x[1]*），另一个是访客的年龄（*x[2]*）。当我们收集一段时间内的数据并分析其输出值时，这个值可以是1（如果访客回来了）或-1（如果访客没有回来）。当数据被绘制出来时，可以呈现任何形式——从线性关系到任何其他复杂形式，如图所示：
- en: '![The human brain](img/B03980_11_05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![人类大脑](img/B03980_11_05.jpg)'
- en: Something like a linear relationship looks straight forward and more complex
    relationships complicate the dynamics of the model. Can parameter *θ* have an
    optimal value at all? We might have to apply optimization techniques and in the
    next sections to follow, we will cover these techniques such as perceptrons and
    gradient descent methods among others. If we want to develop a program to do this,
    we need to know what our brain does to recognize these digits, and even if we
    knew, these programs might be very complex in nature.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像线性关系的东西看起来很简单，而更复杂的关系则使模型动态复杂化。参数 *θ* 是否真的有一个最优值？我们可能需要应用优化技术，在接下来的章节中，我们将介绍这些技术，例如感知器和梯度下降法等。如果我们想开发一个执行此操作的程序，我们需要知道大脑是如何识别这些数字的，即使我们知道了，这些程序也可能非常复杂。
- en: Neural networks
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Neural computations have been a primary interest of the study to understand
    how parallel computations work in neurons (the concept of flexible connections)
    and solve practical problems like a human brain does. Let''s now look at the core
    fundamental unit of the human brain, the *neuron*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算一直是研究的主要兴趣，旨在了解神经元（灵活连接的概念）中的并行计算是如何工作的，以及如何像人类大脑一样解决实际问题。现在让我们看看人类大脑的核心基本单元——*神经元*：
- en: '![Neural networks](img/B03980_11_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/B03980_11_06.jpg)'
- en: Neuron
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元
- en: The human brain is all about neurons and connections. A neuron is the smallest
    part of the brain, and if we take a small rice grain sized piece of the brain,
    it is known to contain at least 10000 neurons. Every neuron on an average has
    around 6000 connections with other neurons. If we look at the general structure
    of a neuron, it looks as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑就是由神经元和连接组成的。神经元是大脑的最小部分，如果我们取一小块大脑，其大小与一小粒米饭粒相当，已知至少包含 10000 个神经元。平均每个神经元与其他神经元有大约
    6000 个连接。如果我们观察神经元的总体结构，它看起来如下。
- en: 'Every feeling that we humans go through, be it thought or emotion, is because
    of these millions of cells in our brain called neurons. As a result of these neurons
    communicating with each other by passing messages, humans feel, act, and form
    perceptions. The diagram here depicts the biological neural structure and its
    parts:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类经历的所有感觉，无论是思考还是情感，都是因为这些被称为神经元的数百万个细胞在大脑中的活动。这些神经元通过传递信息相互沟通，使人类能够感受、行动和形成感知。此处所示的图解描绘了生物神经结构和其组成部分：
- en: '![Neuron](img/B03980_11_07.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![神经元](img/B03980_11_07.jpg)'
- en: Every neuron has a central cell body; as any cell, in general, it has an axon
    and a dendritic tree that are responsible for sending and receiving messages respectively
    with other neurons. The place where axons connect to the dendritic tree is called
    a synapse. The synapses themselves have an interesting structure. They contain
    transmitter molecules that trigger transmission, which can either be positive
    or negative in nature.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元都有一个中央细胞体；像任何细胞一样，它有一个负责与其他神经元发送和接收信息的轴突和树突。轴突连接到树突的地方称为突触。突触本身有一个有趣的结构。它们包含触发传递的传递分子，这种传递可以是正的或负的。
- en: The inputs to the neurons are aggregated, and when they exceed the threshold,
    an electrical spike is transmitted to the next neuron.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的输入被汇总，当它们超过阈值时，就会向下一个神经元传递一个电脉冲。
- en: Synapses
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 突触
- en: The following diagram depicts the model of a synapse depicting the flow of messages
    from axon to dendrite. The job of the synapse is not just the transmission of
    messages, but in fact, adapt themselves to the flow of signals and have the ability
    to learn from past activities.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了突触模型，描述了从轴突到树突的消息流动。突触的工作不仅仅是传输消息，实际上，它还会根据信号流动进行适应，并具有从过去活动中学习的能力。
- en: '![Synapses](img/B03980_11_08.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![突触](img/B03980_11_08.jpg)'
- en: As an analogy in the field of Machine learning, the strength of the incoming
    connection is determined on the basis of how often it is used, and thus its impact
    on the neuron output is determined. This is how new concepts are learned by humans
    subconsciously.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习领域的类比，输入连接的强度是根据其被使用的频率来确定的，因此其影响神经元输出的程度也随之确定。这就是人类潜意识中学习新概念的方式。
- en: There can additionally be external factors such as medication or body chemistry
    that might impact this learning process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可能有外部因素，如药物或身体化学成分，可能会影响这一学习过程。
- en: 'Now we will finally summarize how the learning happens inside the brain with
    the help of the following list:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将最终通过以下列表总结大脑内部的学习过程：
- en: Neurons communicate with other neurons or sometimes receptors. The cortical
    neurons use spikes for communication.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元与其他神经元或有时受体进行通信。皮层神经元使用尖峰进行通信。
- en: The strengths of connections between neurons can change. They can take positive
    or negative values by either establishing and removing connections between neurons
    or by strengthening the connection based on the influence that a neuron can have
    over the other. A process called **long-term potentiation** (**LTP**) occurs that
    results in this long-term impact.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元之间的连接强度可以改变。它们可以通过建立和移除神经元之间的连接，或者根据神经元对其他神经元可能产生的影响来加强连接，从而取正值或负值。一个称为**长期增强**（**LTP**）的过程发生，从而产生这种长期影响。
- en: There are about 1011 neurons having weights that make the computations that
    the human brain can do more efficiently than a workstation.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大约有10^11个神经元，它们的权重使得人类大脑的计算比工作站更有效率。
- en: Finally, the brain is modular; different parts of the cortex are responsible
    for doing different things. Some tasks infuse more blood flow in some regions
    over the other and thus, ensuring different results.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，大脑是模块化的；大脑皮层的不同部分负责做不同的事情。某些任务在某个区域比其他区域注入更多的血流，从而确保不同的结果。
- en: Before schematizing the neuron model into the **artificial neural network**
    (**ANN**), let us first look at different types, categories, or aspects of neurons,
    and in specific the Artificial neuron or Perceptron, the deep learning equivalent
    of a biological neuron. This approach is known to have produced extremely efficient
    results in some of the use cases we listed in the previous section. ANNs are also
    called feed-forward neural networks, **Multi-Layer Perceptrons** (**MLP**), and,
    recently, deep networks or learning. One of the important characteristics has
    been the need for feature engineering, whereas deep learning represents applications
    that require minimum feature engineering, where learning happens through multiple
    learned layers of neurons.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在将神经元模型 schematizing 到**人工神经网络**（**ANN**）之前，让我们首先看看神经元的不同类型、类别或方面，特别是人工神经元或感知器，这是生物神经元的深度学习等效物。这种方法已知在我们在上一节中列出的某些用例中产生了极其高效的结果。ANN
    也被称为前馈神经网络、**多层感知器**（**MLP**），以及最近，深度网络或学习。其中一个重要特征是需要特征工程，而深度学习则代表需要最少特征工程的应用，学习通过多个学习的神经元层发生。
- en: Artificial neurons or perceptrons
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经元或感知器
- en: 'It is obvious that artificial neurons draw inspiration from biological neurons,
    as represented previously. The features of an artificial neuron are listed here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，人工神经元是从生物神经元那里获得灵感的，正如之前所展示的那样。人工神经元的特点如下：
- en: There is a set of inputs received from other neurons that activate the neuron
    in context
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从其他神经元接收一组输入，这些输入激活了神经元
- en: There is an output transmitter that transfers signals or an activation of the
    other neurons
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个输出传输器，用于传输信号或激活其他神经元
- en: Finally, the core processing unit is responsible for producing output activations
    from the input activations
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，核心处理单元负责从输入激活中产生输出激活
- en: Idealizing for a neuron is a process that is applied to building models. In
    short, it is a simplification process. Once simplified, it is possible to apply
    mathematics and relate analogies. To this case, we can easily add complexities
    and make the model robust under identified conditions. Necessary care needs to
    be taken in ensuring that none of the significantly contributing aspects are removed
    as a part of the simplification process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 理想化神经元是一个应用于构建模型的过程。简而言之，它是一个简化过程。一旦简化，就可以应用数学和建立类比。在这种情况下，我们可以轻松地增加复杂性，并在已识别的条件下使模型更加健壮。在简化过程中，需要特别注意不要删除任何显著贡献的方面。
- en: Linear neurons
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性神经元
- en: 'Linear neurons are the simplest form of neurons; they can be represented as
    follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 线性神经元是神经元的最简单形式；它们可以表示如下：
- en: '![Linear neurons](img/B03980_11_10.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![线性神经元](img/B03980_11_10.jpg)'
- en: 'The output *y* is a summation of the product of the input *x[i]* and its weight
    *w[i]*. This is mathematically represented as shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 *y* 是输入 *x[i]* 和其权重 *w[i]* 的乘积之和。这可以用以下方式在数学上表示：
- en: '![Linear neurons](img/B03980_11_41.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![线性神经元](img/B03980_11_41.jpg)'
- en: Here, *b* is the bias.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*b* 是偏置。
- en: 'A graph representation of the previous equation is given as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 之前方程的图形表示如下：
- en: '![Linear neurons](img/B03980_11_11.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![线性神经元](img/B03980_11_11.jpg)'
- en: Rectified linear neurons / linear threshold neurons
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩形线性神经元/线性阈值神经元
- en: 'Rectified linear neurons are similar to linear neurons, as explained in the
    preceding section, with a minor difference where the output parameter value is
    set to zero in cases where it is less than (<) zero (0), and in case the output
    value is greater than (>) zero (0), it continues to remain as the linear weighted
    sum of the inputs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性神经元与线性神经元类似，如前所述，只是在输出参数值小于（<）零（0）时将其设置为零，而在输出值大于（>）零（0）时，它继续作为输入的线性加权和：
- en: '![Rectified linear neurons / linear threshold neurons](img/B03980_11_12.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![矩形线性神经元/线性阈值神经元](img/B03980_11_12.jpg)'
- en: Binary threshold neurons
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二元阈值神经元
- en: The binary threshold neurons were introduced by McCulloch and Pitts in 1943\.
    This class of neurons first have the weighted sum of the inputs computed, similar
    to the linear neurons. If this value exceeds a defined threshold, a fixed size
    spike to activity is sent out. This spike is called as the *truth value of a proposition*.
    Another important point is the output. The output at any given point in time is
    binary (0 or 1).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 二元阈值神经元由McCulloch和Pitts于1943年引入。这类神经元首先计算输入的加权总和，类似于线性神经元。如果这个值超过一个定义的阈值，就会发送一个固定大小的脉冲到活动状态。这个脉冲被称为命题的*真值*。另一个重要点是输出。在任何给定时间点的输出是二进制的（0或1）。
- en: 'The equation that demonstrates this behavior is given here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 展示这种行为的方程如下：
- en: '![Binary threshold neurons](img/B03980_11_42.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![二元阈值神经元](img/B03980_11_42.jpg)'
- en: And
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: '*y = 1* if *z ≥ θ*,'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 1* 如果 *z ≥ θ*，'
- en: '*y = 0* otherwise'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 0* 否则'
- en: here *θ = -b (bias)*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *θ = -b (偏置)*
- en: (OR)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: （或者）
- en: '![Binary threshold neurons](img/B03980_11_43.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![二元阈值神经元](img/B03980_11_43.jpg)'
- en: And
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: '*y = 1* if *z ≥ 0*,'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 1* 如果 *z ≥ 0*，'
- en: '*y = 0* otherwise'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 0* 否则'
- en: 'Moreover, a graphical representation of the previous equation is given here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，之前方程的图形表示如下：
- en: '![Binary threshold neurons](img/B03980_11_13.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![二元阈值神经元](img/B03980_11_13.jpg)'
- en: Sigmoid neurons
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid神经元
- en: Sigmoid neurons are highly adopted in artificial neural networks. These neurons
    are known to provide the output that is smooth, real-valued, and therefore a bounded
    function of all the inputs. Unlike the types of the neurons that we have seen
    until now, these neurons use the logistic function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid神经元在人工神经网络中得到了广泛的应用。这些神经元因其提供平滑、实值输出而闻名，因此是所有输入的有界函数。与迄今为止我们所看到的神经元类型不同，这些神经元使用对数函数。
- en: 'The logistic function is known to have an easy-to-calculate derivative that
    makes learning easy. This derivative value is used in computing the weights. Following
    is the equation for the sigmoid neuron output:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对数函数因其易于计算的导数而闻名，这使得学习变得容易。这个导数值用于计算权重。以下是sigmoid神经元的输出方程：
- en: '![Sigmoid neurons](img/B03980_11_44.jpg)![Sigmoid neurons](img/B03980_11_45.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid神经元](img/B03980_11_44.jpg)![Sigmoid神经元](img/B03980_11_45.jpg)'
- en: 'The diagrammatic/graphical representation is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图形表示如下：
- en: '![Sigmoid neurons](img/B03980_11_14.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid神经元](img/B03980_11_14.jpg)'
- en: Stochastic binary neurons
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机二元神经元
- en: 'Stochastic binary neurons use the same equation as logistic units, with one
    important difference that the output is measured for a probabilistic value, which
    measures the probability of producing a spike in a short window of time. So, the
    equation looks like this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随机二进制神经元使用与逻辑单元相同的方程，但有一个重要区别，即输出测量的是概率值，该值衡量在短时间内产生尖峰的概率。因此，方程看起来是这样的：
- en: '![Stochastic binary neurons](img/B03980_11_46.jpg)![Stochastic binary neurons](img/B03980_11_47.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![随机二进制神经元](img/B03980_11_46.jpg)![随机二进制神经元](img/B03980_11_47.jpg)'
- en: 'Moreover, the graphical representation of this equation is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该方程的图形表示如下：
- en: '![Stochastic binary neurons](img/B03980_11_15.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![随机二进制神经元](img/B03980_11_15.jpg)'
- en: 'Overall, what we can observe is that each neuron takes in a weighted sum of
    a bunch of inputs on which a non-linear activation function is applied. Rectified
    linear function is typically applied for solving regression problems and for classification
    problems, logistic functions are applied. A generic representation of this can
    be given as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们可以观察到每个神经元接受了一组输入的加权和，并应用了非线性激活函数。对于解决回归问题，通常应用修正线性函数；对于分类问题，应用逻辑函数。这种通用的表示可以如下给出：
- en: '![Stochastic binary neurons](img/B03980_11_16.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![随机二进制神经元](img/B03980_11_16.jpg)'
- en: 'Now, these inputs can be fed into a series of layers of neurons. Let''s look
    at what happens next and how this happens. The input layer pushes input values;
    the hidden layers of neurons then take the values as input. It is possible to
    have multiple layers within these hidden layers, where the output from one layer
    feeds as the input to the subsequent layer. Each of these layers can be responsible
    for the specialized learning. Moreover, finally, the last in the hidden layer
    feeds into the final output layer. This typical structure of an ANN is illustrated
    next. Every circle in the next diagram represents a neuron. The concept of the
    **Credit Assignment Path** (**CAP**) refers to the path from input to output.
    In the feed-forward networks, the length of the path is the total number of hidden
    layers along with the output layer. The following diagram shows a feed-forward
    neural network with a single hidden layer and connections between layers:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些输入可以被输入到一系列的神经元层中。让我们看看接下来会发生什么以及它是如何发生的。输入层推动输入值；然后神经元隐藏层将这些值作为输入。在这些隐藏层中可能有多个层，其中一层输出作为下一层的输入。每一层都可以负责专门的学习。此外，最后，隐藏层的最后一个输入到最终输出层。以下图展示了人工神经网络（ANN）的典型结构。下一个图中的每个圆圈代表一个神经元。**信用分配路径**（**CAP**）的概念指的是从输入到输出的路径。在前馈网络中，路径的长度是总隐藏层数量加上输出层。以下图显示了一个具有单个隐藏层和层间连接的前馈神经网络：
- en: '![Stochastic binary neurons](img/B03980_11_17.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![随机二进制神经元](img/B03980_11_17.jpg)'
- en: 'The case of two hidden layers are shown in here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了具有两个隐藏层的案例：
- en: '![Stochastic binary neurons](img/B03980_11_18.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![随机二进制神经元](img/B03980_11_18.jpg)'
- en: Neural Network size
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络大小
- en: 'Computing the number of neurons or parameters is shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 计算神经元或参数的数量如下所示：
- en: 'For the single layer network:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于单层网络：
- en: Total number of neurons = 4 + 2 = 6 (inputs are not counted)
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经元总数 = 4 + 2 = 6（输入不计入）
- en: Total weights = [3 x 4] + [4 x 2] = 20
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总权重 = [3 x 4] + [4 x 2] = 20
- en: Total bias = 4 + 2 = 6, for 26 learnable parameters.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总偏差 = 4 + 2 = 6，对于26个可学习的参数。
- en: 'For the two layer network:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于双层网络：
- en: Total number of neurons = 4 + 4 + 1 = 9 (inputs are not counted)
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经元总数 = 4 + 4 + 1 = 9（输入不计入）
- en: Total weights = [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总权重 = [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32
- en: Total bias = 4 + 4 + 1 = 9 for 41 learnable parameters
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总偏差 = 4 + 4 + 1 = 9，对于41个可学习的参数
- en: So, what is the optimal size of neural networks? It is important to identify
    the possible number of hidden layers along with the size of each layer. These
    decisions determine the capacity of the network. A higher value helps to support
    a higher capacity.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，神经网络的理想大小是什么？识别每个层的可能大小以及隐藏层的数量非常重要。这些决策决定了网络的能力。更高的值有助于支持更高的能力。
- en: 'Let''s take an example where we will try three different sizes of the hidden
    layer by obtaining the following classifiers:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子，我们将通过获取以下分类器来尝试三种不同大小的隐藏层：
- en: '![Neural Network size](img/B03980_11_19.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络大小](img/B03980_11_19.jpg)'
- en: Clearly, with more neurons, functions with higher complexity can be expressed,
    which is good, but we need to watch out for the over-fitting case. So, a smaller-sized
    network works well for simpler data. With the increasing data complexity, the
    need for a bigger size arises. The trade-off is always between handling the complexity
    of the model versus. over-fitting. Deep learning addresses this problem as it
    applies complex models to extremely complex problems and handles over-fitting
    by taking additional measures.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，随着神经元数量的增加，可以表达更复杂的函数，这是好事，但我们需要注意过拟合的情况。因此，对于更简单的数据，较小的网络效果较好。随着数据复杂性的增加，需要更大的网络。这种权衡总是在处理模型的复杂性与过拟合之间。深度学习通过应用复杂模型来解决极复杂问题，并通过采取额外措施来处理过拟合。
- en: An example
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个例子
- en: 'A face recognition case using the multi-layered perceptron approach is shown
    next:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了使用多层感知器方法进行的人脸识别案例：
- en: '![An example](img/B03980_11_20.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![一个例子](img/B03980_11_20.jpg)'
- en: Multiple layers take this image as input and finally, a classifier definition
    is created and stored.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 多层将此图像作为输入，最终创建并存储分类器定义。
- en: '![An example](img/B03980_11_21.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![一个例子](img/B03980_11_21.jpg)'
- en: Given a photograph, each layer focuses on learning a specific part of the photograph
    and finally stores the output pixels.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张照片，每一层专注于学习照片的特定部分，最终存储输出像素。
- en: 'Some key notes on the weights and error measures are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 关于权重和误差度量的一些关键点如下：
- en: The training data is the source of learning the weights of neurons
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据是学习神经元权重的来源
- en: The error measure or the cost function is different from the regression and
    classification problems. For classification, log functions are applied, and for
    regression, least square measures are used.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误度量或损失函数与回归和分类问题不同。对于分类，应用对数函数，而对于回归，使用最小二乘度量。
- en: These methods help to keep these error measures in check by updating the weights
    using convex optimization techniques such as decent gradient methods
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法通过使用凸优化技术（如梯度下降法）更新权重来帮助控制这些误差度量。
- en: Neural network types
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络类型
- en: 'In this section, we will cover some key types of neural networks. The following
    concept map lists a few principal types of neural networks:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些关键类型的神经网络。以下概念图列出了几种主要的神经网络类型：
- en: '![Neural network types](img/B03980_11_22.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络类型](img/B03980_11_22.jpg)'
- en: Multilayer fully connected feedforward networks or Multilayer Perceptrons (MLP)
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层全连接前馈网络或多层感知器（MLP）
- en: 'As covered in the introductory sections about neural networks, an MLP has multiple
    layers where the output of one layer feeds as an input to a subsequent layer.
    A multilayer perceptron is represented as shown in the following diagram:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如神经网络简介部分所述，MLP有多个层，其中一层的输出作为下一层的输入。多层感知器如图所示：
- en: '![Multilayer fully connected feedforward networks or Multilayer Perceptrons
    (MLP)](img/B03980_11_23.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![多层全连接前馈网络或多层感知器（MLP）](img/B03980_11_23.jpg)'
- en: Jordan networks
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Jordan网络
- en: 'Jordan networks are partially recurrent networks. These networks are the current
    feedforward networks with a difference of having additional context neurons inside
    the input layer. These context neurons are self-imposed and created using the
    direct feedback from input neurons. In Jordon networks, the number of context
    neurons is always equal to the input neurons. The following diagram depicts the
    difference in the input layer:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Jordan网络是部分循环网络。这些网络是当前的前馈网络，不同之处在于输入层内部有额外的上下文神经元。这些上下文神经元是自我强制的，并使用来自输入神经元的直接反馈创建。在Jordan网络中，上下文神经元的数量始终等于输入神经元。以下图显示了输入层的差异：
- en: '![Jordan networks](img/B03980_11_48.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Jordan networks](img/B03980_11_48.jpg)'
- en: Elman networks
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Elman网络
- en: 'Elman networks, just as Jordon networks, are partially recurrent feedforward
    networks. These networks also have context neurons, but in this case, the main
    difference is that the context neurons receive the feed from the output neurons,
    and not from the hidden layers. There is no direct correlation between the number
    of context neurons and input neurons; rather, the number of context neurons is
    the same as the number of hidden neurons. This, in turn, makes this model more
    flexible, just as the number of hidden neurons do on a case-by-case basis:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Elman网络，就像Jordan网络一样，是部分递归的前馈网络。这些网络也有上下文神经元，但在这个情况下，主要区别在于上下文神经元接收来自输出神经元的输入，而不是来自隐藏层。上下文神经元的数量与输入神经元的数量之间没有直接关联；相反，上下文神经元的数量与隐藏神经元的数量相同。这反过来使得这个模型更加灵活，就像隐藏神经元的数量在具体情况下一样：
- en: '![Elman networks](img/B03980_11_49.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Elman网络](img/B03980_11_49.jpg)'
- en: Radial Bias Function (RBF) networks
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 径向偏置函数（RBF）网络
- en: Radial Bias Function networks are also feed-forward neural networks. These networks
    have a special hidden layer of special neurons called radially symmetric neurons.
    These neurons are for converting the distance value between the input vector and
    the center using a Gaussian measure. The advantage of this additional layer is
    that it gives an additional capability to determine the number of layers required
    without a manual intervention. The choice of the linear function determines the
    optimal output layer. Therefore, the learning happens relatively faster in these
    networks even in comparison to back propagation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 径向偏置函数网络也是前馈神经网络。这些网络有一个特殊的隐藏层，包含称为径向对称神经元的特殊神经元。这些神经元用于使用高斯度量将输入向量与中心之间的距离值进行转换。这个额外层的好处是它提供了确定所需层数的能力，而无需人工干预。线性函数的选择决定了最优输出层。因此，在这些网络中，学习过程相对较快，甚至与反向传播相比也是如此。
- en: The only downside of this method is its ability to handle large input vectors.
    The diagram below depicts the hidden layer of radially symmetric neurons.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的唯一缺点是处理大型输入向量的能力。下面的图示展示了径向对称神经元的隐藏层。
- en: '![Radial Bias Function (RBF) networks](img/B03980_11_50.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![径向偏置函数（RBF）网络](img/B03980_11_50.jpg)'
- en: Hopfield networks
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跳时网络
- en: 'Hopfield networks work around a concept called *energy of network*. This is
    nothing but an optimal local minima of the network that defines an equilibrium
    state for the functionality. Hopfield networks target the state of achieving this
    equilibrium state. An equilibrium state is when the output of one layer becomes
    equal to the output of the previous layer. The following diagram depicts how the
    input and output states are checked and managed in the Hopfield network:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 跳时网络围绕一个称为“网络能量”的概念进行工作。这实际上就是网络的最优局部最小值，它定义了功能性的平衡状态。跳时网络的目标是达到这个平衡状态。平衡状态是指某一层的输出等于前一层的输出。以下图示展示了如何在跳时网络中检查和管理输入和输出状态：
- en: '![Hopfield networks](img/B03980_11_51.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![跳时网络](img/B03980_11_51.jpg)'
- en: Dynamic Learning Vector Quantization (DLVQ) networks
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态学习向量量化（DLVQ）网络
- en: The **Dynamic Learning Vector Quantization** (**DLVQ**) network model is another
    variation of neural networks that starts with a smaller number hidden layers and
    dynamically generates these hidden layers. It is important to have similarities
    in patterns that belong to the same class; hence, this algorithm best suits classification
    problems, such as recognition of patterns, digits, and so on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态学习向量量化**（**DLVQ**）网络模型是神经网络的一种变体，它从较少的隐藏层开始，并动态生成这些隐藏层。对于属于同一类的模式具有相似性是很重要的；因此，这个算法最适合分类问题，如模式识别、数字识别等。'
- en: Gradient descent method
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降法
- en: 'In this section, we will look at one of the most popular ways of optimizing
    the neural network, minimizing the cost function, minimizing errors, and improving
    the accuracy of the neural network: the Gradient descent method. The graph here
    shows the actual versus. The predicted value along with the cases of inaccuracy
    in the predictions:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨优化神经网络最流行的一种方法之一，即最小化成本函数、最小化错误并提高神经网络准确性的方法：梯度下降法。这里的图表显示了实际值与预测值之间的差异，以及预测不准确的情况：
- en: '![Gradient descent method](img/B03980_11_52.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降法](img/B03980_11_52.jpg)'
- en: Backpropagation algorithm
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: Taking forward the topic of training the networks, the Gradient descent algorithm
    helps neural networks to learn the weights and biases. Moreover, to compute the
    gradient of the cost function, we use an algorithm called backpropagation. Backpropagation
    was first discussed in the 1970s and became more prominent regarding its application
    only in the 1980s. It was proven that neural network learning was much faster
    when backpropagation algorithm was employed.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论网络训练的话题时，梯度下降算法帮助神经网络学习权重和偏置。此外，为了计算损失函数的梯度，我们使用一个称为反向传播的算法。反向传播首次在20世纪70年代被讨论，直到20世纪80年代才在应用方面变得突出。已经证明，当使用反向传播算法时，神经网络学习速度更快。
- en: In the earlier sections of this chapter, we saw how a matrix-based algorithm
    works; a similar notation is used for the backpropagations algorithm. For a given
    weight *w* and bias *b*, the cost function C has two partial derivatives which
    are *∂C/∂w* and *∂C/∂b*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们看到了基于矩阵的算法是如何工作的；反向传播算法使用了类似的符号。对于一个给定的权重*w*和偏置*b*，损失函数C有两个偏导数，分别是*∂C/∂w*和*∂C/∂b*。
- en: 'Some key assumptions regarding the cost function for backpropagation are stated
    here. Let''s assume that the cost function is defined by the equation here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 关于反向传播的损失函数的一些关键假设在此处陈述。让我们假设损失函数由以下方程定义：
- en: '![Backpropagation algorithm](img/B03980_11_53.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_53.jpg)'
- en: Where, *n* = number of training examples
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*n* = 训练示例的数量
- en: '*x* = sum across the individual training sets'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* = 对单个训练集求和'
- en: '*y* = *y(x)* is the expected output'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *y(x)*是期望输出'
- en: '*L* = total number of layers in the neural network'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*L* = 神经网络中的总层数'
- en: '*a^L* = *a^L(x)* is the output activation vector'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*a^L* = *a^L(x)*是输出激活向量'
- en: 'Assumption 1: The overall cost function can be an average of the individual
    cost functions. For *x* individual training sets, the cost function can now be
    stated as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设1：整体损失函数可以是单个损失函数的平均值。对于*x*个单独的训练集，损失函数现在可以表述如下：
- en: '![Backpropagation algorithm](img/B03980_11_54.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_54.jpg)'
- en: 'Moreover, the cost function for an individual training set can be as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，单个训练集的损失函数可以如下所示：
- en: '![Backpropagation algorithm](img/B03980_11_55.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_55.jpg)'
- en: With this assumption, since we can compute the partial derivatives for each
    training set *x* as ∂[x]C/∂w and ∂[x]C/∂b, the overall partial derivative functions
    ∂C/∂w and ∂C/∂b can be an average of the partial derivatives for each training
    set.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个假设，由于我们可以计算每个训练集*x*的偏导数∂[x]C/∂w和∂[x]C/∂b，所以整体偏导数函数∂C/∂w和∂C/∂b可以是每个训练集偏导数的平均值。
- en: 'Assumption 2: This hypothesis is about the cost function *C* that *C* can be
    the function of outputs from the neural networks as shown here:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设2：这个假设是关于损失函数*C*，即*C*可以是神经网络输出的函数，如图所示：
- en: '![Backpropagation algorithm](img/B03980_11_56.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_56.jpg)'
- en: Extending the previous equation of the cost function, the quadratic cost function
    for each training example set *x* can now be written as follows. We can see how
    this acts as a function of the output activations as well.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 将损失函数的先前方程式扩展，每个训练示例集*x*的二次损失函数现在可以写成如下形式。我们可以看到它如何作为输出激活的函数。
- en: '![Backpropagation algorithm](img/B03980_11_57.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_57.jpg)'
- en: Back propagation is about the impact the weights and bias have on the overall
    cost function value.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是关于权重和偏置对整体损失函数值的影响。
- en: 'First, we compute the error in the *j^(th)* neuron in the *l^(th)* layer, *δ^l[j]*,
    and then use this value to calculate the partial derivatives that relate to this
    error *δ^l[j]*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算第*l*层的第*j*个神经元的误差*δ^l[j]*，然后使用这个值来计算与这个误差*δ^l[j]*相关的偏导数：
- en: '![Backpropagation algorithm](img/B03980_11_58.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_58.jpg)'
- en: 'The error function *δ^l[j]* of the *j^(th)* neuron in the *l^(th)* layer can
    be defined as:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第*l*层的第*j*个神经元的误差函数*δ^l[j]*可以定义为：
- en: '![Backpropagation algorithm](img/B03980_11_59.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_59.jpg)'
- en: Thus, the error for the layer *L* *δ^L* can be computed as well. This, in turn,
    helps to compute the gradient of the cost function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，层*L*的误差*δ^L*也可以计算。这反过来又帮助计算损失函数的梯度。
- en: 'The following equations are used by the back propagation algorithm in sequence,
    as shown here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法按顺序使用以下方程，如图所示：
- en: 'Equation 1: The computing error in the layer *L*, *δ^L*, given the neuron at
    the position *j*.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 1：给定位置 *j* 的神经元，层 *L* 的计算误差 *δ^L*。
- en: '![Backpropagation algorithm](img/B03980_11_60.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_60.jpg)'
- en: 'Equation 2: The computing error in the layer *L*, *δ^L*, given the error in
    the next layer *δ^(L+1)*.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2：给定下一层的误差 *δ^(L+1)*，层 *L* 的计算误差 *δ^L*。
- en: '![Backpropagation algorithm](img/B03980_11_61.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_61.jpg)'
- en: Note
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Hadamard product is a matrix multiplication technique that is used for
    element-wise matrix multiplication as shown here:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Hadamard 积是一种矩阵乘法技术，用于逐元素矩阵乘法，如下所示：
- en: '![Backpropagation algorithm](img/B03980_11_62.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_62.jpg)'
- en: The notation ![Backpropagation algorithm](img/B03980_11_70.jpg) is used to represent
    this method.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 符号 ![反向传播算法](img/B03980_11_70.jpg) 用于表示此方法。
- en: 'Equation 3: This equation measures the impact on the cost and gives a change
    in the bias:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 3：此方程衡量对成本的影响，并给出偏差的变化：
- en: '![Backpropagation algorithm](img/B03980_11_34.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_34.jpg)'
- en: 'Moreover, we will get the following from equations 1 and 2:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以从方程式 1 和 2 得到以下内容：
- en: '![Backpropagation algorithm](img/B03980_11_35.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_35.jpg)'
- en: This is because the error value is the same as the rate of change of the partial
    derivative.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为误差值与偏导数的改变率相同。
- en: 'Equation 4: This equation is used to compute the rate of change of the cost
    as a relationship to the weight.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4：此方程用于计算成本变化率与权重的关系。
- en: '![Backpropagation algorithm](img/B03980_11_36.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播算法](img/B03980_11_36.jpg)'
- en: At every stage of these algorithms, there is some kind of learning that impacts
    the overall output from the network.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些算法的每个阶段，都有某种学习影响网络的总体输出。
- en: 'The final backpropagation algorithm as compiled is explained here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里解释了编译后的最终反向传播算法：
- en: The input layer *x*, and for *x =1* Set the activation as *a¹*.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层 *x*，对于 *x =1* 设置激活为 *a¹*。
- en: For each of the other layers *L = 2, 3, 4 …. L*, compute the activations as:![Backpropagation
    algorithm](img/B03980_11_37.jpg)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于其他每一层 *L = 2, 3, 4 … L*，计算激活如下：![反向传播算法](img/B03980_11_37.jpg)
- en: Compute the error *δ^L* using equations 1 and 2.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用方程式 1 和 2 计算误差 *δ^L*。
- en: Backpropagate the error for *l = L-1, L-2, … 2*, 1 using the equation 3.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用方程式 3 反向传播误差 *l = L-1, L-2, … 2*, 1。
- en: Finally, compute the gradient of the cost function using equation 4.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用方程式 4 计算成本函数的梯度。
- en: If we observe the algorithm, the error vectors *δ^l* are calculated backwards,
    starting from the output layer. This is the fact that the cost is a function of
    outputs from the network. To understand the impact of earlier weights on the cost,
    a chain rule needs to be applied that works backwards through all the layers.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察算法，错误向量 *δ^l* 是从输出层开始反向计算的。这是事实，因为成本是网络输出的函数。为了理解早期权重对成本的影响，需要应用链式法则，该方法通过所有层反向工作。
- en: Softmax regression technique
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax 回归技术
- en: Softmax regression is also known as multinomial logistic regression. This section
    does not cover the concept of logistic regression in depth as it is covered in
    the chapter related to a regression in this book. Instead, we will specifically
    look at understanding how this technique is employed in digit recognition-related
    problems in deep learning use cases.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归也称为多项式逻辑回归。本节不深入探讨逻辑回归的概念，因为它在本书中关于回归的章节中已有涉及。相反，我们将具体探讨如何将这种技术在深度学习用例中的数字识别相关问题上应用。
- en: This technique is a special case of the logistic regression that works for multiple
    classes. As we learned, the result of logistic regression is a binary value *{0,1}*.
    Softmax regression facilitates handling *y(i)<--{1,…,n}*, where *n* is the number
    of classes against the binary classification. In the MNIST digit recognition case,
    the value *n* is 10, representing 10 different classes. For example, in the MNIST
    digit recognition task, we would have *K=10* different classes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术是逻辑回归的一个特例，适用于多类别。正如我们所学的，逻辑回归的结果是一个二进制值 *{0,1}*。Softmax 回归便于处理 *y(i)<--{1,…,n}*，其中
    *n* 是类别的数量，相对于二进制分类。在 MNIST 数字识别案例中，*n* 的值是 10，代表 10 个不同的类别。例如，在 MNIST 数字识别任务中，我们会遇到
    *K=10* 个不同的类别。
- en: As a result of its ability to process multiple classes, this technique is used
    actively in neural network-based, problem-solving areas.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其处理多个类别的能力，这项技术在基于神经网络的解决问题领域中得到了积极的应用。
- en: Deep learning taxonomy
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习分类法
- en: 'The feature learning taxonomy for deep learning cases is depicted here:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了深度学习案例的特征学习分类法：
- en: '![Deep learning taxonomy](img/B03980_11_63.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习分类](img/B03980_11_63.jpg)'
- en: 'Some of the frameworks that are used to implement neural network applications
    are listed here:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了用于实现神经网络应用的一些框架：
- en: Theano is a Python library
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano 是一个 Python 库
- en: Torch a Lua programming language
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torch 是 Lua 编程语言
- en: Deeplearning4J is an open, source Java-based framework that works with Spark
    and Hadoop
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning4J 是一个开源的基于 Java 的框架，与 Spark 和 Hadoop 一起工作
- en: Caffe is a C++ based framework
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffe 是一个基于 C++ 的框架
- en: Convolutional neural networks (CNN/ConvNets)
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络 (CNN/ConvNets)
- en: CNN, also known as convolution nets (ConvNets), are a variation of the regular
    neural networks.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，也称为卷积网（ConvNets），是常规神经网络的变体。
- en: Let us recap the function of the regular neural network. Regular neural networks
    have a single vector-based input that is transformed through a series of hidden
    layers where the neurons in each layer are connected with the neurons in its neighboring
    layers. The last layer in this series provides the output. This layer is called
    the output layer.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下常规神经网络的函数。常规神经网络有一个基于向量的单个输入，它通过一系列隐藏层进行转换，其中每一层的神经元与其相邻层的神经元相连。这个系列中的最后一层提供输出。这个层被称为输出层。
- en: When the input to the neural network is an image and does not just fit into
    a single vector structure, the complexity grows. CNN have this slight variation
    where the input is assumed as a three-dimensional vector having depth (D), height
    (H) and width (W). This assumption changes the way the neural network is organized
    and the way it functions. The following diagram compares the standard three layers
    neural network with the CNN.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络输入是一个图像，并且不仅仅适合单个向量结构时，复杂性会增加。CNN 有这种轻微的变化，其中输入被假定为具有深度（D）、高度（H）和宽度（W）的三维向量。这种假设改变了神经网络的组织方式和功能方式。以下图比较了标准三层神经网络与
    CNN。
- en: '![Convolutional neural networks (CNN/ConvNets)](img/B03980_11_38.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络 (CNN/ConvNets)](img/B03980_11_38.jpg)'
- en: As we see, the convolutional net shown previously arranges neurons in a three-dimensional
    way; every layer in the network transforms this into a 3D output of neuron activations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，前面展示的卷积网以三维方式排列神经元；网络中的每一层都将这些转换为神经元激活的 3D 输出。
- en: 'Convolution network architecture comprises a fixed set of layers designated
    for specialized functions. The most critical layers are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络架构包含一组用于特定功能的固定层。其中最关键的层如下：
- en: '**Convolutional layer** (**CONV**)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层** (**CONV**)'
- en: '**Pooling layer** (**POOL**)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层** (**POOL**)'
- en: '**Full-connected** (**FC**) layer'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接** (**FC**) 层'
- en: In some cases, the activation function is written as another layer (RELU); a
    distinct normalization layer for FC layer conversion may exist.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，激活函数被写成另一个层（RELU）；全连接层转换可能存在一个独特的归一化层。
- en: Convolutional layer (CONV)
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层 (CONV)
- en: The convolutional layer forms the core of convolution nets. This layer is responsible
    for holding the neurons in a three-dimensional format and is therefore responsible
    for a three-dimensional output. The following is an example of an input volume
    with the dimensions 32 x 32 x 3\. As shown, each neuron is connected to a particular
    input region. Along the depth, there can be many neurons; we can see five neurons
    in the example.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是卷积网的核心。这个层负责以三维格式保持神经元，因此负责三维输出。以下是一个具有 32 x 32 x 3 维度的输入体积的示例。如图所示，每个神经元都连接到特定的输入区域。在深度方向上可以有多个神经元；在示例中我们可以看到五个神经元。
- en: '![Convolutional layer (CONV)](img/B03980_11_39.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层 (CONV)](img/B03980_11_39.jpg)'
- en: 'The diagram here shows how the net convolution function works in the neuron
    function representation:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此图展示了网络卷积函数在神经元函数表示中的工作方式：
- en: '![Convolutional layer (CONV)](img/B03980_11_40.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层 (CONV)](img/B03980_11_40.jpg)'
- en: That said, the core function of the neuron remains unchanged and is responsible
    for computing the product of weights and the inputs followed by an observation
    of non-linear behavior. The only difference is the restrictions on the connectivity
    to the local regions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，神经元的核心理念保持不变，负责计算权重和输入的乘积，然后观察非线性行为。唯一的区别是对局部区域的连接限制。
- en: Pooling layer (POOL)
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化层 (POOL)
- en: There can be multiple convolution layers and, between these convolution layers,
    there can be a pooling layer. The pooling layer is responsible for reducing the
    chances of over-fitting by reducing the spatial size of the input volume. The
    reduction of the spatial size implies reducing the number of parameters or the
    amount of computations in the network. The MAX functions contribute to reducing
    the spatial size. The pooling layers use the MAX functions and apply it on every
    slice in the three-dimensional representation, sliced depth-wise. Usually, the
    pooling layers apply filters of size 2 X 2 applied along both width and height.
    This can discard around 75% of the activations.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 可以有多个卷积层，在这些卷积层之间，可以有池化层。池化层负责通过减少输入体积的空间大小来降低过拟合的可能性。空间大小的减少意味着减少网络中的参数数量或计算量。MAX函数有助于减少空间大小。池化层使用MAX函数，并在三维表示的每个切片上应用它，切片是按深度进行的。通常，池化层应用2
    X 2大小的过滤器，沿宽度和高度应用。这可以丢弃大约75%的激活。
- en: 'Overall, the pooling layer has the following characteristics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，池化层具有以下特点：
- en: Always considers a volume size of W1×H1×D1 as an input
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是考虑一个体积大小为W1×H1×D1作为输入
- en: 'Applies stride S and spatial extent F and generates the W2×H2×D2 output where:'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用步长S和空间范围F，生成W2×H2×D2输出，其中：
- en: W2=(W1−F)/S+1
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: W2=(W1−F)/S+1
- en: H2=(H1−F)/S+1
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: H2=(H1−F)/S+1
- en: D2=D1
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D2=D1
- en: Fully connected layer (FC)
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全连接层（FC）
- en: The fully connected layer is very similar to the regular or traditional neural
    networks, responsible for establishing extensive connections to the previous layer
    activations. The connection activations are computed using matrix multiplication
    techniques. More details on this can be found upon referring to the earlier sections
    of this chapter.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层与常规或传统神经网络非常相似，负责与前一层激活建立广泛的连接。连接激活是通过矩阵乘法技术计算的。更多细节可以参考本章前面的部分。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: 'RNNs are a special case of neural networks that are known to be very efficient
    in remembering information, because the hidden state is stored in a distributed
    manner, and so it can hold more information about the experiences. These networks
    also apply non-linear functions to update the hidden state. The following diagram
    depicts how the hidden states link in RNNs:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs是神经网络的一种特殊情况，已知在记住信息方面非常高效，因为隐藏状态是以分布式方式存储的，因此它可以存储更多关于经验的信息。这些网络还应用非线性函数来更新隐藏状态。以下图表描述了RNNs中隐藏状态是如何连接的：
- en: '![Recurrent Neural Networks (RNNs)](img/B03980_11_64.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络（RNNs）](img/B03980_11_64.jpg)'
- en: 'In most of the real world examples, the inputs and outputs are not independent
    of each other. For example, if we had to predict the next word, it would be important
    for us to know the words that came before it. As the word suggests, "Recurrent"
    Neural Networks execute the same task over and over again, where the input of
    one execution is the output of the previous execution. Usually, RNNs are known
    to go back only a few steps in the past and not always through all the iterations.
    The following diagram depicts how RNNs work; it shows how RNNs unfold across iterations:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现实世界的例子中，输入和输出并不是相互独立的。例如，如果我们必须预测下一个单词，那么了解它之前的单词对我们来说很重要。正如其名所示，“循环”神经网络一次又一次地执行相同的任务，其中一次执行的输入是前一次执行的输出。通常，RNNs只回溯过去的一步或几步，而不是总是通过所有迭代。以下图表描述了RNNs的工作原理；它显示了RNNs如何在迭代中展开：
- en: '![Recurrent Neural Networks (RNNs)](img/B03980_11_65.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络（RNNs）](img/B03980_11_65.jpg)'
- en: In the previous example, the requirement was to predict the next word, and if
    there were five words in the input, then RNN unfolds upto five layers.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，要求预测下一个单词，如果输入有五个单词，那么RNN展开到五层。
- en: Restricted Boltzmann Machines (RBMs)
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（RBMs）
- en: RBMs came into existence to solve the difficulty in training RNNs. The rise
    of restricted recurrent models to handle these training difficulties simplified
    the problem context, and additionally, learning algorithms are applied to solve
    the problem. The Hopfield Neural Network is an example of a restricted model that
    addresses the previously described problem.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs的出现是为了解决训练RNNs的困难。受限循环模型的出现简化了问题背景，并且还应用了学习算法来解决问题。Hopfield神经网络是解决之前描述问题的受限模型的一个例子。
- en: 'As a first step, Boltzmann machines came into existence. These models were
    a special case of Hopfield Neural Networks with a stochastic element. In this
    case, the neurons were of two categories: the ones that resulted in visible states
    and the others in hidden states. This was also similar to the Hidden Markov''s
    model. A RBM is again a special case of Boltzmann machine, where the difference
    is primarily to do with the absence of connections between the neurons in the
    same layer. So, for the given states of the neurons of one group, the states of
    the neurons in the other group are independent. The following diagram depicts
    a typical RBN structure and the previous definition:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，玻尔兹曼机出现了。这些模型是具有随机元素的Hopfield神经网络的特例。在这种情况下，神经元分为两类：一类产生可见状态，另一类产生隐藏状态。这也类似于隐藏马尔可夫模型。RBM是玻尔兹曼机的特例，主要区别在于同一层神经元之间没有连接。因此，对于一组神经元的给定状态，另一组神经元的状态是独立的。以下图展示了典型的RBN结构和之前的定义：
- en: '![Restricted Boltzmann Machines (RBMs)](img/B03980_11_66.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机 (RBMs)](img/B03980_11_66.jpg)'
- en: Taking this definition further for a deeper interpretation, some visible states
    of neurons are observable, and there are hidden states of neurons that are not
    visible or cannot directly be seen. There are a few probabilistic conclusions
    made on the hidden states based on the available visible states, and this is how
    the training model is formed.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步从这个定义进行深入解释，一些神经元的可见状态是可观察的，还有一些神经元的隐藏状态是不可见的或不能直接看到。基于可用的可见状态，对隐藏状态做出一些概率推断，这就是训练模型形成的方式。
- en: 'In an RBM, the connectivity is restricted, and this, in turn, eases the inferencing
    and learning. It typically takes only one step to reach an equilibrium state with
    the visible states clamped. The following formula shows how the probability of
    the hidden state is computed, given that the information about the visible states
    is provided:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在RBM中，连接是受限的，这反过来又简化了推理和学习。通常只需一步就能达到一个平衡状态，其中可见状态被固定。以下公式显示了在提供可见状态信息的情况下，如何计算隐藏状态的概率：
- en: '![Restricted Boltzmann Machines (RBMs)](img/B03980_11_67.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机 (RBMs)](img/B03980_11_67.jpg)'
- en: Deep Boltzmann Machines (DBMs)
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机 (DBMs)
- en: DBMs are a special case of conventional Boltzmann machines with a lot of missing
    connections, and, unlike the sequential stochastic updates, parallel updates are
    allowed for ensuring efficiency in the model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: DBMs是具有许多缺失连接的传统玻尔兹曼机的特例，并且与顺序随机更新不同，允许并行更新以确保模型效率。
- en: 'DBMs restrict the connections between hidden variables and primarily use unlabeled
    data for training the models. Labeled data is used for fine-tuning the model.
    The following diagram depicts the general structure of a three-layered DBM:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: DBMs限制了隐藏变量之间的连接，主要使用未标记数据来训练模型。标记数据用于微调模型。以下图展示了三层DBM的一般结构：
- en: '![Deep Boltzmann Machines (DBMs)](img/B03980_11_68.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![深度玻尔兹曼机 (DBMs)](img/B03980_11_68.jpg)'
- en: Autoencoders
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Before we understand autoencoders, let's first learn about **autoassociators**
    (**AAs**). The goal of AAs is to receive an input to the maximum possible precision.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解自动编码器之前，让我们先了解**自联想器**（**AA**）。AA的目标是以尽可能高的精度接收输入。
- en: 'The purpose of an AA is to receive the output as an image of the input as precisely
    as possible. There are two categories of AAs: one is generating AAs, and the second
    is synthesizing AAs. RBMs covered in the previous section are categorized as generating
    AAs, and autoencoders synthesize AAs.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: AA（自动编码器）的目的是尽可能精确地将输出作为输入图像。AA分为两大类：一类是生成AA，另一类是合成AA。上一节中提到的RBMs属于生成AA，而自动编码器则是合成AA。
- en: 'An autoencoder is a type of neural network that has a single open layer. Applying
    backpropagation and unsupervised learning techniques, autoencoders start with
    an assumption that the target value is equal to the input value, *y = x*. The
    following diagram depicts an autoencoder that learns the function *h[W,b] (x)
    ≈ x*:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种具有单个开放层的神经网络。应用反向传播和无监督学习技术，自动编码器从假设目标值等于输入值开始，*y = x*。以下图展示了一个学习函数*h[W,b]
    (x) ≈ x*的自动编码器：
- en: '![Autoencoders](img/B03980_11_69.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![自动编码器](img/B03980_11_69.jpg)'
- en: The layer in the middle is open, and as depicted in the previous diagram, for
    optimal output, it is essential for this layer to have lesser number of neurons
    than that of the input layer. The goal of this model is to learn an approximation
    to the identity function in such a way that the values of **Layer L[3]** are equal
    to values in **Layer L[1]**.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层是开放的，如图中所示，为了获得最佳输出，这个层的神经元数量必须少于输入层的神经元数量。这个模型的目标是学习一个近似于恒等函数的函数，使得**层L[3]**的值等于**层L[1]**的值。
- en: The data is compressed when it passes through the input to output layers. When
    an image of certain pixels, say 100 pixels (10 X 10 pixels), is input to the model
    for a hidden layer with 50 neurons, the expectation is that the network tries
    to compress the image by keeping the pixel configuration intact. This kind of
    compression is possible only if there are hidden interconnections and other characteristic
    correlations that can reduce the input data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在通过输入到输出层时被压缩。当将某个像素图像（例如100像素，10 X 10像素）输入到具有50个神经元的隐藏层时，网络试图通过保持像素配置不变来压缩图像。这种压缩只有在存在隐藏的互连和其他可以减少输入数据的特征相关性时才可能实现。
- en: Another variation of an autoencoder is **denoising autoencoder** (**DA**). The
    difference in this variation of autoencoder is its additional capability to recover
    and restore the state impacted by the corrupt input data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的另一种变体是**降噪自编码器**（**DA**）。这种自编码器变体的不同之处在于它具有额外的能力，可以恢复和恢复受损坏输入数据影响的原始状态。
- en: Implementing ANNs and Deep learning methods
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现ANN和深度学习方法
- en: Refer to the source code provided for this chapter for implementing artificial
    neural networks and other deep learning methods covered in this chapters (source
    code path `.../chapter11/...` under each of the folders for the technologies).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码来实现本章中涵盖的人工神经网络和其他深度学习方法（源代码路径为`.../chapter11/...`，位于每个技术文件夹下）。
- en: Using Mahout
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Mahout
- en: Refer to the folder `.../mahout/chapter11/annexample/`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../mahout/chapter11/annexample/`。
- en: Refer to the folder `.../mahout/chapter11/dlexample/`.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../mahout/chapter11/dlexample/`。
- en: Using R
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用R
- en: Refer to the folder `.../r/chapter11/annexample/`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../r/chapter11/annexample/`。
- en: Refer to the folder `.../r/chapter11/dlexample/`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../r/chapter11/dlexample/`。
- en: Using Spark
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark
- en: Refer to the folder `.../spark/chapter11/annexample/`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../spark/chapter11/annexample/`。
- en: Refer to the folder `.../spark/chapter11/dlexample/`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../spark/chapter11/dlexample/`。
- en: Using Python (Scikit-learn)
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python（Scikit-learn）
- en: Refer to the folder `.../python-scikit-learn/chapter11/annexample/`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../python-scikit-learn/chapter11/annexample/`。
- en: Refer to the folder `.../python-scikit-learn/chapter11/dlexample/`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../python-scikit-learn/chapter11/dlexample/`。
- en: Using Julia
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Julia
- en: Refer to the folder `.../julia/chapter11/annexample/`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../julia/chapter11/annexample/`。
- en: Refer to the folder `.../julia/chapter11/dlexample/`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../julia/chapter11/dlexample/`。
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the model of a biological neuron and how an artificial
    neuron is related to its function. You learned the core concepts of neural networks,
    and how fully connected layers work. We have also explored some key activation
    functions that are used in conjunction with matrix multiplication.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了生物神经元的模型以及人工神经元如何与其功能相关。你学习了神经网络的核心概念，以及全连接层是如何工作的。我们还探讨了与矩阵乘法结合使用的一些关键激活函数。
