- en: Chapter 4. Building Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce **Artificial Neural Networks** (**ANNs**).
    We will study the basic representation of ANNs and then discuss several ANN models
    that can be used in both supervised and unsupervised machine learning problems.
    We also introduce the **Enclog** Clojure library to build ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are well suited for finding patterns in some given data and
    have several practical applications, such as handwriting recognition and machine
    vision, in computing. ANNs are often combined or interconnected to model a given
    problem. Interestingly, they can be applied to several machine learning problems,
    such as regression and classification. ANNs have applications in several areas
    in computing and are not restricted to the scope of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is a form of machine learning in which the given
    training data doesn''t contain any information about which class a given sample
    of input belongs to. As the training data is *unlabeled*, an unsupervised learning
    algorithm must determine the various categories in the given data completely on
    its own. Generally, this is done by seeking out similarities between different
    pieces of data and then grouping the data into several categories. This technique
    is called **cluster analysis,** and we shall study more about this methodology
    in the following chapters. ANNs are used in unsupervised machine learning techniques
    mostly due to their ability to quickly recognize patterns in some unlabeled data.
    This specialized form of unsupervised learning exhibited by ANNs is termed as
    **competitive learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting fact about ANNs is that they are modeled from the structure and
    behavior of the central nervous system of higher-order animals that demonstrate
    learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding nonlinear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By this time, the reader must be aware of the fact that the gradient descent
    algorithm can be used to estimate both linear and logistic regression models for
    regression and classification problems. An obvious question would be: what is
    the need of neural networks when we can use gradient descent to estimate linear
    regression and logistic regression models from the training data? To understand
    the necessity of ANNs, we must first understand *nonlinear regression*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a single feature variable *X* and a dependent variable
    *Y* that varies with *X*, as shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding nonlinear regression](img/4351OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in the preceding plot, it's hard, if not impossible, to model
    the dependent variable *Y* as a linear equation of the independent variable *X*.
    We could model the dependent variable *Y* to be a high-order polynomial equation
    of the dependent variable *X*, thus converting the problem into the standard form
    of linear regressions. Hence, the dependent variable *Y* is said to vary nonlinearly
    with the independent variable *X*. Of course, there is also a good chance that
    data cannot be modeled using a polynomial function either.
  prefs: []
  type: TYPE_NORMAL
- en: It can also be shown that calculating the weights or coefficients of all the
    terms in a polynomial function using gradient descent has a time complexity of
    ![Understanding nonlinear regression](img/4351OS_04_02.jpg), where *n* is the
    number of features in the training data. Similarly, the algorithmic complexity
    of calculating the coefficients of all the terms in a third-order polynomial equation
    is ![Understanding nonlinear regression](img/4351OS_04_03.jpg). It's apparent
    that the time complexity of gradient descent increases geometrically with the
    number of features of the model. Thus, gradient descent on its own is not efficient
    enough to model nonlinear regression models with a large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs, on the other hand, are very efficient at modeling nonlinear regression
    models of data with a high number of features. We will now study the foundational
    ideas of ANNs and several ANN models that can be used in supervised and unsupervised
    learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Representing neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ANNs are modeled from the behavior of the central nervous system of organisms,
    such as mammals and reptiles, that are capable of learning. The central nervous
    system of these organisms comprises the organism''s brain, spinal cord, and a
    network of supporting neural tissues. The brain processes information and generates
    electric signals that are transported through the network of neural fibers to
    the various organs of the organism. Although the organism''s brain performs a
    lot of complex processing and control, it is actually a collection of neurons.
    The actual processing of sensory signals, however, is performed by several complex
    combinations of these neurons. Of course, each neuron is capable of processing
    an extremely small portion of the information processed by the brain. The brain
    actually functions by routing electrical signals from the various sensory organs
    of the body to its motor organs through this complex network of neurons. An individual
    neuron has a cellular structure as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing neural networks](img/4351OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A neuron has several dendrites close to the nucleus of the cell and a single
    *axon* that transports signals from the nucleus of the cell. The dendrites are
    used to receive signals from other neurons and can be thought of as the input
    to the neuron. Similarly, the axon of the neuron is analogous to the output of
    the neuron. The neuron can thus be mathematically represented as a function that
    processes several inputs and produces a single output.
  prefs: []
  type: TYPE_NORMAL
- en: Several of these neurons are interconnected, and this network is termed as a
    **neural network**. A neuron essentially performs its function by relaying weak
    electrical signals from and to other neurons. The interconnecting space between
    two neurons is called a **synapse**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ANN comprises several interconnected neurons. Each neuron can be represented
    by a mathematical function that consumes several input values and produces an
    output value, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing neural networks](img/4351OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A single neuron can be illustrated by the preceding diagram. In mathematical
    terms, it's simply a function ![Representing neural networks](img/4351OS_04_06.jpg)
    that maps a set of input values ![Representing neural networks](img/4351OS_04_07.jpg)
    to an output value ![Representing neural networks](img/4351OS_04_08.jpg). The
    function ![Representing neural networks](img/4351OS_04_06.jpg) is called the **activation
    function** of the neuron, and its output value ![Representing neural networks](img/4351OS_04_08.jpg)
    is called the **activation of the neuron**. This representation of a neuron is
    termed as a **perceptron**. Perceptron can be used on its own and is effective
    enough to estimate supervised machine learning models, such as linear regression
    and logistic regression. However, complex nonlinear data can be better modeled
    with several interconnected perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, a bias input is added to the set of input values supplied to a perceptron.
    For the input values ![Representing neural networks](img/4351OS_04_07.jpg), we
    add the term ![Representing neural networks](img/4351OS_04_09.jpg) as a bias input
    such that ![Representing neural networks](img/4351OS_04_10.jpg). A neuron with
    this added bias value can be illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing neural networks](img/4351OS_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each input value ![Representing neural networks](img/4351OS_04_12.jpg) supplied
    to the perceptron has an associated weight ![Representing neural networks](img/4351OS_04_13.jpg).
    This weight is analogous to the coefficients of the features of a linear regression
    model. The activation function is applied to these weights and their corresponding
    input values. We can formally define the estimated output value ![Representing
    neural networks](img/4351OS_04_06.jpg) of the perceptron in terms of the input
    values, their weights, and the perceptron''s activation function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing neural networks](img/4351OS_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The activation function to be used by the nodes of an ANN depends greatly on
    the sample data that has to be modeled. Generally, the **sigmoid** or **hyperbolic
    tangent** functions are used as the activation function for classification problems
    (for more information, refer to *Wavelet Neural Network (WNN) approach for calibration
    model building based on gasoline near infrared (NIR) spectr*). The sigmoid function
    is said to be *activated* for a given threshold input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the variance of the sigmoid function to depict this behavior, as
    shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing neural networks](img/4351OS_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ANNs can be broadly classified into *feed-forward neural networks* and *recurrent
    neural networks* (for more information, refer to *Bidirectional recurrent neural
    networks*). The difference between these two types of ANNs is that in feed-forward
    neural networks, the connections between the nodes of the ANN do not form a directed
    cycle as opposed to recurrent neural networks where the node interconnections
    do form a directed cycle. Thus, in feed-forward neural networks, each node in
    a given layer of the ANN receives input only from the nodes in the immediate previous
    layer in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ANN models that have practical applications, and we will explore
    a few of them in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multilayer perceptron ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now introduce a simple model of feed-forward neural networks—the **Multilayer
    Perceptron** model. This model represents a basic feed-forward neural network
    and is versatile enough to model regression and classification problems in the
    domain of supervised learning. All the input flows through a feed-forward neural
    network in a single direction. This is a direct consequence of the fact that there
    is no *feedback* from or to any layer in a feed-forward neural network.
  prefs: []
  type: TYPE_NORMAL
- en: By feedback, we mean that the output of a given layer is fed back as input to
    the perceptrons in a previous layer in the ANN. Also, using a single layer of
    perceptrons would mean using only a single activation function, which is equivalent
    to using *logistic regression* to model the given training data. This would mean
    that the model cannot be used to fit nonlinear data, which is the primary motivation
    of ANNs. We must note that we discussed logistic regression in [Chapter 3](ch03.html
    "Chapter 3. Categorizing Data"), *Categorizing Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A multilayer perceptron ANN can be illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A multilayer perceptron ANN comprises several layers of perceptron nodes. It
    exhibits a single input layer, a single output layer, and several hidden layers
    of perceptrons. The input layer simply relays the input values to the first hidden
    layer of the ANN. These values are then propagated to the output layer through
    the other hidden layers, where they are weighted and summed using the activation
    function, to finally produce the output values.
  prefs: []
  type: TYPE_NORMAL
- en: Each sample in the training data is represented by the ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_17.jpg) tuple, where ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_18.jpg) is the expected output and ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_19.jpg) is the input value of the ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_20.jpg) training sample. The ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_19.jpg) input vector comprises a number
    of values equal to the number of features in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of each node is termed as the **activation** of the node and is
    represented by the term ![Understanding multilayer perceptron ANNs](img/4351OS_04_21.jpg)
    for the ![Understanding multilayer perceptron ANNs](img/4351OS_04_20.jpg) node
    in the layer ![Understanding multilayer perceptron ANNs](img/4351OS_04_22.jpg).
    As we mentioned earlier, the activation function used to produce this value is
    the sigmoid function or the hyperbolic tangent function. Of course, any other
    mathematical function could be used to fit the sample data. The input layer of
    a multilayer perceptron network simply adds a bias input to the input values,
    and the set of inputs supplied to the ANN are relayed to the next layer. We can
    formally represent this equality as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The synapses between every pair of layers in the ANN have an associated weight
    matrix. The number of rows in these matrices is equal to the number of input values,
    that is, the number of nodes in the layer closer to the input layer of the ANN
    and the number of columns equal to the number of nodes in the layer of the synapse
    that is closer to the output layer of the ANN. For a layer *l*, the weight matrix
    is represented by the term ![Understanding multilayer perceptron ANNs](img/4351OS_04_24.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation values of a layer *l* can be determined using the activation
    function of the ANN. The activation function is applied on the products of the
    weight matrix and the activation values produced by the previous layer in the
    ANN. Generally, the activation function used for a multilayer perceptron is a
    sigmoid function. This equality can be formally represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generally, the activation function used for a multilayer perceptron is a sigmoid
    function. Note that we do not add a bias value in the output layer of an ANN.
    Also, the output layer can produce any number of output values. To model a *k-class*
    classification problem, we would require an ANN producing *k* output values.
  prefs: []
  type: TYPE_NORMAL
- en: To perform binary classification, we can only model a maximum of two classes
    of input data. The output value generated by an ANN used for binary classification
    is always 0 or 1\. Thus, for ![Understanding multilayer perceptron ANNs](img/4351OS_04_26.jpg)
    classes, ![Understanding multilayer perceptron ANNs](img/4351OS_04_27.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also model a multiclass classification using the *k* binary output values,
    and thus, the output of the ANN is a ![Understanding multilayer perceptron ANNs](img/4351OS_04_28.jpg)
    matrix. This can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we can use a multilayer perceptron ANN to perform binary and multiclass
    classifications. A multilayer perceptron ANN can be trained using the **backpropagation**
    **algorithm**, which we will study and implement later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that we want to model the behavior of a logical XOR gate. An XOR
    gate can be thought of a binary classifier that requires two inputs and generates
    a single output. An ANN that models the XOR gate would have a structure as shown
    in the following diagram. Interestingly, linear regression can be used to model
    both AND and OR logic gates but cannot be used to model an XOR gate. This is due
    to the nonlinear nature of the output of an XOR gate, and thus, ANNs are used
    to overcome this limitation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The multilayer perceptron illustrated in the preceding diagram has three nodes
    in the input layer, four nodes in the hidden layer, and one node in the output
    layer. Observe that every layer other than the output layer adds a bias input
    to the set of input values for the nodes in the next layer. There are two synapses
    in the ANN, shown in the preceding diagram, and they are associated with the weight
    matrices ![Understanding multilayer perceptron ANNs](img/4351OS_04_31.jpg) and
    ![Understanding multilayer perceptron ANNs](img/4351OS_04_32.jpg). Note that the
    first synapse is between the input layer and hidden layer, and the second synapse
    is between the hidden layer and the output layer. The weight matrix ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_31.jpg) has a size of ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_33.jpg), and the weight matrix ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_32.jpg) has a size of ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_34.jpg). Also, the term ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_35.jpg) is used to represent all the
    weight matrices in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the activation function of each node in a multilayer perceptron ANN is a
    sigmoid function, we can define the cost function of the weights of the nodes
    of the ANN similar to the cost function of a logistic regression model. The cost
    function of an ANN can be defined in terms of the weight matrices as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding cost function is essentially the average of the cost functions
    of each node in the output layer of an ANN (for more information, refer to *Neural
    Networks in Materials Science*). For a multilayer perceptron ANN with *K* output
    values, we perform the average over the *K* terms. Note that ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_37.jpg) represents the ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_38.jpg) output value of the ANN, ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_39.jpg) represents the input variables
    of the ANN, and *N* is the number of sample values in the training data. The cost
    function is essentially that of logistic regression but is applied here for the
    *K* output values. We can add a regularization parameter to the preceding cost
    function and express the regularized cost function using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The cost function defined in the preceding equation adds a regularization term
    similar to that of logistic regression. The regularization term is essentially
    the sum of the squares of all weights of all input values of the several layers
    of the ANN, excluding the weights for the added bias input. Also, the term ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_41.jpg) refers to the number of nodes
    in layer *l* of the ANN. An interesting point to note is that in the preceding
    regularized cost function, only the regularization term depends on the number
    of layers in the ANN. Hence, the *generalization* of the estimated model is based
    on the number of layers in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the backpropagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **backpropagation learning** algorithm is used to train a multilayer perceptron
    ANN from a given set of sample values. In brief, this algorithm first calculates
    the output value for a set of given input values and also calculates the amount
    of error in the output of the ANN. The amount of error in the ANN is determined
    by comparing the predicted output value of the ANN to the expected output value
    for the given input values from the training data provided to the ANN. The calculated
    error is then used to modify the weights of the ANN. Thus, after training the
    ANN with a reasonable number of samples, the ANN will be able to predict the output
    value for a set of input values. The algorithm comprises of three distinct phases.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A forward propagation phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A backpropagation phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A weight update phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of the synapses in the ANN are first initialized to random values
    within the ranges ![Understanding the backpropagation algorithm](img/4351OS_04_42.jpg)
    and ![Understanding the backpropagation algorithm](img/4351OS_04_43.jpg). We initialize
    the weights to values within this range to avoid a symmetry in the weight matrices.
    This avoidance of symmetry is called **symmetry breaking,** and it is performed
    so that each iteration of the backpropagation algorithm produces a noticeable
    change in the weights of the synapses in the ANN. This is desirable in an ANN
    as each of its node should learn independently of other nodes in the ANN. If all
    the nodes were to have identical weights, the estimated learning model will be
    either overfit or underfit.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the backpropagation learning algorithm requires two additional parameters,
    which are the learning rate ![Understanding the backpropagation algorithm](img/4351OS_04_44.jpg)
    and the learning momentum ![Understanding the backpropagation algorithm](img/4351OS_04_45.jpg).
    We will see the effects of these parameters in the example later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward propagation phase of the algorithm simply calculates the activation
    values of all nodes in the various layers of the ANN. As we mentioned earlier,
    the activation values of the nodes in the input layer are the input values and
    the bias input of the ANN. This can be formally defined by using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these activation values from the input layer of the ANN, the activation
    of the nodes in the other layers of the ANN is determined. This is done by applying
    the activation function to the products of the weight matrix of a given layer
    and the activation values from the previous layer in the ANN. This can be formally
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation explains that the activation value of a layer *l* is
    equal to the activation function applied to the output (or activation) values
    of the previous layer and the given layer''s weight matrix. Next, the activation
    values of the output layer are *backpropagated*. By this, we mean that that the
    activation values are traversed from the output layer through the hidden layers
    to the input layer of the ANN. During this phase, we determine the amount of error
    or delta in each node in the ANN. The delta values of the output layer are determined
    by calculating the difference between the expected output values, ![Understanding
    the backpropagation algorithm](img/4351OS_04_18.jpg), and the activation values
    of the output layer, ![Understanding the backpropagation algorithm](img/4351OS_04_47.jpg).
    This difference calculation can be summarized by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The term ![Understanding the backpropagation algorithm](img/4351OS_04_49.jpg)
    of a layer *l* is a matrix of size ![Understanding the backpropagation algorithm](img/4351OS_04_50.jpg)
    where *j* is the number of nodes in layer *l*. This term can be formally defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The delta terms of the layers other than the output layer of the ANN are determined
    by the following equality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the binary operation ![Understanding the backpropagation
    algorithm](img/4351OS_04_53.jpg) is used to represent an element-wise multiplication
    of two matrices of equal size. Note that this operation is different from matrix
    multiplication, and an element-wise multiplication will return a matrix composed
    of the products of the elements with the same position in two matrices of equal
    size. The term ![Understanding the backpropagation algorithm](img/4351OS_04_54.jpg)
    represents the derivative of the activation function used in the ANN. As we are
    using the sigmoid function as our activation function, the term ![Understanding
    the backpropagation algorithm](img/4351OS_04_55.jpg) has the value ![Understanding
    the backpropagation algorithm](img/4351OS_04_56.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can calculate the delta values of all nodes in the ANN. We can use
    these delta values to determine the gradients of the synapses of the ANN. We now
    move on to the final weight update phase of the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradients of the various synapses are first initialized to matrices with
    all the elements as 0\. The size of a gradient matrix of a given synapse is the
    same size as the weight matrix of the synapse. The gradient term ![Understanding
    the backpropagation algorithm](img/4351OS_04_57.jpg) represents the gradients
    of the synapse layer that is present immediately after layer *l* in the ANN. The
    initialization of the gradients of the synapses in the ANN is formally expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For each sample value in the training data, we calculate the deltas and activation
    values of all nodes in the ANN. These values are added to the gradients of the
    synapses using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then calculate the average of the gradients for all the sample values and
    use the delta and gradient values of a given layer to update the weight matrix
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the learning rate and learning momentum parameters of the algorithm come
    into play only in the weight update phase. The preceding three equations represent
    a single iteration of the backpropagation algorithm. A large number of iterations
    must be performed until the overall error in the ANN converges to a small value.
    We can now summarize the backpropagation learning algorithm using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights of the synapses of the ANN to random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a sample value and forward propagate the sample values through several
    layers of the ANN to generate the activations of every node in the ANN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the activations generated by the last layer of the ANN through
    the hidden layers and to the input layer of the ANN. Through this step, we calculate
    the error or delta of every node in the ANN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the product of the errors generated from step 3 with the synapse weights
    or input activations for all the nodes in the ANN. This step produces the gradient
    of weight for each node in the network. Each gradient is represented by a ratio
    or percentage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the changes in the weights of the synapse layers in the ANN using
    the gradients and deltas of a given layer in the ANN. These changes are then subtracted
    from the weights of the synapses in the ANN. This is essentially the weight update
    step of the backpropagation algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 to 5 for the rest of the samples in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are several distinct parts in the backpropagation learning algorithm,
    and we will now implement each part and combine it into a complete implementation.
    As the deltas and weights of the synapses and activations in an ANN can be represented
    by matrices, we can write a vectorized implementation of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that for the following example, we require functions from the `incanter.core`
    namespace from the Incanter library. The functions in this namespace actually
    use the Clatrix library for the representation of a matrix and its manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we need to implement an ANN to model a logical XOR gate.
    The sample data is simply the truth table of the XOR gate and can be represented
    as a vector, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Each element defined in the preceding vector `sample-data` is itself a vector
    comprising other vectors for the input and output values of an XOR gate. We will
    use this vector as our training data for building an ANN. This is essentially
    a classification problem, and we will use ANNs to model it. In abstract terms,
    an ANN should be capable of performing both binary and multiclass classifications.
    We can define the protocol of an ANN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `NeuralNetwork` protocol defined in the preceding code has three functions.
    The `train-ann` function can be used to train the ANN and requires some sample
    data. The `run` and `run-binary` functions can be used on this ANN to perform
    multiclass and binary classifications, respectively. Both the `run` and `run-binary`
    functions require a set of input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of the backpropagation algorithm is the initialization of the
    weights of the synapses of the ANN. We can use the `rand` and `matrix` functions
    to generate these weights as a matrix, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `rand-list` function shown in the preceding code creates a list of random
    elements in the positive and negative range of `epsilon`. As we described earlier,
    we choose this range to break the symmetry of the weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The `random-initial-weights` function generates several weight matrices for
    different layers of the ANN. As defined in the preceding code, the `layers` argument
    must be a vector of the sizes of the layers of the ANN. For an ANN with two nodes
    in the input layer, three nodes in the hidden layer, and one node in the output
    layer, we pass `layers` as `[2 3 1]` to the `random-initial-weights` function.
    Each weight matrix has a number of columns equal to the number of inputs and number
    of rows equal to the number of nodes in the next layer of the ANN. We set the
    number of columns in a weight matrix of a given layer to the number of inputs,
    plus an extra input for the bias of the neural layer. Note that we use a slightly
    different form of the `matrix` function. This form takes a single vector and partitions
    this vector into a matrix that has a number of columns as specified by second
    argument to this function. Thus, the vector passed to this form of the `matrix`
    function must have `(* rows cols)` elements, where `rows` and `cols` are the number
    of rows and columns, respectively, in the weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will need to apply the sigmoid function to all the activations of a layer
    in the ANN, we must define a function that applies the sigmoid function on all
    the elements in a given matrix. We can use the `div`, `plus`, `exp`, and `minus`
    functions from the `incanter.core` namespace to implement such a function, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that all of the previously defined functions apply the corresponding arithmetic
    operation on all the elements in a given matrix and returns a new matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to implicitly add a bias node to each layer in an ANN. This
    can be done by wrapping around the `bind-rows` function, which adds a row of elements
    to a matrix, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since the bias value is always 1, we specify the row of elements as `[1]` to
    the `bind-rows` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the functions defined earlier, we can implement forward propagation.
    We essentially have to multiply the weights of a given synapse between two layers
    in an ANN and then apply the sigmoid function on each of the generated activation
    values, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first define a `matrix-mult` function, which performs
    matrix multiplication and ensures that the result is a matrix. Note that to define
    `matrix-mult`, we use the `mmult` function instead of the `mult` function that
    multiplies the corresponding elements in two matrices of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `matrix-mult` and `sigmoid` functions, we can implement the forward
    propagation step between two layers in the ANN. This is done in the `forward-propagate-layer`
    function, which simply multiplies the matrices representing the weights of the
    synapse between two layers in the ANN and the input activation values while ensuring
    that the returned value is always a matrix. To propagate a given set of values
    through all the layers of an ANN, we must add a bias input and apply the `forward-propagate-layer`
    function for each layer. This can be done concisely using the `reduce` function
    over a closure of the `forward-propagate-layer` function as shown in the `forward-propagate`
    function defined in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the `forward-propagate` function can determine the output activations
    of the ANN, we actually require the activations of all the nodes in the ANN to
    use backpropagation. We can do this by translating the `reduce` function to a
    recursive function and introducing an accumulator variable to store the activations
    of every layer in the ANN. The `forward-propagate-all-activations` function, which
    is defined in the following code, implements this idea and uses the `loop` form
    to recursively apply the `forward-propagate-layer` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `forward-propagate-all-activations` function defined in the preceding code
    requires all the weights of the nodes in the ANN and the input values to pass
    through the ANN as activation values. We first use the `bind-bias` function to
    add the bias input to the input activations of the ANN. We then store this value
    in an accumulator, that is, the variable `all-activations`, as a vector of all
    the activations in the ANN. The `forward-propagate-layer` function is then applied
    over the weight matrices of the various layers of the ANN, and each iteration
    adds a bias input to the input activations of the corresponding layer in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that we do not add the bias input in the last iteration as it computes
    the output layer of the ANN. Thus, the `forward-propagate-all-activations` function
    applies forward propagation of input values through an ANN and returns the activations
    of every node in the ANN. Note that the activation values in this vector are in
    the order of the layers of the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now implement the backpropagation phase of the backpropagation learning
    algorithm. First, we would have to implement a function to calculate the error
    term ![Understanding the backpropagation algorithm](img/4351OS_04_49.jpg) from
    the equation ![Understanding the backpropagation algorithm](img/4351OS_04_61.jpg).
    We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `back-propagate-layer` function defined in the preceding code calculates
    the errors, or deltas, of a synapse layer *l* in the ANN from the weights of the
    layer and the deltas of the next layer in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that we only use matrix multiplication to calculate the term ![Understanding
    the backpropagation algorithm](img/4351OS_04_62.jpg) via the `matrix-mult` function.
    All other multiplication operations are element-wise multiplication of matrices,
    which is done using the `mult` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, we have to apply this function from the output layer to the input
    layer through the various hidden layers of an ANN to produce the delta values
    of every node in the ANN. These delta values can then be added to the activations
    of the nodes, thus producing the gradient values by which we must adjust the weights
    of the nodes in the ANN. We can do this in a manner similar to the `forward-propagate-all-activations`
    function, that is, by recursively applying the `back-propagate-layer` function
    over the various layers of the ANN. Of course, we have to traverse the layers
    of the ANN in the reverse order, that is, starting from the output layer, through
    the hidden layers, to the input layer. We will do this with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `calc-deltas` function determines the delta values of all the perceptron
    nodes in the ANN. For this calculation, the input and output activations are not
    needed. Only the hidden activations, bound to the `hidden-activations` variable,
    are needed to calculate the delta values. Also, the weights of the input layer
    are skipped as they are bound to the `hidden-weights` variable. The `calc-deltas`
    function then applies the `back-propagate-layer` function to all the weight matrices
    of each synapse layer in the ANN, thus determining the deltas of all the nodes
    in the matrix. Note that we don't add the delta of the bias nodes to a computed
    set of deltas. This is done using the `rest` function, `(rest deltas')`, on the
    calculated deltas of a given synapse layer, as the first delta is that of a bias
    input in a given layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, the gradient vector terms for a given synapse layer ![Understanding
    the backpropagation algorithm](img/4351OS_04_57.jpg) are determined by multiplying
    the matrices ![Understanding the backpropagation algorithm](img/4351OS_04_63.jpg)
    and ![Understanding the backpropagation algorithm](img/4351OS_04_64.jpg), which
    represent the deltas of the next layer and activations of the given layer respectively.
    We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calc-gradients` function shown in the preceding code is a concise implementation
    of the term ![Understanding the backpropagation algorithm](img/4351OS_04_65.jpg).
    As we will be dealing with a sequence of delta and activation terms, we use the
    `map` function to apply the preceding equality to the corresponding deltas and
    activations in the ANN. Using the `calc-deltas` and `calc-gradient` functions,
    we can determine the total error in the weights of all nodes in the ANN for a
    given training sample. We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `calc-error` function defined in the preceding code requires two parameters—the
    weight matrices of the synapse layers in the ANN and a sample training value,
    which is shown as `[input expected-output]`. The activations of all the nodes
    in the ANN are first calculated using the `forward-propagate-all-activations`
    function, and the delta value of the last layer is calculated as the difference
    of the expected output value and the actual output value produced by the ANN.
    The output value calculated by the ANN is simply the last activation value produced
    by the ANN, shown as `(last activations)` in the preceding code. Using the calculated
    activations, the deltas of all the perceptron nodes are determined via the `calc-deltas`
    function. These delta values are in turn used to determine the gradients of weights
    in the various layers of the ANN using the `calc-gradients` function. The **Mean
    Square Error** (**MSE**) of the ANN for the given sample value is also calculated
    by adding the squares of the delta values of the output layer of the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given weight matrix of a layer in the ANN, we must initialize the gradients
    for the layer as a matrix with the same dimensions as the weight matrix, and all
    the elements in the gradient matrix must be set to `0`. This can be implemented
    using a composition of the `dim` function, which returns the size of a matrix
    as a vector, and a variant form of the `matrix` function, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the `new-gradient-matrix` function defined in the preceding code, the `matrix`
    function expects a value, the number of rows and the number of columns to initialize
    a matrix. This function produces an initialized gradient matrix with the same
    structure as the supplied weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now implement the `calc-gradients-and-error` function to apply the `calc-error`
    function on a set of weight matrices and sample values. We must basically apply
    the `calc-error` function to each sample and accumulate the sum of the gradient
    and the MSE values. We then calculate the average of these accumulated values
    to return the gradient matrices and total MSE for the given sample values and
    weight matrices. We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `calc-gradients-and-error` function defined in the preceding code relies
    on the `calc-gradients-and-error'` helper function. `The calc-gradients-and-error'`
    function initializes the gradient matrices, performs the application of the `calc-error`
    function, and accumulates the calculated gradient values and MSE. The `calc-gradients-and-error`
    function simply calculates the average of the accumulated gradient matrices and
    MSE returned from the `calc-gradients-and-error'` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the only missing piece in our implementation is modifying the weights
    of the nodes in the ANN using calculated gradients. In brief, we must repeatedly
    update the weights until a convergence in the MSE is observed. This is actually
    a form of gradient descent applied to the nodes of an ANN. We will now implement
    this variant of gradient descent in order to train the ANN by repeatedly modifying
    the weights of the nodes in the ANN, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `gradient-descent-complete?` function defined in the preceding code simply
    checks for the termination condition of gradient descent. This function assumes
    that the ANN, represented as a network, is a map or record that contains the `:options`
    keyword. The value of this key is in turn another map that contains the various
    configuration options of the ANN. The `gradient-descent-complete?` function checks
    whether the total MSE of the ANN is less than the desired MSE, which is specified
    by the `:desired-error` option. Also, we add another condition to check if the
    number of iterations performed exceeds the maximum number of iterations specified
    by the `:max-iters` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will implement a `gradient-descent` function for multilayer perceptron
    ANNs. In this implementation, the changes in weights are calculated by the `step`
    function provided by the gradient descent algorithm. These calculated changes
    are then simply added to the existing weights of the synapse layers of the ANN.
    We will implement the `gradient-descent` function for multilayer perceptron ANNs
    with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `apply-weight-changes` function defined in the preceding code simply adds
    the weights and the calculated changes in the weights of the ANN. The `gradient-descent`
    function requires a `step` function (specified as `step-fn`), the initial state
    of the ANN, the ANN itself, and the sample data to train the ANN. This function
    must calculate the weight changes from the ANN, the initial gradient matrices,
    and the initial state of the ANN. The `step-fn` function also returns the changed
    state of the ANN. The weights of the ANN are then updated using the `apply-weight-changes`
    function, and this iteration is repeatedly performed until the `gradient-descent-complete?`
    function returns as `true`. The weights of the ANN are specified by the `:weights`
    keyword in the `network` map. These weights are then updated by simply overwriting
    the value on the `network` specified by the `:weights` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of the backpropagation algorithm, we need to specify the learning
    rate and learning momentum by which the ANN must be trained. These parameters
    are needed to determine the changes in the weights of the nodes in the ANN. A
    function implementing this calculation must then be specified as the `step-fn`
    parameter to the `gradient-descent` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `calc-weight-changes` function defined in the preceding code calculates
    the change of weights, termed as ![Understanding the backpropagation algorithm](img/4351OS_04_66.jpg),
    from the gradient values and deltas of a given layer in the ANN. The `bprop-step-fn`
    function extracts the learning rate and learning momentum parameters from the
    ANN that is represented by `network` and uses the `calc-weight-changes` function.
    As the weights will be added with the changes by the `gradient-descent` function,
    we return the changes in weights as negative values using the `minus` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gradient-descent-bprop` function simply initializes the gradient matrices
    for the given weights of the ANN and calls the `gradient-descent` function by
    specifying `bprop-step-fn` as the `step` function to be used. Using the `gradient-descent-bprop`
    function, we can implement the abstract `NeuralNetwork` protocol we had defined
    earlier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `MultiLayerPerceptron` record defined in the preceding code trains a multilayer
    perceptron ANN using the `gradient-descent-bprop` function. The `train-ann` function
    first extracts the values for the number of hidden neurons and the constant ![Understanding
    the backpropagation algorithm](img/4351OS_04_43.jpg) from the options map specified
    to the ANN. The sizes of the various synapse layers in the ANN are first determined
    from the sample data and bound to the `layer-sizes` variable. The weights of the
    ANN are then initialized using the `random-initial-weights` function and updated
    in the record `network` using the `assoc` function. Finally, the `gradient-descent-bprop`
    function is called to train the ANN using the backpropagation learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The ANN defined by the `MultiLayerPerceptron` record also implements two other
    functions, `run` and `run-binary`, from the `NeuralNetwork` protocol. The `run`
    function uses the `forward-propagate` function to determine the output values
    of a trained `MultiLayerPerceptron` ANN. The `run-binary` function simply rounds
    the value of the output returned by the `run` function for the given set of input
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ANN created using the `MultiLayerPerceptron` record requires a single `options`
    parameter containing the various options we can specify for the ANN. We can define
    the default options for such an ANN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The map defined by the `default-options` variable contains the following keys
    that specify the options for the `MultiLayerPerceptron` ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:max-iter`: This key specifies the maximum number of iterations to run the
    `gradient-descent` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:desired-error`: This variable specifies the expected or acceptable MSE in
    the ANN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:hidden-neurons`: This variable specifies the number of hidden neural nodes
    in the network. The value `[3]` represents a single hidden layer with three neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:learning-rate` and `:learning-momentum`: These keys specify the learning
    rate and learning momentum for the weight update phase of the backpropagation
    learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:epsilon`: This variable specifies the constant used by the `random-initial-weights`
    function to initialize the weights of the ANN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also define a simple helper function `train` to create an ANN of the `MultiLayerPerceptron`
    type and train the ANN using the `train-ann` function and the sample data specified
    by the `samples` parameter. We can now create a trained ANN from the training
    data specified by the `sample-data` variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the trained ANN to predict the output of some input values.
    The output generated by the ANN defined by `MLP` closely matches the output of
    an XOR gate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the trained ANN produces incorrect outputs for some set of inputs
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are several measures we can implement in order to improve the accuracy
    of the trained ANN. First, we can regularize the calculated gradients using the
    weights matrices of the ANN. This modification will produce a noticeable improvement
    in the preceding implementation. We can also increase the maximum number of iterations
    to be performed. We can also tune the algorithm to perform better by tweaking
    the learning rate, the learning momentum, and the number of hidden nodes in the
    ANN. These modifications are skipped as they have to be done by the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Enclog** library ([http://github.com/jimpil/enclog](http://github.com/jimpil/enclog))
    is a Clojure wrapper library for the **Encog** library for machine learning algorithms
    and ANNs. The Encog library ([http://github.com/encog](http://github.com/encog))
    has two primary implementations: one in Java and one in .NET. We can use the Enclog
    library to easily generate customized ANNs to model both supervised and unsupervised
    machine learning problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Enclog library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that the Enclog library requires the Encog Java library as a dependency.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create an ANN from the Enclog library using the `neural-pattern` and
    `network` functions from the `enclog.nnets` namespace. The `neural-pattern` function
    is used to specify a neural network model for the ANN. The `network` function
    accepts a neural network model returned from the `neural-pattern` function and
    creates a new ANN. We can provide several options to the `network` function depending
    on the specified neural network model. A feed-forward multilayer perceptron network
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For a feed-forward neural network, we can specify the activation function with
    the `:activation` key to the `network` function. For our example, we used the
    sigmoid function, which is specified as `:sigmoid`, as the activation function
    for the ANNs nodes. We also specified the number of nodes in the input, output,
    and hidden layers of the ANN using the `:input`, `:output`, and `:hidden` keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train an ANN created by the `network` function with some sample data, we
    use the `trainer` and `train` functions from the `enclog.training` namespace.
    The learning algorithm to be used to train the ANN must be specified as the first
    parameter to the `trainer` function. For the backpropagation algorithm, this parameter
    is the `:back-prop` keyword. The value returned by the trainer function represents
    an ANN as well as the learning algorithm to be used to train the ANN. The `train`
    function is then used to actually run the specified training algorithm on the
    ANN. We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `train-network` function defined in the preceding code takes three parameters.
    The first parameter is an ANN created by the network function, the second parameter
    is the training data to be used to train the ANN, and the third parameter specifies
    the learning algorithm by which the ANN must be trained. As shown in the preceding
    code, we can specify the ANN and the training data to the `trainer` function using
    the key parameters, `:network` and `:training-set`. The `train` function is then
    used to run the training algorithm on the ANN using the sample data. We can specify
    the expected error in the ANN and the maximum number of iterations to run the
    training algorithm as the first and second parameters to the `train` function.
    In the preceding example, the desired error is `0.01`, and the maximum number
    of iterations is 1000\. The last parameter passed to the `train` function is a
    vector specifying the behaviors of the ANN, and we ignore it by passing it as
    an empty vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training data to be used to run the training algorithm on the ANN can be
    created using Enclog''s `data` function. For example, we can create a training
    data for a logical XOR gate using the `data` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `data` function requires the type of data as the first parameter of the
    function, followed by the input and output values of the training data as vectors.
    For our example, we will use the `:basic-dataset` and `:basic` parameters. The
    `:basic-dataset` keyword can be used to create training data, and the `:basic`
    keyword can be used to specify a set of input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data defined by the `dataset` variable and the `train-network` function,
    we can train the ANN''s `MLP` to model the output of an XOR gate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown by the preceding output, the trained ANN has an error of about 3.16
    percent. We can now use the trained ANN to predict the output of a set of input
    values. To do this, we use the Java `compute` and `getData` methods, which are
    specified by `.compute` and `.getData` respectively. We can define a simple helper
    function to call the `.compute` method for a vector of input values and round
    the output to a binary value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `run-network` function to test the trained ANN using a vector
    of input values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the trained ANN represented by `MLP` completely
    matches the behavior of an XOR gate.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the Enclog library gives us a small set of powerful functions
    that can be used to build ANNs. In the preceding example, we explored a feed-forward
    multilayer perceptron model. The library provides several other ANN models, such
    as **Adaptive Resonance Theory** (**ART**), **Self-Organizing Maps** (**SOM**),
    and Elman networks. The Enclog library also allows us to customize the activation
    function of the nodes in a particular neural network model. For the feed-forward
    network in our example, we've used the sigmoid function. Several mathematical
    functions, such as sine, hyperbolic tan, logarithmic, and linear functions, are
    also supported by the library. There are also several machine learning algorithms
    supported by the Enclog library that can be used to train an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now switch our focus to recurrent neural networks and study a simple
    recurrent neural network model. An **Elman neural network** is a simple recurrent
    ANN with a single input, output, and hidden layer. There is also an extra *context
    layer* of neural nodes. Elman neural networks are used to simulate short-term
    memory in supervised and unsupervised machine learning problems. Enclog does include
    support for Elman neural networks, and we will demonstrate how we can build an
    Elman neural network using the Enclog library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The context layer of an Elman neural network receives unweighted inputs from
    the hidden layer of the ANN. In this way, the ANN can remember the previous values
    that we generated using the hidden layer and use these values to affect the predicted
    value. Thus, the context layer serves as a type of short-term memory for the ANN.
    An Elman neural network can be illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding recurrent neural networks](img/4351OS_04_67.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The structure of an Elman network, as depicted by the preceding diagram, resembles
    that of a feed-forward multilayer perceptron ANN. An Elman network adds an extra
    context layer of neural nodes to the ANN. The Elman network illustrated in the
    preceding diagram takes two inputs and produces two outputs. The input and hidden
    layers of the Elman network add an extra bias input, similar to a multilayer perceptron.
    The activations of the hidden layers' neurons are fed directly to the two context
    nodes ![Understanding recurrent neural networks](img/4351OS_04_68.jpg) and ![Understanding
    recurrent neural networks](img/4351OS_04_69.jpg). The values stored in these context
    nodes are then used later by the nodes in the hidden layer of the ANN to recollect
    the previous activations to determine the new activation values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create an Elman network that specifies the `:elman` keyword to the `neural-pattern`
    function from the Enclog library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the Elman network, we can use the resilient propagation algorithm
    (for more information, refer to *Empirical Evaluation of the Improved Rprop Learning
    Algorithm*). This algorithm can also be used to train other recurrent networks
    supported by Enclog. Interestingly, the resilient propagation algorithm can be
    used to train feed-forward networks as well. This algorithm also performs significantly
    better than the backpropagation learning algorithm. Although a complete description
    of this algorithm is beyond the scope of this book, the reader is encouraged to
    learn more about this learning algorithm. The resilient propagation algorithm
    is specified as the `:resilient-prop` keyword to the `train-network` function,
    which we had defined earlier. We can train the Elman neural network using the
    `train-network` function and the `dataset` variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the resilient propagation algorithm requires
    a relatively smaller number of iterations in comparison to the backpropagation
    algorithm. We can now use this trained ANN to simulate an XOR gate just like we
    did in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, recurrent neural network models and training algorithms are the
    other useful models that can be used to model classification or regression problems
    using ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Building SOMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SOM (pronounced as **ess-o-em**) is another interesting ANN model that is useful
    for unsupervised learning. SOMs are used in several practical applications such
    as handwriting and image recognition. We will also revisit SOMs when we discuss
    clustering in [Chapter 7](ch07.html "Chapter 7. Clustering Data"), *Clustering
    Data*.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the sample data contains no expected output values,
    and the ANN must recognize and match patterns from the input data entirely on
    its own. SOMs are used for *competitive learning*, which is a special class of
    unsupervised learning in which the neurons in the output layer of the ANN compete
    among themselves for activation. The activated neuron determines the final output
    value of the ANN, and hence, the activated neuron is also termed as a **winning
    neuron**.
  prefs: []
  type: TYPE_NORMAL
- en: Neurobiological studies have shown that different sensory inputs sent to the
    brain are mapped to the corresponding areas of the brain's *cerebral cortex* in
    an orderly pattern. Thus, neurons that deal with closely related operations are
    kept close together. This is known as the **principle of topographic formation**,
    and SOMs are, in fact, modeled on this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'An SOM essentially transforms input data with a large number of dimensions
    to a low-dimensional discrete map. The SOM is trained by placing the neurons at
    the nodes of this map. This internal map of the SOM usually has one or two dimensions.
    The neurons in the SOM become *selectively tuned* to the patterns in the input
    values. When a particular neuron in the SOM is activated for a particular input
    pattern, its neighboring neurons tend to get more excited and more tuned to the
    pattern in the input values. This behavior is termed as **lateral interaction**
    of a set of neurons. An SOM, thus, finds patterns in the input data. When a similar
    pattern is found in a set of inputs, the SOM recognizes this pattern. The layers
    of neural nodes in an SOM can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building SOMs](img/4351OS_04_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An SOM has an input layer and a computational layer, as depicted by the preceding
    diagram. The computational layer is also termed as the **feature map** of the
    SOM. The input nodes map the input values to several neurons in the computational
    layer. Each node in the computational layer has its output connected to its neighboring
    node, and each of these connections has a weight associated with it. These weights
    are termed as the **connection weights** of the feature map. The SOM remembers
    patterns in the input values by adjusting the connection weights of the nodes
    in its computational layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-organizing process of an SOM can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The connection weights are first initialized to random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each input pattern, the neural nodes in the computational layer compute
    a value using a discriminant function. These values are then used to decide the
    winning neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neuron with the least value for the discriminant function is selected, and
    the connection weights to its surrounding neurons are modified to be activated
    for similar patterns in the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weights must be modified such that the value produced by the discriminant
    function for the neighboring nodes is reduced for the given pattern in the input.
    Thus, the winning node and its surrounding nodes produce higher output or activation
    values for similar patterns in the input data. The amount of change by which the
    weights are adjusted depends on the learning rate specified to the training algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given number of dimensions *D* in the input data, the discriminant function
    can be formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building SOMs](img/4351OS_04_71.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the term ![Building SOMs](img/4351OS_04_72.jpg) is
    the weight vector of the ![Building SOMs](img/4351OS_04_73.jpg) neuron in the
    SOM. The length of the vector ![Building SOMs](img/4351OS_04_72.jpg) is equal
    to the number of neurons connected to the ![Building SOMs](img/4351OS_04_73.jpg)
    neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have selected the winning neuron in an SOM, we must select the neighboring
    neurons of the winning neuron. We must adjust the weights of these neighboring
    neurons along with the weight of the winning neuron. A variety of schemes can
    be used for the selection of the winning neuron's neighboring nodes. In the simplest
    case, we can select a single neighboring neuron.
  prefs: []
  type: TYPE_NORMAL
- en: We can alternatively use the `bubble` function or the `radial bias` function
    to select a group of neighboring neurons surrounding the winning neuron (for more
    information, refer to *Multivariable functional interpolation and adaptive networks*).
  prefs: []
  type: TYPE_NORMAL
- en: 'To train an SOM, we must perform the following steps as part of the training
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the weights of the nodes in the computational layer to random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a sample input pattern from the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the winning neuron for the selected set of input patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights of the winning neuron and its surrounding nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 to 4 for all the samples in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Enclog library does support the SOM neural network model and training algorithm.
    We can create and train an SOM from the Enclog library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `som` variable appearing in the preceding code represents an SOM. The `train-som`
    function can be used to train the SOM. The SOM training algorithm is specified
    as `:basic-som`. Note that we specify the learning rate as `0.7` using the `:learning-rate`
    key.
  prefs: []
  type: TYPE_NORMAL
- en: The `:neighborhood-fn` key passed to the `trainer` function in the preceding
    code specifies how we select the neighbors of the winning node in the SOM for
    a given set of input values. We specify that a single neighboring node of the
    winning node must be selected with the help of `(neighborhood-F :single)`. We
    can also specify different neighborhood functions. For example, we can specify
    the `bubble` function as `:bubble` or the `radial basis` function as `:rbf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `train-som` function to train the SOM with some input patterns.
    Note that the training data to be used to train the SOM will not have any output
    values. The SOM must recognize patterns in the input data on its own. Once the
    SOM is trained, we can use the Java `classify` method to detect patterns in the
    input. For the following example, we provide only two input patterns to train
    the SOM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run the `train-and-run-som` function defined in the preceding code and
    observe that the SOM recognizes the two input patterns in the training data as
    two distinct classes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, SOMs are a great model for dealing with unsupervised learning
    problems. Also, we can easily build SOMs to model such problems using the Enclog
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have explored a few interesting ANN models in this chapter. These models
    can be applied to solve both supervised and unsupervised machine learning problems.
    The following are some of the other points that we covered:'
  prefs: []
  type: TYPE_NORMAL
- en: We have explored the necessity of ANNs and their broad types, that is, feed-forward
    and recurrent ANNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have studied the multilayer perceptron ANN and the backpropagation algorithm
    used to train this ANN. We've also provided a simple implementation of the backpropagation
    algorithm in Clojure using matrices and matrix operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have introduced the Enclog library that can be used to build ANNs. This library
    can be used to model both supervised and unsupervised machine learning problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have explored recurrent Elman neural networks, which can be used to produce
    ANNs with a small error in a relatively less number of iterations. We've also
    described how we can create and train such an ANN using the Enclog library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced SOMs, which are neural networks that can be applied in the domain
    of unsupervised learning. We've also described how we can create and train an
    SOM using the Enclog library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
