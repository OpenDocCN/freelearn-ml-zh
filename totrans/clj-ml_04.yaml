- en: Chapter 4. Building Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 建立神经网络
- en: In this chapter, we will introduce **Artificial Neural Networks** (**ANNs**).
    We will study the basic representation of ANNs and then discuss several ANN models
    that can be used in both supervised and unsupervised machine learning problems.
    We also introduce the **Enclog** Clojure library to build ANNs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**人工神经网络**（**ANNs**）。我们将研究ANNs的基本表示，然后讨论可用于监督和未监督机器学习问题的多个ANN模型。我们还介绍了**Enclog**
    Clojure库来构建ANNs。
- en: Neural networks are well suited for finding patterns in some given data and
    have several practical applications, such as handwriting recognition and machine
    vision, in computing. ANNs are often combined or interconnected to model a given
    problem. Interestingly, they can be applied to several machine learning problems,
    such as regression and classification. ANNs have applications in several areas
    in computing and are not restricted to the scope of machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常适合在给定的数据中寻找模式，并在计算领域有多个实际应用，例如手写识别和机器视觉。人工神经网络（ANNs）通常被组合或相互连接来模拟给定的问题。有趣的是，它们可以应用于多个机器学习问题，如回归和分类。ANNs在计算领域的多个领域都有应用，并不局限于机器学习的范围。
- en: '**Unsupervised learning** is a form of machine learning in which the given
    training data doesn''t contain any information about which class a given sample
    of input belongs to. As the training data is *unlabeled*, an unsupervised learning
    algorithm must determine the various categories in the given data completely on
    its own. Generally, this is done by seeking out similarities between different
    pieces of data and then grouping the data into several categories. This technique
    is called **cluster analysis,** and we shall study more about this methodology
    in the following chapters. ANNs are used in unsupervised machine learning techniques
    mostly due to their ability to quickly recognize patterns in some unlabeled data.
    This specialized form of unsupervised learning exhibited by ANNs is termed as
    **competitive learning**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**是一种机器学习方法，其中给定的训练数据不包含任何关于给定输入样本属于哪个类别的信息。由于训练数据是*未标记的*，无监督学习算法必须完全依靠自己确定给定数据中的各种类别。通常，这是通过寻找不同数据之间的相似性，然后将数据分组到几个类别中实现的。这种技术称为**聚类分析**，我们将在以下章节中更详细地研究这种方法。ANNs在无监督机器学习技术中的应用主要是由于它们能够快速识别某些未标记数据中的模式。这种ANNs表现出的特殊形式的无监督学习被称为**竞争学习**。'
- en: An interesting fact about ANNs is that they are modeled from the structure and
    behavior of the central nervous system of higher-order animals that demonstrate
    learning capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ANNs的一个有趣的事实是，它们是从具有学习能力的更高阶动物的中枢神经系统的结构和行为中建模的。
- en: Understanding nonlinear regression
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解非线性回归
- en: 'By this time, the reader must be aware of the fact that the gradient descent
    algorithm can be used to estimate both linear and logistic regression models for
    regression and classification problems. An obvious question would be: what is
    the need of neural networks when we can use gradient descent to estimate linear
    regression and logistic regression models from the training data? To understand
    the necessity of ANNs, we must first understand *nonlinear regression*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，读者必须意识到梯度下降算法可以用来估计回归和分类问题的线性回归和逻辑回归模型。一个明显的问题会是：当我们可以使用梯度下降从训练数据中估计线性回归和逻辑回归模型时，为什么还需要神经网络？为了理解ANNs的必要性，我们首先必须理解*非线性回归*。
- en: 'Let''s assume that we have a single feature variable *X* and a dependent variable
    *Y* that varies with *X*, as shown in the following plot:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个单个特征变量*X*和一个随*X*变化的因变量*Y*，如下面的图所示：
- en: '![Understanding nonlinear regression](img/4351OS_04_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![理解非线性回归](img/4351OS_04_01.jpg)'
- en: As illustrated in the preceding plot, it's hard, if not impossible, to model
    the dependent variable *Y* as a linear equation of the independent variable *X*.
    We could model the dependent variable *Y* to be a high-order polynomial equation
    of the dependent variable *X*, thus converting the problem into the standard form
    of linear regressions. Hence, the dependent variable *Y* is said to vary nonlinearly
    with the independent variable *X*. Of course, there is also a good chance that
    data cannot be modeled using a polynomial function either.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，将因变量 *Y* 模型化为自变量 *X* 的线性方程是困难的，甚至是不可能的。我们可以将因变量 *Y* 模型为自变量 *X* 的高阶多项式方程，从而将问题转化为线性回归的标准形式。因此，说因变量
    *Y* 非线性地随自变量 *X* 变化。当然，也有可能数据无法使用多项式函数进行建模。
- en: It can also be shown that calculating the weights or coefficients of all the
    terms in a polynomial function using gradient descent has a time complexity of
    ![Understanding nonlinear regression](img/4351OS_04_02.jpg), where *n* is the
    number of features in the training data. Similarly, the algorithmic complexity
    of calculating the coefficients of all the terms in a third-order polynomial equation
    is ![Understanding nonlinear regression](img/4351OS_04_03.jpg). It's apparent
    that the time complexity of gradient descent increases geometrically with the
    number of features of the model. Thus, gradient descent on its own is not efficient
    enough to model nonlinear regression models with a large number of features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以证明，使用梯度下降法计算多项式函数中所有项的权重或系数的时间复杂度为 ![理解非线性回归](img/4351OS_04_02.jpg)，其中 *n*
    是训练数据中的特征数量。同样，计算三次多项式方程中所有项的系数的算法复杂度为 ![理解非线性回归](img/4351OS_04_03.jpg)。很明显，梯度下降的时间复杂度随着模型特征数量的增加而呈几何级数增长。因此，仅使用梯度下降本身不足以对具有大量特征的非线性回归模型进行建模。
- en: ANNs, on the other hand, are very efficient at modeling nonlinear regression
    models of data with a high number of features. We will now study the foundational
    ideas of ANNs and several ANN models that can be used in supervised and unsupervised
    learning problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，神经网络在建模具有大量特征的数据的非线性回归模型方面非常高效。我们现在将研究神经网络的基础理念以及可用于监督学习和无监督学习问题的几种神经网络模型。
- en: Representing neural networks
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示神经网络
- en: 'ANNs are modeled from the behavior of the central nervous system of organisms,
    such as mammals and reptiles, that are capable of learning. The central nervous
    system of these organisms comprises the organism''s brain, spinal cord, and a
    network of supporting neural tissues. The brain processes information and generates
    electric signals that are transported through the network of neural fibers to
    the various organs of the organism. Although the organism''s brain performs a
    lot of complex processing and control, it is actually a collection of neurons.
    The actual processing of sensory signals, however, is performed by several complex
    combinations of these neurons. Of course, each neuron is capable of processing
    an extremely small portion of the information processed by the brain. The brain
    actually functions by routing electrical signals from the various sensory organs
    of the body to its motor organs through this complex network of neurons. An individual
    neuron has a cellular structure as illustrated in the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（ANNs）是从能够学习的生物体（如哺乳动物和爬行动物）的中枢神经系统行为中建模的。这些生物体的中枢神经系统包括生物体的脑、脊髓和一系列支持性神经组织。大脑处理信息并产生电信号，这些信号通过神经网络纤维传输到生物体的各个器官。尽管生物体的脑执行了许多复杂的处理和控制，但实际上它是由神经元组成的集合。然而，实际处理感觉信号的是这些神经元的一些复杂组合。当然，每个神经元都能够处理大脑处理的信息的极小部分。大脑实际上是通过将来自身体各种感觉器官的电信号通过这个复杂的神经元网络路由到其运动器官来发挥作用的。以下图示了单个神经元的细胞结构：
- en: '![Representing neural networks](img/4351OS_04_04.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![表示神经网络](img/4351OS_04_04.jpg)'
- en: A neuron has several dendrites close to the nucleus of the cell and a single
    *axon* that transports signals from the nucleus of the cell. The dendrites are
    used to receive signals from other neurons and can be thought of as the input
    to the neuron. Similarly, the axon of the neuron is analogous to the output of
    the neuron. The neuron can thus be mathematically represented as a function that
    processes several inputs and produces a single output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元有几个接近细胞核的树突和一个单一的*轴突*，轴突用于从细胞的核传递信号。树突用于接收来自其他神经元的信号，可以将其视为神经元的输入。同样，神经元的轴突类似于神经元的输出。因此，神经元可以用一个处理多个输入并产生单个输出的函数来数学地表示。
- en: Several of these neurons are interconnected, and this network is termed as a
    **neural network**. A neuron essentially performs its function by relaying weak
    electrical signals from and to other neurons. The interconnecting space between
    two neurons is called a **synapse**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些神经元是相互连接的，这种网络被称为**神经网络**。神经元通过从其他神经元中传递微弱的电信号来执行其功能。两个神经元之间的连接空间称为**突触**。
- en: 'An ANN comprises several interconnected neurons. Each neuron can be represented
    by a mathematical function that consumes several input values and produces an
    output value, as shown in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工神经网络（ANN）由多个相互连接的神经元组成。每个神经元都可以用一个数学函数来表示，该函数消耗多个输入值并产生一个输出值，如下面的图所示：
- en: '![Representing neural networks](img/4351OS_04_05.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![表示神经网络](img/4351OS_04_05.jpg)'
- en: A single neuron can be illustrated by the preceding diagram. In mathematical
    terms, it's simply a function ![Representing neural networks](img/4351OS_04_06.jpg)
    that maps a set of input values ![Representing neural networks](img/4351OS_04_07.jpg)
    to an output value ![Representing neural networks](img/4351OS_04_08.jpg). The
    function ![Representing neural networks](img/4351OS_04_06.jpg) is called the **activation
    function** of the neuron, and its output value ![Representing neural networks](img/4351OS_04_08.jpg)
    is called the **activation of the neuron**. This representation of a neuron is
    termed as a **perceptron**. Perceptron can be used on its own and is effective
    enough to estimate supervised machine learning models, such as linear regression
    and logistic regression. However, complex nonlinear data can be better modeled
    with several interconnected perceptrons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用前面的图来表示单个神经元。从数学的角度来看，它只是一个将一组输入值![表示神经网络](img/4351OS_04_07.jpg)映射到输出值![表示神经网络](img/4351OS_04_08.jpg)的函数![表示神经网络](img/4351OS_04_06.jpg)。这个函数![表示神经网络](img/4351OS_04_06.jpg)被称为神经元的**激活函数**，其输出值![表示神经网络](img/4351OS_04_08.jpg)被称为神经元的**激活**。这种神经元的表示称为**感知器**。感知器可以单独使用，并且足够有效，可以估计监督机器学习模型，如线性回归和逻辑回归。然而，复杂非线性数据可以用多个相互连接的感知器更好地建模。
- en: 'Generally, a bias input is added to the set of input values supplied to a perceptron.
    For the input values ![Representing neural networks](img/4351OS_04_07.jpg), we
    add the term ![Representing neural networks](img/4351OS_04_09.jpg) as a bias input
    such that ![Representing neural networks](img/4351OS_04_10.jpg). A neuron with
    this added bias value can be illustrated by the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，会将一个偏差输入添加到供给感知器的输入值集合中。对于输入值![表示神经网络](img/4351OS_04_07.jpg)，我们添加项![表示神经网络](img/4351OS_04_09.jpg)作为偏差输入，使得![表示神经网络](img/4351OS_04_10.jpg)。具有这个附加偏差值的神经元可以用以下图来表示：
- en: '![Representing neural networks](img/4351OS_04_11.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![表示神经网络](img/4351OS_04_11.jpg)'
- en: 'Each input value ![Representing neural networks](img/4351OS_04_12.jpg) supplied
    to the perceptron has an associated weight ![Representing neural networks](img/4351OS_04_13.jpg).
    This weight is analogous to the coefficients of the features of a linear regression
    model. The activation function is applied to these weights and their corresponding
    input values. We can formally define the estimated output value ![Representing
    neural networks](img/4351OS_04_06.jpg) of the perceptron in terms of the input
    values, their weights, and the perceptron''s activation function as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 供给感知器的每个输入值![表示神经网络](img/4351OS_04_12.jpg)都有一个相关的权重![表示神经网络](img/4351OS_04_13.jpg)。这个权重与线性回归模型特征的系数类似。激活函数应用于这些权重及其相应的输入值。我们可以如下形式地定义感知器的估计输出值![表示神经网络](img/4351OS_04_06.jpg)：
- en: '![Representing neural networks](img/4351OS_04_14.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![表示神经网络](img/4351OS_04_14.jpg)'
- en: The activation function to be used by the nodes of an ANN depends greatly on
    the sample data that has to be modeled. Generally, the **sigmoid** or **hyperbolic
    tangent** functions are used as the activation function for classification problems
    (for more information, refer to *Wavelet Neural Network (WNN) approach for calibration
    model building based on gasoline near infrared (NIR) spectr*). The sigmoid function
    is said to be *activated* for a given threshold input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ANN（人工神经网络）的节点所使用的激活函数很大程度上取决于需要建模的样本数据。通常，**Sigmoid**或**双曲正切**函数被用作分类问题的激活函数（更多信息，请参阅*基于汽油近红外（NIR）光谱的校准模型构建的波形神经网络（WNN）方法*）。据说Sigmoid函数在给定的阈值输入时会被**激活**。
- en: 'We can plot the variance of the sigmoid function to depict this behavior, as
    shown in the following plot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制Sigmoid函数的方差来描述这种行为，如下面的图表所示：
- en: '![Representing neural networks](img/4351OS_04_15.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![表示神经网络](img/4351OS_04_15.jpg)'
- en: ANNs can be broadly classified into *feed-forward neural networks* and *recurrent
    neural networks* (for more information, refer to *Bidirectional recurrent neural
    networks*). The difference between these two types of ANNs is that in feed-forward
    neural networks, the connections between the nodes of the ANN do not form a directed
    cycle as opposed to recurrent neural networks where the node interconnections
    do form a directed cycle. Thus, in feed-forward neural networks, each node in
    a given layer of the ANN receives input only from the nodes in the immediate previous
    layer in the ANN.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ANN可以广泛地分为**前馈神经网络**和**循环神经网络**（更多信息，请参阅*双向循环神经网络*）。这两种类型ANN之间的区别在于，在前馈神经网络中，ANN节点的连接不形成一个有向循环，而循环神经网络中节点之间的连接则形成一个有向循环。因此，在前馈神经网络中，ANN给定层的每个节点只接收来自ANN中直接前一层的节点的输入。
- en: There are several ANN models that have practical applications, and we will explore
    a few of them in this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种ANN模型具有实际应用，我们将在本章中探讨其中的一些。
- en: Understanding multilayer perceptron ANNs
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解多层感知器ANN
- en: We now introduce a simple model of feed-forward neural networks—the **Multilayer
    Perceptron** model. This model represents a basic feed-forward neural network
    and is versatile enough to model regression and classification problems in the
    domain of supervised learning. All the input flows through a feed-forward neural
    network in a single direction. This is a direct consequence of the fact that there
    is no *feedback* from or to any layer in a feed-forward neural network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍一个简单的**前馈神经网络**模型——**多层感知器**模型。该模型代表了一个基本的前馈神经网络，并且足够灵活，可以用于监督学习领域中回归和分类问题的建模。所有输入都通过一个前馈神经网络以单一方向流动。这是没有从或向任何层反馈的事实的一个直接后果。
- en: By feedback, we mean that the output of a given layer is fed back as input to
    the perceptrons in a previous layer in the ANN. Also, using a single layer of
    perceptrons would mean using only a single activation function, which is equivalent
    to using *logistic regression* to model the given training data. This would mean
    that the model cannot be used to fit nonlinear data, which is the primary motivation
    of ANNs. We must note that we discussed logistic regression in [Chapter 3](ch03.html
    "Chapter 3. Categorizing Data"), *Categorizing Data*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反馈，我们指的是给定层的输出被反馈作为ANN中前一层的感知器的输入。此外，使用单层感知器意味着只使用一个激活函数，这相当于使用**逻辑回归**来建模给定的训练数据。这意味着该模型不能用于拟合非线性数据，而这正是ANN的主要动机。我们必须注意，我们在[第3章](ch03.html
    "第3章。数据分类")*数据分类*中讨论了逻辑回归。
- en: 'A multilayer perceptron ANN can be illustrated by the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器ANN可以通过以下图表来表示：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_16.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_16.jpg)'
- en: A multilayer perceptron ANN comprises several layers of perceptron nodes. It
    exhibits a single input layer, a single output layer, and several hidden layers
    of perceptrons. The input layer simply relays the input values to the first hidden
    layer of the ANN. These values are then propagated to the output layer through
    the other hidden layers, where they are weighted and summed using the activation
    function, to finally produce the output values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器ANN由多个感知器节点层组成。它具有单个输入层、单个输出层和多个隐藏层。输入层只是将输入值传递给ANN的第一个隐藏层。然后这些值通过其他隐藏层传播到输出层，在这些层中使用激活函数进行加权求和，最终产生输出值。
- en: Each sample in the training data is represented by the ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_17.jpg) tuple, where ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_18.jpg) is the expected output and ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_19.jpg) is the input value of the ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_20.jpg) training sample. The ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_19.jpg) input vector comprises a number
    of values equal to the number of features in the training data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中的每个样本由![理解多层感知器ANN](img/4351OS_04_17.jpg)元组表示，其中![理解多层感知器ANN](img/4351OS_04_18.jpg)是期望输出，![理解多层感知器ANN](img/4351OS_04_19.jpg)是![理解多层感知器ANN](img/4351OS_04_20.jpg)训练样本的输入值。![理解多层感知器ANN](img/4351OS_04_19.jpg)输入向量包含与训练数据中特征数量相等的值。
- en: 'The output of each node is termed as the **activation** of the node and is
    represented by the term ![Understanding multilayer perceptron ANNs](img/4351OS_04_21.jpg)
    for the ![Understanding multilayer perceptron ANNs](img/4351OS_04_20.jpg) node
    in the layer ![Understanding multilayer perceptron ANNs](img/4351OS_04_22.jpg).
    As we mentioned earlier, the activation function used to produce this value is
    the sigmoid function or the hyperbolic tangent function. Of course, any other
    mathematical function could be used to fit the sample data. The input layer of
    a multilayer perceptron network simply adds a bias input to the input values,
    and the set of inputs supplied to the ANN are relayed to the next layer. We can
    formally represent this equality as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点的输出被称为该节点的**激活**，对于![理解多层感知器ANN](img/4351OS_04_20.jpg)层中的![理解多层感知器ANN](img/4351OS_04_21.jpg)节点，用术语![理解多层感知器ANN](img/4351OS_04_22.jpg)表示。正如我们之前提到的，用于生成此值的激活函数是Sigmoid函数或双曲正切函数。当然，任何其他数学函数都可以用来拟合样本数据。多层感知器网络的输入层只是将一个偏置输入加到输入值上，并将提供给ANN的输入集传递到下一层。我们可以用以下等式正式表示这个关系：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_23.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_23.jpg)'
- en: The synapses between every pair of layers in the ANN have an associated weight
    matrix. The number of rows in these matrices is equal to the number of input values,
    that is, the number of nodes in the layer closer to the input layer of the ANN
    and the number of columns equal to the number of nodes in the layer of the synapse
    that is closer to the output layer of the ANN. For a layer *l*, the weight matrix
    is represented by the term ![Understanding multilayer perceptron ANNs](img/4351OS_04_24.jpg).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ANN中每一对层之间的突触都有一个相关的权重矩阵。这些矩阵的行数等于输入值的数量，即ANN输入层附近层的节点数，列数等于突触层中靠近ANN输出层的节点数。对于层*l*，权重矩阵用术语![理解多层感知器ANN](img/4351OS_04_24.jpg)表示。
- en: 'The activation values of a layer *l* can be determined using the activation
    function of the ANN. The activation function is applied on the products of the
    weight matrix and the activation values produced by the previous layer in the
    ANN. Generally, the activation function used for a multilayer perceptron is a
    sigmoid function. This equality can be formally represented as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 层*l*的激活值可以使用ANN的激活函数来确定。激活函数应用于权重矩阵和由ANN中前一层的激活值产生的乘积。通常，用于多层感知器的激活函数是Sigmoid函数。这个等式可以正式表示如下：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_25.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_25.jpg)'
- en: Generally, the activation function used for a multilayer perceptron is a sigmoid
    function. Note that we do not add a bias value in the output layer of an ANN.
    Also, the output layer can produce any number of output values. To model a *k-class*
    classification problem, we would require an ANN producing *k* output values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，用于多层感知器的激活函数是Sigmoid函数。请注意，我们不在ANN的输出层中添加偏置值。此外，输出层可以产生任意数量的输出值。为了建模一个*k*类分类问题，我们需要一个产生*k*个输出值的ANN。
- en: To perform binary classification, we can only model a maximum of two classes
    of input data. The output value generated by an ANN used for binary classification
    is always 0 or 1\. Thus, for ![Understanding multilayer perceptron ANNs](img/4351OS_04_26.jpg)
    classes, ![Understanding multilayer perceptron ANNs](img/4351OS_04_27.jpg).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行二元分类，我们只能对最多两个类别的输入数据进行建模。用于二元分类的ANN生成的输出值总是0或1。因此，对于![理解多层感知器ANN](img/4351OS_04_26.jpg)类，![理解多层感知器ANN](img/4351OS_04_27.jpg)。
- en: 'We can also model a multiclass classification using the *k* binary output values,
    and thus, the output of the ANN is a ![Understanding multilayer perceptron ANNs](img/4351OS_04_28.jpg)
    matrix. This can be formally expressed as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用**k**个二进制输出值来模拟多类分类，因此，人工神经网络（ANN）的输出是一个![理解多层感知器ANN](img/4351OS_04_28.jpg)矩阵。这可以形式化地表达如下：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_29.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_29.jpg)'
- en: Hence, we can use a multilayer perceptron ANN to perform binary and multiclass
    classifications. A multilayer perceptron ANN can be trained using the **backpropagation**
    **algorithm**, which we will study and implement later in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用多层感知器ANN来执行二类和多类分类。多层感知器ANN可以使用**反向传播**算法进行训练，我们将在本章后面学习和实现它。
- en: Let's assume that we want to model the behavior of a logical XOR gate. An XOR
    gate can be thought of a binary classifier that requires two inputs and generates
    a single output. An ANN that models the XOR gate would have a structure as shown
    in the following diagram. Interestingly, linear regression can be used to model
    both AND and OR logic gates but cannot be used to model an XOR gate. This is due
    to the nonlinear nature of the output of an XOR gate, and thus, ANNs are used
    to overcome this limitation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要模拟逻辑异或门的行为。异或门可以被看作是一个需要两个输入并生成单个输出的二进制分类器。模拟异或门的人工神经网络将具有以下图中所示的结构。有趣的是，线性回归可以用来模拟AND和OR逻辑门，但不能用来模拟异或门。这是因为异或门输出的非线性特性，因此，ANN被用来克服这一限制。
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_30.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_30.jpg)'
- en: The multilayer perceptron illustrated in the preceding diagram has three nodes
    in the input layer, four nodes in the hidden layer, and one node in the output
    layer. Observe that every layer other than the output layer adds a bias input
    to the set of input values for the nodes in the next layer. There are two synapses
    in the ANN, shown in the preceding diagram, and they are associated with the weight
    matrices ![Understanding multilayer perceptron ANNs](img/4351OS_04_31.jpg) and
    ![Understanding multilayer perceptron ANNs](img/4351OS_04_32.jpg). Note that the
    first synapse is between the input layer and hidden layer, and the second synapse
    is between the hidden layer and the output layer. The weight matrix ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_31.jpg) has a size of ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_33.jpg), and the weight matrix ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_32.jpg) has a size of ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_34.jpg). Also, the term ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_35.jpg) is used to represent all the
    weight matrices in the ANN.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前图中所示的多层感知器有三个输入层节点，四个隐藏层节点和一个输出层节点。观察发现，除了输出层之外，每个层都会向下一层的节点输入值集合中添加一个偏置输入。ANN中有两个突触，如图所示，它们与权重矩阵![理解多层感知器ANN](img/4351OS_04_31.jpg)和![理解多层感知器ANN](img/4351OS_04_32.jpg)相关联。请注意，第一个突触位于输入层和隐藏层之间，第二个突触位于隐藏层和输出层之间。权重矩阵![理解多层感知器ANN](img/4351OS_04_31.jpg)的大小为![理解多层感知器ANN](img/4351OS_04_33.jpg)，权重矩阵![理解多层感知器ANN](img/4351OS_04_32.jpg)的大小为![理解多层感知器ANN](img/4351OS_04_34.jpg)。此外，术语![理解多层感知器ANN](img/4351OS_04_35.jpg)用来表示ANN中的所有权重矩阵。
- en: 'As the activation function of each node in a multilayer perceptron ANN is a
    sigmoid function, we can define the cost function of the weights of the nodes
    of the ANN similar to the cost function of a logistic regression model. The cost
    function of an ANN can be defined in terms of the weight matrices as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多层感知器ANN中每个节点的激活函数是Sigmoid函数，我们可以将ANN节点权重的成本函数定义为与逻辑回归模型的成本函数类似。ANN的成本函数可以用权重矩阵来定义，如下所示：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_36.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_36.jpg)'
- en: 'The preceding cost function is essentially the average of the cost functions
    of each node in the output layer of an ANN (for more information, refer to *Neural
    Networks in Materials Science*). For a multilayer perceptron ANN with *K* output
    values, we perform the average over the *K* terms. Note that ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_37.jpg) represents the ![Understanding multilayer
    perceptron ANNs](img/4351OS_04_38.jpg) output value of the ANN, ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_39.jpg) represents the input variables
    of the ANN, and *N* is the number of sample values in the training data. The cost
    function is essentially that of logistic regression but is applied here for the
    *K* output values. We can add a regularization parameter to the preceding cost
    function and express the regularized cost function using the following equation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代价函数本质上是对ANN输出层中每个节点代价函数的平均值（更多信息，请参阅*材料科学中的神经网络*）。对于具有 *K* 个输出值的多层感知器ANN，我们对
    *K* 个项进行平均。请注意，![理解多层感知器ANN](img/4351OS_04_37.jpg) 代表ANN的![理解多层感知器ANN](img/4351OS_04_38.jpg)
    输出值，![理解多层感知器ANN](img/4351OS_04_39.jpg) 代表ANN的输入变量，*N* 是训练数据中的样本值数量。代价函数本质上与逻辑回归相同，但在这里应用于
    *K* 个输出值。我们可以在前面的代价函数中添加一个正则化参数，并使用以下方程表示正则化代价函数：
- en: '![Understanding multilayer perceptron ANNs](img/4351OS_04_40.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![理解多层感知器ANN](img/4351OS_04_40.jpg)'
- en: The cost function defined in the preceding equation adds a regularization term
    similar to that of logistic regression. The regularization term is essentially
    the sum of the squares of all weights of all input values of the several layers
    of the ANN, excluding the weights for the added bias input. Also, the term ![Understanding
    multilayer perceptron ANNs](img/4351OS_04_41.jpg) refers to the number of nodes
    in layer *l* of the ANN. An interesting point to note is that in the preceding
    regularized cost function, only the regularization term depends on the number
    of layers in the ANN. Hence, the *generalization* of the estimated model is based
    on the number of layers in the ANN.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中定义的代价函数添加了一个类似于逻辑回归的正则化项。正则化项本质上是由ANN的多个层中所有输入值的权重平方和组成的，但不包括添加的偏置输入的权重。此外，术语![理解多层感知器ANN](img/4351OS_04_41.jpg)指的是ANN中第
    *l* 层的节点数。值得注意的是，在前面的正则化代价函数中，只有正则化项依赖于ANN中的层数。因此，估计模型的*泛化能力*基于ANN中的层数。
- en: Understanding the backpropagation algorithm
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播算法
- en: 'The **backpropagation learning** algorithm is used to train a multilayer perceptron
    ANN from a given set of sample values. In brief, this algorithm first calculates
    the output value for a set of given input values and also calculates the amount
    of error in the output of the ANN. The amount of error in the ANN is determined
    by comparing the predicted output value of the ANN to the expected output value
    for the given input values from the training data provided to the ANN. The calculated
    error is then used to modify the weights of the ANN. Thus, after training the
    ANN with a reasonable number of samples, the ANN will be able to predict the output
    value for a set of input values. The algorithm comprises of three distinct phases.
    They are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播学习**算法用于从给定的一组样本值中训练一个多层感知器ANN。简而言之，该算法首先计算给定输入值集的输出值，并计算ANN输出的误差量。ANN中的误差量是通过将ANN的预测输出值与训练数据提供给ANN的给定输入值的预期输出值进行比较来确定的。然后，计算出的误差用于修改ANN的权重。因此，在用合理数量的样本训练ANN后，ANN将能够预测输入值集的输出值。该算法由三个不同的阶段组成。具体如下：'
- en: A forward propagation phase
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播阶段
- en: A backpropagation phase
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播阶段
- en: A weight update phase
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重更新阶段
- en: The weights of the synapses in the ANN are first initialized to random values
    within the ranges ![Understanding the backpropagation algorithm](img/4351OS_04_42.jpg)
    and ![Understanding the backpropagation algorithm](img/4351OS_04_43.jpg). We initialize
    the weights to values within this range to avoid a symmetry in the weight matrices.
    This avoidance of symmetry is called **symmetry breaking,** and it is performed
    so that each iteration of the backpropagation algorithm produces a noticeable
    change in the weights of the synapses in the ANN. This is desirable in an ANN
    as each of its node should learn independently of other nodes in the ANN. If all
    the nodes were to have identical weights, the estimated learning model will be
    either overfit or underfit.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ANN中突触的权重最初被初始化为在![理解反向传播算法](img/4351OS_04_42.jpg)和![理解反向传播算法](img/4351OS_04_43.jpg)范围内的随机值。我们将权重初始化为这个范围内的值，以避免权重矩阵中的对称性。这种避免对称性的做法称为**对称破缺**，其目的是使反向传播算法的每次迭代都能在ANN中突触的权重上产生明显的变化。这在人工神经网络中是可取的，因为每个节点都应该独立于ANN中的其他节点进行学习。如果所有节点都具有相同的权重，那么估计的学习模型可能会过拟合或欠拟合。
- en: Also, the backpropagation learning algorithm requires two additional parameters,
    which are the learning rate ![Understanding the backpropagation algorithm](img/4351OS_04_44.jpg)
    and the learning momentum ![Understanding the backpropagation algorithm](img/4351OS_04_45.jpg).
    We will see the effects of these parameters in the example later in this section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，反向传播学习算法还需要两个额外的参数，即学习率![理解反向传播算法](img/4351OS_04_44.jpg)和学习动量![理解反向传播算法](img/4351OS_04_45.jpg)。我们将在本节后面的示例中看到这些参数的影响。
- en: 'The forward propagation phase of the algorithm simply calculates the activation
    values of all nodes in the various layers of the ANN. As we mentioned earlier,
    the activation values of the nodes in the input layer are the input values and
    the bias input of the ANN. This can be formally defined by using the following
    equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的正向传播阶段简单地计算ANN各个层中所有节点的激活值。正如我们之前提到的，输入层中节点的激活值是ANN的输入值和偏置输入。这可以通过以下方程形式化定义：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_23.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_23.jpg)'
- en: 'Using these activation values from the input layer of the ANN, the activation
    of the nodes in the other layers of the ANN is determined. This is done by applying
    the activation function to the products of the weight matrix of a given layer
    and the activation values from the previous layer in the ANN. This can be formally
    expressed as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自人工神经网络（ANN）输入层的这些激活值，可以确定ANN其他层中节点的激活状态。这是通过将给定层的权重矩阵与ANN前一层中节点的激活值相乘，然后应用激活函数来实现的。这可以形式化地表示如下：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_46.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_46.jpg)'
- en: 'The preceding equation explains that the activation value of a layer *l* is
    equal to the activation function applied to the output (or activation) values
    of the previous layer and the given layer''s weight matrix. Next, the activation
    values of the output layer are *backpropagated*. By this, we mean that that the
    activation values are traversed from the output layer through the hidden layers
    to the input layer of the ANN. During this phase, we determine the amount of error
    or delta in each node in the ANN. The delta values of the output layer are determined
    by calculating the difference between the expected output values, ![Understanding
    the backpropagation algorithm](img/4351OS_04_18.jpg), and the activation values
    of the output layer, ![Understanding the backpropagation algorithm](img/4351OS_04_47.jpg).
    This difference calculation can be summarized by the following equation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程解释了层 *l* 的激活值等于将前一层（或激活）值和给定层的权重矩阵的输出（或激活）值应用激活函数的结果。接下来，输出层的激活值将被*反向传播*。通过这种方式，我们指的是激活值从输出层通过隐藏层传播到ANN的输入层。在这个阶段，我们确定ANN中每个节点的误差或delta值。输出层的delta值是通过计算期望输出值![理解反向传播算法](img/4351OS_04_18.jpg)和输出层的激活值![理解反向传播算法](img/4351OS_04_47.jpg)之间的差异来确定的。这种差异计算可以总结为以下方程：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_48.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_48.jpg)'
- en: 'The term ![Understanding the backpropagation algorithm](img/4351OS_04_49.jpg)
    of a layer *l* is a matrix of size ![Understanding the backpropagation algorithm](img/4351OS_04_50.jpg)
    where *j* is the number of nodes in layer *l*. This term can be formally defined
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 层*l*的术语![理解反向传播算法](img/4351OS_04_49.jpg)是一个大小为![理解反向传播算法](img/4351OS_04_50.jpg)的矩阵，其中*j*是层*l*中的节点数。这个术语可以正式定义为以下内容：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_51.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_51.jpg)'
- en: 'The delta terms of the layers other than the output layer of the ANN are determined
    by the following equality:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ANN中除了输出层之外的其他层的delta项由以下等式确定：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_52.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_52.jpg)'
- en: In the preceding equation, the binary operation ![Understanding the backpropagation
    algorithm](img/4351OS_04_53.jpg) is used to represent an element-wise multiplication
    of two matrices of equal size. Note that this operation is different from matrix
    multiplication, and an element-wise multiplication will return a matrix composed
    of the products of the elements with the same position in two matrices of equal
    size. The term ![Understanding the backpropagation algorithm](img/4351OS_04_54.jpg)
    represents the derivative of the activation function used in the ANN. As we are
    using the sigmoid function as our activation function, the term ![Understanding
    the backpropagation algorithm](img/4351OS_04_55.jpg) has the value ![Understanding
    the backpropagation algorithm](img/4351OS_04_56.jpg).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，二进制运算![理解反向传播算法](img/4351OS_04_53.jpg)用于表示两个相同大小的矩阵的逐元素乘法。请注意，这种运算与矩阵乘法不同，逐元素乘法将返回一个由两个相同大小矩阵中相同位置的元素乘积组成的矩阵。术语![理解反向传播算法](img/4351OS_04_54.jpg)表示在ANN中使用的激活函数的导数。由于我们使用sigmoid函数作为激活函数，因此术语![理解反向传播算法](img/4351OS_04_55.jpg)的值为![理解反向传播算法](img/4351OS_04_56.jpg)。
- en: Thus, we can calculate the delta values of all nodes in the ANN. We can use
    these delta values to determine the gradients of the synapses of the ANN. We now
    move on to the final weight update phase of the backpropagation algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以计算人工神经网络（ANN）中所有节点的delta值。我们可以使用这些delta值来确定ANN中突触的梯度。我们现在继续到反向传播算法的最后一步，即权重更新阶段。
- en: 'The gradients of the various synapses are first initialized to matrices with
    all the elements as 0\. The size of a gradient matrix of a given synapse is the
    same size as the weight matrix of the synapse. The gradient term ![Understanding
    the backpropagation algorithm](img/4351OS_04_57.jpg) represents the gradients
    of the synapse layer that is present immediately after layer *l* in the ANN. The
    initialization of the gradients of the synapses in the ANN is formally expressed
    as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 各个突触的梯度首先初始化为所有元素均为0的矩阵。给定突触的梯度矩阵的大小与该突触的权重矩阵的大小相同。梯度项![理解反向传播算法](img/4351OS_04_57.jpg)表示在ANN中位于层*l*之后的突触层的梯度。ANN中突触梯度的初始化可以正式表示如下：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_58.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_58.jpg)'
- en: 'For each sample value in the training data, we calculate the deltas and activation
    values of all nodes in the ANN. These values are added to the gradients of the
    synapses using the following equation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据中的每个样本值，我们计算ANN中所有节点的delta和激活值。这些值通过以下方程添加到突触的梯度中：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_59.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_59.jpg)'
- en: 'We then calculate the average of the gradients for all the sample values and
    use the delta and gradient values of a given layer to update the weight matrix
    as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算所有样本值的梯度的平均值，并使用给定层的delta和梯度值来更新权重矩阵，如下所示：
- en: '![Understanding the backpropagation algorithm](img/4351OS_04_60.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![理解反向传播算法](img/4351OS_04_60.jpg)'
- en: 'Thus, the learning rate and learning momentum parameters of the algorithm come
    into play only in the weight update phase. The preceding three equations represent
    a single iteration of the backpropagation algorithm. A large number of iterations
    must be performed until the overall error in the ANN converges to a small value.
    We can now summarize the backpropagation learning algorithm using the following
    steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法的学习率和学习动量参数仅在权重更新阶段发挥作用。前面的三个方程代表反向传播算法的单次迭代。必须执行大量迭代，直到ANN的整体误差收敛到一个很小的值。现在我们可以使用以下步骤总结反向传播学习算法：
- en: Initialize the weights of the synapses of the ANN to random values.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ANN的突触权重初始化为随机值。
- en: Select a sample value and forward propagate the sample values through several
    layers of the ANN to generate the activations of every node in the ANN.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个样本值，并通过ANN的几层前向传播样本值以生成ANN中每个节点的激活。
- en: Backpropagate the activations generated by the last layer of the ANN through
    the hidden layers and to the input layer of the ANN. Through this step, we calculate
    the error or delta of every node in the ANN.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ANN最后一层生成的激活反向传播通过隐藏层到ANN的输入层。通过这一步，我们计算ANN中每个节点的误差或delta。
- en: Calculate the product of the errors generated from step 3 with the synapse weights
    or input activations for all the nodes in the ANN. This step produces the gradient
    of weight for each node in the network. Each gradient is represented by a ratio
    or percentage.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从步骤3生成的误差与ANN中所有节点的突触权重或输入激活的乘积。这一步产生了网络中每个节点的权重梯度。每个梯度由一个比率或百分比表示。
- en: Calculate the changes in the weights of the synapse layers in the ANN using
    the gradients and deltas of a given layer in the ANN. These changes are then subtracted
    from the weights of the synapses in the ANN. This is essentially the weight update
    step of the backpropagation algorithm.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定层的梯度和delta来计算ANN中突触层权重的变化。然后，将这些变化从ANN中突触的权重中减去。这本质上是反向传播算法的权重更新步骤。
- en: Repeat steps 2 to 5 for the rest of the samples in the training data.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练数据中的其余样本重复步骤2到5。
- en: There are several distinct parts in the backpropagation learning algorithm,
    and we will now implement each part and combine it into a complete implementation.
    As the deltas and weights of the synapses and activations in an ANN can be represented
    by matrices, we can write a vectorized implementation of this algorithm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播学习算法有几个不同的部分，我们现在将实现每个部分并将它们组合成一个完整的实现。由于ANN中的突触和激活的delta和权重可以用矩阵表示，我们可以编写这个算法的向量化实现。
- en: Note
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that for the following example, we require functions from the `incanter.core`
    namespace from the Incanter library. The functions in this namespace actually
    use the Clatrix library for the representation of a matrix and its manipulation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于以下示例，我们需要从Incanter库的`incanter.core`命名空间中获取函数。这个命名空间中的函数实际上使用Clatrix库来表示矩阵及其操作。
- en: 'Let''s assume that we need to implement an ANN to model a logical XOR gate.
    The sample data is simply the truth table of the XOR gate and can be represented
    as a vector, shown as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要实现一个人工神经网络（ANN）来模拟逻辑异或（XOR）门。样本数据仅仅是异或门的真值表，可以表示为一个向量，如下所示：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each element defined in the preceding vector `sample-data` is itself a vector
    comprising other vectors for the input and output values of an XOR gate. We will
    use this vector as our training data for building an ANN. This is essentially
    a classification problem, and we will use ANNs to model it. In abstract terms,
    an ANN should be capable of performing both binary and multiclass classifications.
    We can define the protocol of an ANN as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面定义的向量 `sample-data` 中，每个元素本身也是一个向量，包含异或门的输入和输出值。我们将使用这个向量作为我们的训练数据来构建ANN。这本质上是一个分类问题，我们将使用ANN来模拟它。在抽象意义上，ANN应该能够执行二进制和多类分类。我们可以定义ANN的协议如下：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `NeuralNetwork` protocol defined in the preceding code has three functions.
    The `train-ann` function can be used to train the ANN and requires some sample
    data. The `run` and `run-binary` functions can be used on this ANN to perform
    multiclass and binary classifications, respectively. Both the `run` and `run-binary`
    functions require a set of input values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的 `NeuralNetwork` 协议有三个函数。`train-ann` 函数可以用来训练 ANN，并需要一些样本数据。`run` 和
    `run-binary` 函数可以用于此 ANN 来执行多类和二分类，分别。`run` 和 `run-binary` 函数都需要一组输入值。
- en: 'The first step of the backpropagation algorithm is the initialization of the
    weights of the synapses of the ANN. We can use the `rand` and `matrix` functions
    to generate these weights as a matrix, shown as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法的第一步是初始化 ANN 突触的权重。我们可以使用 `rand` 和 `matrix` 函数生成这些权重作为矩阵，如下所示：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `rand-list` function shown in the preceding code creates a list of random
    elements in the positive and negative range of `epsilon`. As we described earlier,
    we choose this range to break the symmetry of the weight matrix.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中显示的 `rand-list` 函数在 `epsilon` 的正负范围内创建一个随机元素列表。如我们之前所述，我们选择这个范围来打破权重矩阵的对称性。
- en: The `random-initial-weights` function generates several weight matrices for
    different layers of the ANN. As defined in the preceding code, the `layers` argument
    must be a vector of the sizes of the layers of the ANN. For an ANN with two nodes
    in the input layer, three nodes in the hidden layer, and one node in the output
    layer, we pass `layers` as `[2 3 1]` to the `random-initial-weights` function.
    Each weight matrix has a number of columns equal to the number of inputs and number
    of rows equal to the number of nodes in the next layer of the ANN. We set the
    number of columns in a weight matrix of a given layer to the number of inputs,
    plus an extra input for the bias of the neural layer. Note that we use a slightly
    different form of the `matrix` function. This form takes a single vector and partitions
    this vector into a matrix that has a number of columns as specified by second
    argument to this function. Thus, the vector passed to this form of the `matrix`
    function must have `(* rows cols)` elements, where `rows` and `cols` are the number
    of rows and columns, respectively, in the weight matrix.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`random-initial-weights` 函数为 ANN 的不同层生成多个权重矩阵。如前述代码中定义的，`layers` 参数必须是一个向量，包含
    ANN 各层的尺寸。对于一个输入层有两个节点、隐藏层有三个节点、输出层有一个节点的 ANN，我们将 `layers` 作为 `[2 3 1]` 传递给 `random-initial-weights`
    函数。每个权重矩阵的列数等于输入的数量，行数等于 ANN 下一个层的节点数。我们设置给定层的权重矩阵的列数为输入的数量，并额外添加一个用于神经网络偏置的输入。请注意，我们使用了一个稍微不同的
    `matrix` 函数形式。这种形式接受一个单一向量，并将该向量分割成一个矩阵，其列数由该函数的第二个参数指定。因此，传递给这种 `matrix` 函数的向量必须包含
    `(* rows cols)` 个元素，其中 `rows` 和 `cols` 分别是权重矩阵的行数和列数。'
- en: 'As we will need to apply the sigmoid function to all the activations of a layer
    in the ANN, we must define a function that applies the sigmoid function on all
    the elements in a given matrix. We can use the `div`, `plus`, `exp`, and `minus`
    functions from the `incanter.core` namespace to implement such a function, as
    shown in the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要将 sigmoid 函数应用于 ANN 中某一层的所有激活，我们必须定义一个函数，该函数将对给定矩阵中的所有元素应用 sigmoid 函数。我们可以使用
    `incanter.core` 命名空间中的 `div`、`plus`、`exp` 和 `minus` 函数来实现这样的函数，如下所示：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that all of the previously defined functions apply the corresponding arithmetic
    operation on all the elements in a given matrix and returns a new matrix.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有之前定义的函数都在给定矩阵的所有元素上执行相应的算术运算，并返回一个新的矩阵。
- en: 'We will also need to implicitly add a bias node to each layer in an ANN. This
    can be done by wrapping around the `bind-rows` function, which adds a row of elements
    to a matrix, as shown in the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要在 ANN 的每一层中隐式地添加一个偏置节点。这可以通过围绕 `bind-rows` 函数进行操作来实现，该函数向矩阵添加一行，如下所示：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since the bias value is always 1, we specify the row of elements as `[1]` to
    the `bind-rows` function.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于偏置值始终为 1，我们将元素行 `[1]` 指定给 `bind-rows` 函数。
- en: 'Using the functions defined earlier, we can implement forward propagation.
    We essentially have to multiply the weights of a given synapse between two layers
    in an ANN and then apply the sigmoid function on each of the generated activation
    values, as shown in the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前定义的函数，我们可以实现前向传播。我们本质上需要在人工神经网络（ANN）中两个层之间给定突触的权重相乘，然后对每个生成的激活值应用sigmoid函数，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we first define a `matrix-mult` function, which performs
    matrix multiplication and ensures that the result is a matrix. Note that to define
    `matrix-mult`, we use the `mmult` function instead of the `mult` function that
    multiplies the corresponding elements in two matrices of the same size.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先定义了一个`matrix-mult`函数，该函数执行矩阵乘法并确保结果是矩阵。请注意，为了定义`matrix-mult`，我们使用`mmult`函数而不是`mult`函数，后者用于乘以两个相同大小的矩阵中的对应元素。
- en: Using the `matrix-mult` and `sigmoid` functions, we can implement the forward
    propagation step between two layers in the ANN. This is done in the `forward-propagate-layer`
    function, which simply multiplies the matrices representing the weights of the
    synapse between two layers in the ANN and the input activation values while ensuring
    that the returned value is always a matrix. To propagate a given set of values
    through all the layers of an ANN, we must add a bias input and apply the `forward-propagate-layer`
    function for each layer. This can be done concisely using the `reduce` function
    over a closure of the `forward-propagate-layer` function as shown in the `forward-propagate`
    function defined in the preceding code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`matrix-mult`和`sigmoid`函数，我们可以实现ANN中两个层之间的前向传播步骤。这通过`forward-propagate-layer`函数完成，该函数简单地乘以代表ANN中两个层之间突触权重的矩阵和输入激活值，同时确保返回的值始终是一个矩阵。为了将给定的一组值通过ANN的所有层传播，我们必须添加一个偏置输入，并对每个层应用`forward-propagate-layer`函数。这可以通过在`forward-propagate`函数中定义的`forward-propagate-layer`函数的闭包上使用`reduce`函数来简洁地完成。
- en: 'Although the `forward-propagate` function can determine the output activations
    of the ANN, we actually require the activations of all the nodes in the ANN to
    use backpropagation. We can do this by translating the `reduce` function to a
    recursive function and introducing an accumulator variable to store the activations
    of every layer in the ANN. The `forward-propagate-all-activations` function, which
    is defined in the following code, implements this idea and uses the `loop` form
    to recursively apply the `forward-propagate-layer` function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`forward-propagate`函数可以确定ANN的输出激活，但我们实际上需要ANN中所有节点的激活来进行反向传播。我们可以通过将`reduce`函数转换为递归函数并引入一个累加器变量来存储ANN中每一层的激活来实现这一点。在下面的代码中定义的`forward-propagate-all-activations`函数实现了这个想法，并使用`loop`形式递归地应用`forward-propagate-layer`函数：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `forward-propagate-all-activations` function defined in the preceding code
    requires all the weights of the nodes in the ANN and the input values to pass
    through the ANN as activation values. We first use the `bind-bias` function to
    add the bias input to the input activations of the ANN. We then store this value
    in an accumulator, that is, the variable `all-activations`, as a vector of all
    the activations in the ANN. The `forward-propagate-layer` function is then applied
    over the weight matrices of the various layers of the ANN, and each iteration
    adds a bias input to the input activations of the corresponding layer in the ANN.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`forward-propagate-all-activations`函数需要ANN中所有节点的权重和输入值作为激活值通过ANN。我们首先使用`bind-bias`函数将偏置输入添加到ANN的输入激活中。然后我们将此值存储在一个累加器中，即变量`all-activations`，作为一个包含ANN中所有激活的向量。然后，`forward-propagate-layer`函数被应用于ANN各个层的权重矩阵，每次迭代都会向ANN中相应层的输入激活添加一个偏置输入。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we do not add the bias input in the last iteration as it computes
    the output layer of the ANN. Thus, the `forward-propagate-all-activations` function
    applies forward propagation of input values through an ANN and returns the activations
    of every node in the ANN. Note that the activation values in this vector are in
    the order of the layers of the ANN.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在最后一次迭代中我们不添加偏置输入，因为它计算ANN的输出层。因此，`forward-propagate-all-activations`函数通过ANN对输入值进行前向传播，并返回ANN中每个节点的激活值。注意，这个向量中的激活值是按照ANN层的顺序排列的。
- en: 'We will now implement the backpropagation phase of the backpropagation learning
    algorithm. First, we would have to implement a function to calculate the error
    term ![Understanding the backpropagation algorithm](img/4351OS_04_49.jpg) from
    the equation ![Understanding the backpropagation algorithm](img/4351OS_04_61.jpg).
    We will do this with the help of the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现反向传播学习算法的反向传播阶段。首先，我们必须实现一个函数来从方程![理解反向传播算法](img/4351OS_04_61.jpg)计算误差项![理解反向传播算法](img/4351OS_04_49.jpg)。我们将借助以下代码来完成这项工作：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `back-propagate-layer` function defined in the preceding code calculates
    the errors, or deltas, of a synapse layer *l* in the ANN from the weights of the
    layer and the deltas of the next layer in the ANN.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`back-propagate-layer`函数计算ANN中突触层*l*的误差或delta值，这些误差或delta值来自层的权重和ANN中下一层的delta值。
- en: Note
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we only use matrix multiplication to calculate the term ![Understanding
    the backpropagation algorithm](img/4351OS_04_62.jpg) via the `matrix-mult` function.
    All other multiplication operations are element-wise multiplication of matrices,
    which is done using the `mult` function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仅使用矩阵乘法通过`matrix-mult`函数计算术语![理解反向传播算法](img/4351OS_04_62.jpg)。所有其他乘法操作都是矩阵的逐元素乘法，这使用`mult`函数完成。
- en: 'Essentially, we have to apply this function from the output layer to the input
    layer through the various hidden layers of an ANN to produce the delta values
    of every node in the ANN. These delta values can then be added to the activations
    of the nodes, thus producing the gradient values by which we must adjust the weights
    of the nodes in the ANN. We can do this in a manner similar to the `forward-propagate-all-activations`
    function, that is, by recursively applying the `back-propagate-layer` function
    over the various layers of the ANN. Of course, we have to traverse the layers
    of the ANN in the reverse order, that is, starting from the output layer, through
    the hidden layers, to the input layer. We will do this with the help of the following
    code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们必须从输出层通过ANN的各个隐藏层应用到输入层，以产生ANN中每个节点的delta值。然后，这些delta值可以添加到节点的激活中，从而产生通过调整ANN中节点权重所需的梯度值。我们可以以类似于`forward-propagate-all-activations`函数的方式来做这件事，即通过递归地应用`back-propagate-layer`函数到ANN的各个层。当然，我们必须以相反的顺序遍历ANN的层，即从输出层，通过隐藏层，到输入层。我们将借助以下代码来完成这项工作：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `calc-deltas` function determines the delta values of all the perceptron
    nodes in the ANN. For this calculation, the input and output activations are not
    needed. Only the hidden activations, bound to the `hidden-activations` variable,
    are needed to calculate the delta values. Also, the weights of the input layer
    are skipped as they are bound to the `hidden-weights` variable. The `calc-deltas`
    function then applies the `back-propagate-layer` function to all the weight matrices
    of each synapse layer in the ANN, thus determining the deltas of all the nodes
    in the matrix. Note that we don't add the delta of the bias nodes to a computed
    set of deltas. This is done using the `rest` function, `(rest deltas')`, on the
    calculated deltas of a given synapse layer, as the first delta is that of a bias
    input in a given layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc-deltas`函数确定ANN中所有感知器节点的delta值。为此计算，不需要输入和输出激活。只需要与`hidden-activations`变量绑定的隐藏激活来计算delta值。此外，输入层的权重被跳过，因为它们绑定到`hidden-weights`变量。然后`calc-deltas`函数将`back-propagate-layer`函数应用于ANN中每个突触层的所有权重矩阵，从而确定矩阵中所有节点的delta值。请注意，我们不将偏置节点的delta值添加到计算出的delta集合中。这是通过在给定突触层的计算delta值上使用`rest`函数`(rest
    deltas'')`来完成的，因为第一个delta是给定层中偏置输入的delta。'
- en: 'By definition, the gradient vector terms for a given synapse layer ![Understanding
    the backpropagation algorithm](img/4351OS_04_57.jpg) are determined by multiplying
    the matrices ![Understanding the backpropagation algorithm](img/4351OS_04_63.jpg)
    and ![Understanding the backpropagation algorithm](img/4351OS_04_64.jpg), which
    represent the deltas of the next layer and activations of the given layer respectively.
    We will do this with the help of the following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，给定突触层![理解反向传播算法](img/4351OS_04_57.jpg)的梯度向量项是通过乘以矩阵![理解反向传播算法](img/4351OS_04_63.jpg)和![理解反向传播算法](img/4351OS_04_64.jpg)来确定的，这些矩阵分别表示下一层的delta值和给定层的激活。我们将借助以下代码来完成这项工作：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `calc-gradients` function shown in the preceding code is a concise implementation
    of the term ![Understanding the backpropagation algorithm](img/4351OS_04_65.jpg).
    As we will be dealing with a sequence of delta and activation terms, we use the
    `map` function to apply the preceding equality to the corresponding deltas and
    activations in the ANN. Using the `calc-deltas` and `calc-gradient` functions,
    we can determine the total error in the weights of all nodes in the ANN for a
    given training sample. We will do this with the help of the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中显示的`calc-gradients`函数是对术语![理解反向传播算法](img/4351OS_04_65.jpg)的简洁实现。由于我们将处理一系列delta和激活项，我们使用`map`函数将前面的等式应用于ANN中相应的delta和激活项。使用`calc-deltas`和`calc-gradient`函数，我们可以确定给定训练样本中ANN所有节点权重中的总误差。我们将通过以下代码来完成这项工作：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `calc-error` function defined in the preceding code requires two parameters—the
    weight matrices of the synapse layers in the ANN and a sample training value,
    which is shown as `[input expected-output]`. The activations of all the nodes
    in the ANN are first calculated using the `forward-propagate-all-activations`
    function, and the delta value of the last layer is calculated as the difference
    of the expected output value and the actual output value produced by the ANN.
    The output value calculated by the ANN is simply the last activation value produced
    by the ANN, shown as `(last activations)` in the preceding code. Using the calculated
    activations, the deltas of all the perceptron nodes are determined via the `calc-deltas`
    function. These delta values are in turn used to determine the gradients of weights
    in the various layers of the ANN using the `calc-gradients` function. The **Mean
    Square Error** (**MSE**) of the ANN for the given sample value is also calculated
    by adding the squares of the delta values of the output layer of the ANN.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的`calc-error`函数需要两个参数——ANN中突触层的权重矩阵和一个样本训练值，这显示为`[输入期望输出]`。首先使用`forward-propagate-all-activations`函数计算ANN中所有节点的激活值，然后计算最后一层的delta值，即期望输出值与ANN产生的实际输出值之间的差值。ANN计算得出的输出值仅仅是ANN产生的最后一个激活值，如前面代码中所示为`(last
    activations)`。使用计算出的激活值，通过`calc-deltas`函数确定所有感知器节点的delta值。这些delta值随后用于通过`calc-gradients`函数确定ANN中各层的权重梯度。对于给定的样本值，ANN的**均方误差（MSE**）也通过添加输出层delta值的平方来计算。
- en: 'For a given weight matrix of a layer in the ANN, we must initialize the gradients
    for the layer as a matrix with the same dimensions as the weight matrix, and all
    the elements in the gradient matrix must be set to `0`. This can be implemented
    using a composition of the `dim` function, which returns the size of a matrix
    as a vector, and a variant form of the `matrix` function, as shown in the following
    code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ANN中某一层的给定权重矩阵，我们必须初始化该层的梯度为一个与权重矩阵具有相同维度的矩阵，并且梯度矩阵中的所有元素都必须设置为`0`。这可以通过使用`dim`函数的组合来实现，该函数返回矩阵的大小为一个向量，以及`matrix`函数的变体形式，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the `new-gradient-matrix` function defined in the preceding code, the `matrix`
    function expects a value, the number of rows and the number of columns to initialize
    a matrix. This function produces an initialized gradient matrix with the same
    structure as the supplied weight matrix.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中定义的`new-gradient-matrix`函数中，`matrix`函数期望一个值、行数和列数来初始化一个矩阵。此函数生成一个具有与提供的权重矩阵相同结构的初始化梯度矩阵。
- en: 'We now implement the `calc-gradients-and-error` function to apply the `calc-error`
    function on a set of weight matrices and sample values. We must basically apply
    the `calc-error` function to each sample and accumulate the sum of the gradient
    and the MSE values. We then calculate the average of these accumulated values
    to return the gradient matrices and total MSE for the given sample values and
    weight matrices. We will do this with the help of the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在实现`calc-gradients-and-error`函数，以便在一系列权重矩阵和样本值上应用`calc-error`函数。我们基本上需要将`calc-error`函数应用于每个样本，并累积梯度值和均方误差（MSE）的总和。然后我们计算这些累积值的平均值，以返回给定样本值和权重矩阵的梯度矩阵和总MSE。我们将通过以下代码来完成这项工作：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `calc-gradients-and-error` function defined in the preceding code relies
    on the `calc-gradients-and-error'` helper function. `The calc-gradients-and-error'`
    function initializes the gradient matrices, performs the application of the `calc-error`
    function, and accumulates the calculated gradient values and MSE. The `calc-gradients-and-error`
    function simply calculates the average of the accumulated gradient matrices and
    MSE returned from the `calc-gradients-and-error'` function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`calc-gradients-and-error`函数依赖于`calc-gradients-and-error'`辅助函数。`calc-gradients-and-error'`函数初始化梯度矩阵，执行`calc-error`函数的应用，并累计计算出的梯度值和MSE。`calc-gradients-and-error`函数简单地计算从`calc-gradients-and-error'`函数返回的累计梯度矩阵和MSE的平均值。
- en: 'Now, the only missing piece in our implementation is modifying the weights
    of the nodes in the ANN using calculated gradients. In brief, we must repeatedly
    update the weights until a convergence in the MSE is observed. This is actually
    a form of gradient descent applied to the nodes of an ANN. We will now implement
    this variant of gradient descent in order to train the ANN by repeatedly modifying
    the weights of the nodes in the ANN, as shown in the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实现中唯一缺少的部分是使用计算出的梯度修改ANN中节点的权重。简而言之，我们必须反复更新权重，直到观察到MSE的收敛。这实际上是对ANN节点应用的一种梯度下降形式。我们现在将实现这种梯度下降的变体，通过反复修改ANN中节点的权重来训练ANN，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `gradient-descent-complete?` function defined in the preceding code simply
    checks for the termination condition of gradient descent. This function assumes
    that the ANN, represented as a network, is a map or record that contains the `:options`
    keyword. The value of this key is in turn another map that contains the various
    configuration options of the ANN. The `gradient-descent-complete?` function checks
    whether the total MSE of the ANN is less than the desired MSE, which is specified
    by the `:desired-error` option. Also, we add another condition to check if the
    number of iterations performed exceeds the maximum number of iterations specified
    by the `:max-iters` option.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`gradient-descent-complete?`函数简单地检查梯度下降的终止条件。这个函数假设ANN，作为一个网络，是一个包含`:options`关键字的映射或记录。这个键的值反过来又是一个包含ANN各种配置选项的映射。`gradient-descent-complete?`函数检查ANN的总均方误差（MSE）是否小于由`:desired-error`选项指定的期望MSE。此外，我们还添加了另一个条件来检查执行的迭代次数是否超过了由`:max-iters`选项指定的最大迭代次数。
- en: 'Now, we will implement a `gradient-descent` function for multilayer perceptron
    ANNs. In this implementation, the changes in weights are calculated by the `step`
    function provided by the gradient descent algorithm. These calculated changes
    are then simply added to the existing weights of the synapse layers of the ANN.
    We will implement the `gradient-descent` function for multilayer perceptron ANNs
    with the help of the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为多层感知器人工神经网络（ANNs）实现一个`梯度下降`函数。在这个实现中，权重的变化是通过梯度下降算法提供的`step`函数来计算的。然后，这些计算出的变化简单地添加到ANN的突触层现有权重中。我们将使用以下代码帮助实现多层感知器ANN的`梯度下降`函数：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `apply-weight-changes` function defined in the preceding code simply adds
    the weights and the calculated changes in the weights of the ANN. The `gradient-descent`
    function requires a `step` function (specified as `step-fn`), the initial state
    of the ANN, the ANN itself, and the sample data to train the ANN. This function
    must calculate the weight changes from the ANN, the initial gradient matrices,
    and the initial state of the ANN. The `step-fn` function also returns the changed
    state of the ANN. The weights of the ANN are then updated using the `apply-weight-changes`
    function, and this iteration is repeatedly performed until the `gradient-descent-complete?`
    function returns as `true`. The weights of the ANN are specified by the `:weights`
    keyword in the `network` map. These weights are then updated by simply overwriting
    the value on the `network` specified by the `:weights` keyword.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`apply-weight-changes`函数简单地添加了ANN的权重和计算出的权重变化。`gradient-descent`函数需要一个`step`函数（指定为`step-fn`）、ANN的初始状态、ANN本身以及用于训练ANN的样本数据。这个函数必须从ANN、初始梯度矩阵和ANN的初始状态计算权重变化。`step-fn`函数还返回ANN的更改状态。然后，使用`apply-weight-changes`函数更新ANN的权重，并且这个迭代过程会重复执行，直到`gradient-descent-complete?`函数返回`true`。ANN的权重由`network`映射中的`:weights`关键字指定。然后，简单地通过覆盖由`:weights`关键字指定的`network`上的值来更新这些权重。
- en: 'In the context of the backpropagation algorithm, we need to specify the learning
    rate and learning momentum by which the ANN must be trained. These parameters
    are needed to determine the changes in the weights of the nodes in the ANN. A
    function implementing this calculation must then be specified as the `step-fn`
    parameter to the `gradient-descent` function, as shown in the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播算法的上下文中，我们需要指定 ANN 必须训练的学习率和学习动量。这些参数用于确定 ANN 中节点权重的变化。然后必须指定一个实现此计算的函数作为
    `gradient-descent` 函数的 `step-fn` 参数，如下面的代码所示：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `calc-weight-changes` function defined in the preceding code calculates
    the change of weights, termed as ![Understanding the backpropagation algorithm](img/4351OS_04_66.jpg),
    from the gradient values and deltas of a given layer in the ANN. The `bprop-step-fn`
    function extracts the learning rate and learning momentum parameters from the
    ANN that is represented by `network` and uses the `calc-weight-changes` function.
    As the weights will be added with the changes by the `gradient-descent` function,
    we return the changes in weights as negative values using the `minus` function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `calc-weight-changes` 函数根据给定 ANN 中某一层的梯度值和 delta 值计算权重的变化，称为 ![理解反向传播算法](img/4351OS_04_66.jpg)。`bprop-step-fn`
    函数从表示为 `network` 的 ANN 中提取学习率和学习动量参数，并使用 `calc-weight-changes` 函数。由于权重将由 `gradient-descent`
    函数添加变化，我们使用 `minus` 函数以负值返回权重的变化。
- en: 'The `gradient-descent-bprop` function simply initializes the gradient matrices
    for the given weights of the ANN and calls the `gradient-descent` function by
    specifying `bprop-step-fn` as the `step` function to be used. Using the `gradient-descent-bprop`
    function, we can implement the abstract `NeuralNetwork` protocol we had defined
    earlier, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`gradient-descent-bprop` 函数简单地初始化 ANN 给定权重的梯度矩阵，并通过指定 `bprop-step-fn` 作为要使用的
    `step` 函数来调用 `gradient-descent` 函数。使用 `gradient-descent-bprop` 函数，我们可以实现我们之前定义的抽象
    `NeuralNetwork` 协议，如下所示：'
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `MultiLayerPerceptron` record defined in the preceding code trains a multilayer
    perceptron ANN using the `gradient-descent-bprop` function. The `train-ann` function
    first extracts the values for the number of hidden neurons and the constant ![Understanding
    the backpropagation algorithm](img/4351OS_04_43.jpg) from the options map specified
    to the ANN. The sizes of the various synapse layers in the ANN are first determined
    from the sample data and bound to the `layer-sizes` variable. The weights of the
    ANN are then initialized using the `random-initial-weights` function and updated
    in the record `network` using the `assoc` function. Finally, the `gradient-descent-bprop`
    function is called to train the ANN using the backpropagation learning algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `MultiLayerPerceptron` 记录使用 `gradient-descent-bprop` 函数训练一个多层感知器人工神经网络
    (ANN)。`train-ann` 函数首先从指定的 ANN 选项映射中提取隐藏神经元的数量和常数 ![理解反向传播算法](img/4351OS_04_43.jpg)
    的值。ANN 中各种突触层的尺寸首先从样本数据中确定，并绑定到 `layer-sizes` 变量。然后使用 `random-initial-weights`
    函数初始化 ANN 的权重，并在 `network` 记录中使用 `assoc` 函数更新。最后，通过指定 `bprop-step-fn` 作为要使用的 `step`
    函数，调用 `gradient-descent-bprop` 函数来使用反向传播学习算法训练 ANN。
- en: The ANN defined by the `MultiLayerPerceptron` record also implements two other
    functions, `run` and `run-binary`, from the `NeuralNetwork` protocol. The `run`
    function uses the `forward-propagate` function to determine the output values
    of a trained `MultiLayerPerceptron` ANN. The `run-binary` function simply rounds
    the value of the output returned by the `run` function for the given set of input
    values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `MultiLayerPerceptron` 记录定义的 ANN 还实现了 `NeuralNetwork` 协议中的两个其他函数，`run` 和 `run-binary`。`run`
    函数使用 `forward-propagate` 函数确定训练好的 `MultiLayerPerceptron` ANN 的输出值。`run-binary`
    函数简单地将 `run` 函数为给定输入值集返回的输出值四舍五入。
- en: 'An ANN created using the `MultiLayerPerceptron` record requires a single `options`
    parameter containing the various options we can specify for the ANN. We can define
    the default options for such an ANN as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `MultiLayerPerceptron` 记录创建的 ANN 需要一个包含我们可以为 ANN 指定的各种选项的单个 `options` 参数。我们可以如下定义此类
    ANN 的默认选项：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The map defined by the `default-options` variable contains the following keys
    that specify the options for the `MultiLayerPerceptron` ANN:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `default-options` 变量定义的映射包含以下键，这些键指定了 `MultiLayerPerceptron` ANN 的选项：
- en: '`:max-iter`: This key specifies the maximum number of iterations to run the
    `gradient-descent` function.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:max-iter`: 此键指定运行 `gradient-descent` 函数的最大迭代次数。'
- en: '`:desired-error`: This variable specifies the expected or acceptable MSE in
    the ANN.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:desired-error`：此变量指定ANN中期望或可接受的均方误差（MSE）。'
- en: '`:hidden-neurons`: This variable specifies the number of hidden neural nodes
    in the network. The value `[3]` represents a single hidden layer with three neurons.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:hidden-neurons`：此变量指定网络中隐藏神经节点的数量。值`[3]`表示一个包含三个神经元的单个隐藏层。'
- en: '`:learning-rate` and `:learning-momentum`: These keys specify the learning
    rate and learning momentum for the weight update phase of the backpropagation
    learning algorithm.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:learning-rate`和`:learning-momentum`：这些键指定反向传播学习算法权重更新阶段的学习率和学习动量。'
- en: '`:epsilon`: This variable specifies the constant used by the `random-initial-weights`
    function to initialize the weights of the ANN.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:epsilon`：此变量指定`random-initial-weights`函数用于初始化ANN权重的常数。'
- en: 'We also define a simple helper function `train` to create an ANN of the `MultiLayerPerceptron`
    type and train the ANN using the `train-ann` function and the sample data specified
    by the `samples` parameter. We can now create a trained ANN from the training
    data specified by the `sample-data` variable as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个简单的辅助函数`train`，用于创建`MultiLayerPerceptron`类型的ANN，并使用`train-ann`函数和由`samples`参数指定的样本数据来训练ANN。现在，我们可以根据`sample-data`变量指定的训练数据创建一个训练好的ANN，如下所示：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can then use the trained ANN to predict the output of some input values.
    The output generated by the ANN defined by `MLP` closely matches the output of
    an XOR gate as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用训练好的ANN来预测一些输入值的输出。由`MLP`定义的ANN生成的输出与XOR门的输出非常接近，如下所示：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'However, the trained ANN produces incorrect outputs for some set of inputs
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练好的ANN对于某些输入集产生了不正确的输出，如下所示：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are several measures we can implement in order to improve the accuracy
    of the trained ANN. First, we can regularize the calculated gradients using the
    weights matrices of the ANN. This modification will produce a noticeable improvement
    in the preceding implementation. We can also increase the maximum number of iterations
    to be performed. We can also tune the algorithm to perform better by tweaking
    the learning rate, the learning momentum, and the number of hidden nodes in the
    ANN. These modifications are skipped as they have to be done by the reader.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高训练好的ANN的准确性，我们可以实施几种措施。首先，我们可以使用ANN的权重矩阵来正则化计算的梯度。这种修改将使先前的实现产生明显的改进。我们还可以增加要执行的最大迭代次数。我们还可以调整算法，通过调整学习率、学习动量和ANN中的隐藏节点数量来提高性能。这些修改被跳过，因为它们需要由读者来完成。
- en: 'The **Enclog** library ([http://github.com/jimpil/enclog](http://github.com/jimpil/enclog))
    is a Clojure wrapper library for the **Encog** library for machine learning algorithms
    and ANNs. The Encog library ([http://github.com/encog](http://github.com/encog))
    has two primary implementations: one in Java and one in .NET. We can use the Enclog
    library to easily generate customized ANNs to model both supervised and unsupervised
    machine learning problems.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**Enclog**库（[http://github.com/jimpil/enclog](http://github.com/jimpil/enclog)）是一个Clojure包装库，用于机器学习算法和ANN的**Encog**库。Encog库（[http://github.com/encog](http://github.com/encog)）有两个主要实现：一个在Java中，一个在.NET中。我们可以使用Enclog库轻松生成定制的ANN来模拟监督和非监督机器学习问题。'
- en: Note
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Enclog library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`project.clj`文件中添加以下依赖项将Enclog库添加到Leiningen项目中：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that the Enclog library requires the Encog Java library as a dependency.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Enclog库需要Encog Java库作为依赖项。
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can create an ANN from the Enclog library using the `neural-pattern` and
    `network` functions from the `enclog.nnets` namespace. The `neural-pattern` function
    is used to specify a neural network model for the ANN. The `network` function
    accepts a neural network model returned from the `neural-pattern` function and
    creates a new ANN. We can provide several options to the `network` function depending
    on the specified neural network model. A feed-forward multilayer perceptron network
    is defined as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`enclog.nnets`命名空间中的`neural-pattern`和`network`函数从Enclog库创建一个ANN。`neural-pattern`函数用于指定ANN的神经网络模型。`network`函数接受从`neural-pattern`函数返回的神经网络模型，并创建一个新的ANN。我们可以根据指定的神经网络模型向`network`函数提供几个选项。以下是一个前馈多层感知器网络的定义：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For a feed-forward neural network, we can specify the activation function with
    the `:activation` key to the `network` function. For our example, we used the
    sigmoid function, which is specified as `:sigmoid`, as the activation function
    for the ANNs nodes. We also specified the number of nodes in the input, output,
    and hidden layers of the ANN using the `:input`, `:output`, and `:hidden` keys.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前馈神经网络，我们可以通过将`:activation`键指定给`network`函数来指定激活函数。在我们的例子中，我们使用了sigmoid函数，它被指定为`:sigmoid`作为ANN节点的激活函数。我们还使用`:input`、`:output`和`:hidden`键指定了ANN的输入、输出和隐藏层的节点数量。
- en: 'To train an ANN created by the `network` function with some sample data, we
    use the `trainer` and `train` functions from the `enclog.training` namespace.
    The learning algorithm to be used to train the ANN must be specified as the first
    parameter to the `trainer` function. For the backpropagation algorithm, this parameter
    is the `:back-prop` keyword. The value returned by the trainer function represents
    an ANN as well as the learning algorithm to be used to train the ANN. The `train`
    function is then used to actually run the specified training algorithm on the
    ANN. We will do this with the help of the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用一些样本数据训练由`network`函数创建的ANN，我们使用`enclog.training`命名空间中的`trainer`和`train`函数。用于训练ANN的学习算法必须作为`trainer`函数的第一个参数指定。对于反向传播算法，此参数是`:back-prop`关键字。`trainer`函数返回的值代表一个ANN以及用于训练ANN的学习算法。然后使用`train`函数在ANN上实际运行指定的训练算法。我们将通过以下代码来完成这项工作：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `train-network` function defined in the preceding code takes three parameters.
    The first parameter is an ANN created by the network function, the second parameter
    is the training data to be used to train the ANN, and the third parameter specifies
    the learning algorithm by which the ANN must be trained. As shown in the preceding
    code, we can specify the ANN and the training data to the `trainer` function using
    the key parameters, `:network` and `:training-set`. The `train` function is then
    used to run the training algorithm on the ANN using the sample data. We can specify
    the expected error in the ANN and the maximum number of iterations to run the
    training algorithm as the first and second parameters to the `train` function.
    In the preceding example, the desired error is `0.01`, and the maximum number
    of iterations is 1000\. The last parameter passed to the `train` function is a
    vector specifying the behaviors of the ANN, and we ignore it by passing it as
    an empty vector.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`train-network`函数接受三个参数。第一个参数是由`network`函数创建的ANN，第二个参数是用于训练ANN的训练数据，第三个参数指定了ANN必须通过的学习算法。如前所述的代码所示，我们可以使用关键字参数`:network`和`:training-set`将ANN和训练数据指定给`trainer`函数。然后使用`train`函数通过样本数据在ANN上运行训练算法。我们可以将ANN中期望的错误和训练算法的最大迭代次数作为`train`函数的第一个和第二个参数指定。在前面的例子中，期望的错误是`0.01`，最大迭代次数是1000。传递给`train`函数的最后一个参数是一个指定ANN行为的向量，我们通过传递一个空向量来忽略它。
- en: 'The training data to be used to run the training algorithm on the ANN can be
    created using Enclog''s `data` function. For example, we can create a training
    data for a logical XOR gate using the `data` function as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在对人工神经网络（ANN）运行训练算法时使用的训练数据可以通过使用Enclog的`data`函数来创建。例如，我们可以使用`data`函数创建一个用于逻辑异或门（XOR
    gate）的训练数据，如下所示：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `data` function requires the type of data as the first parameter of the
    function, followed by the input and output values of the training data as vectors.
    For our example, we will use the `:basic-dataset` and `:basic` parameters. The
    `:basic-dataset` keyword can be used to create training data, and the `:basic`
    keyword can be used to specify a set of input values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`函数需要将数据类型作为函数的第一个参数，随后是训练数据的输入和输出值作为向量。在我们的例子中，我们将使用`:basic-dataset`和`:basic`参数。`:basic-dataset`关键字可以用来创建训练数据，而`:basic`关键字可以用来指定一组输入值。'
- en: 'Using the data defined by the `dataset` variable and the `train-network` function,
    we can train the ANN''s `MLP` to model the output of an XOR gate as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由`dataset`变量定义的数据和`train-network`函数，我们可以训练ANN的`MLP`来模拟异或门的输出，如下所示：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As shown by the preceding output, the trained ANN has an error of about 3.16
    percent. We can now use the trained ANN to predict the output of a set of input
    values. To do this, we use the Java `compute` and `getData` methods, which are
    specified by `.compute` and `.getData` respectively. We can define a simple helper
    function to call the `.compute` method for a vector of input values and round
    the output to a binary value as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述输出所示，训练好的ANN的错误率约为3.16%。现在我们可以使用训练好的ANN来预测一组输入值的输出。为此，我们使用Java的`compute`和`getData`方法，分别由`.compute`和`.getData`指定。我们可以定义一个简单的辅助函数来调用`.compute`方法，为输入值向量计算结果，并将输出四舍五入到二进制值，如下所示：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now use the `run-network` function to test the trained ANN using a vector
    of input values, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`run-network`函数，通过输入值向量来测试训练好的ANN，如下所示：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As shown in the preceding code, the trained ANN represented by `MLP` completely
    matches the behavior of an XOR gate.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，由`MLP`表示的训练好的ANN完全符合XOR门的行为。
- en: In conclusion, the Enclog library gives us a small set of powerful functions
    that can be used to build ANNs. In the preceding example, we explored a feed-forward
    multilayer perceptron model. The library provides several other ANN models, such
    as **Adaptive Resonance Theory** (**ART**), **Self-Organizing Maps** (**SOM**),
    and Elman networks. The Enclog library also allows us to customize the activation
    function of the nodes in a particular neural network model. For the feed-forward
    network in our example, we've used the sigmoid function. Several mathematical
    functions, such as sine, hyperbolic tan, logarithmic, and linear functions, are
    also supported by the library. There are also several machine learning algorithms
    supported by the Enclog library that can be used to train an ANN.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Enclog库为我们提供了一组强大的函数，可以用来构建ANN。在前面的例子中，我们探讨了前馈多层感知器模型。该库还提供了其他几种ANN模型，例如**自适应共振理论**（**ART**）、**自组织映射**（**SOM**）和Elman网络。Enclog库还允许我们自定义特定神经网络模型中节点的激活函数。在我们的例子中，我们使用了sigmoid函数。库还支持几种数学函数，如正弦、双曲正切、对数和线性函数。Enclog库还支持一些机器学习算法，可用于训练ANN。
- en: Understanding recurrent neural networks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解循环神经网络
- en: We will now switch our focus to recurrent neural networks and study a simple
    recurrent neural network model. An **Elman neural network** is a simple recurrent
    ANN with a single input, output, and hidden layer. There is also an extra *context
    layer* of neural nodes. Elman neural networks are used to simulate short-term
    memory in supervised and unsupervised machine learning problems. Enclog does include
    support for Elman neural networks, and we will demonstrate how we can build an
    Elman neural network using the Enclog library.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注点转向循环神经网络，并研究一个简单的循环神经网络模型。**Elman神经网络**是一个简单的循环ANN，具有单个输入、输出和隐藏层。还有一个额外的**上下文层**的神经网络节点。Elman神经网络用于模拟监督和无监督机器学习问题中的短期记忆。Enclog库确实包括对Elman神经网络的支持，我们将演示如何使用Enclog库构建Elman神经网络。
- en: 'The context layer of an Elman neural network receives unweighted inputs from
    the hidden layer of the ANN. In this way, the ANN can remember the previous values
    that we generated using the hidden layer and use these values to affect the predicted
    value. Thus, the context layer serves as a type of short-term memory for the ANN.
    An Elman neural network can be illustrated by the following diagram:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Elman神经网络的上下文层从ANN的隐藏层接收无权重的输入。这样，ANN可以记住我们使用隐藏层生成的先前值，并使用这些值来影响预测值。因此，上下文层充当ANN的短期记忆。以下图示可以说明Elman神经网络：
- en: '![Understanding recurrent neural networks](img/4351OS_04_67.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![理解循环神经网络](img/4351OS_04_67.jpg)'
- en: The structure of an Elman network, as depicted by the preceding diagram, resembles
    that of a feed-forward multilayer perceptron ANN. An Elman network adds an extra
    context layer of neural nodes to the ANN. The Elman network illustrated in the
    preceding diagram takes two inputs and produces two outputs. The input and hidden
    layers of the Elman network add an extra bias input, similar to a multilayer perceptron.
    The activations of the hidden layers' neurons are fed directly to the two context
    nodes ![Understanding recurrent neural networks](img/4351OS_04_68.jpg) and ![Understanding
    recurrent neural networks](img/4351OS_04_69.jpg). The values stored in these context
    nodes are then used later by the nodes in the hidden layer of the ANN to recollect
    the previous activations to determine the new activation values.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述图所示，Elman网络的结构类似于前馈多层感知器ANN。Elman网络向ANN添加了一个额外的上下文层神经网络。前述图中的Elman网络接受两个输入并产生两个输出。Elman网络的输入和隐藏层添加了一个额外的偏置输入，类似于多层感知器。隐藏层神经元的激活直接馈送到两个上下文节点![理解循环神经网络](img/4351OS_04_68.jpg)和![理解循环神经网络](img/4351OS_04_69.jpg)。这些上下文节点中存储的值随后被ANN隐藏层的节点使用，以回忆先前的激活并确定新的激活值。
- en: 'We can create an Elman network that specifies the `:elman` keyword to the `neural-pattern`
    function from the Enclog library as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个Elman网络，将`:elman`关键字指定给Enclog库中的`neural-pattern`函数，如下所示：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To train the Elman network, we can use the resilient propagation algorithm
    (for more information, refer to *Empirical Evaluation of the Improved Rprop Learning
    Algorithm*). This algorithm can also be used to train other recurrent networks
    supported by Enclog. Interestingly, the resilient propagation algorithm can be
    used to train feed-forward networks as well. This algorithm also performs significantly
    better than the backpropagation learning algorithm. Although a complete description
    of this algorithm is beyond the scope of this book, the reader is encouraged to
    learn more about this learning algorithm. The resilient propagation algorithm
    is specified as the `:resilient-prop` keyword to the `train-network` function,
    which we had defined earlier. We can train the Elman neural network using the
    `train-network` function and the `dataset` variable as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练Elman网络，我们可以使用弹性传播算法（更多信息，请参阅*Empirical Evaluation of the Improved Rprop
    Learning Algorithm*）。此算法也可以用于训练Enclog支持的其他循环网络。有趣的是，弹性传播算法还可以用于训练前馈网络。此算法的性能也显著优于反向传播学习算法。尽管此算法的完整描述超出了本书的范围，但鼓励读者了解更多关于此学习算法的信息。弹性传播算法指定为`train-network`函数的`:resilient-prop`关键字，这是我们之前定义的。我们可以使用`train-network`函数和`dataset`变量来训练Elman神经网络，如下所示：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As shown in the preceding code, the resilient propagation algorithm requires
    a relatively smaller number of iterations in comparison to the backpropagation
    algorithm. We can now use this trained ANN to simulate an XOR gate just like we
    did in the previous example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，与反向传播算法相比，弹性传播算法需要相对较少的迭代次数。现在我们可以使用这个训练好的ANN来模拟一个XOR门，就像我们在上一个例子中所做的那样。
- en: In summary, recurrent neural network models and training algorithms are the
    other useful models that can be used to model classification or regression problems
    using ANNs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，循环神经网络模型和训练算法是其他有用的模型，可以用于使用ANN来建模分类或回归问题。
- en: Building SOMs
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建SOMs
- en: SOM (pronounced as **ess-o-em**) is another interesting ANN model that is useful
    for unsupervised learning. SOMs are used in several practical applications such
    as handwriting and image recognition. We will also revisit SOMs when we discuss
    clustering in [Chapter 7](ch07.html "Chapter 7. Clustering Data"), *Clustering
    Data*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: SOM（发音为**ess-o-em**）是另一个有趣的ANN模型，它对无监督学习很有用。SOMs被用于多个实际应用中，如手写识别和图像识别。当我们讨论[第7章](ch07.html
    "第7章。聚类数据")中的聚类时，我们也将重新审视SOMs，*聚类数据*。
- en: In unsupervised learning, the sample data contains no expected output values,
    and the ANN must recognize and match patterns from the input data entirely on
    its own. SOMs are used for *competitive learning*, which is a special class of
    unsupervised learning in which the neurons in the output layer of the ANN compete
    among themselves for activation. The activated neuron determines the final output
    value of the ANN, and hence, the activated neuron is also termed as a **winning
    neuron**.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，样本数据不包含预期的输出值，人工神经网络（ANN）必须完全依靠自身识别和匹配输入数据中的模式。SOM用于*竞争学习*，这是无监督学习的一个特殊类别，其中ANN输出层的神经元相互竞争以激活。激活的神经元决定了ANN的最终输出值，因此，激活的神经元也被称为**获胜神经元**。
- en: Neurobiological studies have shown that different sensory inputs sent to the
    brain are mapped to the corresponding areas of the brain's *cerebral cortex* in
    an orderly pattern. Thus, neurons that deal with closely related operations are
    kept close together. This is known as the **principle of topographic formation**,
    and SOMs are, in fact, modeled on this behavior.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 神经生物学研究表明，大脑接收到的不同感官输入以有序的模式映射到大脑*大脑皮层*的相应区域。因此，处理密切相关操作的神经元被保持在一起。这被称为**拓扑形成原理**，而SOM实际上是基于这种行为构建的。
- en: 'An SOM essentially transforms input data with a large number of dimensions
    to a low-dimensional discrete map. The SOM is trained by placing the neurons at
    the nodes of this map. This internal map of the SOM usually has one or two dimensions.
    The neurons in the SOM become *selectively tuned* to the patterns in the input
    values. When a particular neuron in the SOM is activated for a particular input
    pattern, its neighboring neurons tend to get more excited and more tuned to the
    pattern in the input values. This behavior is termed as **lateral interaction**
    of a set of neurons. An SOM, thus, finds patterns in the input data. When a similar
    pattern is found in a set of inputs, the SOM recognizes this pattern. The layers
    of neural nodes in an SOM can be illustrated as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织映射（SOM）本质上是将具有大量维度的输入数据转换为一个低维离散映射。通过将神经元放置在这个映射的节点上对SOM进行训练。SOM的内部映射通常有一到两个维度。SOM中的神经元会*选择性地调整*到输入值中的模式。当SOM中的某个特定神经元被激活以响应特定的输入模式时，其邻近的神经元往往会变得更加兴奋，并且更倾向于调整到输入值中的模式。这种行为被称为一组神经元的**横向交互**。因此，SOM可以在输入数据中找到模式。当在输入数据集中找到相似模式时，SOM会识别这个模式。SOM中神经节点层的结构可以描述如下：
- en: '![Building SOMs](img/4351OS_04_70.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![构建SOMs](img/4351OS_04_70.jpg)'
- en: An SOM has an input layer and a computational layer, as depicted by the preceding
    diagram. The computational layer is also termed as the **feature map** of the
    SOM. The input nodes map the input values to several neurons in the computational
    layer. Each node in the computational layer has its output connected to its neighboring
    node, and each of these connections has a weight associated with it. These weights
    are termed as the **connection weights** of the feature map. The SOM remembers
    patterns in the input values by adjusting the connection weights of the nodes
    in its computational layer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织映射（SOM）有一个输入层和一个计算层，如前图所示。计算层也被称为SOM的**特征图**。输入节点将输入值映射到计算层中的几个神经元。计算层中的每个节点都有其输出连接到其邻近节点，并且每个连接都有一个与之相关的权重。这些权重被称为特征图的**连接权重**。SOM通过调整其计算层中节点的连接权重来记住输入值中的模式。
- en: 'The self-organizing process of an SOM can be described as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织映射（SOM）的自组织过程可以描述如下：
- en: The connection weights are first initialized to random values.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接权重最初被初始化为随机值。
- en: For each input pattern, the neural nodes in the computational layer compute
    a value using a discriminant function. These values are then used to decide the
    winning neuron.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个输入模式，计算层中的神经节点使用判别函数计算一个值。然后，这些值被用来决定获胜的神经元。
- en: The neuron with the least value for the discriminant function is selected, and
    the connection weights to its surrounding neurons are modified to be activated
    for similar patterns in the input data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有最小判别函数值的神经元被选中，并且对其周围神经元的连接权重进行修改，以便激活输入数据中的相似模式。
- en: The weights must be modified such that the value produced by the discriminant
    function for the neighboring nodes is reduced for the given pattern in the input.
    Thus, the winning node and its surrounding nodes produce higher output or activation
    values for similar patterns in the input data. The amount of change by which the
    weights are adjusted depends on the learning rate specified to the training algorithm.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 必须修改权重，使得对于输入中的给定模式，判别函数对邻近节点的值减少。因此，获胜节点及其周围节点对于输入数据中的相似模式产生更高的输出或激活值。权重调整的量取决于指定给训练算法的学习率。
- en: 'For a given number of dimensions *D* in the input data, the discriminant function
    can be formally defined as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入数据中的给定维度数 *D*，判别函数可以正式定义为以下内容：
- en: '![Building SOMs](img/4351OS_04_71.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![Building SOMs](img/4351OS_04_71.jpg)'
- en: In the preceding equation, the term ![Building SOMs](img/4351OS_04_72.jpg) is
    the weight vector of the ![Building SOMs](img/4351OS_04_73.jpg) neuron in the
    SOM. The length of the vector ![Building SOMs](img/4351OS_04_72.jpg) is equal
    to the number of neurons connected to the ![Building SOMs](img/4351OS_04_73.jpg)
    neuron.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，术语 ![Building SOMs](img/4351OS_04_72.jpg) 是SOM中 ![Building SOMs](img/4351OS_04_73.jpg)
    神经元的权重向量。向量 ![Building SOMs](img/4351OS_04_72.jpg) 的长度等于连接到 ![Building SOMs](img/4351OS_04_73.jpg)
    神经元的神经元数量。
- en: Once we have selected the winning neuron in an SOM, we must select the neighboring
    neurons of the winning neuron. We must adjust the weights of these neighboring
    neurons along with the weight of the winning neuron. A variety of schemes can
    be used for the selection of the winning neuron's neighboring nodes. In the simplest
    case, we can select a single neighboring neuron.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在SOM中选定了获胜神经元，我们必须选择获胜神经元的邻近神经元。我们必须调整这些邻近神经元的权重以及获胜神经元的权重。可以使用各种方案来选择获胜神经元邻近节点。在最简单的情况下，我们可以选择一个邻近神经元。
- en: We can alternatively use the `bubble` function or the `radial bias` function
    to select a group of neighboring neurons surrounding the winning neuron (for more
    information, refer to *Multivariable functional interpolation and adaptive networks*).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以改用 `bubble` 函数或 `radial bias` 函数来选择围绕获胜神经元的一组邻近神经元（更多信息，请参阅 *Multivariable
    functional interpolation and adaptive networks*）。
- en: 'To train an SOM, we must perform the following steps as part of the training
    algorithm:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个SOM，我们必须在训练算法中执行以下步骤：
- en: Set the weights of the nodes in the computational layer to random values.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算层中节点的权重设置为随机值。
- en: Select a sample input pattern from the training data.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中选择一个样本输入模式。
- en: Find the winning neuron for the selected set of input patterns.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到所选输入模式集的获胜神经元。
- en: Update the weights of the winning neuron and its surrounding nodes.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新获胜神经元及其周围节点的权重。
- en: Repeat steps 2 to 4 for all the samples in the training data.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练数据中的所有样本，重复步骤2到4。
- en: 'The Enclog library does support the SOM neural network model and training algorithm.
    We can create and train an SOM from the Enclog library as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Enclog库支持SOM神经网络模型和训练算法。我们可以按照以下方式从Enclog库创建和训练一个SOM：
- en: '[PRE31]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `som` variable appearing in the preceding code represents an SOM. The `train-som`
    function can be used to train the SOM. The SOM training algorithm is specified
    as `:basic-som`. Note that we specify the learning rate as `0.7` using the `:learning-rate`
    key.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中出现的 `som` 变量代表一个自组织映射（SOM）。可以使用 `train-som` 函数来训练SOM。SOM的训练算法指定为 `:basic-som`。注意，我们使用
    `:learning-rate` 键将学习率指定为 `0.7`。
- en: The `:neighborhood-fn` key passed to the `trainer` function in the preceding
    code specifies how we select the neighbors of the winning node in the SOM for
    a given set of input values. We specify that a single neighboring node of the
    winning node must be selected with the help of `(neighborhood-F :single)`. We
    can also specify different neighborhood functions. For example, we can specify
    the `bubble` function as `:bubble` or the `radial basis` function as `:rbf`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中传递给 `trainer` 函数的 `:neighborhood-fn` 键指定了对于给定的一组输入值，我们在SOM中如何选择获胜节点的邻近节点。我们指定必须使用
    `(neighborhood-F :single)` 来选择获胜节点的单个邻近节点。我们还可以指定不同的邻域函数。例如，我们可以指定 `bubble` 函数为
    `:bubble` 或径向基函数为 `:rbf`。
- en: 'We can use the `train-som` function to train the SOM with some input patterns.
    Note that the training data to be used to train the SOM will not have any output
    values. The SOM must recognize patterns in the input data on its own. Once the
    SOM is trained, we can use the Java `classify` method to detect patterns in the
    input. For the following example, we provide only two input patterns to train
    the SOM:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`train-som`函数使用一些输入模式来训练SOM。请注意，用于训练SOM的训练数据将没有任何输出值。SOM必须自行识别输入数据中的模式。一旦SOM被训练，我们可以使用Java的`classify`方法来检测输入中的模式。对于以下示例，我们只提供两个输入模式来训练SOM：
- en: '[PRE32]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can run the `train-and-run-som` function defined in the preceding code and
    observe that the SOM recognizes the two input patterns in the training data as
    two distinct classes as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行前面代码中定义的`train-and-run-som`函数，并观察到SOM将训练数据中的两个输入模式识别为两个不同的类别，如下所示：
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In conclusion, SOMs are a great model for dealing with unsupervised learning
    problems. Also, we can easily build SOMs to model such problems using the Enclog
    library.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，SOMs是处理无监督学习问题的优秀模型。此外，我们可以轻松地使用Enclog库构建SOM来模拟这些问题。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We have explored a few interesting ANN models in this chapter. These models
    can be applied to solve both supervised and unsupervised machine learning problems.
    The following are some of the other points that we covered:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中探索了几种有趣的ANN模型。这些模型可以应用于解决监督学习和无监督机器学习问题。以下是我们所涵盖的一些其他要点：
- en: We have explored the necessity of ANNs and their broad types, that is, feed-forward
    and recurrent ANNs.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨了ANN的必要性和它们的广泛类型，即前馈和循环ANN。
- en: We have studied the multilayer perceptron ANN and the backpropagation algorithm
    used to train this ANN. We've also provided a simple implementation of the backpropagation
    algorithm in Clojure using matrices and matrix operations.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们研究了多层感知器ANN及其用于训练此ANN的反向传播算法。我们还提供了一个使用矩阵和矩阵运算在Clojure中实现的简单反向传播算法。
- en: We have introduced the Enclog library that can be used to build ANNs. This library
    can be used to model both supervised and unsupervised machine learning problems.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了Enclog库，该库可用于构建ANN。这个库可以用于模拟监督学习和无监督机器学习问题。
- en: We have explored recurrent Elman neural networks, which can be used to produce
    ANNs with a small error in a relatively less number of iterations. We've also
    described how we can create and train such an ANN using the Enclog library.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索了循环Elman神经网络，它可以用于在相对较少的迭代次数中产生具有小误差的ANN。我们还描述了如何使用Enclog库创建和训练这样的ANN。
- en: We introduced SOMs, which are neural networks that can be applied in the domain
    of unsupervised learning. We've also described how we can create and train an
    SOM using the Enclog library.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了SOMs，这些神经网络可以应用于无监督学习的领域。我们还描述了如何使用Enclog库创建和训练SOM。
