["```py\n// logistic implements the logistic function, which\n// is used in logistic regression.\nfunc logistic(x float64) float64 {\n        return 1 / (1 + math.Exp(-x))\n}\n```", "```py\n// Create a new plot.\np, err := plot.New()\nif err != nil {\n    log.Fatal(err)\n}\np.Title.Text = \"Logistic Function\"\np.X.Label.Text = \"x\"\np.Y.Label.Text = \"f(x)\"\n\n// Create the plotter function.\nlogisticPlotter := plotter.NewFunction(func(x float64) float64 { return logistic(x) })\nlogisticPlotter.Color = color.RGBA{B: 255, A: 255}\n\n// Add the plotter function to the plot.\np.Add(logisticPlotter)\n\n// Set the axis ranges.  Unlike other data sets,\n// functions don't set the axis ranges automatically\n// since functions don't necessarily have a\n// finite range of x and y values.\np.X.Min = -10\np.X.Max = 10\np.Y.Min = -0.1\np.Y.Max = 1.1\n\n// Save the plot to a PNG file.\nif err := p.Save(4*vg.Inch, 4*vg.Inch, \"logistic.png\"); err != nil {\n    log.Fatal(err)\n}\n```", "```py\n$ head loan_data.csv \nFICO.Range,Interest.Rate\n735-739,8.90%\n715-719,12.12%\n690-694,21.98%\n695-699,9.99%\n695-699,11.71%\n670-674,15.31%\n720-724,7.90%\n705-709,17.14%\n685-689,14.33%\n```", "```py\nconst (\n    scoreMax = 830.0\n    scoreMin = 640.0\n)\n```", "```py\n// Open the loan dataset file.\nf, err := os.Open(\"loan_data.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a new CSV reader reading from the opened file.\nreader := csv.NewReader(f)\nreader.FieldsPerRecord = 2\n\n// Read in all of the CSV records\nrawCSVData, err := reader.ReadAll()\nif err != nil {\n    log.Fatal(err)\n}\n\n// Create the output file.\nf, err = os.Create(\"clean_loan_data.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a CSV writer.\nw := csv.NewWriter(f)\n\n// Sequentially move the rows writing out the parsed values.\nfor idx, record := range rawCSVData {\n\n    // Skip the header row.\n    if idx == 0 {\n\n        // Write the header to the output file.\n        if err := w.Write(record); err != nil {\n            log.Fatal(err)\n        }\n        continue\n    }\n\n    // Initialize a slice to hold our parsed values.\n    outRecord := make([]string, 2)\n\n    // Parse and standardize the FICO score.\n    score, err := strconv.ParseFloat(strings.Split(record[0], \"-\")[0], 64)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    outRecord[0] = strconv.FormatFloat((score-scoreMin)/(scoreMax-scoreMin), 'f', 4, 64)\n\n    // Parse the Interest rate class.\n    rate, err := strconv.ParseFloat(strings.TrimSuffix(record[1], \"%\"), 64)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    if rate <= 12.0 {\n        outRecord[1] = \"1.0\"\n\n        // Write the record to the output file.\n        if err := w.Write(outRecord); err != nil {\n            log.Fatal(err)\n        }\n        continue\n    }\n\n    outRecord[1] = \"0.0\"\n\n    // Write the record to the output file.\n    if err := w.Write(outRecord); err != nil {\n        log.Fatal(err)\n    }\n}\n\n// Write any buffered data to the underlying writer (standard output).\nw.Flush()\n\nif err := w.Error(); err != nil {\n    log.Fatal(err)\n}\n```", "```py\n$ go build\n$ ./example3 \n$ head clean_loan_data.csv \nFICO_score,class\n0.5000,1.0\n0.3947,0.0\n0.2632,0.0\n0.2895,1.0\n0.2895,1.0\n0.1579,0.0\n0.4211,1.0\n0.3421,0.0\n0.2368,0.0\n```", "```py\n// Open the CSV file.\nloanDataFile, err := os.Open(\"clean_loan_data.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer loanDataFile.Close()\n\n// Create a dataframe from the CSV file.\nloanDF := dataframe.ReadCSV(loanDataFile)\n\n// Use the Describe method to calculate summary statistics\n// for all of the columns in one shot.\nloanSummary := loanDF.Describe()\n\n// Output the summary statistics to stdout.\nfmt.Println(loanSummary)\n\n// Create a histogram for each of the columns in the dataset.\nfor _, colName := range loanDF.Names() {\n\n    // Create a plotter.Values value and fill it with the\n    // values from the respective column of the dataframe.\n    plotVals := make(plotter.Values, loanDF.Nrow())\n    for i, floatVal := range loanDF.Col(colName).Float() {\n        plotVals[i] = floatVal\n    }\n\n    // Make a plot and set its title.\n    p, err := plot.New()\n    if err != nil {\n        log.Fatal(err)\n    }\n    p.Title.Text = fmt.Sprintf(\"Histogram of a %s\", colName)\n\n    // Create a histogram of our values.\n    h, err := plotter.NewHist(plotVals, 16)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Normalize the histogram.\n    h.Normalize(1)\n\n    // Add the histogram to the plot.\n    p.Add(h)\n\n    // Save the plot to a PNG file.\n    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+\"_hist.png\"); err != nil {\n        log.Fatal(err)\n    }\n}\n```", "```py\n$ go build\n$ ./myprogram\n[7x3] DataFrame\n\n column FICO_score class \n 0: mean 0.346782 0.396800\n 1: stddev 0.184383 0.489332\n 2: min 0.000000 0.000000\n 3: 25% 0.210500 0.000000\n 4: 50% 0.315800 0.000000\n 5: 75% 0.447400 1.000000\n 6: max 1.000000 1.000000\n <string> <float> <float>     \n\n$ ls *.png\nclass_hist.png FICO_score_hist.png\n```", "```py\n// Open the clean loan dataset file.\nf, err := os.Open(\"clean_loan_data.csv\")        \nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a dataframe from the CSV file.\n// The types of the columns will be inferred.\nloanDF := dataframe.ReadCSV(f)\n\n// Calculate the number of elements in each set.\ntrainingNum := (4 * loanDF.Nrow()) / 5\ntestNum := loanDF.Nrow() / 5\nif trainingNum+testNum < loanDF.Nrow() {\n    trainingNum++\n}\n\n// Create the subset indices.\ntrainingIdx := make([]int, trainingNum)\ntestIdx := make([]int, testNum)\n\n// Enumerate the training indices.\nfor i := 0; i < trainingNum; i++ {\n    trainingIdx[i] = i\n}\n\n// Enumerate the test indices.\nfor i := 0; i < testNum; i++ {\n    testIdx[i] = trainingNum + i\n}\n\n// Create the subset dataframes.\ntrainingDF := loanDF.Subset(trainingIdx)\ntestDF := loanDF.Subset(testIdx)\n\n// Create a map that will be used in writing the data\n// to files.\nsetMap := map[int]dataframe.DataFrame{\n    0: trainingDF,\n    1: testDF,\n}\n\n// Create the respective files.\nfor idx, setName := range []string{\"training.csv\", \"test.csv\"} {\n\n    // Save the filtered dataset file.\n    f, err := os.Create(setName)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Create a buffered writer.\n    w := bufio.NewWriter(f)\n\n    // Write the dataframe out as a CSV.\n    if err := setMap[idx].WriteCSV(w); err != nil {\n        log.Fatal(err)\n    }\n}\n```", "```py\n$ go build\n$ ./myprogram \n$ wc -l *.csv\n 2046 clean_loan_data.csv\n  410 test.csv\n 1638 training.csv\n 4094 total\n```", "```py\n// logisticRegression fits a logistic regression model\n// for the given data.\nfunc logisticRegression(features *mat64.Dense, labels []float64, numSteps int, learningRate float64) []float64 {\n\n        // Initialize random weights.\n        _, numWeights := features.Dims()\n        weights := make([]float64, numWeights)\n\n        s := rand.NewSource(time.Now().UnixNano())\n        r := rand.New(s)\n\n        for idx, _ := range weights {\n                weights[idx] = r.Float64()\n        }\n\n        // Iteratively optimize the weights.\n        for i := 0; i < numSteps; i++ {\n\n        // Initialize a variable to accumulate error for this iteration.\n        var sumError float64\n\n        // Make predictions for each label and accumulate error.\n        for idx, label := range labels {\n\n            // Get the features corresponding to this label.\n            featureRow := mat64.Row(nil, idx, features)\n\n            // Calculate the error for this iteration's weights.\n            pred := logistic(featureRow[0]*weights[0]\n            featureRow[1]*weights[1])\n            predError := label - pred\n            sumError += math.Pow(predError, 2)\n\n            // Update the feature weights.\n            for j := 0; j < len(featureRow); j++ {\n                weights[j] += learningRate * predError * pred * (1 - pred) * featureRow[j]\n            }\n        }\n    }\n\n    return weights\n}\n```", "```py\n// Open the training dataset file.\nf, err := os.Open(\"training.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a new CSV reader reading from the opened file.\nreader := csv.NewReader(f)\nreader.FieldsPerRecord = 2\n\n// Read in all of the CSV records\nrawCSVData, err := reader.ReadAll()\nif err != nil {\n    log.Fatal(err)\n}\n\n// featureData and labels will hold all the float values that\n// will eventually be used in our training.\nfeatureData := make([]float64, 2*len(rawCSVData))\nlabels := make([]float64, len(rawCSVData))\n\n// featureIndex will track the current index of the features\n// matrix values.\nvar featureIndex int\n\n// Sequentially move the rows into the slices of floats.\nfor idx, record := range rawCSVData {\n\n    // Skip the header row.\n    if idx == 0 {\n        continue\n    }\n\n    // Add the FICO score feature.\n    featureVal, err := strconv.ParseFloat(record[0], 64)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    featureData[featureIndex] = featureVal\n\n    // Add an intercept.\n    featureData[featureIndex+1] = 1.0\n\n    // Increment our feature row.\n    featureIndex += 2\n\n    // Add the class label.\n    labelVal, err := strconv.ParseFloat(record[1], 64)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    labels[idx] = labelVal\n}        \n\n// Form a matrix from the features.\nfeatures := mat64.NewDense(len(rawCSVData), 2, featureData)\n\n// Train the logistic regression model.\nweights := logisticRegression(features, labels, 100, 0.3)\n\n// Output the Logistic Regression model formula to stdout.\nformula := \"p = 1 / ( 1 + exp(- m1 * FICO.score - m2) )\"\nfmt.Printf(\"\\n%s\\n\\nm1 = %0.2f\\nm2 = %0.2f\\n\\n\", formula, weights[0], weights[1])\n```", "```py\n$ go build\n$ ./myprogram\n\np = 1 / ( 1 + exp(- m1 * FICO.score - m2) )\n\nm1 = 13.65\nm2 = -4.89\n\n```", "```py\n// predict makes a prediction based on our\n// trained logistic regression model.\nfunc predict(score float64) float64 {\n\n    // Calculate the predicted probability.\n    p := 1 / (1 + math.Exp(-13.65*score+4.89))\n\n    // Output the corresponding class.\n    if p >= 0.5 {\n        return 1.0\n    }\n\n    return 0.0\n}\n```", "```py\n// Open the test examples.\nf, err := os.Open(\"test.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a new CSV reader reading from the opened file.\nreader := csv.NewReader(f)\n\n// observed and predicted will hold the parsed observed and predicted values\n// form the labeled data file.\nvar observed []float64\nvar predicted []float64\n\n// line will track row numbers for logging.\nline := 1\n\n// Read in the records looking for unexpected types in the columns.\nfor {\n\n    // Read in a row. Check if we are at the end of the file.\n    record, err := reader.Read()\n    if err == io.EOF {\n        break\n    }\n\n    // Skip the header.\n    if line == 1 {\n        line++\n        continue\n    }\n\n    // Read in the observed value.\n    observedVal, err := strconv.ParseFloat(record[1], 64)\n    if err != nil {\n        log.Printf(\"Parsing line %d failed, unexpected type\\n\", line)\n        continue\n    }\n\n    // Make the corresponding prediction.\n    score, err := strconv.ParseFloat(record[0], 64)\n    if err != nil {\n        log.Printf(\"Parsing line %d failed, unexpected type\\n\", line)\n        continue\n    }\n\n    predictedVal := predict(score)\n\n    // Append the record to our slice, if it has the expected type.\n    observed = append(observed, observedVal)\n    predicted = append(predicted, predictedVal)\n    line++\n}\n\n// This variable will hold our count of true positive and\n// true negative values.\nvar truePosNeg int\n\n// Accumulate the true positive/negative count.\nfor idx, oVal := range observed {\n    if oVal == predicted[idx] {\n        truePosNeg++\n    }\n}\n\n// Calculate the accuracy (subset accuracy).\naccuracy := float64(truePosNeg) / float64(len(observed))\n\n// Output the Accuracy value to standard out.\nfmt.Printf(\"\\nAccuracy = %0.2f\\n\\n\", accuracy)\n\n```", "```py\n$ go build\n$ ./myprogram\n\nAccuracy = 0.83\n```", "```py\n$ head iris.csv \nsepal_length,sepal_width,petal_length,petal_width,species\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n```", "```py\n// Read in the iris data set into golearn \"instances\".\nirisData, err := base.ParseCSVToInstances(\"iris.csv\", true)\nif err != nil {\n    log.Fatal(err)\n}\n```", "```py\n// Initialize a new KNN classifier.  We will use a simple\n// Euclidean distance measure and k=2.\nknn := knn.NewKnnClassifier(\"euclidean\", \"linear\", 2)\n\n// Use cross-fold validation to successively train and evaluate the model\n// on 5 folds of the data set.\ncv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, knn, 5)\nif err != nil {\n    log.Fatal(err)\n}\n```", "```py\n// Get the mean, variance and standard deviation of the accuracy for the\n// cross validation.\nmean, variance := evaluation.GetCrossValidatedMetric(cv, evaluation.GetAccuracy)\nstdev := math.Sqrt(variance)\n\n// Output the cross metrics to standard out.\nfmt.Printf(\"\\nAccuracy\\n%.2f (+/- %.2f)\\n\\n\", mean, stdev*2)\n```", "```py\n$ go build\n$ ./myprogram \nOptimisations are switched off\nOptimisations are switched off\nOptimisations are switched off\nOptimisations are switched off\nOptimisations are switched off\nKNN: 95.00 % done\nAccuracy\n0.95 (+/- 0.05)\n```", "```py\n// Read in the iris data set into golearn \"instances\".\nirisData, err := base.ParseCSVToInstances(\"iris.csv\", true)\nif err != nil {\n    log.Fatal(err)\n}\n\n// This is to seed the random processes involved in building the\n// decision tree.\nrand.Seed(44111342)\n\n// We will use the ID3 algorithm to build our decision tree.  Also, we\n// will start with a parameter of 0.6 that controls the train-prune split.\ntree := trees.NewID3DecisionTree(0.6)\n\n// Use cross-fold validation to successively train and evaluate the model\n// on 5 folds of the data set.\ncv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, tree, 5)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Get the mean, variance and standard deviation of the accuracy for the\n// cross validation.\nmean, variance := evaluation.GetCrossValidatedMetric(cv, evaluation.GetAccuracy)\nstdev := math.Sqrt(variance)\n\n// Output the cross metrics to standard out.\nfmt.Printf(\"\\nAccuracy\\n%.2f (+/- %.2f)\\n\\n\", mean, stdev*2)\n```", "```py\n$ go build\n$ ./myprogram \n\nAccuracy\n0.94 (+/- 0.06)\n```", "```py\n// Assemble a random forest with 10 trees and 2 features per tree,\n// which is a sane default (number of features per tree is normally set\n// to sqrt(number of features)).\nrf := ensemble.NewRandomForest(10, 2)\n\n// Use cross-fold validation to successively train and evaluate the model\n// on 5 folds of the data set.\ncv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, rf, 5)\nif err != nil {\n    log.Fatal(err)\n}\n```", "```py\n// convertToBinary utilizes built in golearn functionality to\n// convert our labels to a binary label format.\nfunc convertToBinary(src base.FixedDataGrid) base.FixedDataGrid {\n    b := filters.NewBinaryConvertFilter()\n    attrs := base.NonClassAttributes(src)\n    for _, a := range attrs {\n        b.AddAttribute(a)\n    }\n    b.Train()\n    ret := base.NewLazilyFilteredInstances(src, b)\n    return ret\n}\n```", "```py\n// Read in the loan training data set into golearn \"instances\".\ntrainingData, err := base.ParseCSVToInstances(\"training.csv\", true)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Initialize a new Naive Bayes classifier.\nnb := naive.NewBernoulliNBClassifier()\n\n// Fit the Naive Bayes classifier.\nnb.Fit(convertToBinary(trainingData))\n\n// Read in the loan test data set into golearn \"instances\".\n// This time we will utilize a template of the previous set\n// of instances to validate the format of the test set.\ntestData, err := base.ParseCSVToTemplatedInstances(\"test.csv\", true, trainingData)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Make our predictions.\npredictions := nb.Predict(convertToBinary(testData))\n\n// Generate a Confusion Matrix.\ncm, err := evaluation.GetConfusionMatrix(testData, predictions)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Retrieve the accuracy.\naccuracy := evaluation.GetAccuracy(cm)\nfmt.Printf(\"\\nAccuracy: %0.2f\\n\\n\", accuracy)\n```", "```py\n$ go build\n$ ./myprogram \n\nAccuracy: 0.63\n```"]