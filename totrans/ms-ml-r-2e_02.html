<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Linear Regression - The Blocking and Tackling of Machine Learning</h1>
            </header>

            <article>
                
<div class="packt_quote">"Some people try to find things in this game that don't exist, but football is only two things - blocking and tackling."<br/>
                                                                    - Vince Lombardi, Hall of Fame Football Coach</div>
<p>It is important that we get started with a simple, yet extremely effective technique that has been used for a long time: <strong>linear regression</strong>. Albert Einstein is believed to have remarked at one time or another that things should be made as simple as possible, but no simpler. This is sage advice and a good rule of thumb in the development of algorithms for machine learning. Considering the other techniques that we will discuss later, there is no simpler model than tried and tested linear regression, which uses the <strong>least squares approach</strong> to predict a quantitative outcome. In fact, one can consider it to be the foundation of all the methods that we will discuss later, many of which are mere extensions. If you can master the linear regression method, well, then quite frankly, I believe you can master the rest of this book. Therefore, let us consider this a good starting point for our journey towards becoming a machine learning guru.</p>
<p>This chapter covers introductory material, and an expert in this subject can skip ahead to the next topic. Otherwise, ensure that you thoroughly understand this topic before venturing to other, more complex learning methods. I believe you will discover that many of your projects can be addressed by just applying what is discussed in the following section. Linear regression is probably the easiest model to explain to your customers, most of whom will have at least a cursory understanding of <strong>R-squared</strong>. Many of them will have been exposed to it at great depth and thus be comfortable with variable contribution, collinearity, and the like.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Univariate linear regression</h1>
            </header>

            <article>
                
<p>We begin by looking at a simple way to predict a quantitative response, <em>Y</em>, with one predictor variable, <em>x</em>, assuming that <em>Y</em> has a linear relationship with <em>x</em>. The model for this can be written as, <em>Y = B0 + B1x + e</em>. We can state it as the expected value of <em>Y</em> being a function of the parameters <em>B0</em> (the intercept) plus <em>B1</em> (the slope) times <em>x</em>, plus an error term <em>e</em>. The least squares approach chooses the model parameters that minimize the <strong>Residual Sum of Squares</strong> (<strong>RSS</strong>) of the predicted <em>y</em> values versus the actual <em>Y</em> values. For a simple example, let's say we have the actual values of <em>Y1</em> and <em>Y2</em> equal to <em>10</em> and <em>20</em> respectively, along with the predictions of <em>y1</em> and <em>y2</em> as <em>12</em> and <em>18</em>. To calculate RSS, we add the squared differences <em>RSS = (Y1 - y1)<sup>2</sup> + (Y2 - y2)<sup>2</sup></em>, which, with simple substitution, yields <em>(10 - 12)<sup>2</sup> + (20 - 18)<sup>2</sup> = 8</em>.</p>
<p>I once remarked to a peer during our Lean Six Sigma Black Belt training that it's all about the sum of squares; understand the sum of squares and the rest will flow naturally. Perhaps that is true, at least to some extent.</p>
<p>Before we begin with an application, I want to point out that, if you read the headlines of various research breakthroughs, you should do so with a jaded eye and a skeptical mind as the conclusion put forth by the media may not be valid. As we shall see, R, and any other software for that matter, will give us a solution regardless of the inputs. However, just because the math makes sense and a high correlation or R-squared statistic is reported doesn't mean that the conclusion is valid.</p>
<p>To drive this point home, let's have a look at the famous <kbd>Anscombe</kbd> dataset, which is available in R. The statistician Francis Anscombe produced this set to highlight the importance of data visualization and outliers when analyzing data. It consists of four pairs of <em>X</em> and <em>Y</em> variables that have the same statistical properties but when plotted show something very different. I have used the data to train colleagues and to educate business partners on the hazards of fixating on statistics without exploring the data and checking assumptions. I think this is a good place to start should you have a similar need. It is a brief digression before moving on to serious modeling:</p>
<pre>
    <strong>&gt; #call up and explore the data</strong><br/>    <br/>    <strong>&gt; data(anscombe)</strong><br/>    <br/>    <strong>&gt; attach(anscombe)</strong><br/>    <br/>    <strong>&gt; anscombe</strong><br/>    <strong>   x1 x2 x3 x4    y1   y2    y3    y4</strong><br/>    <strong>1  10 10 10  8  8.04 9.14  7.46  6.58</strong><br/>    <strong>2   8  8  8  8  6.95 8.14  6.77  5.76</strong><br/>    <strong>3  13 13 13  8  7.58 8.74 12.74  7.71</strong><br/>    <strong>4   9  9  9  8  8.81 8.77  7.11  8.84</strong><br/>    <strong>5  11 11 11  8  8.33 9.26  7.81  8.47</strong><br/>    <strong>6  14 14 14  8  9.96 8.10  8.84  7.04</strong><br/>    <strong>7   6  6  6  8  7.24 6.13  6.08  5.25</strong><br/>    <strong>8   4  4  4 19  4.26 3.10  5.39 12.50</strong><br/>    <strong>9  12 12 12  8 10.84 9.13  8.15  5.56</strong><br/>    <strong>10  7  7  7  8  4.82 7.26  6.42  7.91</strong><br/>    <strong>11  5  5  5  8  5.68 4.74  5.73  6.89</strong>
</pre>
<p>As we shall see, each of the pairs has the same correlation coefficient: <kbd>0.816</kbd>. The first two are as follows:</p>
<pre>
    <strong>&gt; cor(x1, y1) #correlation of x1 and y1</strong><br/>    <strong>[1] 0.8164205</strong><br/>    <br/>    <strong>&gt; cor(x2, y1) #correlation of x2 and y2</strong><br/>    <br/>    <strong>[1] 0.8164205</strong>
</pre>
<p>The real insight here, as <kbd>Anscombe</kbd> intended, is when we plot all the four pairs together, as follows:</p>
<pre>
    <strong>&gt; par(mfrow = c(2,2)) #create a 2x2 grid for <br/>       plotting</strong><br/>    <br/>    <strong>&gt; plot(x1, y1, main = "Plot 1")</strong><br/>    <br/>    <strong>&gt; plot(x2, y2, main = "Plot 2")</strong><br/>    <br/>    <strong>&gt; plot(x3, y3, main = "Plot 3")</strong><br/>    <br/>    <strong>&gt; plot(x4, y4, main = "Plot 4")</strong>
</pre>
<div class="packt_tip"><span class="packt_screen">Downloading the example code</span><br/>
You can download the example code files for all Packt books you have purchased from your account at <a href="http://www.packtpub.com"><span class="URLPACKT">http://www.packtpub.com</span></a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support"><span class="URLPACKT">http://www.packtpub.com/support</span></a> and register to have the files e-mailed directly to you.</div>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="394" width="618" class="image-border" src="assets/image_02_01.png"/></div>
<p>As we can see, <strong>Plot 1</strong> appears to have a true linear relationship, <strong>Plot 2</strong> is curvilinear, <strong>Plot 3</strong> has a dangerous outlier, and <strong>Plot 4</strong> is driven by one outlier. There you have it, a cautionary tale about  the dangers of solely relying on correlation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>Our first case focuses on the goal of predicting the water yield (in inches) of the Snake River Watershed in Wyoming, USA, as a function of the water content of the year's snowfall. This forecast will be useful in managing the water flow and reservoir levels as the Snake River provides much-needed irrigation water for the farms and ranches of several western states. The <kbd>snake</kbd> dataset is available in the <kbd>alr3</kbd> package (note that alr stands for applied linear regression):</p>
<pre>
    <strong>&gt; install.packages("alr3")</strong><br/>    <strong>&gt; library(alr3)</strong><br/>    <strong>&gt; data(snake)</strong><br/>    <strong>&gt; dim(snake)</strong><br/>    <strong>[1] 17  2</strong><br/>    <strong>&gt; head(snake)</strong><br/>    <strong>     X    Y</strong><br/>    <strong>1 23.1 10.5</strong><br/>    <strong>2 32.8 16.7</strong><br/>    <strong>3 31.8 18.2</strong><br/>    <strong>4 32.0 17.0</strong><br/>    <strong>5 30.4 16.3</strong><br/>    <strong>6 24.0 10.5</strong>
</pre>
<p>Now that we have <kbd>17</kbd> observations, data exploration can begin. But first, let's change <kbd>X</kbd> and <kbd>Y</kbd> to meaningful variable names, as follows:</p>
<pre>
    <strong>&gt; names(snake) &lt;- c("content", "yield")</strong><br/>    <strong>&gt; attach(snake) # attach data with new names</strong><br/>    <strong>&gt; head(snake)</strong><br/>    <br/>    <strong>  content yield</strong><br/>    <strong>1    23.1  10.5</strong><br/>    <strong>2    32.8  16.7</strong><br/>    <strong>3    31.8  18.2</strong><br/>    <strong>4    32.0  17.0</strong><br/>    <strong>5    30.4  16.3</strong><br/>    <strong>6    24.0  10.5</strong><br/>    <br/>    <strong>&gt; plot(content, yield, xlab = "water content of <br/>        snow", ylab = "water yield")</strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="337" width="529" class="image-border" src="assets/image_02_02.png"/></div>
<p>This is an interesting plot as the data is linear and has a slight curvilinear shape driven by two potential outliers at both ends of the extreme. As a result, transforming the data or deleting an outlying observation may be warranted.</p>
<p>To perform a linear regression in R, one uses the <kbd>lm()</kbd> function to create a model in the standard form of <em>fit = lm(Y ~ X)</em>. You can then test your assumptions using various functions on your fitted model by using the following code:</p>
<pre>
    <strong>&gt; yield.fit &lt;- lm(yield ~ content)</strong><br/>    <br/>    <strong>&gt; summary(yield.fit)</strong><br/>    <br/>    <strong>Call:</strong><br/>    <strong>lm(formula = yield ~ content)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>        Min      1Q  Median      3Q     Max</strong><br/>    <strong>-2.1793 -1.5149 -0.3624  1.6276  3.1973</strong><br/>    <br/>    <strong>Coefficients:</strong><strong> Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept)  0.72538    1.54882   0.468    0.646    </strong><br/>    <strong>content      0.49808    0.04952  10.058 4.63e-08 <br/>    ***</strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <br/>    <strong>Residual standard error: 1.743 on 15 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.8709,    Adjusted R-squared:  <br/>       0.8623</strong><br/>    <strong>F-statistic: 101.2 on 1 and 15 DF,  p-value: <br/>       4.632e-08</strong>
</pre>
<p>With the <kbd>summary()</kbd> function, we can examine a number of items including the model specification, descriptive statistics about the residuals, the coefficients, codes to model significance, and a summary on model error and fit. Right now, let's focus on the parameter coefficient estimates, see if our predictor variable has a significant p-value, and if the overall model F-test has a significant p-value. Looking at the parameter estimates, the model tells us that the <kbd>yield</kbd> is equal to <kbd>0.72538</kbd> plus <kbd>0.49808</kbd> times the <kbd>content</kbd>. It can be stated that, for every 1 unit change in the content, the yield will increase by <kbd>0.49808</kbd> units. The <kbd>F-statistic</kbd> is used to test the null hypothesis that the model coefficients are all 0.</p>
<p>Since the <kbd>p-value</kbd> is highly significant, we can reject the null and move on to the t-test for content, which tests the null hypothesis that it is 0. Again, we can reject the null. Additionally, we can see <kbd>Multiple R-squared</kbd> and <kbd>Adjusted R-squared</kbd> values. <kbd>Adjusted R-squared</kbd> will be covered under the multivariate regression topic, so let's zero in on <kbd>Multiple R-squared</kbd>; here we see that it is <kbd>0.8709</kbd>. In theory, it can range from 0 to 1 and is a measure of the strength of the association between <em>X</em> and <em>Y</em>. The interpretation in this case is that 87 percent of the variation in the <strong>water yield</strong> can be explained by the <strong>water content of snow</strong>. On a side note, R-squared is nothing more than the correlation coefficient of [X, Y] squared.</p>
<p>We can recall our scatterplot and now add the best fit line produced by our model using the following code:</p>
<pre>
    <strong>&gt; plot(content, yield)</strong><br/>    <br/>    <strong>&gt; abline(yield.fit, lwd=3, col="red")</strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="327" width="512" class="image-border" src="assets/image_02_03.png"/></div>
<p>A linear regression model is only as good as the validity of its assumptions, which can be summarized as follows:</p>
<ul>
<li><strong>Linearity</strong>: This is a linear relationship between the predictor and the response variables. If this relationship is not clearly present, transformations (log, polynomial, exponent, and so on) of <em>X</em> or <em>Y</em> may solve the problem.</li>
<li><strong>Non-correlation of errors</strong>: A common problem in time series and panel data where <em>e<sub>n</sub> = beta<sub>n-1</sub></em>; if the errors are correlated, you run the risk of creating a poorly specified model.</li>
<li><strong>Homoscedasticity</strong>: Normally the distributed and constant variance of errors, which means that the variance of errors is constant across different values of inputs. Violations of this assumption can create biased coefficient estimates, leading to statistical tests for significance that can be either too high or too low. This, in turn, leads to a wrong conclusion. This violation is referred to as <strong>heteroscedasticity.</strong></li>
<li><strong>No collinearity</strong>: No linear relationship between two predictor variables, which is to say that there should be no correlation between the features. This, again, can lead to biased estimates.</li>
<li><strong>Presence of outliers</strong>: Outliers can severely skew the estimation, and ideally they must be removed prior to fitting a model using linear regression; As we saw in the Anscombe example, this can lead to a biased estimate.</li>
</ul>
<p>As we are building a univariate model independent of time, we will concern ourselves only with linearity and heteroscedasticity. The other assumptions will become important in the next section. The best way to initially check the assumptions is by producing plots. The <kbd>plot()</kbd> function, when combined with a linear model fit, will automatically produce four plots allowing you to examine the assumptions. R produces the plots one at a time and you advance through them by hitting the <em><span class="KeyPACKT">Enter</span></em> key. It is best to examine all four simultaneously and we do it in the following manner:</p>
<pre>
    <strong>&gt; par(mfrow = c(2,2))</strong><br/>    <br/>    <strong>&gt; plot(yield.fit)</strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="351" width="549" class="image-border" src="assets/image_02_04.png"/></div>
<p>The two plots on the left allow us to examine the homoscedasticity of errors and non-linearity. What we are looking for is some type of pattern or, more importantly, that no pattern exists. Given the sample size of only 17 observations, nothing obvious can be seen. Common heteroscedastic errors will appear to be u-shaped, inverted u-shaped, or clustered close together on the left of the plot. They will become wider as the fitted values increase (a funnel shape). It is safe to conclude that no violation of homoscedasticity is apparent in our model.</p>
<p>The <strong>Normal Q-Q</strong> plot in the upper-right corner helps us to determine if the residuals are normally distributed. The <strong>Quantile-Quantile</strong> (<strong>Q-Q</strong>) represents the quantile values of one variable plotted against the quantile values of another. It appears that the outliers (observations <strong>7</strong>, <strong>9</strong>, and <strong>10</strong>), may be causing a violation of the assumption. The <strong>Residuals vs Leverage</strong> plot can tell us what observations, if any, are unduly influencing the model; in other words, if there are any outliers we should be concerned about. The statistic is <strong>Cook's distance</strong> or <strong>Cook's D</strong>, and it is generally accepted that a value greater than 1 should be worthy of further inspection.</p>
<p>What exactly is further inspection? This is where art meets science. The easy way out would be to simply delete the observation, in this case number <strong>9</strong>, and redo the model. However, a better option may be to transform the predictor and/or the response variables. If we just delete observation <strong>9</strong>, then maybe observations <strong>10</strong> and <strong>13</strong> would fall outside the band for greater than 1. I believe that this is where domain expertise can be critical. More times than I can count, I have found that exploring and understanding outliers can yield valuable insights. When we first examined the previous scatterplot, I pointed out the potential outliers and these happen to be observations number <strong>9</strong> and number <strong>13</strong>. As an analyst, it would be critical to discuss with the appropriate subject matter experts to understand why this is the case. Is it a measurement error? Is there a logical explanation for these observations? I certainly don't know, but this is an opportunity to increase the value that you bring to an organization.</p>
<p>Having said that, we can drill down on the current model by examining, in more detail, the <strong>Normal Q-Q</strong> plot. R does not provide confidence intervals to the default Q-Q plot, and given our concerns in looking at the base plot, we should check the confidence intervals. The <kbd>qqPlot()</kbd> function of the <kbd>car</kbd> package automatically provides these confidence intervals. Since the <kbd>car</kbd> package is loaded along with the <kbd>alr3</kbd> package, I can produce the plot with one line of code:</p>
<pre>
    <strong>&gt; qqPlot(yield.fit)</strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="345" width="541" class="image-border" src="assets/image_02_05.png"/></div>
<p>According to the plot, the residuals are normally distributed. I think this can give us the confidence to select the model with all the observations. A clear rationale and judgment would be needed to attempt other models. If we could clearly reject the assumption of normally distributed errors, then we would probably have to examine the variable transformations and/or observation deletion.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Multivariate linear regression</h1>
            </header>

            <article>
                
<p>You may be asking yourself whether you will ever have just one predictor variable in the real world. That is indeed a fair question and certainly a very rare case (time series can be a common exception). Most likely, several, if not many, predictor variables or features--as they are affectionately termed in machine learning--will have to be included in your model. And with that, let's move on to multivariate linear regression and a new business case.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>In keeping with the water conservation/prediction theme, let's look at another dataset in the <kbd>alr3</kbd> package, appropriately named <kbd>water</kbd>. During the writing of the first edition of this book, the severe drought in Southern California caused much alarm. Even the Governor, Jerry Brown, began to take action with a call to citizens to reduce water usage by 20 percent. For this exercise, let's say we have been commissioned by the state of California to predict water availability. The data provided to us contains 43 years of snow precipitation, measured at six different sites in the Owens Valley. It also contains a response variable for water availability as the stream runoff volume near Bishop, California, which feeds into the Owens Valley aqueduct, and eventually the Los Angeles aqueduct. Accurate predictions of the stream runoff will allow engineers, planners, and policy makers to plan conservation measures more effectively. The model we are looking to create will consist of the form <em>Y = B0 + B1x1 +...Bnxn + e</em>, where the predictor variables (features) can be from 1 to <em>n</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>To begin, we will load the dataset named <kbd>water</kbd> and define the structure of the <kbd>str()</kbd> function as follows:</p>
<pre>
    <strong>&gt; data(water)</strong><br/>    <br/>    <strong>&gt; str(water)</strong><br/>    <strong>'data.frame':   43 obs. of  8 variables:</strong><br/>    <strong>$ Year   : int  1948 1949 1950 1951 1952 1953 1954 <br/>      1955 1956 1957 ...</strong><br/>    <strong>$ APMAM  : num  9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 <br/>       10.5 9.1 ...</strong><br/>    <strong>$ APSAB  : num  3.58 4.82 3.77 4.46 4.99 5.65 1.45 <br/>       7.44 5.85 6.13 ...</strong><br/>    <strong>$ APSLAKE: num  3.91 5.2 3.67 3.93 4.88 4.91 1.77 <br/>       6.51 3.38 4.08 ...</strong><br/>    <strong>$ OPBPC  : num  4.1 7.55 9.52 11.14 16.34 ...</strong><br/>    <strong>$ OPRC   : num  7.43 11.11 12.2 15.15 20.05 ...</strong><br/>    <strong>$ OPSLAKE: num  6.47 10.26 11.35 11.13 22.81 ...</strong><br/>    <strong>$ BSAAM  : int  54235 67567 66161 68094 107080 <br/>       67594 65356 67909 92715 70024 ...</strong>
</pre>
<p>Here we have eight features and one response variable, <kbd>BSAAM</kbd>. The observations start in 1943 and run for 43 consecutive years. Since for this exercise we are not concerned with what year the observations occurred in, it makes sense to create a new data frame excluding the year vector. This is quite easy to do. With one line of code, we can create the new data frame, and then verify that it works with the <kbd>head()</kbd> function:</p>
<pre>
    <strong>&gt; socal.water &lt;- water[ ,-1] #new dataframe with <br/>      the deletion of <br/>      column 1</strong><br/>    <br/>    <strong>&gt; head(socal.water)</strong><br/>    <strong>  APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM</strong><br/>    <strong>1  9.13  3.58    3.91  4.10  7.43    6.47  54235</strong><br/>    <strong>2  5.28  4.82    5.20  7.55 11.11   10.26  67567</strong><br/>    <strong>3  4.20  3.77    3.67  9.52 12.20   11.35  66161</strong><br/>    <strong>4  4.60  4.46    3.93 11.14 15.15   11.13  68094</strong><br/>    <strong>5  7.15  4.99    4.88 16.34 20.05   22.81 107080</strong><br/>    <strong>6  9.70  5.65    4.91  8.88  8.15    7.41  67594</strong>
</pre>
<p>With all the features being quantitative, it makes sense to look at the correlation statistics and then produce a matrix of scatterplots. The correlation coefficient or <strong>Pearson's r</strong>, is a measure of both the strength and direction of the linear relationship between two variables. The statistic will be a number between -1 and 1, where -1 is the total negative correlation and +1 is the total positive correlation. The calculation of the coefficient is the covariance of the two variables divided by the product of their standard deviations. As previously discussed, if you square the correlation coefficient, you will end up with R-squared.</p>
<p>There are a number of ways to produce a matrix of correlation plots. Some prefer to produce <strong>heatmaps</strong>, but I am a big fan of what is produced with the <kbd>corrplot</kbd> package. It can produce a number of different variations including ellipse, circle, square, number, shade, color, and pie. I like the <kbd>ellipse</kbd> method, but feel free to experiment with the others. Let's load the <kbd>corrplot</kbd> package, create a correlation object using the base <kbd>cor()</kbd> function, and examine the following results:</p>
<pre>
    <strong>&gt; library(corrplot)</strong><br/>    <br/>    <strong>&gt; water.cor &lt;- cor(socal.water)</strong><br/>    <br/>    <strong>&gt; water.cor</strong><br/>  <strong>  APMAM      APSAB    APSLAKE      OPBPC      </strong><br/>    <strong>APMAM   1.0000000 0.82768637 0.81607595 0.12238567 </strong><br/>    <strong>APSAB   0.8276864 1.00000000 0.90030474 0.03954211 </strong><br/>    <strong>APSLAKE 0.8160760 0.90030474 1.00000000 0.09344773 </strong><br/>    <strong>OPBPC   0.1223857 0.03954211 0.09344773 1.00000000 </strong><br/>    <strong>OPRC    0.1544155 0.10563959 0.10638359 0.86470733 </strong><br/>    <strong>OPSLAKE 0.1075421 0.02961175 0.10058669 0.94334741 </strong><br/>    <strong>BSAAM   0.2385695 0.18329499 0.24934094 0.88574778 </strong><br/>  <strong>           OPRC    OPSLAKE     BSAAM</strong><br/>    <strong>APMAM   0.1544155 0.10754212 0.2385695</strong><br/>    <strong>APSAB   0.1056396 0.02961175 0.1832950</strong><br/>    <strong>APSLAKE 0.1063836 0.10058669 0.2493409</strong><br/>    <strong>OPBPC   0.8647073 0.94334741 0.8857478</strong><br/>    <strong>OPRC    1.0000000 0.91914467 0.9196270</strong><br/>    <strong>OPSLAKE 0.9191447 1.00000000 0.9384360</strong><br/>    <strong>BSAAM   0.9196270 0.93843604 1.0000000</strong>
</pre>
<p>So, what does this tell us? First of all, the response variable is highly and positively correlated with the OP features with <kbd>OPBPC</kbd> as <kbd>0.8857</kbd>, <kbd>OPRC</kbd> as <kbd>0.9196</kbd>, and <kbd>OPSLAKE</kbd> as <kbd>0.9384</kbd>. Also note that the AP features are highly correlated with each other and the OP features as well. The implication is that we may run into the issue of multi-collinearity. The correlation plot matrix provides a nice visual of the correlations as follows:</p>
<pre>
    <strong>&gt; corrplot(water.cor, method = "ellipse")</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="272" width="427" class="image-border" src="assets/image_02_06.png"/></div>
<p>Another popular visual is a scatterplot matrix. This can be called with the <kbd>pairs()</kbd> function. It reinforces what we saw in the correlation plot in the previous output:</p>
<pre>
<strong>    &gt; pairs(~ ., data = socal.water)</strong>
</pre>
<p><span>The output of the preceding code snippet is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="407" width="639" class="image-border" src="assets/image_02_07.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>One of the key elements that we will cover here is the very important task of feature selection. In this chapter, we will discuss the best subsets regression methods stepwise, using the <kbd>leaps</kbd> package. Later chapters will cover more advanced techniques.</p>
<p><strong>Forward stepwise selection</strong> starts with a model that has zero features; it then adds the features one at a time until all the features are added. A selected feature is added in the process that creates a model with the lowest RSS. So in theory, the first feature selected should be the one that explains the response variable better than any of the others, and so on.</p>
<div class="packt_infobox">It is important to note that adding a feature will always decrease RSS and increase R-squared, but it will not necessarily improve the model <kbd>fit</kbd> and interpretability.</div>
<p><strong>Backward stepwise regression</strong> begins with all the features in the model and removes the least useful, one at a time. A hybrid approach is available where the features are added through forward stepwise regression, but the algorithm then examines if any features that no longer improve the model fit can be removed. Once the model is built, the analyst can examine the output and use various statistics to select the features they believe provide the best fit.</p>
<p>It is important to add here that stepwise techniques can suffer from serious issues. You can perform a forward stepwise on a dataset, then a backward stepwise, and end up with two completely conflicting models. The bottomline is that stepwise can produce biased regression coefficients; in other words, they are too large and the confidence intervals are too narrow (Tibshirani, 1996).</p>
<p>Best subsets regression can be a satisfactory alternative to the stepwise methods for feature selection. In best subsets regression, the algorithm fits a model for all the possible feature combinations; so if you have 3 features, 7 models will be created. As with stepwise regression, the analyst will need to apply judgment or statistical analysis to select the optimal model. Model selection will be the key topic in the discussion that follows. As you might have guessed, if your dataset has many features, this can be quite a task, and the method does not perform well when you have more features than observations (<kbd>p</kbd> is greater than <kbd>n</kbd>).</p>
<p>Certainly, these limitations for best subsets do not apply to our task at hand. Given its limitations, we will forgo stepwise, but please feel free to give it a try. We will begin by loading the <kbd>leaps</kbd> package. In order that we may see how feature selection works, we will first build and examine a model with all the features, then drill down with best subsets to select the best fit.</p>
<p>To build a linear model with all the features, we can again use the <kbd>lm()</kbd> function. It will follow the form: <em>fit = lm(y ~ x1 + x2 + x3...xn)</em>. A neat shortcut, if you want to include all the features, is to use a period after the tilde symbol instead of having to type them all in. For starters, let's load the <kbd>leaps</kbd> package and build a model with all the features for examination as follows:</p>
<pre>
    <strong>&gt; library(leaps)</strong><br/>    <br/>    <strong>&gt; fit &lt;- lm(BSAAM ~ ., data = socal.water)</strong><br/>    <br/>    <strong>&gt; summary(fit)</strong><br/>    <br/>    <strong>Call:</strong><br/>    <strong>lm(formula = BSAAM ~ ., data = socal.water)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>   Min     1Q Median     3Q    Max</strong><br/>    <strong>-12690  -4936  -1424   4173  18542</strong><br/>    <br/>    <strong>Coefficients:</strong><br/><strong>    Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept) 15944.67    4099.80   3.889 0.000416 <br/>      ***</strong><br/>    <strong>APMAM         -12.77     708.89  -0.018 0.985725    </strong><br/>    <strong>APSAB        -664.41    1522.89  -0.436 0.665237    </strong><br/>    <strong>APSLAKE      2270.68    1341.29   1.693 0.099112 .  </strong><br/>    <strong>OPBPC          69.70     461.69   0.151 0.880839    </strong><br/>    <strong>OPRC         1916.45     641.36   2.988 0.005031 **</strong><br/>    <strong>OPSLAKE      2211.58     752.69   2.938 0.005729 **</strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 '' 1</strong><br/>    <strong>Residual standard error: 7557 on 36 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.9248,    Adjusted R-squared:  <br/>      0.9123</strong><br/>    <strong>F-statistic: 73.82 on 6 and 36 DF,  p-value: &lt; <br/>      2.2e-16</strong>
</pre>
<p>Just like univariate regression, we examine the <kbd>p-value</kbd> on the <kbd>F-statistic</kbd> to verify that at least one of the coefficients is not zero.  Indeed, the <kbd>p-value</kbd> is highly significant. We should also have significant <kbd>p-values</kbd> for the <kbd>OPRC</kbd> and <kbd>OPSLAKE</kbd> parameters. Interestingly, <kbd>OPBPC</kbd> is not significant despite being highly correlated with the response variable. In short, when we control for the other OP features, <kbd>OPBPC</kbd> no longer explains any meaningful variation of the predictor, which is to say that the feature <kbd>OPBPC</kbd> adds nothing from a statistical standpoint with <kbd>OPRC</kbd> and <kbd>OPSLAKE</kbd> in the model.</p>
<p>With the first model built, let's move on to best subsets. We create the <kbd>sub.fit</kbd> object using the <kbd>regsubsets()</kbd> function of the <kbd>leaps</kbd> package as follows:</p>
<pre>
    <strong>&gt; sub.fit &lt;- regsubsets(BSAAM ~ ., data = <br/>       socal.water)</strong>
</pre>
<p>Then we create the <kbd>best.summary</kbd> object to examine the models further. As with all R objects, you can use the <kbd>names()</kbd> function to list what outputs are available:</p>
<pre>
    <strong>&gt; best.summary &lt;- summary(sub.fit)</strong><br/>    <br/>    <strong>&gt; names(best.summary)</strong><br/>    <strong>[1] "which"  "rsq"    "rss"    "adjr2"  "cp"     <br/>       "bic"    "outmat" "obj"</strong>
</pre>
<p>Other valuable functions in model selection include <kbd>which.min()</kbd> and <kbd>which.max()</kbd>. These functions will provide the model that has the minimum or maximum value respectively, as shown in following code snippet:</p>
<pre>
    <strong>&gt; which.min(best.summary$rss)</strong><br/>    <strong>[1] 6</strong>
</pre>
<p>The code tells us that the model with six features has the smallest RSS, which it should have, as that is the maximum number of inputs and more inputs mean a lower RSS. An important point here is that adding features will always decrease RSS! Furthermore, it will always increase R-squared. We could add a completely irrelevant feature such as the number of wins for the Los Angeles Lakers and RSS would decrease and R-squared would increase. The amount would likely be miniscule, but present nonetheless. As such, we need an effective method to properly select the relevant features.</p>
<p>For feature selection, there are four statistical methods that we will talk about in this chapter: <strong>Aikake's Information</strong> <strong>Criterion</strong> (<strong>AIC</strong>), <strong>Mallow's Cp</strong> (<strong>Cp</strong>), <strong>Bayesian Information Criterion</strong> (<strong>BIC</strong>), and the adjusted R-squared. With the first three, the goal is to minimize the value of the statistic; with adjusted R-squared, the goal is to maximize the statistics value. The purpose of these statistics is to create as parsimonious a model as possible, in other words, to penalize model complexity.</p>
<p>The formulation of these four statistics is as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/image_02_08-1.png"/></div>
<p>In a linear model, <kbd>AIC</kbd> and <kbd>Cp</kbd> are proportional to each other, so we will only concern ourselves with <kbd>Cp</kbd>, which follows the output available in the <kbd>leaps</kbd> package. <kbd>BIC</kbd> tends to select the models with fewer variables than <kbd>Cp</kbd>, so we will compare both. To do so, we can create and analyze two plots side by side. Let's do this for <kbd>Cp</kbd>, followed by <kbd>BIC</kbd>,with the help of the following code snippet:</p>
<pre>
    <strong>&gt; par(mfrow = c(1,2))</strong><br/>    <br/>    <strong>&gt; plot(best.summary$cp, xlab = "number of <br/>       features", ylab = "cp")</strong><br/>    <br/>    <strong>&gt; plot(sub.fit, scale = "Cp")</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="287" width="450" class="image-border" src="assets/image_02_08.png"/></div>
<p>In the plot on the left-hand side, the model with three features has the lowest <strong>cp</strong>. The plot on the right-hand side displays those features that provide the lowest <strong>Cp</strong>. The way to read this plot is to select the lowest <strong>Cp</strong> value at the top of the y axis, which is <strong>1.2</strong>. Then, move to the right and look at the colored blocks corresponding to the x axis. Doing this, we see that <strong>APSLAKE</strong>, <strong>OPRC</strong>, and <strong>OPSLAKE</strong> are the features included in this specific model. By using the <kbd>which.min()</kbd> and <kbd>which.max()</kbd> functions, we can identify how <strong>cp</strong> compares to BIC and the adjusted R-squared:</p>
<pre>
    <strong>&gt; which.min(best.summary$bic)</strong><br/>    <strong>[1] 3</strong><br/>    <br/>    <strong>&gt; which.max(best.summary$adjr2)</strong><br/>    <strong>[1] 3</strong>
</pre>
<p>In this example, BIC and adjusted R-squared match the <strong>Cp</strong> for the optimal model. Now, just as with univariate regression, we need to examine the model and test the assumptions. We'll do this by creating a linear model object and examining the plots much as we did earlier, as follows:</p>
<pre>
    <strong>&gt; best.fit &lt;- lm(BSAAM ~ APSLAKE + OPRC + OPSLAKE, <br/>      data = <br/>      socal.water)</strong><br/>    <br/>    <strong>&gt; summary(best.fit)</strong><br/>    <strong>Call:</strong><br/>    <strong>lm(formula = BSAAM ~ APSLAKE + OPRC + OPSLAKE)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>   Min     1Q Median     3Q    Max</strong><br/>    <strong>-12964  -5140  -1252   4446  18649</strong><br/>    <br/>    <strong>Coefficients:</strong><br/>    <strong>            Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept)  15424.6     3638.4   4.239 0.000133 <br/>    ***</strong><br/>    <strong>APSLAKE       1712.5      500.5   3.421 0.001475 **</strong><br/>    <strong>OPRC          1797.5      567.8   3.166 0.002998 **</strong><br/>    <strong>OPSLAKE       2389.8      447.1   5.346 4.19e-06 <br/>    ***</strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <br/>    <strong>Residual standard error: 7284 on 39 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.9244,    Adjusted R-squared:  <br/>      0.9185</strong><br/>    <strong>F-statistic: 158.9 on 3 and 39 DF,  p-value: &lt; <br/>      2.2e-16</strong>
</pre>
<p>With the three-feature model, <kbd>F-statistic</kbd> and all the t-tests have significant p-values. Having passed the first test, we can produce our diagnostic plots:</p>
<pre>
    <strong>&gt; par(mfrow = c(2,2))</strong><br/>    <br/>    <strong>&gt; plot(best.fit)</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="374" width="631" class="image-border" src="assets/image_02_09.png"/></div>
<p>Looking at the plots, it seems safe to assume that the residuals have a constant variance and are normally distributed. There is nothing in the leverage plot that would indicate a requirement for further investigation.</p>
<p>To investigate the issue of collinearity, one can call up the <strong>Variance Inflation Factor</strong> (<strong>VIF</strong>) statistic. VIF is the ratio of the variance of a feature's coefficient, when fitting the full model, divided by the feature's coefficient variance when fitted by itself. The formula is <em>1 / (1-R<sup>2</sup><sub>i</sub>)</em>, where <kbd>R2i</kbd> is the R-squared for our feature of interest, <kbd>i</kbd>, being regressed by all the other features. The minimum value that the VIF can take is 1, which means no collinearity at all. There are no hard and fast rules, but in general a VIF value that exceeds 5 (or some say 10) indicates a problematic amount of collinearity (James, p.101, 2013). A precise value is difficult to select, because there is no hard statistical cut-off point for when multi-collinearity makes your model unacceptable.</p>
<p>The <kbd>vif()</kbd> function in the <kbd>car</kbd> package is all that is needed to produce the values, as can be seen in the following code snippet:</p>
<pre>
    <strong>&gt; vif(best.fit)</strong><br/>    <br/>    <strong>APSLAKE     OPRC  OPSLAKE</strong><br/>    <strong>1.011499 6.452569 6.444748</strong>
</pre>
<p>It shouldn't be surprising that we have a potential collinearity problem with <strong>OPRC</strong> and <strong>OPSLAKE</strong> (values greater than 5) based on the correlation analysis. A plot of the two variables drives the point home, as seen in the following screenshot:</p>
<pre>
    <strong>&gt; plot(socal.water$OPRC, socal.water$OPSLAKE, xlab <br/>      = "OPRC", ylab = "OPSLAKE")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="294" width="496" class="image-border" src="assets/image_02_10.png"/></div>
<p>The simple solution to address collinearity is to drop the variables to remove the problem without compromising the predictive ability. If we look at the adjusted R-squared from the best subsets, we can see that the two-variable model of APSLAKE and OPSLAKE has produced a value of <kbd>0.90</kbd>, while adding OPRC has only marginally increased it to <kbd>0.92</kbd>:</p>
<pre>
    <strong>&gt; best.summary$adjr2 #adjusted r-squared values</strong><br/>    <strong>[1] 0.8777515 0.9001619 0.9185369 0.9168706 <br/>      0.9146772 0.9123079</strong>
</pre>
<p>Let's have a look at the two-variable model and test its assumptions:</p>
<pre>
    <strong>&gt; fit.2 &lt;- lm(BSAAM ~ APSLAKE+OPSLAKE, data = <br/>      socal.water)</strong><br/>    <br/>    <strong>&gt; summary(fit.2)</strong><br/>    <br/>    <strong>Call:</strong><br/>    <strong>lm(formula = BSAAM ~ APSLAKE + OPSLAKE)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>          Min       1Q   Median       3Q      Max</strong><br/>    <strong>-13335.8  -5893.2   -171.8   4219.5  19500.2</strong><br/>    <br/>    <strong>Coefficients:</strong><br/><strong>        Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept)  19144.9     3812.0   5.022  1.1e-05 <br/>    ***</strong><br/>    <strong>APSLAKE       1768.8      553.7   3.194  0.00273 **</strong><br/>    <strong>OPSLAKE       3689.5      196.0  18.829  &lt; 2e-16  <br/>    ***</strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <br/>    <strong>Residual standard error: 8063 on 40 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.9049,    Adjusted R-squared:  <br/>      0.9002</strong><br/>    <strong>F-statistic: 190.3 on 2 and 40 DF,  p-value: &lt; <br/>      2.2e-16</strong><br/>    <br/>    <strong>&gt; par(mfrow=c(2,2))</strong><br/>    <br/>    <strong>&gt; plot(fit.2)</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="428" width="722" class="image-border" src="assets/image_02_11.png"/></div>
<p>The model is significant, and the diagnostics do not seem to be a cause for concern. This should take care of our collinearity problem as well and we can check that using the <kbd>vif()</kbd> function again:</p>
<pre>
    <strong>&gt; vif(fit.2)</strong><br/>    <br/>    <strong> APSLAKE  OPSLAKE</strong><br/>    <strong>1.010221 1.010221</strong>
</pre>
<p>As I stated previously, I don't believe the plot of fits versus residuals is of concern, but if you have any doubts you can formally test the assumption of the constant variance of errors in R. This test is known as the <strong>Breusch-Pagan</strong> (<strong>BP</strong>) test. For this, we need to load the <kbd>lmtest</kbd> package, and run one line of code. The BP test has the null hypothesis that the error variances are zero versus the alternative of not zero:</p>
<pre>
    <strong>&gt; library(lmtest)</strong><br/>    <br/>    <strong>&gt; bptest(fit.2)</strong><br/>    <br/>  <strong>  studentized Breusch-Pagan test</strong><br/>    <br/>    <strong>data:  fit.2</strong><br/>    <strong>BP = 0.0046, df = 2, p-value = 0.9977</strong>
</pre>
<p>We do not have evidence to reject the null that implies that the error variances are zero because <kbd>p-value = 0.9977</kbd>. The <kbd>BP = 0.0046</kbd> value in the summary of the test is the chi-squared value.</p>
<p>All things considered, it appears that the best predictive model is with the two features APSLAKE and OPSLAKE. The model can explain 90 percent of the variation in the stream runoff volume. To forecast the runoff, it would be equal to 19,145 (the intercept) plus 1,769 times the measurement at APSLAKE plus 3,690 times the measurement at OPSLAKE. A scatterplot of the <kbd>Predicted vs. Actual</kbd> values can be done in base R using the fitted values from the model and the response variable values as follows:</p>
<pre>
    <strong>&gt; plot(fit.2$fitted.values, socal.water$BSAAM, xlab <br/>     = "predicted", ylab = "actual", main = "Predicted <br/>       vs.Actual")</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="300" width="508" class="image-border" src="assets/image_02_12.png"/></div>
<p>Although informative, the base graphics of R are not necessarily ready for a presentation to be made to business partners. However, we can easily spruce up this plot in R. Several packages to improve graphics are available for this example, I will use <kbd>ggplot2</kbd>. Before producing the plot, we must put the predicted values into our data frame, <kbd>socal.water</kbd>. I also want to rename <kbd>BSAAM</kbd> as <kbd>Actual</kbd> and put in a new vector within the data frame, as shown in the following code snippet:</p>
<pre>
    <strong>&gt; socal.water["Actual"] = water$BSAAM #create the <br/>       vector Actual</strong><br/>    <br/>    <strong>&gt; socal.water$Forecast = predict(fit.2) #populate <br/>       Forecast with the predicted values</strong>
</pre>
<p>Next we will load the <kbd>ggplot2</kbd> package and produce a nicer graphic with just one line of code:</p>
<pre>
    <strong>&gt; library(ggplot2)</strong><br/>    <br/>    <strong>&gt; ggplot(socal.water, aes(x = Forecast, y = <br/>       Actual)) + geom_point() + geom_smooth(method = <br/>          lm) + labs(title = "Forecast versus Actuals")</strong>
</pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="305" width="513" class="image-border" src="assets/image_02_13.png"/></div>
<p>Let's examine one final model selection technique before moving on. In the upcoming chapters, we will be discussing cross-validation at some length. Cross-validation is a widely used and effective method of model selection and testing. Why is this necessary at all? It comes down to the bias-variance trade-off. Professor Tarpey of Wright State University has a nice quote on the subject:</p>
<div class="packt_quote">"Often we use regression models to predict future observations. We can use our data to fit the model. However, it is cheating to then access how well the model predicts responses using the same data that was used to estimate the model - this will tend to give overly optimistic results in terms of how well a model is able to predict future observations. If we leave out an observation, fit the model and then predict the left out response, then this will give a less biased idea of how well the model predicts."</div>
<p>The cross-validation technique discussed by Professor Tarpey in the preceding quote is known as the <strong>Leave-One-Out Cross-Validation</strong> (<strong>LOOCV</strong>). In linear models, you can easily perform an LOOCV by examining the <strong>Prediction Error Sum of Squares</strong> (<strong>PRESS</strong>) statistic and selecting the model that has the lowest value. The R library <kbd>MPV</kbd> will calculate the statistic for you, as shown in the following code:</p>
<pre>
    <strong>&gt; library(MPV)  </strong><br/>    <br/>    <strong>&gt; PRESS(best.fit) </strong><br/>    <strong> [1] 2426757258  </strong><br/>    <br/>    <strong>&gt; PRESS(fit.2) </strong><br/>    <strong> [1] 2992801411  </strong>
</pre>
<p>By this statistic alone, we could select our <kbd>best.fit</kbd> model. However, as described previously, I still believe that the more parsimonious model is better in this case. You can build a simple function to calculate the statistic on your own, taking advantage of some elegant matrix algebra as shown in the following code:</p>
<pre>
    <strong>&gt; PRESS.best = sum((resid(best.fit)/(1 - <br/>       hatvalues(best.fit)))^2)  </strong><br/>    <br/>    <strong>&gt; PRESS.fit.2 = sum((resid(fit.2)/(1  -<br/>       hatvalues(fit.2)))^2)</strong><br/>    <br/>    <strong>&gt; PRESS.best </strong><br/>    <strong>[1] 2426757258  </strong><br/>    <br/>    <strong>&gt; PRESS.fit.2 </strong><br/>    <strong>[1] 2992801411  </strong>
</pre>
<p>"What are <kbd>hatvalues</kbd>?" you might ask. Well, if we take our linear model <em>Y = B0 + B1x + e</em>, we can turn this into a matrix notation: <em>Y = XB + E</em>. In this notation, <kbd>Y</kbd> remains unchanged, <kbd>X</kbd> is the matrix of the input values, <kbd>B</kbd> is the coefficient, and <kbd>E</kbd> represents the errors. This linear model solves for the value of <kbd>B</kbd>. Without going into the painful details of matrix multiplication, the regression process yields what is known as a <strong>Hat Matrix</strong>. This matrix maps, or as some say projects, the calculated values of your model to the actual values; as a result, it captures how influential a specific observation is in your model. So, the sum of the squared residuals divided by 1 minus <kbd>hatvalues</kbd> is the same as LOOCV.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Other linear model considerations</h1>
            </header>

            <article>
                
<p>Before moving on, there are two additional linear model topics that we need to discuss. The first is the inclusion of a qualitative feature, and the second is an interaction term; both are explained in the following sections.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Qualitative features</h1>
            </header>

            <article>
                
<p>A qualitative feature, also referred to as a factor, can take on two or more levels such as Male/Female or Bad/Neutral/Good. If we have a feature with two levels, say gender, then we can create what is known as an indicator or dummy feature, arbitrarily assigning one level as <kbd>0</kbd> and the other as <kbd>1</kbd>. If we create a model with just the indicator, our linear model would still follow the same formulation as before, that is, <em>Y = B0 + B1x + e</em>. If we code the feature as male being equal to 0 and female equal to 1, then the expectation for male would just be the intercept <em>B0</em>, while for female it would be <em>B0 + B1x</em>. In the situation where you have more than two levels of the feature, you can create n-1 indicators; so, for three levels you would have two indicators. If you created as many indicators as levels, you would fall into the dummy variable trap, which results in perfect multi-collinearity.</p>
<p>We can examine a simple example to learn how to interpret the output. Let's load the <kbd>ISLR</kbd> package and build a model with the <kbd>Carseats</kbd> dataset using the following code snippet:</p>
<pre>
    <strong>&gt; library(ISLR)</strong><br/>    <br/>    <strong>&gt; data(Carseats)</strong><br/>    <br/>    <strong>&gt; str(Carseats)</strong><br/>    <br/>    <strong>'data.frame':   400 obs. of  11 variables:</strong><br/>    <strong>$ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...</strong><br/>    <strong>$ CompPrice  : num  138 111 113 117 141 124 115 136 <br/>       132 132 ...</strong><br/>    <strong>$ Income     : num  73 48 35 100 64 113 105 81 110 <br/>       113 ...</strong><br/>    <strong>$ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...</strong><br/>    <strong>$ Population : num  276 260 269 466 340 501 45 425 <br/>       108 131 ...</strong><br/>    <strong>$ Price      : num  120 83 80 97 128 72 108 120 124        <br/>       124 ...</strong><br/>    <strong>$ ShelveLoc  : Factor w/ 3 levels <br/>       "Bad","Good","Medium": 1 2 3 3 1 <br/>      1 3 2 3 3 ...</strong><br/>    <strong>$ Age        : num  42 65 59 55 38 78 71 67 76 76 <br/>      ...</strong><br/>    <strong>$ Education  : num  17 10 12 14 13 16 15 10 10 17 <br/>      ...</strong><br/>    <strong>$ Urban      : Factor w/ 2 levels "No","Yes": 2 2 2 <br/>      2 2 1 2 2 1 1 <br/>      ...</strong><br/>    <strong>$ US         : Factor w/ 2 levels "No","Yes": 2 2 2 <br/>      2 1 2 1 2 1 2 <br/>      ..</strong>
</pre>
<p>For this example, we will predict the sales of <kbd>Carseats</kbd> using just <kbd>Advertising</kbd>, a quantitative feature and the qualitative feature <kbd>ShelveLoc</kbd>, which is a factor of three levels: <kbd>Bad</kbd>, <kbd>Good</kbd>, and <kbd>Medium</kbd>. With factors, R will automatically code the indicators for the analysis. We build and analyze the model as follows:</p>
<pre>
    <strong>&gt; sales.fit &lt;- lm(Sales ~ Advertising + ShelveLoc, <br/>       data = Carseats)</strong><br/>    <br/>    <strong>&gt; summary(sales.fit)</strong><br/>    <br/>    <strong>Call:</strong><br/>    <strong>lm(formula = Sales ~ Advertising + ShelveLoc, data = </strong><br/>    <strong>Carseats)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>    Min      1Q  Median      3Q     Max</strong><br/>    <strong>-6.6480 -1.6198 -0.0476  1.5308  6.4098</strong><br/>    <br/>    <strong>Coefficients:</strong><br/> <strong>     Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept)      4.89662    0.25207  19.426  &lt; 2e-<br/>      16 ***</strong><br/>    <strong>Advertising      0.10071    0.01692   5.951 5.88e-<br/>      09 ***</strong><br/>    <strong>ShelveLocGood    4.57686    0.33479  13.671  &lt; 2e-<br/>      16 ***</strong><br/>    <strong>ShelveLocMedium  1.75142    0.27475   6.375 5.11e-<br/>      10 ***</strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <br/>    <strong>Residual standard error: 2.244 on 396 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.3733,    Adjusted R-squared:  <br/>      0.3685</strong><br/>    <strong>F-statistic: 78.62 on 3 and 396 DF,  p-value: &lt; <br/>      2.2e-16</strong>
</pre>
<p>If the shelving location is good, the estimate of sales is almost double of that when the location is bad, given an intercept of <kbd>4.89662</kbd>. To see how R codes the indicator features, you can use the <kbd>contrasts()</kbd> function:</p>
<pre>
    <strong>&gt; contrasts(Carseats$ShelveLoc)</strong><br/>    <br/>    <strong>        Good Medium</strong><br/>    <strong>Bad       0      0</strong><br/>    <strong>Good      1      0</strong><br/>    <strong>Medium    0      1</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Interaction terms</h1>
            </header>

            <article>
                
<p>Interaction terms are similarly easy to code in R. Two features interact if the effect on the prediction of one feature depends on the value of the other feature. This would follow the formulation, <em>Y = B0 + B1x + B2x + B1B2x + e</em>. An example is available in the <kbd>MASS</kbd> package with the <kbd>Boston</kbd> dataset. The response is the median home value, which is <kbd>medv</kbd> in the output. We will use two features: the percentage of homes with a low socioeconomic status, which is termed <kbd>lstat</kbd>, and the age of the home in years, which is termed <kbd>age</kbd> in the following output:</p>
<pre>
    <strong>&gt; library(MASS)</strong><br/>    <br/>    <strong>&gt; data(Boston)</strong><br/>    <br/>    <strong>&gt; str(Boston)</strong><br/>    <br/>    <strong>'data.frame':   506 obs. of  14 variables:</strong><br/>    <strong>$ crim   : num  0.00632 0.02731 0.02729 0.03237 <br/>       0.06905 ...</strong><br/>    <strong>$ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 <br/>       ...</strong><br/>    <strong>$ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 <br/>       7.87 7.87 7.87 <br/>      ...</strong><br/>    <strong>$ chas   : int  0 0 0 0 0 0 0 0 0 0 ...</strong><br/>    <strong>$ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 <br/>      0.524 0.524 <br/>      0.524 0.524 ...</strong><br/>    <strong>$ rm     : num  6.58 6.42 7.18 7 7.15 ...</strong><br/>    <strong>$ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 <br/>      96.1 100 85.9 <br/>      ...</strong><br/>    <strong>$ dis    : num  4.09 4.97 4.97 6.06 6.06 ...</strong><br/>    <strong>$ rad    : int  1 2 2 3 3 3 5 5 5 5 ...</strong><br/>    <strong>$ tax    : num  296 242 242 222 222 222 311 311 311 <br/>      311 ...</strong><br/>    <strong>$ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 <br/>      15.2 15.2 15.2 <br/>      ...</strong><br/>    <strong>$ black  : num  397 397 393 395 397 ...</strong><br/>    <strong>$ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...</strong><br/>    <strong>$ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 <br/>      27.1 16.5 18.9 ...</strong>
</pre>
<p>Using <em>feature1*feature2</em> with the <kbd>lm()</kbd> function in the code puts both the features as well as their interaction term in the model, as follows:</p>
<pre>
    <strong>&gt; value.fit &lt;- lm(medv ~ lstat * age, data = <br/>      Boston)</strong><br/>    <br/>    <strong>&gt; summary(value.fit)</strong><br/>    <br/>    <strong>Call:</strong><br/>    <strong>lm(formula = medv ~ lstat * age, data = Boston)</strong><br/>    <br/>    <strong>Residuals:</strong><br/>    <strong>    Min      1Q  Median      3Q     Max</strong><br/>    <strong>-15.806  -4.045  -1.333   2.085  27.552</strong><br/>    <br/>    <strong>Coefficients:</strong><br/><strong>      Estimate Std. Error t value Pr(&gt;|t|)    </strong><br/>    <strong>(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 <br/>      ***</strong><br/>    <strong>lstat       -1.3921168  0.1674555  -8.313 8.78e-16 <br/>      ***</strong><br/>    <strong>age         -0.0007209  0.0198792  -0.036   0.9711    </strong><br/>    <strong>lstat:age    0.0041560  0.0018518   2.244   0.0252 <br/>      *  </strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <br/>    <strong>Residual standard error: 6.149 on 502 degrees of <br/>      freedom</strong><br/>    <strong>Multiple R-squared:  0.5557,    Adjusted R-squared:  <br/>      0.5531</strong><br/>    <strong>F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; <br/>      2.2e-16</strong>
</pre>
<p>Examining the output, we can see that, while the socioeconomic status is a highly predictive feature, the age of the home is not. However, the two features have a significant interaction to positively explain the home value.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In the context of machine learning, we train a model and test it to predict or forecast an outcome. In this chapter, we had an in-depth look at the simple yet extremely effective method of linear regression to predict a quantitative response. Later chapters will cover more advanced techniques, but many of them are mere extensions of what we have learned in this chapter. We discussed the problem of not visually inspecting the dataset and simply relying on the statistics to guide you in model selection.</p>
<p>With just a few lines of code, you can make powerful and insightful predictions to support decision-making. Not only is it simple and effective, but also you can include quantitative variables and interaction terms among the features. Indeed, this is a method that anyone delving into the world of machine learning must master.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>