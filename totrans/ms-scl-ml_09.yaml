- en: Chapter 9. NLP in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes a few common techniques of **Natural Language Processing**
    (**NLP**), specifically, the ones that can benefit from Scala. There are some
    NLP packages in the open source out there. The most famous of them is probably
    NLTK ([http://www.nltk.org](http://www.nltk.org)), which is written in Python,
    and ostensibly even a larger number of proprietary software solutions emphasizing
    different aspects of NLP. It is worth mentioning Wolf ([https://github.com/wolfe-pack](https://github.com/wolfe-pack)),
    FACTORIE ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)), and ScalaNLP
    ([http://www.scalanlp.org](http://www.scalanlp.org)), and skymind ([http://www.skymind.io](http://www.skymind.io)),
    which is partly proprietary. However, few open source projects in this area remain
    active for a long period of time for one or another reason. Most projects are
    being eclipsed by Spark and MLlib capabilities, particularly, in the scalability
    aspect.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of giving a detailed description of each of the NLP projects, which
    also might include speech-to-text, text-to-speech, and language translators, I
    will provide a few basic techniques focused on leveraging Spark MLlib in this
    chapter. The chapter comes very naturally as the last analytics chapter in this
    book. Scala is a very natural-language looking computer language and this chapter
    will leverage the techniques I developed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: NLP arguably is the core of AI. Originally, the AI was created to mimic the
    humans, and natural language parsing and understanding is an indispensable part
    of it. Big data techniques has started to penetrate NLP, even though traditionally,
    NLP is very computationally intensive and is regarded as a small data problem.
    NLP often requires extensive deep learning techniques, and the volume of data
    of all written texts appears to be not so large compared to the logs volumes generated
    by all the machines today and analyzed by the big data machinery.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the Library of Congress counts millions of documents, most of them
    can be digitized in PBs of actual digital data, a volume that any social websites
    is able to collect, store, and analyze within a few seconds. Complete works of
    most prolific authors can be stored within a few MBs of files (refer to *Table
    09-1*). Nonetheless, the social network and ADTECH companies parse text from millions
    of users and in hundreds of contexts every day.
  prefs: []
  type: TYPE_NORMAL
- en: '| The complete works of | When lived | Size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Plato* | 428/427 (or 424/423) - 348/347 BC | 2.1 MB |'
  prefs: []
  type: TYPE_TB
- en: '| *William Shakespeare* | 26 April 1564 (baptized) - 23 April 1616 | 3.8 MB
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Fyodor Dostoevsky* | 11 November 1821 - 9 February 1881 | 5.9 MB |'
  prefs: []
  type: TYPE_TB
- en: '| *Leo Tolstoy* | 9 September 1828 - 20 November 1910 | 6.9 MB |'
  prefs: []
  type: TYPE_TB
- en: '| *Mark Twain* | November 30, 1835 - April 21, 1910 | 13 MB |'
  prefs: []
  type: TYPE_TB
- en: Table 09-1\. Complete Works collections of some famous writers (most can be
    acquired on Amazon.com today for a few dollars, later authors, although readily
    digitized, are more expensive)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The natural language is a dynamic concept that changes over time, technology,
    and generations. We saw the appearance of emoticons, three-letter abbreviations,
    and so on. Foreign languages tend to borrow from each other; describing this dynamic
    ecosystem is a challenge on itself.
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate
    the language analysis rather than rewriting the tools in Scala. As the topic is
    so large, I will not claim to cover all aspects of NLP here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discussing NLP with the example of text processing pipeline and stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning techniques for simple text analysis in terms of bags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about **Term Frequency Inverse Document Frequency** (**TF-IDF**) technique
    that goes beyond simple bag analysis and de facto the standard in **Information
    Retrieval** (**IR**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about document clustering with the example of the **Latent Dirichlet
    Allocation** (**LDA**) approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing semantic analysis using word2vec n-gram-based algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text analysis pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we proceed to detailed algorithms, let's look at a generic text-processing
    pipeline depicted in *Figure 9-1*. In text analysis, the input is usually presented
    as a stream of characters (depending on the specific language).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lexical analysis has to do with breaking this stream into a sequence of words
    (or lexemes in linguistic analysis). Often it is also called tokenization (and
    the words called the tokens). **ANother Tool for Language Recognition** (**ANTLR**)
    ([http://www.antlr.org/](http://www.antlr.org/)) and Flex ([http://flex.sourceforge.net](http://flex.sourceforge.net))
    are probably the most famous in the open source community. One of the classical
    examples of ambiguity is lexical ambiguity. For example, in the phrase *I saw
    a bat.* *bat* can mean either an animal or a baseball bat. We usually need context
    to figure this out, which we will discuss next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text analysis pipeline](img/B04935_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Typical stages of an NLP process.
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic analysis, or parsing, traditionally deals with matching the structure
    of the text with grammar rules. This is relatively more important for computer
    languages that do not allow any ambiguity. In natural languages, this process
    is usually called chunking and tagging. In many cases, the meaning of the word
    in human language can be subject to context, intonation, or even body language
    or facial expression. The value of such analysis, as opposed to the big data approach,
    where the volume of data trumps complexity is still a contentious topic—one example
    of the latter is the word2vec approach, which will be described later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic analysis is the process of extracting language-independent meaning
    from the syntactic structures. As much as possible, it also involves removing
    features specific to particular cultural and linguistic contexts, to the extent
    that such a project is possible. The sources of ambiguity at this stage are: phrase
    attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal
    speech, and so on. Again, word2vec partially deals with these issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Disclosure integration partially deals with the issue of the context: the meaning
    of a sentence or an idiom can depend on the sentences or paragraphs before that.
    Syntactic analysis and cultural background play an important role here.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret
    what is said in terms of what the intention was. How does this change the state
    of the world? Is it actionable?
  prefs: []
  type: TYPE_NORMAL
- en: Simple text analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The straightforward representation of the document is a bag of words. Scala,
    and Spark, provides an excellent paradigm to perform analysis on the word distributions.
    First, we read the whole collection of texts, and then count the unique words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us just an estimate of the number of distinct words in the repertoire
    of quite different authors. The simplest way to find intersection between the
    two corpuses is to find the common vocabulary (which will be quite different as
    *Leo Tolstoy* wrote in Russian and French, while *Shakespeare* was an English-writing
    author):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A few thousands word indices are manageable with the current implementations.
    For any new story, we can determine whether it is more likely to be written by
    Leo Tolstoy or *William Shakespeare*. Let''s take a look at *The King James Version
    of the Bible*, which also can be downloaded from Project Gutenberg ([https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This seems reasonable as the religious language was popular during the Shakespearean
    time. On the other hand, plays by *Anton Chekhov* have a larger intersection with
    the *Leo Tolstoy* vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a very simple approach that works, but there are a number of commonly
    known improvements we can make. First, a common technique is to stem the words.
    In many languages, words have a common part, often called root, and a changeable
    prefix or suffix, which may depend on the context, gender, time, and so on. Stemming
    is the process of improving the distinct count and intersection by approximating
    this flexible word form to the root, base, or a stem form in general. The stem
    form does not need to be identical to the morphological root of the word, it is
    usually sufficient that related words map to the same stem, even if this stem
    is not in itself a valid grammatical root. Secondly, we probably should account
    for the frequency of the words—while we will describe more elaborate methods in
    the next section, for the purpose of this exercise, we'll exclude the words with
    very high count, that usually are present in any document such as articles and
    possessive pronouns, which are usually called stop words, and the words with very
    low count. Specifically, I'll use the optimized **Porter Stemmer** implementation
    that I described in more detail at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)
    site contains some of the Porter Stemmer implementations in Scala and other languages,
    including a highly optimized ANSI C, which may be more efficient, but here I will
    provide another optimized Scala version that can be used immediately with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Stemmer example will stem the words and count the relative intersections
    between them, removing the stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When one runs the main class example from the command line, it outputs the
    stemmed bag sizes and intersection for datasets specified as parameters (these
    are directories in the home filesystem with documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This, in this case, just confirms the hypothesis that Bible's vocabulary is
    closer to *William Shakespeare* than to Leo Tolstoy and other sources. Interestingly,
    modern vocabularies of *NY Times* articles and Enron's e-mails from the previous
    chapters are much closer to *Leo Tolstoy's*, which is probably more an indication
    of the translation quality.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to notice is that the pretty complex analysis took about *40*
    lines of Scala code (not counting the libraries, specifically the Porter Stemmer,
    which is about ~ *100* lines) and about 12 seconds. The power of Scala is that
    it can leverage other libraries very efficiently to write concise code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Serialization**'
  prefs: []
  type: TYPE_NORMAL
- en: We already talked about serialization in [Chapter 6](ch06.xhtml "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*. As Spark's tasks are
    executed in different threads and potentially JVMs, Spark does a lot of serialization/deserialization
    when passing the objects. Potentially, I could use `map { val stemmer = new Stemmer;
    stemmer.stem(_) }` instead of `map { stemmer.stem(_) }`, but the latter reuses
    the object for multiple iterations and seems to be linguistically more appealing.
    One suggested performance optimization is to use *Kryo serializer*, which is less
    flexible than the Java serializer, but more performant. However, for integrative
    purpose, it is much easier to just make every object in the pipeline serializable
    and use default Java serialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, let''s compute the distribution of word frequencies, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The distribution of relative frequencies on the log-log scale is presented
    in the following diagram. With the exception of the first few tokens, the dependency
    of frequency on rank is almost linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple text analysis](img/B04935_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. A typical distribution of word relative frequencies on log-log
    scale (Zipf's Law)
  prefs: []
  type: TYPE_NORMAL
- en: MLlib algorithms in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's halt at MLlib that complements other NLP libraries written in Scala. MLlib
    is primarily important because of scalability, and thus supports a few of the
    data preparation and text processing algorithms, particularly in the area of feature
    construction ([http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)).
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the preceding analysis can already give a powerful insight, the piece
    of information that is missing from the analysis is term frequency information.
    The term frequencies are relatively more important in information retrieval, where
    the collection of documents need to be searched and ranked in relation to a few
    terms. The top documents are usually returned to the user.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is a standard technique where term frequencies are offset by the frequencies
    of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses
    a hash function to identify the terms. This approach avoids the need to compute
    a global term-to-index map, but can be subject to potential hash collisions, the
    probability of which is determined by the number of buckets of the hash table.
    The default feature dimension is *2^20=1,048,576*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Spark implementation, each document is a line in the dataset. We can
    convert it into to an RDD of iterables and compute the hashing by the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When computing `hashingTF`, we only need a single pass over the data, applying
    IDF needs two passes: first to compute the IDF vector and second to scale the
    term frequencies by IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here we see each document represented by a set of terms and their scores.
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LDA in Spark MLlib is a clustering mechanism, where the feature vectors represent
    the counts of words in a document. The model maximizes the probability of observing
    the word counts, given the assumption that each document is a mixture of topics
    and the words in the documents are generated based on **Dirichlet distribution**
    (a generalization of beta distribution on multinomial case) for each of the topic
    independently. The goal is to derive the (latent) distribution of the topics and
    the parameters of the words generation statistical model.
  prefs: []
  type: TYPE_NORMAL
- en: The MLlib implementation is based on 2009 LDA paper ([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))
    and uses GraphX to implement a distributed **Expectation Maximization** (**EM**)
    algorithm for assigning topics to the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the Enron e-mail corpus discussed in [Chapter 7](ch07.xhtml "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*, where we tried to figure
    out communications graph. For e-mail clustering, we need to extract the body of
    the e-mail and place is as a single line in the training file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use Scala/Spark to construct a corpus dataset containing the document
    ID, followed by a dense array of word counts in the bag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also list the words and their relative importance for the topic in the
    descending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To find out the top documents per topic or top topics per document, we need
    to convert this model to `DistributedLDA` or `LocalLDAModel`, which extend `LDAModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Segmentation, annotation, and chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the text is presented in digital form, it is relatively easy to find words
    as we can split the stream on non-word characters. This becomes more complex in
    spoken language analysis. In this case, segmenters try to optimize a metric, for
    example, to minimize the number of distinct words in the lexicon and the length
    or complexity of the phrase (*Natural Language Processing with Python* by *Steven
    Bird et al*, *O'Reilly Media Inc*, 2009).
  prefs: []
  type: TYPE_NORMAL
- en: Annotation usually refers to parts-of-speech tagging. In English, these are
    nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions,
    and interjections. For example, in the phrase *we saw the yellow dog*, *we* is
    a pronoun, *saw* is a verb, *the* is an article, *yellow* is an adjective, and
    *dog* is a noun.
  prefs: []
  type: TYPE_NORMAL
- en: In some languages, the chunking and annotation depends on context. For example,
    in Chinese, *爱江山人* literally translates to *love country person* and can mean
    either *country-loving person* or *love country-person*. In Russian, *казнить
    нельзя помиловать*, literally translating to *execute not pardon*, can mean *execute,
    don't pardon*, or *don't execute, pardon*. While in written language, this can
    be disambiguated using commas, in a spoken language this is usually it is very
    hard to recognize the difference, even though sometimes the intonation can help
    to segment the phrase properly.
  prefs: []
  type: TYPE_NORMAL
- en: For techniques based on word frequencies in the bags, some extremely common
    words, which are of little value in helping select documents, are explicitly excluded
    from the vocabulary. These words are called stop words. There is no good general
    strategy for determining a stop list, but in many cases, this is to exclude very
    frequent words that appear in almost every document and do not help to differentiate
    between them for classification or information retrieval purposes.
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'POS tagging probabilistically annotates each word with it''s grammatical function—noun,
    verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic
    and semantic analysis. Let''s demonstrate POS tagging on the FACTORIE toolkit
    example, a software library written in Scala ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)).
    To start, you need to download the binary image or source files from [https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)
    and build it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After the build, which also includes model training, the following command
    will start a network server on `port 3228`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all traffic to `port 3228` will be interpreted (as text), and the output
    will be tokenized and annotated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This POS is a single-path left-right tagger that can process the text as a stream.
    Internally, the algorithm uses probabilistic techniques to find the most probable
    assignment. Let's also look at other techniques that do not use grammatical analysis
    and yet proved to be very useful for language understanding and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Using word2vec to find word relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec has been developed by Tomas Mikolov at Google, around 2012\. The original
    idea behind word2vec was to demonstrate that one might improve efficiency by trading
    the model''s complexity for efficiency. Instead of representing a document as
    bags of words, word2vec takes each word context into account by trying to analyze
    n-grams or skip-grams (a set of surrounding tokens with potential the token in
    question skipped). The words and word contexts themselves are represented by an
    array of floats/doubles ![Using word2vec to find word relationships](img/B04935_09_01F.jpg).
    The objective function is to maximize log likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using word2vec to find word relationships](img/B04935_09_02F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using word2vec to find word relationships](img/B04935_09_03F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By choosing the optimal ![Using word2vec to find word relationships](img/B04935_09_01F.jpg)
    and to get a comprehensive word representation (also called **map optimization**).
    Similar words are found based on cosine similarity metric (dot product) of ![Using
    word2vec to find word relationships](img/B04935_09_01F.jpg). Spark implementation
    uses hierarchical softmax, which reduces the complexity of computing the conditional
    probability to ![Using word2vec to find word relationships](img/B04935_09_04F.jpg),
    or log of the vocabulary size *V*, as opposed to ![Using word2vec to find word
    relationships](img/B04935_09_05F.jpg), or proportional to *V*. The training is
    still linear in the dataset size, but is amenable to big data parallelization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec` is traditionally used to predict the most likely word given context
    or find similar words with a similar meaning (synonyms). The following code trains
    in `word2vec` model on *Leo Tolstoy''s Wars and Peace*, and finds synonyms for
    the word *circle*. I had to convert the Gutenberg''s representation of *War and
    Peace* to a single-line format by running the `cat 2600.txt | tr "\n\r" " " >
    warandpeace.txt` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: While in general, it is hard to some with an objective function, and `freedom`
    is not listed as a synonym to `life` in the English Thesaurus, the results do
    make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each word in the word2vec model is represented as an array of doubles. Another
    interesting application is to find associations *a to b is the same as c to ?*
    by performing subtraction *vector(a) - vector(b) + vector(c)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This can be used to find relationships in the language.
  prefs: []
  type: TYPE_NORMAL
- en: A Porter Stemmer implementation of the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Porter Stemmer was first developed around the 1980s and there are many implementations.
    The detailed steps and original reference are provided at [http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt).
    It consists of roughly 6-9 steps of suffix/endings replacements, some of which
    are conditional on prefix or stem. I will provide a Scala-optimized version with
    the book code repository. For example, step 1 covers the majority of stemming
    cases and consists of 12 substitutions: the last 8 of which are conditional on
    the number of syllables and the presence of vowels in the stem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The complete code is available at [https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I described basic NLP concepts and demonstrated a few basic
    techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed
    and tested in a few lines of Scala code. This is definitely just the tip of the
    iceberg as a lot of NLP techniques are being developed now, including the ones
    based on in-CPU parallelization as part of GPUs. (refer to, for example, **Puck**
    at [https://github.com/dlwh/puck](https://github.com/dlwh/puck)). I also gave
    a flavor of major Spark MLlib NLP implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which will be the final chapter of this book, I'll cover
    systems and model monitoring.
  prefs: []
  type: TYPE_NORMAL
