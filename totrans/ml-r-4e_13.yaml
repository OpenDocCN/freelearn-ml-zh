- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Challenging Data – Too Much, Too Little, Too Complex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Challenging data takes many forms throughout the course of a machine learning
    project, and the journey of each new project represents an adventure requiring
    a pioneer spirit. Beginning with uncharted data that must be explored, the data
    must then be wrangled before it can be used with the learning algorithm. Even
    then, there may still be wild aspects of the data that need to be tamed for the
    project to be successful. Extraneous information must be culled, small-but-important
    details must be cultivated, and tangled webs of complexity must be cleared from
    the learner’s path.
  prefs: []
  type: TYPE_NORMAL
- en: Conventional wisdom in the big data era suggests that data is treasure, but
    as the saying goes, one can have “too much of a good thing.” Most machine learning
    algorithms will happily indulge in as much data as they are fed, which leads to
    a new set of problems akin to overeating. An abundance of data can overwhelm the
    learner with unnecessary information, obscure important patterns, and shift the
    learner’s attention from the details that matter to those that are obvious. Thus,
    it may be better to avoid the “more is always better” mindset and instead find
    a balance between quantity and quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to consider techniques that can be used to adapt
    to a dataset’s signal-to-noise ratio. You will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: How to handle datasets with an overwhelming number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods for making use of feature values that are missing or appear very infrequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches for modeling rare target outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will discover that some learning algorithms are better equipped at performing
    these techniques independently, while others will require you to intervene more
    extensively in the process. In either case, due to the prevalence of these types
    of data issues and their status as some of the most challenging problems in machine
    learning, it is important to understand the ways that they can be remedied.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of high-dimension data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If someone says that they are struggling to handle the size of a dataset, it
    is easy to assume that they are talking about having too many rows or that the
    data uses too much memory or storage space. Indeed, these are common issues that
    cause problems for new machine learning practitioners. In this scenario, the solutions
    tend to be technical rather than methodological; one generally chooses a more
    efficient algorithm or uses hardware or a cloud computing platform capable of
    consuming large datasets. In the worst case, one can take a random sampling and
    simply discard some of the excessive rows.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of having too much data can also apply to a dataset’s columns,
    making the dataset overly wide rather than overly long. It may require some creative
    thinking to imagine why this happens, or why it is a problem, because it is rarely
    encountered in the tidy confines of teaching examples. Even in real-world practice,
    it may be quite some time before someone encounters this problem, as useful predictors
    can be scarce, and datasets are often scrounged piece by piece. For such projects,
    having too many predictors would be a good problem to have!
  prefs: []
  type: TYPE_NORMAL
- en: However, consider a situation in which a data-driven organization, acutely aware
    of the competitive advantage of big data, has amassed a war chest of information
    from a variety of sources. Perhaps they collected some of the data directly through
    the ordinary course of business, purchased supplemental data from vendors, and
    gathered some via additional sensors or indirect, passive interactions via the
    internet. All these sources are merged into a single table that provides a rich
    but highly complex and varied set of features. The resulting table was not carefully
    constructed piece by piece, but rather through a mishmash of data elements, some
    of which will be more useful than others. Today, this type of data treasure trove
    is found predominantly in very large or very data-savvy organizations, but it
    is likely that an increasing number will have access to similar datasets in the
    future. Datasets are growing increasingly wide over time, even before considering
    inherently feature-rich sources such as text, audio, image, or genetic data.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of these types of high-dimension datasets, in short, has much
    to do with the fact that more data points have been collected than are truly needed
    to represent the underlying pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The additional data points add noise or subtle variations across examples and
    may distract a learning algorithm from the important trends. This describes the
    **curse of dimensionality** in which learners fail as the number of features increases.
    If we imagine each additional feature as a new dimension of the example—and here,
    the word “dimension” is used both in a literal and a metaphorical sense—then as
    the dimensions increase, the richness of our understanding of any given example
    increases, but so does the example’s relative uniqueness. In a sufficiently high-dimension
    space, every example is unique, as it is comprised of its own distinct combination
    of feature values.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an analogy. A fingerprint uniquely identifies individual people but
    it is not necessary to store all of a fingerprint’s details to make an accurate
    match. In fact, out of the limitless details found in each fingerprint, a forensic
    investigator may use only 12 to 20 distinct points to confirm a match; even computerized
    fingerprint scanners use only 60 to 80 points. Any additional detail is likely
    to be superfluous and would detract from the match quality, and taken to an extreme,
    might cause failed matches—even if the fingerprints are from the same person!
    For example, including too much detail might lead to false negatives as the learning
    algorithm is distracted by the print’s orientation or image quality, but too little
    detail may lead to false positives as the algorithm has too few features to distinguish
    among similar candidates. Clearly, it is important to find a balance between too
    much and too little detail. This is, in essence, the goal of **dimensionality
    reduction**, which seeks to remedy the curse of dimensionality by identifying
    the important details.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text  Description automatically generated](img/B17290_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Dimensionality reduction helps to ignore noise and emphasize the
    key details that will be helpful to learn the underlying pattern'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the problem of very long datasets, the solutions needed to learn
    from wide datasets are completely different and are as much conceptual as they
    are practical. One cannot simply randomly discard columns as is possible with
    rows because some columns are more useful than others. Instead, a systematic approach
    is taken, often cooperating with the learning algorithm itself to find the balance
    between too much and too little detail. As you will learn in the coming sections,
    some of these methods are integrated into the learning process while others will
    require a more hands-on approach.
  prefs: []
  type: TYPE_NORMAL
- en: Applying feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the context of supervised machine learning, the goal of feature selection
    is to alleviate the curse of dimensionality by choosing only the most important
    predictors. Feature selection may also be beneficial even in the case of unsupervised
    learning due to its ability to simplify datasets by eliminating redundant or useless
    information. In addition to feature selection’s primary goal of assisting a learning
    algorithm’s attempts to separate the signal from the noise, additional benefits
    of the practice include:'
  prefs: []
  type: TYPE_NORMAL
- en: Shrinking the size of the dataset and decreasing storage requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the time or computational expense for model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling data scientists to focus on fewer features for data exploration and
    visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rather than attempting to find the single most optimal complete set of predictors,
    which can be very computationally expensive, feature selection tends to focus
    on identifying useful individual features or subsets of features. To do so, feature
    selection typically relies on heuristics that reduce the number of subsets that
    are searched. This reduces the computing cost but may lead to missing the best
    possible solution.
  prefs: []
  type: TYPE_NORMAL
- en: To search for subsets of useful features is to assume that some predictors are
    useless, or at least less useful than others. Yet, despite the validity of this
    premise, it is not always clear what makes some features useful and others not.
    Of course, there may be obviously irrelevant features that provide no predictive
    value, but there may also be useful features that are redundant and therefore
    unnecessary for the learning algorithm. The trick is recognizing that something
    that appears redundant in one context may actually be useful in a different context.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates the ability of useful features to disguise
    themselves as seemingly useless and redundant predictors. The scatterplot depicts
    a relationship between two hypothetical features, each having values in the approximate
    range of -50 to 50, and being used to predict a binary outcome, triangles versus
    circles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Features 1 and 2 are seemingly useless and redundant but have
    predictive value when used together'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the value of feature 1 or 2 alone provides virtually no value toward
    predicting the outcome of the target, as the circles and triangles are almost
    completely evenly split for any value of either feature. In quantitative terms,
    this is demonstrated by a very weak correlation between the features and the outcome.
    A simple feature selection algorithm that examines only the relationship between
    one feature and the outcome may thus determine that neither feature is useful
    for prediction. Additionally, because the correlation between the two features
    is about 0.90, a more sophisticated feature selection algorithm that simultaneously
    considers the pair may inadvertently exclude one of the two due to the seeming
    redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the seemingly useless and redundant nature of the two features, the
    scatterplot clearly depicts their predictive ability when used together: if feature
    2 is greater than feature 1, then predict triangle; otherwise, predict circle.
    A useful feature selection method ought to be able to recognize these types of
    patterns; otherwise, it risks excluding important predictors from the learning
    algorithm. However, the feature selection technique also needs to consider computational
    efficiency, as examining every potential combination of features is infeasible
    except for the smallest of datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: The need to balance the search for useful, non-redundant features with the possibility
    that features may only be useful in combination with others is part of the reason
    there is no one-size-fits-all approach to feature selection. Depending on the
    use case and the chosen learning algorithm, different techniques can be applied
    that perform a less rigorous or more thorough search of the features.
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper dive into feature selection, see *An Introduction to Variable and
    Feature Selection, 2003, Guyon, I. and Elisseeff, A., Journal of Machine Learning
    Research, Vol. 3, pp. 1157-1182*.
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perhaps the most accessible form of feature selection is the category of **filter
    methods**, which use a relatively simple scoring function to measure each feature’s
    importance. The resulting scores can then be used to rank the features and limit
    the number used in the predictive model. Due to the simplicity of this approach,
    filter methods are often used as a first step in an iterative process of data
    exploration, feature engineering, and model building. One might initially apply
    a crude filter to identify the most interesting candidate features for in-depth
    exploration and visualization, then apply more vigorous feature selection methods
    later if further reduction is desired.
  prefs: []
  type: TYPE_NORMAL
- en: A single defining characteristic of filter methods is the use of a proxy measure
    of feature importance. The measure is a proxy because it is a substitute for what
    we truly care about—the predictive ability of the feature—but we cannot know this
    without first building the predictive model. Instead, we choose a much simpler
    metric, which we hope reflects the utility of the feature when it is later added
    to the model. For instance, in a numeric prediction model, one might compute bivariate
    correlations between each feature and the target and select only the features
    that are substantially correlated with the target. For a binary or categorical
    target, a comparable approach might involve constructing a single-variable classifier,
    examining contingency tables for strong bivariate relationships between the features
    and the target, or using a metric like information gain, which was described in
    *Chapter 5*, *Divide and Conquer – Classification Using Decision Trees and Rules*.
    A benefit of these types of simple feature selection metrics is that they are
    unlikely to contribute to overfitting because the proxy measures use a different
    approach and make different assumptions about the data than the learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The greatest benefit of filter methods may be the fact that they are scalable
    even for datasets with very large numbers of features. This efficiency stems from
    the fact the filtering method only computes one importance score for each feature
    and then sorts the predictors by these scores from most to least important. Thus,
    as the number of features increases, the computational expense grows relatively
    slowly and in direct proportion to the number of predictors. Note that the product
    of this approach is a rank-ordered list of features rather than a single best
    set of features; therefore, subjective judgment is required to determine the optimal
    cutoff between important and not important features.
  prefs: []
  type: TYPE_NORMAL
- en: Although filter methods are computationally efficient, they lack the ability
    to consider groups of features, which means that important predictors may be excluded
    if they are only useful in combination with others. Additionally, the fact that
    filter methods are unlikely to contribute to overfitting comes with the potential
    downside that they also may not result in the set of features that are best suited
    to work with the desired learning algorithm. The feature selection method described
    in the next section sacrifices computational efficiency to address each of these
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper methods and embedded methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to filter methods, which use a proxy measure of variable importance,
    **wrapper methods** use the machine learning algorithm itself to identify the
    importance of variables or subsets of variables. Wrapper methods are based on
    the simple idea that as more important features are provided to the algorithm,
    its ability to perform the learning task should improve. In other words, its error
    rate should be reduced as important predictors are included or the correct combinations
    are included.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, by iteratively building models composed of different combinations of features
    and examining how the model’s performance changes, it is possible to identify
    the important predictors and sets of predictors. By systematically testing all
    possible combinations of features, it is even possible to identify the overall
    best set of predictors.
  prefs: []
  type: TYPE_NORMAL
- en: However, as one might expect, the process of testing all possible combinations
    of features is extremely computationally inefficient. For a dataset with *p* predictors,
    there are *2*^p potential sets of predictors that must be tested, which causes
    the computational expense of this technique to grow relatively quickly as additional
    features are added. For example, a dataset with only 10 predictors would require
    *2*^(10) *= 1,024* different models to be evaluated, while a dataset adding just
    five more predictors would require *2*^(15) *= 32,768* models, which is over 30
    times the computational cost! Clearly, this approach is not viable except for
    the smallest of datasets and the simplest of machine learning algorithms. One
    solution to this problem might be to first reduce the number of features using
    a filter method, but not only does this risk missing important combinations of
    features but it would also require such a reduction in dimensionality that it
    may negate many of the benefits of wrapper methods.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than letting its inefficiency prevent us from capitalizing on its upsides,
    we can instead use heuristics to avoid searching every combination of features.
    In particular, the “greedy” approach described in *Chapter 5*, *Divide and Conquer
    – Classification Using Decision Trees and Rules*, which helped grow trees efficiently,
    can also be used here. You may recall that the idea of a greedy algorithm is to
    use data on a first-come, first-served basis, with the most predictive features
    used first. Although this technique is not guaranteed to find the optimal solution,
    it drastically reduces the number of combinations that must be tested.
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic approaches for adapting wrapper methods for greedy feature
    selection. Both involve probing the learning algorithm by changing one variable
    at a time. The technique of **forward selection** begins by feeding each feature
    to the model one by one, to determine which of them results in the best one-predictor
    model. The next iteration of forward selection keeps the first best predictor
    in the model and tests the remaining features to identify which makes the best
    two-predictor model. As might be expected, this process can continue selecting
    the best three-predictor model, four-predictor model, and so on, until all features
    have been selected. However, as the point of feature selection is specifically
    not to select the entire set of features, the process of forward selection stops
    early, when adding additional features no longer improves the model’s performance
    beyond a specific threshold.
  prefs: []
  type: TYPE_NORMAL
- en: The similar technique of **backward elimination** works the same way, but in
    reverse. Beginning with a model containing all features, the model iterates repeatedly,
    eliminating the least-predictive feature each time, until stopping when eliminating
    a feature decreases the model’s performance more than a desired threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Learning algorithms known as **embedded methods** have a form of built-in wrappers
    much like forward selection. These methods select the best features automatically
    during the model training process. You are already familiar with one such method,
    decision trees, which uses greedy forward selection to determine the best feature
    subset. Most machine learning techniques do not have embedded feature selection;
    the dimensions must be reduced beforehand. The next section demonstrates how these
    methods can be applied in R via a variant of the machine learning algorithm introduced
    in *Chapter 6*, *Forecasting Numeric Data – Regression Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Using stepwise regression for feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One widely known implementation of wrapper methods is **stepwise regression**,
    which uses forward or backward selection to identify a set of features for a regression
    model. To demonstrate this technique, we’ll revisit the Titanic passenger dataset
    used in the previous two chapters and build a logistic regression model that predicts
    whether each passenger survived the ill-fated voyage. To begin, we’ll use the
    tidyverse to read the data and apply some simple data preparation steps. The following
    sequence of commands creates a missing value indicator for `Age`, imputes the
    average age for the missing `Age` values, imputes `X` for the missing `Cabin`
    and `Embarked` values, and converts `Sex` to a factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The stepwise process needs to know the starting and ending conditions for feature
    selection, or the minimum and maximum set of variables that can be included. In
    our case, we’ll define the simplest possible model as one containing no variables
    at all—a model with only a constant intercept term.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define this model in R, we’ll use the `glm()` function to model survival
    as a function of a constant intercept using the `Survived ~ 1` formula. Setting
    the `family` parameter to `binomial` defines a logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The full model still uses logistic regression, but includes many more predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Forward selection will begin with the simple model and determine which of the
    features in the full model are worth including in the final model. The `step()`
    function in the base R `stats` package provides this functionality; however, because
    other packages also have `step()` functions, specifying `stats::step()` ensures
    the correct one is used. The first function argument provides the starting model,
    the `scope` parameter requires the `formula()` of the full model, and the direction
    is set to `forward` stepwise regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command generates a set of outputs for each iteration of the stepwise process,
    but only the first and last iterations are included here for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: If you are selecting from a large number of variables, set `trace = 0` in the
    `step()` function to turn off the output for each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of the stepwise process, it begins with the simple model using
    the `Survived ~ 1` formula, which models survival using only a constant intercept
    term. The first block of output thus displays the model quality at the start and
    after evaluating seven other candidate models each with a single additional predictor
    added. The row labeled `<none>` refers to the model’s quality at the start of
    this iteration and how it ranks compared to the seven other candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The quality measure used, AIC, is a measure of a model’s relative quality compared
    to other models. In particular, it refers to the **Akaike information criterion**.
    While a formal definition of AIC is outside the scope of this chapter, the measure
    is intended to balance model complexity and model fit. Lower AIC values are better.
    Therefore, the model that includes `Sex` is the best out of the six other candidate
    models as well as the original model. In the final iteration, the base model uses
    `Sex`, `Pclass`, `Age`, and `SibSp`, and no additional features reduce the AIC
    further—the `<none>` row is ranked above the candidate models adding `Embarked`,
    `Fare`, and `Age_MVI` features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the forward selection process stops. We can obtain the formula
    for the final model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also obtain the final model’s estimated regression coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Backward elimination is even simpler to execute. By providing a model with
    the complete set of features to test and setting `direction = "backward"`, the
    model will iterate and systematically eliminate any features that will result
    in a better AIC. For example, the first step begins with a full set of predictors,
    but eliminating the `Fare`, `Age_MVI`, or `Embarked` features results in a lower
    AIC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At each iteration, the worst feature is eliminated, but by the final step,
    eliminating any of the remaining features leads to a higher AIC, and therefore
    leads to a lower-quality model than the baseline. Thus, the process stops here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this case, forward selection and backward elimination resulted in the same
    set of predictors, but this is not necessarily always the case. Differences may
    arise if certain features work better in groups or if they are interrelated in
    some other way.
  prefs: []
  type: TYPE_NORMAL
- en: As noted previously, one of the downsides of the heuristics used by wrapper
    methods is that they are not guaranteed to find the single most optimal set of
    predictors; however, this shortcoming is exactly what makes the feature selection
    process computationally feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Using Boruta for feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a more robust yet much more computationally intensive feature selection
    method, the `Boruta` package implements a wrapper around the random forest algorithm,
    which will be introduced in *Chapter 14*, *Building Better Learners*. For now,
    it suffices to know that random forests are a variant of decision trees, which
    provide a measure of variable importance. By systematically testing random subsets
    of variables repeatedly, it is possible to determine whether a feature is significantly
    more or less important than others using statistical hypothesis testing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Because of its heavy reliance on the random forest technique, it is no surprise
    that the technique shares a name with Boruta, a mythological Slavic creature thought
    to dwell in swamps and forests. To read more about Boruta’s implementation details,
    see *Feature Selection with the Boruta Package, Kursa, M. B. and Rudnicki, W.
    R., 2010, Journal of Statistical Software, Vol. 36, Iss. 11*.
  prefs: []
  type: TYPE_NORMAL
- en: The `Boruta` technique employs a clever trick using so-called “shadow features”
    to determine whether a variable is important. These shadow features are copies
    of the dataset’s original features, but with the values shuffled randomly so that
    any association between the feature and the target outcome is broken. Thus, these
    shadow features are, by definition, nonsense and unimportant, and should provide
    zero predictive benefits to the model except by random chance. They serve as a
    baseline by which the other features are judged.
  prefs: []
  type: TYPE_NORMAL
- en: After running the original features and shadow features through the random forest
    modeling process, the importance of each original feature is compared to the most
    important shadow feature. Features that are significantly better than the shadow
    feature are deemed important; those significantly worse are deemed unimportant
    and permanently removed. The algorithm iterates repeatedly until all features
    are deemed important or unimportant, or the process hits a predetermined limit
    of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: To see this in action, let’s apply the `Boruta` algorithm to the same Titanic
    training dataset constructed in the previous section. Just to prove that the algorithm
    can detect truly useless features, for demonstration purposes we can add one to
    the dataset. First, we’ll set the random seed to the arbitrary number `12345`
    to ensure your results match those shown here. Then, we’ll assign each of the
    891 training examples a random value between 1 and 100\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the numbers are completely random, this feature should almost certainly
    be found useless, except in the case of dumb luck:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll load the `Boruta` package and apply it to the Titanic dataset.
    The syntax is similar to training a machine learning model; here, we specify the
    model using the formula interface to list the target and predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `doTrace` parameter is set to `1` to request verbose output, which produces
    a status update as the algorithm reaches key points in the iteration process.
    Here, we see the output after 10 iterations, which shows that the `rand_vals`
    feature has unsurprisingly been rejected as unimportant, while four features were
    confirmed as important and one feature remains undetermined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the algorithm has completed, type the name of the object to see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `Boruta()` function is set to a limit of 100 runs by default, which it hit
    after iterating 99 times in about 4.5 seconds. Before stopping, four features
    were found to be important and one was found to be unimportant. The `PassengerId`
    feature, which is listed as tentative, was unable to be confirmed as important
    or unimportant. Setting the `maxRuns` parameter to a higher value than 100 can
    help come to a conclusion—in this case, setting `maxRuns = 500` will confirm `PassengerId`
    to be unimportant after 486 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to plot the importance of the features relative to one
    another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting visualization is shown in *Figure 13.3*. For each of the six
    features, as well as the max, mean (average), and min performing shadow features,
    a boxplot shows the distribution of importance metrics for that feature. Using
    these results, we can confirm that the `PassengerId` is slightly less important
    than the max shadow feature, and `rand_vals` is even less important than that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, timeline  Description automatically generated](img/B17290_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Plotting the Boruta output shows the relative importance of features
    compared to each other and the shadow features'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the exploration of the Titanic dataset we performed in *Chapter 11*,
    *Being Successful with Machine Learning*, the high importance of the `Sex` and
    `Pclass` features is unsurprising. Likewise, we would not expect the `PassengerId`
    to be important, unless the IDs were somehow linked to Titanic survival rather
    than being assigned at random. This being said, even though the results of this
    feature selection process did not reveal new insights, the technique would be
    much more helpful for datasets that are not as easy to explore by hand, or where
    the real-world meaning of the features is unknown. Of course, this is just one
    approach for dealing with a large number of features of undetermined importance;
    the next section describes an alternative that may perform better, especially
    if many of the features are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: The `Boruta` technique can be very computationally intensive, and on real-world
    datasets, it will generally take minutes or even hours to complete rather than
    seconds as with the Titanic data. The authors of the package estimate that on
    a modern computer, it needs roughly one hour per million feature-example combinations.
    For example, a dataset with 10,000 rows and 50 features will take roughly half
    an hour to complete. Increasing the size of this dataset to 100,000 rows would
    require about five hours of processing time!
  prefs: []
  type: TYPE_NORMAL
- en: Performing feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature selection is not the only approach available to reduce the dimensionality
    of a highly dimensional dataset. Another possibility is to synthesize a smaller
    number of composite predictors. This is the goal of **feature extraction**, a
    dimensionality reduction technique that creates new features rather than selecting
    a subset of existing features. The extracted features are constructed such that
    they reduce the amount of redundant information while keeping as much useful information
    as possible. Of course, finding the ideal balance between too much and too little
    information is a challenge in itself.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding principal component analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To begin to understand feature extraction, start by imagining a dataset with
    a very large number of features. For instance, to predict applicants likely to
    default on a loan, a dataset may include hundreds of applicant attributes. Obviously,
    some of the features are going to be predictive of the target outcome, but it
    is likely that many of the features are predictive of each other as well. For
    example, a person’s age, education level, income, zip code, and occupation are
    all predictive of their likelihood to pay back a loan, but they are also predictive
    of each other to varying degrees. Their interrelatedness suggests that there is
    a degree of overlap or joint dependency among them, which is reflected in their
    covariance and correlation.
  prefs: []
  type: TYPE_NORMAL
- en: It may be the case that the reason these five attributes of loan applicants
    are related is that they are components of a smaller number of attributes that
    are the true, underlying drivers of loan payment behavior. In particular, we might
    believe that loan payment likelihood is based on an applicant’s responsibility
    and affluence, but because these concepts are difficult to measure directly, we
    instead use multiple readily available proxy measures. The following figure illustrates
    how each of the five features might capture aspects of the two hidden dimensions
    of interest. Note that none of the features fully captures either component dimension,
    but rather, each component dimension is a composite of several features.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a person’s level of responsibility might be captured by their
    age and education level, while their affluence might be reflected in their income,
    occupation, and zip code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Five hypothetical attributes of loan applicants might be more
    simply expressed in two dimensions created from composites of the covariance of
    each attribute'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of **principal component analysis** (**PCA**) is to extract a smaller
    number of underlying dimensions from a larger number of features by expressing
    the covariance of multiple correlated attributes as a single vector. Put simply,
    the covariance refers to the extent to which attributes vary in concert. When
    one goes up or down, the other tends to go up or down. The resulting vectors are
    known as **principal components** and are constructed as weighted combinations
    of the original attributes. When applied to a dataset with many correlated features,
    a much smaller number of principal components may be capable of expressing much
    of the total variance of the higher-dimension dataset. Although this seems like
    a lot of technical jargon, and the math required to implement PCA is beyond the
    scope of the book, we will work toward a conceptual understanding of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principal component analysis is closely related to another technique, called
    **factor analysis**, which is a more formal approach for exploring the relationships
    between observed and unobserved (latent) factors, such as those depicted in the
    figures here. In practice, both can be applied similarly, but PCA is simpler and
    avoids building a formal model; it merely reduces the number of dimensions while
    retaining maximal variation. For a deeper dive into the many subtle distinctions,
    see the following Stack Exchange thread: [https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi/](https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting *Figure 13.4*, each circle is intended to represent the relationships
    among each of the five features. Circles with greater overlap represent correlated
    features that may measure a similar underlying concept. Keep in mind that this
    is a highly simplified representation that does not depict the individual data
    points that would be used to compute the correlations among the features. In reality,
    these individual data points would represent individual loan applicants and would
    be positioned in a five-dimensional space with coordinates determined by each
    applicant’s five feature values. Of course, this is difficult to depict in the
    two dimensions of this book’s pages, so the circles in this simplified representation
    should be understood as a cloud-like mass of people with high values of the attribute.
    In this case, if two features are highly correlated, such as income and education,
    the two clouds will overlap, because people with high values of one attribute
    will tend to have high values of the other. *Figure 13.5* depicts this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_13_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: When two features are highly correlated, points with high values
    of one tend to have high values of the other'
  prefs: []
  type: TYPE_NORMAL
- en: When examining *Figure 13.5*, note that the diagonal arrow that represents the
    relationship between income and education reflects the covariance between the
    two features. Knowing whether a point is closer to the start or end of the arrow
    would provide a good estimate of both income and education. Highly covariant features
    are thus likely to express similar underlying attributes and therefore may be
    redundant. In this way, the information expressed by two dimensions, income and
    education, could be expressed more simply in a single dimension, which would be
    the principal component of these two features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying this relationship to a diagram in three dimensions, we might imagine
    this principal component as the *z* dimension in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_13_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Five attributes with varying degrees of covariance in three dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: As with the two-dimensional case, the positioning of the circles is intended
    to represent the covariance among the features; the circle sizes are meant to
    represent depth, with larger or smaller circles closer to the front or back of
    the space. In the three dimensions here, age and education are close on one dimension,
    occupation and zip code are close on another, and income varies in a third dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we hoped to capture most of the variance while reducing the number of dimensions
    from three to two, we might project this three-dimensional plot onto a two-dimensional
    plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bubble chart  Description automatically generated](img/B17290_13_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Principal component analysis reduces many dimensions into a smaller
    number of key components'
  prefs: []
  type: TYPE_NORMAL
- en: With these two dimensions, we have constructed the two principal components
    of the dataset, and in doing so, we have reduced the dimensionality of the dataset
    from five dimensions with real-world meaning to two dimensions, *x* and *y*, with
    no inherent real-world connection. Instead, the two resulting dimensions now reflect
    linear combinations of the underlying data points; they are useful summaries of
    the underlying data, but are not easily interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could reduce the dimensionality even further by projecting the dataset onto
    a line to create a single principal component, as illustrated in *Figure 13.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bubble chart  Description automatically generated](img/B17290_13_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: The first principal component captures the dimension with the
    greatest variance'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the PCA approach extracted one hybrid feature from the dataset’s
    five original dimensions. Age and education are treated as somewhat redundant,
    as are occupation and income.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, age and education have an opposite impact on the new feature than
    zip code—they are pulling the value of *x* in opposite directions. If this one-dimensional
    representation loses too much of the information stored within the original five
    features, the earlier approaches with two or three components could be used instead.
    As with many techniques in machine learning, there is a balance between over-
    and underfitting the data. We’ll see this reflected in a real-world example shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying PCA, it’s important to know that principal components are identified
    by a deterministic algorithm, which means that the solution is consistent every
    time the process is completed on a given dataset. Each component vector is also
    always orthogonal, or perpendicular, to all previous component vectors. The first
    principal component captures the dimension of highest variance, the next captures
    the next most, and so on, until a principal component has been constructed for
    each in the original dataset, or the algorithm stops early when the desired number
    of components has been reached.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Using PCA to reduce highly dimensional social media data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned previously, PCA is a feature extraction technique that reduces
    the dimensionality of a dataset by synthesizing a smaller set of features from
    the complete set. We’ll apply this technique to the social media data first described
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*. You may recall
    that this dataset includes counts of 36 different words that appeared on the social
    media pages of 30,000 teens in the United States. The words reflect various interests
    and activities such as sports, music, religion, and shopping, and although 36
    is not an unreasonable number for most machine learning algorithms to handle,
    if we had more—perhaps hundreds of features—some algorithms might begin to struggle
    with the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the tidyverse suite of functions to read and prepare the data. First,
    we’ll load the package, and use its `read_csv()` function to read the social media
    data as a tibble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will `select()` only the columns corresponding to the features recording
    the number of times 36 words were used in each social media profile. The notation
    here selects from the column named `basketball` through the column named `drugs`
    and saves the result in a new tibble called `sns_terms`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The PCA technique will only work with a matrix of numeric data. However, because
    each of the resulting 36 columns is a count, no more data preparation is needed.
    If the dataset included categorical features, it would be necessary to convert
    these to numeric before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: Base R includes a built-in PCA function called `prcomp()`, which becomes slow
    to run as datasets get larger. We’ll use a drop-in substitute from the `irlba`
    package by Bryan W. Lewis, which can be stopped early to return only a subset
    of the full set of potential principal components. This truncated approach, plus
    the use of a generally more efficient algorithm, makes the `irlba_prcomp()` function
    much more speedy than `prcomp()` on larger datasets, while keeping the syntax
    and compatibility virtually identical to the base function, in case you are following
    along with older online tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `irlba` package gets its strange-seeming name from the technique it uses:
    the “implicitly restarted Lanczos bidiagonalization algorithm” developed by Jim
    Baglama and Lothar Reichel. For more information on this approach, see the package
    vignette using the `vignette("irlba")` command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before beginning, we’ll set the random seed to an arbitrary value of `2023`
    to ensure your results match the book. Then, after loading the required package,
    we’ll pipe the `sns_terms` dataset into the PCA function. The three parameters
    allow us to limit the result to the first 10 principal components while also standardizing
    the data by centering each feature around zero and scaling them to have a variance
    of one. This is usually desirable for much the same reason it is in the k-Nearest
    Neighbors approach: it prevents features with larger variance from dominating
    the principal components. The results are saved as an object named `sns_pca`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Although PCA is a deterministic algorithm, the sign—positive or negative—is
    arbitrary and can vary from run to run, hence the need to set the random seed
    beforehand to guarantee reproducibility. This Stack Exchange thread has more information
    on this phenomenon: [https://stats.stackexchange.com/questions/88880/](https://stats.stackexchange.com/questions/88880/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that each component in the PCA captures a decreasing amount of the dataset’s
    variance and that we requested 10 of the possible 36 components. A **scree plot**,
    named after the “scree” landslide patterns that form at the bottom of cliffs,
    helps visualize the amount of variance captured by each component and may thus
    help to determine the optimal number of components to use. R’s built-in `screeplot()`
    function can be applied to our result to create such a plot. The four parameters
    supply our PCA result, indicate that we want to plot all 10 components, use a
    line graph rather than a bar plot, and apply the plot title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_13_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: The scree plot depicting the variance of the first 10 principal
    components of the social media dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scree plot shows that there is a substantial drop in the variance captured
    between the first and second components. The second through fifth components capture
    approximately the same amount of variance, and then there are additional substantial
    drops between the fifth and sixth components and between the sixth and seventh
    components. The seventh through tenth components capture approximately the same
    amount of variance. Based on this result, we might decide to use one, five, or
    six principal components as our reduced-dimensionality dataset. We can see this
    numerically by applying the `summary()` function to our PCA results object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the standard deviation, the proportion of the total variance,
    and the cumulative proportion of variance for each of the 10 components (labeled
    `PC1` to `PC10`). Because standard deviation is the square root of variance, squaring
    the standard deviations produces the variance values depicted in the scree plot;
    for example, *1.8237**5*² *= 3.326064*, which is the value shown for the first
    component in the scree plot. A component’s proportion of variance is its variance
    out of the total for all components— not only the 10 shown here, but also the
    remaining 26 that we could have created. Therefore, the cumulative proportion
    of variance maxes out at 41.99% rather than the 100% that would be explained by
    all 36 components.
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA as a dimensionality reduction technique requires the user to determine
    how many components to keep. In this case, if we choose five components, we will
    capture 26.7% of the variance, or one-fourth of the total information in the original
    data. Whether or not this is sufficient depends on how much of the remaining 73.3%
    of variance is signal or noise—something that we can only determine by attempting
    to build a useful learning algorithm. One thing that makes this process easier
    is that regardless of how many components we ultimately decide upon, our PCA process
    is complete; we can simply use as few or as many of the 10 components as desired.
    For instance, there is no need to re-run the algorithm to obtain the best three
    versus the best seven components; finding the first seven components will naturally
    already include the best three and the results will be identical. In a real-world
    application of PCA, it may be wise to test several different cut points.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity here, we’ll reduce the original 36-dimension dataset to five
    principal components. By default, the `irlba_prcomp()` function automatically
    saves a version of the original dataset that has been transformed into the lower-dimension
    space. This is found in the resulting `sns_pca` list object with the name `x`,
    which we can examine with the `str()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed dataset is a numeric matrix with 30,000 rows like the original
    dataset but 10 rather than 36 columns with names from `PC1` to `PC10`. We can
    see this more clearly by using the `head()` command output to see the first few
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Recall that in the original dataset, each of the 36 columns indicated the number
    of times a particular word appeared in the social media profile text. If we standardized
    the data to have a mean of zero, as we did in *Chapter 9*, *Finding Groups of
    Data – Clustering with k-means*, and has been done here for the principal components,
    then positive and negative values indicate profiles with higher or lower-than-average
    values, respectively. The trick is that each of the 36 original columns had an
    obvious interpretation, whereas the PCA results are without apparent meaning.
  prefs: []
  type: TYPE_NORMAL
- en: We can attempt to understand the components by visualizing the PCA **loadings**,
    or the weights that transform the original data into each of the principal components.
    Large loadings are more important to a particular component. These loadings are
    found in the `sns_pca` list object with the name `rotation`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a numeric matrix with 36 rows corresponding to each of the original
    columns in the dataset and 10 columns that provide the loadings for the principal
    components. To construct our visualization, we will need to pivot this data such
    that it has one row per social media term per principal component; that is, we
    will have *36 * 10 = 360* rows in the longer version of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command uses two steps to create the required long dataset. The
    first step creates a tibble including a `SNS_Term` column with one row for each
    of the 36 terms as well as the `sns_pca$rotation` matrix, which is converted into
    a tibble using `as_tibble()`. The combined tibble, with 11 columns and 36 rows,
    is piped into the `pivot_longer()` function, which pivots the table from wide
    to long format. The three parameters tell the function to pivot the 10 columns
    from `PC1` to `PC10`, with the former column names now becoming the rows for a
    column named `PC` and the former column values now becoming row values of a column
    named `Contribution`. The full command creates a tibble with 3 columns and 360
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ggplot()` function can now be used to plot the most important contributing
    terms for a given principal component. For example, to look at the third principal
    component, we’ll `filter()` the rows to limit to only `PC3`, select the top 15
    largest contribution values—considering both positive and negative values using
    the `abs()` absolute value function—and mutate the `SNS_Term` to reorder by the
    contribution amount. Ultimately, this is piped into `ggplot()` with a number of
    adjustments to the formatting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the plot that follows. Because the terms with positive
    and negative impacts seem to be split across subjects related to sex, drugs, and
    rock and roll, one might argue that this principle component has identified a
    stereotypical dimension of teen identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_13_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: The top 15 terms contributing to PC3'
  prefs: []
  type: TYPE_NORMAL
- en: 'By repeating the above `ggplot` code for the four other principal components
    among the first five, we observe similar distinctions, as shown in the figure
    that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_13_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: The top 15 terms contributing to the other four principal components'
  prefs: []
  type: TYPE_NORMAL
- en: '`PC1` is particularly interesting, as every term has a positive impact; this
    may be distinguishing people who have anything versus nothing at all on their
    social media profiles. `PC2` seems to favor shopping-related terms, while `PC4`
    seems to be a combination of music and sports, without sex and drugs. Lastly,
    it seems that `PC5` may be distinguishing between sports and non-sports-related
    terms. Examining the charts in this way will help to understand each component’s
    impact on the predictive model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous visualization method was adapted from an outstanding tutorial
    from Julia Silge, author of *Text Mining with R: A Tidy Approach* (2017). For
    a deeper dive into PCA, see [https://juliasilge.com/blog/stack-overflow-pca/](https://juliasilge.com/blog/stack-overflow-pca/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'An understanding of principal component analysis is of little value if the
    technique is not useful for building machine learning models. In the previous
    example, we reduced the dimensionality of a social media dataset from 36 to 10
    or fewer components. By merging these components back into the original dataset,
    we can use them to make predictions about a profile’s gender or number of friends.
    We’ll begin by using the `cbind()` function to combine the first four columns
    of the original data frame with the transformed profile data from the PCA result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll build a linear regression model predicting the number of social
    media friends as a function of the first five principal components. This modeling
    approach was introduced in *Chapter 6*, *Forecasting Numeric Data – Regression
    Methods*. The resulting output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Because the value of the intercept is approximately 30.18, the average person
    in this dataset has about 30 friends. People with higher values of `PC1`, `PC2`,
    `PC4`, and `PC5` are expected to have more friends, while higher values of `PC3`
    are associated with fewer friends, assuming all else is equal. For example, for
    each unit increase in `PC2`, we would anticipate about one additional friend on
    average. Given our understanding of the components, these findings make sense;
    the positive values of `PC2`, `PC3`, and `PC5` were associated with more social
    activities. In contrast, `PC3` was about sex, drugs, and rock and roll, which
    may be somewhat antisocial.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a very simple example, PCA can be used in the same way with
    much larger datasets. In addition to mitigating the curse of dimensionality, it
    also has the benefit of reducing complexity. For instance, a dataset with a very
    large number of predictors may be too computationally expensive for k-Nearest
    Neighbors or an artificial neural network to run as is, but by selecting a smaller
    number of principal components, such techniques may be within reach.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to experiment with PCA and contrast this type of feature extraction
    with other feature selection methods like filters and wrappers; you may find that
    you have better luck with one approach or the other. Even if you choose not to
    use dimensionality reduction, there are other problems with highly dimensional
    data that you will discover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making use of sparse data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As datasets increase in dimension, some attributes are likely to be **sparse**,
    which means most observations do not share values of the attribute. This is a
    natural consequence of the curse of dimensionality in which this ever-increasing
    detail turns observations into outliers identified by their unique combination
    of attributes. It is very uncommon for sparse data to have any specific value,
    or perhaps even any value at all—as was the case in the sparse matrices for text
    data found in *Chapter 4*, *Probabilistic Learning – Classification Using Naive
    Bayes*, and the sparse matrices for shopping cart data in *Chapter 8*, *Finding
    Patterns – Market Basket Analysis Using Association Rules*.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the same as missing data, where typically a relatively small portion
    of values are unknown. In sparse data, most values are known, but the number of
    interesting, meaningful values is dwarfed by an overwhelming number of values
    that add little value to the learning task. With missing data, machine learning
    algorithms struggle to learn something from nothing; with sparse data, machine
    learning algorithms struggle to find the needle in the haystack.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying sparse data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sparse data can appear in several interrelated forms. Perhaps the most encountered
    form is categorical, in which a single feature has a very large number of levels
    or categories, with some having extremely small counts relative to the others.
    Features like this are said to have high **cardinality** and will lead to sparse
    data problems when fed to a learning algorithm. An example of this is zip codes;
    in the United States, there are over 40,000 postal codes, with some having more
    than 100,000 residents and others having less than 100\. Consequently, if a sparse
    zip code feature is included in a modeling project, the learning algorithm is
    likely to struggle to find the balance between ignoring and overemphasizing areas
    with few residents.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features with many levels are often expressed as a series of binary
    features with one feature per level. We have used such features many times when
    we manually constructed binary dummy variables, and many learning algorithms do
    the same automatically for categorical data. This can lead to a situation in which
    binary features are sparse as the 1 values are overwhelmed by the 0 values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a zip code dataset for the U.S. population, a tiny fraction
    of the 330 million residents will fall into each of the 40,000 postal codes, thus
    making each binary zip code feature highly sparse and difficult for a learning
    algorithm to use.
  prefs: []
  type: TYPE_NORMAL
- en: Many forms of so-called “big” data are inherently highly dimensional and sparse.
    Sparseness is closely related to the curse of dimensionality. Just like how the
    ever-expanding universe creates greater voids of empty space between objects,
    one might argue that every dataset becomes sparse as more dimensions are added.
    Text data is usually sparse because each word can be treated as a dimension and
    there are countless words that can appear, each having a low probability of appearing
    in a specific document. Other big data forms, like DNA data, transactional market
    basket data, and image data, also often exhibit the problem of sparseness. Unless
    the dataset’s density is increased, many learning algorithms will struggle to
    make use of the rich, big dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Even a simple numeric range of data can be sparse. This occurs when the distribution
    of numeric values is wide, which leads to some ranges of the distribution having
    a very low density. Income is one example of this, because the values generally
    become increasingly sparse for higher incomes. This is closely related to the
    problem of outliers, but here we clearly hope to model the outlying values. Sparse
    numeric data can also be found in cases when the numbers have been stored in an
    overly specific degree of precision. For example, if age values are stored with
    decimals rather than integers, such as 24.9167 and 36.4167 rather than simply
    24 and 36, this creates an implied void between numbers that some learning algorithms
    may struggle to ignore. For instance, a decision tree might distinguish between
    people that are 24.92 and 24.90 years old—probably more likely to be related to
    overfitting than a meaningful distinction in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the sparseness of a dataset manually can assist a learning algorithm
    with identifying the important signals and ignoring the noise. The approach used
    depends on the type and degree of sparse data as well as the modeling algorithm
    used. Some algorithms are better than others at handling certain types of sparse
    data. For example, naive Bayes performs relatively well with sparse categorical
    data, regression methods do relatively well with sparse numeric data, and decision
    trees tend to struggle with sparse data in general due to their preference for
    features with a larger number of categories. More sophisticated methods like deep
    neural networks and boosting can help, but in general, it is better if the dataset
    can be made denser prior to the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Remapping sparse categorical data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen in prior chapters, when adding a categorical feature to a dataset,
    it is usually transformed into a set of binary variables equal to the number of
    levels of the original feature using dummy or one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there are 40,000 zip codes in the United States, the machine
    learning algorithm would have 40,000 binary predictors for this feature. This
    is called a **one-of-n mapping** because only one of the 40,000 features would
    have a value of 1 while the remainder would have values of 0—a case of extreme
    growth in dimensionality and sparseness.
  prefs: []
  type: TYPE_NORMAL
- en: To increase the density of a one-of-n mapping, an **m-of-n mapping** may be
    used instead, which reduces the *n* binary variables to a smaller set of *m* variables.
    For example, with zip codes, instead of creating a 40,000-level feature with one
    level per zip code, one might choose to map into 100 levels by using the first
    two digits of the zip code from 00 to 99\. Similarly, if it would create too much
    sparseness to include a binary feature for each of the 200 countries in the world,
    it might be possible to map countries to a smaller set of continents, like Europe,
    North America, and Asia, instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating an m-of-n mapping, it is best if the groupings represent a shared
    underlying characteristic, but it is possible to use other approaches as well.
    Domain knowledge can be helpful for creating a remapping that reflects the shared
    characteristics of the more granular units. In the absence of domain expertise,
    the following methods may be appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: Leave the larger categories as is and group only the categories with small numbers
    of observations. For example, zip codes for dense urban areas could be included
    directly, but sparse rural zip codes could be grouped into larger geographic areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the impact of the categories on the target variable by creating a two-way
    cross table or computing the average outcome by level and group levels that have
    a similar impact on the response variable. For example, if certain zip codes are
    more likely to default on a loan, create a new category composed of these zip
    codes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a more sophisticated variant of the previous method, it may also be possible
    to build a simple machine learning model that predicts the target using the highly
    dimensional feature and then group levels of the feature that have a similar relationship
    with the target or with other predictors. Simple methods like regression and decision
    trees would be ideal for this approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a remapping strategy has been chosen, helpful functions for recoding categorical
    variables can be found in the `forcats` package ([https://forcats.tidyverse.org](https://forcats.tidyverse.org)),
    which is part of the base set of packages comprising the tidyverse. The package
    includes options for automatically recoding categorical variables with sparse
    levels, or manually recoding if a more guided approach is desired. Detailed information
    about the package is available in the *R for Data Science* chapter at [https://r4ds.hadley.nz/factors.html](https://r4ds.hadley.nz/factors.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll examine a couple of approaches for remapping using the Titanic dataset
    and the passenger titles created in *Chapter 12*, *Advanced Data Preparation*.
    Because the `forcats` package is included in the base tidyverse, it can be loaded
    with the entire suite or on its own using the `library(forcats)` command. We’ll
    begin by loading the tidyverse, reading the Titanic dataset, and then examining
    the levels of the title feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous chapter, we used base R’s `recode()` function to combine the
    variants of *Miss*, such as *Ms*, *Mlle*, and *Mme*, into a single group. The
    `forcats` package includes an `fct_collapse()` function, which is more convenient
    to use for categorical features with a large number of levels. We’ll use it here
    to create an m-of-n mapping that creates groups based on knowledge of the titles’
    real-world meanings. Note that several of the new categories are one-to-one mappings
    of the previous categories, but by including a vector of labels, we can map several
    of the old levels to a single new level, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining the new categorization, we see that the 17 original categories have
    been reduced to 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had a much larger set of levels, or in the absence of knowledge of how
    categories should be grouped, we can leave large categories as is and group the
    levels with few examples. The `forcats` package includes a simple function for
    examining the levels of our feature. Although this can also be done with base
    R functions, the `fct_count()` function provides a sorted list of the feature
    levels and their proportions of the overall total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This output can inform groupings based on a minimum number or minimum proportion
    of observations. The `forcats` package has a set of `fct_lump()` functions to
    help with this process of “lumping” factor levels into an “other” group. For example,
    we might take the top three levels and treat everything else as other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can lump together all levels with less than one percent of
    the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we might choose to lump together all levels with less than five observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The choice of which of these three functions to use, as well as the appropriate
    parameter value, will depend on the dataset used and the desired number of levels
    for the m-of-n mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Binning sparse numeric data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While many machine learning methods handle numeric data without trouble, some
    approaches like decision trees are more likely to struggle with numeric data,
    especially when it exhibits some of the characteristics of sparseness. A common
    solution to this problem is called **discretization**, which converts a range
    of numbers into a smaller number of discrete categories called bins. We encountered
    this method previously in *Chapter 4*, *Probabilistic Learning – Classification
    Using Naive Bayes*, as we discretized the numeric data to work with the naive
    Bayes algorithm. Here, we will apply a similar approach, using modern tidyverse
    methods, to reduce the dimensionality of the number range to help address the
    tendency of some methods to over- or underfit to sparse numeric data.
  prefs: []
  type: TYPE_NORMAL
- en: As is the case with many machine learning approaches, ideally one would apply
    subject-matter expertise to determine the cut points for discretizing a numeric
    range. For example, on a range of age values, perhaps meaningful break points
    could occur between well-established childhood, adulthood, and elderly age groups,
    to reflect the impact of these age values in the real world. Similarly, bins may
    be created for salary levels such as lower, middle, and upper class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the absence of real-world knowledge of important categories, it is often
    advisable to use cut points that reflect natural percentiles of data or intuitive
    increments of values. This may mean dividing a range of numbers using strategies
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating groups based on tertiles, quartiles, quintiles, deciles, or percentiles
    that contain equal proportions of examples (33%, 25%, 20%, 10%, or 1%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using familiar cut points for the underlying range of values, such as grouping
    time values by hours, half hours, or quarter hours; grouping 0-100 scale values
    by fives, tens, or twenty-fives; or bucketing large numeric ranges like income
    by large multiples of 10 or 25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the notion of log scaling to skewed data, so that the bins are proportionally
    wider for the skewed portion of the data where the values are sparser; for example,
    income might be bucketed into groups of 0-10,000 followed by 10,000-100,000, then
    100,000-1,000,000, and 1,000,000 or more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate these approaches, we’ll apply discretization techniques to the
    fare values in the Titanic dataset used previously. The `head()` and `summary()`
    functions illustrate that the values are highly granular and highly sparse on
    the high end due to their severe right skew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we are most interested in the difference between first class and other
    passengers and that we assume that the top 25 percent of fares reflect first-class
    tickets. We could easily create a binary feature using the tidyverse `if_else()`
    function as follows. If the fare has a value of at least £31, which is the value
    for the third quartile, then we’ll assume it is a first-class fare and assign
    a value of `1` to the binary-coded `fare_firstclass` feature; if not, it receives
    a `0` value. The `missing` parameter tells the function to assign the value `0`
    if the fare was missing, under the assumption that first-class fares are very
    unlikely to be unknown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This reduces a feature with nearly 250 distinct values into a new feature with
    only two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Although this was a very simple example, it’s a first step toward more complex
    binning strategies. The `if_else()` function, although simple here, would be unwieldy
    to use for creating a new feature with more than two levels. This would require
    nesting `if_else()` functions within each other, which quickly becomes difficult
    to maintain. Instead, a tidyverse function called `case_when()` allows the construction
    of a more complex series of checks to determine the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code that follows, the fare data is binned into three levels corresponding
    roughly to first-, second-, and third-class fare levels. The `case_when()` statement
    is evaluated as a series of ordered if-else statements. The first statement checks
    whether the fare is at least 31 and assigns these examples the first-class category.
    The second can be read as an else-if statement; that is, if the first statement
    is not true—the “else”—we check “if” the fare is at least 15 and assign the second-class
    level if true. The final statement is the ultimate “else” as `TRUE` always evaluates
    to true, and thus all records not categorized by the first and second lines are
    assigned the third-class level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting feature has three levels as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In the case that we have zero understanding of the real-world meaning of the
    fares, such as the knowledge of first, second, and third-class fares, we might
    instead apply the discretization heuristics described previously, which use natural
    percentiles or intuitive cut points of values instead of meaningful groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cut()` function is included in base R and provides a simple method for
    creating a factor from a numeric vector. The `breaks` parameter specifies the
    cut points for the numeric range, shown as follows for a three-level factor that
    matches the previous discretization. The `right = FALSE` parameter indicates that
    the levels should not include the rightmost, or highest, value and the `Inf` break
    point indicates that the final category can span the range of values from 31 to
    infinity. The resulting categories are identical to the prior result, but use
    different labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: By default, `cut()` sets labels for factors that indicate the range of values
    falling into each level. Square brackets indicate that the bracketed number is
    included in the level, while parentheses indicate a number that is not included.
    A `labels` parameter can be assigned a vector of factor labels for the result,
    if desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cut()` function becomes more interesting when combined with a sequence
    of values generated by the `seq()` function. Here, we create levels for the 11
    ranges of values from 0 to 550 in increments of 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Using evenly wide intervals here reduces the dimensionality but doesn’t solve
    the problem of sparseness. The first two levels contain most of the examples,
    but the remainder have very few, or even zero in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to having equally sized intervals, we can construct bins
    with an equal number of examples. We have used the `quantile()` function in previous
    chapters to identify the cut points for quintiles and percentiles, but we would
    still need to use these values with a `cut()` function to create the factor levels.
    The following code creates five bins for the quintiles, but could be adapted for
    quartiles, deciles, or percentiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note that the bins do not contain exactly the same number of examples due to
    the presence of ties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tidyverse also includes a function for creating quantile-based groups,
    which may be easier to use in some cases. This `ntile()` function divides the
    data into `n` groups of equal size. For example, it can create five groups as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the function assigns the groups numeric labels, it is important to
    convert the resulting vector to a factor. This can be done directly in a `mutate()`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting feature has 11 equally proportioned levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Although the level still has numeric labels, because the feature has been coded
    as a factor, it will still be treated as categorical by most R functions. Of course,
    it is still important to find the right balance between too few and too many levels.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The teaching datasets used for examples in previous chapters rarely had the
    problem of missing data, where a value that should be present is instead absent.
    The R language uses the special value `NA` to indicate these missing values, which
    cannot be handled natively by most machine learning functions. In *Chapter 9*,
    *Finding Groups of Data – Clustering with k-means*, we were able to replace missing
    values with a guess of the true value based on other information available in
    the dataset in a process called imputation. Specifically, the missing age values
    of high school students were imputed with the average age of students that had
    the same graduation year. This provided a reasonable estimate of the unknown age
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data is a much greater problem in real-world machine learning projects
    than would be expected given its rarity so far. This is not only because real-world
    projects are messier and more complex than simple textbook examples. Additionally,
    as datasets increase in size—as they include more rows or more columns—a relatively
    small proportion of missingness will cause more problems, as it becomes more likely
    that any given row or any given column contains at least one missing value. For
    example, even if the rate of missingness is only one percent, in a dataset with
    100 columns, we would expect the average row to have one missing value. In this
    case, simply excluding all rows with missing values would drastically reduce the
    size of the dataset to the point of nothingness!
  prefs: []
  type: TYPE_NORMAL
- en: 'In fields like economics, biostatistics, and the social sciences, the gold
    standard approach to missing data is **multiple imputation**, which uses statistical
    modeling or machine learning techniques to impute all the missing feature values
    given the non-missing feature values. Because this tends to decrease the variability
    of the data, and thus inflates the certainty of predictions, modern multiple imputation
    software tends to add random variation to the imputed values to avoid biasing
    the inferences made from the dataset. R has many packages for performing multiple
    imputation, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mice`: Multivariate Imputation by Chained Equations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Amelia`: A Program for Missing Data (named after the famous pilot Amelia Earhart,
    who went missing in 1937 during an attempt to become the first female pilot to
    fly around the globe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Simputation`: Simple Imputation, which attempts to simplify missing data handling
    via the use of tidyverse-compatible functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`missForest`: Nonparametric Missing Value Imputation using Random Forest, a
    package that uses state-of-the-art machine learning methods to impute any type
    of data, even types with complex, nonlinear relationships among the features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the wealth of multiple imputation software tools, in comparison to projects
    in traditional statistics and the social sciences, machine learning projects apply
    simpler methods for handling missing data. This is because the goals and considerations
    differ. Machine learning projects tend to focus on methods that work on very large
    datasets and facilitate prediction on a future, unseen test set, even if certain
    statistical assumptions are violated. On the other hand, the more formal methods
    of the social sciences focus on strategies that tend to be more computationally
    intensive but lead to unbiased estimates for inference and hypothesis testing.
    Keep this distinction in mind while reading the sections that follow, which cover
    common practical techniques for handling missing data, but are generally not advisable
    for formal scientific analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding types of missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all missing data is created equally, and some types are more problematic
    than others. For this reason, when preparing data with missing values, it is useful
    to consider the underlying reasons why a particular value is missing. Try to picture
    yourself inside the process that generated the dataset and ask yourself why certain
    values were left blank. Is there a logical reason it is missing? Or, was it left
    blank purely by mistake or chance alone? Answering these questions helps inform
    the solution for replacing the missing values in a responsible manner. The answers
    to these questions also distinguish three different types of missing data, from
    least problematic to most severe:'
  prefs: []
  type: TYPE_NORMAL
- en: Data **missing completely at random** (**MCAR**) is independent of the other
    features and its own value; in other words, it would not be possible to predict
    whether any particular value is missing. The missingness may be caused due to
    a random data entry error, or some other process that randomly skips the value.
    Missing completely at random can be imagined as a completely unpredictable process
    that takes the final matrix of data and randomly selects cells to delete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data **missing at random** (**MAR**) may depend on other features but not on
    the underlying value, which means that certain, predictable rows are more likely
    than others to contain missing values. For example, households in certain geographic
    regions may be less willing to report their household income, but assuming they
    do disclose such information, they do so honestly. Essentially, MAR implies that
    the missing values are randomly selected after controlling for the underlying
    factor or factors causing the missingness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data **missing not at random** (**MNAR**) is missing due to a reason related
    to the missing value itself. This data is in essence censored from the dataset
    for some reason impossible to discern from the other features in the dataset.
    For instance, poorer individuals may feel less comfortable sharing their income,
    so they simply leave it blank. Another example might be a temperature sensor that
    reports a missing value for extremely high or low temperatures. It is probable
    that most real-world missing values are MNAR, as there is usually some unmeasured,
    hidden mechanism causing the missingness. Very little is truly random in the real
    world.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Imputation methods work well for the first two types of missing data. Although
    one might be led to believe that MCAR data is the most challenging to impute due
    to its independence and unpredictability, it is actually the ideal type of missing
    data to handle. Even though the missingness is completely random, the values that
    have been randomly hidden may be predictable given the other available features.
    Stated differently, the *missingness* itself is unpredictable, but the underlying
    missing *values* may be quite predictable. Similarly, MAR data is also readily
    predictable by the given features.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, NMAR data, which is perhaps the most common type of missing data,
    is the least capable of being predicted. Because the missing values were censored
    by an unknowable process, any model built on this data will have an incomplete
    picture of the relationship between the missing and non-missing data, and the
    results are likely to be biased toward the non-missing data. For example, suppose
    we are trying to build a model of loan default, and poorer people are more likely
    to leave the income field blank on the loan application. If we impute the missing
    incomes, the imputed values will tend to be higher than the true values, as our
    imputation was based only on the available data, which is missing more low values
    than high values. If lower-income households are more likely to default, a model
    that uses the biased imputed income values to predict loan outcomes will underestimate
    the probability of default for households that left income blank.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the possibility of such bias, strictly speaking, we should only impute
    MCAR and MAR data. Yet, imputation may be the lesser of two imperfect options,
    since excluding rows with missing data from the training dataset will also bias
    the model if the data is not missing completely at random. Thus, despite violating
    statistical assumptions, machine learning practitioners often impute missing values
    rather than removing missing data from the dataset. The following sections demonstrate
    a few common strategies employed toward this end.
  prefs: []
  type: TYPE_NORMAL
- en: Performing missing value imputation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because `NA` values cannot be handled directly by many R functions nor most
    machine learning algorithms, they must be replaced with something else, and ideally,
    in a way that improves the model’s performance. In machine learning, this type
    of missing value imputation is a barrier to prediction, which means that simpler
    approaches that work reasonably well are favored over more complex approaches—even
    if the complex approaches may be more methodologically and theoretically sound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Missing character-type data may provide the form of missingness with the simplest
    possible solution, as it is possible to merely treat the missing values like any
    other value by recoding the `NA` values to a literal character string like `''Missing''`,
    `''Unknown''`, or another label of your choosing. The string itself is arbitrary;
    it just needs to be consistent for each missing value within the column. For example,
    the Titanic dataset includes two categorical features with missing data: `Cabin`
    and `Embarked`. We can easily impute `''X''` in place of the missing `Cabin` values
    and `''Unknown''` in place of missing `Embarked` values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Although this method has eliminated the `NA` values by replacing them with valid
    character strings, it seems as if more sophisticated approaches ought to be possible.
    After all, couldn’t we use machine learning to predict missing values using the
    remaining columns in the dataset? Indeed, this is possible, as we will learn shortly.
    However, using this type of advanced approach may be overkill if not actually
    detrimental to the model’s predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: The reasons that one might perform missing value imputation vary across disciplines.
    In traditional statistics and the social sciences, models are often used for inference
    and hypothesis testing rather than for predicting and forecasting. When used for
    inference, it is very important that the relationships among features within the
    dataset are preserved as carefully as possible, as statisticians seek to carefully
    estimate and understand each feature’s individual connection to the outcome of
    interest. Imputing arbitrary values into the missing slots may distort these relationships—particularly
    in the case that the values are not missing completely at random.
  prefs: []
  type: TYPE_NORMAL
- en: Rather, more sophisticated approaches use the other available information to
    impute a reasonable guess as to the true value, keeping as many rows of data as
    possible, while also ensuring that the data’s important internal relationships
    across features are preserved. Ultimately, this increases the **statistical power**
    of the analysis, which relates to the capabilities of detecting patterns and testing
    hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, machine learning practitioners are often less concerned with the
    internal relationships among a dataset’s features, and more focused on the features’
    relationship with an external target outcome. From this perspective, there is
    no strong reason to apply sophisticated imputation strategies. Such methods do
    not contribute new information that can be used to better predict the target because
    they merely reinforce internal patterns. On the other hand, by the assumption
    that the data is not missing at random, it may be less helpful to focus on the
    specific value to impute in a missing slot, and instead focus effort on trying
    to use the missingness itself as a predictor of the target.
  prefs: []
  type: TYPE_NORMAL
- en: Simple imputation with missing value indicators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The practice described in the previous section, in which missing categorical
    values were replaced with an arbitrary string like `'Missing'` or `'Unknown'`,
    is one form of **simple imputation**, in which `NA` values are simply replaced
    by a constant value. For numeric features, we can use an equivalent approach.
    For each feature with a missing value, choose a value to impute in place of the
    `NA` values. This can be a summary statistic such as mean, median, or mode, or
    it may be an arbitrary number—the specific value generally doesn’t matter.
  prefs: []
  type: TYPE_NORMAL
- en: Although the exact value generally doesn’t matter, the most common approach
    may be **mean imputation**, perhaps due to the common practice of doing this in
    the field of traditional statistics. An alternative approach is to use a value
    on the same order of magnitude but outside the range of actual values found in
    the data. For example, for missing age values in the range of 0 to 100, you may
    choose to impute the value -1 or 999\. Keep in mind that regardless of the value
    chosen, any summary statistics computed on the feature will be distorted by the
    imputed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to imputing a value in place of the `NA`, it is especially important
    to create a **missing value indicator** (**MVI**), which is a binary variable
    that indicates whether the feature value was imputed. We’ll do this for the missing
    Titanic passenger age values using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Both the feature that has been imputed and the MVI should be included as predictors
    in the machine learning model. The fact that a value was missing is often an important
    predictor of the target, and surprisingly often, one of the strongest predictors
    of the target. This may not actually be unexpected under the simple belief that
    very little in the real world happens at random; if a value is missing, there
    is probably an explanation for it. For example, in the Titanic dataset, perhaps
    the missing age implies something about the passenger’s social status or family
    background. Similarly, someone who refuses to report their income or occupation
    on a loan application may be hiding the fact that they make a very small amount
    of money—which may be a strong predictor of a loan default. This finding that
    missing values are interesting predictors is also true for larger amounts of missing
    data; more missingness may lead to even more interesting predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Missing value patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Expanding upon the belief that a missing value may be a highly useful predictor,
    each additional missing value may contribute to our ability to forecast a specific
    outcome. In the case of loan application data, a person that has missing data
    on a single feature may have been intentionally hiding their answer, or they may
    have just accidentally skipped this question on the loan application form. If
    a person has missing data on multiple features, the latter excuse no longer applies,
    or perhaps it implies that they rushed through the application or are generally
    more irresponsible. We might assume that people with more missing data are more
    likely to default, but of course, we won’t know until we train the model; it could
    just as easily be the case that people with more missing data are in fact less
    likely to default, perhaps because some of the loan application questions don’t
    apply to their situation.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose there actually is a pattern to be found among records with large amounts
    of missingness, but it is not solely based on the number of missing values, but
    instead, the specific features that are missing. For example, someone who is afraid
    to report their income because it is too low may skip related questions on a loan
    application, whereas a small business owner may skip a different section of questions
    on employment history because they do not apply to someone who is self-employed.
    Both cases may have roughly equal numbers of missing values, but their patterns
    may differ substantially.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **missing value pattern** (**MVP**) can be constructed to capture such behaviors
    and use them as features for machine learning. A missing value pattern is essentially
    a character string composed of a series of MVIs, with each character in the string
    representing a feature with missing values. *Figure 13.12* illustrates how the
    process works for a simplified loan application dataset. For each of the eight
    features, we construct a missing value indicator that indicates whether the corresponding
    cell was missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_13_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: Constructing a missing value pattern begins with creating missing
    value indicators for each feature with missing data'
  prefs: []
  type: TYPE_NORMAL
- en: These binary MVIs are then concatenated into a single string. For example, the
    first row would be represented by the string `'11100000'`, which indicates that
    the first three features for this loan applicant were missing. The second applicant,
    who had no missing data, would be represented by `'00000000'`, while the second
    and third would be represented by `'00000111'` and `'01011101'`, respectively.
    The resulting `mvp` R character vector would be converted into a factor to allow
    a learning algorithm to use it for prediction. Each level of the factor represents
    a specific pattern of missingness; loan applicants that follow the same pattern
    may be likely to have similar outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Although missing value patterns can be extremely powerful predictors, they are
    not without some challenges. First and foremost, in a dataset containing *k* features,
    there are *2*^k potential values of a missing value pattern. A dataset with just
    10 features may have as many as 1,024 levels of the MVP predictor, while a dataset
    with 25 features would have over 33 million potential levels. A relatively small
    dataset with 50 features would have almost an uncountable number of potential
    MVP levels, which would make the predictor useless for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this potential issue, the hope with the MVP approach is that the potentially
    huge number of levels avoids the curse of dimensionality due to patterns of missingness
    that are far from uniform or random. In other words, the MVP approach depends
    strongly on data that is not missing at random; we hope there is a strong underlying
    pattern driving the missing values, which the missing value patterns will reflect.
    Overall, the less heterogeneity that is present in the missingness, the more often
    that certain patterns of missingness appear frequently in the data. Unfortunately,
    even if one feature is missing completely at random, it can reduce the utility
    of the MVP approach because even if rows are similar on nearly all of the binary
    missing value indicators, if a single one differs, it will be treated as a completely
    different missing value pattern. To address this issue, an alternative is to use
    the MVI dataset with an unsupervised clustering algorithm like the k-means algorithm
    covered in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, to
    create clusters of people with similar patterns of missingness.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of imbalanced data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most challenging data issues is **imbalanced data**, which occurs
    when one or more class levels are much more common than the others. Many, if not
    most, machine learning algorithms struggle mightily to learn heavily imbalanced
    datasets, and although there isn’t a specific threshold that determines when a
    dataset is too off-balance, the problems caused by the lack of balance become
    increasingly serious as the problem becomes more severe.
  prefs: []
  type: TYPE_NORMAL
- en: In the early stages of class imbalance, small problems are found. For instance,
    simple performance measures like accuracy begin to lose relevance and more sophisticated
    performance measures like those described in *Chapter 10*, *Evaluating Model Performance*,
    are needed. As the imbalance widens, bigger problems occur. For example, with
    extremely imbalanced datasets, some machine learning algorithms might struggle
    to predict the minority group at all. With this in mind, it might be wise to begin
    worrying about imbalanced data when the split is worse than 80% versus 20%, worry
    more when it is worse than 90% versus 10%, and assume the worst when the split
    is more severe than 99% to 1%.
  prefs: []
  type: TYPE_NORMAL
- en: A class imbalance can also occur if the real-world misclassification cost of
    one or more class levels is significantly higher or lower than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imbalanced data is a prevalent yet important challenge because many of the
    real-world outcomes we care to predict are significant mainly because they are
    both rare and costly. This includes prediction tasks to identify outcomes such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Severe illnesses or diseases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extreme weather and natural disasters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraudulent activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loan defaults
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware or mechanical failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wealth or so-called “whale” customers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you will soon learn, there is unfortunately no single best way to handle
    imbalanced classification problems like these and even the more advanced techniques
    are not without downsides. Perhaps the most important approach is to be aware
    of the problem of unbalanced data while recognizing that all solutions are imperfect.
  prefs: []
  type: TYPE_NORMAL
- en: Simple strategies for rebalancing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a dataset has a severe imbalance, with some class levels having too many
    or too few examples, a simple solution to this problem is to subtract examples
    from the majority classes or add examples of the minority classes. The former
    strategy is called **undersampling**, which in the simplest case involves discarding
    records at random from the majority classes. The latter approach is called **oversampling**.
    Ideally, one would simply collect more rows of data, but this is usually not possible.
    Instead, examples of the minority classes are duplicated at random until the desired
    class balance is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Under- and oversampling each have significant drawbacks but can be effective
    in certain circumstances. The main danger of undersampling is the risk of dropping
    examples that express small but important patterns in the data. Therefore, undersampling
    works best if a dataset is large enough to reduce the risk that removing a substantial
    portion of the majority classes will completely exclude key training examples.
    Moreover, it always leads to a feeling of defeat to voluntarily surrender information
    in the era of big data.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling avoids this disappointment by generating additional minority class
    examples but risks overfitting to unimportant patterns or noise in the minority
    cases. Both under- and oversampling have been included in more advanced **focused
    sampling** approaches that avoid simple random sampling in favor of favoring records
    that maximize the decision boundaries between the groups.
  prefs: []
  type: TYPE_NORMAL
- en: Such techniques are rarely used in practice due to their computational inefficiency
    and limited real-world efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more in-depth review of strategies for handling imbalanced data, refer
    to *Data Mining for Imbalanced Datasets: An Overview, Chawla, N., 2010, in Data
    Mining and Knowledge Discovery Handbook, 2nd Edition, Maimon, O. and Rokach. L*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate resampling techniques, we’ll return to the teenage social media
    dataset used previously in this chapter, and begin by loading and preparing it
    with several tidyverse commands. First, using `fct_` functions from the `forcats`
    package, the `gender` feature is recoded as a factor with `Male` and `Female`
    labels and the `NA` values are recoded to `Unknown`. Then, outlier ages below
    13 years or greater than 20 years are replaced by `NA` values. Next, using `group_by()`
    in combination with `mutate()` allows us to impute the missing ages with the median
    age by graduation year. Lastly, we `ungroup()` the data and reorder the columns
    with `select()` such that our features of interest appear first in the dataset.
    The full command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'In this dataset, males and people of unknown gender are underrepresented, which
    we can confirm with the `fct_count()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'One approach would be to undersample the female and male groups such that all
    three have the same number of records. The `caret` package, which was first introduced
    in *Chapter 10*, *Evaluating Model Performance*, includes a `downSample()` function
    that can perform this technique. The `y` parameter is the categorical feature
    with the levels to be balanced, the `x` parameter specifies the remaining columns
    to include in the resampled data frame, and the `yname` parameter is the name
    of the target column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataset includes 2,724 examples of each of the three class levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The `caret` package’s `upSample()` function performs oversampling, such that
    all three levels have the same number of examples as the majority class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataset includes 22,054 examples of each of the three gender
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Whether the oversampling or undersampling approach works better depends on the
    dataset as well as the machine learning algorithm used. It may be wise to build
    models trained on datasets created by each of these resampling techniques and
    see which one performs better in testing. However, it is very important to keep
    in mind that the performance measures should be computed on an unbalanced test
    set; the evaluation should reflect the original class imbalance, as this is how
    the model will need to perform during real-world deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a synthetic balanced dataset with SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to undersampling and oversampling, a third rebalancing approach,
    called **synthetic generation**, creates brand-new examples of the minority class
    with the goal of reducing oversampling’s tendency to overfit the minority class
    examples. Today, there are many synthetic generation rebalancing methods, but
    one of the first to gain widespread prominence was the **SMOTE** algorithm introduced
    by *Chawla* et al. in 2002, with a name that refers to its use of a synthetic
    minority oversampling technique. Put simply, the algorithm uses a set of heuristics
    to construct new records that are similar to but not exactly the same as those
    previously observed. To construct similar records, SMOTE uses the notion of similarity
    described in *Chapter 3*, *Lazy Learning – Classification Using Nearest Neighbors*,
    and in fact, uses aspects of the k-NN approach directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the SMOTE algorithm, see *SMOTE: Synthetic Minority
    Over-Sampling Technique, 2002, Chawla, N., Bowyer, K., Hall, L., and Kegelmeyer,
    W., Journal of Artificial Intelligence Research, Vol. 16, pp. 321-357*.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how SMOTE works, suppose we wanted to oversample a minority class
    such that the resulting dataset has twice as many examples of this class. In the
    case of standard oversampling, we would simply duplicate each minority record
    so that it appears twice. In *synthetic* oversampling techniques like SMOTE, rather
    than duplicating each record, we will create a new synthetic record. If more or
    less oversampling is desired, we simply generate more or less than one new synthetic
    record per original record.
  prefs: []
  type: TYPE_NORMAL
- en: 'A question remains: how exactly are the synthetic records constructed? This
    is where the k-Nearest Neighbors technique comes in. The algorithm finds the *k*
    nearest neighbors of each of the original observations of the minority class.
    By convention, *k* is often set to five, but it can be set larger or smaller if
    desired. For each synthetic record to be created, the algorithm randomly selects
    one of the original observations’ *k* nearest neighbors. For example, to double
    the minority class, it would randomly select one nearest neighbor out of five
    for each of the original observations; to triple the original data, two out of
    the five nearest neighbors would be selected for each observation, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Because randomly selecting the nearest neighbors merely copies the original
    data, one more step is needed to generate synthetic observations. In this step,
    the algorithm identifies the vector between each of the original observations
    and its randomly chosen nearest neighbors. A random number between 0 and 1 is
    chosen to reflect the proportion of distance along this line to place the synthetic
    data point. This point’s feature values will be somewhere between 100 percent
    identical to the original observation’s feature values and 100 percent identical
    to the neighbor’s feature values—or anywhere in between. This is depicted in the
    following figure, which illustrates how synthetic observations can be randomly
    placed on the lines connecting the four original data points to their neighbors.
    Adding the six synthetic observations creates a much better balance of the circle
    and square classes and thus strengthens the decision boundary and potentially
    makes the pattern easier for a learning algorithm to discover.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_13_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: SMOTE can create six synthetic observations from four original
    minority observations, which reinforces the decision boundary between the two
    classes (circles and squares)'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the SMOTE algorithm’s reliance on nearest neighbors and the use of
    distance functions means that the same data preparation caveats apply as would
    with k-NN. First, the dataset needs to be completely numeric. Second, although
    it is not strictly necessary, it may be a good idea to transform the numeric feature
    values to fall on the same scale so that large ranges do not dominate the selection
    of nearest neighbors. We’ll see this in practice in the section that follows.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Applying the SMOTE algorithm in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several R packages that include implementations of the SMOTE algorithm.
    The `DMwR` package has a `SMOTE()` function that is the subject of many tutorials,
    but at present, is unavailable for recent versions of R.
  prefs: []
  type: TYPE_NORMAL
- en: The `smotefamily` package includes a variety of SMOTE functions and is well
    documented, but has not been updated in several years. Thus, we will use the `smote()`
    function in the `themis` package ([https://themis.tidymodels.org](https://themis.tidymodels.org)),
    which is named after Themis, the Greek goddess of justice that is often depicted
    holding balance scales. This package is both easy to use and well incorporated
    into the tidyverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the basic syntax of the `smote()` function, we’ll begin by piping
    in the `snsdata` dataset and using `gender` as the feature to balance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the result, we use the `table()` function on the dataset, which grew
    from 30,000 rows to 66,162 rows but is now balanced across the three gender categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Although this created a gender balance, because the SMOTE algorithm relies on
    nearest neighbors that are determined by distance calculations, it may be better
    to normalize the data prior to generating the synthetic data. For instance, because
    the `friends` feature ranges from 0 to 830 while the `football` feature ranges
    only from 0 to 15, it is likely that the nearest neighbors will gravitate toward
    those with similar friend counts rather than similar interests. Applying min-max
    normalization can help alleviate these concerns by rescaling all features to have
    a range between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve previously written our own normalization function, which we will implement
    again here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'To return the data back to its original scale, we’ll also need an `unnormalize()`
    function. As defined here, the function takes two parameters: the first is a vector,
    `norm_values`, which stores the values that have been normalized; the second is
    a string with the name of the column that has been normalized. We need this column
    name so that we can obtain the minimum and maximum values for this column from
    the original, unnormalized data in the `snsdata` dataset. The resulting `unnormalized_vals`
    vector uses these min and max values to reverse the normalization, and then the
    values are rounded to integers as they were in the original data, except for the
    `age_imp` feature, which was originally a decimal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full `unnormalize()` function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'With a sequence of pipes, we can apply normalization before using the `smote()`
    function, followed by unnormalization afterward. This uses the dplyr `across()`
    function to normalize and unnormalize the columns where the data type is numeric.
    In the case of the `unnormalize()` function, the syntax is slightly more complex
    due to the use of a lambda, denoted by the tilde (`~`) character, which defines
    a function to be used across the columns where the data type is numeric. The `normalize()`
    function did not require the use of a lambda because it uses only one parameter,
    whereas `unnormalize()` uses two. The `.x` refers to the vector of data in the
    column and is passed as the first parameter, while the `cur_column()` function
    is used to pass the name of the current column as the second parameter. The complete
    sequence of commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, comparing the gender balance before and after SMOTE, we see that
    the categories are now equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Note that we now have over four times as many males and over eight times as
    many records with unknown gender—or, that roughly three synthetic male records
    and seven synthetic unknown gender records have been added for each original record
    with male or unknown gender, respectively. The number of female examples has stayed
    the same. This balanced dataset can now be used with machine learning algorithms,
    keeping in mind that the model will be based mostly on synthetic cases rather
    than “real” examples of the minority classes. Whether or not this results in improved
    performance may vary from project to project, for reasons that are discussed in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Considering whether balanced is always better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it is undeniable that severely imbalanced datasets cause challenges
    for learning algorithms, the best approach for handling the imbalance is quite
    unclear. Some even argue that the best approach is to do nothing at all! The issue
    is whether artificially balancing the dataset can improve the *overall* performance
    of a learning algorithm, or if it is just trading a reduction in specificity for
    an improvement in sensitivity. Because a learning algorithm that has been trained
    on an artificially balanced dataset will someday be deployed on the original,
    imbalanced dataset, it seems that the practice of balancing is simply adjusting
    the learner’s sense of the cost of one type of error versus the other. It is therefore
    counterintuitive to understand how throwing away data could result in a smarter
    model—that is, one that is better able to *truly* distinguish among the outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among those skeptical of artificially balancing the training data, prolific
    biostatistician Frank Harrell has written much on this subject. In a thoughtful
    blog post, he wrote that:'
  prefs: []
  type: TYPE_NORMAL
- en: ”Users of machine classifiers know that a highly imbalanced sample with regard
    to a binary outcome variable Y results in a strange classifier… For this reason
    the odd practice of subsampling the controls is used in an attempt to balance
    the frequencies and get some variation that will lead to sensible looking classifiers
    (users of regression models would never exclude good data to get an answer). Then
    they have to, in some ill-defined way, construct the classifier to make up for
    biasing the sample.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clearly, Harrell does not think balancing the sample is generally a wise approach!
  prefs: []
  type: TYPE_NORMAL
- en: For more of Harrell’s writings on this subject, see [http://www.fharrell.com/post/classification/](http://www.fharrell.com/post/classification/)
    as well as [http://www.fharrell.com/post/class-damage/](http://www.fharrell.com/post/class-damage/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Nina Zumel, author of *Practical Data Science with R*, performed experiments
    to determine whether artificially balancing the dataset improved classification
    performance. After conducting an experiment, she concluded that:'
  prefs: []
  type: TYPE_NORMAL
- en: ”Classification tends to be easier when the classes are nearly balanced… But
    I have always been skeptical of the claim that artificially balancing the classes
    always helps, when the model is to be run on a population with the native class
    prevalences… balancing the classes, or enrichment in general, is of limited value
    if your goal is to apply class labels… [it] is not a good idea for logistic regression
    models.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Much like Frank Harrell, Nina Zumel is also suspicious of the need to artificially
    balance datasets for classification models. Yet, both perspectives are in opposition
    to a body of empirical and anecdotal evidence suggesting that artificially balancing
    a dataset, in fact, does improve the performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: For a full description of Zumel’s experiment on imbalanced data classification,
    see [https://win-vector.com/2015/02/27/does-balancing-classes-improve-classifier-performance/](https://win-vector.com/2015/02/27/does-balancing-classes-improve-classifier-performance/).
  prefs: []
  type: TYPE_NORMAL
- en: What explains this contradictory result? It may have something to do with the
    choice of tool. Statistical learning algorithms, such as regression, may be well
    **calibrated**, meaning that they do a good job estimating the true underlying
    probabilities of an outcome—even for rare outcomes. Many machine learning algorithms,
    such as decision trees and naive Bayes, are decidedly not well calibrated, and
    thus may need a bit of help via artificial balancing in order to produce reasonable
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not a balancing strategy is employed, it is important to use a model
    evaluation approach that reflects the natural imbalance that the model will be
    expected to perform on during deployment. This means favoring cost-aware measures
    like kappa, sensitivity and specificity, or precision and recall, as well as an
    examination of the **receiver operating characteristic** (**ROC**) curve, as discussed
    in *Chapter 10*, *Evaluating Model Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: While it is a good idea to be skeptical of artificially balancing the dataset,
    it may also be worth a shot for the most challenging data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was intended to expose you to several new types of challenging
    data that, although infrequently found in simple teaching examples, are regularly
    encountered in practice. Despite popular adages that tell us that “one can’t have
    too much of a good thing” or that “more is always better,” this is not always
    the case for machine learning algorithms, which may be distracted by irrelevant
    data or have trouble finding the needle in the haystack if overwhelmed by less
    important details. One of the seeming paradoxes of the so-called big data era
    is the fact that more data is simultaneously what makes machine learning possible
    and what makes it challenging; indeed, too much data can even lead to a so-called
    “curse of dimensionality.”
  prefs: []
  type: TYPE_NORMAL
- en: As disappointing as it is to throw away some of the treasure of big data, this
    is sometimes necessary to help the learning algorithm perform as desired. Perhaps
    it is better to think of this as data curation in which the most relevant details
    are brought to the forefront. Dimensionality reduction techniques like feature
    selection and feature extraction are important for algorithms that don’t have
    built-in selection methods, but also provide benefits such as improved computational
    efficiency, which can be a key bottleneck for large datasets. Sparse data also
    requires a helping hand to bring the important details to the attention of the
    learning algorithm, much like the problems of outliers and missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data, which has only been a minor problem in the book so far, presents
    a significant challenge in many real-world datasets. Machine learning practitioners
    often choose the simplest approach to solving the problem—that is, the least work
    necessary to get the model to perform reasonably well—yet machine learning-based
    approaches like multiple imputation are being used to create complete datasets
    in the fields of traditional statistics, biostatistics, and the social sciences.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of imbalanced data may be the most difficult form of challenging
    data to address. Many of the most important machine learning applications involve
    predictions on imbalanced datasets, but there are no easy solutions, only compromises.
    Techniques like over- and undersampling are simple but have significant downsides,
    while more complex techniques like SMOTE are promising but may introduce new problems
    of their own, and the community is divided on the best approach. The most important
    lesson, regardless, is to ensure that the evaluation strategy reflects the conditions
    the model will encounter during deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For example, even if the model is trained on an artificially balanced dataset,
    it should be tested and evaluated using the natural balance of outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: With these data challenges now behind us, the next chapter once again focuses
    on model building, and although data preparation is a substantial component of
    building better learners—after all, garbage in leads to garbage out—there is much
    more we can do to enhance the learning process itself. However, such techniques
    will require more than an off-the-shelf algorithm; they will require creativity
    and determination to maximize the learners’ potential.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
