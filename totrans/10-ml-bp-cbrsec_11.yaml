- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Protecting User Privacy with Differential Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the growing prevalence of machine learning, some concerns have been raised
    about how it could potentially be a risk to user privacy. Prior research has shown
    that even carefully anonymized datasets can be analyzed by attackers and de-anonymized
    using pattern analysis or background knowledge. The core idea that privacy is
    based upon is a user’s right to control the collection, storage, and use of their
    data. Additionally, privacy regulations mandate that no sensitive information
    about a user should be leaked, and they also restrict what user information can
    be used for machine learning tasks such as ad targeting or fraud detection. This
    has led to concerns about user data being used for machine learning, and privacy
    is a crucial topic every data scientist needs to know about.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers differential privacy, a technique used to perform data analysis
    while maintaining user privacy at the same time. Differential privacy aims to
    add noise to data and query results, such that the query accuracy is retained
    but no user data is leaked. This can help with simple analytical tasks as well
    as machine learning. We will start by understanding the fundamentals of privacy
    and what it means for users, and for you as engineers, scientists, and developers.
    We will then also work our way through privacy by design, and the legal implications
    of violating privacy regulations. Finally, we will implement differential privacy
    in machine learning and deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentially private machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentially private deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a better understanding of why privacy
    is important and how it can be incorporated into machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%2010.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Privacy is the ability of an individual or a group of individuals to control
    their personal information and to be able to decide when, how, and to whom that
    information is shared. It involves the right to be free from unwanted or unwarranted
    intrusion into their personal life and the right to maintain the confidentiality
    of personal data.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy is an important aspect of individual autonomy, and it is essential for
    maintaining personal freedom, dignity, and trust in personal relationships. It
    can be protected by various means, such as legal safeguards, technological measures,
    and social norms.
  prefs: []
  type: TYPE_NORMAL
- en: With the increasing use of technology in our daily lives, privacy has become
    an increasingly important concern, particularly in relation to the collection,
    use, and sharing of personal data by organizations and governments. As a result,
    there has been growing interest in developing effective policies and regulations
    to protect individual privacy. In this section, we will cover the fundamental
    concepts of privacy, associated legal measures, and the implications to machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Core elements of data privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fundamental principle behind data privacy is that users should be able
    to answer and have control over the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What data about me is being collected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will that data be used for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who will have access to the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the data be protected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will explore these concepts in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data collection** refers to the process of gathering personal information
    or data from individuals. This data can include any information that can identify
    an individual, such as name, address, phone number, email, date of birth, social
    security number, and so on. Organizations that collect personal data must ensure
    that the data is collected only for specific, legitimate purposes and that individuals
    are made aware of what data is being collected and why. In the case of fraud detection
    or other security applications, data collection may seem overly intrusive (such
    as private messages being collected for detecting abuse, or computer processes
    for detecting malware). Additionally, data collection must comply with applicable
    laws and regulations, and organizations must obtain explicit consent from individuals
    before collecting their data.'
  prefs: []
  type: TYPE_NORMAL
- en: Data use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data use** refers to how the collected data is used by organizations or individuals.
    Organizations must ensure that they use personal data only for the specific, legitimate
    purposes for which it was collected and that they do not use the data for any
    other purposes without the individual’s explicit consent. Additionally, organizations
    must ensure that they do not use personal data in a way that discriminates against
    individuals, such as denying them services or opportunities based on their personal
    characteristics. Data use also includes using the data for machine learning models
    as training – some users may not want their data to be used for training or analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Data access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data access** refers to the control that individuals have over their personal
    data. Individuals have the right to know what data is being collected about them,
    who is collecting it, and why it is being collected. They also have the right
    to access their own personal data and correct any inaccuracies. Additionally,
    individuals have the right to know who their data is being shared with and for
    what purposes. This also includes data sharing with other organizations, applications,
    and services (for example, a shopping website selling your search history with
    a marketing company). Personal data should only be shared with the individual’s
    explicit consent and should only be shared for specific, legitimate purposes.
    Organizations must ensure that they have appropriate security measures in place
    to protect personal data from unauthorized access, disclosure, alteration, or
    destruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Data protection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data protection** refers to the measures taken to protect personal data from
    unauthorized access, disclosure, alteration, or destruction. This includes technical,
    physical, and administrative measures to ensure the security and confidentiality
    of personal data. Organizations must ensure that they have appropriate security
    measures in place to protect personal data, such as encryption, access controls,
    and firewalls. Additionally, organizations must ensure that they have policies
    and procedures in place to detect and respond to security incidents or breaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and the GDPR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While privacy has to do with user consent on data, it is not a purely ethical
    or moral concern – there are legal requirements and regulations that organizations
    must comply with. The **GDPR** stands for the **General Data Protection Regulation**.
    It is a comprehensive data protection law that came into effect on May 25, 2018,
    in the **European** **Union** (**EU**).
  prefs: []
  type: TYPE_NORMAL
- en: The GDPR regulates the processing of personal data of individuals within the
    EU, as well as the export of personal data outside the EU. It gives individuals
    more control over their personal data and requires organizations to be transparent
    about how they collect, use, and store personal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GDPR sets out several key principles, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lawfulness, fairness, and transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Purpose limitation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data minimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage limitation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrity and confidentiality (security)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the GDPR, individuals have the right to access their personal data, correct
    inaccurate data, have their data erased in certain circumstances, and object to
    the processing of their data. Organizations that fail to comply with the GDPR
    can face significant fines and other sanctions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in January 2019, the CNIL (the French data protection authority)
    fined Google €50 million for GDPR violations related to the company’s ad personalization
    practices. The CNIL found that Google had violated the GDPR in two key ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of transparency**: The CNIL found that Google had not provided users
    with clear and easily accessible information about how their personal data was
    being used for ad personalization. The information was spread across several different
    documents, making it difficult for users to understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of valid consent**: The CNIL found that Google had not obtained valid
    consent from users for ad personalization. The consent was not sufficiently informed,
    as users were not clearly told what specific data was being collected and how
    it was being used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CNIL’s investigation was initiated following two complaints filed by privacy
    advocacy groups, **None Of Your Business** (**NOYB**) and La Quadrature du Net,
    on May 25, 2018, the same day that the GDPR came into effect.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the fine, the CNIL ordered Google to make changes to its ad personalization
    practices, including making it easier for users to access and understand information
    about how their data is being used and obtaining valid consent.
  prefs: []
  type: TYPE_NORMAL
- en: The Google fine was significant, as it was the largest GDPR fine at the time
    and demonstrated that regulators were willing to take enforcement action against
    large tech companies for GDPR violations. The fine also underscored the importance
    of transparency and valid consent in data processing under the GDPR.
  prefs: []
  type: TYPE_NORMAL
- en: The GDPR has had a significant impact on how organizations handle personal data,
    not only in the EU but also worldwide, as many companies have had to update their
    policies and practices to comply with the regulation.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy by design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Privacy by design** is an approach to privacy protection that aims to embed
    privacy and data protection into the design and architecture of systems, products,
    and services from the outset. The concept was first introduced by the Information
    and Privacy Commissioner of Ontario, Canada, in the 1990s, and has since been
    adopted as a best practice by privacy regulators and organizations worldwide.'
  prefs: []
  type: TYPE_NORMAL
- en: The privacy-by-design approach involves proactively identifying and addressing
    privacy risks, rather than trying to retrofit privacy protections after a system
    or product has been developed. It requires organizations to consider privacy implications
    at every stage of the design process, from the initial planning and conceptualization
    phase through to implementation and ongoing operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As data scientists and machine learning engineers, if you are designing any
    system at scale, understanding privacy concerns is important. You should follow
    the principles of privacy by design while developing any system. There are five
    key principles that define privacy by design: proactivity, privacy as default,
    privacy embedded into the design, full functionality, and end-to-end security.'
  prefs: []
  type: TYPE_NORMAL
- en: Proactive not reactive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of being proactive not reactive means that organizations should
    anticipate potential privacy risks and take steps to mitigate them before they
    become a problem. This involves conducting **privacy impact assessments** (**PIAs**)
    to identify and address potential privacy issues at the outset of a project. By
    taking a proactive approach to privacy, organizations can reduce the likelihood
    of privacy breaches, protect individual rights, and build trust with their customers.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy as the default setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of privacy as the default setting means that individuals should
    not have to take any action to protect their privacy. This means that privacy
    protection should be built into systems, products, and services by default and
    that individuals should not be required to opt out of sharing their data. By making
    privacy the default setting, individuals are empowered to make informed decisions
    about their personal information, without having to navigate complex privacy settings
    or policies.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy embedded into the design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of embedding privacy into the design means that privacy should
    be a core consideration in the development of systems, products, and services
    from the outset. This involves incorporating privacy features and controls into
    the design of these products and services, such as anonymization, encryption,
    and data minimization. By building privacy into the design of products and services,
    organizations can help ensure that privacy is protected by default, rather than
    as an afterthought.
  prefs: []
  type: TYPE_NORMAL
- en: Full functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of full functionality means that privacy protections should not
    come at the expense of functionality or usability. This means that privacy protections
    should be integrated into systems and products without compromising their performance
    or functionality. By adopting a positive-sum approach to privacy, organizations
    can build trust with their customers and demonstrate that they take privacy seriously.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of end-to-end security means that comprehensive security measures
    should be implemented throughout the entire life cycle of a product or service,
    from development to disposal. This involves implementing a range of security measures,
    such as access controls, encryption, and monitoring, to protect against unauthorized
    access, use, and disclosure of personal information. By taking a comprehensive
    approach to security, organizations can help ensure that personal information
    is protected at every stage of the life cycle and build trust with their customers.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Why did we spend all this time discussing the concepts behind privacy, the
    elements of data privacy, and the GDPR? In security areas (such as fraud, abuse,
    and misinformation), significant types and amounts of user data are collected.
    Some of it might be deemed obtrusive, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Browsers collecting users’ mouse movements and click patterns to detect click
    fraud and bots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security software collecting information on system processes to detect the presence
    of malware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media companies extracting information from private messages and images
    to detect child pornography
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data scientists in the security domain, the ultimate goal is to provide
    maximum user security by building a system with the highest precision and recall.
    However, at the same time, it is important to understand the limitations you will
    face in data collection and use while designing your systems. For example, if
    you are building a system for fraud detection, you may not be able to use cookie
    data in France. Additionally, the GDPR will apply if the data you are collecting
    is from European users.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the jurisdiction, you may not be able to collect certain data,
    or even if it is collected, you may not be able to use it for machine learning
    models. These factors must be taken into consideration as you design your systems
    and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we know that machine learning is based on identifying trends and
    patterns from data. Privacy considerations and regulations will severely limit
    your ability to collect data, extract features, and train models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have been introduced to the fundamentals of privacy, we will look
    at differential privacy, which is considered to be state-of-the-art in privacy
    and is used by many tech giants.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the basics of differential privacy, including
    the mathematical definition and a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: What is differential privacy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Differential privacy** (**DP**) is a framework for preserving the privacy
    of individuals in a dataset when it is used for statistical analysis or machine
    learning. The goal of DP is to ensure that the output of a computation on a dataset
    does not reveal sensitive information about any individual in the dataset. This
    is accomplished by adding controlled noise to the computation in order to mask
    the contribution of any individual data point.'
  prefs: []
  type: TYPE_NORMAL
- en: DP provides a mathematically rigorous definition of privacy protection by quantifying
    the amount of information that an attacker can learn about an individual by observing
    the output of a computation. Specifically, DP requires that the probability of
    observing a particular output from a computation is roughly the same whether a
    particular individual is included in the dataset or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally speaking, let D and D ′  be two datasets that differ by, at most,
    one element, and let f be a function that takes a dataset as input and produces
    an output in some range, *R*. Then, the f function satisfies ε-differential privacy
    for any two datasets D and D ′ :'
  prefs: []
  type: TYPE_NORMAL
- en: Pr[f(D) ∈ S] ≤ e ε . Pr[f(D ′ ) ∈ S]
  prefs: []
  type: TYPE_NORMAL
- en: where S is any subset of *R* and *δ* is a small positive number that accounts
    for the probability of events that have low probability. In other words, the probability
    of the output of the f function on dataset D falling within a set S should be
    very similar to the probability of the output of the f function on dataset D ′ 
    falling within the same set S, up to a multiplicative factor of exp(ε). The smaller
    the value of ε, the stronger the privacy protection, but also the less accurate
    the results. The *δ* parameter is typically set to a very small value, such as
    10-9, to ensure that the overall privacy guarantee is strong.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea behind DP is to add random noise to the computation in a way that
    preserves the statistical properties of the data while obscuring the contribution
    of any individual. The amount of noise added is controlled by a parameter called
    the **privacy budget**, which determines the maximum amount of privacy loss that
    can occur during the computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several mechanisms for achieving differential privacy, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Laplace mechanism**: The Laplace mechanism adds random noise to the output
    of a computation based on the sensitivity of the computation. The amount of noise
    added is proportional to the sensitivity of the computation and inversely proportional
    to the privacy budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exponential mechanism**: The Exponential mechanism is used to select an output
    from a set of possible outputs in a way that minimizes the amount of information
    revealed about any individual. This mechanism selects the output with the highest
    utility score, where the utility score is a measure of how well the output satisfies
    the desired properties of the computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Randomized response**: Randomized response is a mechanism used to obtain
    accurate estimates of binary data while preserving privacy. The mechanism involves
    flipping the value of the data point with a certain probability, which is determined
    by the privacy budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DP has become increasingly important in recent years due to the widespread use
    of data in machine learning and statistical analysis. DP can be used to train
    machine learning models on sensitive data while ensuring that the privacy of individuals
    in the dataset is preserved. It is also used in other applications, such as census
    data and medical research.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy – a real-world example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of differential privacy can be clarified in detail using a practical
    example. Suppose a credit card company has a dataset containing information about
    the transaction amounts and times of its customers, and they want to identify
    potential cases of fraud. However, the credit card company is concerned about
    preserving the privacy of its customers and wants to ensure that the analysis
    cannot be used to identify the transaction amounts or times of any individual
    customer.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, the credit card company can use differential privacy to
    add noise to the analysis. Specifically, they can add random noise to the computed
    statistics in a way that preserves the overall statistical properties of the data
    but makes it difficult to determine the transaction amounts or times of any individual
    customer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the credit card company could use the Laplace mechanism to add
    noise to the computed statistics. Let’s say the credit card company wants to compute
    the total transaction amount for a specific time period, and the sensitivity of
    the computation is *1*, meaning that changing the amount of one transaction can
    change the computed total by, at most, 1 dollar. The credit card company wants
    to achieve a privacy budget of *epsilon = 1*, meaning that the probability of
    observing a particular output from the computation should be roughly the same
    whether a particular customer is included in the dataset or not.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Laplace mechanism with these parameters, the credit card company can
    add noise drawn from a Laplace distribution with a scale parameter of *1/epsilon
    = 1*. This will add random noise to the computed total in a way that preserves
    the overall statistical properties of the data but makes it difficult to determine
    the transaction amounts or times of any individual customer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the computed total transaction amount might be $10,000, but with
    the added noise, it might be reported as $10,100\. This ensures that the analysis
    cannot be used to identify the transaction amounts or times of any individual
    customer with high confidence, while still providing useful information about
    the overall transaction amounts for the specific time period.
  prefs: []
  type: TYPE_NORMAL
- en: However, suppose the credit card company wants to achieve a higher level of
    privacy protection and sets the privacy budget to *epsilon = 10* instead of *epsilon
    = 1*. This means that the added noise will be larger and the analysis will be
    more private, but it will also be less accurate. For example, the computed total
    transaction amount might be reported as $15,000 with *epsilon = 10*, which is
    further from the true value of $10,000.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, differential privacy can be used in the context of fraud detection
    to protect the privacy of individuals in a dataset while still allowing useful
    statistical analysis to be performed. However, the choice of privacy budget (epsilon)
    is important and should be balanced with the level of privacy protection and the
    desired accuracy of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why use differential privacy at all? What benefits will it provide to users?
    And what benefits, if any, will it provide to engineers, researchers, and scientists?
    There are several key benefits that differential privacy provides, including strong
    user privacy guarantees, flexibility in analysis, balance between privacy and
    utility, robustness, and transparency. These are more important in the cybersecurity
    domain than others, as we discussed in [*Chapter 1*](B19327_01.xhtml#_idTextAnchor013),
    *On Cybersecurity and* *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: User privacy guarantees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy provides a rigorous mathematical definition of privacy
    protection that offers strong guarantees of privacy. It ensures that an individual’s
    personal data cannot be distinguished from the data of others in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In a cybersecurity context, differential privacy can be used to protect the
    privacy of user data in security logs. For example, let’s say a security analyst
    is examining a log of user login attempts. Differential privacy can be used to
    protect the privacy of individual users by adding random noise to the log data
    so that it is impossible to determine whether a specific user attempted to log
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy can be applied to a wide range of data analysis techniques,
    including queries, machine learning algorithms, and statistical models. In cybersecurity,
    it can be applied to a variety of security-related data analysis techniques. For
    example, it can be used to protect the privacy of user data in intrusion detection
    systems. It can also be applied to the algorithms used by these systems to detect
    anomalous network activity and to ensure that the privacy of individual users
    is protected.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-utility trade-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy provides a way to balance privacy and utility so that accurate
    statistical analysis can be performed on a dataset while minimizing the risk of
    exposing sensitive information. It can be used to protect the privacy of sensitive
    data in cybersecurity applications while still allowing useful insights to be
    obtained. For example, it can be used to protect the privacy of user data in threat
    intelligence-sharing systems. It can also be used to protect the privacy of individual
    users while still allowing organizations to share information about threats and
    vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy is robust to various types of attacks, including statistical
    attacks and inference attacks. Differential privacy is designed to protect against
    a wide range of attacks, including statistical attacks and inference attacks.
    For example, in a cybersecurity context, differential privacy can be used to protect
    the privacy of user data in forensic investigations. It can also be used to ensure
    that sensitive data cannot be inferred from forensic evidence, even if an attacker
    has access to a large amount of other data.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy provides a way to quantify the amount of privacy protection
    provided by a particular technique, which allows individuals and organizations
    to make informed decisions about the level of privacy they need for their data.
  prefs: []
  type: TYPE_NORMAL
- en: It provides a way to measure the effectiveness of privacy protection techniques,
    which can be useful in making decisions about data protection in cybersecurity.
    For example, it can be used to protect the privacy of user data in threat modeling.
    It can also be used to help organizations understand the level of privacy protection
    they need to protect against various types of threats and to measure the effectiveness
    of their existing privacy protection measures.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at privacy and then differential privacy. Now, let us
    see how it can be practically applied in the context of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how a fraud detection model can incorporate
    differential privacy. We will first look at the library we use to implement differential
    privacy, followed by how a credit card fraud detection machine learning model
    can be made differentially private.
  prefs: []
  type: TYPE_NORMAL
- en: IBM Diffprivlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Diffprivlib` is an open source Python library that provides a range of differential
    privacy tools and algorithms for data analysis. The library is designed to help
    data scientists and developers apply differential privacy techniques to their
    data in a simple and efficient way.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of `Diffprivlib` is its extensive range of differentially
    private mechanisms. These include mechanisms for adding noise to data, such as
    the Gaussian, Laplace, and Exponential mechanisms, as well as more advanced mechanisms,
    such as the hierarchical and subsample mechanisms. The library also includes tools
    for calculating differential privacy parameters, such as sensitivity and privacy
    budget (epsilon), and for evaluating the privacy of a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature of `Diffprivlib` is its ease of use. The library provides
    a simple and intuitive API that allows users to apply differential privacy to
    their data with just a few lines of code. The API is designed to be compatible
    with `scikit-learn`, a popular machine learning library for Python, which makes
    it easy to incorporate differential privacy into existing data analysis workflows.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to its core functionality, `Diffprivlib` includes a number of advanced
    features and tools that can be used to improve the accuracy and efficiency of
    differential privacy applications. For example, the library includes tools for
    generating synthetic datasets that are differentially private, which can be used
    to test and validate differential privacy mechanisms. It also includes tools for
    differential private machine learning, which can be used to build models that
    are both accurate and privacy-preserving.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, `Diffprivlib` provides a powerful set of tools for data privacy that
    can be used in a wide range of applications, from healthcare and finance to social
    media and online advertising. Its extensive range of differentially private mechanisms,
    ease of use, and advanced features make it a valuable resource for anyone looking
    to improve the privacy and security of their data analysis workflows.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will use `Diffprivlib` to train and evaluate differentially
    private machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud detection with differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we know, differential privacy is a framework for preserving the privacy of
    individuals while allowing statistical analysis of their data. Many applications
    today are powered by analysis through machine learning, and hence, the application
    of DP in machine learning has been a field of growing interest and importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply differential privacy to a machine learning technique, we will perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the privacy budget**: The first step is to define the privacy budget,
    which determines the level of privacy protection that will be provided. The privacy
    budget is typically expressed as ε, which is a small positive number. The smaller
    the value of ε, the stronger the privacy protection, but also the less accurate
    the results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add noise to the data**: To ensure differential privacy, noise is added to
    the data before logistic regression is performed. Specifically, random noise is
    added to each data point, so that the noise cancels out when the data is aggregated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: Once the data has been randomized, a machine learning
    model is trained on the randomized data. This model will be less accurate than
    a model trained on the original data, but it will still be useful for making predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: Once the model has been trained, it can be used to
    make predictions on new data. The accuracy of the model will depend on the value
    of ε that was chosen, as well as the size and complexity of the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following sections, we will look at how this can be applied in practice
    to two popular classification models: logistic regression and random forests.'
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As a simulation, we will be using the credit card fraud detection dataset from
    Kaggle. You can use any dataset of your choice. We split the data into training
    and test sets, with 2% reserved for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To print the columns, you can simply run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1\uFEFF0.1 – Dataset columns](img/B19327_10_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Dataset columns
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to use columns `V1` through `V28` and `Amount` as features, and `Class`
    as the label. We then want to split the data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we train a logistic regression model to predict the class of the data.
    Note that this is the vanilla logistic regression model from scikit-learn without
    any differential privacy involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we evaluate the performance of this model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This should print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Great! We have nearly 99.9% accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we fit a differentially private logistic regression model on the same
    data. Here, we set the value of the `epsilon` parameter to `1`. You can set this
    to any value you want, as long as it is not zero (an epsilon of zero indicates
    no differential privacy, and the model will be equivalent to the vanilla one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, evaluate it on the test set as we did with the previous model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Wow – that’s a huge drop! The accuracy on the test set dropped from 99.9% to
    about 64%. This is the utility cost associated with increased privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private random forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a fun experiment, let us try the same with a random forest. The code remains
    almost the same, except both classifiers are switched to random forests. Here’s
    the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the drop in accuracy in random forests is much less pronounced
    and is less than 1%. Therefore, random forests would be a better classifier to
    use in this scenario if both increased privacy and utility are to be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the effect of ϵ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will examine how the accuracy of the classifier on the test set varies
    as we change the value of `epsilon`. For multiple values of `epsilon` from `0`
    to `5`, we will train a differentially private classifier and compute the accuracy
    on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After this block is run, we can plot the scores against the corresponding `epsilon`
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you a plot like this:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1\uFEFF0.2 – Accuracy variation with epsilon for logistic regression](img/B19327_10_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Accuracy variation with epsilon for logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: 'How about the same evaluation for a random forest? Just replace the model instantiated
    with a random forest instead of logistic regression. Here is the complete code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting this gives you the following:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1\uFEFF0.3 – Accuracy variation with epsilon for random forest](img/B19327_10_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Accuracy variation with epsilon for random forest
  prefs: []
  type: TYPE_NORMAL
- en: The graph *appears* volatile – but note that the accuracy is always between
    99.8% and 99.83%. This means that higher values of `epsilon` do not cause a meaningful
    difference in accuracy. This model is better suited for differential privacy than
    the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the sections so far, we covered how differential privacy can be implemented
    in standard machine learning classifiers. In this section, we will cover how it
    can be implemented for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: DP-SGD algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Differentially private stochastic gradient descent** (**DP-SGD**) is a technique
    used in machine learning to train models on sensitive or private data without
    revealing the data itself. The technique is based on the concept of differential
    privacy, which guarantees that an algorithm’s output remains largely unchanged,
    even if an individual’s data is added or removed.'
  prefs: []
  type: TYPE_NORMAL
- en: DP-SGD is a variation of the **stochastic gradient descent** (**SGD**) algorithm,
    which is commonly used for training deep neural networks. In SGD, the algorithm
    updates the model parameters by computing the gradient of the loss function on
    a small randomly selected subset (or “batch”) of the training data. This is done
    iteratively until the algorithm converges with a minimum of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In DP-SGD, the SGD algorithm is modified to incorporate a privacy mechanism.
    Specifically, a small amount of random noise is added to the gradients at each
    iteration, which makes it difficult for an adversary to infer individual data
    points from the output of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of noise added to the gradients is controlled by a parameter called
    the privacy budget ε, which determines the maximum amount of information that
    can be leaked about an individual data point. A smaller value of ε corresponds
    to a stronger privacy guarantee but also reduces the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of noise added to the gradients is calculated using a technique called
    the Laplace mechanism. The Laplace mechanism adds random noise sampled from the
    Laplace distribution, which has a probability density function proportional to
    *exp(-|x|/b)*, where *b* is the scale parameter. The larger the value of *b*,
    the smaller the amount of noise added.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the privacy budget ε is not exceeded over the course of the training
    process, a technique called **moment accountant** is used. Moment accountant estimates
    the cumulative privacy loss over multiple iterations of the algorithm and ensures
    that the privacy budget is not exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: DP-SGD differs from a standard SGD only in the gradient calculation step. First,
    the gradient is calculated for a batch as the partial derivative of the loss with
    respect to the parameter. Then, the gradients are clipped so that they remain
    within a fixed window. Finally, random noise is added to the gradients to form
    the final gradients. This final gradient is used in the parameter update step
    in gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, DP-SGD is a variant of SGD that incorporates a privacy mechanism
    by adding random noise to the gradients at each iteration. The privacy level is
    controlled by a parameter called the privacy budget ε, which determines the amount
    of noise added to the gradients. The Laplace mechanism is used to add the noise,
    and the moment accountant technique is used to ensure that the privacy budget
    is not exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: 'DP-SGD has several advantages over traditional SGD algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy-preserving**: The primary advantage of DP-SGD is that it preserves
    the privacy of individual data points. This is particularly important when dealing
    with sensitive or confidential data, such as medical records or financial data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness to re-identification attacks**: DP-SGD provides robustness to
    re-identification attacks, which attempt to match the output of the algorithm
    to individual data points. By adding random noise to the gradients, DP-SGD makes
    it difficult for an attacker to distinguish between individual data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved fairness**: DP-SGD can also improve the fairness of machine learning
    models by ensuring that the model does not rely too heavily on any individual
    data point. This can help prevent biases in the model and ensure that it performs
    well across different demographic groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: DP-SGD can scale to large datasets and complex models. By
    using SGD, DP-SGD can train models on large datasets by processing small batches
    of data at a time. This allows for efficient use of computing resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy trade-off**: Finally, DP-SGD offers a trade-off between accuracy
    and privacy. By adjusting the privacy budget ε, the user can control the level
    of privacy protection while still achieving a reasonable level of accuracy. This
    makes DP-SGD a flexible and adaptable tool for machine learning applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin, as usual, by implementing the necessary libraries. Apart from
    the usual processing and deep learning libraries, we will be using a new one,
    known as `tensorflow-privacy`. This library provides tools for adding differential
    privacy to TensorFlow models, including an implementation of the TensorFlow privacy
    algorithm for training deep learning models with differential privacy. The library
    also includes tools for measuring the privacy properties of a model, such as its
    `epsilon` value, which quantifies the level of privacy protection provided by
    the differential privacy mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We will now write a function that will load and preprocess our MNIST data. The
    MNIST dataset is a large collection of handwritten digits that is commonly used
    as a benchmark dataset for testing machine learning algorithms, particularly those
    related to image recognition and computer vision. The dataset consists of 60,000
    training images and 10,000 testing images, with each image being a grayscale 28x28
    pixel image of a handwritten digit (0-9).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our function will first load the training and test sets from this data. The
    data is then scaled to 1/255th its value, followed by reshaping into the image
    dimensions. The labels, which are integers from `0` to `9`, are converted into
    one-hot vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will define a function that creates our classification model. In this
    case, we will be using CNNs. We have seen and used CNNs in earlier chapters; however,
    we will provide a brief recap here.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN is a type of neural network that is specifically designed for image recognition
    and computer vision tasks. CNNs are highly effective at processing and analyzing
    images due to their ability to detect local patterns and features within an image.
    At a high level, a CNN consists of a series of layers, including convolutional
    layers, pooling layers, and fully connected layers. In the convolutional layers,
    the network learns to detect local features and patterns in the input image by
    applying a set of filters to the image. The pooling layers then downsample the
    feature maps obtained from the convolutional layers to reduce the size of the
    input and make the network more computationally efficient. Finally, the fully
    connected layers process the output of the convolutional and pooling layers to
    generate a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation of CNNs is the use of convolutional layers, which allow the
    network to learn spatially invariant features from the input image. This is achieved
    by sharing weights across different parts of the image, which allows the network
    to detect the same pattern regardless of its position within the image. CNNs have
    achieved state-of-the-art performance in a wide range of computer vision tasks,
    including image classification, object detection, and semantic segmentation. They
    have been used in many real-world applications, such as self-driving cars, medical
    image analysis, and facial recognition systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model creation function initializes an empty list and adds layers to it
    one by one to build up the CNN structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the core functions needed have been defined. Now, we use the data loader
    we implemented earlier to load the training and test data and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will set some hyperparameters that will be used by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NUM_EPOCHS`: This defines the number of epochs (one epoch is a full pass over
    the training data) that the model will undergo while training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BATCH_SIZE`: This defines the number of data instances that will be processed
    in one batch. Processing here involves running the data through the network, calculating
    the predicted labels, the loss, and the gradients, and then updating the weights
    by gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MICRO_BATCHES`: The dataset is divided into smaller units called microbatches,
    with each microbatch containing a single training example by default. This allows
    us to clip gradients for each individual example, which reduces the negative impact
    of clipping on the gradient signal and maximizes the model’s utility. However,
    increasing the size of microbatches can decrease computational overhead, but it
    involves clipping the average gradient across multiple examples. It’s important
    to note that the total number of training examples consumed in a batch remains
    constant, regardless of the microbatch size. To ensure proper division, the number
    of microbatches should evenly divide the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L2_NORM_CLIP`: This refers to the maximum L2-norm that is allowed for the
    gradient of the loss function with respect to the model parameters. During training,
    the gradient computed on a minibatch of data is clipped to ensure that its L2-norm
    does not exceed the `L2_NORM_CLIP` value. This clipping operation is an essential
    step in the DP-SGD algorithm because it helps to bind the sensitivity of the gradient
    with respect to the input data. A higher value can lead to better accuracy but
    may decrease privacy guarantees, while a lower value can provide stronger privacy
    guarantees but may result in slower convergence and lower accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NOISE_MULTIPLIER`: This controls the amount of noise that is added to the
    gradient updates during training to provide privacy guarantees. In DP-SGD, each
    gradient update is perturbed by a random noise vector to mask the contribution
    of individual training examples to the gradient. A higher value increases the
    amount of noise that is added to the gradient, which in turn provides stronger
    privacy guarantees but can decrease the accuracy of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE`: This is the learning rate, and as seen in earlier chapters, controls
    the degree to which gradients are updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the following values we set for these hyperparameters have been derived
    through experimentation. There is no sure way of knowing what the best parameters
    are. In fact, you are encouraged to experiment with different values and examine
    how they affect the privacy and accuracy guarantees of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will initialize the model using the function we defined earlier, and print
    out a summary to verify the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1\uFEFF0.4 – Model structure](img/B19327_10_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Model structure
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will define the loss and optimizer used for training. While the loss
    is categorical cross-entropy (as expected for a multi-class classification problem),
    we will not use the standard Adam optimizer here but will use a specialized optimizer
    for differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '`DPKerasSGDOptimizer` is a class in the TensorFlow `Privacy` library that provides
    an implementation of the SGD optimizer with differential privacy guarantees. It
    uses the DP-SGD algorithm, which adds random noise to the gradients computed during
    each step of the SGD optimization process. The amount of noise added is controlled
    by two parameters: the noise multiplier and the clipping norm. The noise multiplier
    determines the amount of noise added to the gradients, while the clipping norm
    limits the magnitude of the gradients to prevent large updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will build the model and start the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you the training loop as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1\uFEFF0.5 – Training loop](img/B19327_10_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Training loop
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is now trained. The `compute_dp_sgd_privacy` function is useful for
    analyzing the privacy properties of a differentially private machine learning
    model trained using the DP-SGD algorithm. By computing the privacy budget, we
    can ensure that the model satisfies a desired level of privacy protection and
    can adjust the parameters of the algorithm accordingly. The function uses the
    moment accountant method to estimate the privacy budget of the DP-SGD algorithm.
    This method calculates an upper bound on the privacy budget by analyzing the moments
    of the privacy loss distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And this should show you the following privacy measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Differential privacy in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the importance and utility of differential privacy, technology
    giants have started implementing it in their products. Two popular examples are
    Apple and Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Apple routinely collects users’ typing history and behavior locally – this helps
    power features such as autocorrect and automatic completion of messages. However,
    it also invites the risk of collecting personal and sensitive information. Users
    may talk about medical issues, financial details, or other information that they
    want to protect, and hence using it directly would be a privacy violation. Differential
    privacy comes to the rescue here. Apple implements **local differential privacy**,
    which guarantees that it is difficult to determine whether a certain user contributed
    to the computation of an aggregate feature by adding noise to the data before
    it is shared with Apple for computation and processing.
  prefs: []
  type: TYPE_NORMAL
- en: Another tech giant that has been a forerunner in differential privacy is Microsoft.
    The Windows operating system needs to collect telemetry in order to understand
    usage patterns, diagnose faults, and detect malicious software. Microsoft applies
    differential privacy to the features it collects by adding noise before they are
    aggregated and sent to Microsoft. Microsoft Office has a *Suggested Replies* feature,
    which enables auto-completion and response suggestions in Outlook and Word. As
    there might be sensitive data in the emails/documents the model is trained on,
    Microsoft uses differential privacy in order to ensure that the model doesn’t
    learn from or leak any such information.
  prefs: []
  type: TYPE_NORMAL
- en: These algorithms often take longer to train and often require tuning for accuracy,
    but according to Microsoft, this effort can be worth it due to the more rigorous
    privacy guarantees that differential privacy enables.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, user privacy has grown as a field of importance. Users are
    to have full control over their data, including its collection, storage, and use.
    This can be a hindrance to machine learning, especially in the cybersecurity domain,
    where increased privacy causing a decreased utility can lead to fraud, network
    attacks, data theft, or abuse.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter first covered the fundamental aspects of privacy – what it entails,
    why it is important, the legal requirements surrounding it, and how it can be
    incorporated into practice through the privacy-by-design framework. We then covered
    differential privacy, a statistical technique to add noise to data so that analysis
    can be performed while maintaining user privacy. Finally, we looked at how differential
    privacy can be applied to machine learning in the domain of credit card fraud
    detection, as well as deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: This completes our journey into building machine learning solutions for cybersecurity!
    Now, it is time to introspect and develop more skills in the domain. The next
    chapter, which contains a series of interview-related questions and additional
    blueprints, will help you do just that.
  prefs: []
  type: TYPE_NORMAL
