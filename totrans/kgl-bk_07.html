<html><head></head><body>
  <div id="_idContainer168" class="Basic-Text-Frame">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-63" class="chapterTitle">Competition Tasks and Metrics</h1>
    <p class="normal">In a competition, you start by examining the target metric. Understanding how your model’s errors are evaluated is key for scoring highly in every competition. When your predictions are submitted to the Kaggle platform, they are compared to a ground truth based on the target metric.</p>
    <p class="normal">For instance, in the <em class="italic">Titanic</em> competition (<a href="https://www.kaggle.com/c/titanic/"><span class="url">https://www.kaggle.com/c/titanic/</span></a>), all your submissions are evaluated based on <em class="italic">accuracy</em>, the percentage of surviving passengers you correctly predict. The organizers decided upon this metric because the aim of the competition is to find a model that estimates the probability of survival of a passenger under similar circumstances. In another knowledge competition, <em class="italic">House Prices - Advanced Regression Techniques</em> (<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"><span class="url">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</span></a>), your work will be evaluated based on an <em class="italic">average difference</em> between your prediction and the ground truth. This involves computing the logarithm, squaring, and taking the square root, because the model is expected to be able to quantify as correctly as possible the order of the price of a house on sale.</p>
    <p class="normal">In real-world data science, target metrics are also key for the success of your project, though there are certainly differences between the real world and a Kaggle competition. We could easily summarize by saying that there are more complexities in the real world. In real-world projects, you will often have not just one but multiple metrics that your model will be evaluated against. Frequently, some of the evaluation metrics won’t even be related to how your predictions perform against the ground truth you are using for testing. For instance, the domain of knowledge you are working in, the scope of the project, the number of features considered by your model, the overall memory usage, any requirement for special hardware (such as a GPU, for instance), the latency of the prediction process, the complexity of the predicting model, and many other aspects may end up counting more than the mere predictive performance. </p>
    <p class="normal">Real-world problems are indeed dominated by business and tech infrastructure concerns much more than you may imagine before being involved in any of them.</p>
    <p class="normal">Yet you cannot escape the fact that the basic principle at the core of both real-world projects and Kaggle competitions is the same. Your work will be evaluated according to some criteria, and understanding the details of such criteria, optimizing the fit of your model in a smart way, or selecting its parameters according to the criteria will bring you success. If you can learn more about how model evaluation occurs in Kaggle, your real-world data science job will also benefit from it.</p>
    <p class="normal">In this chapter, we are going to detail how evaluation metrics for certain kinds of problems strongly influence the way you can operate when building your model solution in a data science competition. We also address the variety of metrics available in Kaggle competitions to give you an idea of what matters most and, in the margins, we discuss the different effects of metrics on predictive performance and how to correctly translate them into your projects. We will cover the following topics:</p>
    <ul>
      <li class="bulletList">Evaluation metrics and objective functions</li>
      <li class="bulletList">Basic types of tasks: regression, classification, and ordinal</li>
      <li class="bulletList">The Meta Kaggle dataset</li>
      <li class="bulletList">Handling never-before-seen metrics</li>
      <li class="bulletList">Metrics for regression (standard and ordinal)</li>
      <li class="bulletList">Metrics for binary classification (label prediction and probability)</li>
      <li class="bulletList">Metrics for multi-class classification</li>
      <li class="bulletList">Metrics for object detection problems</li>
      <li class="bulletList">Metrics for multi-label classification and recommendation problems</li>
      <li class="bulletList">Optimizing evaluation metrics</li>
    </ul>
    <h1 id="_idParaDest-64" class="heading-1">Evaluation metrics and objective functions</h1>
    <p class="normal">In a Kaggle competition, you can<a id="_idIndexMarker242"/> find the evaluation metric in the left menu on the <strong class="screenText">Overview</strong> page of the competition. By selecting the <strong class="screenText">Evaluation</strong> tab, you will get details about the evaluation metric. Sometimes you will find the metric formula, the code to reproduce it, and some discussion of the metric. On the same page, you will also get an explanation about the submission file format, providing you with the header of the file and a few example rows.</p>
    <p class="normal">The association between the evaluation metric and the submission file is important, because you have to consider that the metric works essentially after you have trained your model and produced some predictions. Consequently, as a first step, you will have to think about the difference <a id="_idIndexMarker243"/>between an <strong class="keyWord">evaluation metric</strong> and an <strong class="keyWord">objective function</strong>.</p>
    <p class="normal">Boiling everything down to the basics, an objective function serves your model during training because it is involved in the process of error minimization (or score maximization, depending on the problem). In contrast, an evaluation metric serves your model <em class="italic">after</em> it has been trained by providing a score. Therefore, it cannot influence how the model fits the data, but does influence it in an indirect way: by helping you to select the most well-performing hyperparameter settings within a model, and the best models among competing ones. Before proceeding with the rest of the chapter, which will show you how this can affect a Kaggle competition and why the analysis of the Kaggle evaluation metric should be your first act in a competition, let’s first discuss some terminology that you may encounter in the discussion forums.</p>
    <p class="normal">You will often hear talk about objective functions, cost functions, and loss functions, sometimes interchangeably. They are not exactly the same thing, however, and we explain the distinction here:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">loss function</strong> is a function that is defined <a id="_idIndexMarker244"/>on a single data point, and, considering the prediction of the model and the ground truth for the data point, computes a penalty.</li>
      <li class="bulletList">A <strong class="keyWord">cost function</strong> takes into <a id="_idIndexMarker245"/>account the whole dataset used for training (or a batch from it), computing a sum or average over the loss penalties of its data points. It can comprise further constraints, such as the L1 or L2 penalties, for instance. The cost function directly affects how the training happens.</li>
      <li class="bulletList">An <strong class="keyWord">objective function</strong> is the <a id="_idIndexMarker246"/>most general (and safe-to-use) term related to the scope of optimization during machine learning training: it comprises cost functions, but it is not limited to them. An objective function, in fact, can also take into account goals that are not related to the target: for instance, requiring sparse coefficients of the estimated model or a minimization of the coefficients’ values, such as in L1 and L2 regularizations. Moreover, whereas loss and cost functions imply an optimization based on minimization, an objective function is neutral and can imply either a maximization or a minimization activity performed by the learning algorithm.</li>
    </ul>
    <p class="normal">Likewise, when it comes to evaluation metrics, you’ll hear about scoring functions and error functions. Distinguishing<a id="_idIndexMarker247"/> between them is easy: a <strong class="keyWord">scoring function</strong> suggests better prediction results if scores from the function are higher, implying a maximization process. </p>
    <p class="normal">An <strong class="keyWord">error function</strong> instead suggests <a id="_idIndexMarker248"/>better predictions if smaller error quantities are reported by the function, implying a minimization process.</p>
    <h1 id="_idParaDest-65" class="heading-1">Basic types of tasks</h1>
    <p class="normal">Not all objective<a id="_idIndexMarker249"/> functions are suitable for all problems. From a general point of view, you’ll find two<a id="_idIndexMarker250"/> kinds of problems in Kaggle competitions: <strong class="keyWord">regression</strong> tasks and <a id="_idIndexMarker251"/><strong class="keyWord">classification</strong> tasks. Recently, there have <a id="_idIndexMarker252"/>also been <strong class="keyWord">reinforcement learning</strong> (<strong class="keyWord">RL</strong>) tasks, but RL doesn’t use metrics for evaluation; instead, it relies on a ranking derived from direct match-ups against other competitors whose solutions are assumed to be as well-performing as yours (performing better in this match-up than your peers will raise your ranking, performing worse will lower it). Since RL doesn’t use metrics, we will keep on referring to the regression-classification dichotomy, though <strong class="keyWord">ordinal</strong> tasks, where you <a id="_idIndexMarker253"/>predict ordered labels represented by integer numbers, may elude such categorization and can be dealt with successfully either using a regression or classification approach.</p>
    <h2 id="_idParaDest-66" class="heading-2">Regression</h2>
    <p class="normal"><strong class="keyWord">Regression</strong> requires you to build a model that <a id="_idIndexMarker254"/>can predict a real number; often a positive number, but there have been examples of negative number prediction too. A classic example<a id="_idIndexMarker255"/> of a regression problem is <em class="italic">House Prices - Advanced Regression Techniques</em>, because you have to<a id="_idIndexMarker256"/> guess the value of a house. The evaluation of a regression task involves computing a distance between your predictions and the values of the ground truth. This difference can be evaluated in different ways, for instance by squaring it in order to punish larger errors, or by applying a log to it in order to penalize predictions of the wrong scale.</p>
    <h2 id="_idParaDest-67" class="heading-2">Classification</h2>
    <p class="normal">When facing a <strong class="keyWord">classification</strong> task on <a id="_idIndexMarker257"/>Kaggle, there are more nuances to take into account. The classification, in fact, could be <strong class="keyWord">binary</strong>, <strong class="keyWord">multi-class</strong>, or <strong class="keyWord">multi-label</strong>.</p>
    <p class="normal">In <strong class="keyWord">binary</strong> problems, you have to<a id="_idIndexMarker258"/> guess if an example should be classified or not into a specific class (usually called the <em class="italic">positive</em> class and compared to the <em class="italic">negative</em> one). Here, the evaluation<a id="_idIndexMarker259"/> could involve the straightforward prediction of the class ownership itself, or an estimation of the probability of such ownership. A typical example is the <em class="italic">Titanic</em> competition, where you have to guess a binary outcome: survival or not-survival. In this case, the requirement of the competition is just the prediction, but in many cases, it is necessary to provide a probability because in certain fields, especially for medical applications, it is necessary to rank positive predictions across different options and situations in order to make the best decision.</p>
    <p class="normal">Though counting the exact number of correct matches in a binary classification may seem a valid approach, this won’t actually work well when there is an imbalance, that is, a different number of examples, between the positive and the negative class. Classification based on an imbalanced distribution of classes requires evaluation metrics that take the imbalance into account, if you want to correctly track improvements on your model.</p>
    <p class="normal">When you have more<a id="_idIndexMarker260"/> than two classes, you have a <strong class="keyWord">multi-class</strong> prediction problem. This also requires the use of suitable functions for evaluation, since it is necessary to keep track of the overall performance of the model, but also to ensure that the performance across the classes is comparable (for instance, your model could underperform with respect to certain classes). Here, each case can be in one class exclusively, and<a id="_idIndexMarker261"/> not in any others. A good example is <em class="italic">Leaf Classification</em> (<a href="https://www.kaggle.com/c/leaf-classification"><span class="url">https://www.kaggle.com/c/leaf-classification</span></a>), where each image of a leaf specimen has to be associated with the correct plant species.</p>
    <p class="normal">Finally, when your class predictions are not exclusive and you can predict multiple class ownership for each example, you have <a id="_idIndexMarker262"/>a <strong class="keyWord">multi-label</strong> problem that requires further evaluations in order to control whether your model is predicting the correct classes, as well as the correct number and mix of classes. For instance, in <em class="italic">Greek Media Monitoring Multilabel Classification (WISE 2014)</em> (<a href="https://www.kaggle.com/c/wise-2014"><span class="url">https://www.kaggle.com/c/wise-2014</span></a>), you had to associate each article with all the topics it deals with.</p>
    <h2 id="_idParaDest-68" class="heading-2">Ordinal</h2>
    <p class="normal">In a problem involving a <a id="_idIndexMarker263"/>prediction on an ordinal scale, you have to guess integer numeric labels, which are naturally ordered. As an example, the magnitude of an earthquake is on an ordinal scale. In addition, data from marketing research questionnaires is often recorded on ordinal scales (for instance, consumers’ preferences or opinion agreement). Since <a id="_idIndexMarker264"/>an ordinal scale is made of ordered values, ordinal tasks can be considered somewhat halfway between regression and classification, and you can solve them in both ways.</p>
    <p class="normal">The most common way is to treat your <a id="_idIndexMarker265"/>ordinal task as a <strong class="keyWord">multi-class</strong> problem. In this case, you will get a prediction of an integer value (the class label) but the prediction will not take into account that the classes have a certain order. You can get a feeling that there is something wrong with approaching the problem as a multi-class problem if you look at the prediction probability for the classes. Often, probabilities will be distributed across the entire range of possible values, depicting a multi-modal and often asymmetric distribution (whereas you should expect a Gaussian distribution around the maximum probability class).</p>
    <p class="normal">The other way to solve the ordinal <a id="_idIndexMarker266"/>prediction problem is to treat it as a <strong class="keyWord">regression</strong> problem and then post-process your result. In this way, the order among classes will be taken into consideration, though the prediction output won’t be immediately useful for scoring on the evaluation metric. In fact, in a regression you get a float number as an output, not an integer representing an ordinal class; moreover, the result will include the full range of values between the integers of your ordinal distribution and possibly also values outside of it. Cropping the output values and casting them into integers by unit rounding may do the trick, but this might lead to inaccuracies requiring some more sophisticated post-processing (we’ll discuss more on this later in the chapter).</p>
    <p class="normal">Now, you may be wondering what kind of evaluation you should master in order to succeed in Kaggle. Clearly, you always have to master the evaluation metric of the competition you have taken on. However, some metrics are more common than others, which is information you can use to your advantage. What are the most common metrics? How can we figure out where to look for insights in competitions that have used similar evaluation metrics? The answer is to consult the Meta Kaggle dataset.</p>
    <h1 id="_idParaDest-69" class="heading-1">The Meta Kaggle dataset</h1>
    <p class="normal">The Meta Kaggle dataset (<a href="https://www.kaggle.com/kaggle/meta-kaggle"><span class="url">https://www.kaggle.com/kaggle/meta-kaggle</span></a>) is a collection of rich data about Kaggle’s community and activity, published by Kaggle itself as a public dataset. It contains<a id="_idIndexMarker267"/> CSV tables filled with public activity<a id="_idIndexMarker268"/> from Competitions, Datasets, Notebooks, and Discussions. All you have to do is to start a Kaggle Notebook (as you saw in <em class="chapterRef">Chapters 2</em> and <em class="chapterRef">3</em>), add to it the Meta Kaggle dataset, and start analyzing the data. The CSV tables are updated daily, so you’ll have to refresh your analysis often, but that’s worth it given the insights you can extract.</p>
    <p class="normal">We will sometimes refer to the Meta Kaggle dataset in this book, both as inspiration for many interesting examples of the dynamics in a competition and as a way to pick up useful examples for your learning and competition strategies. Here, we are going to use it in order to figure out what evaluation metrics have been used most frequently for competitions in the last seven years. By looking at the most common ones in this chapter, you’ll be able to start any competition from solid ground and then refine your knowledge of the metric, picking up competition-specific nuances using the discussion you find in the forums.</p>
    <p class="normal">Below, we introduce the code necessary to produce a data table of metrics and their counts per year. It is designed to run directly on the Kaggle platform:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
comps = pd.read_csv(<span class="hljs-string">"/kaggle/input/meta-kaggle/Competitions.csv"</span>)
evaluation = [<span class="hljs-string">'EvaluationAlgorithmAbbreviation'</span>,
              <span class="hljs-string">'EvaluationAlgorithmName'</span>,
              <span class="hljs-string">'EvaluationAlgorithmDescription'</span>,]
compt = [<span class="hljs-string">'Title'</span>, <span class="hljs-string">'EnabledDate'</span>, <span class="hljs-string">'HostSegmentTitle'</span>]
df = comps[compt + evaluation].copy()
df[<span class="hljs-string">'year'</span>] = pd.to_datetime(df.EnabledDate).dt.year.values
df[<span class="hljs-string">'comps'</span>] = <span class="hljs-number">1</span>
time_select = df.year &gt;= <span class="hljs-number">2015</span>
competition_type_select = df.HostSegmentTitle.isin(
						[<span class="hljs-string">'Featured'</span>, <span class="hljs-string">'Research'</span>])
pd.pivot_table(df[time_select&amp;competition_type_select],
               values=<span class="hljs-string">'comps'</span>,
               index=[<span class="hljs-string">'EvaluationAlgorithmAbbreviation'</span>],
               columns=[<span class="hljs-string">'year'</span>],
               fill_value=<span class="hljs-number">0.0</span>,
               aggfunc=np.<span class="hljs-built_in">sum</span>,
               margins=<span class="hljs-literal">True</span>
              ).sort_values(
                by=(<span class="hljs-string">'All'</span>), ascending=<span class="hljs-literal">False</span>).iloc[<span class="hljs-number">1</span>:,:].head(<span class="hljs-number">20</span>)
</code></pre>
    <p class="normal">In this code, we read in the CSV table<a id="_idIndexMarker269"/> containing the data relating to the competitions. We focus on the columns representing the evaluation and the columns informing us about the competition name, start date, and type. We limit the rows to those competitions held since 2015 and that are of the Featured or Research type (which are the most common ones). We complete the analysis by creating a pandas pivot table, combining the evaluation algorithm with the year, and counting the number of competitions using it. We just display the top 20 algorithms.</p>
    <p class="normal">Here is the resulting table (at the time of writing):</p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">year</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2015</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2016</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2017</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2018</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2019</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2020</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">2021</strong></p>
          </td>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">Tot</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Evaluation Algorithm</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">AUC</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">17</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">LogLoss</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">16</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MAP@{K}</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">10</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">CategorizationAccuracy</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">8</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MulticlassLoss</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">8</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">RMSLE</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">8</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">QuadraticWeightedKappa</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">7</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MeanFScoreBeta</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">7</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MeanBestErrorAtK</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">6</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MCRMSLE</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">6</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MCAUC</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">RMSE</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Dice</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">GoogleGlobalAP</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MacroFScore</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Score</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">CRPS</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">OpenImagesObjectDetectionAP</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">MeanFScore</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">RSNAObjectDetectionAP</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Using the same variables we just instantiated in <a id="_idIndexMarker270"/>order to generate the table, you can also check the data to find the competitions where the metric of your choice has been adopted:</p>
    <pre class="programlisting code"><code class="hljs-code">metric = <span class="hljs-string">'AUC'</span>
metric_select = df[<span class="hljs-string">'EvaluationAlgorithmAbbreviation'</span>]==metric
<span class="hljs-built_in">print</span>(df[time_select&amp;competition_type_select&amp;metric_select]
        [[<span class="hljs-string">'</span><span class="hljs-string">Title'</span>, <span class="hljs-string">'year'</span>]])
</code></pre>
    <p class="normal">In the above snippet, we decided to represent the competitions that have been using the AUC metric. You just have to change the string representing the chosen metric and the resulting list will be updated accordingly.</p>
    <p class="normal">Coming back to the table generated, we can examine the most popular evaluation metrics used in competitions hosted on Kaggle:</p>
    <ul>
      <li class="bulletList">The two top metrics are closely related to each other and to binary probability classification problems. The <strong class="keyWord">AUC</strong> metric helps to measure if your model’s predicted probabilities tend to <a id="_idIndexMarker271"/>predict positive cases with high probabilities, and the <strong class="keyWord">Log Loss</strong> helps to measure how far your predicted probabilities <a id="_idIndexMarker272"/>are from the ground truth (and as you optimize for Log Loss, you also optimize for the AUC metric).</li>
      <li class="bulletList">In 3<sup class="superscript">rd</sup> position, we find <strong class="keyWord">MAP@{K}</strong>, which is a common metric in recommender systems and search engines. In Kaggle competitions, this metric has been used mostly for information retrieval evaluations, such as in the <em class="italic">Humpback Whale Identification</em> competition (<a href="https://www.kaggle.com/c/humpback-whale-identification"><span class="url">https://www.kaggle.com/c/humpback-whale-identification</span></a>), where you have to precisely identify a whale and you have five possible guesses. Another example of MAP@{K} usage is in the <em class="italic">Quick, Draw! Doodle Recognition Challenge</em> (<a href="https://www.kaggle.com/c/quickdraw-doodle-recognition/"><span class="url">https://www.kaggle.com/c/quickdraw-doodle-recognition/</span></a>), where your goal is to guess the content of a drawn sketch and you are allowed three attempts. In essence, when MAP@{K} is the evaluation metric, you can score not just if you can guess correctly, but also if your correct guess is among a certain number (the “K” in the name of the function) of other incorrect predictions.</li>
      <li class="bulletList">Only in 6<sup class="superscript">th</sup> position can we find a regression metric, the <strong class="keyWord">RMSLE</strong>, or <strong class="keyWord">Root Mean Squared Logarithmic Error</strong>, and in 7<sup class="superscript">th</sup> place the<a id="_idIndexMarker273"/> <strong class="keyWord">Quadratic Weighted Kappa</strong>, a metric particularly useful for estimating model performance on problems that involve guessing a progressive integer number (an ordinal scale problem).</li>
    </ul>
    <p class="normal">As you skim through the list of top metrics, you will<a id="_idIndexMarker274"/> keep on finding metrics that are commonly discussed in machine learning textbooks. In the next few sections, after first discussing what to do when you meet a never-before-seen metric (something that happens in Kaggle competitions more frequently than you may expect), we will revise some of the most common metrics found in regression and classification competitions.</p>
    <h1 id="_idParaDest-70" class="heading-1">Handling never-before-seen metrics</h1>
    <p class="normal">Before proceeding, we have to <a id="_idIndexMarker275"/>consider that the top 20 table doesn’t cover all the metrics used in competitions. We should be aware that there are metrics that have only been used once in recent years. </p>
    <p class="normal">Let’s keep on using the results from the previous code to find out what they are:</p>
    <pre class="programlisting code"><code class="hljs-code">counts = (df[time_select&amp;competition_type_select]
          .groupby(<span class="hljs-string">'EvaluationAlgorithmAbbreviation'</span>))
total_comps_per_year = (df[time_select&amp;competition_type_select]
                        .groupby(<span class="hljs-string">'year'</span>).<span class="hljs-built_in">sum</span>())
single_metrics_per_year = (counts.<span class="hljs-built_in">sum</span>()[counts.<span class="hljs-built_in">sum</span>().comps==<span class="hljs-number">1</span>]
                           .groupby(<span class="hljs-string">'year'</span>).<span class="hljs-built_in">sum</span>())
single_metrics_per_year
table = (total_comps_per_year.rename(columns={<span class="hljs-string">'comps'</span>: <span class="hljs-string">'n_comps'</span>})
         .join(single_metrics_per_year / total_comps_per_year)
         .rename(columns={<span class="hljs-string">'comps'</span>: <span class="hljs-string">'pct_comps'</span>}))
<span class="hljs-built_in">print</span>(table)
</code></pre>
    <p class="normal">As a result, we get the following table showing, for each year, how many competitions used a metric that has never been used afterward (<code class="inlineCode">n_comps</code>), and the proportion of these competitions per year (<code class="inlineCode">pct_comps</code>):</p>
    <pre class="programlisting con"><code class="hljs-con">      n_comps pct_comps
year                   
2015       28  0.179
2016       19  0.158
2017       34  0.177
2018       35  0.229
2019       36  0.278
2020       43  0.302
2021        8  0.250
</code></pre>
    <p class="normal">Observing the relative share of <a id="_idIndexMarker276"/>competitions with a never-to-be-seen-afterward metric, we immediately notice how it is growing year by year and that it reached the 25%-30% level in recent years, implying that typically one competition out of every three or four requires you to study and understand a metric from scratch.</p>
    <p class="normal">You can get the list of such metrics that have occurred in the past with a simple code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(counts.<span class="hljs-built_in">sum</span>()[counts.<span class="hljs-built_in">sum</span>().comps==<span class="hljs-number">1</span>].index.values)
</code></pre>
    <p class="normal">By executing the code, you will get a list similar to this one:</p>
    <pre class="programlisting con"><code class="hljs-con">['AHD@{Type}' 'CVPRAutoDrivingAveragePrecision' 'CernWeightedAuc'
'FScore_1' 'GroupMeanLogMAE' 'ImageNetObjectLocalization'
'IndoorLocalization'  'IntersectionOverUnionObjectSegmentationBeta'
'IntersectionOverUnionObjectSegmentationWithClassification'
'IntersectionOverUnionObjectSegmentationWithF1' 'Jaccard'
'JaccardDSTLParallel' 'JigsawBiasAUC' 'LaplaceLogLikelihood'
'LevenshteinMean' 'Lyft3DObjectDetectionAP' 'M5_WRMSSE' 'MASpearmanR' 'MCRMSE' 'MCSpearmanR' 'MWCRMSE' 'MeanColumnwiseLogLoss' 'MulticlassLossOld' 'NDCG@{K}' 'NQMicroF1' 'NWRMSLE' 'PKUAutoDrivingAP' 'R2Score' 'RValue' 'RootMeanSquarePercentageError' 'SIIMDice' 'SMAPE' 'SantaResident' 'SantaRideShare' 'SantaWorkshopSchedule2019' 'TrackML'
 'TravelingSanta2' 'TwoSigmaNews' 'WeightedAUC' 'WeightedMulticlassLoss' 'WeightedPinballLoss' 'WeightedRowwisePinballLoss' 'YT8M_MeanAveragePrecisionAtK' 'ZillowMAE' 'football' 'halite' 'mab']
</code></pre>
    <p class="normal">By close inspection, you can find many metrics relating to deep learning and reinforcement learning competitions.</p>
    <p class="normal">What do you do when you meet a <a id="_idIndexMarker277"/>metric that has never been used before? Of course, you can rely on the discussions in the Kaggle discussion forums, where you can always find good inspiration and many Kagglers who will help you. However, if you want to build up your own knowledge about the metric, aside from Googling it, we advise that you try to experiment with it by coding the evaluation function by yourself, even in an imperfect way, and trying to simulate how the metric reacts to different types of error produced by the model. You could also directly test how it functions on a sample from the competition training data or synthetic data that you have prepared.</p>
    <p class="normal">We can quote a few examples of this approach as used by Kagglers:</p>
    <ul>
      <li class="bulletList"><em class="italic">Carlo Lepelaars</em> with Spearman’s Rho: <a href="https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho"><span class="url">https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho</span></a></li>
      <li class="bulletList">Carlo Lepelaars with Quadratic Weighted Kappa: <a href="https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa"><span class="url">https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa</span></a></li>
      <li class="bulletList"><em class="italic">Rohan Rao</em> with Laplace Log Likelihood: <a href="https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood"><span class="url">https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood</span></a></li>
    </ul>
    <p class="normal">This can give you increased insight into the evaluation and an advantage over other competitors relying only on answers from Googling and Kaggle forums.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Rohan_Rao.png" alt=""/>
      </div>
      <p class="intervieweeName">Rohan Rao</p>
      <p class="normal"><a href="https://www.kaggle.com/rohanrao"><span class="url">https://www.kaggle.com/rohanrao</span></a></p>
      <p class="normal">Before we start exploring <a id="_idIndexMarker278"/>different metrics, let’s catch up with Rohan Rao (aka Vopani) himself, Quadruple Grandmaster and Senior Data Scientist at <code class="inlineCode">H2O.ai</code>, about his successes on Kaggle and the wisdom he has to share with us.</p>
      <p class="interviewHeader">What’s your favourite kind of competition and why? In terms of techniques and solving approaches, what is your speciality on Kaggle?</p>
      <p class="normal"><em class="italic">I like to dabble with different types of competitions, but my favorite would certainly be time series ones. I don’t quite like the typical approaches to and concepts of time series in the industry, so I tend to innovate and think out of the box by building solutions in an unorthodox way, which has ended up being very successful for me.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">For any Kaggle competition, my typical workflow would look like this:</em></p>
      <ul>
        <li class="bulletList"><em class="italic">Understand the problem statement and read all the information related to rules, format, timelines, datasets, metrics, and deliverables.</em></li>
        <li class="bulletList"><em class="italic">Dive deep into the data. Slice and dice it in every way possible and explore/visualize it to be able to answer any question about it.</em></li>
        <li class="bulletList"><em class="italic">Build a simple pipeline with a baseline model and make a submission to confirm the process works.</em></li>
        <li class="bulletList"><em class="italic">Engineer features, tune hyperparameters, and </em><em class="italic"><a id="_idIndexMarker279"/></em><em class="italic">experiment with multiple models to get a sense of what’s generally working and what’s not.</em></li>
        <li class="bulletList"><em class="italic">Constantly go back to analyzing the data, reading discussions on the forum, and tweaking the features and models to the fullest. Maybe team up at some point.</em></li>
        <li class="bulletList"><em class="italic">Ensemble multiple models and decide which submissions to make as final.</em></li>
      </ul>
      <p class="normal"><em class="italic">In my day-to-day work in data science, most of this happens too. But there are two crucial elements that are additionally required:</em></p>
      <ul>
        <li class="bulletList"><em class="italic">Curating and preparing datasets for the problem statement.</em></li>
        <li class="bulletList"><em class="italic">Deploying the final model or solution into production.</em></li>
      </ul>

      <p class="normal"><em class="italic">The majority of my time has been spent in these two activities for most of the projects I’ve worked on in the past.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">The vast majority of everything I’ve learned in machine learning has come from Kaggle. The community, the platform, and the content are pure gold and there is an incredible amount of stuff you can learn.</em></p>
      <p class="normal"><em class="italic">What has benefitted me the most is the experience of competing in Kaggle competitions; it has given me immense confidence in understanding, structuring, and solving problems across domains, which I have been able to apply successfully in many of the companies and projects I worked on outside Kaggle.</em></p>
      <p class="normal"><em class="italic">Many recruiters have contacted me for opportunities looking at my Kaggle achievements, primarily in Competitions. It gives a fairly good indication of a candidate’s ability in solving data science problems and hence it is a great platform to showcase your skills </em><em class="italic"><a id="_idIndexMarker280"/></em><em class="italic">and build a portfolio.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">I’ve made some mistake in every competition! That’s how you learn and improve. Sometimes it’s a coding bug, sometimes a flawed validation setup, sometimes an incorrect submission selection!</em></p>
      <p class="normal"><em class="italic">What’s important is to learn from these and ensure you don’t repeat them. Iterating over this process automatically helps to improve your overall performance on Kaggle.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis/machine learning?</p>
      <p class="normal"><em class="italic">I strongly believe in never marrying a technology. Use whatever works best, whatever is most comfortable and effective, but constantly be open to learning new tools and libraries.</em></p>
    </div>
    <h1 id="_idParaDest-71" class="heading-1">Metrics for regression (standard and ordinal)</h1>
    <p class="normal">When working with regression<a id="_idIndexMarker281"/> problems, that is, problems that involve <a id="_idIndexMarker282"/>estimating a continuous value (that could range from minus infinity to infinity), the most commonly used error<a id="_idIndexMarker283"/> measures are <strong class="keyWord">RMSE</strong> (<strong class="keyWord">root mean squared error</strong>) and <strong class="keyWord">MAE</strong> (<strong class="keyWord">mean absolute error</strong>), but you <a id="_idIndexMarker284"/>can also find slightly different error measures useful, such as RMSLE or MCRMSLE.</p>
    <h2 id="_idParaDest-72" class="heading-2">Mean squared error (MSE) and R squared</h2>
    <p class="normal">The root mean squared error is the<a id="_idIndexMarker285"/> root <a id="_idIndexMarker286"/>of the <strong class="keyWord">mean squared error</strong> (<strong class="keyWord">MSE</strong>), which is nothing else but the mean of the good old <strong class="keyWord">sum of squared errors</strong> (<strong class="keyWord">SSE</strong>) that you<a id="_idIndexMarker287"/> learned about when you studied how a regression works.</p>
    <p class="normal">Here is the formula for the MSE:</p>
    <p class="center"><img src="../Images/B17574_05_001.png" alt=""/></p>
    <p class="normal">Let’s start by explaining how the formula works. First of all, <em class="italic">n</em> indicates the number of cases, <img src="../Images/B17574_05_002.png" alt=""/> is the ground truth, and <img src="../Images/B17574_05_003.png" alt=""/> the prediction. You first get the difference between your predictions and your real values. You square the differences (so they become positive or simply zero), then you sum them all, resulting in your SSE. Then you just have to divide this measure by the number of predictions to obtain the average value, the MSE. Usually, all regression models minimize the SSE, so you won’t have great problems trying to minimize MSE or its direct derivatives<a id="_idIndexMarker288"/> such as <strong class="keyWord">R squared</strong> (also called<a id="_idIndexMarker289"/> the <strong class="keyWord">coefficient of determination</strong>), which is given by:</p>
    <p class="center"><img src="../Images/B17574_05_004.png" alt=""/></p>
    <p class="normal">Here, SSE (the sum of squared errors) is <a id="_idIndexMarker290"/>compared to the <strong class="keyWord">sum of squares total</strong> (<strong class="keyWord">SST</strong>), which is just the variance of the response. In statistics, in fact, SST is defined as the squared difference between your target values and their mean:</p>
    <p class="center"><img src="../Images/B17574_05_005.png" alt=""/></p>
    <p class="normal">To put it another way, R squared compares the squared errors of the model against the squared errors from the simplest model possible, the average of the response. Since both SSE and SST have the same scale, R squared can help you to determine whether transforming your target is helping to obtain better predictions.</p>
    <div class="note">
      <p class="normal">Please remember that linear transformations, such as minmax (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</span></a>) or standardization (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</span></a>), do not change the performance of any regressor, since they are linear transformations of the target. <strong class="keyWord">Non-linear</strong> <a id="_idIndexMarker291"/>transformations, such as the square root, the cubic root, the logarithm, the exponentiation, and their combinations, should instead definitely modify the performance of your regression model on the evaluation metric (hopefully for the better, if you decide on the right transformation).</p>
    </div>
    <p class="normal">MSE is a great instrument for<a id="_idIndexMarker292"/> comparing regression models applied to the same problem. The bad news is that the MSE is seldom used in Kaggle competitions, since <a id="_idIndexMarker293"/>RMSE is preferred. In fact, by taking the root of MSE, its value will resemble the original scale of your target and it will be easier at a glance to figure out if your model is doing a good job or not. In addition, if you are considering the same regression model across different data problems (for instance, across various datasets or data competitions), R squared is better because it is perfectly correlated with MSE and its values range between 0 and 1, making all comparisons easier.</p>
    <h2 id="_idParaDest-73" class="heading-2">Root mean squared error (RMSE)</h2>
    <p class="normal">RMSE is just the square<a id="_idIndexMarker294"/> root of MSE, but this implies some subtle change. Here is its formula:</p>
    <p class="center"><img src="../Images/B17574_05_006.png" alt=""/></p>
    <p class="normal">In the above formula, <em class="italic">n</em> indicates the number of cases, <img src="../Images/B17574_05_002.png" alt=""/> is the ground truth, and <img src="../Images/B17574_05_008.png" alt=""/> the prediction. In MSE, large prediction errors are greatly penalized because of the squaring activity. In RMSE, this dominance is lessened because of the root effect (however, you should always pay attention to outliers; they can affect your model performance a lot, no matter whether you are evaluating based on MSE or RMSE). </p>
    <p class="normal">Consequently, depending on the problem, you can get a better fit with an algorithm using MSE as an objective function by first applying the square root to your target (if possible, because<a id="_idIndexMarker295"/> it requires positive values), then squaring the results. Functions such as the <code class="inlineCode">TransformedTargetRegressor</code> in <a id="_idIndexMarker296"/>Scikit-learn help you to appropriately transform your regression target in order to get better-fitting results with respect to <a id="_idIndexMarker297"/>your evaluation metric.</p>
    <div class="note">
      <p class="normal">Recent competitions where RMSE has been used include:</p>
      <ul>
        <li class="bulletList"><em class="italic">Avito Demand Prediction Challenge</em>: <a href="https://www.kaggle.com/c/avito-demand-prediction"><span class="url">https://www.kaggle.com/c/avito-demand-prediction</span></a></li>
        <li class="bulletList"><em class="italic">Google Analytics Customer Revenue Prediction</em>: <a href="https://www.kaggle.com/c/ga-customer-revenue-prediction"><span class="url">https://www.kaggle.com/c/ga-customer-revenue-prediction</span></a></li>
        <li class="bulletList"><em class="italic">Elo Merchant Category Recommendation</em> <a href="https://www.kaggle.com/c/elo-merchant-category-recommendation"><span class="url">https://www.kaggle.com/c/elo-merchant-category-recommendation</span></a></li>
      </ul>
    </div>
    <h2 id="_idParaDest-74" class="heading-2">Root mean squared log error (RMSLE)</h2>
    <p class="normal">Another common transformation of<a id="_idIndexMarker298"/> MSE is<a id="_idIndexMarker299"/> <strong class="keyWord">root mean squared log error</strong> (<strong class="keyWord">RMSLE</strong>). MCRMSLE is just a variant made popular by the COVID-19 forecasting competitions, and it is the column-wise average of the RMSLE values of each single target when there are multiple ones. Here is the formula for RMSLE:</p>
    <p class="center"><img src="../Images/B17574_05_009.png" alt=""/></p>
    <p class="normal">In the formula, <em class="italic">n</em> indicates the number of cases, <img src="../Images/B17574_05_002.png" alt=""/> is the ground truth, and <img src="../Images/B17574_05_008.png" alt=""/> the prediction. Since you are applying a logarithmic transformation to your predictions and your ground truth before all the other squaring, averaging, and rooting operations, you don’t penalize huge differences between the predicted and the actual values, especially when both are large numbers. In other words, what you care the most about when using RMSLE is <em class="italic">the scale of your predictions with respect to the scale of the ground truth</em>. As with RMSE, machine learning algorithms for regression can better <a id="_idIndexMarker300"/>optimize for RMSLE if you apply a logarithmic transformation to the target before<a id="_idIndexMarker301"/> fitting it (and then reverse the effect using the exponential function).</p>
    <div class="note">
      <p class="normal">Recent competitions using RMSLE as an evaluation metric are:</p>
      <ul>
        <li class="bulletList"><em class="italic">ASHRAE - Great Energy Predictor III</em>: <a href="https://www.kaggle.com/c/ashrae-energy-prediction"><span class="url">https://www.kaggle.com/c/ashrae-energy-prediction</span></a></li>
        <li class="bulletList"><em class="italic">Santander Value Prediction Challenge</em>: <a href="https://www.kaggle.com/c/santander-value-prediction-challenge"><span class="url">https://www.kaggle.com/c/santander-value-prediction-challenge</span></a></li>
        <li class="bulletList"><em class="italic">Mercari Price Suggestion Challenge</em>: <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge"><span class="url">https://www.kaggle.com/c/mercari-price-suggestion-challenge</span></a></li>
        <li class="bulletList"><em class="italic">Sberbank Russian Housing Market</em>: <a href="https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market"><span class="url">https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market</span></a></li>
        <li class="bulletList"><em class="italic">Recruit Restaurant Visitor Forecasting</em>: <a href="https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting"><span class="url">https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting</span></a></li>
      </ul>
    </div>
    <p class="normal">By far, at the moment, RMSLE is the most used evaluation metric for regression in Kaggle competitions.</p>
    <h2 id="_idParaDest-75" class="heading-2">Mean absolute error (MAE)</h2>
    <p class="normal">The <strong class="keyWord">MAE</strong> (<strong class="keyWord">mean absolute error</strong>) evaluation <a id="_idIndexMarker302"/>metric is the absolute<a id="_idIndexMarker303"/> value of the difference between the predictions and the targets. Here is the formulation of MAE:</p>
    <p class="center"><img src="../Images/B17574_05_012.png" alt=""/></p>
    <p class="normal">In the formula, <em class="italic">n</em> stands for the number of cases, <img src="../Images/B17574_05_002.png" alt=""/> is the ground truth, and <img src="../Images/B17574_05_008.png" alt=""/> the prediction. MAE is not particularly sensitive to outliers (unlike MSE, where errors are squared), hence you may find it is an evaluation metric in many competitions whose datasets present outliers. Moreover, you can easily work with it since<a id="_idIndexMarker304"/> many algorithms can directly use it as an objective function; otherwise, you can optimize for it indirectly by just training on the square root of your target and then squaring the predictions.</p>
    <p class="normal">In terms of downside, using MAE as an objective function results in much slower convergence, since you are actually optimizing for predicting the median of the target (also called the L1 norm), instead of the<a id="_idIndexMarker305"/> mean (also called the L2 norm), as occurs by MSE minimization. This results in more<a id="_idIndexMarker306"/> complex computations for the optimizer, so the training time can even grow exponentially based on your number of training cases (see, for instance, this Stack Overflow question: <a href="https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to"><span class="url">https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to</span></a>).</p>
    <div class="note">
      <p class="normal">Notable recent competitions that used MAE as an evaluation metric are:</p>
      <ul>
        <li class="bulletList"><em class="italic">LANL Earthquake Prediction</em>: <a href="https://www.kaggle.com/c/LANL-Earthquake-Prediction"><span class="url">https://www.kaggle.com/c/LANL-Earthquake-Prediction</span></a></li>
        <li class="bulletList"><em class="italic">How Much Did It Rain? II</em>: <a href="https://www.kaggle.com/c/how-much-did-it-rain-ii"><span class="url">https://www.kaggle.com/c/how-much-did-it-rain-ii</span></a></li>
      </ul>
    </div>
    <p class="normal">Having mentioned the ASHRAE competition earlier, we should also mention that regression evaluation measures are quite relevant to forecasting competitions. For instance, the M5 forecasting competition was held recently (<a href="https://mofc.unic.ac.cy/m5-competition/"><span class="url">https://mofc.unic.ac.cy/m5-competition/</span></a>) and data from all the other M competitions is available too. If you are interested in forecasting competitions, of which there are a few on Kaggle, please see <a href="https://robjhyndman.com/hyndsight/forecasting-competitions/"><span class="url">https://robjhyndman.com/hyndsight/forecasting-competitions/</span></a> for an overview about M competitions and how valuable Kaggle is for obtaining better practical and theoretical results from such competitions. </p>
    <p class="normal">Essentially, forecasting competitions do not require a very different evaluation to regression competitions. When dealing with forecasting tasks, it is true that <a id="_idIndexMarker307"/>you can get some unusual <a id="_idIndexMarker308"/>evaluation metrics such as the <strong class="keyWord">Weighted Root Mean Squared Scaled Error</strong> (<a href="https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation"><span class="url">https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation</span></a>) or the <strong class="keyWord">symmetric mean absolute percentage error</strong>, better <a id="_idIndexMarker309"/>known as <strong class="keyWord">sMAPE</strong> (<a href="https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation"><span class="url">https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation</span></a>). However, in the end they are just variations of the usual RMSE or MAE that you can handle using the right target transformations.</p>
    <h1 id="_idParaDest-76" class="heading-1">Metrics for classification (label prediction and probability)</h1>
    <p class="normal">Having discussed the<a id="_idIndexMarker310"/> metrics for regression problems, we are going now to illustrate the metrics for classification problems, starting from the binary classification problems (when you have to predict between two classes), moving to the multi-class (when you have more than two classes), and then to the multi-label (when the classes overlap).</p>
    <h2 id="_idParaDest-77" class="heading-2">Accuracy</h2>
    <p class="normal">When analyzing the performance of a <a id="_idIndexMarker311"/>binary classifier, the most common and accessible metric that is used is <strong class="keyWord">accuracy</strong>. A misclassification error is <a id="_idIndexMarker312"/>when your model predicts the wrong class for an example. The accuracy is just the complement of the misclassification error and it can be calculated as the ratio between the number of correct numbers divided by the number of answers:</p>
    <p class="center"><img src="../Images/B17574_05_015.png" alt=""/></p>
    <div class="note">
      <p class="normal">This metric has been used, for instance, in <em class="italic">Cassava Leaf Disease Classification</em> (<a href="https://www.kaggle.com/c/cassava-leaf-disease-classification"><span class="url">https://www.kaggle.com/c/cassava-leaf-disease-classification</span></a>) and <em class="italic">Text Normalization Challenge - English Language</em> (<a href="https://www.kaggle.com/c/text-normalization-challenge-english-language"><span class="url">https://www.kaggle.com/c/text-normalization-challenge-english-language</span></a>), where you scored a correct prediction only if your predicted text matched the actual string.</p>
    </div>
    <p class="normal">As a metric, the accuracy is<a id="_idIndexMarker313"/> focused strongly on the effective performance of the model in a real setting: it tells you if the model works as expected. However, if your purpose is to evaluate and compare and have a clear picture of how effective your approach really is, you have to be cautious when using the accuracy because it can lead to wrong conclusions when the classes are imbalanced (when they have different<a id="_idIndexMarker314"/> frequencies). For instance, if a certain class makes up just 10% of the data, a predictor that predicts nothing but the majority class will be 90% accurate, proving itself quite useless in spite of the high accuracy.</p>
    <p class="normal">How can you spot such a problem? You can do this<a id="_idIndexMarker315"/> easily by using a <strong class="keyWord">confusion matrix</strong>. In a confusion matrix, you create a two-way table comparing the actual classes on the rows against the predicted classes on the columns. You can create a straightforward one using the Scikit-learn <code class="inlineCode">confusion_matrix</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">sklearn.metrics.confusion_matrix(
    y_true, y_pred, *, labels=<span class="hljs-literal">None</span>, sample_weight=<span class="hljs-literal">None</span>,
    normalize=<span class="hljs-literal">None</span>
)
</code></pre>
    <p class="normal">Providing the <code class="inlineCode">y_true</code> and <code class="inlineCode">y_pred</code> vectors will suffice to return you a meaningful table, but you can also provide row/column labels and sample weights for the examples in consideration, and normalize (set the marginals to sum to 1) over the true examples (the rows), the predicted examples (the columns), or all the examples. A perfect classifier will have all the cases on the principal diagonal of the matrix. Serious problems with the validity of the predictor are highlighted if there are few or no cases on one of the cells of the diagonal.</p>
    <p class="normal">In order to give you a better idea of how it works, you can try the graphical example offered by Scikit-learn at <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="url">https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py</span></a>:</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.1: Confusion matrix, with each cell normalized to 1.00, to represent the share of matches</p>
    <p class="normal">You can attempt to improve the<a id="_idIndexMarker316"/> usability of the accuracy by considering the accuracy relative to each of the classes and averaging them, but you will find it more useful to rely on other metrics such as <strong class="keyWord">precision</strong>, <strong class="keyWord">recall</strong>, and the <strong class="keyWord">F1-score</strong>.</p>
    <h2 id="_idParaDest-78" class="heading-2">Precision and recall</h2>
    <p class="normal">To obtain the <a id="_idIndexMarker317"/>precision and recall metrics, we again start from the <a id="_idIndexMarker318"/>confusion matrix. First, we have to name each of the cells:</p>
    <table id="table002" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell" colspan="2"/>
          <td class="table-cell" colspan="2">
            <p class="normal"><strong class="keyWord">Predicted</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Negative</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Positive</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="2">
            <p class="normal"><strong class="keyWord">Actual</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Negative</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">True Negative</p>
          </td>
          <td class="table-cell">
            <p class="normal">False Positive</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Positive</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">False Negative</p>
          </td>
          <td class="table-cell">
            <p class="normal">True Positive</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 5.1: Confusion matrix with cell names</p>
    <p class="normal">Here is how we define the cells:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TP</strong> (<strong class="keyWord">true positives</strong>): These are located in the upper-left cell, containing examples that have correctly been predicted as positive ones.</li>
      <li class="bulletList"><strong class="keyWord">FP</strong> (<strong class="keyWord">false positives</strong>): These are located in the upper-right cell, containing examples that have been predicted as positive but are actually negative.</li>
      <li class="bulletList"><strong class="keyWord">FN</strong> (<strong class="keyWord">false negatives</strong>): These are located in the lower-left cell, containing examples that have been predicted as negative but are actually positive.</li>
      <li class="bulletList"><strong class="keyWord">TN</strong> (<strong class="keyWord">true negatives</strong>): These are located in the lower-right cell, containing examples that have been correctly predicted as negative ones.</li>
    </ul>
    <p class="normal">Using these cells, you can <a id="_idIndexMarker319"/>actually get more precise information about how your<a id="_idIndexMarker320"/> classifier works and how you can tune your model better. First, we can easily revise the accuracy formula:</p>
    <p class="center"><img src="../Images/B17574_05_016.png" alt=""/></p>
    <p class="normal">Then, the first informative <a id="_idIndexMarker321"/>metric is called <strong class="keyWord">precision</strong> (or <strong class="keyWord">specificity</strong>) and it is actually the accuracy of the positive cases:</p>
    <p class="center"><img src="../Images/B17574_05_017.png" alt=""/></p>
    <p class="normal">In the computation, only the number of true positives and the number of false positives are involved. In essence, the metric tells you how often you are correct when you predict a positive.</p>
    <p class="normal">Clearly, your model could get high scores by predicting positives for only the examples it has high confidence in. That is actually the purpose of the measure: to force models to predict a positive class only when they are sure and it is safe to do so.</p>
    <p class="normal">However, if it is in your interest <a id="_idIndexMarker322"/>also to predict as many positives as possible, then you’ll also need to watch over the <strong class="keyWord">recall</strong> (or <strong class="keyWord">coverage</strong> or <strong class="keyWord">sensitivity</strong> or even <strong class="keyWord">true positive rate</strong>) metric:</p>
    <p class="center"><img src="../Images/B17574_05_018.png" alt=""/></p>
    <p class="normal">Here, you will also need to know about false <a id="_idIndexMarker323"/>negatives. The interesting thing about these two metrics is that, since they are based on examples classification, and a classification is actually based on probability (which is usually set between the positive and negative class at the <code class="inlineCode">0.5</code> threshold), you can change the threshold and have one of the two metrics<a id="_idIndexMarker324"/> improved at the expense of the other. </p>
    <p class="normal">For instance, if you increase the threshold, you will get more precision (the classifier is more confident of the prediction) but less recall. If you decrease the threshold, you get <a id="_idIndexMarker325"/>less precision but more recall. This is also called the <strong class="keyWord">precision/recall trade-off</strong>.</p>
    <p class="normal">The Scikit-learn website <a id="_idIndexMarker326"/>offers a simple and practical overview of this trade-off (<a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"><span class="url">https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html</span></a>), helping you to trace a <strong class="keyWord">precision/recall curve</strong> and thus understand how these two measures can be exchanged to obtain a result that better fits your needs:</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.2: A two-class precision-recall curve with its characteristic steps</p>
    <p class="normal">One metric associated with the precision/recall trade-off is<a id="_idIndexMarker327"/> the <strong class="keyWord">average precision</strong>. Average precision computes the mean precision for recall values from 0 to 1 (basically, as you vary the threshold from 1 to 0). Average precision is very popular for tasks related to object detection, which we will discuss a bit later on, but it is also very useful for classification in tabular data. In practice, it proves valuable when you want to monitor model performance on a very rare class (when the data is extremely imbalanced) in a more <a id="_idIndexMarker328"/>precise and exact way, which is often the case with<a id="_idIndexMarker329"/> fraud detection problems. </p>
    <p class="normal">For more specific insights on this, read <em class="italic">Gael Varoquaux’s</em> discussion: <a href="http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision"><span class="url">http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision</span></a>.</p>
    <h2 id="_idParaDest-79" class="heading-2">The F1 score</h2>
    <p class="normal">At this point, you have probably <a id="_idIndexMarker330"/>already figured out that using precision or recall as an <a id="_idIndexMarker331"/>evaluation metric is not an ideal choice because you can only optimize one at the expense of the other. For this reason, there are no Kaggle competitions that use only one of the two metrics. You should combine them (as in the average precision). A single metric, the <strong class="keyWord">F1 score</strong>, which is the harmonic mean of precision and recall, is commonly considered to be the best solution:</p>
    <p class="center"><img src="../Images/B17574_05_019.png" alt=""/></p>
    <p class="normal">If you get a high <em class="italic">F</em>1 score, it is<a id="_idIndexMarker332"/> because your model has improved in precision or recall or in both. You can find a fine example of the usage of this metric in the <em class="italic">Quora</em><em class="italic"><a id="_idIndexMarker333"/></em><em class="italic"> Insincere Questions Classification</em> competition (<a href="https://www.kaggle.com/c/quora-insincere-questions-classification"><span class="url">https://www.kaggle.com/c/quora-insincere-questions-classification</span></a>).</p>
    <p class="normal">In some competitions, you <a id="_idIndexMarker334"/>also get the <strong class="keyWord">F-beta</strong> score. This is simply the weighted harmonic mean between precision and recall, and beta decides the weight of the recall in the combined score:</p>
    <p class="center"><img src="../Images/B17574_05_020.png" alt=""/></p>
    <p class="normal">Since we have already introduced the concept of threshold and classification probability, we can now discuss the log loss and ROC-AUC, both quite common classification metrics.</p>
    <h2 id="_idParaDest-80" class="heading-2">Log loss and ROC-AUC</h2>
    <p class="normal">Let’s start with<a id="_idIndexMarker335"/> the <strong class="keyWord">log loss</strong>, which is also known as <strong class="keyWord">cross-entropy</strong><strong class="keyWord"><a id="_idIndexMarker336"/></strong> in deep learning models. The log loss is the difference between the<a id="_idIndexMarker337"/> predicted probability and the ground truth probability:</p>
    <p class="center"><img src="../Images/B17574_05_021.png" alt=""/></p>
    <p class="normal">In the above formula, <em class="italic">n</em> stands for the number of examples, <img src="../Images/B17574_05_002.png" alt=""/> is the ground truth for the <em class="italic">i</em><sup class="superscript">th</sup> case, and <img src="../Images/B17574_05_008.png" alt=""/> the prediction.</p>
    <p class="normal">If a competition uses the log loss, it is implied that the objective is to estimate as correctly as possible the probability of an <a id="_idIndexMarker338"/>example being of a positive class. You can actually find the log loss in quite a lot of competitions. </p>
    <p class="normal">We suggest you have a look, for instance, at the recent<a id="_idIndexMarker339"/> <em class="italic">Deepfake Detection Challenge</em> (<a href="https://www.kaggle.com/c/deepfake-detection-challenge"><span class="url">https://www.kaggle.com/c/deepfake-detection-challenge</span></a>) or at the older <em class="italic">Quora Question Pairs</em> (<a href="https://www.kaggle.com/c/quora-question-pairs"><span class="url">https://www.kaggle.com/c/quora-question-pairs</span></a>).</p>
    <p class="normal">The <strong class="keyWord">ROC curve</strong>, or <strong class="keyWord">receiver operating characteristic curve</strong>, is a graphical chart used to evaluate the <a id="_idIndexMarker340"/>performance of a<a id="_idIndexMarker341"/> binary classifier and to compare multiple classifiers. It is the building block of the ROC-AUC metric, because the metric is simply the area delimited under the ROC curve. The ROC curve consists of the true positive rate (the recall) plotted against the false positive rate (the ratio of negative instances that are incorrectly classified as positive ones). It is equivalent to one minus the true negative rate (the ratio of negative examples that are correctly classified). Here are a few examples:</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.3: Different ROC curves and their AUCs</p>
    <p class="normal">Ideally, a ROC curve of a<a id="_idIndexMarker342"/> well-performing classifier should quickly climb up the true positive rate (recall) at low values of the false positive rate. A ROC-AUC between 0.9 to 1.0 is considered very good. </p>
    <p class="normal">A bad classifier can be spotted by the ROC curve appearing very similar, if not identical, to the diagonal of the chart, which represents the <a id="_idIndexMarker343"/>performance of a purely random classifier, as in the top left of the figure above; ROC-AUC scores near 0.5 are considered to be almost random results. If you are comparing different classifiers, and you are <a id="_idIndexMarker344"/>using the <strong class="keyWord">area under the curve</strong> (<strong class="keyWord">AUC</strong>), the classifier with the higher area is the more performant one.</p>
    <p class="normal">If the classes are balanced, or not too imbalanced, increases in the AUC are proportional to the effectiveness of the trained model and they can be intuitively thought of as the ability of the model to output higher probabilities for true positives. We also think of it as the ability to order the examples more properly from positive to negative. However, when the positive class is rare, the AUC starts high and its increments may mean very little in terms of predicting the<a id="_idIndexMarker345"/> rare class better. As we mentioned before, in such a case, average precision is a more helpful metric.</p>
    <div class="note">
      <p class="normal">AUC has recently been used for quite a lot of different competitions. We suggest you have a look at these three:</p>
      <ul>
        <li class="bulletList"><em class="italic">IEEE-CIS Fraud Detection</em>: <a href="https://www.kaggle.com/c/ieee-fraud-detection"><span class="url">https://www.kaggle.com/c/ieee-fraud-detection</span></a></li>
        <li class="bulletList"><em class="italic">Riiid Answer Correctness Prediction</em>: <a href="https://www.kaggle.com/c/riiid-test-answer-prediction"><span class="url">https://www.kaggle.com/c/riiid-test-answer-prediction</span></a></li>
        <li class="bulletList"><em class="italic">Jigsaw Multilingual Toxic Comment Classification</em>: <a href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/"><span class="url">https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/</span></a></li>
      </ul>
    </div>
    <p class="normal">You can read a detailed treatise in the following paper: Su, W., Yuan, Y., and Zhu, M. <em class="italic">A relationship between the average precision and the area under the ROC curve.</em> Proceedings of the<a id="_idIndexMarker346"/> 2015 International Conference on The Theory of Information Retrieval. 2015.</p>
    <h2 id="_idParaDest-81" class="heading-2">Matthews correlation coefficient (MCC)</h2>
    <p class="normal">We complete our overview of binary <a id="_idIndexMarker347"/>classification metrics with the <strong class="keyWord">Matthews correlation coefficient </strong>(<strong class="keyWord">MCC</strong>), which made its <a id="_idIndexMarker348"/>appearance in <em class="italic">VSB Power Line Fault Detection</em> (<a href="https://www.kaggle.com/c/vsb-power-line-fault-detection"><span class="url">https://www.kaggle.com/c/vsb-power-line-fault-detection</span></a>) and <em class="italic">Bosch Production Line Performance</em> (<a href="https://www.kaggle.com/c/bosch-production-line-performance"><span class="url">https://www.kaggle.com/c/bosch-production-line-performance</span></a>).</p>
    <p class="normal">The formula for the MCC is:</p>
    <p class="center"><img src="../Images/B17574_05_024.png" alt="" style="height: 3.25em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">In the above formula, <em class="italic">TP</em> stands for true positives, <em class="italic">TN</em> for true negatives, <em class="italic">FP</em> for false positives, and <em class="italic">FN</em> for false negatives. It is the same nomenclature as we met when discussing precision and recall.</p>
    <p class="normal">Behaving as a correlation coefficient, in <a id="_idIndexMarker349"/>other words, ranging from +1 (perfect prediction) to -1 (inverse prediction), this metric can be considered a measure of the quality of the classification even when the classes are <a id="_idIndexMarker350"/>quite imbalanced.</p>
    <p class="normal">In spite of its complexity, the formula can be reformulated and simplified, as demonstrated by Neuron Engineer (<a href="https://www.kaggle.com/ratthachat"><span class="url">https://www.kaggle.com/ratthachat</span></a>) in his Notebook: <a href="https://www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc"><span class="url">www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc</span></a>.</p>
    <p class="normal">The work done by Neuron Engineer in understanding the ratio of the evaluation metric is indeed exemplary. In fact, his reformulated MCC becomes:</p>
    <p class="center"><img src="../Images/B17574_05_025.png" alt="" style="height: 1.5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">Where each element of the formula is:</p>
    <p class="center"><img src="../Images/B17574_05_026.png" alt=""/></p>
    <p class="center"><img src="../Images/B17574_05_027.png" alt=""/></p>
    <p class="center"><img src="../Images/B17574_05_028.png" alt=""/></p>
    <p class="center"><img src="../Images/B17574_05_029.png" alt=""/></p>
    <p class="center"><img src="../Images/B17574_05_030.png" alt=""/></p>
    <p class="normal">The reformulation helps to clarify, in a more intelligible form than the original, that you can get higher performance from improving both positive and negative class precision, but that’s not enough: you also<a id="_idIndexMarker351"/> have to have positive and negative predictions in proportion to the ground truth, or your submission will be greatly penalized.</p>
    <h1 id="_idParaDest-82" class="heading-1">Metrics for multi-class classification</h1>
    <p class="normal">When moving to multi-class<a id="_idIndexMarker352"/> classification, you simply use the binary classification metrics that we have just seen, applied to each class, and then you summarize them using some of the averaging strategies that are commonly used for multi-class situations.</p>
    <p class="normal">For instance, if you want to evaluate your solution based on the <em class="italic">F</em>1 score, you have three possible averaging choices:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Macro averaging</strong>: Simply<a id="_idIndexMarker353"/> calculate the <em class="italic">F</em>1 score for each class and then average all the results. In this way, each class will count as much the others, no matter how frequent its positive cases are or how important they are for your problem, resulting therefore in equal penalizations when the model doesn’t perform well with any class:</li>
    </ul>
    <p class="center"><img src="../Images/B17574_05_031.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Micro averaging</strong>: This approach<a id="_idIndexMarker354"/> will sum all the contributions from each class to compute an aggregated <em class="italic">F</em>1 score. It results in no particular favor to or penalization of any class, since all the computations are made regardless of each class, so it can more accurately account for class imbalances:</li>
    </ul>
    <p class="center"><img src="../Images/B17574_05_032.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Weighting</strong>: As with macro averaging, you first calculate the <em class="italic">F</em>1 score for each class, but then you <a id="_idIndexMarker355"/>make a weighted average mean of all of them using a weight that depends on the number of true labels of each class. By using such a set of weights, you can take into account the frequency of positive cases from each class or the relevance of that class for your problem. This approach clearly favors the majority classes, which will be weighted more in the computations:</li>
    </ul>
    <p class="center"><img src="../Images/B17574_05_033.png" alt=""/></p>
    <p class="center"><img src="../Images/B17574_05_034.png" alt=""/></p>
    <p class="normal">Common multi-class metrics that you may encounter in Kaggle competitions are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Multiclass accuracy (weighted)</strong>: <em class="italic">Bengali.AI Handwritten Grapheme Classification</em> (<a href="https://www.kaggle.com/c/bengaliai-cv19"><span class="url">https://www.kaggle.com/c/bengaliai-cv19</span></a>)</li>
      <li class="bulletList"><strong class="keyWord">Multiclass log loss (MeanColumnwiseLogLoss)</strong>: <em class="italic">Mechanisms of Action (MoA) </em><em class="italic"><a id="_idIndexMarker356"/></em><em class="italic">Prediction </em>(<a href="https://www.kaggle.com/c/lish-moa/"><span class="url">https://www.kaggle.com/c/lish-moa/</span></a>)</li>
      <li class="bulletList"><strong class="keyWord">Macro-F1</strong> and <strong class="keyWord">Micro-F1 (NQMicroF1)</strong>: <em class="italic">University of Liverpool - Ion Switching</em> (<a href="https://www.kaggle.com/c/liverpool-ion-switching"><span class="url">https://www.kaggle.com/c/liverpool-ion-switching</span></a>), <em class="italic">Human Protein Atlas Image Classification</em> (<a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/"><span class="url">https://www.kaggle.com/c/human-protein-atlas-image-classification/</span></a>), <em class="italic">TensorFlow </em><em class="italic"><a id="_idIndexMarker357"/></em><em class="italic">2.0 Question </em><em class="italic"><a id="_idIndexMarker358"/></em><em class="italic">Answering</em> (<a href="https://www.kaggle.com/c/tensorflow2-question-answering"><span class="url">https://www.kaggle.com/c/tensorflow2-question-answering</span></a>)</li>
      <li class="bulletList"><strong class="keyWord">Mean-F1</strong>: <em class="italic">Shopee - Price Match Guarantee</em> (<a href="https://www.kaggle.com/c/shopee-product-matching/"><span class="url">https://www.kaggle.com/c/shopee-product-matching/</span></a>). Here, the <em class="italic">F</em>1 score is calculated for every predicted<a id="_idIndexMarker359"/> row, then averaged, whereas the Macro-F1 score is defined as the mean of class-wise/label-wise <em class="italic">F</em>1 scores.</li>
    </ul>
    <p class="normal">Then there is also <strong class="keyWord">Quadratic Weighted Kappa</strong>, which we will explore later on as a smart evaluation metric for ordinal prediction problems. In its simplest form, the<a id="_idIndexMarker360"/> <strong class="keyWord">Cohen Kappa</strong> score, it just measures the agreement between your predictions and the ground truth. The metric was actually created for <a id="_idIndexMarker361"/>measuring <strong class="keyWord">inter-annotation agreement</strong>, but it is really versatile and has found even better uses.</p>
    <p class="normal">What is inter-annotation agreement? Let’s imagine that you have a labeling task: classifying some photos based on whether they contain an image of a cat, a dog, or neither. If you ask a set of people to do the task for you, you may incur some erroneous labels because someone (called the <em class="italic">judge</em> in this kind of task) may misinterpret a dog as a cat or vice versa. The smart way to do this job correctly is to divide the work among multiple judges labeling the same photos, and then measure their level of agreement based on the Cohen Kappa score.</p>
    <p class="normal">Therefore, the Cohen Kappa is devised as a score expressing the level of agreement between two annotators on a labeling (classification) problem:</p>
    <p class="center"><img src="../Images/B17574_05_035.png" alt=""/></p>
    <p class="normal">In the formula, <em class="italic">p</em><sub class="subscript">0</sub> is the relative observed agreement among raters, and <em class="italic">p</em><sub class="subscript">e</sub> is the hypothetical probability of chance agreement. Using the confusion matrix nomenclature, this can be rewritten as:</p>
    <p class="center"><img src="../Images/B17574_05_036.png" alt=""/></p>
    <p class="normal">The interesting aspect of this formula is that the score takes into account the empirical probability that the agreement has happened just by chance, so the measure has a correction for all the most probable classifications. The metric ranges from 1, meaning complete agreement, to -1, meaning the judges completely oppose each other (total disagreement). </p>
    <p class="normal">Values around 0 signify that agreement and disagreement among the judges is happening by mere chance. This helps you figure out if the model is really performing better than chance in most situations.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Andrey_Lukyanenko_-_Copy.png" alt=""/>
      </div>
      <p class="intervieweeName">Andrey Lukyanenko</p>
      <p class="normal"><a href="https://www.kaggle.com/artgor"><span class="url">https://www.kaggle.com/artgor</span></a></p>
      <p class="normal">Our second interview of the chapter is with Andrey Lukyanenko, a Notebooks and Discussions Grandmaster and Competitions Master. In his day job, he is a Machine Learning Engineer and TechLead at MTS Group. He had many interesting things to say about his Kaggle experiences!</p>
      <p class="interviewHeader">What’s your favourite kind of competition and why? In terms of techniques, solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">I prefer competitions where solutions can be general enough to be transferable to other datasets/domains. I’m interested in trying various neural net architectures, state-of-the-art approaches, and post-processing tricks. I don’t favor those competitions that require reverse engineering or creating some “golden features,” as these approaches won’t be applicable in other datasets.</em></p>
      <p class="interviewHeader">While you were competing on Kaggle, you also became a Grandmaster in Notebooks (and ranked number one) and Discussions. Have you invested in these two objectives?</p>
      <p class="normal"><em class="italic">I have invested a lot of time and effort into writing Notebooks, but the Discussion Grandmaster rank happened kind of on its own.</em></p>
      <p class="normal"><em class="italic">Let’s start with the Notebook ranking.</em></p>
      <p class="normal"><em class="italic">There was a special competition in 2018 called DonorsChoose.org Application Screening. DonorsChoose is a fund that empowers public school teachers from across the country to request much-needed materials and experiences for their students. It organized a competition, where the winning solutions were based not on the score on the leaderboard, but on the number of the upvotes on the Notebook. This looked interesting and I wrote a Notebook for the competition. Many participants advertised their analysis on social media and I did the same. As a result, I reached second place and won a Pixelbook (I’m still using it!).</em></p>
      <p class="normal"><em class="italic">I was very motivated by this success and continued writing Notebooks. At first, I simply wanted to share my analysis and get feedback, because I wanted to try to compare my analytics and visualization skills with other people to see what I could do and what people thought of it. People started liking my kernels and I wanted to improve my skills even further. Another motivation was a desire to improve my skill at making a quick MVP (minimum viable product). When a new competition starts, many people begin writing Notebooks, and if you want to be one of the first, you have to be able to do it fast without sacrificing quality. This is challenging, but fun and rewarding.</em></p>

      <p class="normal"><em class="italic">I was able to get the Notebook Grandmaster rank in the February of 2019; after some time, I reached first place and held it for more than a year. Now I write Notebooks less frequently, but I still enjoy doing it.</em></p>
      <p class="normal"><em class="italic">As for discussions, I think it kind of happened on its own. I answered the comments on my Notebooks, and shared and discussed ideas about competitions in which I took part, and my discussion ranking steadily increased.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">It was the </em>Predicting Molecular Properties<em class="italic"> competition. I have written a blog post about it in more detail here (</em><a href="https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1"><span class="url">https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1</span></a><em class="italic">). It was a domain-specific competition aimed at predicting interactions between atoms in molecules. Nuclear Magnetic Resonance (NMR) is a technology that uses principles similar to MRI to understand the structure and dynamics of proteins and molecules. Researchers around the world conduct NMR experiments to further understand the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science. In this competition, we tried to predict the magnetic interaction between two atoms in a molecule (the scalar coupling constant). State-of-the-art methods from quantum mechanics can calculate these coupling constants given only a 3D molecular structure as input. But these calculations are very resource-intensive, so can’t be always used. If machine learning approaches could predict these values, it would really help medicinal chemists to gain structural insights faster and more cheaply.</em></p>
      <p class="normal"><em class="italic">I usually write EDA kernels for new Kaggle competitions, and this one was no exception. A common approach for tabular data in Kaggle competitions is extensive feature engineering and using gradient boosting models. I used LGBM too in my early attempts, but knew that there should be better ways to work with graphs. I realized that domain expertise would provide a serious advantage, so I hunted for every piece of such information. Of course, I noticed that there were several active experts, who wrote on the forum and created kernels, so I read everything from them. And one day I received an e-mail from an expert in this domain who thought that our skills could complement each other. Usually, I prefer to work on competitions by myself for some time, but in this case, combining forces seemed to be a good idea to me. And this decision turned out to be a great one! With time we were able to gather an amazing team.</em></p>
      <p class="normal"><em class="italic">After some time, we noticed a potential for neural nets in the competition: a well-known Kaggler, Heng, posted an example of an MPNN (Message Passing Neural Network) model. After some time, I was even able to run it, but the results were worse compared to our models. Nevertheless, our team knew that we would need to work with these Neural Nets if we wanted to aim high. It was amazing to see how Christof was able to build new neural nets extremely fast. Soon, we focused only on developing those models.</em></p>

      <p class="normal"><em class="italic">After that, my role switched to a support one. I did a lot of experiments with our neural nets: trying various hyperparameters, different architectures, various little tweaks to training schedules, and so on. Sometimes I did EDA on our predictions to find our interesting or wrong cases, and later we used this information to improve our models even further.</em></p>
      <p class="normal"><em class="italic">We got the 8</em><sup class="superscript-italic" style="font-style: italic;">th</sup><em class="italic"> place and I learned a lot during this competition.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Kaggle definitely helped me a lot, especially with my skills and my personal brand. Writing and publishing Kaggle Notebooks taught me not only EDA and ML skills, but it forced me to become adaptable, to be able to understand new topics and tasks quickly, to iterate more efficiently between approaches. At the same time, it provided a measure of visibility for me, because people appreciated my work.</em></p>
      <p class="normal"><em class="italic">My first portfolio (</em><a href="https://erlemar.github.io/"><span class="url">https://erlemar.github.io/</span></a><em class="italic">) had a lot of different Notebooks, and half of them were based on old Kaggle competitions. It was definitely helpful in getting my first jobs. My Kaggle achievements also helped me attract recruiters from good companies, sometimes even to skip steps of the interview process, and even led me to several consulting gigs.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">I think we need to separate inexperienced Kagglers into two groups: those who are inexperienced in data science in general and those who are inexperienced on Kaggle.</em></p>
      <p class="normal"><em class="italic">Those who are inexperienced in general make a number of different mistakes (and it is okay, everyone started somewhere):</em></p>
      <ul>
        <li class="bulletList"><em class="italic">One of the most serious problems: lack of critical thinking and not knowing how to do their own research;</em></li>
        <li class="bulletList"><em class="italic">Not knowing when and what tools/approaches to use;</em></li>
        <li class="bulletList"><em class="italic">Blindly taking public Notebooks and using them without understanding how they work;</em></li>
        <li class="bulletList"><em class="italic">Fixating on a certain idea and spending too much time pursuing it, even when it doesn’t work;</em></li>
        <li class="bulletList"><em class="italic">Despairing and losing motivation when their experiments fail.</em></li>
      </ul>
      <p class="normal"><em class="italic">As for those people who have experience in data science but don’t have experience with Kaggle, I’d say that the most serious thing they overlook is that they underestimate Kaggle’s difficulty. They don’t expect Kaggle to be very competitive, that you need to try many different things to succeed, that there are a lot of tricks that work only in competitions, that there are people who professionally participate in competitions.</em></p>

      <p class="normal"><em class="italic">Also, people often overestimate domain expertise. I admit that there were a number of competitions when the teams with domain experts in them won gold medals and prizes, but in most cases experienced Kagglers triumph.</em></p>
      <p class="normal"><em class="italic">Also, I have seen the following situation many times: some person proclaims that winning Kaggle is easy, and that he (or his group of people) will get a gold medal or many gold medals in the recent future. In most cases, they silently fail.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <ul>
        <li class="bulletList"><em class="italic">Not</em><em class="italic"> enough looking in the data. Sometimes I wasn’t able to generate better features or apply better postprocessing due to this. And reserve engineering and “golden features” is a whole additional topic.</em></li>
        <li class="bulletList"><em class="italic">Spending too much time on a single idea because I hoped it would work. This is called sunk-cost fallacy.</em></li>
        <li class="bulletList"><em class="italic">Not enough experiments. The effort pays off – if you don’t spend enough time and resources on the competition, you won’t get a high place on a leaderboard.</em></li>
        <li class="bulletList"><em class="italic">Entering “wrong” competitions. There were competitions with leaks, reverse engineering, etc. There were competitions with an unreasonable split between public and private test data and a shake-up ensured. There were competitions that weren’t interesting enough for me and I shouldn’t have started participating in them.</em></li>
        <li class="bulletList"><em class="italic">Teaming up with the wrong people. There were cases when my teammates weren’t as active as I expected a</em><em class="italic">nd it led to a worse team score.</em></li>
      </ul>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">I think it is important to remember your goal, know what are you ready to invest into this competition, and think about the possible outcomes. There are many possible goals that people have while entering a competition:</em></p>
      <ul>
        <li class="bulletList"><em class="italic">Win</em><em class="italic">ning money or getting a medal;</em></li>
        <li class="bulletList"><em class="italic">Getting new skills or improving existing ones;</em></li>
        <li class="bulletList"><em class="italic">Working with a new task/domain;</em></li>
        <li class="bulletList"><em class="italic">Networking;</em></li>
        <li class="bulletList"><em class="italic">PR;</em></li>
        <li class="bulletList"><em class="italic">etc;</em></li>
      </ul>
      <p class="normal"><em class="italic">Of course, it is possible to have multiple motivations.</em></p>

      <p class="normal"><em class="italic">As for what are you ready to invest, it is usually about the amount of time and effort you are ready to spend as well as the hardware that you have.</em></p>
      <p class="normal"><em class="italic">When I speak about the outcomes, I mean what will happen when the competition ends. It is possible that you will invest a lot in this competition and win, but you could also lose. Are you ready for this reality? Is winning a particular competition critical to you? Maybe you need to be prepared to invest more effort; on the other hand, maybe you have long-term goals and one failed competition won’t hurt much.</em></p>
    </div>
    <h1 id="_idParaDest-83" class="heading-1">Metrics for object detection problems</h1>
    <p class="normal">In recent years, deep learning<a id="_idIndexMarker362"/> competitions have become more and more <a id="_idIndexMarker363"/>common on Kaggle. Most of these competitions, focused on image recognition or on natural language processing tasks, have not required the use of evaluation metrics much different from the ones we have explored up to now. However, a couple of specific problems have required some special metric to be evaluated correctly: those relating to <strong class="keyWord">object detection </strong>and <strong class="keyWord">segmentation</strong>.</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.4: Computer vision tasks. (Source: https://cocodataset.org/#explore?id=38282, https://cocodataset.org/#explore?id=68717)</p>
    <p class="normal">In <strong class="keyWord">object detection</strong>, you don’t have to<a id="_idIndexMarker364"/> classify an image, but instead find relevant portions of a picture and label them accordingly. For instance, in <em class="italic">Figure 5.4</em>, an object detection classifier has been entrusted to locate within a photo the portions of the<a id="_idIndexMarker365"/> picture where either dogs or cats are present and classify each of them with a proper label. The example on the left shows the localization of a cat using a rectangular box (called a <strong class="keyWord">bounding box</strong>). The example on the right presents how multiple cats and dogs are detected in the picture by bounding boxes and then correctly classified (the blue bounding boxes are for dogs, the red ones for cats).</p>
    <div class="note">
      <p class="normal">In order to describe the spatial location of an object, in object detection we use <strong class="keyWord">bounding boxes</strong>, which define a<a id="_idIndexMarker366"/> rectangular area in which the object lies. A bounding box is usually specified using two (<em class="italic">x</em>, <em class="italic">y</em>) coordinates: the upper-left and lower-right corners. In terms of a machine learning algorithm, finding the coordinates of bounding boxes corresponds to applying a regression problem to multiple targets. However, you probably won’t frame the problem from scratch but rely on pre-built and often pre-trained models such as Mask R-CNN (<a href="https://arxiv.org/abs/1703.06870"><span class="url">https://arxiv.org/abs/1703.06870</span></a>), RetinaNet (<a href="https://arxiv.org/abs/2106.05624v1"><span class="url">https://arxiv.org/abs/2106.05624v1</span></a>), FPN (<a href="https://arxiv.org/abs/1612.03144v2"><span class="url">https://arxiv.org/abs/1612.03144v2</span></a>), YOLO (<a href="https://arxiv.org/abs/1506.02640v1"><span class="url">https://arxiv.org/abs/1506.02640v1</span></a>), Faster R-CNN (<a href="https://arxiv.org/abs/1506.01497v1"><span class="url">https://arxiv.org/abs/1506.01497v1</span></a>), or SDD (<a href="https://arxiv.org/abs/1512.02325"><span class="url">https://arxiv.org/abs/1512.02325</span></a>).</p>
    </div>
    <p class="normal">In <strong class="keyWord">segmentation</strong>, you instead have <a id="_idIndexMarker367"/>a classification at the <em class="italic">pixel</em> level, so if you have<a id="_idIndexMarker368"/> a 320x200 image, you actually have to make 64,000 pixel classifications. Depending on the task, you can have a <strong class="keyWord">semantic segmentation</strong> where you have<a id="_idIndexMarker369"/> to classify every pixel in a photo, or an <strong class="keyWord">instance segmentation</strong> where you only have to classify the pixels representing <a id="_idIndexMarker370"/>objects of a certain type of interest (for instance, a cat as in our example in <em class="italic">Figure 5.5 </em>below):</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.5: Semantic segmentation and instance segmentation on the same image. (Source: https://cocodataset.org/#explore?id=338091)</p>
    <p class="normal">Let’s start with an overview of the specific metrics for these tasks, metrics that can work well for both problems, since, in both cases, you are predicting entire areas (rectangular ones in object detection, polygonal ones in segmentation) of a picture and you have to compare your predictions against a ground truth, which is, again, expressed as areas. On the side of segmentation, the easiest <a id="_idIndexMarker371"/>metric is the <strong class="keyWord">pixel accuracy</strong>, which, as the name suggests, is the accuracy on the pixel classification. </p>
    <p class="normal">It is not a great metric because, as happens with accuracy on binary and multi-class problems, your score may look great if the relevant pixels do not take up very much of the image (you just predict the majority claim, thus you don’t segment).</p>
    <p class="normal">Therefore, there are two metrics<a id="_idIndexMarker372"/> that are used much more, especially in competitions: the <strong class="keyWord">intersection over union</strong> and the <strong class="keyWord">dice coefficient</strong>.</p>
    <h2 id="_idParaDest-84" class="heading-2">Intersection over union (IoU)</h2>
    <p class="normal">The <strong class="keyWord">intersection over union</strong> (<strong class="keyWord">IoU</strong>) is also<a id="_idIndexMarker373"/> known as the <strong class="keyWord">Jaccard index</strong>. When used in segmentation<a id="_idIndexMarker374"/> problems, using <a id="_idIndexMarker375"/>IoU implies that you have two images to compare: one is your prediction and the other is the mask revealing the ground truth, which is usually a binary matrix where the value 1 stands for the ground truth and 0 otherwise. In the case of multiple objects, you have multiple masks, each one labeled with the class of the object.</p>
    <p class="normal">When used in object detection problems, you have the boundaries of two rectangular areas (those of the prediction and the ground truth), expressed by the coordinates of their vertices. For each classified class, you compute the area of overlap between your prediction and the ground truth mask, and then you divide this by the area of the union between your prediction and the ground truth, a sum that takes into account any overlap. In this way, you are proportionally penalized both if you predict a larger area than what it should be (the denominator will be larger) or a smaller one (the numerator will be smaller):</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.6: Visual representation of the IoU calculation</p>
    <p class="normal">In <em class="italic">Figure 5.6</em> you can see a visual representation of the areas involved in the computation. By imagining the <a id="_idIndexMarker376"/>squares <a id="_idIndexMarker377"/>overlapping more, you can figure out how the metric efficiently penalizes your solution when your prediction, even if covering the ground truth, exceeds it (the area of union becomes larger).</p>
    <div class="note">
      <p class="normal">Here are some examples of competitions where IoU has been used:</p>
      <ul>
        <li class="bulletList"><em class="italic">TGS Salt Identification Challenge</em> (<a href="https://www.kaggle.com/c/tgs-salt-identification-challenge/"><span class="url">https://www.kaggle.com/c/tgs-salt-identification-challenge/</span></a>) with Intersection Over Union Object Segmentation</li>
        <li class="bulletList"><em class="italic">iMaterialist (Fashion) 2019 at FGVC6</em> (<a href="https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6"><span class="url">https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6</span></a>) with Intersection Over Union Object Segmentation With Classification</li>
        <li class="bulletList"><em class="italic">Airbus Ship Detection Challenge</em> (<a href="https://www.kaggle.com/c/airbus-ship-detection"><span class="url">https://www.kaggle.com/c/airbus-ship-detection</span></a>) with Intersection Over Union Object Segmentation Beta</li>
      </ul>
    </div>
    <h2 id="_idParaDest-85" class="heading-2">Dice</h2>
    <p class="normal">The other useful metric is the <strong class="keyWord">Dice coefficient</strong>, which is the area of overlap between the prediction and ground<a id="_idIndexMarker378"/> truth doubled and then divided by the sum <a id="_idIndexMarker379"/>of the prediction and ground truth areas:</p>
    <figure class="mediaobject"><img src="../Images/B17574_05_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.7: Visual representation of the Dice calculation</p>
    <p class="normal">In this case, with respect to the Jaccard index, you do not take into account the overlap of the prediction with the ground truth in the denominator. Here, the expectation is that, as you maximize the area of overlap, you predict the correct area size. Again, you are penalized if you predict areas larger than you should be predicting. In fact, the two metrics are positively correlated and they produce almost the same results for a single classification problem.</p>
    <p class="normal">The differences actually arise when you are working with multiple classes. In fact, both with IoU and the Dice coefficient, when you have multiple classes you average the result of all of them. However, in doing so, the IoU metric tends to penalize the overall average more if a single class prediction is wrong, whereas the Dice coefficient is more lenient and tends to represent the average performance.</p>
    <div class="note">
      <p class="normal">Examples of Kaggle competitions using the Dice coefficient (it is often encountered in competitions with medical purposes, but not necessarily only there, because it can also be used for clouds and cars):</p>
      <ul>
        <li class="bulletList"><em class="italic">HuBMAP - Hacking the Kidney</em>: <a href="https://www.kaggle.com/c/hubmap-kidney-segmentation"><span class="url">https://www.kaggle.com/c/hubmap-kidney-segmentation</span></a></li>
        <li class="bulletList"><em class="italic">Ultrasound Nerve Segmentation</em>: <a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation"><span class="url">https://www.kaggle.com/c/ultrasound-nerve-segmentation</span></a></li>
        <li class="bulletList"><em class="italic">Understanding Clouds from Satellite Images</em>: <a href="https://www.kaggle.com/c/understanding_cloud_organization"><span class="url">https://www.kaggle.com/c/understanding_cloud_organization</span></a></li>
        <li class="bulletList"><em class="italic">Carvana Image Masking Challenge</em>: <a href="https://www.kaggle.com/c/carvana-image-masking-challenge"><span class="url">https://www.kaggle.com/c/carvana-image-masking-challenge</span></a></li>
      </ul>
    </div>
    <p class="normal">IoU and Dice constitute the basis for all the more complex metrics in segmentation and object detection. By choosing an appropriate threshold level for IoU or Dice (usually 0.5), you can decide whether<a id="_idIndexMarker380"/> or not to confirm a detection, therefore a classification. At this point, you can use previously discussed metrics for classification, such as precision, recall, and <em class="italic">F</em>1, such as is done in popular object detection and segmentation challenges such as Pascal VOC (<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012"><span class="url">http://host.robots.ox.ac.uk/pascal/VOC/voc2012</span></a>) or COCO (<a href="https://cocodataset.org"><span class="url">https://cocodataset.org</span></a>).</p>
    <h1 id="_idParaDest-86" class="heading-1">Metrics for multi-label classification and recommendation problems</h1>
    <p class="normal">Recommender systems <a id="_idIndexMarker381"/>are one of the most popular applications of data analysis and machine learning, and there are quite a few competitions on Kaggle that have used the recommendation approach. For instance, the <em class="italic">Quick, Draw! Doodle Recognition Challenge</em> was a prediction evaluated as a recommender system. Some other competitions on Kaggle, however, truly <a id="_idIndexMarker382"/>strived to build effective recommender systems (such as <em class="italic">Expedia Hotel Recommendations</em>: <a href="https://www.kaggle.com/c/expedia-hotel-recommendations"><span class="url">https://www.kaggle.com/c/expedia-hotel-recommendations</span></a>) and RecSYS, the conference on recommender systems (<a href="https://recsys.acm.org/"><span class="url">https://recsys.acm.org/</span></a>), even hosted one of its yearly contests on Kaggle (<em class="italic">RecSYS 2013</em>: <a href="https://www.kaggle.com/c/yelp-recsys-2013"><span class="url">https://www.kaggle.com/c/yelp-recsys-2013</span></a>).</p>
    <p class="normal"><strong class="keyWord">Mean Average Precision at K </strong>(<strong class="keyWord">MAP@{K}</strong>) is typically<a id="_idIndexMarker383"/> the metric of choice for evaluating the performance of recommender systems, and it is the most common metric you will encounter on Kaggle in all the competitions that try to build or approach a problem as a recommender system. </p>
    <p class="normal">There are also some other metrics, such as the <strong class="keyWord">precision at k</strong>, or <strong class="keyWord">P@K</strong>, and the <strong class="keyWord">average precision at k</strong>, or <strong class="keyWord">AP@K</strong>, which are loss functions, in other words, computed at the level of each single prediction. Understanding how they work can help you better understand the MAP@K and how it can perform both in recommendations and in multi-label classification.</p>
    <p class="normal">In fact, analogous to recommender systems, multi-label classifications imply that your model outputs a series of class predictions. Such results could be evaluated using some average of some binary classification metrics (such as in <em class="italic">Greek Media Monitoring Multilabel Classification (WISE 2014)</em>, which used the mean <em class="italic">F</em>1 score: <a href="https://www.kaggle.com/c/wise-2014"><span class="url">https://www.kaggle.com/c/wise-2014</span></a>) as well as metrics that are more typical of recommender systems, such as MAP@K. In the end, you can deal with both recommendations and multi-label predictions as <em class="italic">ranking tasks</em>, which translates into a set of ranked suggestions in a recommender system and into a set of labels (without a precise order) in multi-label classification.</p>
    <h2 id="_idParaDest-87" class="heading-2">MAP@{K}</h2>
    <p class="normal">MAP@K is a complex metric <a id="_idIndexMarker384"/>and it derives from many computations. In order to understand the MAP@K metric fully, let’s start<a id="_idIndexMarker385"/> with its simplest component, the <strong class="keyWord">precision at </strong><strong class="bold-italic" style="font-style: italic;">k</strong> (<strong class="keyWord">P@K</strong>). In this case, since the prediction for an example is a ranked sequence of predictions (from the most probable to the least), the function takes into account only the top <em class="italic">k</em> predictions, then it computes how many matches it got with respect to the ground truth and divides that number by <em class="italic">k</em>. In a few words, it is quite similar to an accuracy measure averaged over <em class="italic">k</em> predictions.</p>
    <p class="normal">A bit more complex in terms <a id="_idIndexMarker386"/>of computation, but conceptually simple, the <strong class="keyWord">average precision at </strong><strong class="bold-italic" style="font-style: italic;">k</strong> (<strong class="keyWord">AP@K</strong>) is the average of P@K computed over all the values ranging from <em class="italic">1</em> to <em class="italic">k</em>. In this way, the metric evaluates how well the prediction works overall, using the top prediction, then the top two predictions, and so on until the top <em class="italic">k</em> predictions.</p>
    <p class="normal">Finally, <strong class="keyWord">MAP@K</strong> is the mean of the AP@K for the entire predicted sample, and it is a metric because it comprises all the predictions in its evaluation. Here is the MAP@5 formulation you can find in the <em class="italic">Expedia Hotel Recommendations</em> competition (<a href="https://www.kaggle.com/c/expedia-hotel-recommendations"><span class="url">https://www.kaggle.com/c/expedia-hotel-recommendations</span></a>):</p>
    <p class="center"><img src="../Images/B17574_05_037.png" alt="" style="height: 5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">In the formula, <img src="../Images/B17574_05_038.png" alt=""/> is the number of user recommendations, <em class="italic">P(k)</em> is the precision at cutoff <em class="italic">k</em>, and <em class="italic">n</em> is the number of predicted hotel clusters (you could predict up to 5 hotels for each recommendation). </p>
    <p class="normal">It is clearly a bit more daunting than our <a id="_idIndexMarker387"/>explanation, but the formula just expresses that the MAP@K is the mean of all the AP@K evaluations over all the predictions.</p>
    <p class="normal">Having completed this overview of specific metrics for different regression and classification metrics, let’s discuss how to deal with evaluation metrics in a Kaggle competition.</p>
    <h1 id="_idParaDest-88" class="heading-1">Optimizing evaluation metrics</h1>
    <p class="normal">Summing up what we have<a id="_idIndexMarker388"/> discussed so far, an objective function is a function inside your learning algorithm that measures how well the algorithm’s internal model is fitting the provided data. The objective function also provides feedback to the algorithm in order for it to improve its fit across successive iterations. Clearly, since the entire algorithm’s efforts are recruited to perform well based on the objective function, if the Kaggle evaluation metric perfectly matches the objective function of your algorithm, you will get the best results.</p>
    <p class="normal">Unfortunately, this is not frequently the case. Often, the evaluation metric provided can only be approximated by existing objective functions. Getting a good approximation, or striving to get your predictions performing better with respect to the evaluation criteria, is the secret to performing well in Kaggle competitions. When your objective function does not match your evaluation metric, you have a few alternatives:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Modify your learning algorithm and have it incorporate an objective function that matches your evaluation metric, though this is not possible for all algorithms (for instance, algorithms such as LightGBM and XGBoost allow you to set custom objective functions, but most Scikit-learn models don’t allow this).</li>
      <li class="numberedList">Tune your model’s hyperparameters, choosing the ones that make the result shine the most when using the evaluation metric.</li>
      <li class="numberedList">Post-process your results so they match the evaluation criteria more closely. For instance, you could code an optimizer that performs transformations on your predictions (probability calibration algorithms are an example, and we will discuss them at the end of the chapter).</li>
    </ol>
    <p class="normal">Having the competition metric incorporated into your machine learning algorithm is really the most effective method to achieve better predictions, though only a few algorithms can be hacked into using the competition metric as your objective function. The second approach is <a id="_idIndexMarker389"/>therefore the more common one, and many competitions end up in a struggle to get the best hyperparameters for your models to perform on the evaluation metric. </p>
    <p class="normal">If you already have your evaluation function coded, then doing the right cross-validation or choosing the appropriate test set plays the lion share. If you don’t have the coded function at hand, you have to first code it in a suitable way, following the formulas provided by Kaggle.</p>
    <p class="normal">Invariably, doing the following will make the difference:</p>
    <ul>
      <li class="bulletList">Looking for all the relevant information about the evaluation metric and its coded function on a search engine</li>
      <li class="bulletList">Browsing through the most common packages (such as Scikit-learn: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation"><span class="url">https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation</span></a> or TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/losses</span></a>)</li>
      <li class="bulletList">Browsing GitHub projects (for instance, <em class="italic">Ben Hammer’s</em> Metrics project: <a href="https://github.com/benhamner/Metrics"><span class="url">https://github.com/benhamner/Metrics</span></a>)</li>
      <li class="bulletList">Asking or looking around in the forums and available Kaggle Notebooks (both for the current competition and for similar competitions)</li>
      <li class="bulletList">In addition, as we mentioned before, querying the Meta Kaggle dataset (<a href="https://www.kaggle.com/kaggle/meta-kaggle"><span class="url">https://www.kaggle.com/kaggle/meta-kaggle</span></a>) and looking in the <strong class="screenText">Competitions</strong> table will help you find out which other Kaggle competitions used that same evaluation metric, and immediately provides you with useful code and ideas to try out</li>
    </ul>
    <p class="normal">Let’s discuss in greater detail the alternatives you have when your evaluation metric doesn’t match your algorithm’s objective function. We’ll start by exploring custom metrics.</p>
    <h2 id="_idParaDest-89" class="heading-2">Custom metrics and custom objective functions</h2>
    <p class="normal">As a first option when your objective function does not match your evaluation metric, we learned above that you<a id="_idIndexMarker390"/> can solve this by creating your own custom objective function, but that only a few algorithms can easily be modified to incorporate a specific objective function.</p>
    <p class="normal">The good news is that the<a id="_idIndexMarker391"/> few algorithms that allow this are among the most effective ones in Kaggle competitions and data science projects. Of course, creating your own custom objective function may sound a little bit tricky, but it is an incredibly rewarding approach to increasing your score in a competition. For instance, there are options to do this when using gradient boosting algorithms such as XGBoost, CatBoost, and LightGBM, as well as with all deep learning models based on TensorFlow or PyTorch.</p>
    <p class="normal">You can find great tutorials for custom metrics and objective functions in TensorFlow and PyTorch here:</p>
    <ul>
      <li class="bulletList"><a href="https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279"><span class="url">https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279</span></a></li>
      <li class="bulletList"><a href="https://petamind.com/advanced-keras-custom-loss-functions/"><span class="url">https://petamind.com/advanced-keras-custom-loss-functions/</span></a></li>
      <li class="bulletList"><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/"><span class="url">https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/</span></a></li>
    </ul>
    <p class="normal">These will provide you with the basic function templates and some useful suggestions about how to code a custom objective or evaluation function.</p>
    <div class="packt_tip">
      <p class="normal">If you want just to get straight to the custom objective function you need, you can try this Notebook by RNA (<a href="https://www.kaggle.com/bigironsphere"><span class="url">https://www.kaggle.com/bigironsphere</span></a>): <a href="https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook"><span class="url">https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook</span></a>. It contains a large range of custom loss functions for both TensorFlow and PyTorch that have appeared in different competitions.</p>
    </div>
    <p class="normal">If you need to create a custom loss in LightGBM, XGBoost, or CatBoost, as indicated in their respective documentation, you have to code a function that takes as inputs the prediction and the ground truth, and that returns as outputs the gradient and the hessian.</p>
    <div class="note">
      <p class="normal">You can consult this post on Stack Overflow for a better understanding of what a gradient and a hessian are: <a href="https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based"><span class="url">https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based</span></a>.</p>
    </div>
    <p class="normal">From a code implementation<a id="_idIndexMarker392"/> perspective, all you have to do is to<a id="_idIndexMarker393"/> create a function, using closures if you need to pass more parameters beyond just the vector of predicted labels and true labels. Here is a simple example of a <strong class="keyWord">focal loss</strong> (a loss that aims to heavily weight the minority class in the loss computations as described in Lin, T-Y. et al. <em class="italic">Focal loss for dense object detection</em>: <a href="https://arxiv.org/abs/1708.02002"><span class="url">https://arxiv.org/abs/1708.02002</span></a>) function that you can use as a model for your own custom functions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy.misc <span class="hljs-keyword">import</span> derivative
<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">focal_loss</span><span class="hljs-function">(</span><span class="hljs-params">alpha, gamma</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss_func</span><span class="hljs-function">(</span><span class="hljs-params">y_pred, y_true</span><span class="hljs-function">):</span>
        a, g = alpha, gamma
        <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_loss</span><span class="hljs-function">(</span><span class="hljs-params">y_pred, y_true</span><span class="hljs-function">):</span>
            p = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pred))
            loss = (-(a * y_true + (<span class="hljs-number">1</span> - a)*(<span class="hljs-number">1</span> - y_true)) * 
                    ((<span class="hljs-number">1</span> - (y_true * p + (<span class="hljs-number">1</span> - y_true) * 
                     (<span class="hljs-number">1</span> - p)))**g) * (y_true * np.log(p) + 
                    (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - p)))
            <span class="hljs-keyword">return</span> loss
        partial_focal = <span class="hljs-keyword">lambda</span> y_pred: get_loss(y_pred, y_true)
        grad = derivative(partial_focal, y_pred, n=<span class="hljs-number">1</span>, dx=<span class="hljs-number">1e-6</span>)
        hess = derivative(partial_focal, y_pred, n=<span class="hljs-number">2</span>, dx=<span class="hljs-number">1e-6</span>)
        <span class="hljs-keyword">return</span> grad, hess
    <span class="hljs-keyword">return</span> loss_func
xgb = xgb.XGBClassifier(objective=focal_loss(alpha=<span class="hljs-number">0.25</span>, gamma=<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">In the above code snippet, we have defined a new cost function, <code class="inlineCode">focal_loss</code>, which is then fed into an XGBoost instance’s object parameters. The example is worth showing because the focal loss requires the specification of some parameters in order to work properly on your problem (alpha and gamma). The more simplistic solution of having their values directly coded into the function is not ideal, since you may have to change them systematically as you are tuning your model. Instead, in the proposed function, when you input the parameters into the <code class="inlineCode">focal_loss</code> function, they reside in memory and they are referenced by the <code class="inlineCode">loss_func</code> function that is returned to XGBoost. The returned cost function, therefore, will work, referring to the alpha and gamma values that you have initially instantiated.</p>
    <p class="normal">Another interesting aspect of the<a id="_idIndexMarker394"/> example is that it really makes it easy to compute the gradient and the hessian of the cost function by means of the derivative function from SciPy. If your cost function is differentiable, you don’t have to<a id="_idIndexMarker395"/> worry about doing any calculations by hand. However, creating a custom objective function requires some mathematical knowledge and quite a lot of effort to make sure it works properly for your purposes. You can read about the difficulties that <em class="italic">Max Halford</em> experienced while implementing a focal loss for the LightGBM algorithm, and how he overcame them, here: <a href="https://maxhalford.github.io/blog/lightgbm-focal-loss/"><span class="url">https://maxhalford.github.io/blog/lightgbm-focal-loss/</span></a>. Despite the difficulty, being able to conjure up a custom loss can really determine your success in a Kaggle competition where you have to extract the maximum possible result from your model.</p>
    <p class="normal">If building your own objective function isn’t working out, you can simply lower your ambitions, give up building your function as an objective function used by the optimizer, and instead code it as a custom <em class="italic">evaluation metric</em>. Though your model won’t be directly optimized to perform against this function, you can still improve its predictive performance with hyperparameter optimization based on it. This is the second option we talked about in the previous section.</p>
    <p class="normal">Just remember, if you are writing a metric from scratch, sometimes you may need to abide by certain code conventions for your function to work properly. For instance, if you use Scikit-learn, you have to convert your functions using the <code class="inlineCode">make_scorer</code> function. The <code class="inlineCode">make_scorer</code> function is actually a wrapper that makes your evaluation function suitable for working with the Scikit-learn API. It will wrap your function while considering some meta-information, such as whether to use probability estimates or predictions, whether you need to specify a threshold for prediction, and, last but not least, the directionality of the optimization, that is, whether you want to maximize or minimize the score it returns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> make_scorer
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> average_precision_score
scorer = make_scorer(average_precision_score, 
average=<span class="hljs-string">'weighted'</span>, greater_is_better=<span class="hljs-literal">True</span>, needs_proba=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">In the above example, you prepare a scorer based on the average precision metric, specifying that it should use a<a id="_idIndexMarker396"/> weighted computation when <a id="_idIndexMarker397"/>dealing with multi-class classification problems.</p>
    <div class="note">
      <p class="normal">If you are optimizing for your evaluation metric, you can apply grid search, random search, or some more sophisticated optimization such as Bayesian optimization and find the set of parameters that makes your algorithm perform optimally for your evaluation metric, even if it works with a different cost function. We will explore how to best arrange parameter optimization and obtain the best results on Kaggle competitions after having discussed model validation, specifically in the chapter dealing with tabular data problems.</p>
    </div>
    <h2 id="_idParaDest-90" class="heading-2">Post-processing your predictions</h2>
    <p class="normal">Post-processing tuning<a id="_idIndexMarker398"/> implies that your predictions are transformed, by means of a function, into something else in order to present a better evaluation. After building your custom loss or optimizing for your evaluation metric, you can also improve your results by leveraging the characteristics of your evaluation metric using a specific function applied to your predictions. Let’s take the Quadratic Weighted Kappa, for instance. We mentioned previously that this metric is useful when you have to deal with the prediction of an ordinal value. To recap, the original Kappa coefficient is a chance-adjusted index of agreement between the algorithm and the ground truth. It is a kind of accuracy measurement corrected by the probability that the match between the prediction and the ground truth is due to a fortunate chance.</p>
    <p class="normal">Here is the original version of the Kappa coefficient, as seen before:</p>
    <p class="center"><img src="../Images/B17574_05_039.png" alt="" style="height: 1.5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">In the formula, <em class="italic">p</em><sub class="subscript">0</sub> is the relative observed agreement among raters, and <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">e</sub> is the hypothetical probability of chance agreement. Here, you need just two matrices, the one with the observed scores and the one with the expected scores based on chance agreement. When the Kappa coefficient is weighted, you also consider a weight matrix and the formula turns into this:</p>
    <p class="center"><img src="../Images/B17574_05_040.png" alt="" style="height: 1.5em !important; vertical-align: -0.30em !important;"/></p>
    <p class="normal">The matrix <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">p</sub> contains the penalizations to weight errors differently, which is very useful for ordinal predictions since this matrix can penalize much more when the predictions deviate further from the ground truths. Using the quadratic form, that is, squaring the resulting <em class="italic">k</em>, makes the <a id="_idIndexMarker399"/>penalization even more severe. However, optimizing for such a metric is really not easy, since it is very difficult to implement it as a cost function. Post-processing can help you.</p>
    <p class="normal">An example can be found in the <em class="italic">PetFinder.my Adoption Prediction</em> competition (<a href="https://www.kaggle.com/c/petfinder-adoption-prediction"><span class="url">https://www.kaggle.com/c/petfinder-adoption-prediction</span></a>). In this competition, given that the results could have 5 possible ratings (0, 1, 2, 3, or 4), you could deal with them either using a classification or a regression. If you used a regression, a post-processing transformation of the regression output could improve the model’s performance against the Quadratic Weighted Kappa metric, outperforming the results you could get from a classification directly outputting discrete predictions.</p>
    <p class="normal">In the case of the PetFinder competition, the post-processing consisted of an optimization process that started by transforming the regression results into integers, first using the boundaries [0.5, 1.5, 2.5, 3.5] as thresholds and, by an iterative fine-tuning, finding a better set of boundaries that maximized the performance. The fine-tuning of the boundaries required the computations of an optimizer such as SciPy’s <code class="inlineCode">optimize.minimize</code>, which is based on the Nelder-Mead algorithm. The boundaries found by the optimizer were validated by a cross-validation scheme. You can read more details about this post-processing directly from the post made by <em class="italic">Abhishek Thakur</em> during the competition: <a href="https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107"><span class="url">https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107</span></a>.</p>
    <div class="note">
      <p class="normal">Aside from the PetFinder competition, many other competitions have demonstrated that smart post-processing can lead to improved results and rankings. We’ll point out a few examples here:</p>
      <ul>
        <li class="bulletList"><a href="https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw"><span class="url">https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw</span></a></li>
        <li class="bulletList"><a href="https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage"><span class="url">https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage</span></a></li>
        <li class="bulletList"><a href="https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization"><span class="url">https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization</span></a></li>
      </ul>
    </div>
    <p class="normal">Unfortunately, post-processing is often very dependent on the metric you are using (understanding the metric is imperative for devising any good post-processing) and often also data-specific, for instance, in the case of time series data and leakages. Hence, it is very difficult to generalize <a id="_idIndexMarker400"/>any procedure for figuring out the right post-processing for any competition. Nevertheless, always be aware of this possibility and be on the lookout in a competition for any hint that post-processing results is favorable. You can always get hints about post-processing from previous competitions that have been similar, and by forum discussion – eventually, someone will raise the topic.</p>
    <h3 id="_idParaDest-91" class="heading-3">Predicted probability and its adjustment</h3>
    <p class="normal">To complete the above <a id="_idIndexMarker401"/>discussion on metrics optimization (post-processing of predictions), we will discuss situations where it is paramount to predict correct probabilities, but you are not sure if the algorithm you are using is doing a good job. As we detailed previously, classification probabilities concern both binary and multiclass classification problems, and they are commonly evaluated using the logarithmic loss (aka log loss or logistic loss or cross-entropy loss) in its binary or multi-class version (for more details, see the previous sections on <em class="italic">Metrics for classification (label prediction and probability)</em> and <em class="italic">Metrics for multi-class classification</em>).</p>
    <p class="normal">However, evaluating or optimizing for the log loss may not prove enough. The main problems to be on the lookout for when striving to achieve correct probabilistic predictions with your model are:</p>
    <ul>
      <li class="bulletList">Models that do not return a truly probabilistic estimate</li>
      <li class="bulletList">Unbalanced distribution of classes in your problem</li>
      <li class="bulletList">Different class distribution between your training data and your test data (on both public and private leaderboards)</li>
    </ul>
    <p class="normal">The first point alone provides reason to check and verify the quality of classification predictions in terms of modeled uncertainty. In fact, even if many algorithms are provided in the Scikit-learn package together with a <code class="inlineCode">predict_proba</code> method, this is a very weak assurance that they will return a true probability.</p>
    <p class="normal">Let’s take, for instance, decision trees, which are the basis of many effective methods to model tabular data. The probability outputted by a classification decision tree (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</span></a>) is based on terminal leaves; that is, it depends on the distribution of classes on the leaf that contains the case to be predicted. If the tree is fully grown, it is highly likely that the case is in a small leaf with very few other cases, so the predicted probability will be very high. If you change parameters such as <code class="inlineCode">max_depth</code>, <code class="inlineCode">max_leaf_nodes</code>, or <code class="inlineCode">min_samples_leaf</code>, the resulting probability will drastically change from higher values to lower ones depending on the growth of the tree.</p>
    <p class="normal">Decision trees are the most<a id="_idIndexMarker402"/> common base model for ensembles such as bagging models and random forests, as well as boosted models such as gradient boosting (with its high-performing implementations XGBoost, LightGBM, and CatBoost). But, for the same reasons – probability estimates that are not truly based on solid probabilistic estimations – the problem affects many other commonly used models, such as support-vector machines and <em class="italic">k</em>-nearest neighbors. Such aspects were mostly unknown to Kagglers until the <em class="italic">Otto Group Product Classification Challenge</em> (<a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/"><span class="url">https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/</span></a>), when it was raised by <em class="italic">Christophe Bourguignat</em> and others during the competition (see <a href="https://www.kaggle.com/cbourguignat/why-calibration-works"><span class="url">https://www.kaggle.com/cbourguignat/why-calibration-works</span></a>), and easily solved at the time using the calibration functions that had recently been added to Scikit-learn.</p>
    <p class="normal">Aside from the model you will be using, the<a id="_idIndexMarker403"/> presence of imbalance between classes in your problem may also result in models that are not at all reliable. Hence, a good approach in the case of unbalanced classification problems is to rebalance the classes using undersampling or oversampling strategies, or different custom weights for each class to be applied when the loss is computed by the algorithm. All these strategies may render your model more performant; however, they will surely distort the probability estimates and you may have to adjust them in order to obtain an even better model score on the leaderboard.</p>
    <p class="normal">Finally, a third point of concern is related to how the test set is distributed. This kind of information is usually concealed, but there are often ways to estimate it and figure it out (for instance, by trial and error based on the public leaderboard results, as we mentioned in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Kaggle and Other Data Science Competitions</em>). </p>
    <p class="normal">For instance, this happened in the <em class="italic">iMaterialist Furniture Challenge</em> (<a href="https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/"><span class="url">https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/</span></a>) and the more popular <em class="italic">Quora Question Pairs</em> (<a href="https://www.kaggle.com/c/quora-question-pairs"><span class="url">https://www.kaggle.com/c/quora-question-pairs</span></a>). Both competitions gave rise to various discussions on how to post-process in order to adjust probabilities to test expectations (see <a href="https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/"><span class="url">https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/</span></a> and <a href="https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb"><span class="url">https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb</span></a> for more details on the method used). From a general point of view, assuming that you do not have an idea of the test distribution of classes to be predicted, it is still very beneficial to correctly predict probability based on the priors you get from the training data (and until you get evidence to the contrary, that is the probability distribution that <a id="_idIndexMarker404"/>your model should mimic). In fact, it will be much easier to correct your predicted probabilities if your predicted probability distribution matches those in the training set.</p>
    <p class="normal">The solution, when your predicted probabilities are misaligned with the training distribution of the target, is to use the <a id="_idIndexMarker405"/><strong class="keyWord">calibration function</strong> provided by Scikit-learn, <code class="inlineCode">CalibratedClassifierCV</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">sklearn.calibration.CalibratedClassifierCV(base_estimator=<span class="hljs-literal">None</span>, *,
    method=<span class="hljs-string">'sigmoid'</span>, cv=<span class="hljs-literal">None</span>, n_jobs=<span class="hljs-literal">None</span>, ensemble=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The purpose of the calibration function is to apply a post-processing function to your predicted probabilities in order to make them adhere more closely to the empirical probabilities seen in the ground truth. Provided that your model is a Scikit-learn model or behaves similarly to one, the function will act as a wrapper for your model and directly pipe its predictions into a post-processing function. You have the choice between using two methods for post-processing. The<a id="_idIndexMarker406"/> first is the <strong class="keyWord">sigmoid</strong> method (also called Plat’s scaling), which<a id="_idIndexMarker407"/> is nothing more than a logistic regression. The second is<a id="_idIndexMarker408"/> the <strong class="keyWord">isotonic regression</strong>, which is a non-parametric regression; beware that it tends to overfit if there are few examples.</p>
    <p class="normal">You also have to choose how to fit this calibrator. Remember that it is a model that is applied to the results of your model, so you <a id="_idIndexMarker409"/>have to avoid overfitting by systematically reworking predictions. You could use a <strong class="keyWord">cross-validation</strong> (more on this in the following chapter on <em class="chapterRef">Designing Good Validation</em>) and then produce a number of models that, once averaged, will provide your predictions (<code class="inlineCode">ensemble=True</code>). Otherwise, and this is our usual choice, resort to an <a id="_idIndexMarker410"/><strong class="keyWord">out-of-fold prediction</strong> (more on this in the following chapters) and calibrate on that using all the data available (<code class="inlineCode">ensemble=False</code>).</p>
    <p class="normal">Even if <code class="inlineCode">CalibratedClassifierCV</code> can handle most situations, you can also figure out some empirical way to fix probability estimates for the best performance at test time. You can use any transformation function, from a handmade one to a sophisticated one derived by genetic algorithms, for instance. Your only limit is simply that you should cross-validate it and possibly have a good final result from the public leaderboard (but not necessarily, because you <a id="_idIndexMarker411"/>should trust your local cross-validation score more, as we are going to discuss in the next chapter). A good example of such a strategy is provided by Silogram (<a href="https://www.kaggle.com/psilogram"><span class="url">https://www.kaggle.com/psilogram</span></a>), who, in the <em class="italic">Microsoft Malware Classification Challenge</em>, found out a way to tune the unreliable probabilistic outputs of random forests into probabilistic ones simply by raising the output to a power determined by grid search (see <a href="https://www.kaggle.com/c/malware-classification/discussion/13509"><span class="url">https://www.kaggle.com/c/malware-classification/discussion/13509</span></a>).</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Sudalai_Rajkumar.png" alt=""/>
      </div>
      <p class="intervieweeName">Sudalai Rajkumar</p>
      <p class="normal"><a href="https://www.kaggle.com/sudalairajkumar"><span class="url">https://www.kaggle.com/sudalairajkumar</span></a></p>
      <p class="normal">In our final interview of the chapter, we speak to Sudalai Rajkumar, SRK, a Grandmaster in Competitions, Datasets, and Notebooks, and a Discussion Master. He is ranked #1 in the Analytics Vidhya data science platform, and works as an AI/ML advisor for start-ups.</p>
      <p class="interviewHeader">What’s your favourite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">My favorite kinds of competition are ones that involve a good amount of feature engineering. I think that is my strength as well. I am generally interested in data exploration to get a deep understanding of the data (which you can infer from my series of simple exploration Notebooks (</em><a href="https://www.kaggle.com/sudalairajkumar/code"><span class="url">https://www.kaggle.com/sudalairajkumar/code</span></a><em class="italic">)) and then creating features based on it.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work?</p>
      <p class="normal"><em class="italic">The framework for a competition involves data exploration, finding the right validation method, feature engineering, model building, and ensembling/stacking. All these are involved in my day job as well. But in addition to this, there is a good amount of stakeholder discussion, data collection, data tagging, model deployment, model monitoring, and data storytelling that is involved in my daily job.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal">Santander Product Recommendation<em class="italic"> is a memorable competition that we entered. Rohan &amp; I did a lot of feature engineering and built multiple models. When we did final ensembling, we used different weights for different products and some of them did not add up to 1. From the data exploration and understanding, we hand-picked these weights, which helped us. This made us realise the domain/data importance in solving problems and how data science is an art as much as science.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Kaggle played a very important role in my career. I was able to secure my last two jobs mainly because of Kaggle. Also, the success from Kaggle helps to connect with other stalwarts in the data science field easily and learn from them. It also helps a lot in my current role as AI / ML advisor for start-ups, as it gives credibility.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">Understanding the data in depth. Often this is overlooked, and people get into model-building right away. Exploring the data plays a very important role in the success of any Kaggle competition. This helps to create proper cross validation and to create better features and to extract more value from the data.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">It is a very big list, and I would say that they are learning opportunities. In every competition, out of 20-30 ideas that I try, only 1 may work. These mistakes/failures give much more learning than the actual success or things that worked. For example, I learnt about overfitting the very hard way by falling from top deciles to bottom deciles in one of my very first competitions. But that learning stayed with me forever thereafter.</em></p>
      <p class="normal"><em class="italic">Are there any particular tools or libraries that you would recommend using for data analysis/machine learning?</em></p>
      <p class="normal"><em class="italic">I primarily use XGBoost/LightGBM in the case of tabular data. I also use open source AutoML libraries and Driverless AI to get early benchmarks these days. I use Keras, Transformers, and PyTorch for deep learning models.</em></p>

      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Consistency is the key. Each competition will have its own ups and downs. There will be multiple days without any progress, but we should not give up and keep trying. I think this is applicable for anything and not just Kaggle competitions.</em></p>
      <p class="interviewHeader">Do you use other competition platforms? How do they compare to Kaggle?</p>
      <p class="normal"><em class="italic">I have also taken part on other platforms like the Analytics Vidhya DataHack platform, Driven Data, CrowdAnalytix etc. They are good too, but Kaggle is more widely adopted and global in nature, so the amount of competition on Kaggle is much higher compared to other platforms.</em></p>
    </div>
    <h1 id="_idParaDest-92" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we have discussed evaluation metrics in Kaggle competitions. First, we explained how an evaluation metric can differ from an objective function. We also remarked on the differences between regression and classification problems. For each type of problem, we analyzed the most common metrics that you can find in a Kaggle competition.</p>
    <p class="normal">After that, we discussed the metrics that have never previously been seen in a competition and that you won’t likely see again. Finally, we explored and studied different common metrics, giving examples of where they have been used in previous Kaggle competitions. We then proposed a few strategies for optimizing an evaluation metric. In particular, we recommended trying to code your own custom cost functions and provided suggestions on possible useful post-processing steps.</p>
    <p class="normal">You should now have grasped the role of an evaluation metric in a Kaggle competition. You should also have a strategy to deal with every common or uncommon metric, by retracing past competitions and by gaining a full understanding of the way a metric works. In the next chapter, we are going to discuss how to use evaluation metrics and properly estimate the performance of your Kaggle solution by means of a validation strategy.</p>
    <h1 id="_idParaDest-93" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>