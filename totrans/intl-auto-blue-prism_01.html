<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer042">
			<h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Machine Learning as a Service: the Digital Exchange and Web APIs</h1>
			<p>Most of<a id="_idIndexMarker000"/> how we interact with ML today is done through <strong class="bold">web APIs</strong>. Even when<a id="_idIndexMarker001"/> using <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) chatbots through a web browser, web API calls are being made in the background to give us the reply. More often than not, your BP Processes will also use web APIs to obtain the results of an ML prediction that’s <span class="No-Break">hosted online.</span></p>
			<p>In this chapter, we’re going to explore the most popular<a id="_idIndexMarker002"/> ML web APIs for IA, how to find them on BP’s <strong class="bold">Digital Exchange</strong> (<strong class="bold">DX</strong>), how to connect them to BP, and how to build one yourself so that predictions can be made in your automation use cases. More specifically, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understanding what the most common ML services are, some of their common use cases, and how to find them on <span class="No-Break">the DX</span></li>
				<li>Going through two examples of using pre-built downloadable assets from the DX to make <span class="No-Break">ML predictions</span></li>
				<li>Building a BP <strong class="bold">Web API Service</strong> from scratch to connect to an ML service that is not currently available on <span class="No-Break">the DX</span></li>
			</ul>
			<p>By the end of the <a id="_idIndexMarker003"/>chapter, we’ll have covered examples of three of<a id="_idIndexMarker004"/> the most commonly used <strong class="bold">machine learning as a service</strong> (<strong class="bold">MLaaS</strong>) platforms: <strong class="bold">Amazon Web Service</strong> (<strong class="bold">AWS</strong>), <strong class="bold">Azure</strong>, and <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>). These examples also cover some of the most<a id="_idIndexMarker005"/> common IA use<a id="_idIndexMarker006"/> cases: extracting data from unstructured text, extracting data from forms, and extracting text from images. We will have also covered some of the key concepts that will inform our solution design in the future: <strong class="bold">single</strong> versus <strong class="bold">batch</strong> and <strong class="bold">synchronous</strong> versus <span class="No-Break"><strong class="bold">asynchronous</strong></span><span class="No-Break"> predictions.</span></p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor017"/>Technical requirements</h1>
			<p>For this chapter, ensure that the following are <span class="No-Break">in place:</span></p>
			<ul>
				<li>A valid Blue Prism Portal (<a href="https://portal.blueprism.com">https://portal.blueprism.com</a>) account. This is required to download assets from the DX. An account can be created free <span class="No-Break">of charge.</span></li>
				<li>An active account at AWS, Azure, and GCP. We will go over examples using each vendor in this chapter. All examples can be run within the free tiers offered by <span class="No-Break">the services.</span></li>
				<li>Download the following file from GitHub at <a href="https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_1_to_3.bprelease">https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_1_to_3.bprelease</a>. Import the <strong class="source-inline">.bprelease</strong> file into BP. This contains sample Processes that will be used in <span class="No-Break">our examples.</span></li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Using the DX</h1>
			<p>The <a id="_idIndexMarker007"/>DX is BP’s marketplace, containing many BP-developed and community-submitted <strong class="bold">assets</strong> that<a id="_idIndexMarker008"/> allow you to quickly connect to cloud-hosted MLaaS APIs. An <em class="italic">asset</em> is DX terminology, which means <em class="italic">something that can be imported into BP</em>. Assets can be in any of the BP file formats, including <strong class="source-inline">.bprelease</strong>, <strong class="source-inline">.bpskill</strong>, <strong class="source-inline">.bpobject</strong>, <strong class="source-inline">.bpprocess</strong>, and <strong class="source-inline">.xml</strong>. Most of the assets on the DX are free of charge to download and integrate; however, there are likely costs associated with using the ML API services themselves. Using pre-built assets from the DX is the <em class="italic">easiest and fastest way</em> to get ML into your automated business processes, provided the service fits your <span class="No-Break">use case.</span></p>
			<p>In this section, we’ll see what popular ML services are available on the DX and what the potential use cases are. Real-life use case examples are provided whenever possible, drawn from my own research examining over 100 IA use cases <span class="No-Break">and technologies.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>Accessing the DX</h2>
			<p>The <a id="_idIndexMarker009"/>DX can<a id="_idIndexMarker010"/> be accessed at <a href="https://digitalexchange.blueprism.com">https://digitalexchange.blueprism.com</a>. You’ll need to log in using your BP Portal account credentials to <span class="No-Break">download assets.</span></p>
			<p>There’s much more available on the DX than just web APIs. Since we’re only interested in web APIs for this chapter, click on <strong class="bold">More Filters</strong> and filter based on <strong class="bold">Connector</strong> to narrow down the <span class="No-Break">search results:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B18416_01_1.jpg" alt="Figure 1.1 – Filtering for web APIs on the DX" width="1627" height="611"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Filtering for web APIs on the DX</p>
			<p>The <strong class="bold">Search for Assets</strong> search box can then be used to find specific ML services <em class="italic">if you already know the service name</em>. Later, I’ll provide a summary of all of the most popular search terms you can use to find ML assets on the DX based on their use case. But first, let’s discuss some fundamentals that are needed to use ML <span class="No-Break">web APIs.</span></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>Machine learning web API fundamentals</h2>
			<p>Under traditional RPA, BP<a id="_idIndexMarker011"/> interacts with desktop applications or with websites through a web browser. Connecting to ML algorithms can’t normally be done through either of those methods. Instead, over 90% of commercially available algorithms, including those on AWS, Azure, GCP, OpenAI, and so on, are exposed as <span class="No-Break">web APIs.</span></p>
			<p>A standard web API call from a BP Digital Worker is shown in the following image. First, the Digital Worker makes an API request, which reaches the API endpoint through the Internet. The ML prediction is made and the endpoint returns an API response containing the prediction back <span class="No-Break">to BP.</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B18416_01_2.jpg" alt="Figure 1.2 – The most common way to use ML with RPA: web API calls over the internet" width="1650" height="300"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – The most common way to use ML with RPA: web API calls over the internet</p>
			<p>Many ML models are proprietary. Vendors want to protect their intellectual property by hosting models on their own servers, which have protected API endpoints. If your BP environment isn’t allowed to connect to the Internet, you’ll need to consult with your networking and security teams to see whether exemptions can <span class="No-Break">be made.</span></p>
			<p>If not, then <a id="_idIndexMarker012"/>using publicly hosted ML APIs won’t be possible. You’ll either have to develop and host your own models over the Intranet or make predictions directly on the Digital Workers themselves. Note that it’s the Digital Workers that make the ML requests. Some people mistakenly believe that the BP application server will make calls to the <span class="No-Break">API endpoint.</span></p>
			<p>Note the distinction between a <em class="italic">web API</em>, which is what we connect to get an ML prediction, and a <em class="italic">Web API Service</em>, which is a BP product feature. The web API services feature is used to define and set up connections to web APIs. We’ll create a Web API Service from scratch as the last example in <span class="No-Break">this chapter.</span></p>
			<p>Official documentation on this product feature can be found <span class="No-Break">here: </span><a href="https://bpdocs.blueprism.com/bp-7-1/en-us/Web%20API/HTML/configure-api-definition.htm"><span class="No-Break">https://bpdocs.blueprism.com/bp-7-1/en-us/Web%20API/HTML/configure-api-definition.htm</span></a><a href="https://bpdocs.blueprism.com/bp-7-1/en-us/Web%20API/HTML/configure-api-definition.htm%0D"/></p>
			<p>In the remaining parts of this section, we’ll look at what’s important to know about web APIs from an IA integration perspective. This includes <strong class="bold">authentication</strong>, <strong class="bold">JSON</strong>, <strong class="bold">pricing</strong>, <strong class="bold">single</strong> versus <strong class="bold">batch</strong> prediction, and <strong class="bold">synchronous</strong> versus <span class="No-Break"><strong class="bold">asynchronous</strong></span><span class="No-Break"> prediction.</span></p>
			<h3>Authentication</h3>
			<p>Most web API <a id="_idIndexMarker013"/>services <a id="_idIndexMarker014"/>require you to sign up before they can be used. After signing up, you’ll be given one or more unique ID(s), similar to a username and password, to uniquely identify you. Depending on the specific service, these unique ID(s) might be enough to directly gain access and call your ML API to receive your prediction. This is shown in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B18416_01_3.jpg" alt="Figure 1.3 – When the unique IDs are valid as authentication credentials" width="1404" height="258"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – When the unique IDs are valid as authentication credentials</p>
			<p>An example of this is Azure. You can pass in the unique ID, called<a id="_idIndexMarker015"/> a <strong class="bold">subscription key</strong> in Azure terminology, to access your endpoint <span class="No-Break">for prediction.</span></p>
			<p>However, most <a id="_idIndexMarker016"/>services will require you to use your <a id="_idIndexMarker017"/>ID(s) to request for a temporary access token before calling the ML API. First, you make a request to receive a temporary access token using your unique credentials. Then, pass in this temporary token along with the input data to your ML API request to get your predicted result. For example, AWS calls their<a id="_idIndexMarker018"/> unique IDs <a id="_idIndexMarker019"/>the <strong class="bold">access key ID</strong> and the <strong class="bold">secret access key</strong>. In GCP, they are called<a id="_idIndexMarker020"/> the <strong class="bold">Client ID</strong> and the <strong class="bold">Client Secret</strong>. Both<a id="_idIndexMarker021"/> AWS and GCP require passing in a temporary access token to the ML <span class="No-Break">API call.</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B18416_01_4.jpg" alt="Figure 1.4 – A two-part API call: getting the access token and calling the ML API" width="1633" height="715"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – A two-part API call: getting the access token and calling the ML API</p>
			<p>The specific way that this works differs from vendor to vendor. For instance, some will have you submit the unique IDs as query parameters, while others need you to submit them in the request headers. Some use a proprietary algorithm to generate your temporary access token (including AWS) while others use open standards such as OAuth2 (<span class="No-Break">including GCP).</span></p>
			<p>You’ll need to<a id="_idIndexMarker022"/> consult the vendor’s documentation <a id="_idIndexMarker023"/>to find out specific details. One of the main benefits of using a pre-built asset from the DX is that the authentication logic is usually handled for you! All you need to do is generate your unique credentials from the ML provider, and the temporary access token handling logic (if any) will be handled for <span class="No-Break">you already.</span></p>
			<h3>JSON</h3>
			<p>JSON is one<a id="_idIndexMarker024"/> of the most<a id="_idIndexMarker025"/> popular data formats used to exchange data between web services. Almost all ML web APIs expect to receive JSON as input data and provide JSON as output data. When using a web API from the DX, almost all of the conversions between BP’s data types to JSON will be done <span class="No-Break">for you.</span></p>
			<p>There may be situations where you need to manually convert data from BP into JSON. This is most likely to happen for <strong class="bold">nested collections</strong> and <a id="_idIndexMarker026"/>binary data used in <em class="italic">file uploads</em>. BP has a <strong class="source-inline">Utility – JSON</strong> VBO, which can be found in the <strong class="source-inline">VBO</strong> sub-folder where BP <span class="No-Break">is installed.</span></p>
			<p>Sending files in JSON format requires you to first encode them, usually in Base64 format. There are three VBOs provided by BP on the DX that can help with this: <strong class="source-inline">Utility – Encode Decode</strong>, <strong class="source-inline">Utility – File Manipulation</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">Base64Encoder</strong></span><span class="No-Break">.</span></p>
			<h3>Pricing</h3>
			<p>MLaaS <a id="_idIndexMarker027"/>offerings typically have free usage up to a limit. After the free limit is used, you’ll start to pay per transaction. The limits are usually reset on a monthly basis. For example, let’s look at Azure’s Computer Vision API (<a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/</a>). At the time of writing this book, it offers 5,000 free transactions per month. For the next 1 million transactions, the cost is 10 cents per transaction. Between 1 and 10 million transactions, the cost is 6.5 cents per transaction. As you make more transactions per month, your cost per transaction <span class="No-Break">goes down.</span></p>
			<p>The pricing varies based on many factors, such as what region you’re in, what specific service you’re using in their Computer Vision library, and whether you’re willing to pre-commit to using a minimum number <span class="No-Break">of transactions.</span></p>
			<p>It’s important to note that <em class="italic">one transaction is not the same as one API call</em>! For example, there may be a service that allows you to send in multiple documents at once. Each document <a id="_idIndexMarker028"/>might be counted as a separate transaction, despite being submitted in one API call. The devil is in the details, so you really need to look at the pricing pages carefully when evaluating how suitable an MLaaS offering is for your use case. This represents ongoing costs to the operation of the IA solution that must be weighed against <span class="No-Break">the benefits.</span></p>
			<h3>Single versus batch predictions</h3>
			<p>It’s key to<a id="_idIndexMarker029"/> understand the API by reading through its documentation before using it. One important thing to note when looking at the documentation is whether the API allows for only a single prediction per API request or whether it can accept multiple inputs in a batch for a single API call. There may be separate endpoints for one versus <span class="No-Break">the other.</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B18416_01_5.jpg" alt="Figure 1.5 – Batch prediction" width="1270" height="784"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Batch prediction</p>
			<p>For batch predictions, you’ll first send the input data for multiple predictions to the API endpoint in a single request (the first step in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em>). The predictions will be made (second step) and the response will return all of the predicted results (third step). In the single prediction case, only one prediction can be made per API <span class="No-Break">request call.</span></p>
			<p>Note that <em class="italic">Single Request</em> and <em class="italic">Single Response</em> shown in the image are NOT characteristics of single versus <a id="_idIndexMarker030"/>batch; it is a question of synchronous versus asynchronous, which will be <span class="No-Break">discussed next.</span></p>
			<h3>Synchronous vs. asynchronous predictions</h3>
			<p>Single <a id="_idIndexMarker031"/>versus batch refers to the number of predictions that can be requested for in a single API call. Synchronous versus asynchronous, on the other hand, refers to whether the prediction results are returned in the same API call as <span class="No-Break">the request.</span></p>
			<p>For a synchronous (also called <strong class="bold">online</strong>) call, the predicted result(s) is returned in the same call as the request. This is used when the prediction can be made relatively quickly, such as with short documents and single images. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em> also shows an online case where the response is returned in the same <span class="No-Break">API call.</span></p>
			<p>For asynchronous (also called <strong class="bold">offline</strong>) calls, you won’t receive the prediction in the same API call as the request. Instead, you’ll first receive a unique ID for your request that you can use to enquire about the status of your prediction job. Imagine if you need to process a large video as input. It may take minutes or even hours before the ML service can come back with its prediction. Let’s look at AWS’s Rekognition Video API as <span class="No-Break">an example:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B18416_01_6.jpg" alt="Figure 1.6 – AWS’s Rekognition Video API: asynchronous calls" width="1461" height="1461"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – AWS’s Rekognition Video API: asynchronous calls</p>
			<p>First, you <a id="_idIndexMarker032"/>create an API request for a prediction by providing an S3 link to the video. In this first API call, you’ll receive a <strong class="source-inline">JobId </strong>back. For the second API call, you pass in the JobId to the <strong class="source-inline">GetLabelDetection</strong> endpoint, which returns the <strong class="source-inline">JobStatus</strong>. If the video is still being processed, you’ll receive a <strong class="source-inline">JobStatus</strong> <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">IN_PROGRESS</strong></span><span class="No-Break">.</span></p>
			<p>Depending on the size of the video, you may need to wait a few seconds or even a few minutes before the prediction is finished. During this time, you can continue to call the <strong class="source-inline">GetLabelDetection</strong> API endpoint to check the status of your job. When it’s finished, the JobStatus will turn into <strong class="source-inline">SUCCEEDED</strong> and the predicted labels will be returned with <span class="No-Break">the request.</span></p>
			<p>While batch often implies asynchronous, this isn’t always the case! It’s possible for a single prediction to be asynchronous (such as in the AWS Rekognition Video example) and for batch <a id="_idIndexMarker033"/>prediction to be synchronous (as will be seen in the last example in <span class="No-Break">this chapter).</span></p>
			<p>Now that we’ve covered the basics of ML web APIs, let’s look at how we can use the DX to download pre-built assets and quickly make an ML prediction without having to develop an ML <span class="No-Break">model ourselves.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor021"/>An overview of MLaaS on the DX</h2>
			<p>MLaaS <a id="_idIndexMarker034"/>offerings, such as those from Azure, AWS, GCP, and IBM, allow <a id="_idIndexMarker035"/>you to quickly integrate pre-trained ML algorithms into BP. You’ll be able to use the ML parts of IA without the complexities of the ML lifecycle, such as training, choosing between algorithms, tuning, hosting, maintenance, etc. However, MLaaS is often less flexible, as the models are pre-trained and usually not customizable to your specific use case. Most of these services provide <em class="italic">generic</em> rather than <em class="italic">specific</em> <span class="No-Break">ML capabilities.</span></p>
			<p>To save you time searching the DX, I’ve compiled and summarized a list of the most popular MLaaS offerings currently available, grouped by the type of input data you’re looking to provide into the algorithm. This will give you a sense of what IA cases you can quickly and simply implement. The types of input data are categorized as <strong class="bold">Images</strong>, <strong class="bold">Video</strong>, <strong class="bold">Speech</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Text</strong></span><span class="No-Break">.</span></p>
			<p>Multi-modal ML models that can accept different types of inputs are starting to appear. An example of this is GPT-4, which can accept both text and image as input. These models won’t be covered in this section, as they aren’t fully available during the writing of <span class="No-Break">this book.</span></p>
			<p>Note that numerical data isn’t listed as an input. Numerical input is usually used for regression problems, and there aren’t many services that provide ML models that are generic enough to be useful for regression. If you’re trying to predict numerical data, you’ll probably need to build your own <span class="No-Break">ML model.</span></p>
			<p>From a development or proof-of-concept perspective, most ML vendors offer generous free usage tiers of their ML prediction services. This allows you to evaluate whether it makes sense to pursue a particular IA project in a short period <span class="No-Break">of time.</span></p>
			<p>While investigating 122 IA use cases for my thesis research, I found that 75% of them could potentially have been fulfilled by MLaaS, so investigating these services and the DX is well worth your time. Text-based services were by far the most common, representing over 66% of all IA <span class="No-Break">use cases.</span></p>
			<h3>Image services</h3>
			<p>Image services<a id="_idIndexMarker036"/> return insight into what’s found in images. Most of these services have <a id="_idIndexMarker037"/>specific requirements with regard to what images can be submitted to their service. This can include file size requirements, image formats (PNG and JPG are the most common), image resolution, minimum and maximum dimensions, and <span class="No-Break">image orientation.</span></p>
			<p>Some will require you to upload the files as part of the API request body content, while others will ask you to first upload the files somewhere and provide URLs to the images. Many of these APIs offer <em class="italic">both single and batch operations</em> and are typically <em class="italic">synchronous</em>, returning the detected labels in the same request as the <span class="No-Break">initial response.</span></p>
			<p>Image-based ML services only accounted for 10% of IA use cases that I found. An issue with these types of services is that the generic objects returned by the predictions aren’t specific enough to be useful. However, many of these services also allow you to customize the labels, meaning that you can further train an ML model, using the vendor’s model as a starting base, to match what you’re interested in finding. This is where many potential image-based use cases can <span class="No-Break">be unlocked.</span></p>
			<p>After a prediction is made, you’ll get back <span class="No-Break">the following:</span></p>
			<ul>
				<li>A list of labels that are found in <span class="No-Break">the image(s)</span></li>
				<li><em class="italic">X</em> and <em class="italic">Y</em> coordinates representing the boundaries of the <span class="No-Break">discovered objects</span></li>
				<li>Confidence scores showing the degree of certainty that the object is of a <span class="No-Break">particular label</span></li>
			</ul>
			<p>The services and potential use cases of image-based services include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Object detection</strong>: This<a id="_idIndexMarker038"/> allows you to find labels in <a id="_idIndexMarker039"/>images drawn from generic categories, such as animals, people, cars, buildings, etc. This can be further customized, in many cases, to <a id="_idIndexMarker040"/>detect <em class="italic">specific objects</em>, such as through AWS’s <strong class="bold">DetectCustomLabels</strong> and<a id="_idIndexMarker041"/> Azure’s <strong class="bold">Custom Vision</strong>. Some real-life IA use cases include quality<a id="_idIndexMarker042"/> assurance in<a id="_idIndexMarker043"/> factories for counting and finding product defects and dispensing <span class="No-Break">prescription medication.</span></li>
				<li><strong class="bold">Face and facial feature detection</strong>: This predicts whether faces are present in an image<a id="_idIndexMarker044"/> and the gender, estimated<a id="_idIndexMarker045"/> age, and overall mood displayed by the face. This could be used for surveillance and the processing of applications that contain photographs. The only IA use case I found that implemented facial recognition was to take attendance and to quantify student engagement during online <span class="No-Break">learning courses.</span></li>
				<li><strong class="bold">Image content moderation</strong>: This <a id="_idIndexMarker046"/>allows us to determine whether an image contains adult or violent<a id="_idIndexMarker047"/> themes. This is most often used for user-uploaded content moderation on social media. I didn’t find real use cases for this <span class="No-Break">in IA.</span></li>
				<li><strong class="bold">Text detection</strong>: This <a id="_idIndexMarker048"/>returns where in the image text<a id="_idIndexMarker049"/> is found and what the text is. This is one of the most common image-based IA <span class="No-Break">use cases.</span></li>
				<li><strong class="bold">Form detection</strong>: This<a id="_idIndexMarker050"/> tells us whether an image has extractable form fields or tables in it. This is generically useful for IA and could be used for processing invoices, receipts, and financial reporting data. A real-life example of this is extracting data from scans <a id="_idIndexMarker051"/>of hand-written bank account <span class="No-Break">application forms.</span></li>
			</ul>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style" colspan="4">
							<p><strong class="bold">Search term for </strong><span class="No-Break"><strong class="bold">DX skill</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AWS</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Azure</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Google Cloud</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">IBM</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Object detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Rekognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Computer <span class="No-Break">Vision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Vision API, Google <span class="No-Break">Vision Skill</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Visual Recognition Skill <span class="No-Break">IBM Watson</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Face and facial feature <span class="No-Break">detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Rekognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Computer <span class="No-Break">Vision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Vision API, Google <span class="No-Break">Vision Skill</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Visual Recognition Skill <span class="No-Break">IBM Watson</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Content <span class="No-Break">moderation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Rekognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Computer <span class="No-Break">Vision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Vision API, Google <span class="No-Break">Vision Skill</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Visual Recognition Skill <span class="No-Break">IBM Watson</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Text detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Rekognition, <span class="No-Break">Textract</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Computer <span class="No-Break">Vision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Vision API, Google <span class="No-Break">Vision Skill</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Visual Recognition Skill <span class="No-Break">IBM Watson</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Form detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Textract</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Form Recognizer <span class="No-Break">Client</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Document AI</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – Search terms for image-based ML services on the DX</p>
			<p><strong class="bold">*Available from the vendor but not on the DX, so you must build your own </strong><span class="No-Break"><strong class="bold">web API.</strong></span></p>
			<h3>Video services</h3>
			<p>Services that <a id="_idIndexMarker052"/>take videos as input have a similar functionality to image services. They<a id="_idIndexMarker053"/> allow for the analysis of videos in <em class="italic">almost real time</em>. Depending on the service, you might be asked to provide your input as the URL of a stream, or to upload your video files to <span class="No-Break">predesignated area.</span></p>
			<p>Almost real time is a key point to keep in mind. Most ML services that process video are <em class="italic">asynchronous</em> and don’t immediately give you the predicted result. First, you make the call to the API to request a prediction. Then, you have to make a further call(s) to check whether the prediction <span class="No-Break">is ready.</span></p>
			<p>There are usually maximum limits set on the length of the videos (3–6 hours) and file sizes (10–50 GB) that can be processed. These limits differ from vendor to vendor. Please check their API documentation and follow their guidelines with regard to these limits, resolutions, camera angles, video bitrates, etc. In my research, I didn’t find any use cases that used video as the input source. Instead of video, it may make sense to take snapshots of the video at regular intervals (e.g., every 5 seconds) and to use image-based ML <span class="No-Break">services instead.</span></p>
			<p>After a prediction is made, you’ll get back <span class="No-Break">the following:</span></p>
			<ul>
				<li>A list of labels that are found in <span class="No-Break">the video</span></li>
				<li><em class="italic">X</em> and <em class="italic">Y</em> coordinates representing the boundaries of the <span class="No-Break">discovered objects</span></li>
				<li>Start and end time ranges or timestamp for each label that <span class="No-Break">is detected</span></li>
				<li>Confidence scores showing the degree of certainty that the object is of a <span class="No-Break">particular label</span></li>
			</ul>
			<p>The services and potential use <a id="_idIndexMarker054"/>cases of image-based services include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Object detection</strong>: This can <a id="_idIndexMarker055"/>be used to detect generic<a id="_idIndexMarker056"/> labels, such as animals, people, cars, buildings, etc. Sample use cases include surveillance, drone footage analysis, and <span class="No-Break">quality assurance.</span></li>
				<li><strong class="bold">Scene change detection</strong>: This <a id="_idIndexMarker057"/>detects whether a <a id="_idIndexMarker058"/>video feed switches from one camera to another. This could be used for video cataloging, archiving, <span class="No-Break">and trimming.</span></li>
				<li><strong class="bold">Content moderation</strong>: This <a id="_idIndexMarker059"/>predicts whether a video contains adult or violent<a id="_idIndexMarker060"/> themes. It’s often used for user-uploaded content moderation on <span class="No-Break">social media.</span></li>
				<li><strong class="bold">Logo detection</strong>: This<a id="_idIndexMarker061"/> detects logos in videos. It could be used to automatically flag or<a id="_idIndexMarker062"/> sensor brands and logos in video or to monitor <span class="No-Break">user-submitted content.</span></li>
				<li><strong class="bold">Face detection</strong>: This <a id="_idIndexMarker063"/>detects the presence of <a id="_idIndexMarker064"/>faces in video. It can be used to automate access control to facilities, to track movement, and <span class="No-Break">for surveillance.</span></li>
				<li><strong class="bold">Text extraction</strong>: This is<a id="_idIndexMarker065"/> used to extract text from<a id="_idIndexMarker066"/> video. Even if the text is partially obscured for portions of the video, these services can reconstruct the full text if the camera moves enough to cover the entire text during the course of the video. This could be used to extract presentation slide content from <span class="No-Break">live events.</span></li>
			</ul>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style" colspan="4">
							<p><strong class="bold">Search term for </strong><span class="No-Break"><strong class="bold">DX Skill</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AWS</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Azure</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Google Cloud</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">IBM</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Object detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Scene changes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Content moderation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Logo detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Face detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Text extraction</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Video Intelligence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.2 - Search terms for video-based ML services on the DX</p>
			<p><strong class="bold">*Available from the vendor but not on the DX, so you must build your own </strong><span class="No-Break"><strong class="bold">web API.</strong></span></p>
			<h3>Speech services</h3>
			<p>These<a id="_idIndexMarker067"/> allow you to extract<a id="_idIndexMarker068"/> data from live-streamed audio and saved audio files. This can be done <em class="italic">synchronously</em> or <em class="italic">asynchronously</em>, depending on the length of the audio or the size of the file. Shorter files are typically <span class="No-Break">processed synchronously.</span></p>
			<p>Depending on the API, the vendor may require you to upload audio files directly into the ML endpoint or upload them to cloud storage. They will also have requirements on the length of the audio and the languages spoken. In my research, I did not find any implemented use cases of IA using speech as <span class="No-Break">the input.</span></p>
			<p>Typical outputs of a speech-based ML service are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <span class="No-Break">transcribed text</span></li>
				<li>Timestamps of intervals of the <span class="No-Break">transcribed text</span></li>
				<li><span class="No-Break">Confidence scores</span></li>
			</ul>
			<p>Common capabilities and use cases of speech ML are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Speech transcription</strong>: This <a id="_idIndexMarker069"/>converts <a id="_idIndexMarker070"/>spoken audio <span class="No-Break">into text.</span></li>
				<li><strong class="bold">Text to speech</strong>: This<a id="_idIndexMarker071"/> allows text to<a id="_idIndexMarker072"/> be read out loud <span class="No-Break">as speech.</span></li>
				<li><strong class="bold">Intent recognition</strong>: This <a id="_idIndexMarker073"/>can be used to categorize the intent of the speaker. The categories<a id="_idIndexMarker074"/> themselves must be predefined and provided as an input into the ML service. For instance, an airline ticket booking system may have the intents defined as <strong class="source-inline">purchase</strong>, <strong class="source-inline">modify</strong>, or <strong class="source-inline">cancel </strong><span class="No-Break"><strong class="source-inline">a ticket</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Speaker recognition</strong>: This <a id="_idIndexMarker075"/>helps you<a id="_idIndexMarker076"/> to identify different speakers. This works best when the number of speakers to predict is sent as an input to the algorithm. It also assigns each speaker with a unique identifier. You can use the unique identifier to map audio back to a human speaker manually if you already know the identities of the speakers. This could be used to automatically add subtitles that include <span class="No-Break">speakers’ names.</span></li>
				<li><strong class="bold">Speech translation</strong>: This allows you to transcribe audio into text of a different <a id="_idIndexMarker077"/>language. This can<a id="_idIndexMarker078"/> also be achieved by chaining a speech transcription service to a <a id="_idIndexMarker079"/>text <span class="No-Break">translation service.</span></li>
			</ul>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style" colspan="4">
							<p><strong class="bold">Search term for </strong><span class="No-Break"><strong class="bold">DX Skill</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AWS</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Azure</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Google Cloud</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">IBM</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Speech transcription</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Transcribe</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Speech <span class="No-Break">to text</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud <span class="No-Break">Speech-to-Text API</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Text <span class="No-Break">to speech</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Polly</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Cloud Text-to-Speech</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Intent recognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Speaker recognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Azure Speaker <span class="No-Break">Recognition Beta</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Speech translation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.3 – Search terms for video-based ML services on the DX</p>
			<p><strong class="bold">*Available from the vendor but not on the DX, so you must build your own </strong><span class="No-Break"><strong class="bold">web API.</strong></span></p>
			<h3>Text services</h3>
			<p>Text-based <a id="_idIndexMarker080"/>ML services are by far the most commonly used ML services in<a id="_idIndexMarker081"/> IA. Around two-thirds of the 122 IA use cases I found were text-based, 52% of the cases involved the use of<a id="_idIndexMarker082"/> translation, <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>), OCR, and document classification, and 15% of the use cases <a id="_idIndexMarker083"/>used chatbots as an interface to <span class="No-Break">trigger IA.</span></p>
			<p>Text services are <em class="italic">usually</em> synchronous, and depending on the service, prediction requests can either be sent in one by one or in a batch. Text-based ML services take in the text to be analyzed and the language as <span class="No-Break">input parameters.</span></p>
			<p>Typical outputs received from text ML are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <span class="No-Break">predicted labels</span></li>
				<li><span class="No-Break">Confidence scores</span></li>
			</ul>
			<p>The following are common ways that text ML can <span class="No-Break">be used:</span></p>
			<ul>
				<li><strong class="bold">Document classification</strong>: This <a id="_idIndexMarker084"/>allows you to differentiate<a id="_idIndexMarker085"/> between different types of documents. This normally leads to further ML processing, such as entity recognition. An example of this is invoice processing. Document classification can be used to classify invoices by vendor. Another example is to separate documents into invoices and purchase orders because they often <span class="No-Break">look similar.</span></li>
				<li><strong class="bold">Entity recognition</strong>: This <a id="_idIndexMarker086"/>extracts text into predefined categories, such<a id="_idIndexMarker087"/> as names, dates, numerical figures, and events. This can be used for categorizing text, filtering CVs, and extracting data from unstructured reports. This is one of the most popular use cases, especially for processing invoices, claims, and other standardized forms. Many vendors allow you to customize the list of entities to match your specific <span class="No-Break">use case.</span></li>
				<li><strong class="bold">Key phrase extraction</strong>: This <a id="_idIndexMarker088"/>extracts the key points of emphasis in a <a id="_idIndexMarker089"/>sentence. For example, if the input is <strong class="source-inline">I waited for 45 minutes before it was my turn</strong>, the key phrases could be “waited” and “<span class="No-Break">45 minutes”.</span></li>
				<li><strong class="bold">Sentiment analysis</strong>: This<a id="_idIndexMarker090"/> finds the high-level opinion or attitude<a id="_idIndexMarker091"/> expressed in a block of text. Some common labels returned include <em class="italic">positive</em>, <em class="italic">neutral</em>, <em class="italic">negative</em>, and <em class="italic">mixed</em>. Most models can’t distinguish more nuanced sentiments under the same label; for example, “excited” and “happy” would both be returned as positive despite being different in reality. Common ways this can be used is to triage emails or customer support tickets and to detect sentiment in social media and <span class="No-Break">product reviews.</span></li>
				<li><strong class="bold">Language detection</strong>: This is <a id="_idIndexMarker092"/>typically used as a first step before sending the<a id="_idIndexMarker093"/> text to other text-based <span class="No-Break">ML</span><span class="No-Break"><a id="_idIndexMarker094"/></span><span class="No-Break"> algorithms.</span></li>
				<li><strong class="bold">Translation</strong>: This can <a id="_idIndexMarker095"/>translate text from one language to another in either real time or on demand. Over 50 languages are supported by <span class="No-Break">all vendors.</span></li>
				<li><strong class="bold">Chatbots</strong>: These <a id="_idIndexMarker096"/>services allow companies to create conversational <a id="_idIndexMarker097"/>interfaces for their applications. For example, one government agency used chatbots as an interface for people to request social security benefits, which further triggered IA to complete <span class="No-Break">the processing.</span></li>
				<li><strong class="bold">Question answering</strong>: This is <a id="_idIndexMarker098"/>one of the main features of LLMs. As this technology is relatively new, I haven’t come across any use cases of this being used in IA <span class="No-Break">just yet.</span></li>
				<li><strong class="bold">Text generation</strong>: This <a id="_idIndexMarker099"/>is another relatively<a id="_idIndexMarker100"/> new use case enabled by LLMs. This would be useful as a way to create responses to user complaints <span class="No-Break">or emails.</span></li>
			</ul>
			<table id="table004" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style" colspan="4">
							<p><strong class="bold">Search term for </strong><span class="No-Break"><strong class="bold">DX Skill</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AWS</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Azure</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Google Cloud</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">IBM</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Document <span class="No-Break">classification</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Form <span class="No-Break">Recognizer client</span></p>
						</td>
						<td class="No-Table-Style">
							<p>AutoML Natural Language, Cloud Natural <span class="No-Break">Language API</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Entity recognition</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Comprehend <span class="No-Break">Capability AWS</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Text analytics, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>AutoML Natural Language, Cloud Natural <span class="No-Break">Language API</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Language <span class="No-Break">understanding</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Key <span class="No-Break">phrase extraction</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Comprehend <span class="No-Break">Capability AWS</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Text analytics, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Natural <span class="No-Break">Language API</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Language <span class="No-Break">understanding</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Sentiment analysis</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Comprehend <span class="No-Break">Capability AWS</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Text analytics, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>AutoML Natural Language, Cloud Natural <span class="No-Break">Language API</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Language <span class="No-Break">understanding</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Language detection</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Comprehend <span class="No-Break">Capability AWS</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Text analytics, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Cloud Natural <span class="No-Break">Language API</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Language <span class="No-Break">understanding</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Translation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Translate <span class="No-Break">capability AWS</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Translate skill Azure Cloud, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Translate Skill <span class="No-Break">Google Cloud</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Language translation <span class="No-Break">skill IBM</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Chatbots</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p>QnAMaker, <span class="No-Break">OpenAI, ChatGPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Question answering</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">OpenAI, ChatGPT^</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Text generation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">OpenAI, ChatGPT^</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A*</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.4 - Search terms for video-based ML services on the DX</p>
			<p><strong class="bold">*Available from the vendor but not on the DX, so you must build your own </strong><span class="No-Break"><strong class="bold">web API.</strong></span></p>
			<p><strong class="bold">^OpenAI is not completely owned by Microsoft, although they have a </strong><span class="No-Break"><strong class="bold">strong partnership.</strong></span></p>
			<p>An interesting <a id="_idIndexMarker101"/>development has happened with the release of GPT-4, Bard, and other LLMs. These models have the innate ability to perform tasks such as text translation, entity extraction, language detection, sentiment analysis, etc., despite not having been trained to do so explicitly. I expect that LLMs will become a viable alternative to many of the text-based and image-based ML APIs as they become <span class="No-Break">more mature.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor022"/>Vendor selection</h2>
			<p>In the <a id="_idIndexMarker102"/>previous section, we looked at four major MLaaS vendors: AWS, Azure, GCP, and IBM. The decision to choose one over the other is complex and highly dependent on constraints that may already exist at your company. Selection criteria can include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Existing preferences or discounts at a <span class="No-Break">particular vendor</span></li>
				<li>Existing organizational or team knowledge in using one platform <span class="No-Break">over another</span></li>
				<li>The presence of must-have services that are offered by only one of <span class="No-Break">the vendors</span></li>
				<li>The availability of the service in a desirable <span class="No-Break">geographic location</span></li>
				<li>The costs of the <span class="No-Break">service itself</span></li>
				<li>The accuracy of the service given your <span class="No-Break">input data</span></li>
				<li>Whether the service supports your desired single versus batch or synchronous versus <span class="No-Break">asynchronous processing</span></li>
				<li>Data retention and <span class="No-Break">security policies</span></li>
				<li>SLAs for uptime and API <span class="No-Break">usage limits</span></li>
				<li>The availability of the service as a ready-to-use asset on <span class="No-Break">the DX</span></li>
			</ul>
			<p>Before moving on to hands-on examples, let’s summarize what we’ve just learned. The DX is BP’s “app store,” where many connectors to ML services are available. Most of these connectors are web APIs that use JSON as a data exchange format. Some of the most challenging parts of using an ML web service (such as authentication) and specific API requirements are already included in the DX asset, allowing you to quickly integrate ML into your <span class="No-Break">BP</span><span class="No-Break"><a id="_idIndexMarker103"/></span><span class="No-Break"> process.</span></p>
			<p>Next, we looked at common use cases of ML from the main ML service vendors: AWS, Azure, GCP, and IBM. Each vendor has marketing names for its ML services. It’s not straightforward to know what each service does based on its name. I’ve broken down all of the services based on the type of input it expects to take in: image, video, speech, and text. I’ve also listed their marketing names as well as the search terms you can use to find them on the DX, if they exist. Finally, we looked at some factors that can influence our decision to choose one ML vendor <span class="No-Break">over another.</span></p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Examples</h1>
			<p>In this section, we’ll be going through three examples of using web APIs to make ML predictions. For the first two examples, we’ll be <em class="italic">downloading</em> and using two different assets from the DX. In the third example, we’ll be creating a Web API Service from scratch. One web API from each major vendor will be used: AWS, Azure, and GCP. These examples will require you to sign up for free accounts to use their services. If you don’t have accounts with each vendor, follow along with the ones you do have <span class="No-Break">accounts with.</span></p>
			<p>From AWS, we’ll use Comprehend for text analysis of an email service request. With Azure, we’ll use Form Recognizer to extract data from an invoice. Finally, from GCP, we’ll use the Cloud Vision API to extract text from an image-based PDF. These three services were chosen because they’re the most popular ML service vendors, have different authentication methods, and cover the most common use cases encountered in IA, processing text data, forms, <span class="No-Break">and images.</span></p>
			<p>The most difficult parts of getting a web API working are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Authentication</strong>: Part of <a id="_idIndexMarker104"/>this is done on the MLaaS vendor’s portal to generate the right keys. The other part is inputting those keys into the correct part of BP, usually a Credential. The DX asset’s documentation will guide you on how this should <span class="No-Break">be done.</span></li>
				<li><strong class="bold">Formatting input data</strong>: This normally requires reading the API’s documentation, but this is provided to you from the Web API Service or <span class="No-Break">the Object.</span></li>
				<li><strong class="bold">Formatting output data</strong>: This normally requires reading the API’s documentation, but this is provided to you from the Web API Service or <span class="No-Break">the Object.</span></li>
			</ul>
			<p>The three examples that we will go through and how they differ from one another are summarized in the <span class="No-Break">following table:</span></p>
			<table id="table005" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Service</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Implementation for API </strong><span class="No-Break"><strong class="bold">in BP</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Authentication</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Batch / </strong><span class="No-Break"><strong class="bold">single</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Sync / </strong><span class="No-Break"><strong class="bold">async</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Use case</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">AWS Comprehend</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Object + <span class="No-Break">web API</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Temporary access token (object) + access key ID + secret <span class="No-Break">access key</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Single</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Sync</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Extract entities from support <span class="No-Break">ticket text</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Azure <span class="No-Break">Form Recognizer</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Object + HTTP and <span class="No-Break">JSON VBOs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Single <span class="No-Break">subscription key</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Single</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Async</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Extract data from <span class="No-Break">digital invoices</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>GCP <span class="No-Break">Cloud Vision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Custom-built Web <span class="No-Break">API Service</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Temporary access token (OAuth2) + Client ID + <span class="No-Break">Client Secret</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Batch</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Sync</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Extract text <span class="No-Break">from images</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.5 – A summary of the three examples we will go through</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor024"/>Example 1 – AWS Comprehend for text entity extraction, key phrase extraction, and sentiment analysis</h2>
			<p>AWS Comprehend <a id="_idIndexMarker105"/>is an ML service that extracts information and understands data in text. In this example, we’ll use Comprehend to triage cases for email support. Unlike submitting a support request through a web form or a chatbot where you can ask the user to categorize the issue for you, triaging issues sent directly through email has to be done either manually, through rules, or <span class="No-Break">by ML.</span></p>
			<p>It’s possible to build a custom model to directly classify text into your desired categories in Comprehend, but that’s outside the scope of this example. Instead, we’ll be using AWS’s pre-built models to extract the entities, key phrases, and sentiment of a support request. Even without a custom model, predictions from these pre-built models are still useful to the customer support agent. The support request used in this example is <span class="No-Break">the following:</span></p>
			<p class="list-inset">I am writing to request assistance with my iPhone 14, which has a cracked screen. My customer ID is abcd@email.com. I would greatly appreciate your help in resolving this issue. I understand that the device may be covered by a warranty or insurance plan, and I would like to explore all available options for repairing or replacing the phone. If possible, could you please provide me with information on the next steps I should take to initiate a repair or replacement request? Additionally, please let me know if there are any costs associated with <span class="No-Break">this process.</span></p>
			<p>Comprehend currently has a free usage tier of 5 million characters per API per month. The actions provided by the DX asset work in <em class="italic">single</em> and <span class="No-Break"><em class="italic">synchronous</em></span><span class="No-Break"> modes.</span></p>
			<p>AWS web APIs use a proprietary algorithm to implement their refresh token authentication. Once you’ve imported the DX asset, you’ll notice that there’s both a Web API Service and an Object. The Object’s main purpose is to act as a wrapper around the web API that implements the custom authentication. In this example, we will be performing <a id="_idIndexMarker106"/>four <span class="No-Break">high-level steps:</span></p>
			<ol>
				<li>Downloading the asset from <span class="No-Break">the DX</span></li>
				<li>Importing the asset <span class="No-Break">into BP</span></li>
				<li>Configuring a <span class="No-Break">BP Credential</span></li>
				<li>Testing the DX API asset by making an <span class="No-Break">ML prediction</span></li>
			</ol>
			<h3>Downloading from the DX</h3>
			<p>In this step, we’ll visit the <a id="_idIndexMarker107"/>DX (<a href="https://digitalexchange.blueprism.com/dx/search">https://digitalexchange.blueprism.com/dx/search</a>) to search for and download the AWS <span class="No-Break">Comprehend asset:</span></p>
			<ol>
				<li>Visit the DX and type <strong class="source-inline">comprehend capability aws</strong> (capitalization doesn’t matter) into the search bar. Search for and click on the resulting <strong class="bold">Comprehend Capability AWS </strong><span class="No-Break"><strong class="bold">Cloud</strong></span><span class="No-Break"> asset.</span></li>
				<li>Click on the green download button on the right side of the screen and save the <strong class="source-inline">.bprelease</strong> asset to your computer. If you haven’t logged in already, the DX website will ask you to log in. This <strong class="source-inline">.bprelease</strong> contains one web API, one Object, and <span class="No-Break">one Credential.</span></li>
				<li>Scroll down further on the <strong class="bold">Connector for Comprehend Capability AWS Cloud</strong> page and download the <strong class="bold">AWS Comprehend </strong><span class="No-Break"><strong class="bold">User Guide</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B18416_01_7.jpg" alt="Figure 1.7 – Downloading the asset and the AWS Comprehend User Guide" width="1359" height="481"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Downloading the asset and the AWS Comprehend User Guide</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The AWS Comprehend User Guide<a id="_idIndexMarker108"/> provides links and details on how to set up your AWS account to allow for API calls from BP. Please follow the steps outlined in the user guide to properly set up your AWS account and authentication details. We won’t be going through the steps to set up your AWS <span class="No-Break">account here.</span></p>
			<h3>Importing the Comprehend asset into BP</h3>
			<p>Let’s import the <a id="_idIndexMarker109"/>downloaded asset into BP and check what <span class="No-Break">was imported:</span></p>
			<ol>
				<li>Open BP and log in. Click on <strong class="bold">File</strong> | <strong class="bold">Import</strong> | <strong class="bold">Release / Skill</strong>. Import the AWS Comprehend Release. Accept all of the defaults <span class="No-Break">when importing.</span></li>
				<li>Verify that the <strong class="source-inline">.bprelease</strong> was successfully imported into BP. There should be one Object, one Web API Service, and one Credential. The Object can be found in the <strong class="bold">Studio</strong> section <span class="No-Break">of BP.</span></li>
			</ol>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B18416_01_8.jpg" alt="Figure 1.8 – Finding the Object in the Studio section" width="789" height="508"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Finding the Object in the Studio section</p>
			<ol>
				<li value="3">Find the Comprehend web API under <strong class="bold">System</strong> | <strong class="bold">Objects</strong> | <strong class="bold">Web </strong><span class="No-Break"><strong class="bold">API Services</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B18416_01_9.jpg" alt="Figure 1.9 – Finding AWS: Comprehend under System | Objects | Web API Services" width="1007" height="478"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Finding AWS: Comprehend under System | Objects | Web API Services</p>
			<ol>
				<li value="4">Find the Credential under <strong class="bold">System</strong><a id="_idIndexMarker110"/> | <strong class="bold">Security</strong> | <span class="No-Break"><strong class="bold">Credentials</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B18416_01_10.jpg" alt="Figure 1.10 – Finding the AWS Credential under System | Security | Credentials" width="691" height="348"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Finding the AWS Credential under System | Security | Credentials</p>
			<h3>Configuring the AWS credential</h3>
			<p>In this step, we’ll<a id="_idIndexMarker111"/> import the <strong class="source-inline">AWS</strong> credentials (look at the downloaded user guide if you haven’t created these yet) into BP. Then, we’ll set the permissions on the Credential so that we can test it in <span class="No-Break">a Process:</span></p>
			<ol>
				<li>Copy your AWS access key ID into the <strong class="bold">Username</strong> field of the <strong class="source-inline">AWS</strong> credential. Copy your secret access key into the two <span class="No-Break"><strong class="bold">Password</strong></span><span class="No-Break"> fields.</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18416_01_11.jpg" alt="Figure 1.11 – Entering the AWS access key ID the secret access key" width="373" height="388"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Entering the AWS access key ID the secret access key</p>
			<ol>
				<li value="2">On the <strong class="bold">Access Rights</strong> tab of<a id="_idIndexMarker112"/> the Credential, set <strong class="bold">Security Roles</strong> to <strong class="bold">All Roles</strong>, <strong class="bold">Processes (legacy)</strong> to <strong class="bold">All Process</strong> and <strong class="bold">Resources (legacy)</strong> to <strong class="bold">All Resources</strong>. While this is not best practice, we’re only doing this for <span class="No-Break">testing purposes.</span></li>
			</ol>
			<h3>Test the AWS Comprehend object</h3>
			<p>Finally, let’s <a id="_idIndexMarker113"/>test out the DX asset and make a prediction! Notice in the test Process that the Actions use the <em class="italic">Object</em> and not the Web API Service. Using the Web API Service directly wouldn’t work since it doesn’t contain the authentication parts. Take the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Open <strong class="source-inline">Example 1 - Test AWS Comprehend</strong> Process in the <strong class="source-inline">Ch1</strong> Group in the Process Studio. This Process also requires the <strong class="source-inline">Utility – General</strong> VBO to be imported. This can be found in the <strong class="source-inline">VBO</strong> sub-folder where BP <span class="No-Break">is installed.</span></li>
				<li>Run the Process <a id="_idIndexMarker114"/>in the Process Studio. This Process makes three API calls: 1) detect entities, 2) find key phrases, and 3) detect sentiment. If successful, the three Collections on the <strong class="source-inline">Main Page</strong> in the <strong class="bold">Predictions</strong> Block <span class="No-Break">are populated.</span></li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18416_01_12.jpg" alt="Figure 1.12 – The three predicted Collections from the three API calls" width="384" height="312"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – The three predicted Collections from the three API calls</p>
			<p class="list-inset">Viewing the <strong class="source-inline">Entities Response</strong> Collection shows that AWS’s generic model can extract the email and phone <span class="No-Break">model number:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18416_01_13.jpg" alt="Figure 1.13 – The phone model and customer ID are extracted as entities" width="712" height="277"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – The phone model and customer ID are extracted as entities</p>
			<p>Even without a custom model to classify emails to the exact label, Comprehend’s pre-trained model is able to extract high-level information that can simplify the work of a customer support agent. We also see that the predictions come with a <strong class="bold">Score</strong>. This confidence score will be used as a design element for IA solutions in <em class="italic">Part 2</em> of <span class="No-Break">this book.</span></p>
			<p>In this example, we<a id="_idIndexMarker115"/> used AWS Comprehend to extract data from unstructured text. Since AWS uses a custom authentication scheme, the DX asset comes packaged with both a Web API Service and an Object. Normally, dealing with authentication is complex, but the object handles everything for us. After importing the asset, we only need to set up a Credential to get things working. That’s how easy it is to connect BP with AWS’ <span class="No-Break">Comprehend APIs.</span></p>
			<p>Let’s move on to our next example, which <span class="No-Break">uses Azure.</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>Example 2 – Azure Form Recognizer for invoice extraction</h2>
			<p>Azure’s Form Recognizer <a id="_idIndexMarker116"/>contains pre-built models that allow you to extract data from receipts, invoices, tax forms, business cards, and generic documents. Extracting data from invoices and receipts in usable formats <em class="italic">is a very common IA use case</em>. It’s so common that BP and most other RPA vendors have products specifically targeting invoice extraction. More information about BP’s document extraction product, Decipher, can be found in the last chapter of this book. A free pricing tier of Form Recognizer is available <span class="No-Break">for testing.</span></p>
			<p>The invoice used in this example can be downloaded here for <span class="No-Break">reference: </span><a href="https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex2_invoice.pdf"><span class="No-Break">https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex2_invoice.pdf</span></a></p>
			<p>This PDF is directly embedded into this example’s process as a binary Data Item, so downloading this <span class="No-Break">is optional.</span></p>
			<p>You can also build custom models inside of Form Recognizer to recognize your specific type of document, e.g., a bank or credit card statement. These custom models can unlock even more use cases for your business and be invoked by BP using the DX asset used in <span class="No-Break">this example.</span></p>
			<p>There are two major differences between this Azure API and AWS Comprehend beyond the APIs purpose. The first difference is the <em class="italic">authentication</em> scheme used. For Azure, we only need to pass in a subscription key together with every request. This is simpler than AWS, which requires a separate object to refresh the temporary access token. Unlike AWS Comprehend, the Form Recognizer calls are <em class="italic">asynchronous</em>, requiring you to repeatedly check whether the prediction <span class="No-Break">is complete.</span></p>
			<p>The DX asset for Form Recognizer is stored as a <strong class="source-inline">.bprelease</strong> file and contains one BP Object. This is different from most other web APIs available on the DX, which come packaged as a <strong class="source-inline">.bpskill</strong> file and appear in the <strong class="bold">System</strong> | <strong class="bold">Objects</strong> | <strong class="bold">Web API Services</strong> section <span class="No-Break">of BP.</span></p>
			<p>Form Recognizer was chosen as one of the examples because form processing is also a primary IA use case. The DX asset for Form Recognizer also shows a different way of implementing a web API connector that you can consider—implementing it directly as an object using the HTTP and JSON VBO instead of a Web API Service. Implementing as an Object allows companies running older versions of BP (&lt; V6.4) to call <span class="No-Break">web APIs.</span></p>
			<p>In this example, we will be performing four <span class="No-Break">high-level steps:</span></p>
			<ol>
				<li>Downloading the asset from GitHub (or <span class="No-Break">the DX)</span></li>
				<li>Importing the asset <span class="No-Break">into BP</span></li>
				<li>Configuring the Form <span class="No-Break">Recognizer object</span></li>
				<li>Testing the API call by making <span class="No-Break">a prediction</span></li>
			</ol>
			<h3>Downloading from the DX</h3>
			<p class="callout-heading">Important note</p>
			<p class="callout">Microsoft has changed the URL endpoints since the Form Recognizer asset on the DX was released. As a result, the asset that’s on the DX won’t work without changing the URLs in it. An already modified release is provided at <a href="https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_2_Azure_Form_Recognizer_Client_Service.bprelease">https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_2_Azure_Form_Recognizer_Client_Service.bprelease</a>. I recommend downloading this GitHub version and not the version on <span class="No-Break">the DX.</span></p>
			<p>Although we <a id="_idIndexMarker117"/>won’t be downloading the asset from the DX, we still need to download the documentation so that the Azure API can be configured. Here, we will visit the DX (<a href="https://digitalexchange.blueprism.com/dx/search">https://digitalexchange.blueprism.com/dx/search</a>) to download <span class="No-Break">the documentation:</span></p>
			<ol>
				<li>Type in <strong class="source-inline">form recognizer client</strong> (capitalization doesn’t matter) into the DX search bar and search. Click on the <strong class="bold">Form Recognizer </strong><span class="No-Break"><strong class="bold">Client</strong></span><span class="No-Break"> asset.</span></li>
				<li>Scroll down further on the <strong class="bold">Connector for Form Recognizer Client – 1.0.0</strong> page and download the <span class="No-Break"><strong class="bold">User Guide</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18416_01_14.jpg" alt="Figure 1.14 – Download the asset and the user guide" width="1647" height="598"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Download the asset and the user guide</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The user guide provides details on how to set up your Azure account to allow for API calls from BP. Please follow the steps in the guide to properly set up Azure. The guide also lists two other VBOs that must be imported into BP: the <strong class="source-inline">Utility - JSON</strong> and <strong class="source-inline">Utility – HTTP</strong> VBOs. These are both included in the previous GitHub link. If you wish to download them separately, they are also available from <span class="No-Break">the DX.</span></p>
			<h3>Importing into BP</h3>
			<p>After downloading the asset, it <a id="_idIndexMarker118"/>must be imported into BP. This step is the same regardless of whether you’ve downloaded the asset from the DX or <span class="No-Break">from GitHub:</span></p>
			<ol>
				<li>Open BP and log in. Click <strong class="bold">File</strong> | <strong class="bold">Import</strong> | <strong class="bold">Release / Skill</strong> and import <span class="No-Break">the asset.</span></li>
				<li>Verify that the <strong class="source-inline">Form Recognizer Client Service</strong> Object is present. Also verify that the correct versions of the HTTP and JSON VBOs as present, as required by the <span class="No-Break">user guide.</span></li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18416_01_15.jpg" alt="Figure 1.15 – Verifying that the three Objects are present" width="860" height="1338"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Verifying that the three Objects are present</p>
			<h3>Configuring the Form Recognizer Client Service object</h3>
			<p>The author of the <a id="_idIndexMarker119"/>DX asset has opted to store the authentication IDs as Data Items in the Object. This is <em class="italic">not a best practice</em>, as these should be stored as a Credential instead. But since this is how the Object was designed, let’s edit those Data Items to configure the authentication details needed to connect to the web API. If you don’t have a subscription key, please follow the asset documentation and <span class="No-Break">create one.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The access token and base URL needed to configure the Object can be obtained from the <strong class="bold">Form Recognizer</strong> page on Azure’s website. The user guide shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.15</em> has more details on how to find <span class="No-Break">this page.</span></p>
			<ol>
				<li>Open the <strong class="source-inline">Form Recognizer Client Service</strong> Object in the Object Studio. On the <strong class="bold">Initialise</strong> Page, there’s an <strong class="source-inline">Access Token</strong> Collection and a <strong class="source-inline">Base URL</strong> Data<a id="_idIndexMarker120"/> Item that needs to <span class="No-Break">be populated.</span></li>
			</ol>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18416_01_16.jpg" alt="Figure 1.16 – Populating Access Token and Base URL on the Initialise Page" width="622" height="164"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – Populating Access Token and Base URL on the Initialise Page</p>
			<ol>
				<li value="2">Open the <strong class="source-inline">Access Token</strong> Collection and click on the <strong class="bold">Initial Values</strong> tab. Fill in the <strong class="bold">Ocp-Apim-Subscription-Key</strong> field with your subscription key <span class="No-Break">and save.</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18416_01_17.jpg" alt="Figure 1.17 – Filling the subscription key into the access token" width="363" height="219"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – Filling the subscription key into the access token</p>
			<ol>
				<li value="3">Open the <strong class="source-inline">Base URL</strong> Data Item and set the <strong class="bold">Initial Value</strong> to the endpoint URL from Azure’s<a id="_idIndexMarker121"/> website. Do not include a trailing slash at the end of <span class="No-Break">the URL.</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18416_01_18.jpg" alt="Figure 1.18 – Filling in the Base URL without a trailing slash" width="483" height="230"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – Filling in the Base URL without a trailing slash</p>
			<p>Now that we’ve configured the Azure Object, we can test <span class="No-Break">it out.</span></p>
			<h3>Testing the Form Recognizer Client Service object</h3>
			<p>A test Process is<a id="_idIndexMarker122"/> included in the Release file imported in the <em class="italic">Technical requirements</em> section at the beginning of the chapter. This process requires <strong class="source-inline">Utility – General</strong> and <strong class="source-inline">Utility – File Management</strong> to be imported. Both can be found in the <strong class="source-inline">VBO</strong> sub-folder of the <span class="No-Break">installation folder.</span></p>
			<ol>
				<li>Open <strong class="source-inline">Example 2 – Test Azure Form Recognizer Client Service</strong> Process in the <strong class="source-inline">Ch1</strong> Group in the <span class="No-Break">Process Studio.</span></li>
				<li>Run the Process in the Process Studio. This process makes at least two API calls. The first call sends the invoice file to request processing. This gives a <strong class="bold">Result ID</strong> in return. The second API call checks up to six times whether the <em class="italic">asynchronous</em> ML prediction is finished by referencing the <strong class="bold">Result ID</strong>. When the prediction is complete, the Collections and Data Item in the <strong class="bold">Predictions</strong> Block will become populated. You can verify that the extracted contents closely match the actual <span class="No-Break">PDF document.</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18416_01_19.jpg" alt="Figure 1.19 – Predictions from the Azure invoice Form Recognizer" width="388" height="615"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Predictions from the Azure invoice Form Recognizer</p>
			<p>Now we’ve<a id="_idIndexMarker123"/> seen two examples with two different types of web API authentication, as well as synchronous and asynchronous API calling cases. In the final example of this chapter, we will be creating a web API service from scratch for batch PDF processing <span class="No-Break">in GCP.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Example 3 – GCP Cloud Vision batch OCR processing</h2>
			<p>Cloud Vision <a id="_idIndexMarker124"/>from Google Cloud allows you to extract labels and text from images. In IA, this is most commonly used for the OCR of image-based text, such as photographs of documents and receipts. If documents are scanned, GCP recommends using its Document AI service instead. In this example, we’ll be extracting text from an image-based PDF with a small amount of handwriting on it. We’ll also be showing off a synchronous batch case where five PDF pages of an image-based document are processed at once and the predicted results for those five pages are received in the <span class="No-Break">same request.</span></p>
			<p>The open source PDF document used in this example can be found at <a href="https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex3_pdf.pdf">https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex3_pdf.pdf</a>. This PDF is already embedded into the test Process as a Binary Data Item, so downloading it <span class="No-Break">is optional.</span></p>
			<p>GCP uses OAuth2 for authentication, which is also a temporary access token method, similar to what AWS Comprehend does in the first example. The difference between OAuth2 and AWS’s methods is that AWS’ authentication is proprietary. OAuth2 is an open standard adopted by many vendors, and BP’s Web API Services feature can handle it directly. No Object wrapper will be needed, unlike in the AWS <span class="No-Break">Comprehend case.</span></p>
			<p>In this third example, we’ll be building a web API from scratch. There’s no DX asset for the API endpoint that we’ll be using. A key skill needed to build a web API from scratch is the ability to carefully read and understand the vendor’s API documentation. The relevant parts of GCP’s API documentation will be shown alongside the BP Web API Service configuration <span class="No-Break">for reference.</span></p>
			<p>This example has very different steps compared to the two previous examples. At a high level, we will be performing <span class="No-Break">four steps:</span></p>
			<ol>
				<li>Visiting the GCP website to set up the keys needed for <span class="No-Break">API authentication</span></li>
				<li>Saving the API authentication keys into <span class="No-Break">a Credential</span></li>
				<li>Creating a Web API Service <span class="No-Break">in BP</span></li>
				<li>Testing the <a id="_idIndexMarker125"/>web API by making <span class="No-Break">a prediction</span></li>
			</ol>
			<h3>Setting up a service account and key</h3>
			<p>This section contains steps that are <a id="_idIndexMarker126"/>performed on GCP’s website. The purpose is to generate authentication credentials that can be downloaded and used inside of a BP Web API Service. If these steps become outdated, please use the link in <em class="italic">step 1</em> as a <span class="No-Break">reference instead:</span></p>
			<ol>
				<li>Visit <a href="https://cloud.google.com/iam/docs/service-accounts-create#creating_a_service_account">https://cloud.google.com/iam/docs/service-accounts-create#creating_a_service_account</a> and follow the instructions to create your service account. Ensure that your service account has permission to access the <span class="No-Break"><em class="italic">Vision API</em></span><span class="No-Break">.</span></li>
				<li>Create a key for your service account. Under the <strong class="bold">Service Accounts</strong> section of <strong class="bold">IAM &amp; Admin</strong>, click on the <strong class="bold">KEYS</strong> tab and then <span class="No-Break"><strong class="bold">ADD KEY</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B18416_01_20.jpg" alt="Figure 1.20 – Creating a key for your service account" width="1650" height="654"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Creating a key for your service account</p>
			<ol>
				<li value="3">Choose <strong class="bold">JSON</strong> as<a id="_idIndexMarker127"/> the <strong class="bold">Key type</strong> and <span class="No-Break">press </span><span class="No-Break"><strong class="bold">CREATE</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18416_01_21.jpg" alt="Figure 1.21 – Creating a JSON key" width="1292" height="785"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – Creating a JSON key</p>
			<ol>
				<li value="4">Download <a id="_idIndexMarker128"/>the JSON file that appears after <span class="No-Break">pressing </span><span class="No-Break"><strong class="bold">CREATE</strong></span><span class="No-Break">.</span></li>
			</ol>
			<h3>Saving the service account email and private key as a Credential</h3>
			<p>We’ve created <a id="_idIndexMarker129"/>authentication credentials and downloaded them in JSON format. Now we need to save the relevant information into a <span class="No-Break">BP credential:</span></p>
			<ol>
				<li>Open the JSON file in any text editor. You’ll see a <strong class="source-inline">private_key</strong> row. Copy everything between the two double<a id="_idIndexMarker130"/> quotation marks into your clipboard. This is referred to as the <strong class="bold">Client Secret</strong> in <span class="No-Break">GCP documentation.</span><pre class="source-code">
<strong class="bold">"type": "service_account",</strong>
<strong class="bold">"project_id": "project_id_here",</strong>
<strong class="bold">"private_key_id": "abcdefg",</strong>
<strong class="bold">"private_key": "</strong>-----BEGIN PRIVATE KEY-----\n
PRIVATE KEY HERE
\n-----END PRIVATE KEY-----\n",</pre></li>				<li>Visit <strong class="bold">System</strong> | <strong class="bold">Security</strong> | <strong class="bold">Credentials</strong> in BP. Click <strong class="bold">New</strong> to create a <span class="No-Break">new Credential.</span></li>
				<li>Set the <strong class="bold">Name</strong> to <strong class="source-inline">GCP Cloud Vision</strong>. Set the <strong class="bold">Type</strong> to <strong class="bold">OAuth 2.0 (JWT Bearer Token)</strong>. Set <strong class="bold">Issuer</strong> to the service account email address, and for <strong class="bold">Private Key</strong>, paste what was copied in <span class="No-Break"><em class="italic">step 1</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B18416_01_22.jpg" alt="Figure 1.22 – Saving the credential with information from the GCP Portal" width="1180" height="607"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.22 – Saving the credential with information from the GCP Portal</p>
			<ol>
				<li value="4">Click on the <strong class="bold">Access Rights</strong> tab of the Credential. Set <strong class="bold">Security Roles</strong> to <strong class="bold">All Roles</strong>, <strong class="bold">Processes (legacy)</strong> to <strong class="bold">All Process</strong>, and <strong class="bold">Resources (legacy)</strong> to <strong class="bold">All Resources</strong>. While this isn’t best practice, we’re only testing and not deploying<a id="_idIndexMarker131"/> something to production. Save <span class="No-Break">the credential.</span></li>
			</ol>
			<h3>Creating a Web API Service</h3>
			<p>Here, we will be creating <a id="_idIndexMarker132"/>a Web API Service from scratch. Google’s API documentation (<a href="https://cloud.google.com/vision/docs/file-small-batch">https://cloud.google.com/vision/docs/file-small-batch</a>) will be consulted frequently during <span class="No-Break">this exercise.</span></p>
			<p>One key tip to consider while building a web API from scratch is to see whether the vendor has a <em class="italic">different</em> service already available on the DX. If so, download it, import it, and use it as a reference. Each vendor tries to keep its APIs internally consistent. Within a vendor, their APIs are likely using the same authentication methods and have similar JSON structures for their inputs and outputs. You’ll likely be able to copy parts of a working Web API Service into <span class="No-Break">your own!</span></p>
			<ol>
				<li>Visit <strong class="bold">System</strong> | <strong class="bold">Objects</strong> | <strong class="bold">Web API Services</strong>. Click on <span class="No-Break"><strong class="bold">Add Service</strong></span><span class="No-Break">.</span></li>
				<li>Set <strong class="bold">Name</strong> to <strong class="source-inline">Google Cloud Vision Batch Annotation Online</strong>. This is an arbitrary name that will show up as a selectable option in an Action Stage. Set <strong class="bold">Base URL</strong> to <a href="https://vision.googleapis.com/">https://vision.googleapis.com/</a>, including the trailing <a id="_idIndexMarker133"/>slash. At the end of this step, your web API should look <span class="No-Break">like this:</span></li>
			</ol>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B18416_01_23.jpg" alt="Figure 1.23 – Setting the Name and Base URL" width="1164" height="475"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.23 – Setting the Name and Base URL</p>
			<p class="list-inset">The base URL is taken from <span class="No-Break">GCP’s documentation:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B18416_01_24.jpg" alt="Figure 1.24 – The base URL from GCP’s documentation" width="929" height="248"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.24 – The base URL from GCP’s documentation</p>
			<ol>
				<li value="3">Click on <strong class="bold">Common Headers</strong>. Add a row with <strong class="bold">Name</strong>: <strong class="source-inline">Content-Type</strong> and <strong class="bold">Value</strong>: <strong class="source-inline">application/json; charset=utf-8</strong>. The <strong class="bold">Common Headers</strong> section should look <span class="No-Break">like this:</span></li>
			</ol>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B18416_01_25.jpg" alt="Figure 1.25 – Setting the common headers" width="1597" height="356"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.25 – Setting the common headers</p>
			<p class="list-inset">These common headers are taken from the sample API request shown in <span class="No-Break">GCP’s documentation:</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B18416_01_26.jpg" alt="Figure 1.26 – The headers from GCP’s documentation (the other headers can be ignored)" width="1419" height="363"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.26 – The headers from GCP’s documentation (the other headers can be ignored)</p>
			<ol>
				<li value="4">Click <a id="_idIndexMarker134"/>on <strong class="bold">Common Authentication</strong>. Set <strong class="bold">Authentication Type</strong> to <strong class="bold">OAuth 2.0 (JWT Bearer Token)</strong>. Set <strong class="bold">Authorization URI</strong> and <strong class="bold">Audience</strong> to <a href="https://accounts.google.com/o/oauth2/token">https://accounts.google.com/o/oauth2/token</a>. Set Scope to <a href="https://www.googleapis.com/auth/cloud-vision">https://www.googleapis.com/auth/cloud-vision</a>. Set the <strong class="bold">Credential</strong> as <strong class="source-inline">GCP Cloud Vision</strong>, which was created in <em class="italic">step 2</em> in the <em class="italic">Saving the service account email and private key as a credential</em> section. Untick the <strong class="bold">Expose to </strong><span class="No-Break"><strong class="bold">Process</strong></span><span class="No-Break"> box.</span></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">The token endpoint data is taken from <a href="https://accounts.google.com/.well-known/openid-configuration">https://accounts.google.com/.well-known/openid-configuration</a> and the scope endpoint is taken <span class="No-Break">from </span><a href="https://developers.google.com/oauthplayground"><span class="No-Break">https://developers.google.com/oauthplayground</span></a><span class="No-Break">.</span></p>
			<p class="list-inset">The <strong class="bold">Common Authentication</strong> section should look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18416_01_27.jpg" alt="Figure 1.27 – Setting the common authentication" width="1109" height="698"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.27 – Setting the common authentication</p>
			<ol>
				<li value="5">Click on <strong class="bold">New Action</strong> and rename it <strong class="source-inline">Annotate Batch</strong>. This is the name of the Action that<a id="_idIndexMarker135"/> will appear as selectable in BP. Each Action is equivalent to one web API endpoint. At the end of this step, your <strong class="bold">Action</strong> section should look <span class="No-Break">like this:</span></li>
			</ol>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18416_01_28.jpg" alt="Figure 1.28 – Creating an action" width="1344" height="537"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.28 – Creating an action</p>
			<ol>
				<li value="6">Click on <strong class="bold">Parameters</strong>. Add three <strong class="bold">Text</strong> parameters named <strong class="source-inline">mimeType</strong>, <strong class="source-inline">pages,</strong> and <strong class="source-inline">content</strong>. Leave <strong class="bold">Initial Value</strong> blank and tick the <strong class="bold">Expose</strong> box for all three. At the end of this step, your <strong class="bold">Parameters</strong> section should look <span class="No-Break">like this:</span></li>
			</ol>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B18416_01_29.jpg" alt="Figure 1.29 – Setting the action parameters" width="1505" height="417"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.29 – Setting the action parameters</p>
			<p class="list-inset">These three<a id="_idIndexMarker136"/> parameters were taken from GCP’s examples of what they expect to receive in the request JSON body. We’re exposing these as the input parameters of our web API so that they can be passed in and changed according to the needs of <span class="No-Break">the Process.</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18416_01_30.jpg" alt="Figure 1.30 – The parameters from GCP’s documentation" width="909" height="1013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.30 – The parameters from GCP’s documentation</p>
			<ol>
				<li value="7">Click on <strong class="bold">Request</strong>. Set <strong class="bold">Method</strong> to <strong class="bold">POST</strong>, <strong class="bold">URL Path</strong> to <strong class="source-inline">/v1/files:annotate</strong>, and <strong class="bold">Body Content</strong> to <strong class="bold">Template</strong>. Inside the <strong class="bold">Template</strong> text box, put<a id="_idIndexMarker137"/> in <span class="No-Break">the following:</span><pre class="source-code">
{
  "requests": [[
    {
      "inputConfig": {
        "content": "[content]",
        "mimeType": "[mimeType]"
      },
      "features": [[
        {
          "type": "DOCUMENT_TEXT_DETECTION"
        },
        {
          "type": "LABEL_DETECTION"
        }
      ],
      "pages": [[
        [pages]
      ]
    }
  ]
}</pre><p class="list-inset">Notice that we’ve inserted our parameters into the template using <strong class="source-inline">[parameter name]</strong>, similar to BP data items. Some areas of the <strong class="bold">Template</strong> contain double square opening brackets (<strong class="source-inline">[[</strong>) .This is how we can escape the square opening bracket character. We’ve also <em class="italic">hardcoded</em> the type of extraction to be <strong class="source-inline">DOCUMENT_TEXT_DETECTION</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">LABEL_DETECTION</strong></span><span class="No-Break">.</span></p><p class="list-inset">What this section does is set up the exact JSON structure that will be sent to the API endpoint. We want this structure to match up with <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.30</em> exactly. At the end of this <a id="_idIndexMarker138"/>step, the <strong class="bold">Request</strong> section should look <span class="No-Break">like this:</span></p></li>			</ol>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18416_01_31.jpg" alt="Figure 1.31 – Set the request" width="1257" height="1266"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.31 – Set the request</p>
			<ol>
				<li value="8">Click on <strong class="bold">Response</strong>. Set <strong class="bold">Parameter</strong> to <strong class="source-inline">responses</strong>, <strong class="bold">Data Type</strong> as <strong class="bold">Collection</strong>, <strong class="bold">Method</strong> to <strong class="bold">Json Path</strong>, and <strong class="bold">Json Path</strong> to <strong class="source-inline">$.responses</strong>. This part defines what the web API sends back to the Action as Output Parameters. In our case, we’re just going to save the entire JSON response as a Collection. At the end of this<a id="_idIndexMarker139"/> step, your <strong class="bold">Response</strong> configuration should look <span class="No-Break">like this:</span></li>
			</ol>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B18416_01_32.jpg" alt="Figure 1.32 – Setting the response" width="1507" height="536"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.32 – Setting the response</p>
			<ol>
				<li value="9">Our single output <strong class="source-inline">responses</strong> Collection is equivalent to the highlighted item in the sample response output of <span class="No-Break">GCP’s documentation:</span></li>
			</ol>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B18416_01_33.jpg" alt="Figure 1.33 – The response from GCP’s documentation" width="1002" height="729"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.33 – The response from GCP’s documentation</p>
			<ol>
				<li value="10">Save <a id="_idIndexMarker140"/>the Web <span class="No-Break">API Service.</span></li>
			</ol>
			<p>Now we’ve completed the setup of a new web API. Let’s test it out with <span class="No-Break">a Process.</span></p>
			<h3>Testing the batch image Web API Service</h3>
			<p>The sample Process should<a id="_idIndexMarker141"/> have been imported from the <em class="italic">Technical requirements</em> section. We also need to import an additional VBO to convert our PDF file into Base64 format, which is a requirement of the GCP <span class="No-Break">Vision API:</span></p>
			<ol>
				<li>Download and import the <strong class="source-inline">Utility – File Manipulation</strong> asset from the DX at <a href="https://digitalexchange.blueprism.com/dx/entry/9648/solution/utility---file-manipulation">https://digitalexchange.blueprism.com/dx/entry/9648/solution/utility---file-manipulation</a>. This is used to convert the PDF file into the <span class="No-Break">Base64 format.</span></li>
				<li>Open <strong class="source-inline">Example 3 – Test GCP Batch PDF</strong> Process in the <strong class="source-inline">Ch1</strong> Group in the <span class="No-Break">Process Studio.</span></li>
				<li>Run the Process in the Process Studio. This Process makes one API call, which processes five PDF pages in a batch. The call is synchronous, as the predictions for the five pages are returned in the same call as the request. Once this is successful, the Collections and Data Item in the <strong class="bold">Predictions</strong> Block will <span class="No-Break">become populated:</span></li>
			</ol>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B18416_01_34.jpg" alt="Figure 1.34 – Predictions from the newly created web API" width="774" height="628"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.34 – Predictions from the newly created web API</p>
			<p>In this third example, we looked at a batch/synchronous processing case for extracting text from documents. We also built a Web API Service that uses OAuth2 authentication from scratch. This was an exercise in inputting existing API information into the right sections of BP’s Web API Service. In the sample Process, we also used Base64 to encode the PDF file so that it could be sent as plain text in the API <span class="No-Break">request body.</span></p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor027"/>Summary</h1>
			<p>In this chapter, we discussed the DX and how web API connectors can be downloaded to greatly speed up our development time. When using a web API, the areas that people frequently get stuck on include authentication and correctly mapping the input and output JSON data according to the vendor’s <span class="No-Break">API documentation.</span></p>
			<p>ML web APIs also differ in two areas that affects IA solution design. These are single versus batch and synchronous versus asynchronous predictions. We’ll revisit these characteristics later in <span class="No-Break">the book.</span></p>
			<p>Next, we looked at four major MLaaS vendors: AWS, Azure, GCP, and IBM. We explored what ML web APIs they have available on the DX. Each vendor uses marketing names for its services, obscuring what those services actually do. I’ve grouped and summarized what the services behind the marketing names actually do, provided search terms you can use to find them on the DX, and given you ideas on what use cases they <span class="No-Break">can fulfill.</span></p>
			<p>Finally, we went through three hands-on examples. Different vendors, API implementation methods, authentication methods, and use cases were selected to cover most of the scenarios you’ll encounter in real life. The vast majority of real-life IA examples I’ve found during my research have to do with text processing. Our examples showed three different IA text cases: extracting data from <em class="italic">unstructured text</em>, extracting data from <em class="italic">forms</em>, and extracting text <span class="No-Break">from </span><span class="No-Break"><em class="italic">images</em></span><span class="No-Break">.</span></p>
			<p>In the next chapter, we will move away from calling ML predictions from web APIs and look more at how BP can call models directly from the Digital Worker’s command <span class="No-Break">line interface.</span></p>
		</div>
	</div>
</div>
</body></html>