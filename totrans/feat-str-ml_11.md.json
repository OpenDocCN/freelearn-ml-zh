["```py\n!pip install pandas sklearn python-slugify s3fs sagemaker\n```", "```py\nimport os\n```", "```py\nimport numpy as np\n```", "```py\nimport pandas as pd\n```", "```py\nfrom slugify import slugify\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n```", "```py\n\"\"\" If you are executing the notebook outside AWS(Local jupyter lab, google collab or kaggle etc.), please uncomment the following 3 lines of code and set the AWS credentials \"\"\"\n```", "```py\n#os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key>\"\n```", "```py\n#os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\n```", "```py\n#os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n```", "```py\ntelcom = pd.read_csv(\"s3://<bucket_name_path>/telco-customer-churn.csv\")\n```", "```py\n# Replace empty strings with nan\n```", "```py\nchurn_data['TotalCharges'] = churn_data[\"TotalCharges\"].replace(\" \",np.nan)\n```", "```py\n# remove null values\n```", "```py\nchurn_data = churn_data[churn_data[\"TotalCharges\"].notnull()]\n```", "```py\nchurn_data = churn_data.reset_index()[churn_data.columns]\n```", "```py\nchurn_data[\"TotalCharges\"] = churn_data[\"TotalCharges\"].astype(float)\n```", "```py\n# Create tenure_group columns using the tenure\n```", "```py\ndef tenure_label(churn_data) :\n```", "```py\n    if churn_data[\"tenure\"] <= 24 :\n```", "```py\n        return \"0-24\"\n```", "```py\n    elif (churn_data[\"tenure\"] > 24) & (churn_data[\"tenure\"] <= 48) :\n```", "```py\n        return \"24-48\"\n```", "```py\n    elif churn_data[\"tenure\"] > 48:\n```", "```py\n        return \"48-end\"\n```", "```py\nchurn_data[\"tenure_group\"] = churn_data.apply(\n```", "```py\n    lambda churn_data: tenure_label(churn_data), axis = 1)\n```", "```py\n# Replace 'No internet service' to No for the following columns\n```", "```py\nreplace_cols = ['OnlineSecurity', 'OnlineBackup', \n```", "```py\n                'DeviceProtection', 'TechSupport',\n```", "```py\n                'StreamingTV', 'StreamingMovies']\n```", "```py\nfor i in replace_cols : \n```", "```py\n    churn_data[i] = churn_data[i].replace({'No internet service' : 'No'})\n```", "```py\nchurn_data.sample(5)\n```", "```py\nchurn_data.nunique()\n```", "```py\n# filter all the col if unique values in the column is 2\n```", "```py\nbin_cols = churn_data.nunique()[churn_data.nunique() == 2].keys().tolist()\n```", "```py\nle = LabelEncoder()\n```", "```py\nfor i in bin_cols :\n```", "```py\n    churn_data[i] = le.fit_transform(churn_data[i])\n```", "```py\nall_categorical_cols = churn_data.nunique()[churn_data.nunique() <=4].keys().tolist()\n```", "```py\nmulti_value_cols = [col for col in all_categorical_cols if col not in bin_cols]\n```", "```py\nchurn_data = pd.get_dummies(data = churn_data, columns=multi_value_cols)\n```", "```py\nnumerical_cols = ['tenure','MonthlyCharges','TotalCharges']\n```", "```py\nstd = StandardScaler()\n```", "```py\nchurn_data[numerical_cols] = std.fit_transform(churn_data[numerical_cols])\n```", "```py\nchurn_data.columns = [slugify(col, lowercase=True, separator='_') for col in churn_data.columns]\n```", "```py\nchurn_data.head()\n```", "```py\nimport boto3\n```", "```py\nFEATURE_GROUP_NAME = \"telcom-customer-features\"\n```", "```py\nfeature_group_exist = False\n```", "```py\nclient = boto3.client('sagemaker')\n```", "```py\nresponse = client.list_feature_groups(\n```", "```py\n    NameContains=FEATURE_GROUP_NAME)\n```", "```py\nif FEATURE_GROUP_NAME in response[\"FeatureGroupSummaries\"]:\n```", "```py\n  feature_group_exist = True\n```", "```py\nimport sagemaker\n```", "```py\nfrom sagemaker.session import Session\n```", "```py\nimport time\n```", "```py\nfrom sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n```", "```py\nrole = \"arn:aws:iam::<account_number>:role/sagemaker-iam-role\"\n```", "```py\nsagemaker_session = sagemaker.Session()\n```", "```py\nregion = sagemaker_session.boto_region_name\n```", "```py\ns3_bucket_name = \"feast-demo-mar-2022\"\n```", "```py\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n```", "```py\ncustomers_feature_group = FeatureGroup(\n```", "```py\n    name=FEATURE_GROUP_NAME, \n```", "```py\n    sagemaker_session=sagemaker_session\n```", "```py\n)\n```", "```py\nchurn_data[\"event_timestamp\"] = float(round(time.time()))\n```", "```py\nif not feature_group_exist:\n```", "```py\n  customers_feature_group.load_feature_definitions(\n```", "```py\n      churn_data[[col \n```", "```py\n                  for col in churn_data.columns \n```", "```py\n                  if col not in [\"customerid\"]]]) \n```", "```py\n  customer_id_def = FeatureDefinition(\n```", "```py\n      feature_name='customerid', \n```", "```py\n      feature_type=FeatureTypeEnum.STRING)\n```", "```py\n  customers_feature_group.feature_definitions = [customer_id_def] + customers_feature_group.feature_definitions\n```", "```py\n  customers_feature_group.create(\n```", "```py\n    s3_uri=f\"s3://{s3_bucket_name}/{FEATURE_GROUP_NAME}\",\n```", "```py\n    record_identifier_name=\"customerid\",\n```", "```py\n    event_time_feature_name=\"event_timestamp\",\n```", "```py\n    role_arn=role,\n```", "```py\n    enable_online_store=False\n```", "```py\n    )\n```", "```py\ningestion_results = customers_feature_group.ingest(\n```", "```py\n    churn_data, max_workers=1)\n```", "```py\ningestion_results.failed_rows\n```", "```py\n!pip install sagemaker==2.88.0 s3fs joblib scikit-learn==1.0.2 xgboost\n```", "```py\nimport sagemaker\n```", "```py\nfrom sagemaker.session import Session\n```", "```py\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n```", "```py\n#import os\n```", "```py\n#os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key_id>\"\n```", "```py\n#os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\n```", "```py\n#os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n```", "```py\nrole = \"arn:aws:iam::<account_number>:role/sagemaker-iam-role\"\n```", "```py\nFEATURE_GROUP_NAME = \"telcom-customer-features\"\n```", "```py\nsagemaker_session = sagemaker.Session()\n```", "```py\nregion = sagemaker_session.boto_region_name\n```", "```py\ns3_bucket_name = \"feast-demo-mar-2022\"\n```", "```py\ncustomers_feature_group = FeatureGroup(\n```", "```py\n    name=FEATURE_GROUP_NAME, \n```", "```py\n    sagemaker_session=sagemaker_session\n```", "```py\n)\n```", "```py\nget_latest_snapshot_query = customers_feature_group.athena_query()\n```", "```py\nquery = f\"\"\"SELECT *\n```", "```py\nFROM\n```", "```py\n    (SELECT *,\n```", "```py\n         row_number()\n```", "```py\n        OVER (PARTITION BY customerid\n```", "```py\n    ORDER BY  event_timestamp desc, Api_Invocation_Time DESC, write_time DESC) AS row_num\n```", "```py\n    FROM \"{get_latest_snapshot_query.table_name}\")\n```", "```py\nWHERE row_num = 1 and \n```", "```py\nNOT is_deleted;\"\"\"\n```", "```py\nget_latest_snapshot_query.run(\n```", "```py\n    query_string=query, \n```", "```py\n    output_location=f\"s3://{s3_bucket_name}/output\")\n```", "```py\nget_latest_snapshot_query.wait()\n```", "```py\nchurn_data = get_latest_snapshot_query.as_dataframe()\n```", "```py\nchurn_data = churn_data.drop(columns=[\"event_timestamp\", \"write_time\", \"api_invocation_time\", \"is_deleted\", \"row_num\"])\n```", "```py\nfrom sklearn.model_selection import train_test_split\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n```", "```py\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n```", "```py\nfrom sklearn.metrics import roc_auc_score,roc_curve\n```", "```py\nfrom sklearn.metrics import precision_score,recall_score\n```", "```py\nId_col = [\"customerid\"]\n```", "```py\ntarget_col = [\"churn\"]\n```", "```py\n# Split into a train and test set\n```", "```py\ntrain, test = train_test_split(churn_data,\n```", "```py\n                               test_size = .25,\n```", "```py\n                               random_state = 111)\n```", "```py\ncols    = [i for i in churn_data.columns if i not in Id_col + target_col]\n```", "```py\ntraining_x = train[cols]\n```", "```py\ntraining_y = train[target_col]\n```", "```py\ntesting_x  = test[cols]\n```", "```py\ntesting_y  = test[target_col]\n```", "```py\nimport joblib\n```", "```py\nimport boto3\n```", "```py\nmodel = XGBClassifier(max_depth=7, \n```", "```py\n                      objective='binary:logistic')\n```", "```py\nmodel.fit(training_x, training_y)\n```", "```py\npredictions = model.predict(testing_x)\n```", "```py\nprobabilities = model.predict_proba(testing_x)\n```", "```py\nprint(\"\\n Classification report : \\n\", \n```", "```py\n      classification_report(testing_y, predictions))\n```", "```py\nprint(\"Accuracy   Score : \", \n```", "```py\n      accuracy_score(testing_y, predictions))\n```", "```py\n# confusion matrix\n```", "```py\nconf_matrix = confusion_matrix(testing_y, predictions)\n```", "```py\nmodel_roc_auc = roc_auc_score(testing_y, predictions)\n```", "```py\nprint(\"Area under curve : \", model_roc_auc, \"\\n\")\n```", "```py\njoblib.dump(model, '/content/customer-churn-v0.0')\n```", "```py\ns3_client = boto3.client('s3')\n```", "```py\nresponse = s3_client.upload_file('/content/customer-churn-v0.0', s3_bucket_name, \"model-repo/customer-churn-v0.0\")\n```", "```py\nimport boto3\n```", "```py\nfrom datetime import date\n```", "```py\ns3 = boto3.client('s3')\n```", "```py\ns3.download_file(s3_bucket_name, f\"model-repo/customer-churn-v0.0\", \"customer-churn-v0.0\")\n```", "```py\nfeatures = churn_data.drop(['customerid', 'churn'], axis=1)\n```", "```py\nloaded_model = joblib.load('/content/customer-churn-v0.0')\n```", "```py\nprediction = loaded_model.predict(features)\n```", "```py\nprediction.tolist()\n```", "```py\nfile_name = f\"customer_churn_prediction_{date.today()}.parquet\"\n```", "```py\nchurn_data[\"predicted_churn\"] = prediction.tolist()\n```", "```py\ns3_url = f's3://{s3_bucket_name}/prediction_results/{file_name}'\n```", "```py\nchurn_data.to_parquet(s3_url)\n```", "```py\nimport numpy as np\n```", "```py\nimport warnings\n```", "```py\nwarnings.filterwarnings(\"ignore\")\n```", "```py\nimport plotly.offline as py\n```", "```py\nimport plotly.graph_objs as go\n```", "```py\nchurn_data.describe(include='all').T\n```", "```py\ncorr = churn_data.corr()\n```", "```py\ncols = corr.columns.tolist()\n```", "```py\ntrace = go.Heatmap(z=np.array(corr),\n```", "```py\n                   x=cols,\n```", "```py\n                   y=cols,\n```", "```py\n                   colorscale=\"Viridis\",\n```", "```py\n                   colorbar=dict(\n```", "```py\n                       title=\"Pearson Coefficient\",\n```", "```py\n                       titleside=\"right\"\n```", "```py\n                       ),\n```", "```py\n                   )\n```", "```py\nlayout = go.Layout(dict(title=\"Correlation Matrix\",\n```", "```py\n                        height=720,\n```", "```py\n                        width=800,\n```", "```py\n                        margin=dict(r=0, l=210,\n```", "```py\n                                    t=25, b=210,\n```", "```py\n                                    ),\n```", "```py\n                        )\n```", "```py\n                   )\n```", "```py\nfig = go.Figure(data=[trace], layout=layout)\n```", "```py\npy.iplot(fig)\n```", "```py\nfrom datetime import date, timedelta\n```", "```py\nimport pandas as pd\n```", "```py\npred_date = date.today()-timedelta(weeks=4)\n```", "```py\nfile_name = f\"customer_churn_prediction_{pred_date}.parquet\"\n```", "```py\nprediction_data = pd.read_parquet(f\"s3://{s3_bucket_name}/prediction_results/{file_name}\")\n```", "```py\nprediction_y = prediction_data[[\"customerid\", \n```", "```py\n                                \"predicted_churn\"]]\n```", "```py\nacutal_y = churn_data[[\"customerid\", \"churn\"]]\n```", "```py\nmerged_data = prediction_y.merge(acutal_y, on=\"customerid\")\n```", "```py\nmerged_data.head()\n```", "```py\ntesting_y = merged_data[\"churn\"]\n```", "```py\npredictions = merged_data[\"predicted_churn\"]\n```", "```py\nprint(\"\\n Classification report : \\n\", \n```", "```py\n      classification_report(testing_y, predictions))\n```", "```py\nprint(\"Accuracy   Score : \", \n```", "```py\n      accuracy_score(testing_y, predictions))\n```", "```py\n# confusion matrix\n```", "```py\nconf_matrix = confusion_matrix(testing_y, predictions)\n```", "```py\n# roc_auc_score\n```", "```py\nmodel_roc_auc = roc_auc_score(testing_y, predictions)\n```", "```py\nprint(\"Area under curve : \", model_roc_auc, \"\\n\")\n```"]