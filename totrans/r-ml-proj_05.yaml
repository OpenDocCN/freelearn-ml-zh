- en: Customer Segmentation Using Wholesale Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's competitive world, the success of an organization largely depends
    on how much it understands its customers' behavior. Understanding each customer
    individually to better tailor the organizational effort to individual needs is
    a very expensive task. Based on the size of the organization, this task can be
    very challenging as well. As an alternative, organizations rely on something called
    **segmentation**, which attempts to categorize customers into groups based on
    identified similarities. This critical aspect of customer segmentation allows
    organizations to extend their efforts to the individual needs of various customer
    subsets (if not catering to individual needs), therefore reaping greater benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the concept and importance of customer
    segmentation. We''ll then deep dive into learning the various **machine learning**
    (**ML**) methods to identify subgroups of customers based on customer characteristics.
    We''ll implement several projects using the wholesale dataset to understand the
    ML techniques for segmentation. In the next section, we''ll start by learning
    the foundations of customer segmentation and the need for ML techniques to achieve
    segmentation. We will cover the following topics as we progress:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding customer segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the wholesale customer dataset and the segmentation problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the customer segments in wholesale customer data using DIANA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the customer segments in wholesale customer data using AGNES
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding customer segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Customer segmentation, or market segmentation, at a basic level, is the partitioning
    of a broad range of potential customers in a given market into specific subgroups
    of customers, where each of the subgroups contains customers that share certain
    similarities. The following diagram depicts the formal definition of customer
    segmentation where customers are identified into three groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/674f3a2b-403c-4d6b-adf4-f7668ed5c29f.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration depicting customer segmentation definition
  prefs: []
  type: TYPE_NORMAL
- en: 'Customer segmentation needs the organizations to gather data about customers
    and analyze it to identify patterns that can be used to determine subgroups. The
    segmentation of customers could be achieved through multiple data points related
    to customers. The following are some of the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Demographics**: This data point includes race, ethnicity, age, gender, religion,
    level of education, income, life stage, marital status, occupation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Psychographics**: This data point includes lifestyle, values, socioeconomic
    standing, personality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behavioral**: This data point includes product usage, loyalties, awareness,
    occasions, knowledge, liking, and purchase patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With billions of people in the world, efficiently making use of customer segmentation
    will help organizations narrow down the pool and reach only the people that mean
    something to their business, ultimately driving conversions and revenue. The following
    are some of the specific objectives that organizations attempt to achieve through
    identifying segments in their customers:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying higher-percentage opportunities that the sales team can pursue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying customer groups that have a higher interest in the product, and
    customize the product according to the needs of high-interest customers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing very focused marketing messages to specific customer groups so as
    to drive higher-quality inbound interest in the product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the best communication channel for various segments, which might be
    email, social media, radio, or another approach, depending on the segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concentrating on the most profitable customers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upselling and cross-selling other products and services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test pricing options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying new product or service opportunities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an organization needs to perform segmentation, it can typically look for
    common characteristics, such as shared needs, common interests, similar lifestyles,
    or even similar demographic profiles and come up with segments in customer data.
    Unfortunately, creating segments is not that simple. With the availability of
    big data, organizations now have hundreds of characteristics of customers they
    can look at in order to come up with segments. It is not feasible for a person
    or few people in an organization to go through hundreds of types of data, find
    relationships between each of them, and then establish segments based on several
    different values possible for each data point. That's where unsupervised ML, called
    **clustering**, comes to rescue.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is the mechanism of using ML algorithms to identify relationships
    in different types of data, thereby yielding new segments based on those relationships.
    Simply put, clustering finds the relationship between data points so they can
    be segmented.
  prefs: []
  type: TYPE_NORMAL
- en: The terms **cluster analysis** and **customer segmentation** are closely related
    and used interchangeably by ML practitioners. However, there is an important difference
    between these terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is a tool that helps organizations put together data based on similarities
    and statistical connections. Clustering is very helpful in guiding the development
    of suitable customer segments. It also provides useful statistical measures of
    the potential target customers. While the objective for an organization is to
    identify effective customer segments from data, simply applying a clustering technique
    on data and grouping the data in itself may or may not offer effective customer
    segments. This essentially means that the output obtained from clustering, that
    is, the **clusters**, need to be further analyzed to get insight into the meaning
    of each of the clusters, and then determine which clusters can be utilized for
    downstream activities, such as business promotions. The following is a flow diagram
    that helps us to understand the role of clustering in the customer-segmentation
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/966e98d2-c974-4edb-97be-fe67c9089fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Role of clustering in customer segmentation
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand that clustering forms a stepping stone to performing
    customer segmentation, in the rest of the chapter, we will discuss various clustering
    techniques and implement projects around these techniques to create customer segments.
    For our projects, we make use of the wholesale customer dataset. Before delving
    into the projects, let's learn about the dataset and perform **exploratory data
    analysis** (**EDA**) to get a better understanding of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the wholesale customer dataset and the segmentation problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The UCI Machine Learning Repository offers the wholesale customer dataset at
    [https://archive.ics.uci.edu/ml/datasets/wholesale+customers](https://archive.ics.uci.edu/ml/datasets/wholesale+customers).
    The dataset refers to clients of a wholesale distributor. It includes the annual
    spending in **monetary units** (**m.u.**) on diverse product categories. The goal
    of these projects is to apply clustering techniques to identify segments that
    are relevant for certain business activities, such as rolling out a marketing
    campaign. Before we actually use the clustering algorithms to get clusters, let''s
    first read the data and perform some EDA to understand the data using the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7b0f664-aa72-4496-819a-c1aa3088fa7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s check whether there are any entries with missing fields in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the EDA, we see that there are `440` observations available in this dataset
    and there are eight variables. The dataset does not have any missing values. While
    the last six variables are goods that were brought by distributors from the wholesaler,
    the first two variables are factors (categorical variables) representing the location
    and channel of purchase. In our projects, we intend to identify the segments based
    on the sales into different products, therefore, the location and channel variables
    in the data are not very useful. Let''s delete them from the dataset using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We see that only six columns are retained, confirming that the deletion of non-required
    columns is successful. From the summary output in the EDA code, we can also observe
    that the scale across all the retained columns is the same so we do not have to
    explicitly normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: It may be noted that most clustering algorithms involve computation of distance
    of some form (such as Euclidean, Manhattan, Grower). It is important that data
    is scaled across the columns in the dataset so as to ensure a variable does not
    end up as a dominating one in distance computation just because of high scale.
    In case of different scales observed in columns of the data, we will rely on techniques
    such as Z-transform or min-max transform. Applying one of these techniques on
    the data ensures that the columns of the dataset are scaled appropriately therefore
    leaving no dominating variables in the dataset to be used with clustering algorithms.
    Fortunately, we do not have this issue so we can continue with the dataset as
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms impose identification of subgroups in the input dataset
    even if there are no clusters present. To ensure that we get meaningful clusters
    as output from the clustering algorithms, it is important to check whether clusters
    exist in the data at all. **Clustering tendency**, or the feasibility of the clustering
    analysis, is the process of identifying whether the clusters exist in the dataset.
    Given an input dataset, the process determines whether it has a non-random or
    non-uniform data structure distribution that will lead to meaningful clusters.
    The Hopkins statistic measure is used to determine cluster tendency. It takes
    a value between 0 and 1, and if the value of the Hopkins statistic is close to
    0 (far below 0.5), it indicates the existence of valid clusters in the dataset.
    A Hopkins value closer to 1 indicates random structures in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `factoextra` library has a built-in `get_clust_tendency()` function that
    computes the Hopkins statistic on the input dataset. Let''s apply this function
    on our wholesale dataset to determine whether the dataset is valid for clustering
    at all. The following code accomplishes the computation of the Hopkins statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The Hopkins statistic output for our dataset is very close to 0, so we can conclude
    that we have a dataset that is a good candidate for our clustering exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Categories of clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are numerous clustering algorithms available off the shelf in R. However,
    all these algorithms can be grouped into one of two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flat or partitioning algorithms**: These algorithms rely on an input parameter
    that defines the number of clusters to be identified in the dataset. The input
    parameter sometimes comes up as input from business directly or it can be established
    through certain statistical methods. For example, the **Elbow** method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical algorithms**: In these kinds of algorithms, the clusters are
    not identified in a single step. They involves multiple steps that run from a
    single cluster containing all the data points to *n* clusters containing single
    data point. Hierarchical algorithms can be further divided into the following
    two types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive type**: A top-down clustering method where all points are initially
    assigned to a single cluster. In the next step, the cluster is split into two
    clusters which are least similar. The process of splitting the clusters is recursively
    done until each point has its own cluster, for example, the **DIvisive ANAlysis**
    (**DIANA**) clustering algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agglomerative type**: A bottom-up approach where, in the initial run, each
    point in the dataset is assigned *n* unique clusters, where *n* is equal to the
    number of observations in the dataset. In the next iteration, most similar clusters
    are merged (based on the distance between the clusters). The recursive process
    of merging the clusters continues until we are left with just one cluster, for
    example, **agglomerative nesting** (**AGNES**) algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed earlier, there are numerous clustering algorithms available and
    we will focus on implementing projects using one algorithm for each type of clustering.
    We will implement project with k-means that is a flat or partitioning type clustering
    algorithm. We will then do customer segmentation with DIANA and AGNES, which are
    divisive and agglomerative, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the customer segments in wholesale customer data using k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm is perhaps the most popular and commonly-used clustering
    method from partitioning clustering type. Though we usually call the clustering
    algorithm k-means, multiple implementations of this algorithm exist, namely the
    **MacQueen**, **Lloyd and Forgy**, and **Hartigan-Wong** algorithms. It has been
    studied and found that the Hartigan-Wong algorithm performs better than the other
    two algorithms in most situations. K-means in R makes use of the Hartigan-Wong
    implementation by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm requires the k-value to be passed as a parameter. The
    parameter indicates the number of clusters to be made with the input data. It
    is often a challenge for practitioners to determine the optimal k-value. Sometimes,
    we can go to a business and ask them how many clusters they would expect in the
    data. The answer from the business directly translates to be the *k* parameter
    value to be fed to the algorithm. In most cases though, the business is clueless
    as to the number of clusters. In such a case, the onus will be on the ML practitioner
    to determine the k-value. Fortunately, there are several methods available to
    determine this value. These methods can be classified into the following two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct methods**: These methods rely on optimizing a criterion, such as *within
    cluster sums of squares* or *the average silhouette*. Examples of this method
    include the **V Elbow method** and the **V Silhouette method**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing methods**: These methods consists of comparing evidence against a
    null hypothesis. Gap statistic is one popular example of this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to Elbow, Silhouette, and gap statistic methods, there are more
    than 30 other indices and methods that have been published for identifying the
    optimal number of clusters. We will not delve into the theoretical details of
    these methods, as covering 30 methods in a single chapter is not practical. However,
    R offers an excellent library function, called `NbClust` that makes it easy for
    us to implement all these methods in one go. The `NbClust` function is so powerful
    that it determines the optimal clusters by varying all combinations of number
    of clusters, distance measures, and clustering methods and all in one go! Once
    the library function computes all 30 indices, the *majority rule* is applied on
    the output to determine the optimal number of clusters, that is, the k-value to
    be used as input to the algorithm. Let''s implement `NbClust` for our wholesale
    dataset to determine the optimal k-value using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As per the conclusion, we see the k-value that may be used for our problem
    is `3`. Additionally, plotting an elbow curve with the total within-groups sums
    of squares against the number of clusters in a k-means solution can be helpful
    in determining the optimal number of clusters. K-means is defined by the objective
    function, which tries to minimize the sum of all squared distances within a cluster
    (intra-cluster distance) for all clusters. In the elbow-curve plotting method,
    we compute the intra-cluster distance with different values of k, and the intra-cluster
    distance with different k''s is plotted as a graph. A bend in the elbow curve
    suggests the k-value that is optimal for the dataset. The elbow curve can be obtained
    within R using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f2649b3-bb6f-4598-97e9-646ff9060718.png)'
  prefs: []
  type: TYPE_IMG
- en: Even with the elbow curve method output, we see that the number of optimal clusters
    for our dataset is `3`.
  prefs: []
  type: TYPE_NORMAL
- en: We see from the `NbClust` function that we have used the Euclidean distance
    as the distance. There are a number of distance types (`euclidean`, `maximum`,
    `manhattan`, `canberra`, `binary`, `minkowski`) that we could have used as values
    for this distance parameter in the `NbClust` function. Let's understand what this
    distance actually means. We are already aware that each observation in our dataset
    is formed by values that represent features. This essentially means each observation
    of our dataset can be represented as points in multidimensional space. If we have
    to say that two observations are similar, we would expect the distance between
    the two points in the multidimensional space to be lower, that is, both these
    points in multidimensional space are close to each other. A high distance value
    between the two points indicates that they are very dissimilar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Euclidean, Manhattan, and other types of distance measures are various
    ways in which distance can be measured given two points in a multidimensional
    space. Each of the distance measures involves a specific technique to compute
    the distance between the two points. The techniques involved in Manhattan and
    Euclidean, and the difference between their measures, are illustrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c1e5f14-ef4e-4c6e-bba1-564f421de849.png)'
  prefs: []
  type: TYPE_IMG
- en: Difference between Manhattan and Euclidean distance measures
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean distance measures the shortest distance in the plane, whereas
    the Manhattan metric is the shortest path if one is allowed to move horizontally
    or vertically.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if `a` and `b` are two points where `a= (0,0)`, `b = (3,4)`, then
    take a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dist_euclid (a,b) = sqrt(3^2+4^2) = 5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dist_manhattan(a,b) = 3+4 = 7`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a=(a1,...,an), b=(b1,...,bn)` (in *n* dimensions and points)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dist_euclid (a,b) = sqrt((a1-b1)^2 + ... + (an-bn)^2)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dist_manhattan(a,b) = sum(abs(a1-b1) + ... + abs(an-bn))`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both measure the shortest paths, but the Euclidean metric doesn't have any restrictions
    while the Manhattan metric only allows paths constant in all but one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, the other distance measures also involve a certain unique to measure
    the similarity between given points. We will not be going through each one of
    the techniques in detail in this chapter, but the idea to get is that a distance
    measure basically defines the level of similarity between given observations.
    It may be noted that a distance measure is not just used in `NbClust` but in multiple
    ML algorithms, including k-means.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve learned the various ways to identify our k-value and have implemented
    them to identify the optimal number of clusters for our wholesale dataset, let''s
    implement the k-means algorithm with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the output k-means, we can observe and infer several things about the output
    clusters. Obviously, three clusters are formed and this is in line with our k
    parameter that was passed to the algorithm. We see that the first cluster has
    330 observations in it, and the second and third clusters are small with just
    50 and 60 observations. The k-means output also provides us with the cluster centroids.
    The **centroid** is a representative of all points in a particular cluster. As
    it is not feasible to study each of the individual observations assigned to a
    cluster and determine the business characteristics of the cluster, the cluster
    centroid may be used as a pseudo for the points in the cluster. The cluster centroid
    helps us to quickly arrive at a conclusion in terms of the definition of contents
    of the cluster. The k-means output also produced the cluster assignment for each
    observation. Each of the observations in our wholesale dataset is assigned to
    one of the three clusters (1,2,3).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to view the clustering results by using the `fviz_cluster()`
    function available in the `factoextra` library. The function provides a nice illustration
    of the clusters. If there are more than two dimensions (variables), `fviz_cluster`
    will perform **principal component analysis** (**PCA**) and plot the observations
    based on the first two principal components that explain the majority of the variance.
    The clusters visualization can be created though the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following graph as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fbbbb41-9ac6-4c9b-a97c-f552034ea036.png)'
  prefs: []
  type: TYPE_IMG
- en: Working mechanics of the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The execution of the k-means algorithm involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select *k* observations from the dataset as the initial cluster centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each observation in the dataset, perform the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the distance between the observation and each of the cluster centroids.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the cluster centroid that has minimum distance with the observation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the observation to such closest centroid.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: With all points assigned to one of the cluster centroids, compute new cluster
    centroids. This can be done by taking the mean of all the points assigned to a
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform *step 2* and *step 3* repeatedly until the cluster centroids (mean)
    do not change or until a user-defined number of iterations is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One key thing to note in k-means is that the cluster centroids in the initial
    step are selected randomly and the initial cluster assignments are done based
    on the distance between the actual observations and the randomly-picked cluster
    centroids. This essentially means that if we were to pick observations as cluster
    centroids in the initial step other than the observations that were chosen, we
    would obtain different clusters than the one we have obtained. In technical terms,
    this is called a **non-globally-optimal solution** or a **locally-optimal solution**.
    The `cluster` library's k-means function has the `nstart` option, which works
    around this problem of the non-globally-optimal solution encountered with the
    k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `nstart` option enables the algorithm to try several random starts (instead
    of just one) by drawing a number of center observations from the datasets. It
    then checks the cluster sum of squares and proceeds with the best start, resulting
    in a more stable output. In our case, we set the `nstart` value as `50`, therefore
    the best start is chosen by k-means post checking it with 50 random initial sets
    of cluster centroids. The following diagram depicts the high-level steps involved
    in the k-means clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fe651c3-aef1-42f3-902d-cc36a6222eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Steps in k-means clustering
  prefs: []
  type: TYPE_NORMAL
- en: In supervised ML methods, such as classification, we have ground truth, therefore,
    we will be able to able to compare our predictions with the ground truth and measure
    to report the performance of our classification. Unlike the supervised ML method,
    in clustering, we do not have any ground truth. Therefore, computing the performance
    measurement with respect to clustering is a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to performance measurement, we use a pseudo-measure called
    **cluster quality**. The cluster quality is generally computed through measures
    known as intra-cluster distance and inter-cluster distance, which are illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b460f049-6e7f-4fec-89d4-d2b37af5d12b.png)'
  prefs: []
  type: TYPE_IMG
- en: Intra-cluster distance and inter-cluster distance defined
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the clustering task is to obtain good-quality clusters. Clusters
    are termed as **high-quality clusters** if the distance within the observations
    is minimum and the distance between the clusters themselves is maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways inter-cluster and intra-cluster distances can be measured:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intra-cluster**: This distance can be measured as (sum, minimum, maximum,
    or mean) of the (absolute/squared) distance between all pairs of points in the
    cluster (or) *diameter*–two farthest points (or) between the centroid and all
    points in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inter-cluster**: This distance is measured as sum of the (squared) distance
    between all pairs of clusters, where distance between two clusters itself is computed
    as one of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance between their centroids
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance between farthest pair of points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance between the closest pair of points belonging to the clusters
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, there is no way to pinpoint the preferred inter-cluster distance
    and intra-cluster distance values. The **Silhouette index** is one metric that
    is based on inter-cluster distance and intra-cluster distance that can be readily
    computed and easily interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Silhouette index is computed using the mean intra-cluster distance, *a*,
    and the mean nearest-cluster distance, *b*, for each of the observations participating
    in the clustering exercise. The Silhouette index for an observation is given by
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e22c4857-0e38-4471-8e3b-5902a1a357e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *b* is the distance between an observation and the nearest cluster that
    the observation is not a part of.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette index value ranges between [-1, 1]. A value of +1 for an observation
    indicates that the observation is far away from its neighboring cluster and it
    is very close to the cluster it is assigned to. Similarly, a value of -1 tells
    us that the observation is close to its neighboring cluster than to the cluster
    it is assigned to. A value of 0 means it's at the boundary of the distance between
    the two clusters. A value of +1 is ideal and -1 is the least preferred. Hence,
    the higher the value, the better the quality of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cluster` library offers the Silhouette function, which can be readily
    used on our k-means clustering output to understand the quality of the clusters
    that were formed. The following code computes the Silhouette index for our three
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen, the Silhouette index can range from -1 to +1, and the latter
    is preferred. From the output, we see that the clusters are all good quality clusters,
    as the average width is a positive number closer to 1 than -1.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the Silhouette index can be used not just to measure the quality of
    clusters formed but also to compute the k-value. Similar to Elbow method, we can
    iterate through multiple values of k and then identify the k that yields the maximum
    Silhouette index values across the clusters. Clustering can then be performed
    using the k that was identified.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous cluster-quality measures described in the literature. The
    Silhouette index is just one measure we covered in this chapter because of its
    popularity in the ML community. The `clusterCrit` library offers a wide range
    of indices to measure the quality of clusters. We are not going to explore the
    other cluster-quality metrics here, but interested readers should refer to this
    library for further information on how to compute cluster quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have covered the k-means clustering algorithm to identify the clusters,
    but the original segmentation task we started with does not end here. Segmentation
    further spans to the task of understanding what each of the clusters formed from
    the clustering exercise mean to businesses. For example, we take our cluster centroids
    obtained from k-means and an attempt is made to identify what these are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some sample insights into each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cluster 1 is low spenders (average spending: 22,841.744), with the majority
    of spending allocated to the fresh category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster 2 is high spenders (average spending: 70,741.42), with the majority
    of spending in the grocery category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster 3 is medium spenders (average spending : 59,077.568), with the majority
    of spending in the fresh category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, based on the business objective, one or more clusters may be selected
    to target. For example, if the objective is to have high spenders spend more,
    promotions may be rolled out to cluster 2 individuals with spending in the `Frozen`
    and `Delicatessen` products less than the centroid values (that is, `Frozen`:
    `1,996.680` and `Delicatessen`: `2,252.020`).'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the customer segments in the wholesale customer data using DIANA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering algorithms are a good choice when we don't necessarily
    have circular (or hyperspherical) clusters in the data, and we essentially don't
    know the number of clusters in advance. With hierarchical clustering algorithm,
    unlike the flat or partitioning algorithms, there is no requirement to decide
    and pass the number of clusters to be formed prior to applying the algorithm on
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering results in a dendogram (tree diagram) that can be visually
    verified to easily determine the number of clusters. Visual verification enables
    us to perform cuts in the dendrogram at suitable places.
  prefs: []
  type: TYPE_NORMAL
- en: The results produced by this type of clustering algorithm are reproducible as
    the algorithm is not sensitive to the choice of the distance metric. In other
    words, irrespective of the distance metric chosen, we will get the same results.
    This type of clustering is also suitable for datasets of a higher complexity (quadratic)
    and in particular for exploring the hierarchical relationships that exist between
    the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Divisive hierarchical clustering, also known as **DIvisive ANAlysis** (**DIANA**),
    is a hierarchical clustering algorithm that follows a top-down approach to identify
    clusters in a given dataset. Here are the steps in DIANA to identify the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: All observations of the dataset are assigned to the root, so in the initial
    step only a single cluster is formed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each iteration, the most heterogeneous cluster is partitioned into two.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Step 2** is repeated until all the observations are in their own cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b267fbb4-0e11-429f-9534-8ac5cc8b72ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Working of divisive hierarchical clustering algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'One obvious question that comes up is about the technique used by the algorithm
    to split the cluster into two. The answer is that it is performed according to
    some (dis)similarity measure. The Euclidean distance is used to measure the distance
    between two given points. This algorithm works by splitting the data on the basis
    of the farthest-distance measure of all the pairwise distances between the data
    points. Linkage defines the specific details of fartherness of the data points.
    The next figure illustrates the various linkages considered by DIANA for splitting
    the clusters. Here are some of the distances considered to split the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-link**: Nearest distance or single linkage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete-link**: Farthest distance or complete linkage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average-link**: Average distance or average linkage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid-link**: Centroid distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward''s method**: Sum of squared `euclidean` distance is minimized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram to better understand the preceding distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f0d00c3-798b-4422-9ab9-c5ab5cad4866.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration depicting various linkage types used by DIANA
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the linkage type to be used is passed as a parameter to the clustering
    algorithm. The `cluster` library offers the `diana` function to perform clustering.
    Let''s apply it on our wholesale dataset with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccfc0e26-aee4-4f83-b19d-16d56310d220.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `plot.hclust()` and `plot.dendrogram()` functions may also be used on the
    DIANA clustering output. `plot.dendrogram()` yields the dendogram that follows
    the natural structure of the splits as done by the DIANA algorithm. Use the following
    code to generate the dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69e2660f-7ac4-44e3-a13c-640f2584e12b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the dendrogram output, each leaf that appears on the right relates to each
    observation in the dataset. As we traverse from right to left, observations that
    are similar to each other are grouped into one branch, which are themselves fused
    at a higher level.
  prefs: []
  type: TYPE_NORMAL
- en: The higher level of the fusion, provided on the horizontal axis, indicates the
    similarity between two observations. The higher the fusion, the more similar the
    observations are. It may be noted that conclusions about the proximity of two
    observations can be drawn only based on the level where branches containing those
    two observations are first fused. In order to identify clusters, we can cut the
    dendrogram at a certain level. The level at which the cut is made defines the
    number of clusters obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make use of the `cutree()` function to obtain the cluster assignment
    for each of the observations in our dataset. Execute the following code to obtain
    the clusters and review the clustering output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also visualize the clustering output through the `fviz_cluster` function
    in the `factoextra` library. Use the following code to get the required visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5b6f638-67e4-46b8-ac3a-e1a4603ef222.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also possible to color-code the clusters within the dendogram itself.
    This can be accomplished with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61cd4544-de37-4978-8a15-ff63d74d7ae6.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the clusters are identified, the steps we discussed to evaluate the
    cluster quality (through the Silhouette index) apply here as well. As we have
    already covered this topic under the k-means clustering algorithm, we are not
    going to repeat the steps here. The code and interpretation of the output remains
    the same as what was discussed under k-means.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, the cluster's output is not the final point to customer
    segmentation exercise we have on hand. Similar to the discussion we had on under
    the k-means algorithm, we could analyze the DIANA cluster output to identify meaningful
    segments so as to roll out business objectives to those specifically-identified
    segments.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the customer segments in the wholesale customers data using AGNES
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AGNES is the reverse of DIANA in the sense that it follows a bottom-up approach
    to clustering the dataset. The following diagram illustrates the working principle
    of the AGNES algorithm for clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56080695-a543-418e-ae0c-0429532fa2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Working of agglomerative hierarchical clustering algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Except for the bottom-up approach followed by AGNES, the implementation details
    behind the algorithm are the same as for DIANA; therefore, we won''t repeat the
    discussion of the concepts here. The following code block clusters our wholesale
    dataset into three clusters with AGNES; it also creates a visualization of the
    clusters thus formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output that you will obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/809fb5ed-f69d-40e4-9f90-966bb3953734.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8337198-628a-46bc-a3fe-3a3ab237d047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6abc2b7f-b61c-40c4-8c72-7606a16b01f8.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the AGNES clustering output that a large number of observations
    from the dataset are assigned to one cluster and very few observations were assigned
    to the other clusters. This is not a great output for our segmentation downstream
    exercise. To obtain better cluster assignments, you could try using other cluster-linkage
    methods aside from the default average linkage method currently used by the AGNES
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the concept of segmentation and its association
    with clustering, an ML unsupervised learning technique. We made use of the wholesale
    dataset available from the UCI repository and implemented clustering using the
    k-means, DIANA, and AGNES algorithms. During the course of this chapter, we also
    studied various aspects related to clustering, such as tendency to cluster, distance,
    linkage measures, and methods to identify the right number of clusters, and measuring
    the output of clustering. We also explored making use of the clustering output
    for customer-segmentation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Can computers see and identify objects and living creatures like humans do?
    Let's explore the answer to this question in the next chapter.
  prefs: []
  type: TYPE_NORMAL
