["```py\ndf = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf.head() \n```", "```py\ndef basic_cleaning(text):\n    text=re.sub(r'https?://www\\.\\S+\\.com','',text)\n    text=re.sub(r'[^A-Za-z|\\s]','',text)\n    text=re.sub(r'\\*+','swear',text) # Capture swear words that are **** out\n    return text \n```", "```py\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\" #emoticons\n                           u\"\\U0001F300-\\U0001F5FF\" #symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\" #transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\" #flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text) \n```", "```py\ndef remove_multiplechars(text):\n    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n    return text \n```", "```py\ndef clean(df):\n    for col in ['text']:#,'selected_text']:\n        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_emoji(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_html(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n    return df \n```", "```py\ndef fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n\n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n\n    return np.array(all_ids) \n```", "```py\ndef preprocess_news(df,stop=stop,n=1,col='text'):\n    '''Function to preprocess and create corpus'''\n    new_corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for text in df[col]:\n        words=[w for w in word_tokenize(text) if (w not in stop)]\n\n        words=[lem.lemmatize(w) for w in words if(len(w)>n)]\n\n        new_corpus.append(words)\n\n    new_corpus=[word for l in new_corpus for word in l]\n    return new_corpus \n```", "```py\ndf.dropna(inplace=True)\ndf_clean = clean(df)\ndf_clean_selection = df_clean.sample(frac=1)\nX = df_clean_selection.text.values\ny = pd.get_dummies(df_clean_selection.sentiment) \n```", "```py\ntokenizer = text.Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(X))\nlist_tokenized_train = tokenizer.texts_to_sequences(X)\nX_t = sequence.pad_sequences(list_tokenized_train, maxlen=128) \n```", "```py\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  \n# Save the loaded tokenizer locally\nsave_path = '/kaggle/working/distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer(\n                 'distilbert_base_uncased/vocab.txt', lowercase=True)\nfast_tokenizer \n```", "```py\nX = fast_encode(df_clean_selection.text.astype(str),\n                fast_tokenizer,\n                maxlen=128) \n```", "```py\ntransformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\nembedding_size = 128\ninput_ = Input(shape=(100,))\ninp = Input(shape=(128, ))\nembedding_matrix=transformer_layer.weights[0].numpy()\nx = Embedding(embedding_matrix.shape[0],\n              embedding_matrix.shape[1],\n              embeddings_initializer=Constant(embedding_matrix),\n              trainable=False)(inp)\nx = Bidirectional(LSTM(50, return_sequences=True))(x)\nx = Bidirectional(LSTM(25, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.5)(x)\nx = Dense(50, activation='relu', kernel_regularizer='L1L2')(x)\nx = Dropout(0.5)(x)\nx = Dense(3, activation='softmax')(x)\nmodel_DistilBert = Model(inputs=[inp], outputs=x)\nmodel_DistilBert.compile(loss='categorical_crossentropy',\n                              optimizer='adam',\n                              metrics=['accuracy']) \n```", "```py\nmodel_DistilBert.fit(X,y,batch_size=32,epochs=10,validation_split=0.1) \n```", "```py\nEpoch 1/10\n27480/27480 [==============================] - 480s 17ms/step - loss: 0.5100 - accuracy: 0.7994\nEpoch 2/10\n27480/27480 [==============================] - 479s 17ms/step - loss: 0.4956 - accuracy: 0.8100\nEpoch 3/10\n27480/27480 [==============================] - 475s 17ms/step - loss: 0.4740 - accuracy: 0.8158\nEpoch 4/10\n27480/27480 [==============================] - 475s 17ms/step - loss: 0.4528 - accuracy: 0.8275\nEpoch 5/10\n27480/27480 [==============================] - 475s 17ms/step - loss: 0.4318 - accuracy: 0.8364\nEpoch 6/10\n27480/27480 [==============================] - 475s 17ms/step - loss: 0.4069 - accuracy: 0.8441\nEpoch 7/10\n27480/27480 [==============================] - 477s 17ms/step - loss: 0.3839 - accuracy: 0.8572 \n```", "```py\ndf_clean_final = df_clean.sample(frac=1)\nX_train = fast_encode(df_clean_selection.text.astype(str),\n                      fast_tokenizer,\n                      maxlen=128)\ny_train = y \n```", "```py\nAdam_name = adam(lr=0.001)\nmodel_DistilBert.compile(loss='categorical_crossentropy',optimizer=Adam_name,metrics=['accuracy'])\nhistory = model_DistilBert.fit(X_train,y_train,batch_size=32,epochs=10) \n```", "```py\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_test.dropna(inplace=True)\ndf_clean_test = clean(df_test)\nX_test = fast_encode(df_clean_test.text.values.astype(str),\n                     fast_tokenizer,\n                     maxlen=128)\ny_test = df_clean_test.sentiment \n```", "```py\ny_preds = model_DistilBert.predict(X_test)\ny_predictions = pd.DataFrame(y_preds,\n                             columns=['negative','neutral','positive'])\ny_predictions_final = y_predictions.idxmax(axis=1)\naccuracy = accuracy_score(y_test,y_predictions_final)\nprint(f\"The final model shows {accuracy:.2f} accuracy on the test set.\") \n```", "```py\ndef word_count(xstring):\n    return xstring.split().str.len() \n```", "```py\ndef spearman_corr(y_true, y_pred):\n        if np.ndim(y_pred) == 2:\n            corr = np.mean([stats.spearmanr(y_true[:, i],\n                                            y_pred[:, i])[0]\nfor i in range(y_true.shape[1])])\n        else:\n            corr = stats.spearmanr(y_true, y_pred)[0]\n        return corr\n\ncustom_scorer = make_scorer(spearman_corr, greater_is_better=True) \n```", "```py\ndef chunks(l, n):\n    for i in range(0, len(l), n):\n        yield l[i:i + n] \n```", "```py\ndef fetch_vectors(string_list, batch_size=64):\n    # Inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-    for-the-first-time/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained\n                    (\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained\n                (\"../input/distilbertbaseuncased/\")\n    model.to(DEVICE)\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n        with torch.no_grad():\n            last_hidden_states = model(input_ids,\n                                       attention_mask=attention_mask)\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n    fin_features = np.vstack(fin_features)\n    return fin_features \n```", "```py\nxtrain = pd.read_csv(data_dir + 'train.csv')\nxtest = pd.read_csv(data_dir + 'test.csv')\nxtrain.head(4) \n```", "```py\ntarget_cols = ['question_asker_intent_understanding',\n               'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking',\n               'question_has_commonly_accepted_answer', \n               'question_interestingness_others',\n               'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice', \n               'question_type_compare', 'question_type_consequence', \n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation',\n               'question_type_spelling', \n               'question_well_written', 'answer_helpful', \n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written'] \n```", "```py\nfor colname in ['question_title', 'question_body', 'answer']:\n    newname = colname + '_word_len'\n\n    xtrain[newname] = xtrain[colname].str.split().str.len()\n    xtest[newname] = xtest[colname].str.split().str.len() \n```", "```py\ncolname = 'answer'\nxtrain[colname+'_div'] = xtrain[colname].apply\n                         (lambda s: len(set(s.split())) / len(s.split()) )\nxtest[colname+'_div'] = xtest[colname].apply\n                        (lambda s: len(set(s.split())) / len(s.split()) ) \n```", "```py\nfor df in [xtrain, xtest]:\n    df['domcom'] = df['question_user_page'].apply\n                   (lambda s: s.split('://')[1].split('/')[0].split('.'))\n    # Count components\n    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))\n    # Pad the length in case some domains have fewer components in the name\n    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])\n    # Components\n    for ii in range(0,4):\n        df['dom_'+str(ii)] = df['domcom'].apply(lambda s: s[ii]) \n```", "```py\n# Shared elements\nfor df in [xtrain, xtest]:\n    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n    df['qa_word_overlap_norm1'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n    df['qa_word_overlap_norm2'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n    df.drop(['q_words', 'a_words'], axis = 1, inplace = True) \n```", "```py\nfor df in [xtrain, xtest]:\n\n    ## Number of characters in the text ##\n    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n    df[\"question_body_num_chars\"] = df[\"question_body\"].apply(lambda x: len(str(x)))\n    df[\"answer_num_chars\"] = df[\"answer\"].apply(lambda x: len(str(x)))\n    ## Number of stopwords in the text ##\n    df[\"question_title_num_stopwords\"] = df[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    df[\"question_body_num_stopwords\"] = df[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    df[\"answer_num_stopwords\"] = df[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    ## Number of punctuations in the text ##\n    df[\"question_title_num_punctuations\"] =df['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n    df[\"question_body_num_punctuations\"] =df['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n    df[\"answer_num_punctuations\"] =df['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n    ## Number of title case words in the text ##\n    df[\"question_title_num_words_upper\"] = df[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df[\"question_body_num_words_upper\"] = df[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df[\"answer_num_words_upper\"] = df[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()])) \n```", "```py\nmodule_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url) \n```", "```py\nembeddings_train = {}\nembeddings_test = {}\nfor text in ['question_title', 'question_body', 'answer']:\n    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()\n\n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n\n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n\n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    print(text) \n```", "```py\nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T \n```", "```py\nfor ii in range(0,6):\n    xtrain['dist'+str(ii)] = dist_features_train[:,ii]\n    xtest['dist'+str(ii)] = dist_features_test[:,ii] \n```", "```py\nlimit_char = 5000\nlimit_word = 25000 \n```", "```py\ntitle_col = 'question_title'\ntitle_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\ntitle_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer(sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n]) \n```", "```py\nbody_col = 'question_body'\nbody_transformer = Pipeline([\n    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\nbody_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n]) \n```", "```py\nanswer_col = 'answer'\nanswer_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\nanswer_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n]) \n```", "```py\nnum_cols = [\n    'question_title_word_len', 'question_body_word_len',\n    'answer_word_len', 'answer_div',\n    'question_title_num_chars','question_body_num_chars',\n    'answer_num_chars',\n    'question_title_num_stopwords','question_body_num_stopwords',\n    'answer_num_stopwords',\n    'question_title_num_punctuations',\n    'question_body_num_punctuations','answer_num_punctuations',\n    'question_title_num_words_upper',\n    'question_body_num_words_upper','answer_num_words_upper',\n    'dist0', 'dist1', 'dist2', 'dist3', 'dist4',       'dist5'\n]\nnum_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scale', PowerTransformer(method='yeo-johnson'))\n]) \n```", "```py\ncat_cols = [ 'dom_0',  'dom_1', 'dom_2', \n    'dom_3', 'category','is_question_no_name_user',\n    'is_answer_no_name_user','dom_cnt'\n]\ncat_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value='')),\n    ('encode', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('title', title_transformer, title_col),\n        ('title2', title_transformer2, title_col),\n        ('body', body_transformer, body_col),\n        ('body2', body_transformer2, body_col),\n        ('answer', answer_transformer, answer_col),\n        ('answer2', answer_transformer2, answer_col),\n        ('num', num_transformer, num_cols),\n        ('cat', cat_transformer, cat_cols)\n    ]\n) \n```", "```py\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('estimator',Ridge(random_state=RANDOM_STATE))\n]) \n```", "```py\nnfolds = 5\nmvalid = np.zeros((xtrain.shape[0], len(target_cols)))\nmfull = np.zeros((xtest.shape[0], len(target_cols)))\nkf = GroupKFold(n_splits= nfolds).split(X=xtrain.question_body, groups=xtrain.question_body) \n```", "```py\nfor ind, (train_index, test_index) in enumerate(kf):\n\n    # Split the data into training and validation\n    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]\n    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]\n    for ii in range(0, ytrain.shape[1]):\n        # Fit model\n        be = clone(pipeline)\n        be.fit(x0, np.array(y0)[:,ii])\n        filename = 'ridge_f' + str(ind) + '_c' + str(ii) + '.pkl'\n        pickle.dump(be, open(filename, 'wb'))\n\n        # Storage matrices for the OOF and test predictions, respectively\n        mvalid[test_index, ii] = be.predict(x1)\n        mfull[:,ii] += be.predict(xtest)/nfolds\n\n    print('---') \n```", "```py\ncorvec = np.zeros((ytrain.shape[1],1))\nfor ii in range(0, ytrain.shape[1]):\n    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]\n    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]\n\n    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]\n\nprint(corvec.mean()) \n```", "```py\ndef get_synonyms(word):\n\n    synonyms = set()\n\n    for syn in wordnet.synsets(word):\n        for l in syn.lemmas():\n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    if word in synonyms:\n        synonyms.remove(word)\n\n    return list(synonyms) \n```", "```py\ndef synonym_replacement(words, n):    \n    words = words.split()    \n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n\n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n\n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n\n        if num_replaced >= n: # Only replace up to n words\n            break\n    sentence = ' '.join(new_words)\n    return sentence \n```", "```py\nprint(f\" Example of Synonym Replacement: {synonym_replacement('The quick brown fox jumps over the lazy dog',4)}\") \n```", "```py\nExample of Synonym Replacement: The spry brown university fox jumpstart over the lazy detent \n```", "```py\ntrial_sent = data['text'][25]\nprint(trial_sent)\nthe free fillin' app on my ipod is fun, im addicted\nfor n in range(3):\n    print(f\" Example of Synonym Replacement: {synonym_replacement(trial_sent,n)}\") \n```", "```py\nExample of Synonym Replacement: the free fillin' app on my ipod is fun, im addict\nExample of Synonym Replacement: the innocent fillin' app on my ipod is fun, im addicted\nExample of Synonym Replacement: the relinquish fillin' app on my ipod is fun, im addict \n```", "```py\ndef swap_word(new_words):    \n    random_idx_1 = random.randint(0, len(new_words)-1)\n    random_idx_2 = random_idx_1\n    counter = 0    \n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words)-1)\n        counter += 1        \n        if counter > 3:\n            return new_words\n\n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n    return new_words \n```", "```py\n# n is the number of words to be swapped\ndef random_swap(words, n):    \n    words = words.split()\n    new_words = words.copy()\n\n    for _ in range(n):\n        new_words = swap_word(new_words)\n\n    sentence = ' '.join(new_words)    \n    return sentence \n```", "```py\ndef random_deletion(words, p):\n    words = words.split()\n\n    # Obviously, if there's only one word, don't delete it\n    if len(words) == 1:\n        return words\n    # Randomly delete words with probability p\n    new_words = []\n    for word in words:\n        r = random.uniform(0, 1)\n        if r > p:\n            new_words.append(word)\n    # If you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n    sentence = ' '.join(new_words)\n\n    return sentence \n```", "```py\nprint(random_deletion(trial_sent,0.2))\nprint(random_deletion(trial_sent,0.3))\nprint(random_deletion(trial_sent,0.4)) \n```", "```py\nthe free fillin' app on my is fun, addicted\nfree fillin' app on my ipod is im addicted\nthe free on my ipod is fun, im \n```", "```py\ndef random_insertion(words, n):    \n    words = words.split()\n    new_words = words.copy()    \n    for _ in range(n):\n        add_word(new_words)        \n    sentence = ' '.join(new_words)\n    return sentence\ndef add_word(new_words):    \n    synonyms = []\n    counter = 0\n\n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return        \n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym) \n```", "```py\nprint(random_insertion(trial_sent,1))\nprint(random_insertion(trial_sent,2))\nprint(random_insertion(trial_sent,3)) \n```", "```py\nthe free fillin' app on my addict ipod is fun, im addicted\nthe complimentary free fillin' app on my ipod along is fun, im addicted\nthe free along fillin' app addict on my ipod along is fun, im addicted \n```", "```py\ndef aug(sent,n,p):\n    print(f\" Original Sentence : {sent}\")\n    print(f\" SR Augmented Sentence : {synonym_replacement(sent,n)}\")\n    print(f\" RD Augmented Sentence : {random_deletion(sent,p)}\")\n    print(f\" RS Augmented Sentence : {random_swap(sent,n)}\")\n    print(f\" RI Augmented Sentence : {random_insertion(sent,n)}\")\naug(trial_sent,4,0.3) \n```", "```py\nOriginal Sentence : the free fillin' app on my ipod is fun, im addicted\nSR Augmented Sentence : the disembarrass fillin' app on my ipod is fun, im hook\nRD Augmented Sentence : the free app on my ipod fun, im addicted\nRS Augmented Sentence : on free fillin' ipod is my the app fun, im addicted\nRI Augmented Sentence : the free fillin' app on gratis addict my ipod is complimentary make up fun, im addicted \n```", "```py\n! pip install nlpaug \n```", "```py\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\ntest_sentence = \"I genuinely have no idea what the output of this sequence of words will be - it will be interesting to find out what nlpaug can do with this!\" \n```", "```py\naug = nac.KeyboardAug(name='Keyboard_Aug', aug_char_min=1,\n                      aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n                      aug_word_min=1, aug_word_max=10, stopwords=None,\n                      tokenizer=None, reverse_tokenizer=None,\n                      include_special_char=True, include_numeric=True,\n                      include_upper_case=True, lang='en', verbose=0,\n                      stopwords_regex=None, model_path=None, min_char=4)\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug) \n```", "```py\nI genuinely have no idea what the output of this sequence of words will be - it will be interesting to find out what nlpaug can do with this!\nI geb&ine:y have no kdeZ qhQt the 8uYput of tTid sequsnDr of aorVs will be - it wi,k be jnterewtlHg to find out what nlpaug can do with this! \n```", "```py\naug = nac.OcrAug(name='OCR_Aug', aug_char_min=1, aug_char_max=10,\n                 aug_char_p=0.3, aug_word_p=0.3, aug_word_min=1,\n                 aug_word_max=10, stopwords=None, tokenizer=None,\n                 reverse_tokenizer=None, verbose=0,\n                 stopwords_regex=None, min_char=1)\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug) \n```", "```py\nI genuinely have no idea what the output of this sequence of words will be - it will be interesting to find out what nlpaug can do with this!\nI 9enoine1y have no idea what the ootpot of this sequence of wokd8 will be - it will be inteke8tin9 to find out what nlpaug can du with this! \n```", "```py\naug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3,\n                     lang='eng', stopwords=None, tokenizer=None,\n                     reverse_tokenizer=None, stopwords_regex=None,\n                     verbose=0)\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug) \n```", "```py\nI genuinely have no idea what the output of this sequence of words will be - it will be interesting to find out what nlpaug can do with this!\nI genuinely lack no idea what the output of this sequence of words will differ - it will differ uninteresting to lose out what nlpaug can unmake with this! \n```", "```py\naug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased',\n                                model_type='', action='substitute',\n                                # temperature=1.0,\n                                top_k=100,\n                                # top_p=None,\n                                name='ContextualWordEmbs_Aug', aug_min=1,\n                                aug_max=10, aug_p=0.3, \n                                stopwords=None, device='cpu',\n                                force_reload=False,\n                                # optimize=None,\n                                stopwords_regex=None,\n                                verbose=0, silence=True)\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug) \n```", "```py\nI genuinely have no idea what the output of this sequence of words will be - it will be interesting to find out what nlpaug can do with this!\ni genuinely have no clue what his rest of this series of words will say - its will seemed impossible to find just what we can do with this! \n```"]