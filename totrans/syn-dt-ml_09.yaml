- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Diffusion Models for Synthetic Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces you to diffusion models, which are cutting-edge approaches
    to synthetic data generation. We will highlight the pros and cons of this novel
    synthetic data generation approach. This will help you to make informed decisions
    about the best methods to utilize for your own problems. We will highlight the
    opportunities and challenges of diffusion models. Moreover, this chapter is enriched
    with a comprehensive practical example, providing hands-on experience in both
    generating and effectively employing synthetic data for a real-world ML application.
    As you go through diffusion models, you will learn about the main ethical issues
    and concerns around utilizing this synthetic data approach in practice. In addition
    to that, we will review some state-of-the-art research on this topic. Thus, this
    chapter will equip you with the necessary knowledge to thoroughly understand this
    novel synthetic data generation approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to diffusion models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models – the pros and cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on diffusion models in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models – ethical issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any code used in this chapter will be available under the corresponding chapter
    folder at this book’s GitHub repository: [https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning).'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to diffusion models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore diffusion models. We will compare them to **Variational
    Autoencoders** (**VAEs**) and **Generative Adversarial Networks** (**GANs**),
    which we covered in [*Chapter 7*](B18494_07.xhtml#_idTextAnchor120). This will
    help you to gain a holistic and comprehensive understanding of generative models.
    Additionally, it will make comparing and contrasting the architectures, training
    procedures, and data flow of these methods straightforward. Furthermore, we will
    also learn how to train a typical diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Diffusion Models** (**DMs**) are generative models that were recently proposed
    as a clever solution to generate images, audio, videos, time series, and texts.
    DMs are excellent at modeling complex probability distributions, structures, temporal
    dependencies, and correlations in data. The initial mathematical model behind
    DMs was first proposed and applied in the field of statistical mechanics to study
    the random motion of particles in gases and liquids. As we will see later, it
    is essential and crucial to learn about DMs, as they are powerful generative models
    that can usually generate higher-quality and more privacy-preserving synthetic
    data compared to other approaches. Additionally, DMs rely on strong mathematical
    and theoretical foundations.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the first works to show that DMs can be utilized to generate photorealistic
    images was *Denoising Diffusion Probabilistic Models* ([https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)),
    which was proposed by researchers from UC Berkeley. This pioneering work was followed
    by another work by OpenAI titled *Diffusion Models Beat GANs on Image Synthesis*
    ([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf)),
    showing that DMs are better at generating photorealistic synthetic images. Then,
    other researchers started to explore the potential of these DMs in different fields
    and compare them to VAEs and GANs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational Autoencoders** (**VAEs**) are one of the earliest solutions for
    generating synthetic data. They are based on using an encoder to encode data from
    a high-dimensional space (such as RGB images) into a latent low-dimensional space.
    Then, the decoder is used to reconstruct these encoded samples from the latent
    space to the original high-dimensional space. In the training process, the VAE
    is forced to minimize the loss between the original training sample and the reconstructed
    one by the decoder. Assuming the model was trained on a sufficient number of training
    samples, it can then be used to generate new synthetic data by sampling points
    from the latent space and using the decoder to decode them, from the latent low-dimensional
    space to the high-dimensional one, as shown in *Figure 9**.1*.'
  prefs: []
  type: TYPE_NORMAL
- en: The training process of DMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs are used to generate synthetic data with a distribution close to the probability
    distribution of the training data. Thus, an important task is to learn the distribution
    of our training data and then leverage our knowledge of the real data to generate
    an unlimited number of synthetic data samples. Usually, we would want to generate
    high-quality and diverse synthetic data using a fast-generation method. However,
    each generation method has its own advantages and disadvantages, as we will see
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 9**.1*, given a training image, ![](img/B18494_F09_001.png),
    from the real data, the DM adds **Gaussian noise** to this image to become ![](img/B18494_F09_002.png).
    The process is repeated until the image simply becomes an image of random noise,
    ![](img/B18494_F09_003.png). This process is called **forward diffusion**. Following
    this, the model starts the denoising process, in which the DM takes the random
    noise, ![](img/B18494_F09_004.png), and reverses the previous process (i.e., forward
    diffusion) to reconstruct the training image. This process is known as **reverse
    diffusion**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The training process and architectures of the main generative
    models – VAEs, GANs, and DMs](img/Figure_09_01_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The training process and architectures of the main generative models
    – VAEs, GANs, and DMs
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from *Figure 9**.1*, the forward diffusion process is a **Markov
    chain**. Each step of the process is a stochastic event, and each event depends
    only on the previous event or state. Thus, they form a sequence of stochastic
    events and, consequently, a Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: A key idea of the DMs’ training process is using a neural network such as *U-Net*
    ([https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)) to predict
    the amount of noise that was added to a given noisy image from the training data.
    This is crucial to reverse the noising process and learn how to generate a synthetic
    sample given random noise.
  prefs: []
  type: TYPE_NORMAL
- en: When the diffusion model converges after a successful training process, we can
    give it random noise, ![](img/B18494_F09_005.png), and the DM, using the reverse
    diffusion path, will give us a synthetic sample based on the provided ![](img/B18494_F09_006.png).
    Thus, we can now generate an unlimited number of new synthetic samples from the
    same training data probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Please note the number of diffusion steps, ![](img/B18494_F09_007.png), in the
    noising/denoising process depends on how smooth you want the training process
    to be. In other words, a higher value of ![](img/B18494_F09_008.png) means less
    abrupt and more gradual noise will be added in the training process. Thus, the
    model’s weights will update steadily and the optimization loss will decrease smoothly.
    However, higher values of ![](img/B18494_F09_009.png) will make the process slower
    (usually, ![](img/B18494_F09_010.png) ).
  prefs: []
  type: TYPE_NORMAL
- en: Applications of DMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs have been utilized for a wide range of applications in various domains such
    as computer vision, natural language processing, finance, and healthcare. Let’s
    briefly discuss some of these applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer vision**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image generation**: Generating diverse photorealistic images is one of the
    ultimate aims of DMs. However, there is usually a trade-off between diversity
    and photorealism. Thus, to maintain good fidelity, the DMs are usually guided
    or conditioned on textual data. For more information, please refer to *GLIDE:
    Towards Photorealistic Image Generation and* *Editing with Text-Guided Diffusion
    Models* ([https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741)).
    As you can imagine, the generated synthetic images can be used for various applications,
    including data augmentation, creative art, prototyping, and visualization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video prediction**: Predicting the next frame has many useful applications,
    as it facilitates predicting future events, which is essential for planning and
    decision-making in fields such as robotics. Simultaneously, it has huge applications
    in fields such as surveillance and security. Video prediction ML models can be
    utilized to anticipate and forecast possible threats, hazards, and potential risks.
    Additionally, predicting future frames accurately can be utilized to generate
    synthetic videos to complement real training data. For more details, refer to
    *Diffusion Models for Video Prediction and Infilling* ([https://arxiv.org/pdf/2206.07696.pdf](https://arxiv.org/pdf/2206.07696.pdf)),
    *Video Diffusion Models* ([https://arxiv.org/pdf/2204.03458.pdf](https://arxiv.org/pdf/2204.03458.pdf)),
    and *Imagen Video: High-Definition Video Generation with Diffusion* *Models* ([https://arxiv.org/pdf/2210.02303.pdf](https://arxiv.org/pdf/2210.02303.pdf)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image inpainting**: This is simply filling in or restoring missing or damaged
    parts in images. It is a fundamental task in areas such as historical archiving,
    privacy protection, and entertainment. Furthermore, it has also been recently
    utilized for synthetic data generation. For example, DMs were utilized recently
    to generate synthetic brain MRIs, with the ability to control tumoral and non-tumoral
    tissues. It was shown that using generated synthetic data can boost performance
    considerably. For more details, please refer to *Multitask Brain Tumor Inpainting
    with Diffusion Models: A Methodological Report* ([https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf](https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf))
    and *RePaint: Inpainting using Denoising Diffusion Probabilistic* *Models* ([https://arxiv.org/abs/2201.09865](https://arxiv.org/abs/2201.09865)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image colorization**: This is the task of transferring grayscale images into
    colored ones. For example, this is essential to improve the photorealism of historical
    or old photographs. Adding color is important to make these photos more appealing
    and more emotionally engaging. DMs were shown to perform very well under this
    task. For further information, please read *Palette: Image-to-Image Diffusion*
    *Models* ([https://arxiv.org/pdf/2111.05826.pdf](https://arxiv.org/pdf/2111.05826.pdf)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural** **language processing**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: Virtual assistants, chatbots, and similar conversational
    text-based systems rely on text generation. DMs have been utilized recently for
    this task to improve the quality and diversity, as they are more capable of capturing
    complex distributions. For an example, please refer to *DiffuSeq: Sequence to
    Sequence Text Generation with Diffusion* *Models* ([https://arxiv.org/abs/2210.08933](https://arxiv.org/abs/2210.08933)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-to-speech synthesis**: This is the process of transforming text into
    audio. While it has many applications in fields such as **Human-Computer Interaction**
    (**HCI**), education and learning, and video games, it has been recently utilized
    to make textual content accessible to individuals with visual impairment. For
    more details about DM and text-to-speech synthesis, please refer to *Diff-TTS:
    A Denoising Diffusion Model for Text-to-Speech* ([https://arxiv.org/pdf/2104.01409.pdf](https://arxiv.org/pdf/2104.01409.pdf))
    and *Prodiff: Progressive fast diffusion model for high-quality text-to-speech*
    ([https://dl.acm.org/doi/abs/10.1145/3503161.3547855](https://dl.acm.org/doi/abs/10.1145/3503161.3547855)).
    Additionally, for a survey of recent text-to-speech DM-based methods, please refer
    to *A Survey on Audio Diffusion Models: Text to Speech Synthesis and Enhancement
    in Generative* *AI* ([https://arxiv.org/pdf/2303.13336.pdf](https://arxiv.org/pdf/2303.13336.pdf)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-driven image generation**: This is another promising field where the
    aim is to generate visual content, such as images based on textual input. It has
    various applications in content generation, marketing, data augmentation, and
    assisted data-labeling tools. As expected, DMs are excellent at modeling complex
    data distribution and very powerful at generating diverse and appealing images.
    Thus, they have been utilized for text-driven image generation. For more details,
    please refer to *Text2Human: text-driven controllable human image* *generation*
    ([https://dl.acm.org/doi/abs/10.1145/3528223.3530104](https://dl.acm.org/doi/abs/10.1145/3528223.3530104)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other applications**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy in healthcare**: Real **Electronic Health Records** (**EHRs**) of
    patients carry rich and very useful information that ML models can leverage to
    help in disease diagnosis, predictive analytics, decision-making, and the optimization
    and management of resources. DMs were shown to generate high-quality and large-scale
    EHRs that can be leveraged in enormous applications, such as ML model training,
    healthcare research, and medical education. To delve into more details, please
    read *MedDiff: Generating Electronic Health Records using Accelerated Denoising
    Diffusion* *Model* ([https://arxiv.org/pdf/2302.04355.pdf](https://arxiv.org/pdf/2302.04355.pdf)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: This is the task of detecting or identifying patterns
    and instances that are not in line with the expected behavior and distribution.
    It has a myriad of applications in cybersecurity, fraud detection, telecommunication,
    and manufacturing. DMs are usually robust to noise and more stable, which makes
    them ideal for these applications. For an example and more details about utilizing
    DMs for anomaly detection in healthcare, please refer to *Diffusion models for
    medical anomaly* *detection* ([https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4](https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-to-motion**: Generating animation or motion from textual input has many
    applications in the training, education, media, and entertainment sectors. DMs
    have shown promising results by producing high-quality animations of human motion.
    For more details, please refer to *Human Motion Diffusion* *Model* ([https://arxiv.org/abs/2209.14916](https://arxiv.org/abs/2209.14916)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have an idea of the domains where diffusion models are used, in
    the next section, we closely examine the pros and cons of DMs.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models – the pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about and examine the main pros and cons of
    using DMs for synthetic data generation. This will help you to weigh the advantages
    and disadvantages of each synthetic data generation method. Consequently, it will
    give you the wisdom to select the best approach for your own problems.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in [*Chapter 7*](B18494_07.xhtml#_idTextAnchor120), GANs work
    very well for certain applications, such as style transfer and image-to-image
    translation, but they are usually very hard to train and unstable. Additionally,
    the generated synthetic samples are usually less diverse and photorealistic. Conversely,
    recent papers have shown that DM-based synthetic data generation approaches surpass
    GANs on many benchmarks. For more details, please refer to *Diffusion Models Beat
    GANs on Image Synthesis* ([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf)).
    Like any other synthetic data generation approach, DMs have pros and cons. Thus,
    you need to consider them carefully for your particular application or problem.
    Then, you can select the best approach to generate the synthetic data that you
    want. With that aim in mind, we will examine the key advantages and disadvantages
    of using DMs.
  prefs: []
  type: TYPE_NORMAL
- en: The pros of using DMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, DMs are excellent at modeling complex data probability distributions
    and capturing temporal dependencies and hidden patterns. This is possible with
    DMs because they use a diffusion process to model data distributions, using a
    sequence of conditional distributions. Thus, we can highlight the main strengths
    and merits of DMs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalizability and applicability to a wide range of problems**: Unlike
    other generative methods, which are limited to image and video generation, DMs
    can be utilized to generate images, audio, videos, texts, molecular structures,
    and many other data types and modalities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stability in the training process**: The architecture, the training process,
    and the optimization technique of DMs make them more stable compared to other
    generative models such as GANs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-quality synthetic data generation**: Due to their distinctive architecture
    and innovative gradual and iterative training process, DMs generate high-quality
    synthetic data that surpasses other generative models such as VAEs and GANs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cons of using DMS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two main limitations and shortcomings of using DMs can be described in
    terms of the computational complexity of the training and inference process, as
    well as the large-scale training datasets required by DMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational complexity**: DMs are computationally heavy. They are usually
    slow compared to other generative models, as the forward and reverse diffusion
    processes are composed of hundreds of steps (usually, the number of steps, ![](img/B18494_F09_011.png),
    is close to ![](img/B18494_F09_012.png)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More training data is required**: DMs require large-scale training datasets
    to converge. Obtaining such datasets is not suitable for certain fields, which
    limits the usability of DMs for certain applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we can clearly identify the pros and cons of using DMs for synthetic data
    generation. Let’s practice utilizing DMs to generate synthetic data to train ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on diffusion models in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s study a practical example that demonstrates the usability of synthetic
    data in the computer vision field. For that aim, we will generate and prepare
    our dataset, build our ML model from scratch, train it, and evaluate its performance.
    The dataset is available at *Kaggle* ([https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih](https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih)).
    The full code, the trained model, and the results are available on GitHub under
    the corresponding chapter folder in the book’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to build an ML model that can classify car images into two distinct
    categories – images depicting car accidents and those that do not. As you can
    imagine, curating such a real dataset is time-consuming and error-prone. It could
    be easy to collect car images without accidents. However, collecting images of
    cars with accidents, collisions, fires, and other dangerous scenarios is extremely
    hard. To solve this problem and to prove the usability of synthetic data, let’s
    first generate our training dataset. We can use a single synthetic data generation
    approach for that aim. However, let’s combine different methods and tools to collect
    more diverse data and practice different approaches as well. In this example,
    we will use the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The DALL·E 2 image generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeepAI text-to-image generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simulator built using a game engine such as Silver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s leverage the remarkable capabilities of recent generative models
    such as *DALL·E 2* ([https://openai.com/dall-e-2](https://openai.com/dall-e-2))
    to generate some car accident images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Generating synthetic images using the DALL·E 2 web application](img/Figure_09_02_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Generating synthetic images using the DALL·E 2 web application
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply use the following prompts to generate these images (see *Figure
    9**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Car accidents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`White` `car accidents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Red` `car accidents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Blue` `car accidents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ambulance` `car accident`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from *Figure 9**.3*, the generated images look photorealistic
    and diverse, which is exactly what we need to train our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Car accident images generated using DALL·E 2](img/Figure_09_03_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Car accident images generated using DALL·E 2
  prefs: []
  type: TYPE_NORMAL
- en: We also use images collected from the *car-accident(resnet)* dataset, licensed
    under CC BY 4.0 ([https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei](https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei))
    using Roboflow ([https://roboflow.com](https://roboflow.com)). We chose this dataset
    as the images are similar to what they would be if they were sourced from a video
    game, such as *BeamNG Drive Crashes* ([https://www.beamng.com/game](https://www.beamng.com/game)).
    We will add these images to our dataset to further improve its diversity. A sample
    of these images is shown in *Figure 9**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Car accident images collected from a video game](img/Figure_09_04_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Car accident images collected from a video game
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to generate similar images for the other category, images of
    cars with no accidents. This time, let’s use the *DeepAI* tool to generate these
    images. As did earlier, we can simply use the following prompts to get the required
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Car` `under rain`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`White car`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Car` `under fog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Blue car`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see in *Figure 9**.5*, we effortlessly obtained another 30 car images
    with no accidents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Intact car images generated using DeepAI](img/Figure_09_05_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Intact car images generated using DeepAI
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have 60 synthetic images, but we still need more images to appropriately
    train our ML model. We can generate any number of images using the previous generative
    models, but let’s explore another way – using the *Silver* simulator ([https://github.com/lsmcolab/Silver](https://github.com/lsmcolab/Silver)).
  prefs: []
  type: TYPE_NORMAL
- en: By specifying the number of images that we want, we can generate the following
    images for this category (*Figure 9**.6*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Intact car images generated using Silver](img/Figure_09_06_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Intact car images generated using Silver
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have collected `600` synthetic images. Now, to assess the
    performance of our trained model, we should test it on real car images. Let’s
    collect real images using the *Unsplash* website ([https://unsplash.com](https://unsplash.com)).
    To further improve our dataset, let’s also manually add images using Roboflow.
    *Figure 9**.7* shows a sample of these images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Sample images of our real dataset](img/Figure_09_07_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Sample images of our real dataset
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our dataset is composed of `600` synthetic images and `250` real ones,
    as shown in *Table 9.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Split** | **Synthetic** | **Real** |'
  prefs: []
  type: TYPE_TB
- en: '| Training | 540 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Validation | 60 | 62 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | - | 188 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 600 | 250 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Our final dataset’s splits and number of images
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will train only on synthetic data and test only on real data. Also,
    note in the validation split that we used both synthetic and real data because
    we are training and testing on two different domains – synthetic and real. Thus,
    a balanced mixture of data from both domains is necessary to give a good understanding
    of our model’s learning of the synthetic data and generalizability to the real
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our ML model is composed of four convolutional and three fully connected layers,
    max pooling, dropout, and batch normalization. We will use the `train.py` file
    in the corresponding chapter’s folder of the book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We train our model from scratch on the training split shown in *Table 9.1* for
    `30` epochs. Then, we will select the best model using the validation split. The
    training loss is shown in *Figure 9**.8*. The loss is smoothly decreasing, as
    expected, which means that our model trains well on our synthetic training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – The training loss during the training stage](img/Figure_09_08_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – The training loss during the training stage
  prefs: []
  type: TYPE_NORMAL
- en: From the validation accuracy shown in *Figure 9**.9*, we can see that our model
    achieved the best results on the validation set at epoch `27`. Thus, we will use
    this model for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Validation accuracy during the training stage](img/Figure_09_09_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Validation accuracy during the training stage
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see the performance of our best model on the test split. As you can see
    from the confusion matrix shown in *Figure 9**.10*, our model classifies no-accident
    cars with an accuracy of *84%*, while it classifies accidents with an accuracy
    of *74%*. Our model achieved a total accuracy of *80%*, which is excellent given
    that our model was trained only on synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – The confusion matrix illustrating classification results](img/Figure_09_10_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – The confusion matrix illustrating classification results
  prefs: []
  type: TYPE_NORMAL
- en: We have provided a detailed example of how to generate and leverage synthetic
    data for training ML models. Now, let’s delve into DMs and the main ethical issues.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models – ethical issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about the main ethical issues associated with
    using DMs for synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion-based generative models are emerging and powerful technologies. Thus,
    their pros and cons need to be considered carefully. Their advantages are huge
    for businesses, industry, and research. However, they possess dangerous capabilities
    that can be leveraged to cause harm to individuals, businesses, societies, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s list the main ethical issues usually associated with generative models
    and, especially, DMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Copyright
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inappropriate content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responsibility issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud and identity theft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s delve into some of the main ethical issues behind using DMs in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs are usually trained on large-scale real datasets. For example, DALL E 2
    was trained on more than 650 million text-image pairs. Thus, obtaining permission
    from the owners of these data samples, such as images, artworks, video clips,
    and others, is not feasible. Additionally, DMs may misrepresent or represent copyrighted
    and intellectual properties with minor changes. For example, *Universal Music*
    asked streaming services and companies to prevent AI/ML companies from accessing
    their songs for training ([https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs](https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs)).
    Thus, it creates many difficult questions for regulators to address and regulations
    for companies to comply with. Taking into account the huge, rapid progress in
    this field, the regulations may not be able to provide an ethical frame to control
    copyright issues.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have outlined in this chapter, DMs learn to generate synthetic data following
    the same distribution of the training data. Thus, if the training data is biased,
    the DM will also generate biased synthetic data. What is more difficult with these
    complex models is assessing this bias compared to examining the raw training data,
    such as images and videos. It becomes even more complex and severe when these
    models are leveraged in decision-making by non-experts. For example, a professional
    comic artist may identify gender, political, and age biases within a comic book.
    However, training a DM on millions of comic books and then assessing the biases
    of the generated comic books, using this model, may not be possible!
  prefs: []
  type: TYPE_NORMAL
- en: Inappropriate content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs may generate inappropriate content without users’ consent. It may include
    inappropriate language, sexual and violent content, hate speech, and racism. Thus,
    DMs need to utilize an effective filtering mechanism to discard unsuitable data.
    Simultaneously, they need to utilize a safeguarding procedure to prevent generating
    inappropriate content.
  prefs: []
  type: TYPE_NORMAL
- en: Responsibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Utilizing generative models such as DMs in decision-making will be almost unavoidable
    in the future. However, it is not possible, with current DMs, to understand why
    a certain decision was made. Thus, it is challenging to understand who is responsible
    for wrong decisions made by DMs that may cause death, injuries, or damage to properties.
    Thus, we need to develop suitable mechanisms to ensure transparency, accountability,
    and tracking in the process of decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs may reveal sensitive information about individuals and organizations since
    the generated synthetic data still follows the statistical properties of the original
    real training data. These models may disclose information that could be utilized
    by a third party to cause harm, loss, or unwanted consequences. For example, ChatGPT
    was banned in Italy due to privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud and identity theft
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DMs are powerful and capable of mimicking the human voice, photos, and videos.
    Thus, generated fake media can be exploited for many fraudulent purposes, such
    as accessing personal information, money laundering, credit card fraud, and cybercrime.
    Furthermore, DMs can be used to impersonate a celebrity, activist, or politician
    to get access to private information, classified material, and documents.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a novel and powerful method to generate synthetic
    data – using DMs. We compared DMs to other state-of-the-art generative models,
    and then, we highlighted the training process of DMs. Furthermore, we discussed
    the pros and cons of utilizing DMs. Additionally, we learned how to generate and
    utilize synthetic data in practice. We also examined the main ethical considerations
    usually raised when deploying DMs for synthetic data generation. You developed
    a comprehensive understanding of generative models, and you learned about standard
    DM architecture, the training process, and the main advantages, benefits, and
    limitations of utilizing DMs in practice. In the next chapter, we will shed light
    on several case studies, highlighting how synthetic data has been successfully
    utilized to improve computer vision solutions in practice. The chapter aims to
    inspire and motivate you to explore the potential of synthetic data in your own
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Part 4:Case Studies and Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, you will be introduced to rich and diverse case studies in three
    cutting-edge areas in the field of ML: **Computer Vision** (**CV**), **Natural
    Language Processing** (**NLP**), and **Predictive Analytics** (**PA**). You will
    comprehend the benefits of employing synthetic data in these fields and identify
    the main challenges and issues. Following this, you will learn about best practices
    that improve the usability of synthetic data in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18494_10.xhtml#_idTextAnchor178), *Case Study 1 – Computer
    Vision*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18494_11.xhtml#_idTextAnchor188), *Case Study 2 – Natural Language
    Processing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18494_12.xhtml#_idTextAnchor203), *Case Study 3 – Predictive
    Analytics*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18494_13.xhtml#_idTextAnchor216), *Best Practices for Applying
    Synthetic Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
