<html><head></head><body>
		<div id="_idContainer313" class="Content">
			<h1 id="_idParaDest-370"><em class="italics"><a id="_idTextAnchor376"/>Appendix</em></h1>
		</div>
		<div>
			<div id="_idContainer314" class="Content">
			</div>
		</div>
		<div id="_idContainer315" class="Content">
			<h2>About</h2>
			<p>This section is included to assist the students to perform the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the book.</p>
		</div>
		<div id="_idContainer324" class="Content">
			<h2 id="_idParaDest-371"><a id="_idTextAnchor377"/>Chapter 1: R for Advanced Analytics</h2>
			<h3 id="_idParaDest-372"><a id="_idTextAnchor378"/>Activity 1: Create an R Markdown File to Read a CSV File and Write a Summary of Data</h3>
			<ol>
				<li>Start the RStudio and navigate to <strong class="bold">Files</strong> | <strong class="bold">New Files</strong> | <strong class="bold">R Markdown</strong>.</li>
				<li>On the New R Markdown window, provide the <strong class="bold">Title</strong> and <strong class="bold">Author</strong> name, as illustrated in the following screenshot. Ensure that you select the <strong class="bold">Word</strong> option under the <strong class="bold">Default Output Format</strong> section:<div id="_idContainer316" class="IMG---Figure"><img src="image/C12624_01_121.jpg" alt="Figure 1.13: Creating a new R Markdown file in Rstudio&#13;&#10;"/></div><h6>Figure 1.13: Creating a new R Markdown file in Rstudio</h6></li>
				<li>Now, use the <strong class="inline">read.csv()</strong> method to read the <strong class="inline">bank-full.csv</strong> file:<div id="_idContainer317" class="IMG---Figure"><img src="image/C12624_01_13.jpg" alt="Figure 1.14: Using the read.csv method to read the data&#13;&#10;"/></div><h6>Figure 1.14: Using the read.csv method to read the data</h6></li>
				<li>Finally, print the summary into a word file using the <strong class="inline">summary</strong> method:</li>
			</ol>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<img src="image/C12624_01_14.jpg" alt="Figure 1.15: Final output after using the summary method&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 1.15: Final output after using the summary method</h6>
			<h3 id="_idParaDest-373"><a id="_idTextAnchor379"/>Activity 2: Create a List of Two Matrices and Access the Values</h3>
			<ol>
				<li value="1">Create two matrices of size <strong class="inline">10 x 4</strong> and <strong class="inline">4 x 5</strong> by randomly generated numbers from a binomial distribution (use <strong class="inline">rbinom</strong> method). Call the matrix <strong class="inline">mat_A</strong> and <strong class="inline">mat_B</strong>, respectively:<p class="snippet">mat_A &lt;- matrix(rbinom(n = 40, size = 100, prob = 0.4),nrow = 10, ncol=4)</p><p class="snippet">mat_B &lt;- matrix(rbinom(n = 20, size = 100, prob = 0.4),nrow = 4, ncol=5)</p></li>
				<li>Now, store the two matrices in a list:<p class="snippet">list_of_matrices &lt;- list(mat_A = mat_A, mat_B =mat_B)</p></li>
				<li>Using the list, access the row 4 and column 2 of <strong class="inline">mat_A</strong> and store it in variable <strong class="inline">A</strong>, and access row 2 and column 1 of <strong class="inline">mat_B</strong> and store it in variable <strong class="inline">B</strong>:<p class="snippet">A &lt;- list_of_matrices[["mat_A"]][4,2]</p><p class="snippet">B &lt;- list_of_matrices[["mat_B"]][2,1]</p></li>
				<li>Multiply the <strong class="inline">A</strong> and <strong class="inline">B</strong> matrices and subtract from row 2 and column 1 of <strong class="inline">mat_A</strong>:<p class="snippet">list_of_matrices[["mat_A"]][2,1] - (A*B)</p><p>The output is as follows:</p><p class="snippet">## [1] -1554</p></li>
			</ol>
			<h3 id="_idParaDest-374"><a id="_idTextAnchor380"/>Activity 3: Create a DataFrame with Five Summary Statistics for All Numeric Variables from Bank Data Using dplyr and tidyr</h3>
			<ol>
				<li value="1">Import the <strong class="inline">dplyr</strong> and <strong class="inline">tidyr</strong> packages in the system:<p class="snippet">library(dplyr)</p><p class="snippet">library(tidyr)</p><p class="snippet">Warning: package 'tidyr' was built under R version 3.2.5</p></li>
				<li>Create the <strong class="inline">df</strong> DataFrame and import the file into it:<p class="snippet">df &lt;- tbl_df(df_bank_detail)</p></li>
				<li>Extract all numeric variables from bank data using <strong class="inline">select()</strong>, and compute min, 1st quartile, 3rd quartile, median, mean, max, and standard deviation using the <strong class="inline">summarise_all()</strong> method:<p class="snippet">df_wide &lt;- df %&gt;%</p><p class="snippet">  select(age, balance, duration, pdays) %&gt;% </p><p class="snippet">  summarise_all(funs(min = min, </p><p class="snippet">                      q25 = quantile(., 0.25), </p><p class="snippet">                      median = median, </p><p class="snippet">                      q75 = quantile(., 0.75), </p><p class="snippet">                      max = max,</p><p class="snippet">                      mean = mean, </p><p class="snippet">                      sd = sd))</p></li>
				<li>The result is a wide data frame. 4 variable, 7 measures:<p class="snippet">dim(df_wide)</p><p class="snippet">## [1]  1 28</p></li>
				<li>Store the result in a DataFrame of wide format named <strong class="inline">df_wide</strong>, reshape it using the <strong class="inline">tidyr</strong> functions, and, finally, convert the wide format to deep, use the gather, separate, and spread functions of the <strong class="inline">tidyr</strong> package:<p class="snippet">df_stats_tidy &lt;- df_wide %&gt;% gather(stat, val) %&gt;%</p><p class="snippet">  separate(stat, into = c("var", "stat"), sep = "_") %&gt;%</p><p class="snippet">  spread(stat, val) %&gt;%</p><p class="snippet">  select(var,min, q25, median, q75, max, mean, sd) # reorder columns</p><p class="snippet">print(df_stats_tidy)</p><p>The output is as follows:</p><p class="snippet">## # A tibble: 4 x 8</p><p class="snippet">##        var   min   q25 median   q75    max       mean         sd</p><p class="snippet">## *    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;</p><p class="snippet">## 1      age    18    33     39    48     95   40.93621   10.61876</p><p class="snippet">## 2  balance -8019    72    448  1428 102127 1362.27206 3044.76583</p><p class="snippet">## 3 duration     0   103    180   319   4918  258.16308  257.52781</p><p class="snippet">## 4    pdays    -1    -1     -1    -1    871   40.19783  100.12875</p></li>
			</ol>
			<h2 id="_idParaDest-375"><a id="_idTextAnchor381"/>Chapter 2: Exploratory Analysis of Data</h2>
			<h3 id="_idParaDest-376"><a id="_idTextAnchor382"/>Activity 4: Plotting Multiple Density Plots and Boxplots</h3>
			<ol>
				<li value="1">First, load the necessary libraries and packages in the RStudio:<p class="snippet">library(ggplot2)</p><p class="snippet">library(cowplot)</p></li>
				<li>Read the <strong class="inline">bank-additional-full.csv</strong> dataset in a DataFrame named <strong class="inline">df</strong>:<p class="snippet">df &lt;- read.csv("bank-additional-full.csv",sep=';')</p></li>
				<li>Define the <strong class="inline">plot_grid_numeric</strong> function for density plot:<p class="snippet">plot_grid_numeric &lt;- function(df,list_of_variables,ncols=2){</p><p class="snippet">  plt_matrix&lt;-list()</p><p class="snippet">  i&lt;-1</p><p class="snippet">  for(column in list_of_variables){</p><p class="snippet">    plt_matrix[[i]]&lt;-ggplot(data=df,aes_string(x=column)) + </p><p class="snippet">      geom_density(fill="red",alpha =0.5)  +</p><p class="snippet">      ggtitle(paste("Density Plot for variable:",column)) + theme_bw()</p><p class="snippet">    i&lt;-i+1</p><p class="snippet">  }</p><p class="snippet">  plot_grid(plotlist=plt_matrix,ncol=2)</p><p class="snippet">}</p></li>
				<li>Plot the density plot for the <strong class="inline">campaign</strong>, <strong class="inline">pdays</strong>, <strong class="inline">previous</strong>, and <strong class="inline">emp.var.rate</strong> variables:<p class="snippet">plot_grid_numeric(df,c("campaign","pdays","previous","emp.var.rate"),2)</p><p>The output is as follows:</p><div id="_idContainer319" class="IMG---Figure"><img src="image/C12624_02_25.jpg" alt="Figure 2.27: Density plot for the campaign, pdays, previous, and emp.var.rate variables&#13;&#10;"/></div><h6>Figure 2.27: Density plot for the campaign, pdays, previous, and emp.var.rate variables</h6><p>Observe that the interpretations we obtained using the histogram are visibly true in density plot as well. Hence, this serves as another alternative plot for looking at the same trend.</p></li>
				<li>Repeat step 4 for boxplot:<p class="snippet">plot_grid_numeric &lt;- function(df,list_of_variables,ncols=2){</p><p class="snippet">  plt_matrix&lt;-list()</p><p class="snippet">  i&lt;-1</p><p class="snippet">  for(column in list_of_variables){</p><p class="snippet">    plt_matrix[[i]]&lt;-ggplot(data=df,aes_string(y=column)) + </p><p class="snippet">      geom_boxplot(outlier.colour="black") +</p><p class="snippet">      ggtitle(paste("Boxplot for variable:",column)) + theme_bw()</p><p class="snippet">    i&lt;-i+1</p><p class="snippet">  }</p><p class="snippet">  plot_grid(plotlist=plt_matrix,ncol=2)</p><p class="snippet">}</p><p class="snippet">plot_grid_numeric(df,c("campaign","pdays","previous","emp.var.rate"),2)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer320" class="IMG---Figure">
					<img src="image/C12624_02_26.jpg" alt="Figure 2.28: Boxplot for the campaign, pdays, previous, and emp.var.rate variables&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.28: Boxplot for the campaign, pdays, previous, and emp.var.rate variables</h6>
			<p>Now, let's explore the last four numeric variable of the dataset, that is, <strong class="inline">nr.employed</strong>, <strong class="inline">euribor3m</strong>, <strong class="inline">cons.conf.index</strong>, and <strong class="inline">duration</strong>, and see whether we could derive some meaningful insights.</p>
			<h2 id="_idParaDest-377"><a id="_idTextAnchor383"/>Chapter 3: Introduction to Supervised Learning</h2>
			<h3 id="_idParaDest-378"><a id="_idTextAnchor384"/>Activity 5: Draw a Scatterplot between PRES and PM2.5 Split by Months</h3>
			<ol>
				<li value="1">Import the <strong class="inline">ggplot2</strong> package into the system:<p class="snippet">library(ggplot2)</p></li>
				<li>In <strong class="inline">ggplot</strong>, assign the component of the <strong class="inline">a()</strong> method with the variable <strong class="inline">PRES</strong>.<p class="snippet">ggplot(data = PM25, aes(x = PRES, y = pm2.5, color = hour)) +   geom_point()</p></li>
				<li>In the next layer of the <strong class="inline">geom_smooth()</strong> method, passing <strong class="inline">colour = "blue"</strong> to differentiate.<p class="snippet">geom_smooth(method='auto',formula=y~x, colour = "blue", size =1)</p></li>
				<li>Finally, in the <strong class="inline">facet_wrap()</strong> layer, use the <strong class="inline">month</strong> variable to draw a separate segregation for each month.<p class="snippet">facet_wrap(~ month, nrow = 4)</p><p>The final code will look like this:</p><p class="snippet">ggplot(data = PM25, aes(x = PRES, y = pm2.5, color = hour)) +      geom_point() +      geom_smooth(method='auto',formula=y~x, colour = "blue", size =1) +      facet_wrap(~ month, nrow = 4)</p><p>The plot is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<img src="image/C12624_03_13.jpg" alt="Figure 3.19: Scatterplot showing the relationship between PRES and PM2.5&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.19: Scatterplot showing the relationship between PRES and PM2.5</h6>
			<h3 id="_idParaDest-379"><a id="_idTextAnchor385"/>Activity 6: Transforming Variables and Deriving New Variables to Build a Model</h3>
			<p>Perform the following steps for building the model:</p>
			<ol>
				<li value="1">Import the required libraries and packages into the system:<p class="snippet">library(dplyr)</p><p class="snippet">library(lubridate)</p><p class="snippet">library(tidyr)</p><p class="snippet">library(ggplot2)</p><p class="snippet">library(grid)</p><p class="snippet">library(zoo)</p></li>
				<li>Combine the year, month, day, and hour into a <strong class="inline">datetime</strong> variable:<p class="snippet">PM25$datetime &lt;- with(PM25, ymd_h(sprintf('%04d%02d%02d%02d', year, month, day,hour)))</p></li>
				<li>Remove the rows with missing values in any column:<p class="snippet">PM25_subset &lt;- na.omit(PM25[,c("datetime","pm2.5")])</p></li>
				<li>Use the <strong class="inline">rollapply()</strong> method from the package <strong class="inline">zoo</strong> to compute the moving average of PM2.5; this is to smoothen any noise from a reading of PM2.5:<p class="snippet">PM25_three_hour_pm25_avg &lt;- rollapply(zoo(PM25_subset$pm2.5,PM25_subset$datetime), 3, mean)</p></li>
				<li>Create two levels of the PM25 pollution, <strong class="inline">0–Normal</strong>, <strong class="inline">1-Above Normal</strong>. We can also create more than two levels; however, for logistic regression, which works best with binary classification, we have used two levels:<p class="snippet">PM25_three_hour_pm25_avg &lt;- as.data.frame(PM25_three_hour_pm25_avg)</p><p class="snippet">PM25_three_hour_pm25_avg$timestamp &lt;- row.names(PM25_three_hour_pm25_avg)</p><p class="snippet">row.names(PM25_three_hour_pm25_avg) &lt;- NULL</p><p class="snippet">colnames(PM25_three_hour_pm25_avg) &lt;- c("avg_pm25","timestamp")</p><p class="snippet">PM25_three_hour_pm25_avg$pollution_level &lt;- ifelse(PM25_three_hour_pm25_avg$avg_pm25 &lt;= 35, 0,1)</p><p class="snippet">PM25_three_hour_pm25_avg$timestamp &lt;- as.POSIXct(PM25_three_hour_pm25_avg$timestamp, format= "%Y-%m-%d %H:%M:%S",tz="GMT")</p></li>
				<li>Merge the resulting data frame (PM25_three_hour_pm25_avg ) with the values of other environmental variables such as <strong class="inline">TEMP</strong>, <strong class="inline">DEWP</strong>, and <strong class="inline">Iws</strong>, which we used in the linear regression model:<p class="snippet">PM25_for_class &lt;- merge(PM25_three_hour_pm25_avg, PM25[,c("datetime","TEMP","DEWP","PRES","Iws","cbwd","Is","Ir")], by.x = "timestamp",by.y = "datetime")</p></li>
				<li>Fit the generalized linear model (<strong class="inline">glm</strong>) on <strong class="inline">pollution_level</strong> using the TEMP, DEWP and Iws variables:<p class="snippet">PM25_logit_model &lt;- glm(pollution_level ~ DEWP + TEMP + Iws, data = PM25_for_class,family=binomial(link='logit'))</p></li>
				<li>	Summarize the model:<p class="snippet">summary(PM25_logit_model)</p><p>The output is as follows:</p><p class="snippet">Call:</p><p class="snippet">glm(formula = pollution_level ~ DEWP + TEMP + Iws, family = binomial(link = "logit"), </p><p class="snippet">    data = PM25_for_class)</p><p class="snippet">Deviance Residuals: </p><p class="snippet">    Min       1Q   Median       3Q      Max  </p><p class="snippet">-2.4699  -0.5212   0.4569   0.6508   3.5824  </p><p class="snippet">Coefficients:</p><p class="snippet">              Estimate Std. Error z value Pr(&gt;|z|)    </p><p class="snippet">(Intercept)  2.5240276  0.0273353   92.34   &lt;2e-16 ***</p><p class="snippet">DEWP         0.1231959  0.0016856   73.09   &lt;2e-16 ***</p><p class="snippet">TEMP        -0.1028211  0.0018447  -55.74   &lt;2e-16 ***</p><p class="snippet">Iws         -0.0127037  0.0003535  -35.94   &lt;2e-16 ***</p><p class="snippet">---</p><p class="snippet">Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</p><p class="snippet">(Dispersion parameter for binomial family taken to be 1)</p><p class="snippet">    Null deviance: 49475  on 41754  degrees of freedom</p><p class="snippet">Residual deviance: 37821  on 41751  degrees of freedom</p><p class="snippet">AIC: 37829</p><p class="snippet">Number of Fisher Scoring iterations: 5</p></li>
			</ol>
			<h2 id="_idParaDest-380"><a id="_idTextAnchor386"/>Chapter 4: Regression</h2>
			<h3 id="_idParaDest-381"><a id="_idTextAnchor387"/>Activity 7: Printing Various Attributes Using Model Object Without Using the summary Function</h3>
			<ol>
				<li value="1">First, print the coefficient values using the following command. Make sure the output is like the output of the <strong class="inline">summary</strong> function using the <strong class="inline">coefficients</strong> option. The coefficients are the fitted values from the model that uses the OLS algorithm:<p class="snippet">multiple_PM25_linear_model$coefficients</p><p>The output is as follows:</p><p class="snippet">(Intercept)        DEWP        TEMP         Iws </p><p class="snippet">161.1512066   4.3841960  -5.1335111  -0.2743375</p></li>
				<li>Find the residual value (difference) of the predicted and actual values of PM2.5, which should be as small as possible. Residual reflects how far the fitted values using the coefficients are from the actual value.<p class="snippet">multiple_PM25_linear_model$residuals</p><p>The output is as follows:</p><p class="snippet">25            26            27            28 </p><p class="snippet">  17.95294914   32.81291348   21.38677872   26.34105878 </p><p class="snippet">           29            30            31            32 </p></li>
				<li>Next, find the fitted values that should be closer to the actual PM2.5 for the best model. Using the coefficients, we can compute the fitted values:<p class="snippet">multiple_PM25_linear_model$fitted.values</p><p>The output is as follows:</p><p class="snippet">25         26         27         28         29 </p><p class="snippet">111.047051 115.187087 137.613221 154.658941 154.414781 </p><p class="snippet">        30         31         32         33         34 </p></li>
				<li>Find the R-Squared values. They should look the same as the one you obtained in the output of the <strong class="inline">summary</strong> function next to the text Multiple R-squared. R-Square helps in evaluating the model performance. If the value is closer to 1, the better the model is:<p class="snippet">summary(multiple_PM25_linear_model)$r.squared</p><p>The output is as follows:</p><p class="snippet">[1] 0.2159579</p></li>
				<li>Find the F-Statistic values. Make sure the output should look same as the one you obtained in the output of the <strong class="inline">summary</strong> function next to the text F-Statistics. This will tell you if your model fits better than just using the mean of the target variable. In many practical applications, F-Statistic is used along with p-values:<p class="snippet">summary(multiple_PM25_linear_model)$fstatistic</p><p>The output is as follows:</p><p class="snippet">    value     numdf     dendf </p><p class="snippet"> 3833.506     3.000 41753.000 </p></li>
				<li>Finally, find the coefficient p-values and make sure the values should look the same as the one you obtained in the output of the <strong class="inline">summary</strong> function under Coefficients for each variable. It will be present under the column titled <strong class="inline">Pr(&gt;|t|):</strong>. If the value is less than 0.05, the variable is statistically significant in predicting the target variable:<p class="snippet">summary(multiple_PM25_linear_model)$coefficients[,4]</p><p>The output is as follows:</p><p class="snippet">  (Intercept)          DEWP          TEMP           Iws </p><p class="snippet"> 0.000000e+00  0.000000e+00  0.000000e+00 4.279601e-224</p></li>
			</ol>
			<p>The attributes of a model are equally essential to understand, especially in linear regression than to obtain the prediction. They help in interpreting the model well and connect the problem to its real use case.</p>
			<h2 id="_idParaDest-382"><a id="_idTextAnchor388"/>Chapter 5: Classification</h2>
			<h3 id="_idParaDest-383"><a id="_idTextAnchor389"/>Activity 8: Building a Logistic Regression Model with Additional Features</h3>
			<ol>
				<li value="1">Create a copy of the <strong class="inline">df_new</strong> data frame into <strong class="inline">df_copy</strong> for the activity:<p class="snippet">df_copy &lt;- df_new</p></li>
				<li>Create new features for square root, square power, and cube power transformations for each of the three selected numeric features:<p class="snippet">df_copy$MaxTemp2 &lt;- df_copy$MaxTemp ^2</p><p class="snippet">df_copy$MaxTemp3 &lt;- df_copy$MaxTemp ^3</p><p class="snippet">df_copy$MaxTemp_root &lt;- sqrt(df_copy$MaxTemp)</p><p class="snippet">df_copy$Rainfall2 &lt;- df_copy$Rainfall ^2</p><p class="snippet">df_copy$Rainfall3 &lt;- df_copy$Rainfall ^3</p><p class="snippet">df_copy$Rainfall_root &lt;- sqrt(df_copy$Rainfall)</p><p class="snippet">df_copy$Humidity3pm2 &lt;- df_copy$Humidity3pm ^2</p><p class="snippet">df_copy$Humidity3pm3 &lt;- df_copy$Humidity3pm ^3</p><p class="snippet">df_copy$Humidity3pm_root &lt;- sqrt(df_copy$Humidity3pm)</p></li>
				<li>Divide the <strong class="inline">df_copy</strong> dataset into train and test in 70:30 ratio:<p class="snippet">#Setting seed for reproducibility</p><p class="snippet">set.seed(2019)</p><p class="snippet">#Creating a list of indexes for the training dataset (70%)</p><p class="snippet">train_index &lt;- sample(seq_len(nrow(df_copy)),floor(0.7 * nrow(df_copy)))</p><p class="snippet">#Split the data into test and train</p><p class="snippet">train_new &lt;- df_copy[train_index,]</p><p class="snippet">test_new &lt;- df_copy[-train_index,]</p></li>
				<li>Fit the logistic regression model with the new training data:<p class="snippet">model &lt;- glm(RainTomorrow~., data=train_new ,family=binomial(link='logit'))</p></li>
				<li>Predict the responses using the fitted model on the train data and create a confusion matrix:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;-factor(ifelse(predict(model,newdata=train_new,</p><p class="snippet">type = "response") &gt; 0.5,"Yes","No"))</p><p class="snippet">#Create the Confusion Matrix</p><p class="snippet">train_metrics &lt;- confusionMatrix(pred_train, train_new$RainTomorrow,positive="Yes")</p><p class="snippet">print(train_metrics)</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  58330  8650</p><p class="snippet">       Yes  3161  8906</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8506          </p><p class="snippet">                 95% CI : (0.8481, 0.8531)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5132          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5073          </p><p class="snippet">            Specificity : 0.9486          </p><p class="snippet">         Pos Pred Value : 0.7380          </p><p class="snippet">         Neg Pred Value : 0.8709          </p><p class="snippet">             Prevalence : 0.2221          </p><p class="snippet">         Detection Rate : 0.1127          </p><p class="snippet">   Detection Prevalence : 0.1527          </p><p class="snippet">      Balanced Accuracy : 0.7279          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes </p></li>
				<li>Predict the responses using the fitted model on test data and create a confusion matrix:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;-factor(ifelse(predict(model,newdata=test_new,</p><p class="snippet">type = "response") &gt; 0.5,"Yes","No"))</p><p class="snippet">#Create the Confusion Matrix</p><p class="snippet">test_metrics &lt;- confusionMatrix(pred_test, test_new$RainTomorrow,positive="Yes")</p><p class="snippet">print(test_metrics)</p><p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25057  3640</p><p class="snippet">       Yes  1358  3823</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8525          </p><p class="snippet">                 95% CI : (0.8486, 0.8562)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5176          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5123          </p><p class="snippet">            Specificity : 0.9486          </p><p class="snippet">         Pos Pred Value : 0.7379          </p><p class="snippet">         Neg Pred Value : 0.8732          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1128          </p><p class="snippet">   Detection Prevalence : 0.1529          </p><p class="snippet">      Balanced Accuracy : 0.7304          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p></li>
			</ol>
			<h3 id="_idParaDest-384"><a id="_idTextAnchor390"/>Activity 9: Create a Decision Tree Model with Additional Control Parameters</h3>
			<ol>
				<li value="1">Load the <strong class="inline">rpart</strong> library.<p class="snippet">library(rpart)</p></li>
				<li>Create the control object for decision tree with new values <strong class="inline">minsplit =15</strong> and <strong class="inline">cp = 0.00</strong>:<p class="snippet">control = rpart.control(</p><p class="snippet">    minsplit = 15, </p><p class="snippet">    cp = 0.001)</p></li>
				<li>Fit the tree model with the train data and pass the control object to the <strong class="inline">rpart</strong> function:<p class="snippet">tree_model &lt;- rpart(RainTomorrow~.,data=train, control = control)</p></li>
				<li>Plot the complexity parameter plot to see how the tree performs at different values of <strong class="inline">CP</strong>:<p class="snippet">plotcp(tree_model)</p><p>The output is as follows:</p><div id="_idContainer322" class="IMG---Figure"><img src="image/C12624_05_071.jpg" alt="Figure 5.10: Decision tree output&#13;&#10;"/></div><h6>Figure 5.10: Decision tree output</h6></li>
				<li>Use the fitted model to make predictions on train data and create the confusion matrix:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- predict(tree_model,newdata = train,type = "class")</p><p class="snippet">confusionMatrix(pred_train, train$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  58494  9032</p><p class="snippet">       Yes  2997  8524</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8478          </p><p class="snippet">                 95% CI : (0.8453, 0.8503)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4979          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4855          </p><p class="snippet">            Specificity : 0.9513          </p><p class="snippet">         Pos Pred Value : 0.7399          </p><p class="snippet">         Neg Pred Value : 0.8662          </p><p class="snippet">             Prevalence : 0.2221          </p><p class="snippet">         Detection Rate : 0.1078          </p><p class="snippet">   Detection Prevalence : 0.1457          </p><p class="snippet">      Balanced Accuracy : 0.7184          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p></li>
				<li>Use the fitted model to make predictions on test data and create the confusion matrix:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- predict(tree_model,newdata = test,type = "class")</p><p class="snippet">confusionMatrix(pred_test, test$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25068  3926</p><p class="snippet">       Yes  1347  3537</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8444          </p><p class="snippet">                 95% CI : (0.8404, 0.8482)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4828          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4739          </p><p class="snippet">            Specificity : 0.9490          </p><p class="snippet">         Pos Pred Value : 0.7242          </p><p class="snippet">         Neg Pred Value : 0.8646          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1044          </p><p class="snippet">   Detection Prevalence : 0.1442          </p><p class="snippet">      Balanced Accuracy : 0.7115          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes </p></li>
			</ol>
			<h3 id="_idParaDest-385"><a id="_idTextAnchor391"/>Activity 10: Build a Random Forest Model with a Greater Number of Trees</h3>
			<ol>
				<li value="1">First, import the <strong class="inline">randomForest</strong> library using the following command:<p class="snippet">library(randomForest)</p></li>
				<li>Build random forest model with all independent features available. Define the number of trees in the model to be 500.<p class="snippet">rf_model &lt;- randomForest(RainTomorrow ~ . , data = train, ntree = 500,                                             importance = TRUE, </p><p class="snippet">                                            maxnodes=60)</p></li>
				<li>Evaluate on training data:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- predict(rf_model,newdata = train,type = "class")</p><p class="snippet">confusionMatrix(pred_train, train$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  59638 10169</p><p class="snippet">       Yes  1853  7387</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8479          </p><p class="snippet">                 95% CI : (0.8454, 0.8504)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4702          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.42077         </p><p class="snippet">            Specificity : 0.96987         </p><p class="snippet">         Pos Pred Value : 0.79946         </p><p class="snippet">         Neg Pred Value : 0.85433         </p><p class="snippet">             Prevalence : 0.22210         </p><p class="snippet">         Detection Rate : 0.09345         </p><p class="snippet">   Detection Prevalence : 0.11689         </p><p class="snippet">      Balanced Accuracy : 0.69532         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes </p></li>
				<li>Evaluate on test data:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- predict(rf_model,newdata = test,type = "class")</p><p class="snippet">confusionMatrix(pred_test, test$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25604  4398</p><p class="snippet">       Yes   811  3065</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8462          </p><p class="snippet">                 95% CI : (0.8424, 0.8501)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4592          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.41069         </p><p class="snippet">            Specificity : 0.96930         </p><p class="snippet">         Pos Pred Value : 0.79076         </p><p class="snippet">         Neg Pred Value : 0.85341         </p><p class="snippet">             Prevalence : 0.22029         </p><p class="snippet">         Detection Rate : 0.09047         </p><p class="snippet">   Detection Prevalence : 0.11441         </p><p class="snippet">      Balanced Accuracy : 0.69000         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes  </p></li>
			</ol>
			<h2 id="_idParaDest-386"><a id="_idTextAnchor392"/>Chapter 6: Feature Selection and Dimensionality Reduction</h2>
			<h3 id="_idParaDest-387"><a id="_idTextAnchor393"/>Activity 11: Converting the CBWD Feature of the Beijing PM2.5 Dataset into One-Hot Encoded Columns</h3>
			<ol>
				<li value="1">Read the Beijing PM2.5 dataset into the DataFrame <strong class="inline">PM25</strong>:<p class="snippet">PM25 &lt;- read.csv("PRSA_data_2010.1.1-2014.12.31.csv")</p></li>
				<li>Create a variable <strong class="inline">cbwd_one_hot</strong> for storing the result of the <strong class="inline">dummyVars</strong> function with <strong class="inline">~ cbwd</strong> as its first argument:<p class="snippet">library(caret)</p><p class="snippet">cbwd_one_hot &lt;- dummyVars(" ~ cbwd", data = PM25) </p></li>
				<li>Use the output of the <strong class="inline">predict()</strong> function on <strong class="inline">cbwd_one_hot</strong> and case it as DataFrame:<p class="snippet">cbwd_one_hot &lt;- data.frame(predict(cbwd_one_hot, newdata = PM25))</p></li>
				<li>Remove the original <strong class="inline">cbwd</strong> variable from the <strong class="inline">PM25</strong> DataFrame:<p class="snippet">PM25$cbwd &lt;- NULL</p></li>
				<li>Using the <strong class="inline">cbind()</strong> function, add <strong class="inline">cbwd_one_hot</strong> to the <strong class="inline">PM25</strong> DataFrame:<p class="snippet">PM25 &lt;- cbind(PM25, cbwd_one_hot)</p></li>
				<li>Print the top 6 rows of <strong class="inline">PM25</strong>:<p class="snippet">head(PM25)</p><p>The output of the previous command is as follows:</p><p class="snippet">##   No year month day hour pm2.5 DEWP TEMP PRES   Iws Is Ir cbwd.cv cbwd.NE</p><p class="snippet">## 1  1 2010     1   1    0    NA  -21  -11 1021  1.79  0  0       0       0</p><p class="snippet">## 2  2 2010     1   1    1    NA  -21  -12 1020  4.92  0  0       0       0</p><p class="snippet">## 3  3 2010     1   1    2    NA  -21  -11 1019  6.71  0  0       0       0</p><p class="snippet">## 4  4 2010     1   1    3    NA  -21  -14 1019  9.84  0  0       0       0</p><p class="snippet">## 5  5 2010     1   1    4    NA  -20  -12 1018 12.97  0  0       0       0</p><p class="snippet">## 6  6 2010     1   1    5    NA  -19  -10 1017 16.10  0  0       0       0</p><p class="snippet">##   cbwd.NW cbwd.SE</p><p class="snippet">## 1       1       0</p><p class="snippet">## 2       1       0</p><p class="snippet">## 3       1       0</p><p class="snippet">## 4       1       0</p><p class="snippet">## 5       1       0</p><p class="snippet">## 6       1       0</p></li>
			</ol>
			<p>Observe the variable <strong class="inline">cbwd</strong> in the output of the <strong class="inline">head(PM25)</strong> command: it is now transformed into one-hot encoded columns with the <strong class="inline">NE</strong>, <strong class="inline">NW</strong>, and <strong class="inline">SE</strong> suffixes.</p>
			<h2 id="_idParaDest-388"><a id="_idTextAnchor394"/>Chapter 7: Model Improvements</h2>
			<h3 id="_idParaDest-389"><a id="_idTextAnchor395"/>Activity 12: Perform Repeated K-Fold Cross Validation and Grid Search Optimization</h3>
			<ol>
				<li value="1">Load the required packages <strong class="inline">mlbench</strong>, <strong class="inline">caret</strong>, and <strong class="inline">dplyr</strong> for the exercise:<p class="snippet">library(mlbench)</p><p class="snippet">library(dplyr)</p><p class="snippet">library(caret)</p></li>
				<li>Load the <strong class="inline">PimaIndianDiabetes</strong> dataset into memory from <strong class="inline">mlbench</strong> package:<p class="snippet">data(PimaIndiansDiabetes)</p><p class="snippet">df&lt;-PimaIndiansDiabetes</p></li>
				<li>Set a <strong class="inline">seed</strong> value as <strong class="inline">2019</strong> for reproducibility:<p class="snippet">set.seed(2019)</p></li>
				<li>Define the K-Fold validation object using the <strong class="inline">trainControl</strong> function from the <strong class="inline">caret</strong> package and define <strong class="inline">method</strong> as <strong class="inline">repeatedcv</strong> instead of <strong class="inline">cv</strong>. Define an additional construct in the <strong class="inline">trainControl</strong> function for the number of repeats in the validation <strong class="inline">repeats = 10</strong>:<p class="snippet">train_control = trainControl(method = "repeatedcv",                                number=5,                               repeats = 10,   savePredictions = TRUE,verboseIter = TRUE)</p></li>
				<li>Define the grid for hyperparameter <strong class="inline">mtry</strong> of random forest model as <strong class="inline">(3,4,5)</strong>:<p class="snippet">parameter_values = expand.grid(mtry=c(3,4,5))</p></li>
				<li>Fit the model with the grid values, cross-validation object, and random forest classifier:<p class="snippet">model_rf_kfold&lt;- train(diabetes~., data=df, trControl=train_control,                    method="rf",  metric= "Accuracy", tuneGrid = parameter_values)</p></li>
				<li>Study the model performance by printing the average accuracy and standard deviation of accuracy:<p class="snippet">print(paste("Average Accuracy :",mean(model_rf_kfold$resample$Accuracy)))</p><p class="snippet">print(paste("Std. Dev Accuracy :",sd(model_rf_kfold$resample$Accuracy)))</p></li>
				<li>Study the model performance by plotting the accuracy across different values of the hyperparameter:<p class="snippet">plot(model_rf_kfold)</p><p>The final output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="image/C12624_07_16.jpg" alt="Figure 7.17: Model performance accuracy across different values of the hyperparameter&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.17: Model performance accuracy across different values of the hyperparameter</h6>
			<p>In this plot, we can see that we perform repeated k-fold cross-validation and grid search optimization on the same model.</p>
			<h2 id="_idParaDest-390"><a id="_idTextAnchor396"/>Chapter 8: Model Deployment</h2>
			<h3 id="_idParaDest-391"><a id="_idTextAnchor397"/>Activity 13: Deploy an R Model Using Plumber</h3>
			<ol>
				<li value="1">Create a <strong class="inline">model.r</strong> script that will load the required libraries, data, fit a regression model and necessary function to predict on unseen data.</li>
				<li>Load the <strong class="inline">mlbench</strong> library that has the data for this activity:<p class="snippet">library(mlbench)</p></li>
				<li>Load <strong class="inline">BostonHousing</strong> data into a DataFrame <strong class="inline">df</strong>:<p class="snippet">data(BostonHousing)</p><p class="snippet">df&lt;-BostonHousing</p></li>
				<li>Create train dataset using the first <strong class="inline">400</strong> rows of df and test with the remaining:<p class="snippet">train &lt;- df[1:400,]</p><p class="snippet">test &lt;- df[401:dim(df)[1],]</p></li>
				<li>Fit a logistic regression model using the <strong class="inline">lm</strong> function with dependent variable as <strong class="inline">medv</strong> (median value) and 10 independent variables, such as, <strong class="inline">crim</strong>, <strong class="inline">zn</strong>, <strong class="inline">indus</strong>, <strong class="inline">chas</strong>, <strong class="inline">nox</strong>, <strong class="inline">rm</strong>, <strong class="inline">age</strong>, <strong class="inline">dis</strong>, <strong class="inline">rad</strong>, and <strong class="inline">tax</strong>.<p class="snippet">model &lt;- lm(medv~crim+zn+indus+chas+</p><p class="snippet"> nox+rm+age+dis+rad+tax,data=train)</p></li>
				<li>Define a model endpoint as <strong class="inline">predict_data</strong>; this will be used as the API endpoint for Plumber:<p class="snippet">#' @get /predict_data</p><p class="snippet">function(crim,zn,indus,chas,nox,rm,age,dis,rad,tax){</p></li>
				<li>Within the function, convert the parameters to numeric and factor (since the API call will pass them as string only):<p class="snippet">  crim &lt;- as.numeric(crim)</p><p class="snippet">  zn &lt;- as.numeric(zn)</p><p class="snippet">  indus &lt;- as.numeric(indus)</p><p class="snippet">  chas &lt;- as.factor(chas)</p><p class="snippet">  nox &lt;- as.numeric(nox)</p><p class="snippet">  rm &lt;- as.numeric(rm)</p><p class="snippet">  age &lt;- as.numeric(age)</p><p class="snippet">  dis &lt;- as.numeric(dis)</p><p class="snippet">  rad &lt;- as.numeric(rad)</p><p class="snippet">  tax &lt;- as.numeric(tax)</p></li>
				<li>Wrap the 10 independent features for the model as a DataFrame named <strong class="inline">sample</strong>, with the same name for the columns:<p class="snippet">  sample &lt;- data.frame(crim  = crim,  zn  = zn,  indus  = indus,  </p><p class="snippet">                       chas  = chas,  nox  = nox,  rm  = rm,  </p><p class="snippet">                       age  = age,  dis  = dis,  rad  = rad,  </p><p class="snippet">                       tax  = tax )</p></li>
				<li>Pass the <strong class="inline">sample</strong> DataFrame to the predict function with the model (created in the 4th step) and return predictions:<p class="snippet">  y_pred&lt;-predict(model,newdata=sample)</p><p class="snippet">  </p><p class="snippet">  list(Answer=y_pred)</p><p class="snippet">}</p><p>The entire <strong class="inline">model.r</strong> file will look like this:</p><p class="snippet">library(mlbench)</p><p class="snippet">data(BostonHousing)</p><p class="snippet">df&lt;-BostonHousing</p><p class="snippet">train &lt;- df[1:400,]</p><p class="snippet">test &lt;- df[401:dim(df)[1],]</p><p class="snippet">model &lt;- lm(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax,data=train)</p><p class="snippet">#' @get /predict_data</p><p class="snippet">function(crim,zn,indus,chas,nox,rm,age,dis,rad,tax){</p><p class="snippet">  </p><p class="snippet">  crim &lt;- as.numeric(crim)</p><p class="snippet">  zn &lt;- as.numeric(zn)</p><p class="snippet">  indus &lt;- as.numeric(indus)</p><p class="snippet">  chas &lt;- as.factor(chas)</p><p class="snippet">  nox &lt;- as.numeric(nox)</p><p class="snippet">  rm &lt;- as.numeric(rm)</p><p class="snippet">  age &lt;- as.numeric(age)</p><p class="snippet">  dis &lt;- as.numeric(dis)</p><p class="snippet">  rad &lt;- as.numeric(rad)</p><p class="snippet">  tax &lt;- as.numeric(tax)</p><p class="snippet">  </p><p class="snippet">  sample &lt;- data.frame(crim  = crim,  zn  = zn,  indus  = indus,  </p><p class="snippet">                       chas  = chas,  nox  = nox,  rm  = rm,  </p><p class="snippet">                       age  = age,  dis  = dis,  rad  = rad,  </p><p class="snippet">                       tax  = tax )</p><p class="snippet">  </p><p class="snippet">  y_pred&lt;-predict(model,newdata=sample)</p><p class="snippet">  </p><p class="snippet">  list(Answer=y_pred)</p><p class="snippet">}</p></li>
				<li>Load the <strong class="inline">plumber</strong> library.<p class="snippet">library(plumber)</p></li>
				<li>Create a plumber object using the <strong class="inline">plumb</strong> function and pass the <strong class="inline">model.r</strong> file (created in part 1).<p class="snippet">r &lt;- plumb(model.r)</p></li>
				<li>Run the plumber object by passing the hostname as <strong class="inline">localhost</strong> or <strong class="inline">127.0.0.1</strong> and a port, say <strong class="inline">8080</strong>.<p class="snippet">http://127.0.0.1:8080/</p></li>
				<li>Test the deployed model using the browser or Postman and invoke the API.<p>API invoke:</p><p>http://127.0.0.1:8080/predict_</p><p>ata?crim=0.01&amp;zn=18&amp;indus=2.3&amp;chas=0&amp;nox=0.5&amp;rm=6&amp;</p><p>age=65&amp;dis=4&amp;rad=1&amp;tax=242</p><p class="snippet">{"Answer":[22.5813]}</p></li>
			</ol>
			<h2 id="_idParaDest-392"><a id="_idTextAnchor398"/>Chapter 9: Capstone Project - Based on Research Papers</h2>
			<h3 id="_idParaDest-393"><a id="_idTextAnchor399"/>Activity 14: Getting the Binary Performance Step with classif.C50 Learner Instead of classif.rpart</h3>
			<ol>
				<li value="1">Define the<a id="_idTextAnchor400"/> algorithm adaptation methods:<p class="snippet">multilabel.lrn3 = makeLearner("multilabel.rFerns")</p><p class="snippet">multilabel.lrn4 = makeLearner("multilabel.randomForestSRC")</p><p class="snippet">multilabel.lrn3</p><p>The output is as follows:</p><p class="snippet">## Learner multilabel.rFerns from package rFerns</p><p class="snippet">## Type: multilabel</p><p class="snippet">## Name: Random ferns; Short name: rFerns</p><p class="snippet">## Class: multilabel.rFerns</p><p class="snippet">## Properties: numerics,factors,ordered</p><p class="snippet">## Predict-Type: response</p><p class="snippet">## Hyperparameters:</p></li>
				<li>Use the problem transformation method, and change the <strong class="inline">classif.rpart</strong> learner to <strong class="inline">classif.C50</strong>:<p class="snippet">lrn = makeLearner("classif.C50", predict.type = "prob")</p><p class="snippet">multilabel.lrn1 = makeMultilabelBinaryRelevanceWrapper(lrn)</p><p class="snippet">multilabel.lrn2 = makeMultilabelNestedStackingWrapper(lrn)</p><h4>Note</h4><p class="callout">You need to install the <strong class="inline">C50</strong> package for this code to work.</p></li>
				<li>Print the learner details:<p class="snippet">lrn</p><p>The output is as follows:</p><p class="snippet">## Learner classif.C50 from package C50</p><p class="snippet">## Type: classif</p><p class="snippet">## Name: C50; Short name: C50</p><p class="snippet">## Class: classif.C50</p><p class="snippet">## Properties: twoclass,multiclass,numerics,factors,prob,missings,weights</p><p class="snippet">## Predict-Type: prob</p><p class="snippet">## Hyperparameters:</p></li>
				<li>Print the multilabel learner details:<p class="snippet">multilabel.lrn1</p><p>The output is as follows:</p><p class="snippet">## Learner multilabel.binaryRelevance.classif.C50 from package C50</p><p class="snippet">## Type: multilabel</p><p class="snippet">## Name: ; Short name: </p><p class="snippet">## Class: MultilabelBinaryRelevanceWrapper</p><p class="snippet">## Properties: numerics,factors,missings,weights,prob,twoclass,multiclass</p><p class="snippet">## Predict-Type: prob</p><p class="snippet">## Hyperparameters:</p></li>
				<li>Train the model using the same dataset with training dataset:<p class="snippet">df_nrow &lt;- nrow(df_scene)</p><p class="snippet">df_all_index &lt;- c(1:df_nrow)</p><p class="snippet">train_index &lt;- sample(1:df_nrow, 0.7*df_nrow)</p><p class="snippet">test_index &lt;- setdiff(df_all_index,train_index)</p><p class="snippet">scene_classi_mod = train(multilabel.lrn1, scene.task, subset = train_index)</p></li>
				<li>Print the model details:<p class="snippet">scene_classi_mod</p><p>The output is as follows:</p><p class="snippet">## Model for learner.id=multilabel.binaryRelevance.classif.C50; learner.class=MultilabelBinaryRelevanceWrapper</p><p class="snippet">## Trained on: task.id = multi; obs = 1684; features = 294</p><p class="snippet">## Hyperparameters:</p></li>
				<li>Predict the output using the <strong class="inline">C50</strong> model we created for the test dataset:<p class="snippet">pred = predict(scene_classi_mod, task = scene.task, subset = test_index)</p><p class="snippet">names(as.data.frame(pred))</p><p>The output is as follows:</p><p class="snippet">##  [1] "id"                   "truth.Beach"          "truth.Sunset"        </p><p class="snippet">##  [4] "truth.FallFoliage"    "truth.Field"          "truth.Mountain"      </p><p class="snippet">##  [7] "truth.Urban"          "prob.Beach"           "prob.Sunset"         </p><p class="snippet">## [10] "prob.FallFoliage"     "prob.Field"           "prob.Mountain"       </p><p class="snippet">## [13] "prob.Urban"           "response.Beach"       "response.Sunset"     </p><p class="snippet">## [16] "response.FallFoliage" "response.Field"       "response.Mountain"   </p><p class="snippet">## [19] "response.Urban"</p></li>
				<li>Print the performance measures:<p class="snippet">MEASURES = list(multilabel.hamloss, multilabel.f1, multilabel.subset01, multilabel.acc, multilabel.tpr, multilabel.ppv)</p><p class="snippet">performance(pred, measures = MEASURES)</p><p>The output is as follows:</p><p class="snippet">##  multilabel.hamloss       multilabel.f1 multilabel.subset01 </p><p class="snippet">##           0.1258645           0.5734901           0.5532503 </p><p class="snippet">##      multilabel.acc      multilabel.tpr      multilabel.ppv </p><p class="snippet">##           0.5412633           0.6207930           0.7249104</p></li>
				<li>Print the performance measures for the <strong class="inline">listMeasures</strong> variable:<p class="snippet">listMeasures("multilabel")</p><p>The output is as follows:</p><p class="snippet">##  [1] "featperc"            "multilabel.tpr"      "multilabel.hamloss" </p><p class="snippet">##  [4] "multilabel.subset01" "timeboth"            "timetrain"          </p><p class="snippet">##  [7] "timepredict"         "multilabel.ppv"      "multilabel.f1"      </p><p class="snippet">## [10] "multilabel.acc"</p></li>
				<li>Run the resampling with cross-validation method:<p class="snippet">rdesc = makeResampleDesc(method = "CV", stratify = FALSE, iters = 3)</p><p class="snippet">r = resample(learner = multilabel.lrn1, task = scene.task, resampling = rdesc,measures = list(multilabel.hamloss), show.info = FALSE)</p><p class="snippet">r</p><p>The output is as follows:</p><p class="snippet">## Resample Result</p><p class="snippet">## Task: multi</p><p class="snippet">## Learner: multilabel.binaryRelevance.classif.C50</p><p class="snippet">## Aggr perf: multilabel.hamloss.test.mean=0.1335695</p><p class="snippet">## Runtime: 72.353</p></li>
				<li>Print the binary performance:<p class="snippet">getMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce, auc))</p><p>The output is as follows:</p><p class="snippet">##             acc.test.mean mmce.test.mean auc.test.mean</p><p class="snippet">## Beach           0.8608226     0.13917740     0.8372448</p><p class="snippet">## Sunset          0.9401745     0.05982551     0.9420085</p><p class="snippet">## FallFoliage     0.9081845     0.09181554     0.9008202</p><p class="snippet">## Field           0.8998754     0.10012464     0.9134458</p><p class="snippet">## Mountain        0.7710843     0.22891566     0.7622767</p><p class="snippet">## Urban           0.8184462     0.18155380     0.7837401</p></li>
			</ol>
		</div>
	</body></html>