<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer021">
<h1 class="chapter-number" id="_idParaDest-46"><a id="_idTextAnchor049"/>3</h1>
<h1 id="_idParaDest-47"><a id="_idTextAnchor050"/>Privacy Issues in Real Data</h1>
<p>ML is becoming an essential part of our daily lives due to its varied applications. Thus, there is a growing concern about privacy issues in ML. Datasets and trained ML models may disclose personal and sensitive information, such as political views, biometric data, mental health, sexual orientation, and other <span class="No-Break">private information.</span></p>
<p>In this chapter, we will learn about privacy issues and why this is a concern in ML. Furthermore, we will provide a brief introduction to <span class="No-Break">privacy-preserving ML.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Why is privacy an issue <span class="No-Break">in ML?</span></li>
<li>What exactly is the privacy problem <span class="No-Break">in ML?</span></li>
<li><span class="No-Break">Privacy-preserving ML</span></li>
<li>Real data challenges <span class="No-Break">and issues</span></li>
</ul>
<h1 id="_idParaDest-48"><a id="_idTextAnchor051"/>Why is privacy an issue in ML?</h1>
<p>As we discussed in the previous<a id="_idIndexMarker069"/> chapters, ML models need large-scale training data to converge and train well. The data can be collected from social media, online transactions, surveys, questionaries, or other sources. Thus, the collected data may contain sensitive information that individuals may not want to share with some organizations or individuals. If the data was shared or accessed by others, and thus the identities of the individuals were identified, that may cause them personal abuse, financial issues, or <span class="No-Break">identity theft.</span></p>
<p>The complexity of a privacy breach in ML is closely related to the following main <span class="No-Break">three factors:</span></p>
<ul>
<li><span class="No-Break">ML task</span></li>
<li><span class="No-Break">Dataset size</span></li>
<li><span class="No-Break">Regulations</span></li>
</ul>
<p>Let’s take a <span class="No-Break">closer look.</span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor052"/>ML task</h2>
<p>The <em class="italic">task</em> mainly defines<a id="_idIndexMarker070"/> the type of training data that we need to collect and annotate. Thus, some ML tasks, such as weather prediction and music generation, may have fewer privacy issues compared to other ML tasks, such as biometric authentication and medical image processing <span class="No-Break">and analysis.</span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor053"/>Dataset size</h2>
<p>The larger the dataset, the more issues<a id="_idIndexMarker071"/> you will have with privacy. If the dataset is a large-scale one, then you cannot store it on one device. Thus, you may need some cloud services and tools to manage your datasets, such as MongoDB, Hadoop, and Cassandra. Consequently, you may have less control over your data. Thus, you need to give more attention to the tools and services that process or manage <span class="No-Break">your data.</span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor054"/>Regulations</h2>
<p>Many countries have clear and restrictive<a id="_idIndexMarker072"/> data protection regulations. For example, the UK has <em class="italic">The Data Protection Act 2018</em> (<a href="https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted">https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted</a>). At the same time, similar regulations can be found in the EU, such as the <em class="italic">General Data Protection Regulation</em> (<a href="https://gdpr-info.eu">https://gdpr-info.eu</a>). Thus, if you are using personal data, you have to consider data protection principles, such as transparency, being used for their specified purposes, and being kept for a <span class="No-Break">specific period.</span></p>
<p>If your dataset includes sensitive and confidential information, then you are subject to more restrictions. Your dataset may contain information about biometrics, gender, health, and religious and political opinions. Thus, if you repurpose a dataset that you, your organization, or someone else collected, this action may <span class="No-Break">be illegal.</span></p>
<p>One of the main limitations of utilizing real datasets is privacy issues. Due to this, we can see why synthetic data is a promising solution for these issues. In the next section, we will take a closer look at the privacy problem and its related topics, such as copyright and intellectual property<a id="_idIndexMarker073"/> infringement, which are major issues when collecting a <span class="No-Break">new dataset.</span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor055"/>What exactly is the privacy problem in ML?</h1>
<p>Within the scope<a id="_idIndexMarker074"/> of privacy in ML, there are two main concerns. The first is regarding the dataset itself – that is, how to collect it, how to keep it private, and how to prevent unauthorized access to sensitive information. The second is associated with the vulnerability of ML models to reveal the training data, which we will discuss in the next section. For now, let’s examine the issues related to dataset privacy <span class="No-Break">in ML.</span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor056"/>Copyright and intellectual property infringement</h2>
<p>Copyright is a legal term<a id="_idIndexMarker075"/> that’s used to protect the ownership of intellectual property. It prevents or limits others from using your work without your permission. For example, if you take a photograph, record a video, or write a blog, your work is protected by copyright. Thus, others may not share, reproduce, or distribute your work without permission. Consequently, images, videos, text, or other information we see on the internet may have restrictive copyrights. Therefore, if you want to collect a dataset, you must consider the copyright problem carefully. As we know, there are different approaches to curating a real dataset. As an ML practitioner, you can do <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Collect data yourself</strong>: You can use a <a id="_idIndexMarker076"/>camera, microphone, sensors, questionaries, and other methods to collect a tremendous amount <span class="No-Break">of data.</span></li>
<li><strong class="bold">Collect data from the internet</strong>: You can use search engines such as Google, Yahoo, Baidu, and DuckDuckGo to collect<a id="_idIndexMarker077"/> your dataset, similar to how the <em class="italic">ImageNet</em> (<a href="https://www.image-net.org/">https://www.image-net.org/</a>) dataset <span class="No-Break">was collected.</span></li>
<li><strong class="bold">Collect data from other datasets</strong>: You can combine different datasets to create a new dataset for a specific application – for example, if you are interested in visual object tracking for only one class – say, humans – in a specific scenario – say, adverse conditions. In this case, you can combine different datasets by excluding irrelevant classes and weather conditions. Thus, you create a new dataset for your<a id="_idIndexMarker078"/> <span class="No-Break">particular problem.</span></li>
</ul>
<p>Taking a photo of someone with our camera or recording their voice using our hardware does not permit us to use their information. At the same time, asking for permission is not a practical solution for large-scale<a id="_idIndexMarker079"/> datasets. To avoid similar issues some, datasets, such as <strong class="bold">ImageNet</strong>, were initially collected from the internet. Then, it was annotated by human annotators. It should be noted that not every image we see on the internet can be used to build our dataset; some images have copyright licenses that restrict how they can be used. Thus, the availability of data that you need on the web does not necessarily mean that you can leverage that data for your problem. The same copyright issues can be seen for combining different datasets <span class="No-Break">as well.</span></p>
<p>If your ML model was trained on, say, Van Gogh paintings and learned how to generate artwork, a question arises<a id="_idIndexMarker080"/> regarding who owns the rights to the generated images: Van Gogh, ML engineers, or both? In ML, this is a controversial<a id="_idIndexMarker081"/> and complex issue. <strong class="bold">ChatGPT</strong> (<a href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</a>) and <strong class="bold">Imagen</strong> (<em class="italic">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</em>) are examples of such models. Thus, training your model on a real dataset may not give you the right to fully leverage the potential<a id="_idIndexMarker082"/> of your ML model. You may still be subject to copyright and the intellectual properties of the samples used in the training process. As you may expect, leveraging synthetic data seems a promising solution to <span class="No-Break">these problems.</span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor057"/>Privacy and reproducibility of experiments</h2>
<p>One of the main issues in ML-based research<a id="_idIndexMarker083"/> is the difficulty in reproducing the experiments, and thus validating the results claimed in research papers. Data privacy is one of the key factors behind this issue. Many companies develop ML solutions using their own datasets. They may share trained models, but not the dataset itself because of many reasons related to privacy issues and regulations. Consequently, other researchers cannot reproduce the experiments and results. This creates a good chance for errors, bias, misinterpretation, falsification, and manipulation of research results. This is also another reason why using synthetic data in certain fields, such as healthcare and finance, can make the research more transparent and the results<a id="_idIndexMarker084"/> <span class="No-Break">more trustworthy.</span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor058"/>Privacy issues and bias</h2>
<p>Bias is another<a id="_idIndexMarker085"/> issue closely related to privacy problems in ML models and real data. ML models can be trained on biased data, which can result in biased outcomes. For example, some face recognition commercial applications were found to be less accurate in recognizing people with darker skin, females, or people aged between 18 and 30. The issue is due to the bias in the training data of the ML models. Thus, when the training data is not available because of privacy concerns, certain bias issues may arise. Consequently, this can lead to unequal treatment and discrimination based on race, gender, and other factors. Next, we will delve into traditional solutions to <span class="No-Break">privacy issues.</span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor059"/>Privacy-preserving ML</h1>
<p><strong class="bold">Privacy-preserving ML</strong> is a hot topic since it proposes solutions<a id="_idIndexMarker086"/> for privacy issues in the ML field. Privacy-preserving ML proposes methods that allow researchers to use sensitive data to train ML models while withholding sensitive information from being shared or accessed by a third party or revealed by the ML model. In the first subsection, we will examine common methods for mitigating privacy issues <span class="No-Break">in datasets.</span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor060"/>Approaches for privacy-preserving datasets</h2>
<p>In this section, we’ll delve<a id="_idIndexMarker087"/> into standard approaches for handling and protecting sensitive information in datasets. We will look at anonymization, centralized data, and <span class="No-Break">differential privacy.</span></p>
<h3>Anonymization</h3>
<p><strong class="bold">Anonymization</strong> can be considered one of the earliest<a id="_idIndexMarker088"/> approaches for privacy issues in datasets. Let’s assume you are given a dataset of patients’ medical records that contains their addresses, phone numbers, and postal codes. To anonymize this dataset, you can simply remove this sensitive data while keeping other medical records. Unfortunately, this approach does not work very well for two reasons. Sometimes, you cannot remove this sensitive information because they are part of the task. At the same time, anonymization may not be sufficient, and thus individuals may be identified by linking and combining other information in the dataset<a id="_idIndexMarker089"/> or other datasets. As an example, check the paper titled <em class="italic">The ’Re-Identification’ of Governor William Weld’s Medical Information: A Critical Re-Examination of Health Data Identification Risks and Privacy Protections, Then and </em><span class="No-Break"><em class="italic">Now</em></span><span class="No-Break"> (</span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397"><span class="No-Break">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397</span></a><span class="No-Break">):</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Name</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Surname</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Has diabetes</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Smoking</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Gender</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Age</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Occupation</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Mike</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Chris</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Programmer</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Emma</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Cunningham</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Lawyer</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Jan</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Wan</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">PhD Student</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Olivia</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Cunningham</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">23</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Student</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A sample dataset that includes sensitive information</p>
<h3>Centralized data</h3>
<p>Another approach<a id="_idIndexMarker090"/> is based on the idea of keeping<a id="_idIndexMarker091"/> the data in a private server and not sharing the data itself but instead answering queries about the data. For example, here are some examples of queries regarding the dataset shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
<ul>
<li>Return the minimum age of patients <span class="No-Break">with diabetes</span></li>
<li>Return the number of female patients <span class="No-Break">with diabetes</span></li>
<li>Return the average age of patients <span class="No-Break">with diabetes</span></li>
</ul>
<p>As you can see, organizations can keep sensitive data while still allowing other researchers or organizations to leverage this data. However, this approach is not resilient against cyberattacks. These queries can be used by attackers to identify individuals and disclose sensitive information by combining complex queries and linking information from other datasets. For example, if you remove the <strong class="bold">Name</strong> and <strong class="bold">Surname</strong> columns but still use information regarding <strong class="bold">Gender</strong>, <strong class="bold">Age</strong>, and <strong class="bold">Occupation</strong>, it may still be possible to use these details<a id="_idIndexMarker092"/> to identify<a id="_idIndexMarker093"/> <span class="No-Break">some patients.</span></p>
<h3>Differential privacy</h3>
<p>As a complementary<a id="_idIndexMarker094"/> solution to the previous approaches, differential privacy<a id="_idIndexMarker095"/> seems a promising solution. However, there are some limitations, as we will <span class="No-Break">see later.</span></p>
<p>The key idea of this approach is keeping individuals’ information secured while still learning about the phenomena under consideration. This approach is based on queries to a server that contains all the sensitive data. Many companies utilize differential privacy, such as Uber, Google, and Apple. The algorithm may add random noise to the information in the dataset or the queries, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 3.2 – Local and global differential privacy" height="976" src="image/B18494_03_002.jpg" width="1371"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Local and global differential privacy</p>
<p>Differential privacy has two <span class="No-Break">main approaches:</span></p>
<ul>
<li><strong class="bold">Local differential privacy</strong> (<strong class="bold">LDP</strong>): The data taken from the users are processed<a id="_idIndexMarker096"/> with noise, such as Laplacian noise. This process<a id="_idIndexMarker097"/> makes the data more secure against attacks. Even if attackers can access the data, they will have a noisy version and they will not be able to <span class="No-Break">identify individuals.</span></li>
<li><strong class="bold">Global differential privacy</strong> (<strong class="bold">GDP</strong>): The raw data taken from clients is not altered but directly<a id="_idIndexMarker098"/> stored in the server without <a id="_idIndexMarker099"/>any noise being added. When a query is received, the raw answer (accurate) is returned after noise has been added to it. This process generates a private answer to protect <span class="No-Break">data privacy.</span></li>
</ul>
<p>Thus, with these data protection mechanisms, it’s supposed that ML models can now be trained on sensitive data without disclosing individual sensitive information. Unfortunately, this assumption<a id="_idIndexMarker100"/> is not always valid, as we will see in the next <span class="No-Break">few sections.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor061"/>Approaches for privacy-preserving ML</h2>
<p><strong class="bold">Privacy-preserving machine learning</strong> (<strong class="bold">PPML</strong>) aims to prevent training data leakage. ML models<a id="_idIndexMarker101"/> may memorize sensitive information in the training<a id="_idIndexMarker102"/> process, and thus confidential information may be revealed by the ML model. Standard PPML methods rely on differential privacy, which we discussed earlier. Next, you will be introduced to <span class="No-Break">federated learning.</span></p>
<h3>Federated learning</h3>
<p><strong class="bold">Federated learning</strong> is a novel strategy that allows ML<a id="_idIndexMarker103"/> to be trained on sensitive<a id="_idIndexMarker104"/> data while not transferring the data outside of the local server or node. Thus, organizations can share their sensitive data to train ML models while keeping the data in the organization’s local servers. At the same time, this is a solution for regulation that prevents sharing data with external parties. It is based on the paradigm of decentralized learning, which we will see next. Please refer to <a href="https://github.com/topics/federated-learning">https://github.com/topics/federated-learning</a> for a wide range of federated learning frameworks and libraries such as <em class="italic">FATE</em>, <em class="italic">FedML</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">PySyft</em></span><span class="No-Break">.</span></p>
<p>To begin, let’s differentiate<a id="_idIndexMarker105"/> between centralized and decentralized <span class="No-Break">ML systems:</span></p>
<ul>
<li><strong class="bold">Centralized ML system</strong>: In centralized ML systems, all the training is done in one server. It is easier<a id="_idIndexMarker106"/> to implement, and thus it was traditionally applied to ML problems. However, it has many limitations due to the communication between the users and the central server. As shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em>, the users need to send information to the central server where the training process will be executed. In addition to the latency issues, this approach is more vulnerable<a id="_idIndexMarker107"/> <span class="No-Break">to attacks:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 3.3 – Centralized and decentralized ML systems" height="1013" src="image/B18494_03_003.jpg" width="1425"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Centralized and decentralized ML systems</p>
<ul>
<li><strong class="bold">Decentralized ML system</strong>: In contrast to the centralized option, this system allows the ML <a id="_idIndexMarker108"/>model to be trained on the client node rather than the centralized server. The clients send the weights and biases of their trained ML models to the admin server. Following this, the weights and biases are utilized to construct the final ML model. This is a clever solution to many privacy issues. For example, many hospitals in the EU cannot share their patients’ data with organizations or people outside the hospital. Thus, using the decentralized ML system, other organizations may access patient data since their data will not leave the hospital’s servers and the training process will also be done on <span class="No-Break">their servers.</span></li>
</ul>
<p>In general, we can see that decentralized ML systems have the <span class="No-Break">following advantages:</span></p>
<ul>
<li>Less communication between<a id="_idIndexMarker109"/> the server <span class="No-Break">and clients</span></li>
<li>Better privacy as the clients do not share raw data; instead, they share weights <span class="No-Break">and biases</span></li>
<li>Better privacy since data is stored<a id="_idIndexMarker110"/> in the local nodes and does not leave the organization during<a id="_idIndexMarker111"/> the <span class="No-Break">training process.</span></li>
</ul>
<p>In the next section, we will briefly discuss the essence of the privacy issues in <span class="No-Break">real data.</span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor062"/>Real data challenges and issues</h1>
<p>So far in this chapter, we have presented different<a id="_idIndexMarker112"/> approaches for mitigating the privacy<a id="_idIndexMarker113"/> issues in real data. As you can see, it is clear that these approaches<a id="_idIndexMarker114"/> have limitations and they are not always practical. One fundamental issue is that ML models memorize the training data. Thus, given a trained ML model, it may be possible to retrieve some of the training data. Many researchers recently raised a red flag about the privacy of ML models, even after applying standard privacy solutions. For more information, please refer to <em class="italic">How To Break Anonymity of the Netflix Prize Dataset</em> (<a href="https://arxiv.org/abs/cs/0610105">https://arxiv.org/abs/cs/0610105</a>) and <em class="italic">The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural </em><span class="No-Break"><em class="italic">Networks</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1802.08232"><span class="No-Break">https://arxiv.org/abs/1802.08232</span></a><span class="No-Break">).</span></p>
<p>The nature of the real data is the essence of the problem. For instance, if you are given real human faces and you do some operations to anonymize this data or if you apply state-of-the-art approaches for PPML training, the data is still at risk of being divulged. Thus, it seems that instead of proposing more complicated and sometimes impractical solutions for privacy issues in real data, it’s time to look at other alternatives and focus on the essence of the problem, which is real data. In other words, it seems that synthetic data represents a rich and safe source for training large-scale ML models, therefore solving these complex <span class="No-Break">privacy issues.</span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor063"/>Summary</h1>
<p>In this chapter, we discussed why privacy is a critical problem in ML. At the same time, we learned what exactly the privacy problem in this field is. We also learned about the main standard solutions and <span class="No-Break">their limitations.</span></p>
<p>In the next chapter, we will discover what synthetic data is. This will help us build a solid foundation so that we can learn how to utilize synthetic data as a solution to the real data problems that we examined in the <span class="No-Break">previous chapters.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer022">
<h1 id="_idParaDest-61" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor064"/>Part 2:An Overview of Synthetic Data for Machine Learning</h1>
<p>In this part, you will be introduced to synthetic data. You will learn about the history and the main types of synthetic data. Then, you will explore its main advantages. You will understand why synthetic data is a promising solution for many complex issues, such as privacy, that hinder the progress of ML in certain fields. You will learn how synthetic data generation approaches can be utilized to generate data for rare scenarios that are usually expensive and dangerous to capture with <span class="No-Break">real data.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18494_04.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">An Introduction to Synthetic Data</em></li>
<li><a href="B18494_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Synthetic Data as a Solution</em></li>
</ul>
</div>
<div>
<div id="_idContainer023">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer024">
</div>
</div>
</div></body></html>