<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Music Genre Recommendation</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we are going to go back to supervised learning. We have built numerous supervised learning algorithms for both classification and regression problems using learning algorithms such as logistic regression, Naive Bayes, random forest, and <strong class="calibre4">Support Vector Machine</strong> (<strong class="calibre4">SVM</strong>). However, the number of outputs from these models we have built has always been one. In our Twitter sentiment analysis project, the output could only be one of positive, negative, or neutral. On the other hand, in our housing price prediction project, the output was a log of house prices predicted. Unlike our previous projects, there are cases where we want our <strong class="calibre4">machine learning</strong> (<strong class="calibre4">ML</strong>) models to output multiple values. A recommendation system is one example of where we need ML models that can produce rank-ordered predictions.</p>
<p class="calibre2">In this chapter, we are going to use a dataset that contains various audio features, compiled from numerous music recordings. With this data, we are going to explore how the values of audio features, such as kurtosis and skewness of the sound spectrum, are distributed across different genres of songs. Then, we are going to start building multiple ML models that output the predicted probabilities of the given song belonging to each music genre, instead of producing just one prediction output of the most likely genre for a given song. Once we have these models built, we are going to take it a step further and ensemble the prediction results of these base models to build a meta model for the final recommendations of song music genres. We are going to use a different model validation metric, <strong class="calibre4">Mean Reciprocal Rank</strong> (<strong class="calibre4">MRR</strong>), to evaluate our ranking models.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Problem definition for the Music Genre Recommendation project</li>
<li class="calibre11">Data analysis for the audio features dataset</li>
<li class="calibre11">ML models for music genre classification</li>
<li class="calibre11">Ensembling base learning models</li>
<li class="calibre11">Evaluating recommendation/rank-ordering models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Problem definition</h1>
                
            
            <article>
                
<p class="calibre2">Let's get into greater detail and properly define what problems we are going to solve and what machine learning models we are going to build for this project. Music streaming services, such as Pandora and Spotify, require music recommendation systems, with which they can recommend and play songs that their listeners might like. There is more than one way to build a music recommendation system. One way is to look at what other similar users listened to, and the way to define similar users is to look at the history of songs that they listened to. However, this approach will not work well if the user is new to the platform and/or if we do not have enough of a history of songs he or she listened to. In this case, we cannot rely on the historical data. Instead, it will be better to use the attributes of the songs that the user is currently listening to recommend other music. One song attribute that can play an important role in music recommendation is the genre. It is highly likely that a user who is currently listening to music on the platform will like to continue listening to the same or similar music. Imagine you were listening to instrumental music and the music streaming application then suddenly played rock music. It would not be a smooth transition and it would not be a good user experience, as you most likely would have wanted to continue listening to instrumental music. By correctly identifying the genre of the songs and recommending the right song type to play, you can avoid disturbing the user experience of your music streaming service.</p>
<p class="calibre2">In order to build a music genre recommendation model, we are going to use <strong class="calibre4">FMA: A Dataset For Music Analysis</strong>, which contains a large amount of data for over 100,000 tracks. The dataset contains information about the album, title, audio attributes, and so forth, and the full dataset can be found and downloaded from this link: <a href="https://github.com/mdeff/fma" class="calibre9">https://github.com/mdeff/fma</a>. With this data, we are going to sub-select the features that are of interest and build numerous ML models that output the probability of each song belonging to different music genres. Then, we are going to rank-order the genres by probability. We will be experimenting with various learning algorithms, such as logistic regression, Naive Bayes, and SVM. We are going to take it a step further by using the ensembling technique to take the output of these models as an input to another ML model that produces the final prediction and recommendation output. We are going to use MRR as the metric to evaluate our music genre recommendation models.</p>
<p class="calibre2"><span class="calibre5">To summarize our problem definition for the music genre recommendation project:</span></p>
<ul class="calibre10">
<li class="calibre11">What is the problem? We need a recommendation model that rank-orders music genres by how likely it is that a song belongs to each genre, so that we can properly identify the genre of a song and recommend what song to play next.</li>
<li class="calibre11">Why is it a problem?<strong class="calibre1"> </strong><span>Use of historical data for music recommendation is not reliable for those users who are new to the platform, as they will not have enough historical data for good music recommendations. </span>In this case, we will have to use audio and other features to identify what music to play next. Correctly identifying and recommending the genre of music is the first step to figuring out what song to play next.</li>
<li class="calibre11">What are some approaches to solving this problem? We are going to use publicly available music data, which not only contains information about the album, title, and artist of the song, but also contains information about numerous audio features. Then, we are going to build ML models that output the probabilities and use this probability output to rank-order genres for given song.</li>
<li class="calibre11">What are the success criteria? We want the correct music genre to come up as one of the top predicted genres. We will use MRR as the metric to evaluate ranking models.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data analysis for the audio features dataset</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's start looking into the audio features dataset. In order to focus on building recommendation models for music genres, we trimmed down the original dataset from <strong class="calibre4">FMA: A Dataset For Music Analysis</strong>. You can download this data from this link: </span><a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Target variable distribution</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We will first look at the distribution of our target variable for this project and figure out how many records we have for each genre in our sample set. The following code snippet shows how we aggregated our sample set by the target variable, <kbd class="calibre12">genre_top</kbd>, and counted the number of records for each genre:</span></p>
<pre class="calibre19">var genreCount = featuresDF.AggregateRowsBy&lt;string, int&gt;(<br class="title-page-name"/>    new string[] { "genre_top" },<br class="title-page-name"/>    new string[] { "track_id" },<br class="title-page-name"/>    x =&gt; x.ValueCount<br class="title-page-name"/>).SortRows("track_id");<br class="title-page-name"/><br class="title-page-name"/>genreCount.Print();<br class="title-page-name"/><br class="title-page-name"/>var barChart = DataBarBox.Show(<br class="title-page-name"/>    genreCount.GetColumn&lt;string&gt;("genre_top").Values.ToArray().Select(x =&gt; x.Substring(0,3)),<br class="title-page-name"/>    genreCount["track_id"].Values.ToArray()<br class="title-page-name"/>).SetTitle(<br class="title-page-name"/>    "Genre Count"<br class="title-page-name"/>);</pre>
<p class="calibre2">Similar to previous chapters, we used the <kbd class="calibre12">AggregateRowsBy</kbd> method in the Deedle data frame to count the number of records per genre. Then, we used the <kbd class="calibre12">DataBarBox</kbd> class to create a bar chart that shows the distribution of the target variable visually. As you can see from this code snippet (in line 10), we are using the first three letters of each genre as a label for each genre in the bar chart.</p>
<p class="calibre2"><span class="calibre5">When you run this code, you will see the following output for the distribution of the target variable:</span></p>
<div class="mce-root"><img class="alignnone32" src="../images/00097.gif"/></div>
<p class="calibre2"><span class="calibre5">The following plot shows the bar chart for the distribution of the target variable:</span></p>
<div class="mce-root"><img src="../images/00098.jpeg" class="calibre86"/></div>
<p class="calibre2"><span class="calibre5">As you can see from this chart, we have the largest number for</span> Instrumental (<strong class="calibre4">Ins</strong>) m<span class="calibre5">usic in our sample set a</span>nd Electronic (<strong class="calibre4">Ele</strong>) and Rock (<strong class="calibre4">Roc</strong>) follo<span class="calibre5">w as the second and third. Although this sample set contains some songs in certain genres more so than others, this is a relatively well balanced set, where one or two genres do not take up the majority of the sample records. Now, let's look at the distributions of some of our features.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Audio features – MFCC</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">For this project, we are going to focus on a subset of features that the full dataset has. We are going to use <strong class="calibre4">Mel Frequency Cepstral Coefficients</strong></span> (<span class="calibre5"><strong class="calibre4">MFCCs</strong></span>) a<span class="calibre5">nd their statistical distributions as the features to our ML models. Simply put, <strong class="calibre4">MFCC</strong> is a representation of the sound spectrum and we will use its statistical distributions, kurtosis, skewness, min, max, mean, median, and standard deviation. If you look at the sample set you have downloaded from the previous step, you will see the columns are named according to the corresponding statistical distribution. We are going to first look at the distributions of each of these features. The following code snippet shows how we computed the quartiles for each feature:</span></p>
<pre class="calibre19">foreach (string col in featuresDF.ColumnKeys)<br class="title-page-name"/>{<br class="title-page-name"/>    if (col.StartsWith("mfcc"))<br class="title-page-name"/>    {<br class="title-page-name"/>        int idx = int.Parse(col.Split('.')[2]);<br class="title-page-name"/>        if(idx &lt;= 4)<br class="title-page-name"/>        {<br class="title-page-name"/>            Console.WriteLine(String.Format("\n\n-- {0} Distribution -- ", col));<br class="title-page-name"/>            double[] quantiles = Accord.Statistics.Measures.Quantiles(<br class="title-page-name"/>                featuresDF[col].ValuesAll.ToArray(),<br class="title-page-name"/>                new double[] { 0, 0.25, 0.5, 0.75, 1.0 }<br class="title-page-name"/>            );<br class="title-page-name"/>            Console.WriteLine(<br class="title-page-name"/>                "Min: \t\t\t{0:0.00}\nQ1 (25% Percentile): \t{1:0.00}\nQ2 (Median): \t\t{2:0.00}\nQ3 (75% Percentile): \t{3:0.00}\nMax: \t\t\t{4:0.00}",<br class="title-page-name"/>                quantiles[0], quantiles[1], quantiles[2], quantiles[3], quantiles[4]<br class="title-page-name"/>            );<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">Similar to previous chapters, we are using the <kbd class="calibre12">Quantiles</kbd> method in the <kbd class="calibre12">Accord.Statistics.Measures</kbd> class to compute quartiles, which are the three numbers that separate the values into four subsets—the middle number between the min and median (25<sup class="calibre64">th</sup> percentile), median (50<sup class="calibre64">th</sup> percentile), and the middle number between the median and max (75<sup class="calibre64">th</sup> percentile). As you can see in </span>line 6 <span class="calibre5">of this code snippet, we are only showing the first four coefficients' statistical distributions. For your further experiments, you can look at the distributions of all the MFCC features, not limited to only these four. Let's quickly take a look at just a couple of the distributions.</span></p>
<p class="calibre2">The distribution for the kurtosis of the first four coefficients looks like the following:</p>
<div class="mce-root"><img class="alignnone33" src="../images/00099.gif"/></div>
<p class="calibre2"><span class="calibre5">As you can see from this output, the majority of the kurtosis values fall between -2 and 5, but there are cases where the kurtosis can take large values. Let's now look at the skewness distributions for the first four coefficients:</span></p>
<div class="mce-root"><img class="alignnone34" src="../images/00100.gif"/></div>
<p class="calibre2"><span class="calibre5">Skewness varies between narrower ranges. Typically, the skewness values seem to fall between -15 and 5. Lastly, let's look at the distributions of the mean of the first four coefficients:</span></p>
<div class="mce-root"><img class="alignnone35" src="../images/00101.jpeg"/></div>
<p class="calibre2"><span class="calibre5">As you can see from this output, the mean values seem to vary and have wide ranges. It can take any values between -1,000 and 300.</span></p>
<p class="calibre2">Now that we have a rough idea of how the audio features' distributions look, let's see if we can find any discrepancies in the feature distributions among different genres. We are going to plot a scatter plot where the <em class="calibre13">x</em> axis is the index of each feature and the <em class="calibre13">y</em> axis is the values for the given feature. Let's look at these plots first, as it will be easier to understand with some visuals.</p>
<p class="calibre2"><span class="calibre5">The following plots show the distributions of kurtosis for four different genres:</span></p>
<div class="mce-root"><img class="alignnone36" src="../images/00102.jpeg"/></div>
<p class="calibre2"><span class="calibre5">As briefly mentioned previously, the <em class="calibre13">x</em> axis refers to the index of each feature. Since we have 20 individual features for kurtosis of MFCCs, the x-values span from 1 to 20. On the other hand, the <em class="calibre13">y</em> axis shows the distributions of the given feature. As you can see from this chart, there are some differences in the feature distributions among different genres, which will help our ML models to learn how to correctly predict the genre of a given song.</span></p>
<p class="calibre2"><span class="calibre5">The following plots show the distributions of skewness for four different genres:</span></p>
<div class="mce-root"><img class="alignnone37" src="../images/00103.jpeg"/></div>
<p class="calibre2">Lastly, t<span class="calibre5">he following plots show the mean distributions for four different genres:</span></p>
<div class="mce-root"><img class="alignnone38" src="../images/00104.jpeg"/></div>
<p class="calibre2"><span class="calibre5">The distributions of the mean values for each feature seem more similar among different genres, when compared to the kurtosis and skewness.</span></p>
<p class="calibre2">In order to create these charts, we have used the <kbd class="calibre12">ScatterplotBox</kbd> class. The following code shows how we created the previous charts:</p>
<pre class="calibre19">string[] attributes = new string[] { "kurtosis", "min", "max", "mean", "median", "skew", "std" };<br class="title-page-name"/>foreach (string attribute in attributes)<br class="title-page-name"/>{<br class="title-page-name"/>    string[] featureColumns = featuresDF.ColumnKeys.Where(x =&gt; x.Contains(attribute)).ToArray();<br class="title-page-name"/>    foreach (string genre in genreCount.GetColumn&lt;string&gt;("genre_top").Values)<br class="title-page-name"/>    {<br class="title-page-name"/>        var genreDF = featuresDF.Rows[<br class="title-page-name"/>            featuresDF.GetColumn&lt;string&gt;("genre_top").Where(x =&gt; x.Value == genre).Keys<br class="title-page-name"/>        ].Columns[featureColumns];<br class="title-page-name"/><br class="title-page-name"/>        ScatterplotBox.Show(<br class="title-page-name"/>            BuildXYPairs(<br class="title-page-name"/>                genreDF.Columns[featureColumns].ToArray2D&lt;double&gt;(),<br class="title-page-name"/>                genreDF.RowCount,<br class="title-page-name"/>                genreDF.ColumnCount<br class="title-page-name"/>            )<br class="title-page-name"/>        ).SetTitle(String.Format("{0}-{1}", genre, attribute));<br class="title-page-name"/>    }<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we start iterating through different statistical distributions (<kbd class="calibre12">kurtosis</kbd>, <kbd class="calibre12">min</kbd>, <kbd class="calibre12">max</kbd>, and so on) from line 2 and, for each of those statistical distributions, we sub-select the columns that we are interested in from <kbd class="calibre12">featuresDF</kbd> in line 7. Then, we wrote and used a helper function that builds an array of x-y pairs for the scatter plot and display it using the <kbd class="calibre12">Show</kbd> method of the <kbd class="calibre12">ScatterplotBox</kbd> class.</span></p>
<p class="calibre2">The code for the helper function that builds x-y pairs for scatter plots is as follows:</p>
<pre class="calibre19">private static double[][] BuildXYPairs(double[,] ary2D, int rowCount, int columnCount)<br class="title-page-name"/>{<br class="title-page-name"/>    double[][] ary = new double[rowCount*columnCount][];<br class="title-page-name"/>    for (int i = 0; i &lt; rowCount; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        for (int j = 0; j &lt; columnCount; j++)<br class="title-page-name"/>        {<br class="title-page-name"/>            ary[i * columnCount + j] = new double[2];<br class="title-page-name"/>            ary[i * columnCount + j][0] = j + 1;<br class="title-page-name"/>            ary[i * columnCount + j][1] = ary2D[i, j];<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>    return ary;<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see from this code, this method takes the index of the feature as an x value and takes the value of the feature as a y value.</p>
<p class="calibre2">The full code for this data analysis step can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs</a>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">ML models for music genre classification</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We will now start building ML models for music genre classification. In this project, the output of our ML models will take a slightly different form. Unlike other supervised learning models that we have built, we want our models to output the likelihoods or probabilities for each genre for a given song. So, instead of the model output being one value, we would like our models to output eight values, where each value will represent the probability of the given song belonging to each of the eight genres—electronic, experimental, folk, hip-hop, instrumental, international, pop, and rock. In order to achieve this, we will be using the <kbd class="calibre12">Probabilities</kbd> method from each of the model classes, on top of the <kbd class="calibre12">Decide</kbd> method that we have been using so far.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Logistic regression</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The first model we are going to experiment with is logistic regression. The following code shows how we built a logistic regression classifier with an 80/20 split for training and testing sets:</span></p>
<pre class="calibre19">// 1. Train a LogisticRegression Classifier<br class="title-page-name"/>Console.WriteLine("\n---- Logistic Regression Classifier ----\n");<br class="title-page-name"/>var logitSplitSet = new SplitSetValidation&lt;MultinomialLogisticRegression, double[]&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Learner = (s) =&gt; new MultinomialLogisticLearning&lt;GradientDescent&gt;()<br class="title-page-name"/>    {<br class="title-page-name"/>        MiniBatchSize = 500<br class="title-page-name"/>    },<br class="title-page-name"/><br class="title-page-name"/>    Loss = (expected, actual, p) =&gt; new ZeroOneLoss(expected).Loss(actual),<br class="title-page-name"/><br class="title-page-name"/>    Stratify = false,<br class="title-page-name"/><br class="title-page-name"/>    TrainingSetProportion = 0.8,<br class="title-page-name"/><br class="title-page-name"/>    ValidationSetProportion = 0.2,<br class="title-page-name"/><br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>var logitResult = logitSplitSet.Learn(input, output);<br class="title-page-name"/><br class="title-page-name"/>var logitTrainedModel = logitResult.Model;<br class="title-page-name"/><br class="title-page-name"/>// Store train &amp; test set indexes to train other classifiers on the same train set<br class="title-page-name"/>// and test on the same validation set<br class="title-page-name"/>int[] trainSetIDX = logitSplitSet.IndicesTrainingSet;<br class="title-page-name"/>int[] testSetIDX = logitSplitSet.IndicesValidationSet;</pre>
<p class="calibre2">As you should be familiar with it already, we used <kbd class="calibre12">SplitSetValidation</kbd> to split our sample set into train and test sets. We are using 80% of our sample set for training and the other 20% for testing and evaluating our models. We are using <kbd class="calibre12">MultinomialLogisticRegression</kbd> as our model for the multi-class classifier and <kbd class="calibre12">MultinomialLogisticLearning</kbd> with <kbd class="calibre12">GradientDescent</kbd> as our learning algorithm. Similar to the previous chapters, we are using <kbd class="calibre12">ZeroOneLoss</kbd> for our <kbd class="calibre12">Loss</kbd> function for the classifier.</p>
<p class="calibre2">As you can see at the base of this code, we are storing the trained logistic regression classifier model into a separate variable, <kbd class="calibre12">logitTrainedModel</kbd>, and also the indexes of the train and test sets for use in training and testing other learning algorithms. We do this so that we can do head-to-head comparisons of model performance among different ML models.</p>
<p class="calibre2">The code to do in-sample and out-of-sample predictions using this trained logistic regression model is as follows:</p>
<pre class="calibre19">// Get in-sample &amp; out-of-sample predictions and prediction probabilities for each class<br class="title-page-name"/>double[][] trainProbabilities = new double[trainSetIDX.Length][];<br class="title-page-name"/>int[] logitTrainPreds = new int[trainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; trainSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    logitTrainPreds[i] = logitTrainedModel.Decide(input[trainSetIDX[i]]);<br class="title-page-name"/>    trainProbabilities[i] = logitTrainedModel.Probabilities(input[trainSetIDX[i]]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>double[][] testProbabilities = new double[testSetIDX.Length][];<br class="title-page-name"/>int[] logitTestPreds = new int[testSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; testSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    logitTestPreds[i] = logitTrainedModel.Decide(input[testSetIDX[i]]);<br class="title-page-name"/>    testProbabilities[i] = logitTrainedModel.Probabilities(input[testSetIDX[i]]);<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As briefly mentioned before, we are using the <kbd class="calibre12">Probabilities</kbd> method from the <kbd class="calibre12">MultinomialLogisticRegression</kbd> model, which outputs an array of probabilities, and each index represents the probability of the given song being the corresponding music genre. The following code shows how we encoded each of the genres:</span></p>
<pre class="calibre19">IDictionary&lt;string, int&gt; targetVarCodes = new Dictionary&lt;string, int&gt;<br class="title-page-name"/>{<br class="title-page-name"/>    { "Electronic", 0 },<br class="title-page-name"/>    { "Experimental", 1 },<br class="title-page-name"/>    { "Folk", 2 },<br class="title-page-name"/>    { "Hip-Hop", 3 },<br class="title-page-name"/>    { "Instrumental", 4 },<br class="title-page-name"/>    { "International", 5 },<br class="title-page-name"/>    { "Pop", 6 },<br class="title-page-name"/>    { "Rock", 7 }<br class="title-page-name"/>};<br class="title-page-name"/>featuresDF.AddColumn("target", featuresDF.GetColumn&lt;string&gt;("genre_top").Select(x =&gt; targetVarCodes[x.Value]));</pre>
<p class="calibre2"><span class="calibre5">Let's try training another ML model using the same indexes for train and test sets that we used for the logistic regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SVM with the Gaussian kernel</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Using the following code, you can train a multi-class SVM model:</span></p>
<pre class="calibre19">// 2. Train a Gaussian SVM Classifier<br class="title-page-name"/>Console.WriteLine("\n---- Gaussian SVM Classifier ----\n");<br class="title-page-name"/>var teacher = new MulticlassSupportVectorLearning&lt;Gaussian&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Learner = (param) =&gt; new SequentialMinimalOptimization&lt;Gaussian&gt;()<br class="title-page-name"/>    {<br class="title-page-name"/>        Epsilon = 2,<br class="title-page-name"/>        Tolerance = 1e-2,<br class="title-page-name"/>        Complexity = 1000,<br class="title-page-name"/>        UseKernelEstimation = true<br class="title-page-name"/>    }<br class="title-page-name"/>};<br class="title-page-name"/>// Train SVM model using the same train set that was used for Logistic Regression Classifier<br class="title-page-name"/>var svmTrainedModel = teacher.Learn(<br class="title-page-name"/>    input.Where((x,i) =&gt; trainSetIDX.Contains(i)).ToArray(),<br class="title-page-name"/>    output.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray()<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, there is one minor difference between the SVM model that we built previously. We are using <kbd class="calibre12">MulticlassSupportVectorLearning</kbd> instead of <kbd class="calibre12">LinearRegressionNewtonMethod</kbd> or <kbd class="calibre12">FanChenLinSupportVectorRegression</kbd>, which we used in <a target="_blank" href="part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 5</a>, <em class="calibre13">Fair Value of House and Property</em>. This is because we now have a multi-class classification problem and need to use a different learning algorithm for such SVM models. As we discussed in another chapter previously, the hyper-parameters, such as <kbd class="calibre12">Epsilon</kbd>, <kbd class="calibre12">Tolerance</kbd>, and <kbd class="calibre12">Complexity</kbd>, can be tuned and you should experiment with other values for better-performing models.</span></p>
<p class="calibre2">One thing to note here is that when we are training our SVM model, we use the same train set that we used for building the logistic regression model. As you can see at the base of the code, we sub-select records with the same indexes in the train set that we used previously for the logistic regression model. This is to make sure that we can correctly do a head-to-head comparison of the performance of this SVM model against that of the logistic regression model.</p>
<p class="calibre2">Similar to the case of the previous logistic regression model, we are using the following code for in-sample and out-of-sample predictions, using the trained SVM model:</p>
<pre class="calibre19">// Get in-sample &amp; out-of-sample predictions and prediction probabilities for each class<br class="title-page-name"/>double[][] svmTrainProbabilities = new double[trainSetIDX.Length][];<br class="title-page-name"/>int[] svmTrainPreds = new int[trainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; trainSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    svmTrainPreds[i] = svmTrainedModel.Decide(input[trainSetIDX[i]]);<br class="title-page-name"/>    svmTrainProbabilities[i] = svmTrainedModel.Probabilities(input[trainSetIDX[i]]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>double[][] svmTestProbabilities = new double[testSetIDX.Length][];<br class="title-page-name"/>int[] svmTestPreds = new int[testSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; testSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    svmTestPreds[i] = svmTrainedModel.Decide(input[testSetIDX[i]]);<br class="title-page-name"/>    svmTestProbabilities[i] = svmTrainedModel.Probabilities(input[testSetIDX[i]]);<br class="title-page-name"/>}</pre>
<p class="calibre2">The <kbd class="calibre12">MulticlassSupportVectorMachine</kbd> class also provides the <kbd class="calibre12">Probabilities</kbd> method, with which we can get the likelihoods of a song belonging to each of the eight genres. We store these probability outputs into separate variables, <kbd class="calibre12">svmTrainProbabilities</kbd> and <kbd class="calibre12">svmTestProbabilities</kbd>, for our future model evaluation and for ensembling the models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Naive Bayes</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We are going to build one more machine learning model for music genre classification. We are going to train a Naive Bayes classifier. The following code shows how you can build a Naive Bayes classifier for input with continuous values:</span></p>
<pre class="calibre19">// 3. Train a NaiveBayes Classifier<br class="title-page-name"/>Console.WriteLine("\n---- NaiveBayes Classifier ----\n");<br class="title-page-name"/>var nbTeacher = new NaiveBayesLearning&lt;NormalDistribution&gt;();<br class="title-page-name"/><br class="title-page-name"/>var nbTrainedModel = nbTeacher.Learn(<br class="title-page-name"/>    input.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray(),<br class="title-page-name"/>    output.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray()<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are using <kbd class="calibre12">NormalDistribution</kbd> as a distribution for <kbd class="calibre12">NaiveBayesLearning</kbd>. Unlike in the previous chapters, where we had word counts as features of our Naive Bayes classifiers, we have continuous values for our audio features. In this case, we need to build a Gaussian Naive Bayes classifier. </span>Similar to when we were building an SVM model, we are training our Naive Bayes classifier with the same train set that we used for the logistic regression model.</p>
<p class="calibre2"><span class="calibre5">The following code shows how we can get the probability output for in-sample and out-of-sample predictions using the trained</span> Naive Bayes <span class="calibre5">classifier:</span></p>
<pre class="calibre19">// Get in-sample &amp; out-of-sample predictions and prediction probabilities for each class<br class="title-page-name"/>double[][] nbTrainProbabilities = new double[trainSetIDX.Length][];<br class="title-page-name"/>int[] nbTrainPreds = new int[trainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; trainSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    nbTrainProbabilities[i] = nbTrainedModel.Probabilities(input[trainSetIDX[i]]);<br class="title-page-name"/>    nbTrainPreds[i] = nbTrainedModel.Decide(input[trainSetIDX[i]]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>double[][] nbTestProbabilities = new double[testSetIDX.Length][];<br class="title-page-name"/>int[] nbTestPreds = new int[testSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; testSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    nbTestProbabilities[i] = nbTrainedModel.Probabilities(input[testSetIDX[i]]);<br class="title-page-name"/>    nbTestPreds[i] = nbTrainedModel.Decide(input[testSetIDX[i]]);<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">Similar to the <kbd class="calibre12">MulticlassSupportVectorMachine</kbd> and <kbd class="calibre12">MultinomialLogisticRegression</kbd> classes, the <kbd class="calibre12">NaiveBayes</kbd> model also provides the <kbd class="calibre12">Probabilities</kbd> method.  As you can see from the code, we store the predicted probabilities for both in-sample and out-of-sample records into two separate variables, <kbd class="calibre12">nbTrainProbabilities</kbd> and <kbd class="calibre12">nbTestProbabilities</kbd>.</span></p>
<p class="calibre2">In the following section, we will take a look at how we can combine and ensemble these models we have built so far. The full code for building ML models can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Ensembling base learning models</h1>
                
            
            <article>
                
<p class="calibre2">Ensemble learning is where you combine trained models together in order to improve their predictive power. The random forest classifier that we built in previous chapters is an example of ensemble learning. It builds a forest of decision trees, where the individual trees are trained with a portion of the samples and features in the sample set. This method of ensemble learning is called <strong class="calibre4">bagging</strong>. The ensemble method that we are going to use in this chapter is <strong class="calibre4">stacking</strong>. Stacking is when you build a new ML model using the outputs of the other models, which are called <strong class="calibre4">base learning models</strong>. </p>
<p class="calibre2">In this project, we are going to built a new Naive Bayes classifier model on top of the predicted probability output from those logistic regression, SVM, and Naive Bayes models that we built in the previous section. The first thing we need to do to build a new model with the probability output of the base models is to build the training input. The following code shows how we combined all the outputs from the base models:</p>
<pre class="calibre19">// 4. Ensembling Base Models<br class="title-page-name"/>Console.WriteLine("\n-- Building Meta Model --");<br class="title-page-name"/>double[][] combinedTrainProbabilities = new double[trainSetIDX.Length][];<br class="title-page-name"/>for (int i = 0; i &lt; trainSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    List&lt;double&gt; combined = trainProbabilities[i]<br class="title-page-name"/>        .Concat(svmTrainProbabilities[i])<br class="title-page-name"/>        .Concat(nbTrainProbabilities[i])<br class="title-page-name"/>        .ToList();<br class="title-page-name"/>    combined.Add(logitTrainPreds[i]);<br class="title-page-name"/>    combined.Add(svmTrainPreds[i]);<br class="title-page-name"/>    combined.Add(nbTrainPreds[i]);<br class="title-page-name"/><br class="title-page-name"/>    combinedTrainProbabilities[i] = combined.ToArray();<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>double[][] combinedTestProbabilities = new double[testSetIDX.Length][];<br class="title-page-name"/>for (int i = 0; i &lt; testSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    List&lt;double&gt; combined = testProbabilities[i]<br class="title-page-name"/>        .Concat(svmTestProbabilities[i])<br class="title-page-name"/>        .Concat(nbTestProbabilities[i])<br class="title-page-name"/>        .ToList();<br class="title-page-name"/>    combined.Add(logitTestPreds[i]);<br class="title-page-name"/>    combined.Add(svmTestPreds[i]);<br class="title-page-name"/>    combined.Add(nbTestPreds[i]);<br class="title-page-name"/><br class="title-page-name"/>    combinedTestProbabilities[i] = combined.ToArray();<br class="title-page-name"/>}<br class="title-page-name"/>Console.WriteLine("\n* input shape: ({0}, {1})\n", combinedTestProbabilities.Length, combinedTestProbabilities[0].Length);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code, we are concatenating the predicted probabilities from all three models that we built so far. Using this probability output data as input, we are going to build a new meta-model, using the Naive Bayes learning algorithm. The following code is how we trained this meta-model:</span></p>
<pre class="calibre19">// Build meta-model using NaiveBayes Learning Algorithm<br class="title-page-name"/>var metaModelTeacher = new NaiveBayesLearning&lt;NormalDistribution&gt;();<br class="title-page-name"/>var metamodel = metaModelTeacher.Learn(<br class="title-page-name"/>    combinedTrainProbabilities, <br class="title-page-name"/>    output.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray()<br class="title-page-name"/>);</pre>
<p class="calibre2"><span class="calibre5">From this code, you can see that we are still using <kbd class="calibre12">NormalDistribution</kbd>, as the input is a set of continuous values. Then, we train this new Naive Bayes classifier with the combined probability output of the base learning models that we trained before. Similar to the previous steps, we get the prediction output from this meta-model by using the <kbd class="calibre12">Probabilities</kbd> method and store these results into separate variables. The code to get the prediction output for the train and test sets using this new meta-model is as follows:</span></p>
<pre class="calibre19">// Get in-sample &amp; out-of-sample predictions and prediction probabilities for each class<br class="title-page-name"/>double[][] metaTrainProbabilities = new double[trainSetIDX.Length][];<br class="title-page-name"/>int[] metamodelTrainPreds = new int[trainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; trainSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    metaTrainProbabilities[i] = metamodel.Probabilities(combinedTrainProbabilities[i]);<br class="title-page-name"/>    metamodelTrainPreds[i] = metamodel.Decide(combinedTrainProbabilities[i]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>double[][] metaTestProbabilities = new double[testSetIDX.Length][];<br class="title-page-name"/>int[] metamodelTestPreds = new int[testSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; testSetIDX.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    metaTestProbabilities[i] = metamodel.Probabilities(combinedTestProbabilities[i]);<br class="title-page-name"/>    metamodelTestPreds[i] = metamodel.Decide(combinedTestProbabilities[i]);<br class="title-page-name"/>}</pre>
<p class="calibre2">Now that we have all the models built, let's start looking at the performances of these models. In the following section, we will evaluate the performance of base models as well as the meta-model we just built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Evaluating recommendation/rank-ordering models</h1>
                
            
            <article>
                
<p class="calibre2">Evaluating recommendation models that rank-order the outcomes is quite different from evaluating classification models. Aside from whether the model prediction is right or wrong, we also care about in which rank the correct outcome comes in the recommendation models. In other words, a model that predicted the correct outcome to be the second from the top is a better model than a model that predicted the correct outcome to be fourth or fifth from the top. For example, when you search for something on a search engine, getting the most appropriate document on the top of the first page is great, but it is still OK to have that document as the second or third link on the first page, as long as it does not appear at the bottom of the first page or the next page. We are going to discuss some ways to evaluate such recommendation and ranking models in<span class="calibre5"><span class="calibre5"> the following</span></span> sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Prediction accuracy</h1>
                
            
            <article>
                
<p class="calibre2">The first and the simplest metric to look at is accuracy. For the first logistic regression model we built, we can use the following code to get the accuracy:</p>
<pre class="calibre19">Console.WriteLine(String.Format("train accuracy: {0:0.0000}", 1-logitResult.Training.Value));<br class="title-page-name"/>Console.WriteLine(String.Format("validation accuracy: {0:0.0000}", 1-logitResult.Validation.Value));</pre>
<p class="calibre2">For the following models, SVM and Naive Bayes classifiers, we can use the following code to compute the accuracy for the train and test set predictions:</p>
<pre class="calibre19">Console.WriteLine(<br class="title-page-name"/>    String.Format(<br class="title-page-name"/>        "train accuracy: {0:0.0000}",<br class="title-page-name"/>        1 - new ZeroOneLoss(output.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray()).Loss(nbTrainPreds)<br class="title-page-name"/>    )<br class="title-page-name"/>);<br class="title-page-name"/>Console.WriteLine(<br class="title-page-name"/>    String.Format(<br class="title-page-name"/>        "validation accuracy: {0:0.0000}",<br class="title-page-name"/>        1 - new ZeroOneLoss(output.Where((x, i) =&gt; testSetIDX.Contains(i)).ToArray()).Loss(nbTestPreds)<br class="title-page-name"/>    )<br class="title-page-name"/>);</pre>
<p class="calibre2">We used the <kbd class="calibre12">SplitSetValidation</kbd> class for the first logistic regression model, so it computes the accuracy while the model is being fit. However, for the subsequent models, we trained SVM and Naive Bayes models individually, so we need to use the <kbd class="calibre12">ZeroOneLoss</kbd> class to compute accuracies.</p>
<p class="calibre2">When you run this code, you will see the accuracy output for the logistic regression model as follows:</p>
<div class="mce-root"><img class="alignnone39" src="../images/00105.gif"/></div>
<p class="calibre2">For the Naive Bayes model, the accuracy results look as follows:</p>
<div class="mce-root"><img class="alignnone40" src="../images/00106.gif"/></div>
<p class="calibre2">And for the SVM model, the output looks as follows:</p>
<div class="mce-root"><img class="alignnone41" src="../images/00107.gif"/></div>
<p class="calibre2">Lastly, the accuracy results for the meta-model look as follows:</p>
<div class="mce-root"><img class="alignnone42" src="../images/00108.gif"/></div>
<p class="calibre2">From these results, we can see that the Naive Bayes classifier performed the best by predicting the correct genre for about 42% of the time. The logistic regression model comes in as the second best model with the second highest accuracy and the SVM model comes in as the worst model in terms of prediction accuracy. Interestingly, the meta-model that we built with the output from the other three models did not perform so well. It did better than the SVM model, but performed worse than the Naive Bayes and logistic regression classifiers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Confusion matrices</h1>
                
            
            <article>
                
<p class="calibre2">The next thing we are going to look at is confusion matrices. In the case of binary classification in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em>, we explored a case where the confusion matrix was a 2 x 2 matrix. However, in this project, our models have <kbd class="calibre12">8</kbd> outcomes and the shape of the confusion matrix will be 8 x 8. Let's first look at how we can build such a confusion matrix:</p>
<pre class="calibre19">// Build confusion matrix<br class="title-page-name"/>string[] confMatrix = BuildConfusionMatrix(<br class="title-page-name"/>    output.Where((x, i) =&gt; testSetIDX.Contains(i)).ToArray(), logitTestPreds, 8<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>System.IO.File.WriteAllLines(Path.Combine(dataDirPath, "logit-conf-matrix.csv"), confMatrix);</pre>
<p class="calibre2">The code for the helper function, <kbd class="calibre12">BuildConfusionMatrix</kbd>, looks as follows:</p>
<pre class="calibre19">private static string[] BuildConfusionMatrix(int[] actual, int[] preds, int numClass)<br class="title-page-name"/>{<br class="title-page-name"/>    int[][] matrix = new int[numClass][];<br class="title-page-name"/>    for(int i = 0; i &lt; numClass; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        matrix[i] = new int[numClass];<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    for(int i = 0; i &lt; actual.Length; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        matrix[actual[i]][preds[i]] += 1;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    string[] lines = new string[numClass];<br class="title-page-name"/>    for(int i = 0; i &lt; matrix.Length; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        lines[i] = string.Join(",", matrix[i]);<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    return lines;<br class="title-page-name"/>}</pre>
<p class="calibre2">Once you run this code, you are going to get an 8 x 8 matrix, where the rows are the actual and observed genres and the columns are the predicted genres from the models. The following is the confusion matrix for our logistic regression model:</p>
<div class="mce-root"><img class="alignnone43" src="../images/00109.gif"/></div>
<p class="calibre2"><span class="calibre5">The numbers in bold represent the number of records that the model predicted correctly. For example, this logistic regression model predicted <strong class="calibre4">79</strong> songs correctly as <strong class="calibre4">Electronic</strong> and <strong class="calibre4">33</strong> songs were predicted as <strong class="calibre4">Electronic</strong>, where they were actually <strong class="calibre4">Experimental</strong>. One thing noticeable here is that this logistic regression model did not do so well for predicting Pop songs. It only had one prediction for Pop, but that prediction was wrong and the song was actually a <strong class="calibre4">Hip-Hop</strong> song. Let's now look at the confusion matrix of the Naive Bayes classifier's predictions:</span></p>
<div class="mce-root"><img class="alignnone44" src="../images/00110.gif"/></div>
<p class="calibre2"><span class="calibre5">As expected from the accuracy results, the confusion matrix looks better than that for logistic regression. A higher proportion of predictions in each category were right, when compared to the logistic regression classifier. The Naive Bayes classifier seemed to do much better for <strong class="calibre4">Pop</strong> songs as well.  </span></p>
<p class="calibre2">The following is the confusion matrix for the SVM classifier:</p>
<div class="mce-root"><img class="alignnone45" src="../images/00111.gif"/></div>
<p class="calibre2"><span class="calibre5">As expected, the prediction results are not good. The SVM model predicted 100% of the records as <strong class="calibre4">Electronic</strong>. Lastly, let's look at how the meta-model did:</span></p>
<div class="mce-root"><img class="alignnone46" src="../images/00112.gif"/></div>
<p class="calibre2"><span class="calibre5">This confusion matrix looks slightly better than that of the SVM model. However, the majority of the predictions were either <strong class="calibre4">Instrumental</strong> or <strong class="calibre4">International</strong> and only a handful of records were predicted as other genres.</span></p>
<p class="calibre2">Looking at the confusion matrix is a good way to check misclassifications by models and find out the weaknesses and strengths of the models. These results are well aligned with the accuracy results, where the Naive Bayes classifier outperformed all the other models and the meta-model did not do so well, although it is not the worst among the four models that we have built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Mean Reciprocal Rank </h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The next evaluation metric we are going to look at is</span> MRR. MRR<span class="calibre5"> can be used where a model produces a list of outcomes and it measures the overall quality of the rankings. Let's first look at the equation:</span></p>
<div class="mce-root"><img class="fm-editor-equation1" src="../images/00113.jpeg"/></div>
<p class="calibre2">As you can see, it is an average of the sum of the inverse of the ranks. Consider the following example:</p>
<div class="mce-root"><img class="alignnone47" src="../images/00114.gif"/></div>
<p class="calibre2">In the first example, the correct genre was the second in rank, so the reciprocal rank is <strong class="calibre4">1/2</strong>.  The second example's correct genre was the first in rank, so the reciprocal rank is <strong class="calibre4">1/1</strong>, which is <strong class="calibre4">1</strong>. Following this process, we can get the reciprocal ranks for all the records and the final MRR value is simply the average of those reciprocal ranks. This tells us the general quality of the rankings. In this example, the <strong class="calibre4">MRR</strong> is <strong class="calibre4">0.57</strong>, which is above 1/2. So, this MRR number suggests that, on average, the correct genres come up within the top two predicted genres by the model.</p>
<p class="calibre2">In order to compute the MRR for our models, we first need to transform the probability output into rankings and then compute the MRR from this transformed model output. The following code snippet shows how we computed the MRR for our models:</p>
<pre class="calibre19">// Calculate evaluation metrics<br class="title-page-name"/>int[][] logitTrainPredRanks = GetPredictionRanks(trainProbabilities);<br class="title-page-name"/>int[][] logitTestPredRanks = GetPredictionRanks(testProbabilities);<br class="title-page-name"/><br class="title-page-name"/>double logitTrainMRRScore = ComputeMeanReciprocalRank(<br class="title-page-name"/>    logitTrainPredRanks,<br class="title-page-name"/>    output.Where((x, i) =&gt; trainSetIDX.Contains(i)).ToArray()<br class="title-page-name"/>);<br class="title-page-name"/>double logitTestMRRScore = ComputeMeanReciprocalRank(<br class="title-page-name"/>    logitTestPredRanks,<br class="title-page-name"/>    output.Where((x, i) =&gt; testSetIDX.Contains(i)).ToArray()<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine("\n---- Logistic Regression Classifier ----\n");<br class="title-page-name"/>Console.WriteLine(String.Format("train MRR score: {0:0.0000}", logitTrainMRRScore));<br class="title-page-name"/>Console.WriteLine(String.Format("validation MRR score: {0:0.0000}", logitTestMRRScore));</pre>
<p class="calibre2">This code uses two helper functions, <kbd class="calibre12">GetPredictionRanks</kbd> and <kbd class="calibre12">ComputeMeanReciprocalRank</kbd>. The <kbd class="calibre12">GetPredictionRanks</kbd> method transforms the probability output of a model into rankings and the <kbd class="calibre12">ComputeMeanReciprocalRank</kbd> method calculates the MRR from the rankings. The helper function, <kbd class="calibre12">GetPredictionRanks</kbd>, looks as follows:</p>
<pre class="calibre19">private static int[][] GetPredictionRanks(double[][] predProbabilities)<br class="title-page-name"/>{<br class="title-page-name"/>    int[][] rankOrdered = new int[predProbabilities.Length][];<br class="title-page-name"/><br class="title-page-name"/>    for(int i = 0; i&lt; predProbabilities.Length; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        rankOrdered[i] = Matrix.ArgSort&lt;double&gt;(predProbabilities[i]).Reversed();<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    return rankOrdered;<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">We are using the <kbd class="calibre12">Matrix.ArgSort</kbd> method from the <kbd class="calibre12">Accord.Math</kbd> package to rank-order the genres for each record. <kbd class="calibre12">Matrix.ArgSort</kbd> returns the indexes of the genres after sorting them by probability in ascending order. However, we want them to be sorted in descending order so that the most likely genre comes up as the first in rank. This is why we reverse the order of the sorted indexes using the <kbd class="calibre12">Reversed</kbd> method.</span></p>
<p class="calibre2">The helper function, <kbd class="calibre12">ComputeMeanReciprocalRank</kbd>, looks as follows:</p>
<pre class="calibre19">private static double ComputeMeanReciprocalRank(int[][] rankOrderedPreds, int[] actualClasses)<br class="title-page-name"/>{<br class="title-page-name"/>    int num = rankOrderedPreds.Length;<br class="title-page-name"/>    double reciprocalSum = 0.0;<br class="title-page-name"/><br class="title-page-name"/>    for(int i = 0; i &lt; num; i++)<br class="title-page-name"/>    {<br class="title-page-name"/>        int predRank = 0;<br class="title-page-name"/>        for(int j = 0; j &lt; rankOrderedPreds[i].Length; j++)<br class="title-page-name"/>        {<br class="title-page-name"/>            if(rankOrderedPreds[i][j] == actualClasses[i])<br class="title-page-name"/>            {<br class="title-page-name"/>                predRank = j + 1;<br class="title-page-name"/>            }<br class="title-page-name"/>        }<br class="title-page-name"/>        reciprocalSum += 1.0 / predRank;<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    return reciprocalSum / num;<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">This is our implementation of the equation for the MRR calculation that we discussed previously. This method iterates through each record and gets the rank of the correct genre.  Then, it reciprocates the rank, sums all of the reciprocals, and finally divides this sum by the number of records to get the MRR number.</span></p>
<p class="calibre2">Let's start looking at the MRR scores for the models that we have built so far. The following output shows the MRR scores for the <kbd class="calibre12">Logistic Regression Classifier</kbd>:</p>
<div class="mce-root"><img class="alignnone48" src="../images/00115.gif"/></div>
<p class="calibre2">The in-sample and out-of-sample MRR scores for the Naive Bayes classifier look as follows:</p>
<div class="mce-root"><img class="alignnone49" src="../images/00116.gif"/></div>
<p class="calibre2">And the results for the SVM classifier are as follows:</p>
<div class="mce-root"><img class="alignnone50" src="../images/00117.gif"/></div>
<p class="calibre2">Lastly, the MRR scores for the meta-model look as follows:</p>
<div class="mce-root"><img class="alignnone51" src="../images/00118.gif"/></div>
<p class="calibre2"><span class="calibre5">From these outputs, we can see that the Naive Bayes classifier has the best MRR scores at around <kbd class="calibre12">0.61</kbd>, while the SVM classifier has the worst MRR scores at around <kbd class="calibre12">0.33</kbd>. The meta-model has MRR scores at around <kbd class="calibre12">0.4</kbd>. This is aligned with the results we have found from looking at the prediction accuracy and confusion matrix in the previous steps.  From these MRR scores, we can see that the correct genres generally fall within the top two ranks for the Naive Bayes classifier. On the other hand, the correct genres typically come up as the third from the top for the SVM classifier and within the top three for the meta-model. </span>As you can see from these cases, we can understand the overall quality of the rankings by looking at the MRR measures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we built our first recommendation model to rank-order the likelihood of each of the outcomes. We started this chapter by defining the problems that we were going to solve and the modeling and the evaluation approaches that we were going to use. Then, we looked at the distributions of the variables in our sample set. First, we looked at how well the target variables were distributed among different classes or genres and noticed that it was a well-balanced sample set with no one genre taking up the majority of the samples in our dataset. Then, we looked at the distributions of the audio features. In this project, we focused mainly on MFCCs and their statistical distributions, such as kurtosis, skewness, min, and max. By looking at the quartiles and the scatter plots of these features, we confirmed that the feature distributions differed among the music genres. </p>
<p class="calibre2">During our model-building step, we experimented with three learning algorithms: logistic regression, SVM, and Naive Bayes. Since we were building multi-class classification models, we had to use different learning algorithms from previous chapters. We learned how to use the <kbd class="calibre12">MultinomialLogisticRegression</kbd> and <kbd class="calibre12">MulticlassSupportVectorMachine</kbd> classes in the Accord.NET framework, as well as when to use <kbd class="calibre12">NormalDistribution</kbd> for <kbd class="calibre12">NaiveBayesLearning</kbd>. We then discussed how we could build a meta-model that ensembled the prediction results from the base learning models to improve the predictive power of the ML models. Lastly, we discussed how evaluating ranking models differed from other classification models and looked at the accuracy, confusion matrix, and MRR metrics to evaluate our ML models.</p>
<p class="calibre2">In the next chapter, we are going to use a hand-written digit image dataset to build a classifier that classifies each image into the corresponding digit. We are going to discuss some techniques to reduce the dimensions of the feature set and how to apply them to the image dataset. We will also discuss how to build a neural network<span class="calibre5"> in C# using the Accord.NET framework, which</span> is the backbone of deep learning.</p>
<p class="calibre2"> </p>


            </article>

            
        </section>
    </body></html>