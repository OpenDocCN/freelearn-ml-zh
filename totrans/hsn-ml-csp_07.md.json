["```py\nList<HaarCascadeStage> stages = new List<HaarCascadeStage>();\n List<HaarFeatureNode[]> nodes;\n HaarCascadeStage stage;\n stage = new HaarCascadeStage(0.822689414024353);\n nodes = new List<HaarFeatureNode[]>();\n nodes.Add(new[] { new HaarFeatureNode(0.004014195874333382, \n   0.0337941907346249, 0.8378106951713562, \n   new int[] { 3, 7, 14, 4, -1 },\n   new int[] { 3, 9, 14, 2, 2 }) });\n nodes.Add(new[] { new HaarFeatureNode(0.0151513395830989,\n   0.1514132022857666, 0.7488812208175659, \n   new int[] { 1, 2, 18, 4, -1 }, \n   new int[] { 7, 2, 6, 4, 3 }) });\n nodes.Add(new[] { new HaarFeatureNode(0.004210993181914091,\n   0.0900492817163467, 0.6374819874763489, \n   new int[] { 1, 7, 15, 9, -1 }, \n   new int[] { 1, 10, 15, 3, 3 })\n  });\n stage.Trees = nodes.ToArray(); stages.Add(stage);\n```", "```py\nHaarCascade cascade = new FaceHaarCascade();\ndetector = new HaarObjectDetector(cascade, 25, \n  ObjectDetectorSearchMode.Single, 1.2f,\n  ObjectDetectorScalingMode.GreaterToSmaller);\n```", "```py\nVideoCaptureDevice videoSource = new \n  VideoCaptureDevice(form.VideoDevice);\nforeach (var cap in device.VideoCapabilities)\n{\n\n  if (cap.FrameSize.Height == 240)\n  return cap;\n  if (cap.FrameSize.Width == 320)\n  return cap;\n}\nreturn device.VideoCapabilities.Last();\n```", "```py\nthis.videoSourcePlayer.NewFrameReceived += new\n  Accord.Video.NewFrameEventHandler\n  (this.videoSourcePlayer_NewFrame);\n```", "```py\nResizeNearestNeighbor resize = new ResizeNearestNeighbor(160, 120);\nUnmanagedImage downsample = resize.Apply(im);\n```", "```py\nRectangle[] regions = detector?.ProcessFrame(downsample);\nif (regions != null && regions.Length > 0)\n{\n  tracker?.Reset();\n  // Will track the first face found\n  Rectangle face = regions[0];\n  // Reduce the face size to avoid tracking background\n  Rectangle window = new Rectangle(\n    (int)((regions[0].X + regions[0].Width / 2f) * xscale),\n    (int)((regions[0].Y + regions[0].Height / 2f) * \n    yscale), 1, 1);\n  window.Inflate((int)(0.2f * regions[0].Width * xscale),\n    (int)(0.4f * regions[0].Height * yscale));\n  // Initialize tracker\n  if (tracker != null)\n  {\n    tracker.SearchWindow = window;\n    tracker.ProcessFrame(im);\n  }\nmarker = new RectanglesMarker(window);\nmarker.ApplyInPlace(im);\nargs.Frame = im.ToManagedImage();\ntracking = true;\n}\n  else\n  {\n    detecting = true;\n  }\n```", "```py\n// create motion detector\nMotionDetector detector = new MotionDetector(\n  new SimpleBackgroundModelingDetector(),\n  new MotionAreaHighlighting());\n// continuously feed video frames to motion detector\nwhile ()\n{\n  // process new video frame and check motion level\n  if (detector.ProcessFrame(videoFrame) > 0.02)\n  {\n   // ring alarm or do somethng else\n  }\n}\nOpening our video source\nvideoSourcePlayer.VideoSource = new AsyncVideoSource(source);\n```", "```py\nprivate void videoSourcePlayer_NewFrame(object sender,  \n  NewFrameEventArgs args)\n{\n  lock (this)\n{\n  if (detector != null)\n{\n  float motionLevel = detector.ProcessFrame(args.Frame);\n  if (motionLevel > motionAlarmLevel)\n  {\n    // flash for 2 seconds\n    flash = (int)(2 * (1000 / alarmTimer.Interval));\n  }\n// check objects' count\nif (detector.MotionProcessingAlgorithm is BlobCountingObjectsProcessing)\n{\n  BlobCountingObjectsProcessing countingDetector = \n    (BlobCountingObjectsProcessing)\n    detector.MotionProcessingAlgorithm;\n  detectedObjectsCount = countingDetector.ObjectsCount;\n}\nelse\n{\ndetectedObjectsCount = -1;\n}\n// accumulate history\nmotionHistory.Add(motionLevel);\nif (motionHistory.Count > 300)\n{\n  motionHistory.RemoveAt(0);\n}\nif (showMotionHistoryToolStripMenuItem.Checked)\n  DrawMotionHistory(args.Frame);\n}\n}\n```", "```py\nfloat motionLevel = detector.ProcessFrame(args.Frame);\nif (motionLevel > motionAlarmLevel)\n{\n// flash for 2 seconds\nflash = (int)(2 * (1000 / alarmTimer.Interval));\n}\n\n```"]