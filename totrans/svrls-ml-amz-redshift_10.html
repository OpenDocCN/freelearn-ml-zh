<html><head></head><body>
		<div id="_idContainer187">
			<h1 id="_idParaDest-136" class="chapter-number"><a id="_idTextAnchor178"/>10</h1>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor179"/>Creating a Custom ML Model with XGBoost</h1>
			<p><a id="_idTextAnchor180"/>So far, all of the supervised learning models we have explored have utilized the <strong class="bold">Amazon Redshift Auto ML</strong> feature, which uses <strong class="bold">Amazon SageMaker Autopilot</strong> behind the scenes. In this chapter, we will explore how to create custom <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models. Training a custom model gives you the flexibility to choose the model type and the hyperparameters to use. This chapter will provide examples of this modeling technique. By the end of this chapter, you will know how to create a custom XGBoost model and how to prepare the data to train your model using <span class="No-Break">Redshift SQL.</span></p>
			<p>In this chapter, we will go through the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Introducing XGBoost</span></li>
				<li>Introducing an XGBoost <span class="No-Break">use case</span></li>
				<li>XGBoost model with Auto <span class="No-Break">off feature</span></li>
			</ul>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor181"/>Technical requirements</h1>
			<p>This chapter requires a web browser and access to <span class="No-Break">the following:</span></p>
			<ul>
				<li>An <span class="No-Break">AWS account</span></li>
				<li>An Amazon Redshift <span class="No-Break">Serverless endpoint</span></li>
				<li>Amazon Redshift Query <span class="No-Break">Editor v2</span></li>
			</ul>
			<p>You can find the code used in this <span class="No-Break">chapter here:</span></p>
			<p><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter10/chapter10.sql"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter10/chapter10.sql</span></a></p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor182"/>Introducing XGBoost</h1>
			<p><strong class="bold">XGBoost</strong> gets its <a id="_idIndexMarker454"/>name because it is built on <a id="_idIndexMarker455"/>the <strong class="bold">Gradient Boosting</strong> framework. Using a tree-boosting technique provides a fast method for solving ML problems. As you have seen in previous chapters, you can specify the model type, which can help speed up model training since <strong class="bold">SageMaker Autopilot</strong> does <a id="_idIndexMarker456"/>not have to determine which model type <span class="No-Break">to use.</span></p>
			<p>You can learn more <a id="_idIndexMarker457"/>about XGBoost <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</span></a><span class="No-Break">.</span></p>
			<p>When you create a model with Redshift ML and specify XGBoost as the model type, and optionally specify AUTO OFF, this turns off SageMaker Autopilot and you have more control of model tuning. For example, you can specify the hyperparameters you wish to use. You will see an example of this in the <em class="italic">Creating a binary classification model using </em><span class="No-Break"><em class="italic">XGBoost</em></span><span class="No-Break"> section.</span></p>
			<p>You will have to perform preprocessing when you set <strong class="bold">AUTO</strong> to <strong class="bold">OFF</strong>. Carrying out the preprocessing ensures we will get the best possible model and is also necessary since all inputs must be numeric when you set <strong class="bold">AUTO</strong> to <strong class="bold">OFF</strong>, for example, by making sure data is cleansed, categorical variables are encoded, and numeric variables are standardized.  You will also need to identify the type of problem that you have and select an appropriate model to train. You will be able to create train and test datasets and evaluate models yourself. You also have the ability to tune the hyperparameters. In summary, you get total control of the end-to-end ML model training <span class="No-Break">and building.</span></p>
			<p>By using XGBoost with Amazon Redshift ML, you can solve both regression and classification problems. You also can specify the learning objective of your model. For example, if you are solving a binary classification problem, you would choose <strong class="source-inline">binary:logistic</strong> as your objective or use <strong class="source-inline">multi:softmax</strong> for multi-class <span class="No-Break">classification problems.</span></p>
			<p>At the time of writing this book, the supported learning objectives are <strong class="source-inline">reg:squarederror</strong>, <strong class="source-inline">reg:squaredlogerror</strong>, <strong class="source-inline">reg:logistic</strong>, <strong class="source-inline">reg:pseudohubererror</strong>, <strong class="source-inline">reg:tweedie</strong>, <strong class="source-inline">binary:logistic</strong>, <strong class="source-inline">binary:hinge</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">multi:softmax</strong></span><span class="No-Break">.</span></p>
			<p>For more information about these objectives, see the <em class="italic">Learning Task Parameters</em> section of the<a id="_idIndexMarker458"/> XGBoost documentation <span class="No-Break">here: </span><a href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters"><span class="No-Break">https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters</span></a><span class="No-Break">.</span></p>
			<p>Now that you have learned what XGBoost is, we will take a look at a use case where we can apply XGBoost and solve a common business problem using <span class="No-Break">binary classification.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor183"/>Introducing an XGBoost use case</h1>
			<p>In this section, we <a id="_idIndexMarker459"/>will be discussing a use case where we want to predict whether credit card transactions are fraudulent. We will be going through the <span class="No-Break">following steps:</span></p>
			<ul>
				<li>Defining the <span class="No-Break">business problem</span></li>
				<li>Uploading, analyzing, and preparing data <span class="No-Break">for training</span></li>
				<li>Splitting data into training and <span class="No-Break">testing datasets</span></li>
				<li>Preprocessing the <span class="No-Break">input variables</span></li>
			</ul>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor184"/>Defining the business problem</h2>
			<p>In this section, we<a id="_idIndexMarker460"/> will use a credit card payment transaction dataset to build a binary classification model using XGBoost in Redshift ML. This dataset contains customer and terminal information along with the date and amount related to the transaction. This dataset also has some derived fields based on <strong class="bold">recency</strong>, <strong class="bold">frequency</strong>, and <strong class="bold">monetary</strong> numeric features, along with a few categorical variables, such as whether a transaction occurred during the weekend or at night. Our goal is to identify whether a transaction is fraudulent or non-fraudulent. This use case is taken from <a href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook">https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook</a>. Please refer to the GitHub repository to learn more about this data <span class="No-Break">generation process.</span></p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout"><em class="italic">Reproducible Machine Learning for Credit Card Fraud Detection - Practical Handbook</em>, Le Borgne, Yann-Aël and Siblini, Wissam and Lebichot, Bertrand and Bontempi, Gianluca, <a href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook">https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook</a>, 2022, Université Libre <span class="No-Break">de Bruxelles</span></p>
			<p>Now, we will load our dataset into Amazon Redshift ML and prepare it for <span class="No-Break">model training.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor185"/>Uploading, analyzing, and preparing data for training</h2>
			<p>Before we <a id="_idIndexMarker461"/>begin, let’s first connect to Redshift as an admin or database developer and then load data into <span class="No-Break">Amazon Redshift.</span></p>
			<p>In the following steps, you <a id="_idIndexMarker462"/>will create a schema for all of the tables<a id="_idIndexMarker463"/> and objects needed for this exercise, which involves creating all the needed tables, loading data, and creating the views used for <span class="No-Break">data transformations.</span></p>
			<p>Navigate to Query Editor v2, connect to the serverless endpoint, and then connect to the <strong class="bold">dev</strong> database, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B19071_10_01.jpg" alt="Figure 10.1 – Connect to Query Editor v2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Connect to Query Editor v2</p>
			<ol>
				<li>Execute the following step to create the schema. This schema will be used for all objects and models created in <span class="No-Break">this chapter:</span><pre class="source-code">
CREATE SCHEMA  chapter10_xgboost;</pre></li>
				<li>Next, copy the following SQL statement into Query Editor v2 to create the table for hosting the customer payment transaction history, which we will load in the <span class="No-Break">subsequent step:</span><pre class="source-code">
create table chapter10_xgboost.cust_payment_tx_history</pre><pre class="source-code">
(</pre><pre class="source-code">
transaction_id integer,</pre><pre class="source-code">
tx_datetime timestamp,</pre><pre class="source-code">
customer_id integer,</pre><pre class="source-code">
terminal_id integer,</pre><pre class="source-code">
tx_amount decimal(9,2),</pre><pre class="source-code">
tx_time_seconds integer,</pre><pre class="source-code">
tx_time_days integer,</pre><pre class="source-code">
tx_fraud integer,</pre><pre class="source-code">
tx_fraud_scenario integer,</pre><pre class="source-code">
tx_during_weekend integer,</pre><pre class="source-code">
tx_during_night integer,</pre><pre class="source-code">
customer_id_nb_tx_1day_window decimal(9,2),</pre><pre class="source-code">
customer_id_avg_amount_1day_window decimal(9,2),</pre><pre class="source-code">
customer_id_nb_tx_7day_window decimal(9,2),</pre><pre class="source-code">
customer_id_avg_amount_7day_window decimal(9,2),</pre><pre class="source-code">
customer_id_nb_tx_30day_window decimal(9,2),</pre><pre class="source-code">
customer_id_avg_amount_30day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_nb_tx_1day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_risk_1day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_nb_tx_7day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_risk_7day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_nb_tx_30day_window decimal(9,2),</pre><pre class="source-code">
terminal_id_risk_30day_window decimal(9,2)</pre><pre class="source-code">
)</pre><pre class="source-code">
;</pre></li>
				<li>Now that you have created the table, you can execute the following command in Query Editor v2 to load <span class="No-Break">the table:</span><pre class="source-code">
copy chapter10_xgboost.cust_payment_tx_history</pre><pre class="source-code">
from 's3://packt-serverless-ml-redshift/chapter10/credit_card_transactions_transformed_balanced.csv'</pre><pre class="source-code">
iam_role default</pre><pre class="source-code">
ignoreheader 1</pre><pre class="source-code">
csv region 'eu-west-1';</pre></li>
				<li>Now that you <a id="_idIndexMarker464"/>have loaded the data, it’s a <a id="_idIndexMarker465"/>good practice to sample some data to make <a id="_idIndexMarker466"/>sure our data is loaded properly. Run the following query to sample <span class="No-Break">10 records:</span><pre class="source-code">
select * from</pre><pre class="source-code">
chapter10_xgboost.cust_payment_tx_history</pre><pre class="source-code">
limit 10;</pre></li>
			</ol>
			<p>In the following screenshot, we can see that we have loaded the data correctly with a sampling of different <span class="No-Break">transaction IDs:</span></p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B19071_10_02.jpg" alt="Figure 10.2 – Data sample"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Data sample</p>
			<p>As discussed in earlier chapters, the target variable is the value that we are trying to predict in our model. In our use case, we are trying to predict whether a transaction is fraudulent. In our dataset, this is the <strong class="source-inline">tx_fraud</strong> attribute, which is our target. Let us check our table to see how many transactions were flagged <span class="No-Break">as fraudulent.</span></p>
			<p>Run the following command in Query <span class="No-Break">Editor </span><span class="No-Break">v</span><span class="No-Break">2:</span></p>
			<pre class="source-code">
select tx_fraud, count(*)
from chapter10_xgboost.cust_payment_tx_history
group by 1;</pre>
			<p>We identify <a id="_idIndexMarker467"/>fraudulent transactions in our dataset as <a id="_idIndexMarker468"/>those with a <strong class="source-inline">tx_fraud</strong> value of <strong class="source-inline">1</strong>. We have identified 14,681 transactions <a id="_idIndexMarker469"/>as fraudulent in our dataset. Conversely, a <strong class="source-inline">tx_fraud</strong> value of <strong class="source-inline">0</strong> indicates that a transaction is <span class="No-Break">not fraudulent:</span></p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B19071_10_03.jpg" alt="Figure 10.3 – Fraudulent transactions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Fraudulent transactions</p>
			<p>Let us look at the trend of fraudulent and non-fraudulent transactions over the months. We want to analyze whether there are any unusual spikes in <span class="No-Break">fraudulent transactions.</span></p>
			<p>Run the following SQL command in Query <span class="No-Break">Editor </span><span class="No-Break">v</span><span class="No-Break">2:</span></p>
			<pre class="source-code">
select to_char(tx_datetime, 'yyyymm') as yearmonth,
sum(case when tx_fraud = 1 then 1 else 0 end) fraud_tx,
sum(case when tx_fraud = 0 then 1 else 0 end) non_fraud_tx,
count(*) as total_tx,
(fraud_tx::decimal(10,2 ) / total_tx::decimal(10,2) ) *100 as fraud_txn_pct
from chapter10_xgboost.cust_payment_tx_history
group by yearmonth
order by yearmonth</pre>
			<p>Notice that<a id="_idIndexMarker470"/> fraudulent transactions increased<a id="_idIndexMarker471"/> by<a id="_idIndexMarker472"/> nearly 8 percent in 202207 <span class="No-Break">over 202206:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B19071_10_04.jpg" alt="Figure 10.4 – Fraudulent transaction trends"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Fraudulent transaction trends</p>
			<p>Now that we have loaded our data, let’s get our data prepared for model training by splitting the data into train and test datasets. The training data is used to train the model and the testing data is used to run our <span class="No-Break">prediction queries.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor186"/>Splitting data into train and test datasets</h2>
			<p>To <a id="_idIndexMarker473"/>train the model, we will have transactions that are older than <strong class="source-inline">2022-10-01</strong>, which is ~ 80 percent of <span class="No-Break">the transactions.</span></p>
			<p>To test the model, we will use transactions from after <strong class="source-inline">2022-09-30</strong>, which is 20 percent of <span class="No-Break">the transactions.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor187"/>Preprocessing the input variables</h2>
			<p>We have a <a id="_idIndexMarker474"/>combination of numeric and categorical variables in our input fields. We need to preprocess the categorical variables into one-hot-encoded values and standardize the numeric variables. Since we will be using <strong class="bold">AUTO OFF</strong>, SageMaker does not automatically preprocess the data. Hence, it is important to transform various numeric, datetime, and <span class="No-Break">categorical features.</span></p>
			<p><strong class="bold">Categorical features</strong> (also referred to as nominal) have distinct categories or levels. These can be<a id="_idIndexMarker475"/> categories without an order to them, such as country or gender. Or they can have an order such as level of education (also referred to <span class="No-Break">as ordinal).</span></p>
			<p>Since ML models <a id="_idIndexMarker476"/>need to operate on <strong class="bold">numeric variables</strong>, we <a id="_idIndexMarker477"/>need to apply ordinal encoding or <span class="No-Break">one-hot encoding.</span></p>
			<p>To make things easier, we have created the following view to take care of the transformation logic. This view is somewhat lengthy, but actually, what the view is doing is <span class="No-Break">quite simple:</span></p>
			<ul>
				<li>Calculating the transaction time in seconds <span class="No-Break">and days</span></li>
				<li>Applying one-hot encoding by assigning <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> to classify transactions as weekday, weekend, daytime, or nighttime (such as <strong class="source-inline">TX_DURING_WEEKEND</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">TX_DURING_NIGHT</strong></span><span class="No-Break">)</span></li>
				<li>Applying window functions to transactions so that we make it easy to visualize the data in 1-day, 7-day, and <span class="No-Break">30-day intervals</span></li>
			</ul>
			<p>Execute the following SQL command in Query Editor v2 to create the view by applying the <span class="No-Break">transformation logic:</span></p>
			<pre class="source-code">
create view chapter10_xgboost.credit_payment_tx_history_scaled
as
select
transaction_id, tx_datetime, customer_id, terminal_id,
tx_amount ,
( (tx_amount - avg(tx_amount) over()) /  cast(stddev_pop(tx_amount) over() as dec(14,2)) ) s_tx_amount,
tx_time_seconds ,
  ( (tx_time_seconds - avg(tx_time_seconds) over()) /  cast(stddev_pop(tx_time_seconds) over() as dec(14,2)) ) s_tx_time_seconds,
tx_time_days  ,
  ( (tx_time_days - avg(tx_time_days) over()) /  cast(stddev_pop(tx_time_days) over() as dec(14,2)) ) s_tx_time_days,
tx_fraud  ,
  tx_during_weekend ,
case when tx_during_weekend = 1 then 1 else 0 end as tx_during_weekend_ind,
case when tx_during_weekend = 0 then 1 else 0 end tx_during_weekday_ind,
tx_during_night,
case when tx_during_night = 1 then 1 else 0 end as tx_during_night_ind,
case when tx_during_night = 0 then 1 else 0 end as tx_during_day_ind,
customer_id_nb_tx_1day_window ,
  ( (customer_id_nb_tx_1day_window - avg(customer_id_nb_tx_1day_window) over()) /  cast(stddev_pop(customer_id_nb_tx_1day_window) over() as dec(14,2)) ) s_customer_id_nb_tx_1day_window,
customer_id_avg_amount_1day_window  ,
  ( (customer_id_avg_amount_1day_window - avg(customer_id_avg_amount_1day_window) over()) /  cast(stddev_pop(customer_id_avg_amount_1day_window) over() as dec(14,2)) ) s_customer_id_avg_amount_1day_window,
customer_id_nb_tx_7day_window ,
  ( (customer_id_nb_tx_7day_window - avg(customer_id_nb_tx_7day_window) over()) /  cast(stddev_pop(customer_id_nb_tx_7day_window) over() as dec(14,2)) ) s_customer_id_nb_tx_7day_window,
customer_id_avg_amount_7day_window  ,
  ( (customer_id_avg_amount_7day_window - avg(customer_id_avg_amount_7day_window) over()) /  cast(stddev_pop(customer_id_avg_amount_7day_window) over() as dec(14,2)) ) s_customer_id_avg_amount_7day_window,
customer_id_nb_tx_30day_window  ,
  ( (customer_id_nb_tx_30day_window - avg(customer_id_nb_tx_30day_window) over()) /  cast(stddev_pop(customer_id_nb_tx_30day_window) over() as dec(14,2)) ) s_customer_id_nb_tx_30day_window,
customer_id_avg_amount_30day_window ,
  ( (customer_id_avg_amount_30day_window - avg(customer_id_avg_amount_30day_window) over()) /  cast(stddev_pop(customer_id_avg_amount_30day_window) over() as dec(14,2)) ) s_customer_id_avg_amount_30day_window,
terminal_id_nb_tx_1day_window ,
  ( (terminal_id_nb_tx_1day_window - avg(terminal_id_nb_tx_1day_window) over()) /  cast(stddev_pop(terminal_id_nb_tx_1day_window) over() as dec(14,2)) ) s_terminal_id_nb_tx_1day_window,
terminal_id_risk_1day_window  ,
  ( (terminal_id_risk_1day_window - avg(terminal_id_risk_1day_window) over()) /  cast(stddev_pop(terminal_id_risk_1day_window) over() as dec(14,2)) ) s_terminal_id_risk_1day_window,
terminal_id_nb_tx_7day_window ,
  ( (terminal_id_nb_tx_7day_window - avg(terminal_id_nb_tx_7day_window) over()) /  cast(stddev_pop(terminal_id_nb_tx_7day_window) over() as dec(14,2)) ) s_terminal_id_nb_tx_7day_window,
terminal_id_risk_7day_window  ,
  ( (terminal_id_risk_7day_window - avg(terminal_id_risk_7day_window) over()) /  cast(stddev_pop(terminal_id_risk_7day_window) over() as dec(14,2)) ) s_terminal_id_risk_7day_window,
terminal_id_nb_tx_30day_window  ,
  ( (terminal_id_nb_tx_30day_window - avg(terminal_id_nb_tx_30day_window) over()) /  cast(stddev_pop(terminal_id_nb_tx_30day_window) over() as dec(14,2)) ) s_terminal_id_nb_tx_30day_window,
terminal_id_risk_30day_window ,
  ( (terminal_id_risk_30day_window - avg(terminal_id_risk_30day_window) over()) /  cast(stddev_pop(terminal_id_risk_30day_window) over() as dec(14,2)) ) s_terminal_id_risk_30day_window
from
chapter10_xgboost.cust_payment_tx_history;</pre>
			<p>Now that the <a id="_idIndexMarker478"/>view is created, let’s sample <span class="No-Break">10 records.</span></p>
			<p>Execute the following command in Query <span class="No-Break">Editor </span><span class="No-Break">v</span><span class="No-Break">2:</span></p>
			<pre class="source-code">
SELECT * from chapter10_XGBoost.credit_payment_tx_history_scaled limit 10;</pre>
			<p>We can see some of our transformed values, such as <strong class="source-inline">tx_time_seconds</strong> and <strong class="source-inline">txn_time_days</strong>, in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B19071_10_05.jpg" alt="Figure 10.5 – Transformed data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Transformed data</p>
			<p>Now, let’s <a id="_idIndexMarker479"/>quickly review why we needed to create <span class="No-Break">this view:</span></p>
			<ul>
				<li>Since we are using XGBoost with Auto OFF, we must do our own data preprocessing and <span class="No-Break">feature engineering</span></li>
				<li>We applied one-hot encoding to our <span class="No-Break">categorical variables</span></li>
				<li>We scaled our <span class="No-Break">numeric variables</span></li>
			</ul>
			<p>Here is a summary of the <span class="No-Break">view logic:</span></p>
			<ul>
				<li>The target variable we used <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">TX_FRAUD</strong></span></li>
				<li>The categorical variables we used are <strong class="source-inline">TX_DURING_WEEKEND_IND</strong>, <strong class="source-inline">TX_DURING_WEEKDAY_IND</strong>, <strong class="source-inline">TX_DURING_NIGHT_IND</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">TX_DURING_DAY_IND</strong></span></li>
				<li>The scaled numeric variables are <strong class="source-inline">s_customer_id_nb_tx_1day_window</strong>, <strong class="source-inline">s_customer_id_avg_amount_1day_window</strong>, <strong class="source-inline">s_customer_id_nb_tx_7day_window</strong>, <strong class="source-inline">s_customer_id_avg_amount_7day_window,s_customer_id_nb_tx_30day_window</strong>, <strong class="source-inline">s_customer_id_avg_amount_30day_window</strong>, <strong class="source-inline">s_terminal_id_nb_tx_1day_window</strong>, <strong class="source-inline">s_terminal_id_risk_1day_window</strong>, <strong class="source-inline">s_terminal_id_nb_tx_7day_window</strong>, <strong class="source-inline">s_terminal_id_risk_7day_window</strong>, <strong class="source-inline">s_terminal_id_nb_tx_30day_window</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">s_terminal_id_risk_30day_window</strong></span></li>
			</ul>
			<p>You have now completed data preparation and are ready to create <span class="No-Break">your model!</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor188"/>Creating a model using XGBoost with Auto Off</h1>
			<p>In this exercise, we <a id="_idIndexMarker480"/>are <a id="_idIndexMarker481"/>going to create a custom binary classification model using the XGBoost algorithm. You can achieve this by setting <strong class="bold">AUTO off</strong>. Here are the parameters that <span class="No-Break">are available:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">AUTO OFF</strong></span></li>
				<li><span class="No-Break"><strong class="bold">MODEL_TYPE</strong></span></li>
				<li><span class="No-Break"><strong class="bold">OBJECTIVE</strong></span></li>
				<li><span class="No-Break"><strong class="bold">HYPERPARAMETERS</strong></span></li>
			</ul>
			<p>For the complete list of hyperparameter values that are available and their defaults, please read the documentation <span class="No-Break">found here:</span></p>
			<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_auto_off_create_model"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_auto_off_create_model</span></a></p>
			<p>Now that you have a basic understanding of the parameters available with XGBoost, you can create <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor189"/>Creating a binary classification model using XGBoost</h2>
			<p>Let’s <a id="_idIndexMarker482"/>create a model to predict whether a <a id="_idIndexMarker483"/>transaction is fraudulent or non-fraudulent. As you learned in the previous chapters, creating models with Amazon Redshift ML is simply done by running a SQL command that creates a function. As inputs (or features), you will be using the attributes from the view that you created in the previous section. You will specify <strong class="source-inline">tx_fraud</strong> as the target and give the function name, which you will use later in your prediction queries. Additionally, you will specify hyperparameters to do your own model tuning. <span class="No-Break">Let’s begin!</span></p>
			<p>Execute the following commands in Query Editor v2. The following is a code snippet; you may retrieve<a id="_idIndexMarker484"/> the full code from the <a id="_idIndexMarker485"/><span class="No-Break">following URL:</span></p>
			<p><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter10.sql"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter10.sql</span></a></p>
			<pre class="source-code">
drop model chapter10_XGBoost.cust_cc_txn_fd_xg;
 create model chapter10_xgboost.cust_cc_txn_fd_xg
from (
select
  s_tx_amount,
tx_fraud,
…
  from chapter10_xgboost.payment_tx_history_scaled
  where cast(tx_datetime as date) between '2022-06-01' and '2022-09-30'
)
target tx_fraud
function fn_customer_cc_fd_xg
iam_role default
auto off
model_type xgboost
objective 'binary:logistic'
preprocessors 'none'
hyperparameters default except (num_round '100')
settings (
  s3_bucket '&lt;&lt;your-s3-bucket&gt;&gt;',
            s3_garbage_collect off,
            max_runtime 1500
                   );</pre>
			<p>The <strong class="source-inline">CREATE MODEL</strong> function is going to invoke the XGBoost algorithm and train a binary <a id="_idIndexMarker486"/>classification model. We have set <strong class="bold">AUTO off</strong>, which means Autopilot is not going to perform any tasks for us. We are <a id="_idIndexMarker487"/>customizing the model to be a binary classifier using preprocessed data. We also set the <strong class="source-inline">num_round</strong> hyperparameter value to <strong class="source-inline">100</strong>, which is the number of rounds to run <span class="No-Break">the training.</span></p>
			<p>Now, let’s run <strong class="source-inline">SHOW MODEL</strong> to see whether model training is completed. Run the following command in Query <span class="No-Break">Editor </span><span class="No-Break">v</span><span class="No-Break">2:</span></p>
			<pre class="source-code">
SHOW MODEL  chapter10_XGBoost.cust_cc_txn_fd_xg;</pre>
			<p>Note <strong class="bold">Model State</strong> in the following screenshot, which shows your model is <span class="No-Break">still training:</span></p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B19071_10_06.jpg" alt="Figure 10.6 – Show model output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Show model output</p>
			<p>From the <a id="_idIndexMarker488"/>preceding screenshot, we <a id="_idIndexMarker489"/>notice that the value of <strong class="bold">Model State</strong> is <strong class="bold">TRAINING</strong>, which is self-explanatory – the model is still training. You will also see that Redshift ML has picked up the parameters we supplied in the <strong class="source-inline">CREATE MODEL</strong> statement – <strong class="bold">Model Type</strong> is set to <strong class="bold">xgboost</strong>. <strong class="bold">objective</strong> is set to <strong class="bold">binary:logistic</strong> and the <strong class="bold">num_round</strong> parameter is set <span class="No-Break">to </span><span class="No-Break"><strong class="bold">100</strong></span><span class="No-Break">.</span></p>
			<p>When you have a custom model with <strong class="bold">AUTO OFF</strong> and specify the hyperparameters, the model can be trained much faster. This model will usually finish in under <span class="No-Break">10 minutes.</span></p>
			<p>Run the <strong class="source-inline">SHOW MODEL</strong> command again after 10 minutes to check whether model training is complete or not. As you can see from the following screenshot, model training has completed and the <strong class="bold">train:error</strong> field reports the error rate. Most datasets have a threshold of <em class="italic">.5</em>, so our value of <strong class="bold">0.051870</strong> is very good, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B19071_10_07.jpg" alt="Figure 10.7 – SHOW MODEL output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – SHOW MODEL output</p>
			<p>Now, your <a id="_idIndexMarker490"/>model is complete and has a good<a id="_idIndexMarker491"/> score based on <strong class="source-inline">score – train_error</strong>, which is <strong class="source-inline">0.051870</strong>. You are now ready to use it <span class="No-Break">for predictions.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor190"/>Generating predictions and evaluating model performance</h2>
			<p>Run the<a id="_idIndexMarker492"/> following query in Query Editor v2, which <a id="_idIndexMarker493"/>will compare the actual <strong class="source-inline">tx_fraud</strong> value with the <span class="No-Break"><strong class="source-inline">predicted_tx_fraud</strong></span><span class="No-Break"> value:</span></p>
			<pre class="source-code">
select
tx_fraud ,
fn_customer_cc_fd_xg(
s_tx_amount,
tx_during_weekend_ind,
tx_during_weekday_ind,
tx_during_night_ind,
tx_during_day_ind,
s_customer_id_nb_tx_1day_window,
s_customer_id_avg_amount_1day_window,
s_customer_id_nb_tx_7day_window,
s_customer_id_avg_amount_7day_window,
s_customer_id_nb_tx_30day_window,
s_customer_id_avg_amount_30day_window,
s_terminal_id_nb_tx_1day_window,
s_terminal_id_risk_1day_window,
s_terminal_id_nb_tx_7day_window,
s_terminal_id_risk_7day_window,
s_terminal_id_nb_tx_30day_window,
s_terminal_id_risk_30day_window)
from chapter10_xgboost.credit_payment_tx_history_scaled
where cast(tx_datetime as date) &gt;= '2022-10-01'
;</pre>
			<p>The following <a id="_idIndexMarker494"/>screenshot shows the <a id="_idIndexMarker495"/>sample output. In this screenshot, our predicted values are the same as the <span class="No-Break">actual values:</span></p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B19071_10_08.jpg" alt="Figure 10.8 – Inference query output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Inference query output</p>
			<p>Since we did <a id="_idIndexMarker496"/>not get the F1 value for our<a id="_idIndexMarker497"/> model from Redshift ML, let’s calculate it. We will create a view that contains the logic to <span class="No-Break">accomplish this:</span></p>
			<pre class="source-code">
--drop view if exists chapter10_xgboost.fraud_tx_conf_matrix;
create or replace view chapter10_xgboost.fraud_tx_conf_matrix
as
select
transaction_id,tx_datetime,customer_id,tx_amount,terminal_id, tx_fraud,
  fn_customer_cc_fd_xg(
  s_tx_amount,
tx_during_weekend_ind,
tx_during_weekday_ind,
tx_during_night_ind,
tx_during_day_ind,
s_customer_id_nb_tx_1day_window,
s_customer_id_avg_amount_1day_window,
s_customer_id_nb_tx_7day_window,
s_customer_id_avg_amount_7day_window,
s_customer_id_nb_tx_30day_window,
s_customer_id_avg_amount_30day_window,
s_terminal_id_nb_tx_1day_window,
s_terminal_id_risk_1day_window,
s_terminal_id_nb_tx_7day_window,
s_terminal_id_risk_7day_window,
s_terminal_id_nb_tx_30day_window,
s_terminal_id_risk_30day_window)
as prediction,
case when tx_fraud  =1 and prediction = 1 then 1 else 0 end truepositives,
case when tx_fraud =0 and prediction = 0 then 1 else 0 end truenegatives,
case when tx_fraud =0 and prediction = 1 then 1 else 0 end falsepositives,
case when tx_fraud =1 and prediction = 0 then 1 else 0 end falsenegatives
  from chapter10_xgboost.credit_payment_tx_history_scaled
  where cast(tx_datetime as date) &gt;= '2022-10-01';</pre>
			<p>Run the <a id="_idIndexMarker498"/>following <a id="_idIndexMarker499"/>SQL command in Query Editor v2 to check the F1 score that we calculated in <span class="No-Break">the view:</span></p>
			<pre class="source-code">
select
sum(truepositives+truenegatives)*1.00/(count(*)*1.00) as accuracy,--accuracy of the model,
sum(falsepositives+falsenegatives)*1.00/count(*)*1.00 as error_rate, --how often model is wrong,
sum(truepositives)*1.00/sum (truepositives+falsenegatives) *1.00 as tpr, --or recall how often corrects are rights,
sum(falsepositives)*1.00/sum (falsepositives+truenegatives )*1.00 fpr, --or fall-out how often model said yes when it is no,
sum(truenegatives)*1.00/sum (falsepositives+truenegatives)*1.00 tnr, --or specificity, how often model said no when it is yes,
sum(truepositives)*1.00 / (sum (truepositives+falsepositives)*1.00) as precision, -- when said yes how it is correct,
2*((tpr*precision)/ (tpr+precision) ) as f_score --weighted avg of tpr &amp; fpr
from chapter10_xgboost.fraud_tx_conf_matrix
;</pre>
			<p>You can see our accuracy is 90 percent and our F1 score is 87 percent, which are both very good. Additionally, our confusion matrix values tell us how many times we correctly predicted <strong class="source-inline">True</strong> and correctly <span class="No-Break">predicted </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B19071_10_09.jpg" alt="Figure 10.9 – F1 score"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – F1 score</p>
			<p>Now, let’s check actual versus prediction counts. Run the following query in Query <span class="No-Break">Editor </span><span class="No-Break">v</span><span class="No-Break">2:</span></p>
			<pre class="source-code">
select tx_fraud,prediction, count(*)
from chapter10_xgboost.fraud_tx_conf_matrix
group by tx_fraud,prediction;</pre>
			<p>The <a id="_idIndexMarker500"/>output in<a id="_idIndexMarker501"/> the following screenshot shows, for a given value, what our prediction was compared to the actual value and the count of those records. Our model incorrectly predicted a fraudulent transaction 178 times and incorrectly predicted a non-fraudulent transaction <span class="No-Break">1,081 times:</span></p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B19071_10_10.jpg" alt="Figure 10.10 – Confusion matrix"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Confusion matrix</p>
			<p>This demonstrates how Redshift ML can help you confidently predict whether a transaction <span class="No-Break">is fraudulent.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor191"/>Summary</h1>
			<p>In this chapter, you learned what XGBoost is and how to apply it to a business problem. You learned how to specify your own hyperparameters when using the <strong class="bold">Auto Off</strong> option and how to specify the objective for a binary classification problem. Additionally, you learned how to do your own data preprocessing and calculate the F1 score to validate the <span class="No-Break">model performance.</span></p>
			<p>In the next chapter, you will learn how to bring your own models from Amazon SageMaker for in-database or <span class="No-Break">remote inference.</span></p>
		</div>
	</body></html>