["```py\n$ say 'You are 007! I win!' -v Vicki -o win_007.mp4 \n```", "```py\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.nummist.goldgesture\">\n\n <uses-permission android:name=\"android.permission.CAMERA\" />\n\n <uses-feature android:name=\"android.hardware.camera.front\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/AppTheme\">\n <activity\n android:name=\".CameraActivity\"\n android:screenOrientation=\"landscape\"\n android:theme=\"@android:style/Theme.NoTitleBar.Fullscreen\">\n <intent-filter>\n <action android:name=\"android.intent.action.MAIN\" />\n\n <category android:name=\"android.intent.category.LAUNCHER\" />\n </intent-filter>\n </activity>\n    </application>\n\n</manifest>\n```", "```py\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<android.support.constraint.ConstraintLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n xmlns:opencv=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\".CameraActivity\">\n\n <org.opencv.android.JavaCameraView\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"fill_parent\"\n        android:id=\"@+id/camera_view\"\n        app:layout_constraintBottom_toBottomOf=\"parent\"\n        app:layout_constraintLeft_toLeftOf=\"parent\"\n        app:layout_constraintRight_toRightOf=\"parent\"\n        app:layout_constraintTop_toTopOf=\"parent\"\n        opencv:camera_id=\"front\" />\n\n</android.support.constraint.ConstraintLayout>\n```", "```py\npackage com.nummist.goldgesture;\n\npublic final class BackAndForthGesture {\n\n    private double mMinDistance;\n\n    private double mStartPosition;\n    private double mDelta;\n\n    private int mBackCount;\n    private int mForthCount;\n```", "```py\n    public int getBackAndForthCount() {\n        return Math.min(mBackCount, mForthCount);\n    }\n```", "```py\n    public BackAndForthGesture(final double minDistance) {\n        mMinDistance = minDistance;\n    }\n```", "```py\n    public void start(final double position) {\n        mStartPosition = position;\n        mDelta = 0.0;\n        mBackCount = 0;\n        mForthCount = 0;\n    }\n```", "```py\n    public void update(final double position) {\n        double lastDelta = mDelta;\n        mDelta = position - mStartPosition;\n        if (lastDelta < mMinDistance &&\n                mDelta >= mMinDistance) {\n            mForthCount++;\n        } else if (lastDelta > -mMinDistance &&\n                mDelta < -mMinDistance) {\n            mBackCount++;\n        }\n    }\n```", "```py\n    public void resetCounts() {\n        mBackCount = 0;\n        mForthCount = 0;\n    }\n}\n```", "```py\npackage com.nummist.goldgesture;\n\nimport android.content.Context;\nimport android.media.MediaPlayer;\nimport android.media.MediaPlayer.OnCompletionListener;\n\npublic final class YesNoAudioTree {\n\n    private enum Affiliation { UNKNOWN, MI6, CIA, KGB, CRIMINAL }\n\n    private int mLastAudioResource;\n    private Affiliation mAffiliation;\n\n    private Context mContext;\n    private MediaPlayer mMediaPlayer;\n```", "```py\n    public YesNoAudioTree(final Context context) {\n        mContext = context;\n    }\n```", "```py\n    public void start() {\n        mAffiliation = Affiliation.UNKNOWN;\n        play(R.raw.intro);\n    }\n```", "```py\n    public void stop() {\n        if (mMediaPlayer != null) {\n            mMediaPlayer.release();\n        }\n    }\n```", "```py\n    public void takeYesBranch() {\n\n        if (mMediaPlayer != null && mMediaPlayer.isPlaying()) {\n            // Do not interrupt the audio that is already playing.\n            return;\n        }\n\n        switch (mAffiliation) {\n            case UNKNOWN:\n                switch (mLastAudioResource) {\n                    case R.raw.q_mi6:\n                        mAffiliation = Affiliation.MI6;\n                        play(R.raw.q_martinis);\n                        break;\n                    case R.raw.q_cia:\n                        mAffiliation = Affiliation.CIA;\n                        play(R.raw.q_bond_friend);\n                        break;\n                    case R.raw.q_kgb:\n                        mAffiliation = Affiliation.KGB;\n                        play(R.raw.q_chief);\n                        break;\n                    case R.raw.q_criminal:\n                        mAffiliation = Affiliation.CRIMINAL;\n                        play(R.raw.q_chief);\n                        break;\n                }\n                break;\n            case MI6:\n                // The person works for MI6.\n                switch (mLastAudioResource) {\n                    case R.raw.q_martinis:\n                        // The person drinks shaken martinis (007).\n                        play(R.raw.win_007);\n                        break;\n                    // ...\n                    // See the GitHub repository for more cases.\n                    // ...\n                    default:\n                        // The person remains unknown.\n                        play(R.raw.lose);\n                        break;\n                }\n                break;\n            // ...\n            // See the GitHub repository for more cases.\n            // ...\n         }            \n       }       \n```", "```py\n    public void takeNoBranch() {\n\n        if (mMediaPlayer != null && mMediaPlayer.isPlaying()) {\n            // Do not interrupt the audio that is already playing.\n            return;\n        }\n\n        switch (mAffiliation) {\n            case UNKNOWN:\n                switch (mLastAudioResource) {\n                    case R.raw.q_mi6:\n                        // The person does not work for MI6.\n                        // Ask whether the person works for a criminal\n                        // organization.\n                        play(R.raw.q_criminal);\n                        break;\n                    // ...\n                    // See the GitHub repository for more cases.\n                    // ...\n                    default:\n                        // The person remains unknown.\n                        play(R.raw.lose);\n                        break;\n            // ...\n            // See the GitHub repository for more cases.\n            // ...\n    }\n   }\n }\n```", "```py\n    private void takeAutoBranch() {\n        switch (mLastAudioResource) {\n            case R.raw.intro:\n                play(R.raw.q_mi6);\n                break;\n            case R.raw.win_007:\n            case R.raw.win_blofeld:\n            case R.raw.win_gogol:\n            case R.raw.win_jaws:\n            case R.raw.win_leiter:\n            case R.raw.win_m:\n            case R.raw.win_moneypenny:\n            case R.raw.win_q:\n            case R.raw.win_rublevitch:\n            case R.raw.win_tanner:\n            case R.raw.lose:\n                start();\n                break;\n        }\n    }\n```", "```py\n    private void play(final int audioResource) {\n        mLastAudioResource = audioResource;\n        mMediaPlayer = MediaPlayer.create(mContext, audioResource);\n        mMediaPlayer.setOnCompletionListener(\n                new OnCompletionListener() {\n                    @Override\n                    public void onCompletion(\n                            final MediaPlayer mediaPlayer) {\n                        mediaPlayer.release();\n                        if (mMediaPlayer == mediaPlayer) {\n                            mMediaPlayer = null;\n                        }\n                        takeAutoBranch();\n                    }\n                });\n        mMediaPlayer.start();\n    }\n}\n```", "```py\npackage com.nummist.goldgesture;\n\n// ...\n// See the GitHub repository for imports\n// ...\n\npublic final class CameraActivity extends Activity\n        implements CvCameraViewListener2 {\n\n    // A tag for log output.\n    private static final String TAG = \"CameraActivity\";\n```", "```py\n    // Parameters for face detection.\n    private static final double SCALE_FACTOR = 1.2;\n    private static final int MIN_NEIGHBORS = 3;\n    private static final int FLAGS = Objdetect.CASCADE_SCALE_IMAGE;\n    private static final double MIN_SIZE_PROPORTIONAL = 0.25;\n    private static final double MAX_SIZE_PROPORTIONAL = 1.0;\n```", "```py\n    // The portion of the face that is excluded from feature\n    // selection on each side.\n    // (We want to exclude boundary regions containing background.)\n    private static final double MASK_PADDING_PROPORTIONAL = 0.15;\n```", "```py\n    // Parameters for face tracking.\n    private static final int MIN_FEATURES = 10;\n    private static final int MAX_FEATURES = 80;\n    private static final double MIN_FEATURE_QUALITY = 0.05;\n    private static final double MIN_FEATURE_DISTANCE = 4.0;\n    private static final float MAX_FEATURE_ERROR = 200f;\n```", "```py\n    // Parameters for gesture detection\n    private static final double MIN_SHAKE_DIST_PROPORTIONAL = 0.01;\n    private static final double MIN_NOD_DIST_PROPORTIONAL = 0.0025;\n    private static final double MIN_BACK_AND_FORTH_COUNT = 2;\n```", "```py\n    // The camera view.\n    private CameraBridgeViewBase mCameraView;\n\n    // The dimensions of the image before orientation.\n    private double mImageWidth;\n    private double mImageHeight;\n\n    // The current gray image before orientation.\n    private Mat mGrayUnoriented;\n\n    // The current and previous equalized gray images.\n    private Mat mEqualizedGray;\n    private Mat mLastEqualizedGray;\n```", "```py\n    // The mask, in which the face region is white and the\n    // background is black.\n    private Mat mMask;\n    private Scalar mMaskForegroundColor;\n    private Scalar mMaskBackgroundColor;\n\n    // The face detector, more detection parameters, and\n    // detected faces.\n    private CascadeClassifier mFaceDetector;\n    private Size mMinSize;\n    private Size mMaxSize;\n    private MatOfRect mFaces;\n\n    // The initial features before tracking.\n    private MatOfPoint mInitialFeatures;\n\n    // The current and previous features being tracked.\n    private MatOfPoint2f mFeatures;\n    private MatOfPoint2f mLastFeatures;\n\n    // The status codes and errors for the tracking.\n    private MatOfByte mFeatureStatuses;\n    private MatOfFloat mFeatureErrors;\n\n    // Whether a face was being tracked last frame.\n    private boolean mWasTrackingFace;\n\n    // Colors for drawing.\n    private Scalar mFaceRectColor;\n    private Scalar mFeatureColor;\n```", "```py\n    // Gesture detectors.\n    private BackAndForthGesture mNodHeadGesture;\n    private BackAndForthGesture mShakeHeadGesture;\n\n    // The audio tree for the 20 questions game.\n    private YesNoAudioTree mAudioTree;\n```", "```py\n    @Override\n    protected void onCreate(final Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        if (OpenCVLoader.initDebug()) {\n            Log.i(TAG, \"Initialized OpenCV\");\n        } else {\n            Log.e(TAG, \"Failed to initialize OpenCV\");\n            finish();\n        }\n\n        final Window window = getWindow();\n        window.addFlags(\n                WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);\n\n        setContentView(R.layout.activity_camera);\n        mCameraView = (CameraBridgeViewBase)\n                findViewById(R.id.camera_view);\n        //mCameraView.enableFpsMeter();\n        mCameraView.setCvCameraViewListener(this);\n    }\n```", "```py\n    @Override\n    public void onPause() {\n        if (mCameraView != null) {\n            mCameraView.disableView();\n        }\n        if (mAudioTree != null) {\n            mAudioTree.stop();\n        }\n        resetGestures();\n        super.onPause();\n    }\n```", "```py\n    @Override\n    public void onResume() {\n        super.onResume();\n        if (ContextCompat.checkSelfPermission(this,\n                Manifest.permission.CAMERA)\n                != PackageManager.PERMISSION_GRANTED) {\n            if (ActivityCompat.shouldShowRequestPermissionRationale(this,\n                    Manifest.permission.CAMERA)) {\n                showRequestPermissionRationale();\n            } else {\n                ActivityCompat.requestPermissions(this,\n                        new String[] { Manifest.permission.CAMERA },\n                        PERMISSIONS_REQUEST_CAMERA);\n            }\n        } else {\n            Log.i(TAG, \"Camera permissions were already granted\");\n\n            // Start the camera.\n            mCameraView.enableView();\n        }\n    }\n```", "```py\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        if (mCameraView != null) {\n            // Stop the camera.\n            mCameraView.disableView();\n        }\n        if (mAudioTree != null) {\n            mAudioTree.stop();\n        }\n        resetGestures();\n    }\n```", "```py\n    void showRequestPermissionRationale() {\n        AlertDialog dialog = new AlertDialog.Builder(this).create();\n        dialog.setTitle(\"Camera, please\");\n        dialog.setMessage(\n                \"Goldgesture uses the camera to see you nod or shake \" +\n                \"your head. You will be asked for camera access.\");\n        dialog.setButton(AlertDialog.BUTTON_NEUTRAL, \"OK\",\n                new DialogInterface.OnClickListener() {\n                    public void onClick(DialogInterface dialog,\n                                        int which) {\n                        dialog.dismiss();\n                        ActivityCompat.requestPermissions(\n                                CameraActivity.this,\n                                new String[] {\n                                        Manifest.permission.CAMERA },\n                                PERMISSIONS_REQUEST_CAMERA);\n                    }\n                });\n        dialog.show();\n    }\n```", "```py\n@Override\npublic void onRequestPermissionsResult(final int requestCode,\n        final String permissions[], final int[] grantResults) {\n    switch (requestCode) {\n        case PERMISSIONS_REQUEST_CAMERA: {\n            if (grantResults.length > 0 &&\n                    grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                Log.i(TAG, \"Camera permissions were granted just now\");\n\n                // Start the camera.\n                mCameraView.enableView();\n            } else {\n                Log.e(TAG, \"Camera permissions were denied\");\n                finish();\n            }\n            break;\n        }\n    }\n}\n```", "```py\n    @Override\n    public void onCameraViewStarted(final int width,\n                                    final int height) {\n\n        mImageWidth = width;\n        mImageHeight = height;\n```", "```py\n        initFaceDetector();\n        mFaces = new MatOfRect();\n```", "```py\n        final int smallerSide;\n        if (height < width) {\n            smallerSide = height;\n        } else {\n            smallerSide = width;\n        }\n\n        final double minSizeSide =\n                MIN_SIZE_PROPORTIONAL * smallerSide;\n        mMinSize = new Size(minSizeSide, minSizeSide);\n\n        final double maxSizeSide =\n                MAX_SIZE_PROPORTIONAL * smallerSide;\n        mMaxSize = new Size(maxSizeSide, maxSizeSide);\n```", "```py\n        mInitialFeatures = new MatOfPoint();\n        mFeatures = new MatOfPoint2f(new Point());\n        mLastFeatures = new MatOfPoint2f(new Point());\n        mFeatureStatuses = new MatOfByte();\n        mFeatureErrors = new MatOfFloat();\n```", "```py\n        mFaceRectColor = new Scalar(0.0, 0.0, 255.0);\n        mFeatureColor = new Scalar(0.0, 255.0, 0.0);\n```", "```py\n        final double minShakeDist =\n                smallerSide * MIN_SHAKE_DIST_PROPORTIONAL;\n        mShakeHeadGesture = new BackAndForthGesture(minShakeDist);\n\n        final double minNodDist =\n                smallerSide * MIN_NOD_DIST_PROPORTIONAL;\n        mNodHeadGesture = new BackAndForthGesture(minNodDist);\n```", "```py\n        mAudioTree = new YesNoAudioTree(this);\n        mAudioTree.start();\n```", "```py\n        mGrayUnoriented = new Mat(height, width, CvType.CV_8UC1);\n\n        // The rest of the matrices are transposed.\n\n        mEqualizedGray = new Mat(width, height, CvType.CV_8UC1);\n        mLastEqualizedGray = new Mat(width, height, CvType.CV_8UC1);\n\n        mMask = new Mat(width, height, CvType.CV_8UC1);\n        mMaskForegroundColor = new Scalar(255.0);\n        mMaskBackgroundColor = new Scalar(0.0);\n    }\n```", "```py\n    @Override\n    public void onCameraViewStopped() {\n    }\n```", "```py\n    @Override\n    public Mat onCameraFrame(final CvCameraViewFrame inputFrame) {\n        final Mat rgba = inputFrame.rgba();\n\n        // For processing, orient the image to portrait and equalize\n        // it.\n        Imgproc.cvtColor(rgba, mGrayUnoriented,\n                Imgproc.COLOR_RGBA2GRAY);\n        Core.transpose(mGrayUnoriented, mEqualizedGray);\n        Core.flip(mEqualizedGray, mEqualizedGray, 0);\n        Imgproc.equalizeHist(mEqualizedGray, mEqualizedGray);\n```", "```py\n        final List<Point> featuresList;\n```", "```py\n        mFaceDetector.detectMultiScale(\n                mEqualizedGray, mFaces, SCALE_FACTOR, MIN_NEIGHBORS,\n                FLAGS, mMinSize, mMaxSize);\n```", "```py\n        if (mFaces.rows() > 0) { // Detected at least one face\n\n            // Get the first detected face.\n            final double[] face = mFaces.get(0, 0);\n\n            double minX = face[0];\n            double minY = face[1];\n            double width = face[2];\n            double height = face[3];\n            double maxX = minX + width;\n            double maxY = minY + height;\n\n            // Draw the face.\n            Imgproc.rectangle(\n                    rgba, new Point(mImageWidth - maxY, minX),\n                    new Point(mImageWidth - minY, maxX),\n                    mFaceRectColor);\n```", "```py\n            // Create a mask for the face region.\n            double smallerSide;\n            if (height < width) {\n                smallerSide = height;\n            } else {\n                smallerSide = width;\n            }\n            double maskPadding =\n                    smallerSide * MASK_PADDING_PROPORTIONAL;\n            mMask.setTo(mMaskBackgroundColor);\n            Imgproc.rectangle(\n                    mMask,\n                    new Point(minX + maskPadding,\n                            minY + maskPadding),\n                    new Point(maxX - maskPadding,\n                            maxY - maskPadding),\n                    mMaskForegroundColor, -1);\n\n            // Find features in the face region.\n            Imgproc.goodFeaturesToTrack(\n                    mEqualizedGray, mInitialFeatures, MAX_FEATURES,\n                    MIN_FEATURE_QUALITY, MIN_FEATURE_DISTANCE,\n                    mMask, 3, false, 0.04);\n            mFeatures.fromArray(mInitialFeatures.toArray());\n            featuresList = mFeatures.toList();\n```", "```py\n            if (mWasTrackingFace) {\n                updateGestureDetection();\n            } else {\n                startGestureDetection();\n            }\n            mWasTrackingFace = true;\n```", "```py\n        // if (mFaces.rows > 0) { ... See above ... }\n        } else { // Did not detect any face\n            Video.calcOpticalFlowPyrLK(\n                    mLastEqualizedGray, mEqualizedGray, mLastFeatures,\n                    mFeatures, mFeatureStatuses, mFeatureErrors);\n\n            // Filter out features that are not found or have high\n            // error.\n            featuresList = new LinkedList<Point>(mFeatures.toList());\n            final LinkedList<Byte> featureStatusesList =\n                    new LinkedList<Byte>(mFeatureStatuses.toList());\n            final LinkedList<Float> featureErrorsList =\n                    new LinkedList<Float>(mFeatureErrors.toList());\n            for (int i = 0; i < featuresList.size();) {\n                if (featureStatusesList.get(i) == 0 ||\n                        featureErrorsList.get(i) > MAX_FEATURE_ERROR) {\n                    featuresList.remove(i);\n                    featureStatusesList.remove(i);\n                    featureErrorsList.remove(i);\n                } else {\n                    i++;\n                }\n            }\n```", "```py\n            if (featuresList.size() < MIN_FEATURES) {\n                // The number of remaining features is too low; we have\n                // probably lost the target completely.\n\n                // Discard the remaining features.\n                featuresList.clear();\n                mFeatures.fromList(featuresList);\n\n                mWasTrackingFace = false;\n            } else {\n                mFeatures.fromList(featuresList);\n                updateGestureDetection();\n            }\n        }\n```", "```py\n        // Draw the current features.\n        for (int i = 0; i< featuresList.size(); i++) {\n            final Point p = featuresList.get(i);\n            final Point pTrans = new Point(\n                    mImageWidth - p.y,\n                    p.x);\n            Imgproc.circle(rgba, pTrans, 8, mFeatureColor);\n        }\n```", "```py\n        // Swap the references to the current and previous images.\n        final Mat swapEqualizedGray = mLastEqualizedGray;\n        mLastEqualizedGray = mEqualizedGray;\n        mEqualizedGray = swapEqualizedGray;\n\n        // Swap the references to the current and previous features.\n        final MatOfPoint2f swapFeatures = mLastFeatures;\n        mLastFeatures = mFeatures;\n        mFeatures = swapFeatures;\n```", "```py\n        // Mirror (horizontally flip) the preview.\n        Core.flip(rgba, rgba, 1);\n\n        return rgba;\n    }\n```", "```py\n    private void startGestureDetection() {\n\n        double[] featuresCenter = Core.mean(mFeatures).val;\n\n        // Motion in x may indicate a shake of the head.\n        mShakeHeadGesture.start(featuresCenter[0]);\n\n        // Motion in y may indicate a nod of the head.\n        mNodHeadGesture.start(featuresCenter[1]);\n    }\n```", "```py\n    private void updateGestureDetection() {\n\n        final double[] featuresCenter = Core.mean(mFeatures).val;\n\n        // Motion in x may indicate a shake of the head.\n        mShakeHeadGesture.update(featuresCenter[0]);\n        final int shakeBackAndForthCount =\n                mShakeHeadGesture.getBackAndForthCount();\n        //Log.i(TAG, \"shakeBackAndForthCount=\" +\n        // shakeBackAndForthCount);\n        final boolean shakingHead =\n                (shakeBackAndForthCount >=\n                        MIN_BACK_AND_FORTH_COUNT);\n\n        // Motion in y may indicate a nod of the head.\n        mNodHeadGesture.update(featuresCenter[1]);\n        final int nodBackAndForthCount =\n                mNodHeadGesture.getBackAndForthCount();\n        //Log.i(TAG, \"nodBackAndForthCount=\" +\n        // nodBackAndForthCount);\n        final boolean noddingHead =\n                (nodBackAndForthCount >=\n                        MIN_BACK_AND_FORTH_COUNT);\n\n        if (shakingHead && noddingHead) {\n            // The gesture is ambiguous. Ignore it.\n            resetGestures();\n        } else if (shakingHead) {\n            mAudioTree.takeNoBranch();\n            resetGestures();\n        } else if (noddingHead) {\n            mAudioTree.takeYesBranch();\n            resetGestures();\n        }\n    }\n```", "```py\n    private void resetGestures() {\n        if (mNodHeadGesture != null) {\n            mNodHeadGesture.resetCounts();\n        }\n        if (mShakeHeadGesture != null) {\n            mShakeHeadGesture.resetCounts();\n        }\n    }\n```", "```py\n    private void initFaceDetector() {\n        try {\n            // Load cascade file from application resources.\n\n            InputStream is = getResources().openRawResource(\n                    R.raw.lbpcascade_frontalface);\n            File cascadeDir = getDir(\n                    \"cascade\", Context.MODE_PRIVATE);\n            File cascadeFile = new File(\n                    cascadeDir, \"lbpcascade_frontalface.xml\");\n            FileOutputStream os = new FileOutputStream(cascadeFile);\n\n            byte[] buffer = new byte[4096];\n            int bytesRead;\n            while ((bytesRead = is.read(buffer)) != -1) {\n                os.write(buffer, 0, bytesRead);\n            }\n            is.close();\n            os.close();\n\n            mFaceDetector = new CascadeClassifier(\n                    cascadeFile.getAbsolutePath());\n            if (mFaceDetector.empty()) {\n                Log.e(TAG, \"Failed to load cascade\");\n                finish();\n            } else {\n                Log.i(TAG, \"Loaded cascade from \" +\n                        cascadeFile.getAbsolutePath());\n            }\n\n            cascadeDir.delete();\n\n        } catch (IOException e) {\n            e.printStackTrace();\n            Log.e(TAG, \"Failed to load cascade. Exception caught: \"\n                    + e);\n            finish();\n        }\n    }\n}\n```"]