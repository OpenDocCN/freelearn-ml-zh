["```py\n> heart <- read.table(\"heart.dat\", quote = \"\\\"\")\n> names(heart) <- c(\"AGE\", \"SEX\", \"CHESTPAIN\", \"RESTBP\", \"CHOL\", \"SUGAR\", \"ECG\", \"MAXHR\", \"ANGINA\", \"DEP\", \"EXERCISE\", \"FLUOR\", \"THAL\", \"OUTPUT\")\n```", "```py\n> heart$CHESTPAIN = factor(heart$CHESTPAIN)\n> heart$ECG = factor(heart$ECG)\n> heart$THAL = factor(heart$THAL)\n> heart$EXERCISE = factor(heart$EXERCISE)\n```", "```py\n> heart$OUTPUT = heart$OUTPUT - 1\n```", "```py\n> library(caret)\n> set.seed(987954)\n> heart_sampling_vector <- \n  createDataPartition(heart$OUTPUT, p = 0.85, list = FALSE)\n> heart_train <- heart[heart_sampling_vector,]\n> heart_train_labels <- heart$OUTPUT[heart_sampling_vector]\n> heart_test <- heart[-heart_sampling_vector,]\n> heart_test_labels <- heart$OUTPUT[-heart_sampling_vector]\n```", "```py\n> heart_model <- \n  glm(OUTPUT ~ ., data = heart_train, family = binomial(\"logit\"))\n```", "```py\n> summary(heart_model)\n\nCall:\nglm(formula = OUTPUT ~ ., family = binomial(\"logit\"), data = heart_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7137  -0.4421  -0.1382   0.3588   2.8118  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -7.946051   3.477686  -2.285 0.022321 *  \nAGE         -0.020538   0.029580  -0.694 0.487482    \nSEX          1.641327   0.656291   2.501 0.012387 *  \nCHESTPAIN2   1.308530   1.000913   1.307 0.191098    \nCHESTPAIN3   0.560233   0.865114   0.648 0.517255    \nCHESTPAIN4   2.356442   0.820521   2.872 0.004080 ** \nRESTBP       0.026588   0.013357   1.991 0.046529 *  \nCHOL         0.008105   0.004790   1.692 0.090593 .  \nSUGAR       -1.263606   0.732414  -1.725 0.084480 .  \nECG1         1.352751   3.287293   0.412 0.680699    \nECG2         0.563430   0.461872   1.220 0.222509    \nMAXHR       -0.013585   0.012873  -1.055 0.291283    \nANGINA       0.999906   0.525996   1.901 0.057305 .  \nDEP          0.196349   0.282891   0.694 0.487632    \nEXERCISE2    0.743530   0.560700   1.326 0.184815    \nEXERCISE3    0.946718   1.165567   0.812 0.416655    \nFLUOR        1.310240   0.308348   4.249 2.15e-05 ***\nTHAL6        0.304117   0.995464   0.306 0.759983    \nTHAL7        1.717886   0.510986   3.362 0.000774 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 315.90  on 229  degrees of freedom\nResidual deviance: 140.36  on 211  degrees of freedom\nAIC: 178.36\n\nNumber of Fisher Scoring iterations: 6\n```", "```py\n> pnorm(3.362 , lower.tail = F) * 2\n[1] 0.0007738012\n```", "```py\n> heart_model2 <- glm(OUTPUT ~ AGE, data = heart_train, family = binomial(\"logit\"))\n> summary(heart_model2)\n\nCall:\nglm(formula = OUTPUT ~ AGE, family = binomial(\"logit\"), data = heart_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5027  -1.0691  -0.8435   1.2061   1.6759  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -2.71136    0.86348  -3.140  0.00169 **\nAGE          0.04539    0.01552   2.925  0.00344 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 315.90  on 229  degrees of freedom\nResidual deviance: 306.89  on 228  degrees of freedom\nAIC: 310.89\n\nNumber of Fisher Scoring iterations: 4\n```", "```py\n log_likelihoods <- function(y_labels, y_probs) {\n     y_a <- as.numeric(y_labels)\n     y_p <- as.numeric(y_probs)\n     y_a * log(y_p) + (1 - y_a) * log(1 - y_p)\n }\n\n dataset_log_likelihood <- function(y_labels, y_probs) {\n     sum(log_likelihoods(y_labels, y_probs))\n }\n```", "```py\n deviances <- function(y_labels, y_probs) {\n     -2 * log_likelihoods(y_labels, y_probs)\n }\n\ndataset_deviance <- function(y_labels, y_probs) {\n     sum(deviances(y_labels, y_probs))\n }\n```", "```py\nmodel_deviance <- function(model, data, output_column) {\n  y_labels = data[[output_column]]\n  y_probs = predict(model, newdata = data, type = \"response\")\n  dataset_deviance(y_labels, y_probs)\n}\n```", "```py\n> model_deviance(heart_model, data = heart_train, output_column = \n                 \"OUTPUT\")\n[1] 140.3561\n```", "```py\n null_deviance <- function(data, output_column) {\n     y_labels <- data[[output_column]]\n     y_probs <- mean(data[[output_column]])\n     dataset_deviance(y_labels, y_probs)\n }\n\n> null_deviance(data = heart_training, output_column = \"OUTPUT\")\n[1] 314.3811\n```", "```py\n model_pseudo_r_squared <- function(model, data, output_column) {\n     1 - ( model_deviance(model, data, output_column) / \n           null_deviance(data, output_column) )\n }\n\n> model_pseudo_r_squared(heart_model, data = heart_train, \n                         output_column = \"OUTPUT\")\n[1] 0.5556977\n```", "```py\nmodel_chi_squared_p_value <-  function(model, data, output_column) {\n     null_df <- nrow(data) - 1\n     model_df <- nrow(data) - length(model$coefficients)\n     difference_df <- null_df - model_df\n     null_deviance <- null_deviance(data, output_column)\n     m_deviance <- model_deviance(model, data, output_column)\n     difference_deviance <- null_deviance - m_deviance\n     pchisq(difference_deviance, difference_df,lower.tail = F)\n}\n\n> model_chi_squared_p_value(heart_model, data = heart_train, \n                            output_column = \"OUTPUT\")\n[1] 7.294219e-28\n```", "```py\nmodel_deviance_residuals <- function(model, data, output_column) {\n     y_labels = data[[output_column]]\n     y_probs = predict(model, newdata = data, type = \"response\")\n     residual_sign = sign(y_labels - y_probs)\n     residuals = sqrt(deviances(y_labels, y_probs))\n     residual_sign * residuals\n }\n```", "```py\n> summary(model_deviance_residuals(heart_model, data = \n          heart_train, output_column = \"OUTPUT\"))\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.71400 -0.44210 -0.13820 -0.02765  0.35880  2.81200 \n```", "```py\n> train_predictions <- predict(heart_model, newdata = heart_train, \n                               type = \"response\")\n> train_class_predictions <- as.numeric(train_predictions > 0.5)\n> mean(train_class_predictions == heart_train$OUTPUT)\n[1] 0.8869565\n> test_predictions = predict(heart_model, newdata = heart_test, \n                             type = \"response\")\n> test_class_predictions = as.numeric(test_predictions > 0.5)\n> mean(test_class_predictions == heart_test$OUTPUT)\n[1] 0.9\n```", "```py\n> library(glmnet)\n> heart_train_mat <- model.matrix(OUTPUT ~ ., heart_train)[,-1]\n> lambdas <- 10 ^ seq(8, -4, length = 250)\n> heart_models_lasso <- glmnet(heart_train_mat, \n  heart_train$OUTPUT, alpha = 1, lambda = lambdas, family = \"binomial\")\n> lasso.cv <- cv.glmnet(heart_train_mat, heart_train$OUTPUT, alpha = 1,lambda = lambdas, family = \"binomial\")\n> lambda_lasso <- lasso.cv$lambda.min\n> lambda_lasso\n[1] 0.01057052\n\n> predict(heart_models_lasso, type = \"coefficients\", s = lambda_lasso)\n19 x 1 sparse Matrix of class \"dgCMatrix\"\n                       1\n(Intercept) -4.980249537\nAGE          .          \nSEX          1.029146139\nCHESTPAIN2   0.122044733\nCHESTPAIN3   .          \nCHESTPAIN4   1.521164330\nRESTBP       0.013456000\nCHOL         0.004190012\nSUGAR       -0.587616822\nECG1         .          \nECG2         0.338365613\nMAXHR       -0.010651758\nANGINA       0.807497991\nDEP          0.211899820\nEXERCISE2    0.351797531\nEXERCISE3    0.081846313\nFLUOR        0.947928099\nTHAL6        0.083440880\nTHAL7        1.501844677\n```", "```py\n> lasso_train_predictions <- predict(heart_models_lasso, s = lambda_lasso, newx = heart_train_mat, type = \"response\")\n> lasso_train_class_predictions <- \n  as.numeric(lasso_train_predictions > 0.5)\n> mean(lasso_train_class_predictions == heart_train$OUTPUT)\n[1] 0.8913043\n> heart_test_mat <- model.matrix(OUTPUT ~ ., heart_test)[,-1]\n> lasso_test_predictions <- predict(heart_models_lasso, s = lambda_lasso, newx = heart_test_mat, type = \"response\")\n> lasso_test_class_predictions <- \n  as.numeric(lasso_test_predictions > 0.5)\n> mean(lasso_test_class_predictions == heart_test$OUTPUT)\n[1] 0.925\n```", "```py\n> (confusion_matrix <- table(predicted = train_class_predictions, actual = heart_train$OUTPUT))\n         actual\npredicted   0   1\n        0 118  16\n        1  10  86\n> (precision <- confusion_matrix[2, 2] / sum(confusion_matrix[2,]))\n[1] 0.8958333\n> (recall <- confusion_matrix[2, 2] / sum(confusion_matrix[,2]))\n[1] 0.8431373\n> (f = 2 * precision * recall / (precision + recall))\n[1] 0.8686869\n```", "```py\n> (specificity <- confusion_matrix[1,1]/sum(confusion_matrix[1,]))\n[1] 0.880597\n```", "```py\n> library(ROCR)\n> train_predictions <- predict(heart_model, newdata = heart_train, type = \"response\")\n> pred <- prediction(train_predictions, heart_training$OUTPUT)\n> perf <- performance(pred, measure = \"prec\", x.measure = \"rec\")\n```", "```py\n> thresholds <- data.frame(cutoffs = perf@alpha.values[[1]], recall = perf@x.values[[1]], precision = perf@y.values[[1]])\n> subset(thresholds,(recall > 0.9) & (precision > 0.8))\n      cutoffs    recall precision\n112 0.3491857 0.9019608 0.8288288\n113 0.3472740 0.9019608 0.8214286\n114 0.3428354 0.9019608 0.8141593\n115 0.3421438 0.9019608 0.8070175\n```", "```py\n> glass <- read.csv(\"glass.data\", header = FALSE)\n> names(glass) <- c(\"id\",\"RI\",\"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \n                    \"Ba\", \"Fe\", \"Type\")\n> glass <- glass[,-1]\n```", "```py\n> set.seed(4365677)\n> glass_sampling_vector \n   <- createDataPartition(glass$Type, p = 0.80, list = FALSE)\n> glass_train <- glass[glass_sampling_vector,]\n> glass_test <- glass[-glass_sampling_vector,]\n```", "```py\n> library(nnet)\n> glass_model <- multinom(Type ~ ., data = glass_train, maxit = 1000)\n> summary(glass_model)\nCall:\nmultinom(formula = Type ~ ., data = glass_train, maxit = 1000)\n\nCoefficients:\n  (Intercept)         RI         Na         Mg          Al\n2   52.259841  229.29126 -3.3704788  -5.975435  0.07372541\n3  596.591193 -237.75997 -1.2230210  -2.435149 -0.65752347\n5   -1.107583  -22.94764 -0.7434635  -4.244450  8.39355868\n6   -7.493074  -11.83462 11.7893062  -6.383788 35.54561277\n7  -55.888124  442.23590 -2.5269178 -10.479849  1.35983136\n          Si            K         Ca          Ba           Fe\n2 -4.0428142   -3.4934439 -4.6096363   -6.319183    3.2295218\n3 -2.6703131   -4.1221815 -1.7952780   -3.910554    0.2818498\n5  0.6992306   -0.2149109 -0.8790202   -4.642283    4.3379314\n6 -2.2672275 -138.1047925  0.9011624 -161.700857 -200.9598019\n7 -6.5363409   -7.5444163 -8.5710078   -4.087614  -67.9907347\n\nStd. Errors:\n  (Intercept)         RI         Na        Mg       Al        Si\n2  0.03462075 0.08068713  0.5475710 0.7429120 1.282725 0.1392131\n3  0.05425817 0.08750688  0.7339134 0.9173184 1.544409 0.1805758\n5  0.06674926 0.11759231  1.0866157 1.4062285 2.738635 0.3225212\n6  0.17049665 0.28791033 17.2280091 4.9726046 2.622643 4.3385330\n7  0.06432732 0.10522206  2.2561142 1.5246356 3.244288 0.4733835\n           K        Ca           Ba         Fe\n2 1.98021049 0.4897356 1.473156e+00 2.45881312\n3 2.35233054 0.5949799 4.222783e+00 3.45835575\n5 2.78360034 0.9807043 5.471887e+00 5.52299959\n6 0.02227295 7.2406622 1.656563e-08 0.01779519\n7 3.25038195 1.7310334 4.381655e+00 0.28562065\n\nResidual Deviance: 219.2651 \nAIC: 319.2651\n```", "```py\n> glass_predictions <- predict(glass_model, glass_train)\n> mean(glass_predictions == glass_train$Type)\n[1] 0.7209302\n```", "```py\n> table(predicted = glass_predictions, actual = glass_train$Type)\n         actual\npredicted  1  2  3  5  6  7\n        1 46 17  8  0  0  0\n        2 13 40  6  2  0  1\n        3  0  0  0  0  0  0\n        5  0  1  0  7  0  0\n        6  0  0  0  0  7  0\n        7  0  0  0  0  0 24\n```", "```py\n> glass_test_predictions <- predict(glass_model, glass_test)\n> mean(glass_test_predictions == glass_test$Type)\n[1] 0.6428571\n> table(predicted = glass_test_predictions, actual = \n        glass_test$Type)\n         actual\npredicted  1  2  3  5  6  7\n        1  7  2  2  0  0  0\n        2  4 15  1  2  0  0\n        3  0  0  0  0  0  0\n        5  0  0  0  1  0  2\n        6  0  0  0  0  2  0\n        7  0  1  0  1  0  2\n```", "```py\n> wine <- read.csv(\"winequality-white.csv\", sep = \";\")\n> wine$quality <- factor(ifelse(wine$quality < 5, 0,                     \n                         ifelse(wine$quality > 6, 2, 1)))\n```", "```py\n> set.seed(7644)\n> wine_sampling_vector <- createDataPartition(wine$quality, p = \n                          0.80, list = FALSE)\n> wine_train <- wine[wine_sampling_vector,]\n> wine_test <- wine[-wine_sampling_vector,]\n```", "```py\n> library(MASS)\n> wine_model <- polr(quality ~ ., data = wine_train, Hess = T)\n> summary(wine_model)\nCall:\npolr(formula = quality ~ ., data = wine_train, Hess = T)\n\nCoefficients:\n                          Value Std. Error    t value\nfixed.acidity         4.728e-01   0.055641     8.4975\nvolatile.acidity     -4.211e+00   0.435288    -9.6741\ncitric.acid           9.896e-02   0.353466     0.2800\nresidual.sugar        3.386e-01   0.009835    34.4248\nchlorides            -2.891e+00   0.116025   -24.9162\nfree.sulfur.dioxide   1.176e-02   0.003234     3.6374\ntotal.sulfur.dioxide -1.618e-04   0.001384    -0.1169\ndensity              -7.534e+02   0.625157 -1205.1041\npH                    3.107e+00   0.301434    10.3087\nsulphates             2.199e+00   0.338923     6.4873\nalcohol               2.883e-02   0.041479     0.6951\n\nIntercepts:\n    Value      Std. Error t value   \n1|2  -736.9784     0.6341 -1162.3302\n2|3  -731.4177     0.6599 -1108.4069\n\nResidual Deviance: 4412.75 \nAIC: 4438.75 \n```", "```py\n> prop.table(table(wine$quality))\n\n         1          2          3 \n0.03736219 0.74622295 0.21641486\n```", "```py\n> wine_predictions <- predict(wine_model, wine_train)\n> mean(wine_predictions == wine_train$quality)\n[1] 0.7647359\n> table(predicted = wine_predictions,actual = wine_train$quality)\n         actual\npredicted    1    2    3\n        1    4    1    0\n        2  141 2764  619\n        3    2  159  229\n```", "```py\n> wine_test_predictions <- predict(wine_model, wine_test)\n> mean(wine_test_predictions == wine_test$quality)\n[1] 0.7681307\n> table(predicted = wine_test_predictions, \n           actual = wine_test$quality)\n         actual predicted   \n         1   2   3\n        1   2   2   0\n        2  33 693 155\n        3   1  36  57\n```", "```py\n> wine_model2 <- multinom(quality ~ ., data = wine_train, \n                          maxit = 1000)\n> wine_predictions2 <- predict(wine_model2, wine_test)\n> mean(wine_predictions2 == wine_test$quality)\n[1] 0.7630235\n> table(predicted = wine_predictions2, actual = wine_test$quality)\n         actual\npredicted   1   2   3\n        1   2   2   0\n        2  32 682 149\n        3   2  47  63\n```", "```py\n> AIC(wine_model)\n[1] 4438.75\n> AIC(wine_model2)\n[1] 4367.448\n```", "```py\nFootball: M (SD) = 10.65 (8.20)\nTrack: M (SD) = 6.93 (7.45)\nField Hockey: M (SD) = 2.67 (3.73)\nVolleyball: M (SD) = 1.67 (1.73)\n```"]