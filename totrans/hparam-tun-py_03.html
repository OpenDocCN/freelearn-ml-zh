<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-26"><em class="italic"><a id="_idTextAnchor025"/><a id="_idTextAnchor026"/>Chapter 2:</em> Introducing Hyperparameter Tuning</h1>
			<p>Every <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) project should have a clear goal and success metrics. The success metrics can be in the form of business and/or technical metrics. Evaluating business metrics is hard, and often, they can only be evaluated after the ML model is in production. On the other hand, evaluating technical metrics is more straightforward and can be done during the development phase. We, as ML developers, want to achieve <em class="italic">the best technical metrics that we can get</em> since this is something that we can optimize.</p>
			<p>In this chapter, we'll learn one out of several ways to optimize the chosen technical metrics, called <strong class="bold">hyperparameter tuning</strong>. We will start this chapter by understanding what hyperparameter tuning is, along with its goal. Then, we'll discuss the difference between a hyperparameter and a parameter. We'll also learn the concept of hyperparameter space and possible distributions of hyperparameter values that you may find in practice. </p>
			<p>By the end of this chapter, you will understand the concept of hyperparameter tuning and hyperparameters themselves. Understanding these concepts is crucial for you to get a bigger picture of what will be discussed in the next chapters.</p>
			<p>In this chapter, we'll be covering the following main topics:</p>
			<ul>
				<li>What is hyperparameter tuning?</li>
				<li>Demystifying hyperparameters versus parameters</li>
				<li>Understanding hyperparameter space and distributions</li>
			</ul>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor027"/>What is hyperparameter tuning?</h1>
			<p><strong class="bold">Hyperparameter tuning</strong> is a process <a id="_idIndexMarker047"/>whereby we search for the best set of hyperparameters of an ML model from all of the candidate sets. It is the process of optimizing the technical metrics we care about. The goal of hyperparameter tuning is simply to get the <em class="italic">maximum evaluation score</em> on the validation set <em class="italic">without causing an overfitting issue</em>. </p>
			<p>Hyperparameter tuning is <a id="_idIndexMarker048"/>one of the <em class="italic">model-centric</em> approaches to optimizing a model's performance. In practice, it is suggested to <em class="italic">prioritize data-centric</em> approaches over a model-centric approach when it comes to optimizing a model's performance. Data-centric <a id="_idIndexMarker049"/>means that we are focusing on cleaning, sampling, augmenting, or modifying the data, while model-centric means that we are focusing on the model and its configuration.</p>
			<p>To understand why data-centric is prioritized over model-centric, let's say you are a cook in a restaurant. When it comes to cooking, no matter how expensive and fancy your kitchen setups are, if the ingredients are not in a good condition, it's impossible to serve high-quality food to your customers. In that analogy, ingredients refer to the data, and kitchen setups refer to the model and its configuration. No matter how fancy and complex our model is, if we do not have good data or features in the first place, then we can't achieve <a id="_idIndexMarker050"/>the maximum evaluation score. This is expressed in the famous saying, <strong class="bold">garbage in, garbage out</strong> (<strong class="bold">GIGO</strong>).</p>
			<p>In model-centric approaches, hyperparameter tuning is performed after we have found the most suitable <a id="_idIndexMarker051"/>model framework or architecture. So, it can be said that <em class="italic">hyperparameter tuning is the ultimate step</em> in optimizing the model's performance.</p>
			<p>Now that you are aware of hyperparameter tuning and its purpose, let's discuss hyperparameters themselves What actually is a hyperparameter? What is the difference between hyperparameters and parameters? We will discuss this in the next section.</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor028"/>Demystifying hyperparameters versus parameters</h1>
			<p>The <em class="italic">key difference</em> between <a id="_idIndexMarker052"/>a hyperparameter <a id="_idIndexMarker053"/>and a parameter is how its value is generated. A <strong class="bold">parameter</strong> value is <a id="_idIndexMarker054"/>generated by the model during the model-training phase. In other words, its value is <em class="italic">learned from the given data</em> instead <a id="_idIndexMarker055"/>of given by the developer. On the other hand, a <strong class="bold">hyperparameter</strong> value is <em class="italic">given by the developer</em> since it can't be estimated from the data. </p>
			<p>Parameters are like the heart of the model. Poorly estimated parameters will result in a poorly performing model. In fact, when we said we are training a model, it actually means that we are providing the data to the model so that the model can estimate the value of its parameters, which is usually done by performing some kind of optimization algorithm. Here are several examples of parameters in ML:</p>
			<ul>
				<li>Coefficients (<img src="image/Formula_B18753_02_001.png" alt=""/>) in linear regression</li>
				<li>Weights (<img src="image/Formula_B18753_02_002.png" alt=""/>) in <a id="_idIndexMarker056"/>a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>)</li>
			</ul>
			<p>Hyperparameters, on the other hand, are a set of values that support the model-training process. They <a id="_idIndexMarker057"/>are defined by the developer without <a id="_idIndexMarker058"/>knowing the exact impact on the model's performance. That's why we need to perform hyperparameter tuning to get the best out of our model. The searching process can be done via exhaustive search, heuristic search, Bayesian optimization, or multi-fidelity optimization, which will be discussed in the following chapters. Here are several examples of hyperparameters:</p>
			<ul>
				<li>Dropout rate, number <a id="_idIndexMarker059"/>of epochs, and batch size in a <strong class="bold">neural network</strong> (<strong class="bold">NN</strong>)</li>
				<li>Maximum depth and splitting criterion in a decision tree</li>
				<li>Number of estimators in a random forest</li>
			</ul>
			<p>You also need to be aware that there are models without hyperparameters or parameters, but not both of them. For instance, a linear regression model is a model that has <a id="_idIndexMarker060"/>only parameters but doesn't have any hyperparameters. On the other hand, <strong class="bold">K-Nearest Neighbors</strong> (<strong class="bold">KNN</strong>) is an instance of a model that doesn't contain any parameters but has a <em class="italic">k</em> hyperparameter. </p>
			<p>More possible confusion may appear when we start writing our code and developing the ML model. In programming, arguments in a particular function or class are also often called parameters. What if we utilize a class that implements an ML model, such as a decision-tree model? What should we call the maximum depth or splitting criterion arguments that need to be passed to the class? Are they parameters or hyperparameters? The correct answer is <em class="italic">both</em>! They are <em class="italic">parameters to the class</em> and they are <em class="italic">hyperparameters to the decision-tree model</em>. It's just a matter of perspective! </p>
			<p>In this section, we <a id="_idIndexMarker061"/>have learned what hyperparameters <a id="_idIndexMarker062"/>and parameters are, as well as what makes them different. In the next section, we will dive deeper into the realm of hyperparameters. </p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor029"/>Understanding hyperparameter space  and distributions</h1>
			<p><strong class="bold">Hyperparameter space</strong> is defined <a id="_idIndexMarker063"/>as the universal set of possible hyperparameter value combinations—in other words, it is the space containing all possible hyperparameter values that will be used as the search space during the hyperparameter-tuning phase. That's why it is also often called the hyperparameter-tuning <strong class="bold">search space</strong>. This <a id="_idIndexMarker064"/>space is <em class="italic">predefined</em> before the hyperparameter-tuning phase so that the search will be performed only on this space.</p>
			<p>For example, we want <a id="_idIndexMarker065"/>to perform hyperparameter tuning on a NN. Let's say we want to search what is the best value for the dropout rate, the number of epochs, and batch-size hyperparameters. </p>
			<p>The dropout rate is bounded in nature. Its value can only be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, while for the number of epochs and batch-size hyperparameters, in theory, we can specify any positive integer value. However, there are other considerations that we need to think of. A higher batch size will usually produce a better model performance, but it will be bounded by the memory size of our physical computer. As for the number of epochs, if we go with too high a value, we will more likely be faced with an overfitting issue. That's why we need to <em class="italic">set boundaries</em> for the values of possible hyperparameters, which we call the hyperparameter space. </p>
			<p>Hyperparameters can be in the form of <em class="italic">discrete or continuous</em> values. A discrete hyperparameter can be in the form of integer or string data types, while a continuous hyperparameter will always be in the form of real numbers or floating data types. </p>
			<p>When defining a hyperparameter space, for <em class="italic">some hyperparameter-tuning methods</em>, it is not enough to only specify the possible values of each hyperparameter we care about. We also need to <a id="_idIndexMarker066"/>define what is the underlying <strong class="bold">distribution</strong> for each hyperparameter. Here, a distribution acts as some kind of <em class="italic">policy</em> that rules how likely it is that a specific value will be tested during the hyperparameter-tuning phase. If it is a uniform distribution, then all possible values have the same probability of being chosen. </p>
			<p>There are many types of probability distributions that can be used: uniform, log-uniform, normal, log-normal, and many more. There is no best practice when it comes to choosing the appropriate distribution; you can just treat it as another hyperparameter. It is worth <a id="_idIndexMarker067"/>noting that there are distributions specifically for continuous hyperparameters, and there are also distributions for discrete ones. For discrete hyperparameter distribution, some distributions are specifically designed for discrete values—for instance, an integer uniform distribution—but there are also distributions that are adjusted from a continuous distribution. The latter types of discrete distributions usually have a <em class="italic">discretized</em> or <em class="italic">quantized</em> prefix on their name—for instance, a quantized uniform distribution.</p>
			<p>It is also worth noting that <em class="italic">not all hyperparameters are equally significant</em> when it comes to impacting the model's performance—that's why it is recommended that you prioritize. We do not have to perform hyperparameter tuning on all of the hyperparameters of a model—just <em class="italic">focus on more important hyperparameters</em>.</p>
			<p>In this section, we have learned about hyperparameter space and the concept of a hyperparameter distribution and looked at examples of hyperparameter distributions you may find in practice.  </p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor030"/>Summary</h1>
			<p>In this chapter, we have learned all we need to know about hyperparameter tuning, starting from what it is, what is its goal, and when we should perform hyperparameter tuning. We have also discussed the difference between hyperparameters and parameters, the concept of hyperparameter space, and the concept of hyperparameter distributions. Having a clear picture of the concept of hyperparameter tuning and  hyperparameters themselves will help you a lot in the following chapters.</p>
			<p>As stated previously, we will discuss all of the four categories of hyperparameter-tuning methods in this book. In <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Exhaustive Search</em>, we will start discussing the first group and the most widely used hyperparameter-tuning methods in practice. There will be both high-level and detailed explanations to help you understand each of the methods more easily.</p>
		</div>
	</body></html>