- en: Grouping with Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common and introductory unsupervised learning problem is that of *clustering.*
    Often, you have large datasets that you wish to organize into smaller groups,
    or wish to break up into logically similar groups. For instance, you can try to
    divide census data of household incomes into three groups: low, high, and super
    rich. If you feed the household income data into a clustering algorithm, you would
    expect to see three data points as a result, with each corresponding to the average
    value of your three categories. Even this one-dimensional problem of clustering
    household incomes may be difficult to do by hand, because you might not know where
    one group should end and the other should begin. You could use governmental definitions
    of income brackets, but there''s no guarantee that those brackets are geometrically
    balanced; they were invented by policymakers and may not accurately represent
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: A *cluster* is a group of logically similar data points. They can be users with
    similar behavior, citizens with similar income ranges, pixels with similar colors,
    and so on. The k-means algorithm is numerical and geometric, so the clusters it
    identifies will all be numerically similar, with data points that are geometrically
    close to one another. Fortunately, most data can be represented numerically, so
    the k-means algorithm is useful for many different problem domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm is a powerful, fast, and popular clustering algorithm
    for numerical data. The name k-means is comprised of two parts: ***k***, which
    represents the number of clusters that we want the algorithm to find, and ***means,***
    which is the method of determining where those cluster centers are (you could,
    for instance, also use k-medians or k-modes). Translated into plain English, we
    might ask the algorithm to find us three cluster centers that are the mean values
    of the points they represent. In that case, *k = 3* and we can tell our bosses
    that we did a k-means analysis with *k = 3* when filing our report.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm is an iterative algorithm, which means it runs a loop
    and continually updates its model until the model reaches steady state, at which
    point it will return its results. Put into narrative form, the k-means algorithm
    works like this: plot the data that you wish to analyze, and pick a value for
    *k*. You must know the value of *k* beforehand, or at least have an idea of what
    it should be (though we''ll also explore a way around this later in the chapter).
    Randomly create *k* points (if *k = 5*, create five points) and add them to your
    plot; these points are called the **centroids**, as they will ultimately represent
    the geometric centers of the clusters. For each data point in the plot, find the
    centroid closest to that point and connect or assign it to the point. Once all
    the assignments have been made, look at each centroid in turn and update its position
    to the mean position of all the points assigned to it. Repeat the assign-then-update
    procedure until the centroids stop moving; these final positions of the centroids
    are the output of the algorithm, and can be considered your cluster centers. If
    the narrative is hard to follow, don''t worry, we''ll dig into it more deeply
    as we build this algorithm from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll first discuss the concepts of average and distance
    and how they apply to the k-means algorithm. Then we''ll describe the algorithm
    itself and build a JavaScript class from scratch to implement the k-means algorithm.
    We''ll test our k-means solver with a couple of simple datasets, and then discuss
    what to do when you don''t know the value of *k* beforehand. We''ll build another
    tool that automates the discovery of the value *k*. We''ll also discuss what the
    concept of *error* means for k-means applications, and how to design an error
    algorithm that helps us achieve our goals. The following are the topics that will
    be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Average and distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing the k-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 1—k-means on simple 2D data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 2—3D data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means where *k* is unknown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average and distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The k-means algorithm relies on two concepts in order to operate: average and
    distance. In order to tell you where the center of a cluster is, the algorithm
    will calculate an average value for these points. In this case, we will use the
    arithmetic mean, or the sum of values divided by the number of values, to represent
    our average. In ES5/classic JavaScript (I''m also being purposefully explicit
    in this example, for any readers who are not familiar with calculating the mean),
    we might write a function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In ES6, we can abuse our shorthand privileges and write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a handy ES6 one-liner to keep in your back pocket, however, it assumes
    all values are already numeric and defined, and it will return NaN if you give
    it an empty array. If the shorthand is confusing, we can break it up like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind we can use any type of average, including the median and mode.
    In fact, it's sometimes preferable to use k-medians over k-means. The median does
    a better job of muting outliers than the mean does. You should therefore always
    ask yourself which average you actually need. If you want a representation of
    total resources consumed, for instance, you might use the arithmetic mean. If
    you suspect outliers are caused by faulty measurements and should be ignored,
    k-medians could suit you better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need a concept of distance in this algorithm. It can be any distance
    measure, however, for numeric data you will mostly use the typical Euclidean distance—the
    standard distance measure you''d learn in high school—which in ES5 JavaScript
    looks like this for two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We must support many more than two dimensions, however, so we can generalize
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can write an ES6 one-liner for this, but it won''t be as readable as the
    lengthier, explicit example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Armed with these tools, we can start writing the k-means algorithm itself.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The k-means algorithm is relatively simple to implement, so in this chapter
    we''ll write it from scratch. The algorithm requires only two pieces of information:
    the *k* in k-means (the number of clusters we wish to identify), and the data
    points to evaluate. There are additional parameters the algorithm can use, for
    example, the maximum number of iterations to allow, but they are not required.
    The only required output of the algorithm is *k* centroids, or a list of points
    that represent the centers of the clusters of data. If *k = 3*, then the algorithm
    must return three centroids as its output. The algorithm may also return other
    metrics, such as the total error, the total number of iterations required to reach
    steady state, and so on, but again these are optional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A high-level description of the k-means algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the parameter *k* and the data to process, initialize *k* candidate centroids
    randomly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each data point, determine which candidate centroid is closest to that point
    and *assign* the point to that centroid
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each centroid, update its position to be the mean position of all the points
    assigned to it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Step 2* and *Step 3* until the centroids' positions reach steady state
    (that is, the centroids stop moving)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this process, you may return the positions of the centroids as
    the algorithm's output.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a moment to set up our development environment for this algorithm.
    The environment will be as described in [Chapter 1](370a39e1-e618-475d-9722-9b4315a9a6ee.xhtml), *Exploring
    the Potential of JavaScript,* however, we'll run through the entire process here.
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new folder for this project. I've named the folder `Ch4-kmeans`.
    Create a subfolder called `src` inside `Ch4-kmeans`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add a file called `package.json` to the `Ch4-kmeans` folder. Add the
    following content to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After creating the `package.json` file, switch to your terminal program and
    (from the `Ch4-kmeans` folder) run the `yarn install` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create three new files inside the `Ch4-kmeans/src` folder: `index.js`,
    `data.js`, and `kmeans.js`. We will write the actual k-means algorithm inside
    `kmeans.js`, we will load some example data into `data.js`, and we''ll use `index.js`
    as our bootstrapping point to set up and run a number of examples.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may want to stop and test that everything is working. Add
    a simple `console.log("Hello");` to `index.js` and then run the command `yarn
    start` from the command line. You should see the file compile and run, printing
    `Hello` to the screen before exiting. If you get errors or do not see the `Hello`,
    you may want to take a step back and double-check your environment. If everything
    is working, you can delete the `console.log("Hello");` from `index.js`.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll be working in the `kmeans.js` file. The first thing
    to do is to add our functions for mean and distance to the top of the file. Since
    these are generic functions that can be called **statistically**, we will not
    define them inside a class. Add the following to the top of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create and export a `KMeans` class. We will fill this in with many more
    methods throughout this chapter, but let''s start with the following. Add this
    to the `kmeans.js` file beneath the code you just added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We've created a class called `KMeans` and are exporting it as the default export
    for this file. The preceding code also initializes some of the instance variables
    that the class will need, which we will describe shortly.
  prefs: []
  type: TYPE_NORMAL
- en: The constructor for the class takes two parameters, `k` and `data`, and stores
    both as instance variables. The `k` parameter represents the `k` in k-means, or
    the desired number of clusters as the algorithm's output. The `data` parameter
    is an array of data points that the algorithm will process.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the constructor, we call the `reset()` method, which is used
    to initialize (or reset) the solver''s state. Specifically, the instance variables
    we initialize in the `reset` method are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iterations`, which is a simple counter of how many iterations the solver has
    run, starting from 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`error`, which records the **root mean square error** (**RMSE**) of the points''
    distance to their centroids for the current iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`centroidAssignments`, which is an array of data point index numbers that map
    to a centroid index number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`centroids`, which will store the solver''s candidates for the *k* centroids
    at the current iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that in the `reset` method, we're making a call to `this.initRandomCentroids()`,
    which we have not yet defined. The k-means algorithm must start with a set of
    candidate centroids, so the purpose of that method is to generate the correct
    number of centroids randomly. Because the algorithm starts with a random state,
    it can be expected that multiple runs of the algorithm will return different results
    based on the initial conditions. This is actually a desired property of the k-means
    algorithm, because it is susceptible to finding local optima, and running the
    algorithm multiple times with different initial conditions may help you find the
    global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have some prerequisites to satisfy before we can generate our random centroids.
    First, we must know the dimensionality of the data. Are we working with 2D data,
    3D data, 10D data, or 1324D data? The random centroids we generate must have the
    same number of dimensions as the rest of the data points. This is an easy problem
    to solve; we assume that all the data points have the same number of dimensions,
    so we can just inspect the first data point we encounter. Add the following method
    to the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The other consideration we must make when generating random initial centroids
    is that the centroids should be close to the data that we're working with. For
    instance, if all your data points are points between (0, 0) and (10, 10), you
    would not want to generate a random centroid such as (1200, 740). Similarly, if
    your data points are all negative, you would not want to generate positive centroids,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Why should we care where the random centroids start? In this algorithm, points
    will be assigned to the centroid closest to it and gradually *pull* the centroid
    towards the cluster center. If the centroids are all to the right of the data
    points, then the centroids themselves will follow similar paths towards the data
    and may get all clumped together in one single cluster, converging to a local
    optimum. By making sure that the centroids are randomly distributed within the
    range of the data, we have a better chance of avoiding this type of local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach to generating our centroid starting positions will be to determine
    the range of each dimension of the data, and then choose random values for our
    centroid''s position within those ranges. For instance, imagine three two-dimensional
    data points in an *x, **y* plane: (1, 3), (5, 8), and (3, 0). The range of the
    *x *dimension lies between 1 and 5, while the range of the *y* dimension lies
    between 0 and 8\. Therefore, when creating a randomly initialized centroid, we
    will choose a random number between 1 and 5 for its *x* position, and a random
    number between 0 and 8 for its *y* position.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use JavaScript''s `Math.min` and `Math.max` to determine the data ranges
    for each dimension. Add the following method to the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This method first collects all the values of the given dimension from the data
    points as an array and then returns an object containing that range''s `min` and
    `max`. Returning to our preceding example of three data points ((1, 3), (5, 8),
    and (3, 0)), calling `getRangeForDimension(0)` would return `{min: 1, max: 5}`,
    and calling `getRangeForDimension(1)` would return `{min: 0, max: 8}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be useful for us to have an object of all dimensions and their ranges
    that we can cache while initializing centroids, so add the following method to
    the `KMeans` class as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This method simply looks at all dimensions and returns the `min` and `max` ranges
    for each, structured as objects in an array indexed by the dimension. This method
    is primarily a convenience, but we will use it shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finally generate our randomly initialized centroids. We will need to
    create *k* centroids, and work dimension by dimension to choose a random point
    inside the range of each dimension. Add the following method to the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding algorithm contains two loops; the outer loop creates `k` candidate
    centroids. Because the number of dimensions in the dataset is arbitrary, and because
    each dimension itself has an arbitrary range, we must then work dimension by dimension
    for each centroid in order to generate a random position. If your data is three-dimensional,
    the inner loop will consider dimensions 0, 1, and 2 separately, determining the
    `min` and `max` values for each dimension, choosing a random value in that range,
    and assigning that value to that specific dimension of the centroid point.
  prefs: []
  type: TYPE_NORMAL
- en: Testing random centroid generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already written a significant amount of code, so now would be a good time
    to stop and test our work. We should also start setting up our `data.js` file
    that we'll use to hold some example data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the `data.js` file and add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The values used are the same ones from our simple data point example written
    in the preceding block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, switch to `index.js` and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, we import the `KMeans` class and the `example_data` object from their
    respective files. We print some helpful output to the screen, and then initialize
    a `KMeans` solver instance for our simple data. We can check the randomly initialized
    centroids by inspecting the value of `ex_randomCentroids_solver.centroids`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve added this code, run `yarn start` from the command line, and you
    should see something similar to the following output. Note that because the centroid
    initialization is random, you will not see the same values that I see; however,
    what we''re looking for is to make sure that the random centroids lie within the
    correct ranges. Specifically, we want our centroids to have *x* positions between
    1 and 5, and *y* positions between 0 and 8\. Run the code a number of times to
    make sure that the centroids have the correct position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you see something similar to the preceding block, that means everything is
    working well so far and we're ready to continue implementing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning points to centroids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The iteration loop that the k-means algorithm performs has two steps: assigning
    each point to the centroid closest to it, and then updating the location of the
    centroids to be the mean value of all the points assigned to that centroid. In
    this section, we''ll implement the first part of the algorithm: assigning points
    to centroids.'
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, our task is to consider each point in the dataset and determine
    which centroid is closest to it. We also need to record the results of this assignment
    so that we can later update the centroid's location based on the points assigned
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following method to the body of the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This method considers a single data point, given by its index, and considers
    each centroid in the system in turn. We also keep track of the last centroid this
    point has been assigned to in order to determine if the assignment has changed.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we loop over all centroids and use our `distance` function
    to determine the distance between the point and the centroid. If the distance
    is less than the lowest distance we've seen so far, or if this is the first centroid
    we're considering for this point (`minDistance` will be null in that case), we
    record the distance and the index position of the centroid. After looping over
    all centroids, we will now know which centroid is closest to the point under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we record the assignment of the centroid to the point by setting it
    to the `this.centroidAssignments` array—in this array, the index is the index
    of the point, and the value is the index of the centroid. We return a Boolean
    from this method by comparing the last known centroid assignment to the new centroid
    assignment—it will return `true` if the assignment has changed, or `false` if
    not. We'll use this information to figure out when the algorithm has reached steady
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous method considers only a single point, so we should also write
    a method to process the centroid assignments of all points. Additionally, the
    method we write should determine if *any* point has updated its centroid assignment.
    Add the following to the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This method defines a variable called `didAnyPointsGetReassigned`, initialized
    to `false`, and then loops over all points in the dataset to update their centroid
    assignments. If any point is assigned to a new centroid, the method will return
    `true`. If no assignments were changed, the method returns `false`. The return
    value from this method will become one of our termination conditions; if no points
    update after an iteration, we can consider the algorithm to have reached steady
    state and can terminate it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now address the second part of the k-means algorithm: updating centroid
    locations.'
  prefs: []
  type: TYPE_NORMAL
- en: Updating centroid locations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we implemented the first part of the k-means algorithm:
    looking at all points in the dataset and assigning them to the centroid that''s
    geographically closest. The next step in the algorithm is to look at all centroids
    and update their locations to the mean value of all the points assigned to them.'
  prefs: []
  type: TYPE_NORMAL
- en: To make an analogy, you can imagine each point reaching out and grabbing the
    centroid closest to it. The points give the centroid a tug, trying to pull it
    closer to them. We've already implemented the *reach out and grab portion* of
    the algorithm, and now we'll implement the *pull the centroid closer* portion.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, our task is to loop over all the centroids, and for each, determine
    the mean position of all the points assigned to it. We'll then update the centroid's
    position to that mean value. Breaking this down further, we must first collect
    all the points assigned to a centroid, and then we have to calculate the average
    value of these points, always keeping in mind that points can have any number
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the easy task of collecting all the points assigned to a
    centroid. We already have a mapping of point indexes to centroid indexes in our
    `this.centroidAssignments` instance variable. Add the following code to the body
    of the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding method is quite standard: looping over all data points, we look
    up that point''s centroid assignment, and if it is assigned to the centroid in
    question, we add the point to an output array.'
  prefs: []
  type: TYPE_NORMAL
- en: We can now use this list of points to update the centroid's location. Our goal
    is to update the centroid's location to be the mean value of all the points we
    found previously. Because the data may be multi-dimensional, we must also consider
    each dimension independently.
  prefs: []
  type: TYPE_NORMAL
- en: Using our simple example of points (1, 3), (5, 8), and (3, 0), we would find
    a mean location of (3, 3.6667). To get this value, we first calculate the mean
    value of the *x* dimension ( (1 + 5 + 3) / 3 = 3 ), and then calculate the mean
    value of the *y* dimension ( (3 + 8 + 0) / 3 = 11/3 = 3.6666... ). If we're working
    in more than two dimensions, we simply repeat the procedure for each dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this algorithm in JavaScript. Add the following to the body of
    the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding method considers only one centroid at a time, specified by its
    index. We use the `getPointsForCentroid` method that we just added to get an array
    of points assigned to this centroid. We initialize a variable called `newCentroid`
    as an empty array; this will ultimately replace the current centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Considering one dimension at a time, we collect the positions of the points
    *in that dimension only,* and then calculate the mean. We use JavaScript's `Array.map`
    to extract the positions for the correct dimension only, and then we use our `mean`
    function to calculate the average position in that dimension.
  prefs: []
  type: TYPE_NORMAL
- en: If we work this example by hand with the data points (1, 3), (5, 8), and (3,
    0), we start by examining dimension 0, or the *x* dimension. The result of `thisCentroidPoints.map(point
    => point[dimension])` is the array `[1, 5, 3]` for dimension 0, and for dimension
    1, the result is `[3, 8, 0]`. Each of these arrays is passed to the `mean` function,
    and the mean value is used in `newCentroid` for that dimension.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this method, we update our array of `this.centroids` with the
    newly calculated centroid position.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also write a convenience method to loop over all centroids and update
    their positions. Add the following to the body of the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Before finishing the algorithm and tying together all the pieces, we have one
    final prerequisite to satisfy. We're going to introduce the concept of *error*
    into the algorithm. Calculating the error is not required for the k-means algorithm
    to function, but you'll see later that this can be advantageous in certain situations.
  prefs: []
  type: TYPE_NORMAL
- en: Because this is an unsupervised learning algorithm, our concept of error does
    not relate to semantic error. Instead, we will use an error metric that represents
    the average distance of all points from their assigned centroids. We'll use the
    RMSE for this, which penalizes bigger distances more harshly, so our error metric
    will be a good indication of how tight the clustering is.
  prefs: []
  type: TYPE_NORMAL
- en: To perform this error calculation, we loop over all points and determine the
    distance of that point from its centroid. We square each distance before adding
    it to a running total (the *squared* in root-mean-squared), then divide the running
    total by the number of points (the *mean* in root-mean-squared), and finally take
    the square root of the whole thing (the *root* in root-mean-squared).
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the body of the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We're now ready to tie everything together and implement the main loop of the
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The main loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the supporting and foundational logic for the k-means algorithm is now implemented.
    The last thing to do is to tie it all together and implement the main loop of
    the algorithm. To run the algorithm, we should repeat the procedure of assigning
    points to centroids and then updating centroid locations until the centroids stop
    moving. We can also perform optional steps such as calculating the error and making
    sure that the algorithm does not exceed some maximum allowable number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the body of the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We've written a `solve` method that also accepts a limit on the maximum number
    of iterations allowed, which we've defaulted to `1000`. We run the algorithm in
    a `while` loop, and for each iteration in the loop, we call `assignPointsToCentroids`
    (recording its output value, `didAssignmentsChange`), call `updateCentroidLocations`,
    and call `calculateError`.
  prefs: []
  type: TYPE_NORMAL
- en: In order to help debugging and maintain a history of what the algorithm has
    accomplished, we maintain an array of `this.iterationLogs`, and for each iteration
    we'll record the centroid locations, the iteration number, the calculated error,
    and whether or not the algorithm has reached steady state (which is the opposite
    of `didAssignmentsChange`). We use ES6's array spread operator on `this.centroids`
    when recording logs to avoid passing this array as a reference, otherwise the
    iteration logs would show the last state of centroids instead of its progress
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: If the point/centroid assignments don't change from one iteration to the next,
    we consider the algorithm to have reached steady state and can return results.
    We accomplish this by using the `break` keyword to break from the `while` loop
    early. If the algorithm never reaches steady state, the `while` loop will continue
    until it has performed the maximum number of iterations allowed, and return the
    latest available result. The output for the `solve` method is simply the most
    recent iteration log, which contains all the information a user of this class
    would need to know.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – k-means on simple 2D data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've got our implementation of the k-means algorithm written and coded, so
    now it's time to see how it works. In our first example, we'll run our algorithm
    against a simple dataset of two-dimensional data. The data itself will be contrived
    so that the algorithm can easily find three distinct clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, modify the `data.js` file to add the following data, anywhere preceding
    the `export default` line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update the final export line to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to graph the preceding data points, we would see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29251362-e458-4369-b23a-b53b12378019.png)'
  prefs: []
  type: TYPE_IMG
- en: Visually, we can see that there are three neatly clustered groups of data points.
    When we run the algorithm, we will use *k = 3* and expect that the centroids neatly
    position themselves to the centers of those three clusters. Let's try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `index.js` and add the following. You can either replace the code you
    added earlier (preserving the `import` statements), or simply add this at the
    bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After outputting some headers, we create a new `KMeans` instance called `ex_1_solver`
    and initialize it with k = 3 and the `example_data.example_2d3k` that we just
    added. We call the `solve` method, with no parameters (that is, the max allowed
    iterations will be 1,000), and capture the output in the variable `ex_1_centroids`.
    Finally, we print the results to the screen and add a newline—we'll add a few
    more tests and examples following this point.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the order of your centroids may be different to mine, since random
    initial conditions will differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now run `yarn start` and should see output that looks similar to this. Additionally,
    because of the random initialization, it''s possible that some runs of the solver
    will get caught in local optima and you''ll see different centroids. Run the program
    a few times in a row and see what happens. Here''s what my output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output of the program tells us that the algorithm reached steady state after
    only two iterations (iteration 1 is the 2nd iteration, because we start counting
    from zero), and that our centroids are located at (2.8, 3.9), (13.4, 2.4), and
    (7.6, 7.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s graph these centroids along with the original data and see what it looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5d4cbd5-39f2-4a08-8b02-9aa82f98d24c.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, k-means has done its job splendidly, reporting centroids exactly
    where we would expect them to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a deeper look into what this algorithm is doing, by printing the
    `iterationLogs` after the solution. Add the following code to the bottom of `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `yarn start` again, and you should see output like the following. As always,
    based on initial conditions, your version may have required more or fewer iterations
    than mine so your output will differ, but you should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the algorithm took five iterations to reach steady state, as
    opposed to only two iterations like it did earlier. This is normal and expected
    due to the differing random initial conditions between the two runs. Looking through
    the logs, you can see that the error reported by the algorithm goes down with
    time. Also, notice that the first centroid, (2.8, 3.9), reaches its final destination
    after the first iteration, while the other centroids take more time to catch up.
    This is because the first centroid was randomly initialized to a location very
    close to its final destination, starting at (2.7, 3.7) and finishing at (2.8,
    3.9).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible, though somewhat rare, to catch the algorithm caught in a local
    optimum on this dataset. Let''s add the following code to the bottom of `index.js`
    to run the solver multiple times and see if it''ll find a local optimum instead
    of a global optimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this with `yarn start` a few times until you see an unexpected result.
    In my case, I found the following solution (I''m omitting the other output of
    the preceding program):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The fourth run of the solver found a different answer from the other runs:
    it discovered (11.3, 2.3), (5.1, 5.6), (14.5, 2.5) as a solution. Because the
    other solution is much more common than this one, we can assume the algorithm
    has been trapped in a local optimum. Let''s chart these values against the rest
    of the data and see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98271672-cb1c-4ee0-8c0a-7d5bb65e18b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding chart, our data points are represented by circles, the centroids
    we''d expect are represented by triangles, and the odd result we got is represented
    by **X** marks. Looking at the chart, you can understand how the algorithm may
    have come to this conclusion. One centroid, the **X** mark at (5.1, 5.6), has
    captured two different clusters and is sitting between them. The other two centroids
    have divided the third cluster into two. This is a perfect example of a local
    optimum: it''s a solution that makes sense, it logically clusters the data points,
    but it is not the best available solution (the global optimum) for the data. Most
    likely, the two centroids on the right were both randomly initialized inside that
    cluster and got trapped there.'
  prefs: []
  type: TYPE_NORMAL
- en: This is always a potential outcome for the k-means algorithm, and indeed all
    ML algorithms. Based on initial conditions and the quirks of the dataset, the
    algorithm may occasionally (or even frequently) discover local optima. Fortunately,
    if you compare the errors of the two solutions from the preceding output, the
    global solution has an error of 1.9 and the locally optimum solution reports an
    error of 3.0\. In this case, our error calculation has done its job well, and
    correctly represented the tightness of the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: To combat this issue with the k-means algorithm, you should generally run it
    more than one time, and look for either consensus (for example, four of five runs
    all agree), or the minimum error, and use that as your result.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – 3D data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because we've written the k-means algorithm to handle any arbitrary number of
    dimensions, we can also test it with 3D data (or 10D, or 100D or any number of
    dimensions that you require). While this algorithm will work for more than three
    dimensions, we have no way of visually plotting the higher dimensions and therefore
    can't visually check the results—so we'll test with 3D data and move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `data.js` and add the following to the middle of the file—anywhere
    preceding the `export default` line is OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And then modify the export line to look like this (add the `example_3d3k` variable
    to the export):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding data, when plotted in 3D, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22e12169-6eea-4d16-ac25-6a0ce167876d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are three clear clusters, and we''d expect k-means to
    handle this easily. Now, switch to `index.js` and add the following. We are simply
    creating a new solver for this example, loading the 3D data, and printing the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program with `yarn start` and you should see something like the following.
    I''ve omitted the output from the earlier 2D example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Thankfully, our solver has given us 3D data points, so we know that, at the
    very least, the algorithm can differentiate between 2D and 3D problems. We see
    that it still only took a handful of iterations, and that the error is a reasonable
    number (meaning it's defined, not negative, and not too large).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plot these centroids against our original data we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9fd84dc-522b-4517-8b61-816538a9edc9.png)'
  prefs: []
  type: TYPE_IMG
- en: The circles represent the data points, as before, and now we can see our black
    diamond centroids have found their homes in the middle of their clusters. Our
    algorithm has proven that it can work for three-dimensional data, and it will
    work just as well for any number of dimensions you can give it.
  prefs: []
  type: TYPE_NORMAL
- en: k-means where k is unknown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've been able to define, in advance, how many clusters the algorithm
    should find. In each example, we've started the project knowing that our data
    has three clusters, so we've manually programmed a value of 3 for *k*. This is
    still a very useful algorithm, but you may not always know how many clusters are
    represented in your data. To solve this problem we need to extend the k-means
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A major reason I included the optional error calculation in our k-means implementation
    was to help solve this problem. Using an error metric—in any ML algorithm—doesn't
    only allow us to search for a solution, it also allows us to search for the best
    *parameters* that yield the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a way, we need to build a meta-ML algorithm, or an algorithm that modifies
    our algorithm and its parameters. Our approach will be straightforward but effective:
    we''ll build a new class called `KMeansAutoSolver`, and instead of specifying
    a value of *k*, we''ll specify a range of *k* values to test. The new solver will
    run our k-means code for each value of *k* in the range and determine which value
    of *k* yields the lowest error. Additionally, we''ll also run multiple trials
    of each *k* value so that we avoid getting caught in local optima.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a file to the `src/` folder called `kmeans-autosolver.js`. Add the following
    code to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `KMeansAutoSolver` class contains a constructor which accepts `kMin`, `kMax`,
    `maxTrials`, and `data`. The `data` parameter is the same data you would give
    to the `KMeans` class. Instead of providing the class with a value for *k,* you
    provide a range of *k *values to test, as specified by `kMin` and `kMax`. Additionally,
    we'll also program this solver to run the k-means algorithm a number of times
    for each value of *k,* in order to avoid finding local optima, as we demonstrated
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The main part of the class is the `solve` method, which, like the `KMeans` class,
    also accepts a `maxIterations` argument. The `solve` method also returns the same
    thing the `KMeans` class returns, except that we've also added the value of *k*
    to the output and also the `currentTrial` number. Adding the value of *k* to the
    output is a bit redundant, as you could just count the number of centroids returned,
    but it's nice to see in the output.
  prefs: []
  type: TYPE_NORMAL
- en: The body of the `solve` method is straightforward. For each value of *k* in
    the range between `kMin` and `kMax`*,* we run the `KMeans` solver `maxTrials`
    times*.* If the solution beats the current best solution in terms of error, we
    record this solution as the best. At the end of the method, we return the solution
    with the best (lowest) error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it out. Open up `data.js` and add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And update the export line as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Graphing this data, we see four neat clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77724ab4-a38e-4f3a-8895-592e857470bd.png)'
  prefs: []
  type: TYPE_IMG
- en: However, for the purposes of this example, we do not know how many clusters
    to expect, only that it's likely between one and five.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open up `index.js` and import the `KMeansAutoSolver` at the top of the
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, at the bottom of the file, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `yarn start` and you should see output similar to the following (previous
    output omitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Right away, you can see that the solver found an answer with `k: 4`, which
    is what we expected, and that the algorithm reached steady state in only three
    iterations with a low error value—all good signs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting these centroids against our data, we see that the algorithm has determined
    both the correct value for *k* and the centroid positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0be74388-4e0e-4154-b914-8649691d2d86.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that our k-means auto-solver is also susceptible to local optima, and will
    not always guess the correct value for *k*. The reason? Increasing the value of
    *k* means that we can distribute more centroids amongst the data and reduce the
    error value. If we have 25 data points and set the range of *k* to anywhere from
    1 to 30, the solver may end up finding a solution where k = 25, and each centroid
    sits on top of each individual data point for a total error of 0! This can be
    considered *overfitting*, where the algorithm finds a correct answer but hasn't
    sufficiently generalized the problem to give us the results we want. Even when
    using the auto-solver, you must be careful of the range of *k* values you give
    it, and keep the range as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we increase `kMax` from 5 to 10 in the preceding example, we
    find that it gives a result for *k = 7* as the best. As always, the local optimum
    makes sense, but it is not quite what we were looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd2f5145-2c4f-4057-8e60-0eb0a213423f.png)'
  prefs: []
  type: TYPE_IMG
- en: Because the auto-solver uses the error value calculation as its only guidance,
    you may be able to tune the error calculation to also consider the value of *k*
    and penalize solutions with too many clusters*.* The previous error calculation
    was a purely geometric calculation, representing the average distance each point
    is from its centroid. We may want to upgrade our error calculation so that it
    also prefers solutions with fewer centroids. Let's see what that would look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the `kmeans.js` file and modify the `calculateError` method. Find
    the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And modify it to add the value of `k` to the distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: When running the `KMeans` class by itself, this modification will do no harm
    because the value of `k` will be constant for that solver. The only time this
    modification may be undesirable is if you're actually interpreting and using the
    error value as a representation of *distance specifically*, as opposed to just
    looking for lower error values. Meaning, if you *need* the error to be a representation
    of distance, then you should not make this change. In all other cases, however,
    it can be advantageous, as this modification will prefer solutions with fewer
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, return to `index.js` and modify the `ex_3_solver` to search the range of
    `k` values from 1 through 30\. Run the program again with `yarn start` and you
    will see that the auto-solver once again correctly returns results for *k = 4*!
    While the previous, locally optimal solution with *k = 7* has a low error rate,
    adding the value of `k` to the error penalized the *k = 7* solution enough that
    the solver now prefers the *k = 4* solution. Because of this modification to the
    error calculation, we can be a little less careful when choosing our `kMin` and
    `kMax`, which is very helpful when we have no idea what *k* will be.
  prefs: []
  type: TYPE_NORMAL
- en: While our error calculation is no longer a geometrical representation of the
    cluster tightness, you can see that being thoughtful about the error calculation
    can give you a lot of leeway when trying to optimize for certain system properties.
    In our case, we wanted to find not just the tightest geometrical clusters, but
    the tightest geometrical clusters *with the fewest amount of clusters possible*,
    so updating the error calculation to consider *k* was a very helpful step.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the problem of clustering, or grouping data points
    into logically similar groups. Specifically, we introduced the k-means algorithm,
    which is the most popular numerical clustering algorithm in ML. We then implemented
    the k-means algorithm in the form of a `KMeans` JavaScript class and tested it
    with both two and three-dimensional data. We also discussed how to approach the
    clustering problem when the number of clusters you desire is unknown beforehand,
    and built a new JavaScript class called `KMeansAutoSolver` to solve this problem.
    Along the way, we also discussed the impact of error calculations, and made a
    modification to our error calculation that helps generalize our solution to avoid
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we'll take a look at classification algorithms. Classification
    algorithms are supervised learning algorithms that can be seen as a more sophisticated
    extension of clustering algorithms. Rather than simply grouping data points by
    their similarity or proximity, classification algorithms can be trained to learn
    specific labels that should be applied to data.
  prefs: []
  type: TYPE_NORMAL
