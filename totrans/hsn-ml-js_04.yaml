- en: Grouping with Clustering Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类算法进行分组
- en: 'A common and introductory unsupervised learning problem is that of *clustering.*
    Often, you have large datasets that you wish to organize into smaller groups,
    or wish to break up into logically similar groups. For instance, you can try to
    divide census data of household incomes into three groups: low, high, and super
    rich. If you feed the household income data into a clustering algorithm, you would
    expect to see three data points as a result, with each corresponding to the average
    value of your three categories. Even this one-dimensional problem of clustering
    household incomes may be difficult to do by hand, because you might not know where
    one group should end and the other should begin. You could use governmental definitions
    of income brackets, but there''s no guarantee that those brackets are geometrically
    balanced; they were invented by policymakers and may not accurately represent
    the data.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见且入门级的无监督学习问题是 *聚类*。通常，你拥有大量数据集，希望将其组织成更小的组，或者希望将其分解成逻辑上相似的组。例如，你可以尝试将家庭收入普查数据分为三个组：低收入、高收入和超级富豪。如果你将家庭收入数据输入到聚类算法中，你预计会看到三个数据点作为结果，每个数据点对应于你三个类别的平均值。即使这个一维的聚类家庭收入问题也可能很难手工完成，因为你可能不知道一个组应该在哪里结束，另一个组应该在哪里开始。你可以使用政府关于收入分组的定义，但没有保证这些分组在几何上是平衡的；它们是由政策制定者发明的，可能无法准确代表数据。
- en: A *cluster* is a group of logically similar data points. They can be users with
    similar behavior, citizens with similar income ranges, pixels with similar colors,
    and so on. The k-means algorithm is numerical and geometric, so the clusters it
    identifies will all be numerically similar, with data points that are geometrically
    close to one another. Fortunately, most data can be represented numerically, so
    the k-means algorithm is useful for many different problem domains.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*簇* 是一组逻辑上相似的数据点。它们可以是具有相似行为的用户、具有相似收入范围的公民、具有相似颜色的像素等等。k-means 算法是数值和几何的，因此它所识别的簇都将具有数值上的相似性，并且数据点在几何上彼此接近。幸运的是，大多数数据都可以用数值表示，因此
    k-means 算法适用于许多不同的问题领域。'
- en: 'The k-means algorithm is a powerful, fast, and popular clustering algorithm
    for numerical data. The name k-means is comprised of two parts: ***k***, which
    represents the number of clusters that we want the algorithm to find, and ***means,***
    which is the method of determining where those cluster centers are (you could,
    for instance, also use k-medians or k-modes). Translated into plain English, we
    might ask the algorithm to find us three cluster centers that are the mean values
    of the points they represent. In that case, *k = 3* and we can tell our bosses
    that we did a k-means analysis with *k = 3* when filing our report.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法是一种强大、快速且流行的数值数据聚类算法。名称 k-means 由两部分组成：***k***，它代表我们希望算法找到的簇的数量，和***means***，这是确定那些簇中心位置的方法（例如，你也可以使用
    k-medians 或 k-modes）。用简单的英语来说，我们可能会要求算法为我们找到三个簇中心，这些中心是它们所代表点的平均值。在这种情况下，*k =
    3*，我们可以在提交报告时告诉我们的老板我们进行了 *k = 3* 的 k-means 分析。
- en: 'The k-means algorithm is an iterative algorithm, which means it runs a loop
    and continually updates its model until the model reaches steady state, at which
    point it will return its results. Put into narrative form, the k-means algorithm
    works like this: plot the data that you wish to analyze, and pick a value for
    *k*. You must know the value of *k* beforehand, or at least have an idea of what
    it should be (though we''ll also explore a way around this later in the chapter).
    Randomly create *k* points (if *k = 5*, create five points) and add them to your
    plot; these points are called the **centroids**, as they will ultimately represent
    the geometric centers of the clusters. For each data point in the plot, find the
    centroid closest to that point and connect or assign it to the point. Once all
    the assignments have been made, look at each centroid in turn and update its position
    to the mean position of all the points assigned to it. Repeat the assign-then-update
    procedure until the centroids stop moving; these final positions of the centroids
    are the output of the algorithm, and can be considered your cluster centers. If
    the narrative is hard to follow, don''t worry, we''ll dig into it more deeply
    as we build this algorithm from scratch.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法是一个迭代算法，这意味着它会运行一个循环，并不断更新其模型，直到模型达到稳定状态，此时它将返回其结果。用叙述形式来说，k-means算法的工作方式是这样的：绘制你想要分析的数据，并选择一个*k*的值。你事先必须知道*k*的值，或者至少有一个大概的估计（尽管我们也会在后面的章节中探讨一种绕过这个问题的方法）。随机创建*k*个点（如果*k*等于5，就创建五个点），并将它们添加到你的图表中；这些点被称为**质心**，因为它们最终将代表簇的几何中心。对于图表中的每个数据点，找到离该点最近的质心，并将其连接或分配给该点。一旦所有分配都已完成，依次查看每个质心，并将其位置更新为分配给它的所有点的平均值。重复分配然后更新的过程，直到质心停止移动；这些质心的最终位置是算法的输出，可以被认为是你的簇中心。如果叙述难以理解，不要担心，随着我们从零开始构建这个算法，我们会更深入地探讨它。
- en: 'In this chapter, we''ll first discuss the concepts of average and distance
    and how they apply to the k-means algorithm. Then we''ll describe the algorithm
    itself and build a JavaScript class from scratch to implement the k-means algorithm.
    We''ll test our k-means solver with a couple of simple datasets, and then discuss
    what to do when you don''t know the value of *k* beforehand. We''ll build another
    tool that automates the discovery of the value *k*. We''ll also discuss what the
    concept of *error* means for k-means applications, and how to design an error
    algorithm that helps us achieve our goals. The following are the topics that will
    be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先将讨论平均值和距离的概念以及它们如何应用于k-means算法。然后我们将描述算法本身，并从头开始构建一个JavaScript类来实现k-means算法。我们将用几个简单的数据集测试我们的k-means求解器，然后讨论在事先不知道*k*的值时应该做什么。我们将构建另一个工具来自动发现*k*的值。我们还将讨论对于k-means应用来说，*错误*的概念意味着什么，以及如何设计一个帮助实现我们目标的错误算法。以下是在本章中将要涉及的主题：
- en: Average and distance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均值和距离
- en: Writing the k-means algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写k-means算法
- en: Example 1—k-means on simple 2D data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例1—简单2D数据上的k-means
- en: Example 2—3D data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例2—3D数据
- en: K-means where *k* is unknown
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*k*未知时的K-means
- en: Average and distance
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均值和距离
- en: 'The k-means algorithm relies on two concepts in order to operate: average and
    distance. In order to tell you where the center of a cluster is, the algorithm
    will calculate an average value for these points. In this case, we will use the
    arithmetic mean, or the sum of values divided by the number of values, to represent
    our average. In ES5/classic JavaScript (I''m also being purposefully explicit
    in this example, for any readers who are not familiar with calculating the mean),
    we might write a function like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法依赖于两个概念来运行：平均值和距离。为了告诉你簇的中心在哪里，算法将计算这些点的平均值。在这种情况下，我们将使用算术平均值，即值的总和除以值的数量，来表示我们的平均值。在ES5/经典JavaScript（我还在这个例子中有意地明确指出，对于不熟悉计算平均值的读者），我们可能会编写一个像这样的函数：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In ES6, we can abuse our shorthand privileges and write the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在ES6中，我们可以滥用我们的简写特权，并编写以下代码：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is a handy ES6 one-liner to keep in your back pocket, however, it assumes
    all values are already numeric and defined, and it will return NaN if you give
    it an empty array. If the shorthand is confusing, we can break it up like so:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以随时放在口袋里的ES6单行代码，然而，它假设所有值都已经数字化和定义好了，如果你给它一个空数组，它将返回NaN。如果这个简写让人困惑，我们可以这样拆分它：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Keep in mind we can use any type of average, including the median and mode.
    In fact, it's sometimes preferable to use k-medians over k-means. The median does
    a better job of muting outliers than the mean does. You should therefore always
    ask yourself which average you actually need. If you want a representation of
    total resources consumed, for instance, you might use the arithmetic mean. If
    you suspect outliers are caused by faulty measurements and should be ignored,
    k-medians could suit you better.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need a concept of distance in this algorithm. It can be any distance
    measure, however, for numeric data you will mostly use the typical Euclidean distance—the
    standard distance measure you''d learn in high school—which in ES5 JavaScript
    looks like this for two dimensions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We must support many more than two dimensions, however, so we can generalize
    to the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can write an ES6 one-liner for this, but it won''t be as readable as the
    lengthier, explicit example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Armed with these tools, we can start writing the k-means algorithm itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Writing the k-means algorithm
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The k-means algorithm is relatively simple to implement, so in this chapter
    we''ll write it from scratch. The algorithm requires only two pieces of information:
    the *k* in k-means (the number of clusters we wish to identify), and the data
    points to evaluate. There are additional parameters the algorithm can use, for
    example, the maximum number of iterations to allow, but they are not required.
    The only required output of the algorithm is *k* centroids, or a list of points
    that represent the centers of the clusters of data. If *k = 3*, then the algorithm
    must return three centroids as its output. The algorithm may also return other
    metrics, such as the total error, the total number of iterations required to reach
    steady state, and so on, but again these are optional.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'A high-level description of the k-means algorithm is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Given the parameter *k* and the data to process, initialize *k* candidate centroids
    randomly
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each data point, determine which candidate centroid is closest to that point
    and *assign* the point to that centroid
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each centroid, update its position to be the mean position of all the points
    assigned to it
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Step 2* and *Step 3* until the centroids' positions reach steady state
    (that is, the centroids stop moving)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this process, you may return the positions of the centroids as
    the algorithm's output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a moment to set up our development environment for this algorithm.
    The environment will be as described in [Chapter 1](370a39e1-e618-475d-9722-9b4315a9a6ee.xhtml), *Exploring
    the Potential of JavaScript,* however, we'll run through the entire process here.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new folder for this project. I've named the folder `Ch4-kmeans`.
    Create a subfolder called `src` inside `Ch4-kmeans`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add a file called `package.json` to the `Ch4-kmeans` folder. Add the
    following content to the file:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After creating the `package.json` file, switch to your terminal program and
    (from the `Ch4-kmeans` folder) run the `yarn install` command.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`package.json`文件后，切换到您的终端程序，并在`Ch4-kmeans`文件夹中运行`yarn install`命令。
- en: 'Next, create three new files inside the `Ch4-kmeans/src` folder: `index.js`,
    `data.js`, and `kmeans.js`. We will write the actual k-means algorithm inside
    `kmeans.js`, we will load some example data into `data.js`, and we''ll use `index.js`
    as our bootstrapping point to set up and run a number of examples.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`Ch4-kmeans/src`文件夹内创建三个新文件：`index.js`、`data.js`和`kmeans.js`。我们将在`kmeans.js`中编写实际的k-means算法，将一些示例数据加载到`data.js`中，并使用`index.js`作为我们的启动点来设置和运行多个示例。
- en: At this point, you may want to stop and test that everything is working. Add
    a simple `console.log("Hello");` to `index.js` and then run the command `yarn
    start` from the command line. You should see the file compile and run, printing
    `Hello` to the screen before exiting. If you get errors or do not see the `Hello`,
    you may want to take a step back and double-check your environment. If everything
    is working, you can delete the `console.log("Hello");` from `index.js`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可能想要停下来测试一切是否正常工作。在`index.js`中添加一个简单的`console.log("Hello");`，然后从命令行运行`yarn
    start`命令。您应该看到文件编译并运行，在退出前将`Hello`打印到屏幕上。如果您遇到错误或看不到`Hello`，您可能需要退一步并仔细检查您的环境。如果一切正常，您可以删除`index.js`中的`console.log("Hello");`。
- en: Initializing the algorithm
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化算法
- en: 'In this section, we''ll be working in the `kmeans.js` file. The first thing
    to do is to add our functions for mean and distance to the top of the file. Since
    these are generic functions that can be called **statistically**, we will not
    define them inside a class. Add the following to the top of the file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将工作在`kmeans.js`文件中。首先要做的事情是将我们的均值和距离函数添加到文件顶部。由于这些是通用的函数，可以**统计地**调用，我们不会在类内部定义它们。将以下内容添加到文件顶部：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, create and export a `KMeans` class. We will fill this in with many more
    methods throughout this chapter, but let''s start with the following. Add this
    to the `kmeans.js` file beneath the code you just added:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`kmeans.js`文件中添加并导出`KMeans`类。我们将在本章的其余部分添加更多方法，但让我们从以下内容开始。将以下内容添加到您刚刚添加的代码下方：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We've created a class called `KMeans` and are exporting it as the default export
    for this file. The preceding code also initializes some of the instance variables
    that the class will need, which we will describe shortly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`KMeans`的类，并将其作为此文件的默认导出。前面的代码还初始化了类将需要的某些实例变量，我们将在稍后描述。
- en: The constructor for the class takes two parameters, `k` and `data`, and stores
    both as instance variables. The `k` parameter represents the `k` in k-means, or
    the desired number of clusters as the algorithm's output. The `data` parameter
    is an array of data points that the algorithm will process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数接受两个参数，`k`和`data`，并将它们都存储为实例变量。`k`参数代表k-means中的`k`，或者算法输出中期望的簇数量。`data`参数是算法将处理的数据点的数组。
- en: 'At the end of the constructor, we call the `reset()` method, which is used
    to initialize (or reset) the solver''s state. Specifically, the instance variables
    we initialize in the `reset` method are:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数的末尾，我们调用`reset()`方法，该方法用于初始化（或重置）求解器的状态。具体来说，我们在`reset`方法中初始化的实例变量包括：
- en: '`iterations`, which is a simple counter of how many iterations the solver has
    run, starting from 0'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterations`，它是一个简单的计数器，记录求解器已运行的迭代次数，从0开始'
- en: '`error`, which records the **root mean square error** (**RMSE**) of the points''
    distance to their centroids for the current iteration'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`error`，它记录了当前迭代中点到其质心的**均方根误差**（**RMSE**）'
- en: '`centroidAssignments`, which is an array of data point index numbers that map
    to a centroid index number'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`centroidAssignments`，它是一个数据点索引数组，映射到一个质心索引'
- en: '`centroids`, which will store the solver''s candidates for the *k* centroids
    at the current iteration'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`centroids`，它将存储求解器在当前迭代中候选的`k`个质心'
- en: Notice that in the `reset` method, we're making a call to `this.initRandomCentroids()`,
    which we have not yet defined. The k-means algorithm must start with a set of
    candidate centroids, so the purpose of that method is to generate the correct
    number of centroids randomly. Because the algorithm starts with a random state,
    it can be expected that multiple runs of the algorithm will return different results
    based on the initial conditions. This is actually a desired property of the k-means
    algorithm, because it is susceptible to finding local optima, and running the
    algorithm multiple times with different initial conditions may help you find the
    global optimum.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 `reset` 方法中，我们调用了 `this.initRandomCentroids()`，这是我们尚未定义的。k-means 算法必须从一个候选质心集合开始，所以那个方法的目的就是随机生成正确数量的质心。因为算法从一个随机状态开始，可以预期多次运行算法将基于初始条件返回不同的结果。这实际上是
    k-means 算法的一个期望属性，因为它容易陷入局部最优，多次运行算法使用不同的初始条件可能有助于找到全局最优。
- en: 'We have some prerequisites to satisfy before we can generate our random centroids.
    First, we must know the dimensionality of the data. Are we working with 2D data,
    3D data, 10D data, or 1324D data? The random centroids we generate must have the
    same number of dimensions as the rest of the data points. This is an easy problem
    to solve; we assume that all the data points have the same number of dimensions,
    so we can just inspect the first data point we encounter. Add the following method
    to the `KMeans` class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们生成随机质心之前，我们必须满足一些先决条件。首先，我们必须知道数据的维度。我们是在处理 2D 数据、3D 数据、10D 数据还是 1324D 数据？我们生成的随机质心必须与数据点的其他维度数量相同。这是一个容易解决的问题；我们假设所有数据点都有相同的维度，所以我们只需检查我们遇到的第一个数据点。将以下方法添加到
    `KMeans` 类中：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The other consideration we must make when generating random initial centroids
    is that the centroids should be close to the data that we're working with. For
    instance, if all your data points are points between (0, 0) and (10, 10), you
    would not want to generate a random centroid such as (1200, 740). Similarly, if
    your data points are all negative, you would not want to generate positive centroids,
    and so on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成随机初始质心时，我们必须考虑的其他因素是质心应该接近我们正在处理的数据。例如，如果你的所有数据点都在 (0, 0) 和 (10, 10) 之间，你不会希望生成一个像
    (1200, 740) 这样的随机质心。同样，如果你的数据点都是负数，你不会希望生成正的质心，等等。
- en: Why should we care where the random centroids start? In this algorithm, points
    will be assigned to the centroid closest to it and gradually *pull* the centroid
    towards the cluster center. If the centroids are all to the right of the data
    points, then the centroids themselves will follow similar paths towards the data
    and may get all clumped together in one single cluster, converging to a local
    optimum. By making sure that the centroids are randomly distributed within the
    range of the data, we have a better chance of avoiding this type of local optimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要关心随机质心的起始位置呢？在这个算法中，点会被分配到最近的质心，并逐渐 *拉* 质心向簇中心移动。如果所有质心都在数据点的右侧，那么质心本身也会遵循类似的路径向数据移动，并可能全部聚集在一个单独的簇中，收敛到局部最优。通过确保质心在数据范围内部随机分布，我们更有可能避免这种类型的局部最优。
- en: 'Our approach to generating our centroid starting positions will be to determine
    the range of each dimension of the data, and then choose random values for our
    centroid''s position within those ranges. For instance, imagine three two-dimensional
    data points in an *x, **y* plane: (1, 3), (5, 8), and (3, 0). The range of the
    *x *dimension lies between 1 and 5, while the range of the *y* dimension lies
    between 0 and 8\. Therefore, when creating a randomly initialized centroid, we
    will choose a random number between 1 and 5 for its *x* position, and a random
    number between 0 and 8 for its *y* position.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成质心起始位置的方法将是确定数据的每个维度的范围，然后在那些范围内为质心的位置选择随机值。例如，想象在 *x, **y* 平面上的三个二维数据点：(1,
    3)，(5, 8) 和 (3, 0)。*x* 维度的范围在 1 和 5 之间，而 *y* 维度的范围在 0 和 8 之间。因此，当创建随机初始化的质心时，我们将为其
    *x* 位置选择一个介于 1 和 5 之间的随机数，为其 *y* 位置选择一个介于 0 和 8 之间的随机数。
- en: 'We can use JavaScript''s `Math.min` and `Math.max` to determine the data ranges
    for each dimension. Add the following method to the `KMeans` class:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 JavaScript 的 `Math.min` 和 `Math.max` 来确定每个维度的数据范围。将以下方法添加到 `KMeans` 类中：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This method first collects all the values of the given dimension from the data
    points as an array and then returns an object containing that range''s `min` and
    `max`. Returning to our preceding example of three data points ((1, 3), (5, 8),
    and (3, 0)), calling `getRangeForDimension(0)` would return `{min: 1, max: 5}`,
    and calling `getRangeForDimension(1)` would return `{min: 0, max: 8}`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be useful for us to have an object of all dimensions and their ranges
    that we can cache while initializing centroids, so add the following method to
    the `KMeans` class as well:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method simply looks at all dimensions and returns the `min` and `max` ranges
    for each, structured as objects in an array indexed by the dimension. This method
    is primarily a convenience, but we will use it shortly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finally generate our randomly initialized centroids. We will need to
    create *k* centroids, and work dimension by dimension to choose a random point
    inside the range of each dimension. Add the following method to the `KMeans` class:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding algorithm contains two loops; the outer loop creates `k` candidate
    centroids. Because the number of dimensions in the dataset is arbitrary, and because
    each dimension itself has an arbitrary range, we must then work dimension by dimension
    for each centroid in order to generate a random position. If your data is three-dimensional,
    the inner loop will consider dimensions 0, 1, and 2 separately, determining the
    `min` and `max` values for each dimension, choosing a random value in that range,
    and assigning that value to that specific dimension of the centroid point.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Testing random centroid generation
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already written a significant amount of code, so now would be a good time
    to stop and test our work. We should also start setting up our `data.js` file
    that we'll use to hold some example data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the `data.js` file and add the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The values used are the same ones from our simple data point example written
    in the preceding block.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, switch to `index.js` and add the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First, we import the `KMeans` class and the `example_data` object from their
    respective files. We print some helpful output to the screen, and then initialize
    a `KMeans` solver instance for our simple data. We can check the randomly initialized
    centroids by inspecting the value of `ex_randomCentroids_solver.centroids`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve added this code, run `yarn start` from the command line, and you
    should see something similar to the following output. Note that because the centroid
    initialization is random, you will not see the same values that I see; however,
    what we''re looking for is to make sure that the random centroids lie within the
    correct ranges. Specifically, we want our centroids to have *x* positions between
    1 and 5, and *y* positions between 0 and 8\. Run the code a number of times to
    make sure that the centroids have the correct position:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you see something similar to the preceding block, that means everything is
    working well so far and we're ready to continue implementing the algorithm.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到与前面块类似的内容，这意味着到目前为止一切正常，我们可以继续实现算法。
- en: Assigning points to centroids
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将点分配到质心
- en: 'The iteration loop that the k-means algorithm performs has two steps: assigning
    each point to the centroid closest to it, and then updating the location of the
    centroids to be the mean value of all the points assigned to that centroid. In
    this section, we''ll implement the first part of the algorithm: assigning points
    to centroids.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法执行的迭代循环包含两个步骤：将每个点分配到最近的质心，然后更新质心的位置，使其成为分配给该质心的所有点的平均值。在本节中，我们将实现算法的第一部分：将点分配到质心。
- en: At a high level, our task is to consider each point in the dataset and determine
    which centroid is closest to it. We also need to record the results of this assignment
    so that we can later update the centroid's location based on the points assigned
    to it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们的任务是考虑数据集中的每个点，并确定哪个质心离它最近。我们还需要记录此分配的结果，以便我们可以稍后根据分配给它的点更新质心的位置。
- en: 'Add the following method to the body of the `KMeans` class:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下方法添加到`KMeans`类的主体中：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This method considers a single data point, given by its index, and considers
    each centroid in the system in turn. We also keep track of the last centroid this
    point has been assigned to in order to determine if the assignment has changed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法考虑单个数据点，由其索引给出，并依次考虑系统中的每个质心。我们还跟踪此点最后分配到的质心，以确定分配是否已更改。
- en: In the preceding code, we loop over all centroids and use our `distance` function
    to determine the distance between the point and the centroid. If the distance
    is less than the lowest distance we've seen so far, or if this is the first centroid
    we're considering for this point (`minDistance` will be null in that case), we
    record the distance and the index position of the centroid. After looping over
    all centroids, we will now know which centroid is closest to the point under consideration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们遍历所有质心并使用我们的`distance`函数来确定点与质心之间的距离。如果距离小于迄今为止看到的最低距离，或者这是我们为该点考虑的第一个质心（在这种情况下`minDistance`将为null），我们将记录距离和质心的索引位置。遍历所有质心后，我们现在将知道哪个质心是考虑中的点最近的。
- en: Finally, we record the assignment of the centroid to the point by setting it
    to the `this.centroidAssignments` array—in this array, the index is the index
    of the point, and the value is the index of the centroid. We return a Boolean
    from this method by comparing the last known centroid assignment to the new centroid
    assignment—it will return `true` if the assignment has changed, or `false` if
    not. We'll use this information to figure out when the algorithm has reached steady
    state.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将质心分配给点的索引设置到`this.centroidAssignments`数组中来记录质心分配——在这个数组中，索引是点的索引，值是质心的索引。我们通过比较最后已知的质心分配和新质心分配来从这个方法返回一个布尔值——如果分配已更改，则返回`true`，如果没有更改，则返回`false`。我们将使用这个信息来确定算法何时达到稳态。
- en: 'The previous method considers only a single point, so we should also write
    a method to process the centroid assignments of all points. Additionally, the
    method we write should determine if *any* point has updated its centroid assignment.
    Add the following to the `KMeans` class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法只考虑单个点，因此我们还应该编写一个方法来处理所有点的质心分配。此外，我们编写的方法还应确定是否有任何点更新了其质心分配。将以下内容添加到`KMeans`类中：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This method defines a variable called `didAnyPointsGetReassigned`, initialized
    to `false`, and then loops over all points in the dataset to update their centroid
    assignments. If any point is assigned to a new centroid, the method will return
    `true`. If no assignments were changed, the method returns `false`. The return
    value from this method will become one of our termination conditions; if no points
    update after an iteration, we can consider the algorithm to have reached steady
    state and can terminate it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法定义了一个名为`didAnyPointsGetReassigned`的变量，并将其初始化为`false`，然后遍历数据集中的所有点以更新它们的质心分配。如果有任何点被分配到新的质心，该方法将返回`true`。如果没有分配更改，该方法返回`false`。此方法的返回值将成为我们的终止条件之一；如果在迭代后没有点更新，我们可以认为算法已达到稳态，可以终止它。
- en: 'Let''s now address the second part of the k-means algorithm: updating centroid
    locations.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论k-means算法的第二部分：更新质心位置。
- en: Updating centroid locations
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新质心位置
- en: 'In the previous section, we implemented the first part of the k-means algorithm:
    looking at all points in the dataset and assigning them to the centroid that''s
    geographically closest. The next step in the algorithm is to look at all centroids
    and update their locations to the mean value of all the points assigned to them.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: To make an analogy, you can imagine each point reaching out and grabbing the
    centroid closest to it. The points give the centroid a tug, trying to pull it
    closer to them. We've already implemented the *reach out and grab portion* of
    the algorithm, and now we'll implement the *pull the centroid closer* portion.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, our task is to loop over all the centroids, and for each, determine
    the mean position of all the points assigned to it. We'll then update the centroid's
    position to that mean value. Breaking this down further, we must first collect
    all the points assigned to a centroid, and then we have to calculate the average
    value of these points, always keeping in mind that points can have any number
    of dimensions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the easy task of collecting all the points assigned to a
    centroid. We already have a mapping of point indexes to centroid indexes in our
    `this.centroidAssignments` instance variable. Add the following code to the body
    of the `KMeans` class:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding method is quite standard: looping over all data points, we look
    up that point''s centroid assignment, and if it is assigned to the centroid in
    question, we add the point to an output array.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: We can now use this list of points to update the centroid's location. Our goal
    is to update the centroid's location to be the mean value of all the points we
    found previously. Because the data may be multi-dimensional, we must also consider
    each dimension independently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Using our simple example of points (1, 3), (5, 8), and (3, 0), we would find
    a mean location of (3, 3.6667). To get this value, we first calculate the mean
    value of the *x* dimension ( (1 + 5 + 3) / 3 = 3 ), and then calculate the mean
    value of the *y* dimension ( (3 + 8 + 0) / 3 = 11/3 = 3.6666... ). If we're working
    in more than two dimensions, we simply repeat the procedure for each dimension.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this algorithm in JavaScript. Add the following to the body of
    the `KMeans` class:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The preceding method considers only one centroid at a time, specified by its
    index. We use the `getPointsForCentroid` method that we just added to get an array
    of points assigned to this centroid. We initialize a variable called `newCentroid`
    as an empty array; this will ultimately replace the current centroid.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Considering one dimension at a time, we collect the positions of the points
    *in that dimension only,* and then calculate the mean. We use JavaScript's `Array.map`
    to extract the positions for the correct dimension only, and then we use our `mean`
    function to calculate the average position in that dimension.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: If we work this example by hand with the data points (1, 3), (5, 8), and (3,
    0), we start by examining dimension 0, or the *x* dimension. The result of `thisCentroidPoints.map(point
    => point[dimension])` is the array `[1, 5, 3]` for dimension 0, and for dimension
    1, the result is `[3, 8, 0]`. Each of these arrays is passed to the `mean` function,
    and the mean value is used in `newCentroid` for that dimension.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this method, we update our array of `this.centroids` with the
    newly calculated centroid position.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also write a convenience method to loop over all centroids and update
    their positions. Add the following to the body of the `KMeans` class:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Before finishing the algorithm and tying together all the pieces, we have one
    final prerequisite to satisfy. We're going to introduce the concept of *error*
    into the algorithm. Calculating the error is not required for the k-means algorithm
    to function, but you'll see later that this can be advantageous in certain situations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Because this is an unsupervised learning algorithm, our concept of error does
    not relate to semantic error. Instead, we will use an error metric that represents
    the average distance of all points from their assigned centroids. We'll use the
    RMSE for this, which penalizes bigger distances more harshly, so our error metric
    will be a good indication of how tight the clustering is.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: To perform this error calculation, we loop over all points and determine the
    distance of that point from its centroid. We square each distance before adding
    it to a running total (the *squared* in root-mean-squared), then divide the running
    total by the number of points (the *mean* in root-mean-squared), and finally take
    the square root of the whole thing (the *root* in root-mean-squared).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the body of the `KMeans` class:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We're now ready to tie everything together and implement the main loop of the
    algorithm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The main loop
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the supporting and foundational logic for the k-means algorithm is now implemented.
    The last thing to do is to tie it all together and implement the main loop of
    the algorithm. To run the algorithm, we should repeat the procedure of assigning
    points to centroids and then updating centroid locations until the centroids stop
    moving. We can also perform optional steps such as calculating the error and making
    sure that the algorithm does not exceed some maximum allowable number of iterations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the body of the `KMeans` class:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We've written a `solve` method that also accepts a limit on the maximum number
    of iterations allowed, which we've defaulted to `1000`. We run the algorithm in
    a `while` loop, and for each iteration in the loop, we call `assignPointsToCentroids`
    (recording its output value, `didAssignmentsChange`), call `updateCentroidLocations`,
    and call `calculateError`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In order to help debugging and maintain a history of what the algorithm has
    accomplished, we maintain an array of `this.iterationLogs`, and for each iteration
    we'll record the centroid locations, the iteration number, the calculated error,
    and whether or not the algorithm has reached steady state (which is the opposite
    of `didAssignmentsChange`). We use ES6's array spread operator on `this.centroids`
    when recording logs to avoid passing this array as a reference, otherwise the
    iteration logs would show the last state of centroids instead of its progress
    over time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: If the point/centroid assignments don't change from one iteration to the next,
    we consider the algorithm to have reached steady state and can return results.
    We accomplish this by using the `break` keyword to break from the `while` loop
    early. If the algorithm never reaches steady state, the `while` loop will continue
    until it has performed the maximum number of iterations allowed, and return the
    latest available result. The output for the `solve` method is simply the most
    recent iteration log, which contains all the information a user of this class
    would need to know.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – k-means on simple 2D data
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've got our implementation of the k-means algorithm written and coded, so
    now it's time to see how it works. In our first example, we'll run our algorithm
    against a simple dataset of two-dimensional data. The data itself will be contrived
    so that the algorithm can easily find three distinct clusters.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'First, modify the `data.js` file to add the following data, anywhere preceding
    the `export default` line:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, update the final export line to look like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we were to graph the preceding data points, we would see the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29251362-e458-4369-b23a-b53b12378019.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Visually, we can see that there are three neatly clustered groups of data points.
    When we run the algorithm, we will use *k = 3* and expect that the centroids neatly
    position themselves to the centers of those three clusters. Let's try it out.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `index.js` and add the following. You can either replace the code you
    added earlier (preserving the `import` statements), or simply add this at the
    bottom:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After outputting some headers, we create a new `KMeans` instance called `ex_1_solver`
    and initialize it with k = 3 and the `example_data.example_2d3k` that we just
    added. We call the `solve` method, with no parameters (that is, the max allowed
    iterations will be 1,000), and capture the output in the variable `ex_1_centroids`.
    Finally, we print the results to the screen and add a newline—we'll add a few
    more tests and examples following this point.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Note that the order of your centroids may be different to mine, since random
    initial conditions will differ.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now run `yarn start` and should see output that looks similar to this. Additionally,
    because of the random initialization, it''s possible that some runs of the solver
    will get caught in local optima and you''ll see different centroids. Run the program
    a few times in a row and see what happens. Here''s what my output looks like:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output of the program tells us that the algorithm reached steady state after
    only two iterations (iteration 1 is the 2nd iteration, because we start counting
    from zero), and that our centroids are located at (2.8, 3.9), (13.4, 2.4), and
    (7.6, 7.5).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s graph these centroids along with the original data and see what it looks
    like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5d4cbd5-39f2-4a08-8b02-9aa82f98d24c.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: As you can see, k-means has done its job splendidly, reporting centroids exactly
    where we would expect them to be.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a deeper look into what this algorithm is doing, by printing the
    `iterationLogs` after the solution. Add the following code to the bottom of `index.js`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run `yarn start` again, and you should see output like the following. As always,
    based on initial conditions, your version may have required more or fewer iterations
    than mine so your output will differ, but you should see something like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see, the algorithm took five iterations to reach steady state, as
    opposed to only two iterations like it did earlier. This is normal and expected
    due to the differing random initial conditions between the two runs. Looking through
    the logs, you can see that the error reported by the algorithm goes down with
    time. Also, notice that the first centroid, (2.8, 3.9), reaches its final destination
    after the first iteration, while the other centroids take more time to catch up.
    This is because the first centroid was randomly initialized to a location very
    close to its final destination, starting at (2.7, 3.7) and finishing at (2.8,
    3.9).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible, though somewhat rare, to catch the algorithm caught in a local
    optimum on this dataset. Let''s add the following code to the bottom of `index.js`
    to run the solver multiple times and see if it''ll find a local optimum instead
    of a global optimum:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Run this with `yarn start` a few times until you see an unexpected result.
    In my case, I found the following solution (I''m omitting the other output of
    the preceding program):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The fourth run of the solver found a different answer from the other runs:
    it discovered (11.3, 2.3), (5.1, 5.6), (14.5, 2.5) as a solution. Because the
    other solution is much more common than this one, we can assume the algorithm
    has been trapped in a local optimum. Let''s chart these values against the rest
    of the data and see what it looks like:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98271672-cb1c-4ee0-8c0a-7d5bb65e18b4.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding chart, our data points are represented by circles, the centroids
    we''d expect are represented by triangles, and the odd result we got is represented
    by **X** marks. Looking at the chart, you can understand how the algorithm may
    have come to this conclusion. One centroid, the **X** mark at (5.1, 5.6), has
    captured two different clusters and is sitting between them. The other two centroids
    have divided the third cluster into two. This is a perfect example of a local
    optimum: it''s a solution that makes sense, it logically clusters the data points,
    but it is not the best available solution (the global optimum) for the data. Most
    likely, the two centroids on the right were both randomly initialized inside that
    cluster and got trapped there.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: This is always a potential outcome for the k-means algorithm, and indeed all
    ML algorithms. Based on initial conditions and the quirks of the dataset, the
    algorithm may occasionally (or even frequently) discover local optima. Fortunately,
    if you compare the errors of the two solutions from the preceding output, the
    global solution has an error of 1.9 and the locally optimum solution reports an
    error of 3.0\. In this case, our error calculation has done its job well, and
    correctly represented the tightness of the clustering.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: To combat this issue with the k-means algorithm, you should generally run it
    more than one time, and look for either consensus (for example, four of five runs
    all agree), or the minimum error, and use that as your result.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – 3D data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because we've written the k-means algorithm to handle any arbitrary number of
    dimensions, we can also test it with 3D data (or 10D, or 100D or any number of
    dimensions that you require). While this algorithm will work for more than three
    dimensions, we have no way of visually plotting the higher dimensions and therefore
    can't visually check the results—so we'll test with 3D data and move on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `data.js` and add the following to the middle of the file—anywhere
    preceding the `export default` line is OK:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And then modify the export line to look like this (add the `example_3d3k` variable
    to the export):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding data, when plotted in 3D, looks like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22e12169-6eea-4d16-ac25-6a0ce167876d.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are three clear clusters, and we''d expect k-means to
    handle this easily. Now, switch to `index.js` and add the following. We are simply
    creating a new solver for this example, loading the 3D data, and printing the
    results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Run the program with `yarn start` and you should see something like the following.
    I''ve omitted the output from the earlier 2D example:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Thankfully, our solver has given us 3D data points, so we know that, at the
    very least, the algorithm can differentiate between 2D and 3D problems. We see
    that it still only took a handful of iterations, and that the error is a reasonable
    number (meaning it's defined, not negative, and not too large).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plot these centroids against our original data we will see the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9fd84dc-522b-4517-8b61-816538a9edc9.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: The circles represent the data points, as before, and now we can see our black
    diamond centroids have found their homes in the middle of their clusters. Our
    algorithm has proven that it can work for three-dimensional data, and it will
    work just as well for any number of dimensions you can give it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: k-means where k is unknown
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've been able to define, in advance, how many clusters the algorithm
    should find. In each example, we've started the project knowing that our data
    has three clusters, so we've manually programmed a value of 3 for *k*. This is
    still a very useful algorithm, but you may not always know how many clusters are
    represented in your data. To solve this problem we need to extend the k-means
    algorithm.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A major reason I included the optional error calculation in our k-means implementation
    was to help solve this problem. Using an error metric—in any ML algorithm—doesn't
    only allow us to search for a solution, it also allows us to search for the best
    *parameters* that yield the best solution.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'In a way, we need to build a meta-ML algorithm, or an algorithm that modifies
    our algorithm and its parameters. Our approach will be straightforward but effective:
    we''ll build a new class called `KMeansAutoSolver`, and instead of specifying
    a value of *k*, we''ll specify a range of *k* values to test. The new solver will
    run our k-means code for each value of *k* in the range and determine which value
    of *k* yields the lowest error. Additionally, we''ll also run multiple trials
    of each *k* value so that we avoid getting caught in local optima.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a file to the `src/` folder called `kmeans-autosolver.js`. Add the following
    code to the file:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `KMeansAutoSolver` class contains a constructor which accepts `kMin`, `kMax`,
    `maxTrials`, and `data`. The `data` parameter is the same data you would give
    to the `KMeans` class. Instead of providing the class with a value for *k,* you
    provide a range of *k *values to test, as specified by `kMin` and `kMax`. Additionally,
    we'll also program this solver to run the k-means algorithm a number of times
    for each value of *k,* in order to avoid finding local optima, as we demonstrated
    earlier.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The main part of the class is the `solve` method, which, like the `KMeans` class,
    also accepts a `maxIterations` argument. The `solve` method also returns the same
    thing the `KMeans` class returns, except that we've also added the value of *k*
    to the output and also the `currentTrial` number. Adding the value of *k* to the
    output is a bit redundant, as you could just count the number of centroids returned,
    but it's nice to see in the output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The body of the `solve` method is straightforward. For each value of *k* in
    the range between `kMin` and `kMax`*,* we run the `KMeans` solver `maxTrials`
    times*.* If the solution beats the current best solution in terms of error, we
    record this solution as the best. At the end of the method, we return the solution
    with the best (lowest) error.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it out. Open up `data.js` and add the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And update the export line as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Graphing this data, we see four neat clusters:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77724ab4-a38e-4f3a-8895-592e857470bd.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: However, for the purposes of this example, we do not know how many clusters
    to expect, only that it's likely between one and five.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open up `index.js` and import the `KMeansAutoSolver` at the top of the
    file:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, at the bottom of the file, add the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Run `yarn start` and you should see output similar to the following (previous
    output omitted):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Right away, you can see that the solver found an answer with `k: 4`, which
    is what we expected, and that the algorithm reached steady state in only three
    iterations with a low error value—all good signs.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting these centroids against our data, we see that the algorithm has determined
    both the correct value for *k* and the centroid positions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0be74388-4e0e-4154-b914-8649691d2d86.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: Note that our k-means auto-solver is also susceptible to local optima, and will
    not always guess the correct value for *k*. The reason? Increasing the value of
    *k* means that we can distribute more centroids amongst the data and reduce the
    error value. If we have 25 data points and set the range of *k* to anywhere from
    1 to 30, the solver may end up finding a solution where k = 25, and each centroid
    sits on top of each individual data point for a total error of 0! This can be
    considered *overfitting*, where the algorithm finds a correct answer but hasn't
    sufficiently generalized the problem to give us the results we want. Even when
    using the auto-solver, you must be careful of the range of *k* values you give
    it, and keep the range as small as possible.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we increase `kMax` from 5 to 10 in the preceding example, we
    find that it gives a result for *k = 7* as the best. As always, the local optimum
    makes sense, but it is not quite what we were looking for:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd2f5145-2c4f-4057-8e60-0eb0a213423f.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Because the auto-solver uses the error value calculation as its only guidance,
    you may be able to tune the error calculation to also consider the value of *k*
    and penalize solutions with too many clusters*.* The previous error calculation
    was a purely geometric calculation, representing the average distance each point
    is from its centroid. We may want to upgrade our error calculation so that it
    also prefers solutions with fewer centroids. Let's see what that would look like.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the `kmeans.js` file and modify the `calculateError` method. Find
    the following line:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And modify it to add the value of `k` to the distance:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: When running the `KMeans` class by itself, this modification will do no harm
    because the value of `k` will be constant for that solver. The only time this
    modification may be undesirable is if you're actually interpreting and using the
    error value as a representation of *distance specifically*, as opposed to just
    looking for lower error values. Meaning, if you *need* the error to be a representation
    of distance, then you should not make this change. In all other cases, however,
    it can be advantageous, as this modification will prefer solutions with fewer
    clusters.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Now, return to `index.js` and modify the `ex_3_solver` to search the range of
    `k` values from 1 through 30\. Run the program again with `yarn start` and you
    will see that the auto-solver once again correctly returns results for *k = 4*!
    While the previous, locally optimal solution with *k = 7* has a low error rate,
    adding the value of `k` to the error penalized the *k = 7* solution enough that
    the solver now prefers the *k = 4* solution. Because of this modification to the
    error calculation, we can be a little less careful when choosing our `kMin` and
    `kMax`, which is very helpful when we have no idea what *k* will be.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: While our error calculation is no longer a geometrical representation of the
    cluster tightness, you can see that being thoughtful about the error calculation
    can give you a lot of leeway when trying to optimize for certain system properties.
    In our case, we wanted to find not just the tightest geometrical clusters, but
    the tightest geometrical clusters *with the fewest amount of clusters possible*,
    so updating the error calculation to consider *k* was a very helpful step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the problem of clustering, or grouping data points
    into logically similar groups. Specifically, we introduced the k-means algorithm,
    which is the most popular numerical clustering algorithm in ML. We then implemented
    the k-means algorithm in the form of a `KMeans` JavaScript class and tested it
    with both two and three-dimensional data. We also discussed how to approach the
    clustering problem when the number of clusters you desire is unknown beforehand,
    and built a new JavaScript class called `KMeansAutoSolver` to solve this problem.
    Along the way, we also discussed the impact of error calculations, and made a
    modification to our error calculation that helps generalize our solution to avoid
    overfitting.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we'll take a look at classification algorithms. Classification
    algorithms are supervised learning algorithms that can be seen as a more sophisticated
    extension of clustering algorithms. Rather than simply grouping data points by
    their similarity or proximity, classification algorithms can be trained to learn
    specific labels that should be applied to data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
