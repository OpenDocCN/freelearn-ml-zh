<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Constructing a Classifier</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Building a simple classifier</li>
<li>Building a logistic regression classifier</li>
<li>Building a Naive Bayes classifier</li>
<li>Splitting a dataset for training and testing</li>
<li>Evaluating accuracy using cross-validation</li>
<li>Visualizing a confusion matrix</li>
<li>Extracting a performance report</li>
<li>Evaluating cars based on their characteristics</li>
<li>Extracting validation curves</li>
<li>Extracting learning curves</li>
<li>Estimating a income bracket</li>
<li>Predicting the quality of wine</li>
<li>Newsgroup trending topics classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To work on the recipes in this chapter, you need the following files (available on GitHub):</p>
<ul>
<li><kbd>simple_classifier.py</kbd></li>
<li><kbd>logistic_regression.py</kbd></li>
<li><kbd>naive_bayes.py</kbd></li>
<li><kbd>data_multivar.txt</kbd></li>
<li><kbd>splitting_dataset.py</kbd></li>
<li><kbd>confusion_matrix.py</kbd></li>
<li><kbd><kbd>performance_report.py</kbd></kbd></li>
<li><kbd>car.py</kbd></li>
<li><kbd>car.data.txt</kbd></li>
<li><kbd>income.py</kbd></li>
<li><kbd>adult.data.txt</kbd></li>
<li><kbd>wine.quality.py</kbd></li>
<li><kbd>wine.txt</kbd></li>
<li><kbd>post.classification</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In the field of machine learning, <strong>classification</strong> refers to the process of using the characteristics of data to separate it into a certain number of classes. This is different than regression, which we discussed in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a>, <em>The Realm of Supervised Learning</em>, where the output is a real number. A supervised learning classifier builds a model using labeled training data and then uses this model to classify unknown data.</p>
<p>A classifier can be any algorithm that implements classification. In simple cases, a classifier can be a straightforward mathematical function. In more real-world cases, a classifier can take very complex forms. In the course of study, we will see that classification can be either binary, where we separate data into two classes, or it can be multi-class, where we separate data into more than two classes. The mathematical techniques that are devised to deal with classification problems tend to deal with two classes, so we extend them in different ways to deal with multi-class problems as well.</p>
<p>Evaluating the accuracy of a classifier is vital for machine learning. What we need to know is, how we can use the available data, and get a glimpse of how the model performs in the real world. In this chapter, we will look at recipes that deal with all these things.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a simple classifier</h1>
                </header>
            
            <article>
                
<p>A <strong>classifier</strong> is a system with some characteristics that allow you to identify the class of the sample examined. In different classification methods, groups are called <strong>classes</strong>. The goal of a classifier is to establish the classification criterion to maximize performance. The performance of a classifier is measured by evaluating the capacity for generalization. <strong>Generalization</strong> means attributing the correct class to each new experimental observation. The way in which these classes are identified discriminates between the different methods that are available. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>Classifiers identify the class of a new objective, based on knowledge that's been extracted from a series of samples (a dataset). Starting from a dataset, a classifier extracts a model, which is then used to classify new instances. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p><span>Let's see how to build a simple classifier using some training data:</span></p>
<ol>
<li>We will use the <kbd>simple_classifier.py</kbd> file, already provided to you as a reference. To start, we import the <kbd>numpy</kbd> and <kbd>matplotlib.pyplot</kbd> packages, as we did in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a><span>, </span><em>The Realm of Supervised Learning</em>, and then we create some sample data:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>X = np.array([[3,1], [2,5], [1,8], [6,4], [5,2], [3,5], [4,7], [4,-1]])</pre>
<ol start="2">
<li>Let's assign some labels to these points:</li>
</ol>
<pre style="padding-left: 60px">y = [0, 1, 1, 0, 0, 1, 1, 0]</pre>
<ol start="3">
<li>As we have only two classes, the <kbd>y</kbd> list contains 0's and 1's. In general, if you have <em>N</em> classes, then the values in <kbd>y</kbd> will range from 0 to <em>N-1</em>. Let's separate the data into classes based on the labels:</li>
</ol>
<pre style="padding-left: 60px">class_0 = np.array([X[i] for i in range(len(X)) if y[i]==0])<br/>class_1 = np.array([X[i] for i in range(len(X)) if y[i]==1])</pre>
<ol start="4">
<li>To get an idea about our data, let's plot it, as follows:</li>
</ol>
<pre style="padding-left: 60px">plt.figure()<br/>plt.scatter(class_0[:,0], class_0[:,1], color='black', <strong>marker='s'</strong>)<br/>plt.scatter(class_1[:,0], class_1[:,1], color='black', <strong>marker='x'</strong>)<br/>plt.show()</pre>
<p style="padding-left: 60px">This is a <strong>scatterplot</strong>, where we use squares and crosses to plot the points. In this context, the <kbd>marker</kbd> parameter specifies the shape you want to use. We use squares to denote points in <kbd>class_0</kbd> and crosses to denote points in <kbd>class_1</kbd>. If you run this code, you will see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-972 image-border" src="assets/542cb6f5-7a7c-486e-a1be-823c65eb12d8.png" style="width:24.50em;height:24.75em;"/></p>
<ol start="5">
<li>In the preceding two lines, we just use the mapping between <kbd>X</kbd> and <kbd>y</kbd> to create two lists. If you were asked to inspect the datapoints visually and draw a separating line, what would you do? You would simply draw a line in between them. Let's go ahead and do this:</li>
</ol>
<pre style="padding-left: 60px">line_x = range(10)<br/>line_y = line_x</pre>
<ol start="6">
<li>We just created a line with the mathematical equation <em>y = x</em>. Let's plot it, as follows:</li>
</ol>
<pre style="padding-left: 60px">plt.figure()<br/>plt.scatter(class_0[:,0], class_0[:,1], color='black', marker='s')<br/>plt.scatter(class_1[:,0], class_1[:,1], color='black', marker='x')<br/>plt.plot(line_x, line_y, color='black', linewidth=3)<br/>plt.show()</pre>
<p class="mce-root"/>
<ol start="7">
<li>If you run this code, you should see the following output:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-973 image-border" src="assets/fde419aa-36d1-4312-9a2c-f49b054f62c8.png" style="width:30.08em;height:23.25em;"/></div>
<p style="padding-left: 60px">The preceding shows how that construction of a separation line between the two classes was simple. In this simple example, this operation was easy, but in many cases, building a line of separation between two classes can be very difficult.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we showed how simple it is to build a classifier. We started from a series of identifying pairs of as many points on a plane (<em>x, y</em>). We therefore assigned a class to each of these points (0,1) so as to divide them into two groups. To understand the spatial arrangement of these points, we visualized them by associating <span>a different marker </span>to each class. Finally, to divide the two groups, we have drew the line of the <em>y = x</em> equation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>We built a simple classifier using the following rule—the input point (<em>a, b</em>) belongs to <kbd>class_0</kbd> if <em>a</em> is greater than or equal to <em>b;</em> otherwise, it belongs to <kbd>class_1</kbd>. If you inspect the points one by one, you will see that this is, in fact, true. That's it! You just built a linear classifier that can classify unknown data. It's a linear classifier because the separating line is a straight line. If it's a curve, then it becomes a <em>nonlinear</em> classifier.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This formation worked well, because there were a limited number of points, and we could visually inspect them. What if there were thousands of points? How would we generalize this process? Let's discuss that in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the NumPy library (<a href="http://www.numpy.org/">http://www.numpy.org/</a>)</li>
<li><span>The official documentation of the </span>Matplotlib library (<a href="https://matplotlib.org/">https://matplotlib.org/</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a logistic regression classifier</h1>
                </header>
            
            <article>
                
<p>Despite the word <em>regression</em> being present in the name, logistic regression is actually used for classification purposes. Given a set of datapoints, our goal is to build a model that can draw linear boundaries between our classes. It extracts these boundaries by solving a set of equations derived from the training data. In this recipe, we will build a logistic regression classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Logistic regression is a non-linear regression model used when the dependent variable is dichotomous. The purpose is to establish the probability with which an observation can generate one or the other value of the dependent variable; it can also be used to classify observations, according to their characteristics, into two categories. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a logistic regression classifier:</p>
<ol>
<li>Let's see how to do this in Python. We will use the <kbd>logistic_regression.py</kbd> file, provided to you as a reference. Assuming that you imported the necessary packages, let's create some sample data, along with training labels:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from sklearn import linear_model<br/>import matplotlib.pyplot as plt<br/>X = np.array([[4, 7], [3.5, 8], [3.1, 6.2], [0.5, 1], [1, 2], [1.2, 1.9], [6, 2], [5.7, 1.5], [5.4, 2.2]])<br/>y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])</pre>
<p style="padding-left: 60px">Here, we assume that we have three classes (<kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd>).</p>
<ol start="2">
<li>Let's initialize the logistic regression classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier = linear_model.LogisticRegression(solver='<span>lbfgs</span>', C=100)</pre>
<p style="padding-left: 60px">There are a number of input parameters that can be specified for the preceding function, but a couple of important ones are <kbd>solver</kbd> and <kbd>C</kbd>. The <kbd>solver</kbd> parameter specifies the type of <kbd>solver</kbd> that the algorithm will use to solve the system of equations. The <kbd>C</kbd> parameter controls the regularization strength. A lower value indicates higher regularization strength.</p>
<ol start="3">
<li>Let's train the classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier.fit(X, y)</pre>
<ol start="4">
<li>Let's draw datapoints and boundaries. To do this, first, we need to define ranges to plot the diagram, as follows:</li>
</ol>
<pre style="padding-left: 60px">x_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0<br/>y_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0</pre>
<p style="padding-left: 60px">The preceding values indicate the range of values that we want to use in our figure. The values usually range from the minimum value to the maximum value present in our data. We add some buffers, such as <kbd>1.0</kbd>, to the preceding lines, for clarity.</p>
<ol start="5">
<li>In order to plot the boundaries, we need to evaluate the function across a grid of points and plot it. Let's go ahead and define the grid:</li>
</ol>
<pre style="padding-left: 60px"># denotes the step size that will be used in the mesh grid<br/>step_size = 0.01<br/><br/># define the mesh grid<br/>x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))</pre>
<p style="padding-left: 60px">The <kbd>x_values</kbd> and <kbd>y_values</kbd> variables contain the grid of points where the function will be evaluated.</p>
<ol start="6">
<li>Let's compute the output of the classifier for all these points:</li>
</ol>
<pre style="padding-left: 60px"># compute the classifier output<br/>mesh_output = classifier.predict(np.c_[x_values.ravel(), y_values.ravel()])<br/><br/># reshape the array<br/>mesh_output = mesh_output.reshape(x_values.shape)</pre>
<ol start="7">
<li>Let's plot the boundaries using colored regions:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"># Plot the output using a colored plot <br/>plt.figure()<br/><br/># choose a color scheme you can find all the options <br/># here: http://matplotlib.org/examples/color/colormaps_reference.html<br/>plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)</pre>
<p style="padding-left: 60px">This is basically a 3D plotter that takes the 2D points and the associated values to draw different regions using a color scheme. </p>
<ol start="8">
<li>Let's overlay the training points on the plot:</li>
</ol>
<pre style="padding-left: 60px"># Overlay the training points on the plot <br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)<br/><br/># specify the boundaries of the figure<br/>plt.xlim(x_values.min(), x_values.max())<br/>plt.ylim(y_values.min(), y_values.max())<br/><br/># specify the ticks on the X and Y axes<br/>plt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))<br/>plt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))<br/><br/>plt.show()</pre>
<p style="padding-left: 60px">Here, <kbd>plt.scatter</kbd> plots the points on the 2D graph. <kbd>X[:, 0]</kbd> specifies that we should take all the values along the 0 axis (the <em>x</em> axis in our case), and <kbd>X[:, 1]</kbd> specifies axis 1 (the <em>y</em> axis). The <kbd>c=y</kbd> parameter indicates the color sequence. We use the target labels to map to colors using <kbd>cmap</kbd>. Basically, we want different colors that are based on the target labels. Hence, we use <kbd>y</kbd> as the mapping. The limits of the display figure are set using <kbd>plt.xlim</kbd> and <kbd>plt.ylim</kbd>. In order to mark the axes with values, we need to use <kbd>plt.xticks</kbd> and <kbd>plt.yticks</kbd>. These functions mark the axes with values so that it's easier for us to see where the points are located. In the preceding code, we want the ticks to lie between the minimum and maximum values with a buffer of one unit. Also, we want these ticks to be integers. So, we use the <kbd>int()</kbd> function to round off the values.</p>
<ol start="9">
<li>If you run this code, you should see the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-974 image-border" src="assets/34c330f7-3444-4265-b0cd-f37c27e72382.png" style="width:112.83em;height:55.42em;"/></p>
<ol start="10">
<li>Let's see how the <kbd>C</kbd> parameter affects our model. The <kbd>C</kbd> parameter indicates the penalty for misclassification. If we set it to <kbd>1.0</kbd>, we will get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-975 image-border" src="assets/72defd40-8554-499b-b9ef-562ef7291b64.png" style="width:118.83em;height:58.83em;"/></p>
<p class="mce-root"/>
<ol start="11">
<li>If we set <kbd>C</kbd> to <kbd>10000</kbd>, we get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-976 image-border" src="assets/ac2af76f-7377-4329-a287-9e74512b7094.png" style="width:44.42em;height:21.33em;"/></p>
<p style="padding-left: 60px">As we increase <kbd>C</kbd>, there is a higher penalty for misclassification. Hence, the boundaries become more optimized.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><strong>Logistic regression</strong> is a classification method within the family of supervised learning algorithms. Using statistical methods, logistic regression allows us to generate a result that, in fact, represents a probability that a given input value belongs to a given class. In binomial logistic regression problems, the probability that output belongs to a class will be <em>P</em>, whereas the probability of it belonging to another class will be <em>1-P</em> (where <em>P</em> is a number between 0 and 1 because it expresses probability).</p>
<p class="mce-root">Logistic regression uses the logistic function to determine the classification of input values. Also called the <strong>sigmoid</strong> function, the logistic function is an S-shaped curve that can take any number of of a real value and map it to a value between 0 and 1, extremes excluded. It can be described by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/00e3c0f7-9412-4655-95a8-116901dcc226.png" style="width:14.00em;height:3.33em;"/></p>
<p>This function transforms the real values into numbers between 0 and 1.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>To obtain the logistic regression equation expressed in probabilistic terms, we need to include the probabilities in the logistic regression equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/416b339e-3214-4a8f-bab8-433e2a986a65.png" style="width:13.83em;height:3.58em;"/></p>
<p>Recalling that the <kbd>e</kbd> function is the opposite of the natural logarithm (<kbd>ln</kbd>), we can write:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1ff52b3b-5cbb-41a7-b55b-cd2e44a6804e.png" style="width:11.58em;height:2.92em;"/></p>
<p>This function is called a <strong>logit</strong> function. The logit function, on the other hand, allows us to associate the probabilities (therefore, a value included between 0 and 1) to the whole range of real numbers. It is a link function and represents the inverse of the logistic function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Logit Models for Binary Data</em>, Princeton University: <a href="https://data.princeton.edu/wws509/notes/c3.pdf">https://data.princeton.edu/</a></li>
<li><em>Regression Analysis with R</em>, <span>Giuseppe Ciaburro,</span> Packt Publishing</li>
<li><a href="https://data.princeton.edu/wws509/notes/c3.pdf">wws509/notes/c3.pdf</a></li>
<li><span>Matplotlib color scheme options: </span><a href="http://matplotlib.org/examples/color/colormaps_reference.html">https://matplotlib.org/examples/color/colormaps_reference.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Naive Bayes classifier</h1>
                </header>
            
            <article>
                
<p>A classifier solves the problem of identifying sub-populations of individuals with certain features in a larger set, with the possible use of a subset of individuals known as a priori (a training set). A Naive Bayes classifier is a supervised learning classifier that uses Bayes' theorem to build the model. In this recipe, we will build a Naive Bayes classifier.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>The underlying principle of a Bayesian classifier is that some individuals belong to a class of interest with a given probability based on some observations. This probability is based on the assumption that the characteristics observed can be either dependent or independent from one another; in this second case, the Bayesian classifier is called Naive because it assumes that the presence or absence of a particular characteristic in a given class of interest is not related to the presence or absence of other characteristics, greatly simplifying the calculation. Let's go ahead and build a Naive Bayes classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a Naive Bayes classifier:</p>
<ol>
<li>We will use <kbd>naive_bayes.py</kbd>, provided to you as a reference. Let's import some libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.naive_bayes import GaussianNB</pre>
<ol start="2">
<li>You were provided with a <kbd>data_multivar.txt</kbd> file. This contains data that we will use here. This contains comma-separated numerical data in each line. Let's load the data from this file:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'data_multivar.txt'<br/>X = []<br/>y = []<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = [float(x) for x in line.split(',')]<br/>        X.append(data[:-1])<br/>        y.append(data[-1])<br/>X = np.array(X)<br/>y = np.array(y)</pre>
<p style="padding-left: 60px">We have now loaded the input data into <kbd>X</kbd> and the labels into <kbd>y</kbd>. There are four labels: 0, 1, 2, and 3.</p>
<ol start="3">
<li>Let's build the Naive Bayes classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier_gaussiannb = GaussianNB()<br/>classifier_gaussiannb.fit(X, y)<br/>y_pred = classifier_gaussiannb.predict(X)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The <kbd>gauusiannb</kbd> function specifies the Gaussian Naive Bayes model.</p>
<ol start="4">
<li>Let's compute the <kbd>accuracy</kbd> measure of the classifier:</li>
</ol>
<pre style="padding-left: 60px">accuracy = 100.0 * (y == y_pred).sum() / X.shape[0]<br/>print("Accuracy of the classifier =", round(accuracy, 2), "%")</pre>
<p style="padding-left: 60px">The following accuracy is returned:</p>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier = 99.5 %</strong></pre>
<ol start="5">
<li>Let's plot the data and the boundaries. We will use the procedure followed in the previous recipe, <em>Building a logistic regression classifier</em>:</li>
</ol>
<pre style="padding-left: 60px">x_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0<br/>y_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0<br/><br/># denotes the step size that will be used in the mesh grid<br/>step_size = 0.01<br/><br/># define the mesh grid<br/>x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))<br/><br/># compute the classifier output<br/>mesh_output = classifier_gaussiannb.predict(np.c_[x_values.ravel(), y_values.ravel()])<br/><br/># reshape the array<br/>mesh_output = mesh_output.reshape(x_values.shape)<br/><br/># Plot the output using a colored plot <br/>plt.figure()<br/><br/># choose a color scheme <br/>plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)<br/><br/># Overlay the training points on the plot <br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)<br/><br/># specify the boundaries of the figure<br/>plt.xlim(x_values.min(), x_values.max())<br/>plt.ylim(y_values.min(), y_values.max())<br/><br/># specify the ticks on the X and Y axes<br/>plt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))<br/>plt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))<br/><br/>plt.show()</pre>
<p style="padding-left: 60px">You should see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-977 image-border" src="assets/b8c4578e-99bb-4249-ba9e-1658a56cffc9.png" style="width:128.83em;height:65.00em;"/></p>
<p class="mce-root">There is no restriction on the boundaries to be linear here. In the preceding recipe, <em>Building a logistic regression classifier</em>, we used up all the data for training. A good practice in machine learning is to have non-overlapping data for training and testing. Ideally, we need some unused data for testing so that we can get an accurate estimate of how the model performs on unknown data. There is a provision in <kbd>scikit-learn</kbd> that handles this very well, as shown in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A <strong>Bayesian classifier</strong> is a classifier based on the application of Bayes' theorem. This classifier requires the knowledge of a priori and conditional probabilities related to the problem; quantities that, in general, are not known but are typically estimable. If reliable estimates of the probabilities involved in the theorem can be obtained, the Bayesian classifier is generally reliable and potentially compact.</p>
<p>The probability that a given event (<em>E</em>) occurs, is the ratio between the number (<em>s</em>) of favorable cases of the event itself and the total number (<em>n</em>) of the possible cases, provided all the considered cases are equally probable. This can be better represented using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a6c805cf-487c-462a-aeb1-c403af96d89d.png" style="width:24.50em;height:2.58em;"/></p>
<p>Given two events, <em>A</em> and <em>B</em>, if the two events are independent (the occurrence of one does not affect the probability of the other), the joint probability of the event is equal to the product of the probabilities of <em>A</em> and <em>B</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/79748dbb-b02c-44a6-b528-0b310abefd67.png" style="width:11.92em;height:1.25em;"/></p>
<p>If the two events are dependent (that is, the occurrence of one affects the probability of the other), then the same rule may apply, provided <em>P(B | A)</em> is the probability of event <em>A</em> given that event <em>B</em> has occurred. This condition introduces conditional probability, which we are going to dive into now:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/53491427-e7b8-4df8-bddc-9a084ac16632.png" style="width:13.92em;height:1.33em;"/></p>
<p>The probability that event <em>A</em> occurs, calculated on the condition that event <em>B</em> occurred, is called <strong>conditional probability</strong>, and is indicated by <em>P(A | B)</em>. It is calculated using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cba40f96-9806-4cb3-8c79-9854a24051cd.png" style="width:9.17em;height:2.58em;"/></p>
<p>Let <em>A</em> and <em>B</em> be two dependent events, as we stated that the joint probability between them is calculated using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08b9a04b-428f-404e-b067-96923b283638.png" style="width:13.92em;height:1.33em;"/></p>
<p>Or, similarly, we can use the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0bc29719-ee15-483d-b8e6-a1e6125e4f59.png" style="width:13.92em;height:1.33em;"/></p>
<p>By looking at the two formulas, we see that they have the first equal member. This shows that even the second members are equal, so the following equation can be written:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a06d2fd7-d73f-471d-a77f-e3675deab9c8.png" style="width:16.92em;height:1.33em;"/></p>
<p>By solving these equations for conditional probability, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a145cfc1-adaf-4937-8bd2-f1bca05834ef.png" style="width:12.25em;height:2.67em;"/></p>
<p>The proposed formulas represent the mathematical statement of Bayes' theorem. The use of one or the other depends on what we are looking for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In 1763, an article by Reverend Thomas Bayes was published in England; the article became famous for its implications. According to the article, making predictions about a phenomenon depends not only on the observations that the scientist obtains from his experiments, but also on what he himself thinks and understands of the phenomenon studied, even before proceeding to the experiment itself. These premises were developed in the 1900s by distinguished scholars, such as Bruno de Finetti (<em>La prévision: ses lois logiques, ses sources subjectives</em>, 1937), L J Savage (<em>The Fondations of statistics Reconsidered</em>, 1959), and others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Keras 2.x Projects</em>, Giuseppe Ciaburro,</span> Packt Publishing<span>.</span></li>
<li><em>Bayes' Theorem</em>, Stanford Encyclopedia of Philosophy: <a href="https://plato.stanford.edu/entries/bayes-theorem/">https://plato.stanford.edu/entries/bayes-theorem/</a></li>
<li><span>The official documentation of the <kbd>sklearn.naive_bayes.GaussianNB</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting a dataset for training and testing</h1>
                </header>
            
            <article>
                
<p>Let's see how to split our data properly into training and testing datasets. As we said in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a><span>, </span><em>The Realm of Supervised Learning</em>, in the <em>Building a linear regressor</em> recipe, <span>when we build a machine learning model, we need a way to validate our model to check whether it is performing at a satisfactory level. To do this, we need to separate our data into two groups—a <strong>training</strong> dataset and a <strong>testing</strong> dataset. The training dataset will be used to build the model, and the testing dataset will be used to see how this trained model performs on unknown data. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>In this recipe, we will learn how to split the dataset for training and testing phases.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">The fundamental objective of a model based on machine learning is to make accurate predictions. Before using a model to make predictions, it is necessary to evaluate the predictive performance of the model. To estimate the quality of a model's predictions, it is necessary to use data that you have never seen before. Training a predictive model and testing it on the same data is a methodological error: a model that simply classifies the labels of samples it has just seen would have a high score but would not be able to predict the new data class. Under these conditions, the generalization capacity of the model would be less.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to split the dataset:</p>
<ol>
<li>The first part of the recipe is similar to the previous recipe, <em>Building a Naive Bayes classifier</em> (load the <kbd>Splitting_dataset.py</kbd> file):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.naive_bayes import GaussianNB <br/><br/>input_file = 'data_multivar.txt'<br/><br/>X = []<br/>y = []<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = [float(x) for x in line.split(',')]<br/>        X.append(data[:-1])<br/>        y.append(data[-1]) <br/><br/>X = np.array(X)<br/>y = np.array(y)<br/><br/>#Splitting the dataset for training and testing<br/>from sklearn import model_selection<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)<br/><br/>#Building the classifier<br/>classifier_gaussiannb_new = GaussianNB()<br/>classifier_gaussiannb_new.fit(X_train, y_train)</pre>
<p style="padding-left: 60px">Here, we allocated 25% of the data for testing, as specified by the <kbd>test_size</kbd> parameter. The remaining 75% of the data will be used for training.</p>
<ol start="2">
<li>Let's evaluate the classifier on the test data:</li>
</ol>
<pre style="padding-left: 60px">y_test_pred = classifier_gaussiannb_new.predict(X_test)</pre>
<ol start="3">
<li>Let's compute the <kbd>accuracy</kbd> measure of the classifier:</li>
</ol>
<pre style="padding-left: 60px">accuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]<br/>print("Accuracy of the classifier =", round(accuracy, 2), "%")</pre>
<p style="padding-left: 60px">The following result is printed:</p>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier = 98.0 %</strong></pre>
<ol start="4">
<li>Let's plot the datapoints and the boundaries on the test data:</li>
</ol>
<pre style="padding-left: 60px">#Plot a classifier<br/>#Define the data<br/>X= X_test<br/>y=y_test<br/><br/># define ranges to plot the figure <br/>x_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0<br/>y_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0<br/><br/># denotes the step size that will be used in the mesh grid<br/>step_size = 0.01<br/><br/># define the mesh grid<br/>x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))<br/><br/># compute the classifier output<br/>mesh_output = classifier_gaussiannb_new.predict(np.c_[x_values.ravel(), y_values.ravel()])<br/><br/># reshape the array<br/>mesh_output = mesh_output.reshape(x_values.shape)<br/><br/># Plot the output using a colored plot <br/>plt.figure()<br/><br/># choose a color scheme<br/>plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)<br/><br/># Overlay the training points on the plot <br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)<br/><br/># specify the boundaries of the figure<br/>plt.xlim(x_values.min(), x_values.max())<br/>plt.ylim(y_values.min(), y_values.max())<br/><br/># specify the ticks on the X and Y axes<br/>plt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))<br/>plt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))<br/><br/>plt.show()</pre>
<ol start="5">
<li>You should see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-979 image-border" src="assets/562ba592-f77d-4143-b8f7-ebeadc718a95.png" style="width:80.33em;height:49.17em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we split the data using the <kbd>train_test_split()</kbd> function of the <kbd>scikit-learn</kbd> library. This function splits arrays or matrices into random train and testing subsets. Random division of input data into data sources for training and testing ensures that data distribution is similar for training and testing data sources. You choose this option when it is not necessary to preserve the order of the input data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The performance estimate depends on the data used. Therefore, simply dividing data randomly into a training and a testing set does not guarantee that the results are statistically significant. The repetition of the evaluation on different random divisions and the calculation of the performance in terms of the average and standard deviation of the individual evaluations creates a more reliable estimate.</p>
<p class="mce-root">However, even the repetition of evaluations on different random divisions could prevent the most complex data being classified in the testing (or training) phase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>The official documentation of the <kbd>sklearn.model_selection.train_test_split</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a></span></li>
<li><em>Data Splitting</em>, Charles University: <a href="https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf">https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating accuracy using cross-validation metrics</h1>
                </header>
            
            <article>
                
<p><strong>Cross-validation</strong> is an important concept in machine learning. In the previous recipe, we split the data into training and testing datasets. However, in order to make it more robust, we need to repeat this process with different subsets. If we just fine-tune it for a particular subset, we may end up overfitting the model. <strong>Overfitting</strong> refers to a situation where we fine-tune a model to a dataset too much and it fails to perform well on unknown data. We want our machine learning model to perform well on unknown data. In this recipe, we will learn how to evaluate model accuracy using cross-validation metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready…</h1>
                </header>
            
            <article>
                
<p>When we are dealing with machine learning models, we usually care about three things—precision, recall, and F1 score. We can get the required performance metric using parameter scoring. <strong>Precision</strong> refers to the number of items that are correctly classified as a percentage of the overall number of items in the list. <strong>Recall</strong> refers to the number of items that are retrieved as a percentage of the overall number of items in the training list.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to evaluate model accuracy using cross-validation metrics:</p>
<ol>
<li>We will use the classifier just used in the <em>Building a Naive Bayes</em> classifier recipe (load the <kbd>naive_bayes.py</kbd> file). We will start with the <kbd>accuracy</kbd> measure:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import model_selection<br/>num_validations = 5<br/>accuracy = model_selection.cross_val_score(classifier_gaussiannb,<br/>        X, y, scoring='accuracy', cv=num_validations)<br/>print "Accuracy: " + str(round(100*accuracy.mean(), 2)) + "%"</pre>
<ol start="2">
<li>We will use the preceding function to compute <kbd>precision</kbd>, <kbd>recall</kbd>, and the <kbd>F1</kbd> score as well:</li>
</ol>
<pre>f1 = model_selection.cross_val_score(classifier_gaussiannb,<br/> X, y, scoring='f1_weighted', cv=num_validations)<br/>print "F1: " + str(round(100*f1.mean(), 2)) + "%"<br/>precision = model_selection.cross_val_score(classifier_gaussiannb,<br/> X, y, scoring='precision_weighted', cv=num_validations)<br/>print "Precision: " + str(round(100*precision.mean(), 2)) + "%"<br/>recall = model_selection.cross_val_score(classifier_gaussiannb,<br/> X, y, scoring='recall_weighted', cv=num_validations)<br/>print "Recall: " + str(round(100*recall.mean(), 2)) + "%"<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Let's consider a test dataset containing 100 items, out of which 82 are of interest to us. Now, we want our classifier to identify these 82 items for us. Our classifier picks out 73 items as the items of interest. Out of these 73 items, only 65 are actually items of interest, and the remaining 8 are misclassified. We can compute precision in the following way:</p>
<ul>
<li>The number of correct identifications = 65</li>
<li>The total number of identifications = 73</li>
<li>Precision = 65 / 73 = 89.04%</li>
</ul>
<p>To compute recall, we use the following:</p>
<ul>
<li>The total number of items of interest in the dataset = 82</li>
<li>The number of items retrieved correctly = 65</li>
<li>Recall = 65 / 82 = 79.26%</li>
</ul>
<p>A good machine learning model needs to have good precision and good recall simultaneously. It's easy to get one of them to 100%, but the other metric suffers! We need to keep both metrics high at the same time. To quantify this, we use an F1 score, which is a combination of precision and recall. This is actually the harmonic mean of precision and recall:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c9aa3ca0-efcb-46da-a9ef-329b618d3b7e.png" style="width:17.42em;height:3.08em;"/></p>
<p>In the preceding case, the F1 score will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1af8b3e1-849e-432e-816b-5b30076ce4c4.png" style="width:17.58em;height:2.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In cross-validation, all available data is used, in groups of a fixed size, alternatively as a testing and as a training set. Therefore, each pattern is either classified (at least once) or used for training. The performances obtained depend, however, on the particular division. Therefore, it may be useful to repeat cross-validation several times in order to become independent of the particular division.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.model_selection.cross_val_score</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score</a></li>
<li class="mce-root"><em>Cross-validation</em> (from scikit-learn's official documentation): <a href="http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/cross_validation.html">http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/cross_validation.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing a confusion matrix</h1>
                </header>
            
            <article>
                
<p>A <strong>confusion matrix</strong> is a table that we use to understand the performance of a classification model. This helps us understand how we classify testing data into different classes. When we want to fine-tune our algorithms, we need to understand how data gets misclassified before we make these changes. Some classes are worse than others, and the confusion matrix will help us understand this. Let's look at the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-62 image-border" src="assets/d1564065-1bf8-4576-ac73-5c69187fd71f.png" style="width:25.33em;height:17.50em;"/></p>
<p>In the preceding diagram, we can see how we categorize data into different classes. Ideally, we want all the non-diagonal elements to be 0. This would indicate perfect classification! Let's consider class 0. Overall, 52 items actually belong to class 0. We get 52 if we sum up the numbers in the first row. Now, 45 of these items are being predicted correctly, but our classifier says that 4 of them belong to class 1 and three of them belong to class 2. We can apply the same analysis to the remaining 2 rows as well. An interesting thing to note is that 11 items from class 1 are misclassified as class 0. This constitutes around 16% of the datapoints in this class. This is an insight that we can use to optimize our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">A confusion matrix identifies the nature of the classification errors, as our classification results are compared to real data. In this matrix, the diagonal cells show the number of cases that were correctly classified; all the others cells show the misclassified cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to visualize the confusion matrix:</p>
<ol>
<li>We will use the <kbd>confusion_matrix.py</kbd> file that we already provided to you as a reference. Let's see how to extract the confusion matrix from our data:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.metrics import confusion_matrix</pre>
<p style="padding-left: 60px">We use some sample data here. We have 4 classes with values ranging from 0 to 3. We have predicted labels as well. We use the <kbd>confusion_matrix</kbd> method to extract the confusion matrix and plot it.</p>
<ol start="2">
<li>Let's go ahead and define this function:</li>
</ol>
<pre style="padding-left: 60px"># Show confusion matrix<br/>def plot_confusion_matrix(confusion_mat):<br/>    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Paired)<br/>    plt.title('Confusion matrix')<br/>    plt.colorbar()<br/>    tick_marks = np.arange(4)<br/>    plt.xticks(tick_marks, tick_marks)<br/>    plt.yticks(tick_marks, tick_marks)<br/>    plt.ylabel('True label')<br/>    plt.xlabel('Predicted label')<br/>    plt.show()</pre>
<p style="padding-left: 60px">We use the <kbd>imshow</kbd> function to plot the confusion matrix. Everything else in the function is straightforward! We just set the title, color bar, ticks, and the labels using the relevant functions. The <kbd>tick_marks</kbd> argument range from 0 to 3 because we have 4 distinct labels in our dataset. The <kbd>np.arange</kbd> function gives us this <kbd>numpy</kbd> array.</p>
<ol start="3">
<li>Let's define the data (real and predicted) and then we will call the <kbd>confusion_matrix</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">y_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]<br/>y_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]<br/>confusion_mat = confusion_matrix(y_true, y_pred)<br/>plot_confusion_matrix(confusion_mat)</pre>
<ol start="4">
<li>If you run the preceding code, you will see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-980 image-border" src="assets/a194b459-0515-4dde-9a5c-fa1fd33ecbe9.png" style="width:37.25em;height:32.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>The diagona</span>l colors are strong, and we want them to be strong. The black color indicates zero. There are a couple of gray squares in the non-diagonal spaces, which indicate misclassification. For example, when the real label is 0, the predicted label is 1, as we can see in the first row. In fact, all the misclassifications belong to class 1 in the sense that the second column contains 3 rows that are non-zero. It's easy to see this from the matrix.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A confusion matrix displays information about the actual and predicted classifications made by a model. The performance of such systems is evaluated with the help of data in the matrix.</p>
<p class="mce-root">The following table shows the confusion matrix for a two-class classifier:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 194.931px">
<p> </p>
</td>
<td style="width: 176.042px">
<p><span>PREDICTED </span>POSITIVE</p>
</td>
<td style="width: 158.264px">
<p><span>PREDICTED </span>NEGATIVE</p>
</td>
</tr>
<tr>
<td style="width: 194.931px">
<p><strong>Actual TRUE</strong></p>
</td>
<td style="width: 176.042px">
<p>TP</p>
</td>
<td style="width: 158.264px">
<p>FN</p>
</td>
</tr>
<tr>
<td style="width: 194.931px">
<p><strong>Actual FALSE</strong></p>
</td>
<td style="width: 176.042px">
<p>FP</p>
</td>
<td style="width: 158.264px">
<p>TN</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The entries in the confusion matrix have the following meanings:</p>
<ul>
<li>TP is the number of correct predictions that an instance is positive</li>
<li>FN is the number of incorrect predictions that an instance is negative</li>
<li>FP is the number of incorrect predictions that an instance is positive</li>
<li>TN is the number of correct predictions that an instance is negative</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The confusion matrix shows us the performance of an algorithm. Each row returns the instances in an actual class, while each column returns the instances in an expected class. The term <em>confusion matrix</em> results from the fact that it makes it easy to see whether the system is confusing two classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.metrics.confusion_matrix()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a></li>
<li><em>Confusion Matrix</em>, University of Notre Dame: <a href="https://www3.nd.edu/~busiforc/Confusion_Matrix.html">https://www3.nd.edu/~busiforc/Confusion_Matrix.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting a performance report</h1>
                </header>
            
            <article>
                
<p>In the <em>Evaluating accuracy using cross-validation metrics</em> recipe, we calculated some metrics to measure the accuracy of the model. Let's remember its meaning. The accuracy returns the percentage of correct classifications. Precision returns the percentage of positive classifications that are correct. Recall (sensitivity) returns the percentage of positive elements of the testing set that have been classified as positive. Finally, in F1, both the precision and the recall are used to compute the score. In this recipe, we will learn how to extract a performance report.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We also have a function in <kbd>scikit-learn</kbd> that can directly print the precision, recall, and F1 scores for us. Let's see how to do this.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to extract a performance report:</p>
<ol>
<li>Add the following lines to a new Python file (load the <kbd>performance_report.py</kbd> file):</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import classification_report<br/>y_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]<br/>y_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]<br/>target_names = ['Class-0', 'Class-1', 'Class-2', 'Class-3']<br/>print(classification_report(y_true, y_pred, target_names=target_names))</pre>
<ol start="2">
<li>If you run this code, you will see the following on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-981 image-border" src="assets/7e6c43dc-441a-4ce6-a256-d1fa06905490.png" style="width:30.83em;height:8.25em;"/></p>
<p style="padding-left: 60px">Instead of computing these metrics separately, you can directly use the preceding function to extract those statistics from your model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we used the <kbd>classification_report ()</kbd> function of the scikit-learn library to extract a performance report. This function builds a text report showing the main classification metrics. A text summary of the precision, recall, and the F1 score for each class is returned. Referring to the terms introduced in the confusion matrix addressed in the previous recipe, these metrics are calculated as follows:</p>
<ul>
<li class="mce-root">The precision is the ratio tp / (tp + fp), where tp is the number of true positives and fp the number of false positives. The precision is the ability of the classifier to not label <span>a sample that is negative </span>as positive.</li>
<li class="mce-root">The recall is the ratio tp / (tp + fn), where tp is the number of true positives and fn the number of false negatives. The recall is the ability of the classifier to find the positive samples.</li>
<li class="mce-root">The F1 score is said to be a weighted harmonic mean of the precision and recall, where an F-beta score reaches its peak value at 1 and its lowest score at 0.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The reported averages include the <strong>micro average</strong> (averaging the total true positives, false negatives, and false positives), the <strong>macro average</strong> (averaging the unweighted mean per label), the <strong>weighted</strong> <strong>average</strong> (averaging the support-weighted mean per label), and the <strong>sample average</strong> (only for multilabel classification).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.metrics.classification_report()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating cars based on their characteristics</h1>
                </header>
            
            <article>
                
<p>In this recipe, let's see how we can apply classification techniques to a real-world problem. We will use a dataset that contains some details about cars, such as number of doors, boot space, maintenance costs, and so on. Our goal is to determine the quality of the car. For the purposes of classification, quality can take four values: unacceptable, acceptable, good, or very good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You can download the dataset at <a href="https://archive.ics.uci.edu/ml/datasets/Car+Evaluation">https://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>.</p>
<p>You need to treat each value in the dataset as a string. We consider six attributes in the dataset. Here are the attributes along with the possible values they can take:</p>
<ul>
<li><kbd>buying</kbd>: These will be <kbd>vhigh</kbd>, <kbd>high</kbd>, <kbd>med</kbd>, and <kbd>low</kbd>.</li>
<li><kbd>maint</kbd>: These will be <kbd>vhigh</kbd>, <kbd>high</kbd>, <kbd>med</kbd>, and <kbd>low</kbd>.</li>
<li><kbd>doors</kbd>: These will be <kbd>2</kbd>, <kbd>3</kbd>, <kbd>4</kbd>, <kbd>5</kbd>, and <kbd>more</kbd>.</li>
<li><kbd>persons</kbd>: These will be <kbd>2</kbd>, <kbd>4</kbd>, and <kbd>more</kbd>.</li>
<li><kbd>lug_boot</kbd>: These will be <kbd>small</kbd>, <kbd>med</kbd>, and <kbd>big</kbd>.</li>
<li><kbd>safety</kbd>: These will be <kbd>low</kbd>, <kbd>med</kbd>, and <kbd>high</kbd>.</li>
</ul>
<p>Given that each line contains strings, we need to assume that all the features are strings and design a classifier. In the previous chapter, we used random forests to build a regressor. In this recipe, we will use random forests as a classifier.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to evaluate cars based on their characteristics:</p>
<ol>
<li>We will use the <kbd>car.py</kbd> file that we already provided to you as reference. Let's go ahead and import a couple of packages:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import preprocessing<br/>from sklearn.ensemble import RandomForestClassifier</pre>
<ol start="2">
<li>Let's load the dataset:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'car.data.txt'<br/># Reading the data<br/>X = []<br/>count = 0<br/>with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        data = line[:-1].split(',')<br/>        X.append(data)<br/>X = np.array(X)</pre>
<p style="padding-left: 60px">Each line contains a comma-separated list of words. Therefore, we parse the input file, split each line, and then append the list to the main data. We ignore the last character on each line because it's a newline character. Python packages only work with numerical data, so we need to transform these attributes into something that those packages will understand.</p>
<ol start="3">
<li>In the previous chapter, we discussed label encoding. That is what we will use here to convert strings to numbers:</li>
</ol>
<pre style="padding-left: 60px"># Convert string data to numerical data<br/>label_encoder = []<br/>X_encoded = np.empty(X.shape)<br/>for i,item in enumerate(X[0]):<br/>    label_encoder.append(preprocessing.LabelEncoder())<br/>    X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])<br/>X = X_encoded[:, :-1].astype(int)<br/>y = X_encoded[:, -1].astype(int)</pre>
<p style="padding-left: 60px">As each attribute can take a limited number of values, we can use the label encoder to transform them into numbers. We need to use different label encoders for each attribute. For example, the <kbd>lug_boot</kbd> attribute can take three distinct values, and we need a label encoder that knows how to encode this attribute. The last value on each line is the class, so we assign it to the <em>y</em> variable.</p>
<ol start="4">
<li>Let's train the classifier:</li>
</ol>
<pre style="padding-left: 60px"># Build a Random Forest classifier<br/>params = {'n_estimators': 200, 'max_depth': 8, 'random_state': 7}<br/>classifier = RandomForestClassifier(**params)<br/>classifier.fit(X, y)</pre>
<p style="padding-left: 60px">You can play around with the <kbd>n_estimators</kbd> and <kbd>max_depth</kbd> parameters to see how they affect classification accuracy. We will actually do this soon in a standardized way.</p>
<ol start="5">
<li>Let's perform cross-validation:</li>
</ol>
<pre style="padding-left: 60px"># Cross validation<br/>from sklearn import model_selection<br/><br/>accuracy = model_selection.cross_val_score(classifier, <br/>        X, y, scoring='accuracy', cv=3)<br/>print("Accuracy of the classifier: " + str(round(100*accuracy.mean(), 2)) + "%")</pre>
<p style="padding-left: 60px">Once we train the classifier, we need to see how it performs. We use three-fold cross-validation to calculate the accuracy here. The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier: 78.19%</strong></pre>
<ol start="6">
<li>One of the main goals of building a classifier is to use it on isolated and unknown data instances. Let's use a single datapoint and see how we can use this classifier to categorize it:</li>
</ol>
<pre style="padding-left: 60px"># Testing encoding on single data instance<br/>input_data = ['high', 'low', '2', 'more', 'med', 'high']<br/>input_data_encoded = [-1] * len(input_data)<br/>for i,item in enumerate(input_data):<br/>    input_data_encoded[i] = int(label_encoder[i].transform([input_data[i]]))<br/>input_data_encoded = np.array(input_data_encoded)</pre>
<p style="padding-left: 60px">The first step was to convert that data into numerical data. We need to use the label encoders that we used during training because we want it to be consistent. If there are unknown values in the input datapoint, the label encoder will complain because it doesn't know how to handle that data. For example, if you change the first value in the list from <kbd>high</kbd> to <kbd>abcd</kbd>, then the label encoder won't work because it doesn't know how to interpret this string. This acts like an error check to see whether the input datapoint is valid.</p>
<ol start="7">
<li>We are now ready to predict the output class for this datapoint:</li>
</ol>
<pre style="padding-left: 60px"># Predict and print output for a particular datapoint<br/>output_class = classifier.predict([input_data_encoded])<br/>print("Output class:", label_encoder[-1].inverse_transform(output_class)[0])</pre>
<p style="padding-left: 60px">We use the <kbd>predict()</kbd> method to estimate the output class. If we output the encoded output label, it won't mean anything to us. Therefore, we use the <kbd>inverse_transform</kbd> method to convert this label back to its original form and print out the output class. <span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>Output class: acc</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <strong>random forest</strong> was developed by Leo Breiman (University of California, Berkeley, USA) based on the use of classification trees. He has extended the classification tree technique by integrating it into a Monte Carlo simulation procedure and named it <strong>random forest</strong>. It is based on the creation of a large set of tree classifiers, each of which is proposed to classify a single instance, wherein some features have been evaluated. Comparing the classification proposals provided by each tree in the forest shows the class to which to attribute the request: it is the one that received the most votes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Random forest has three adjustment parameters: the number of trees, the minimum amplitude of the terminal nodes, and the number of variables sampled in each node. The absence of overfitting makes the first two parameters important only from a computational point of view.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.ensemble.RandomForestClassifier()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>
<li class="mce-root"><em>Random Forests</em> by Leo Breiman and Adele Cutler (from the University of California, Berkeley): <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting validation curves</h1>
                </header>
            
            <article>
                
<p>We used random forests to build a classifier in the previous recipe, <em>Evaluating cars based on their characteristics</em>, but we don't exactly know how to define the parameters. In our case, we dealt with two parameters: <kbd>n_estimators</kbd> and <kbd>max_depth</kbd>. They are called <strong>hyperparameters</strong>, and the performance of the classifier depends on them. It would be nice to see how the performance gets affected as we change the hyperparameters. This is where validation curves come into the picture. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Validation curves help us understand how each hyperparameter influences the training score. Basically, all other parameters are kept constant and we vary the hyperparameter of interest according to our range. We will then be able to visualize how this affects the score.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to extract validation curves:</p>
<ol>
<li>Add the following code to the same Python file as in the previous recipe, <em>Evaluating cars based on their characteristics</em>:</li>
</ol>
<pre style="padding-left: 60px"># Validation curves<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import validation_curve<br/><br/>classifier = RandomForestClassifier(max_depth=4, random_state=7)<br/><br/>parameter_grid = np.linspace(25, 200, 8).astype(int)<br/>train_scores, validation_scores = validation_curve(classifier, X, y, "n_estimators", parameter_grid, cv=5)<br/>print("##### VALIDATION CURVES #####")<br/>print("\nParam: n_estimators\nTraining scores:\n", train_scores)<br/>print("\nParam: n_estimators\nValidation scores:\n", validation_scores)</pre>
<p style="padding-left: 60px">In this case, we defined the classifier by fixing the <kbd>max_depth</kbd> parameter. We want to estimate the optimal number of estimators to use, and so have defined our search space using <kbd>parameter_grid</kbd>. It is going to extract training and validation scores by iterating from 25 to 200 in 8 steps.</p>
<ol start="2">
<li>If you run it, you will see the following on your Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-982 image-border" src="assets/1d6021de-2c8e-4628-aeeb-5e64c53c3e5f.png" style="width:36.58em;height:24.00em;"/></p>
<ol start="3">
<li>Let's plot it:</li>
</ol>
<pre style="padding-left: 60px"># Plot the curve<br/>plt.figure()<br/>plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')<br/>plt.title('Training curve')<br/>plt.xlabel('Number of estimators')<br/>plt.ylabel('Accuracy')<br/>plt.show()</pre>
<ol start="4">
<li>Here is what you'll get:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-983 image-border" src="assets/c6c67e06-26df-495b-9a64-833b85df460e.png" style="width:86.08em;height:45.67em;"/></p>
<ol start="5">
<li>Let's do the same for the <kbd>max_depth</kbd> parameter:</li>
</ol>
<pre style="padding-left: 60px">classifier = RandomForestClassifier(n_estimators=20, random_state=7)<br/>parameter_grid = np.linspace(2, 10, 5).astype(int)<br/>train_scores, valid_scores = validation_curve(classifier, X, y, <br/>        "max_depth", parameter_grid, cv=5)<br/>print("\nParam: max_depth\nTraining scores:\n", train_scores)<br/>print("\nParam: max_depth\nValidation scores:\n", validation_scores)</pre>
<p style="padding-left: 60px">We fixed the <kbd>n_estimators</kbd> parameter at 20 to see how the performance varies with <kbd>max_depth</kbd>. Here is the output on the Terminal:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-984 image-border" src="assets/b9cd9ace-c7e5-44aa-835e-8745adf5c9ef.png" style="width:36.92em;height:19.17em;"/></p>
<ol start="6">
<li>Let's plot it:</li>
</ol>
<pre style="padding-left: 60px"># Plot the curve<br/>plt.figure()<br/>plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')<br/>plt.title('Validation curve')<br/>plt.xlabel('Maximum depth of the tree')<br/>plt.ylabel('Accuracy')<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>If you run this code, you will get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-985 image-border" src="assets/359e4feb-d95f-438a-ab51-087000626f5d.png" style="width:85.33em;height:47.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we used the <kbd>validation_curve</kbd> function of the scikit-learn library to plot the validation curve. This function determines training and test scores for varying parameter values and computes scores for an estimator with different values of a specified parameter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Choosing an estimator's hyperparameters is a fundamental procedure for setting up a model. Among the available procedures, grid search is one of the most used. This procedure selects the hyperparameter with the maximum score on a validation set or a multiple validation set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.model_selection.validation_curve()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html</a></li>
<li class="mce-root"><em>Validation curves: plotting scores to evaluate models</em> (from scikit-learn's official documentation): <a href="https://scikit-learn.org/stable/modules/learning_curve.html">https://scikit-learn.org/stable/modules/learning_curve.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting learning curves</h1>
                </header>
            
            <article>
                
<p>Learning curves help us understand how the size of our training dataset influences the machine learning model. This is very useful when you have to deal with computational constraints. Let's go ahead and plot learning curves by varying the size of our training dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">A learning curve shows the validation and training score of an estimator for varying numbers of training samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to extract learning curves:</p>
<ol>
<li>Add the following code to the same Python file as in the previous recipe, <em>Extracting validation curves</em>:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.model_selection import validation_curve<br/><br/>classifier = RandomForestClassifier(random_state=7)<br/><br/>parameter_grid = np.array([200, 500, 800, 1100])<br/>train_scores, validation_scores = validation_curve(classifier, X, y, "n_estimators", parameter_grid, cv=5)<br/>print("\n##### LEARNING CURVES #####")<br/>print("\nTraining scores:\n", train_scores)<br/>print("\nValidation scores:\n", validation_scores)<br/><br/></pre>
<p style="padding-left: 60px">We want to evaluate the performance metrics using training datasets of 200, 500, 800, and 1,100 samples. We use five-fold cross-validation, as specified by the cv parameter in the <kbd>validation_curve</kbd> method.</p>
<ol start="2">
<li>If you run this code, you will get the following output on the Terminal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-986 image-border" src="assets/47a5108a-7027-4eda-a025-d5a845516da1.png" style="width:36.33em;height:13.83em;"/></p>
<ol start="3">
<li>Let's plot it:</li>
</ol>
<pre style="padding-left: 60px"># Plot the curve<br/>plt.figure()<br/>plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')<br/>plt.title('Learning curve')<br/>plt.xlabel('Number of training samples')<br/>plt.ylabel('Accuracy')<br/>plt.show()</pre>
<ol start="4">
<li>Here is the output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-987 image-border" src="assets/5e35fc89-5c17-435c-a647-8ce915bf76a3.png" style="width:61.67em;height:32.25em;"/></p>
<p style="padding-left: 60px">Although smaller training sets seem to give better accuracy, they are prone to overfitting. If we choose a bigger training dataset, it consumes more resources. Therefore, we need to make a trade-off here to pick the right size for the training dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we used the <kbd>validation_curve</kbd> function of the scikit-learn library to plot the learning curve. This function determines cross-validated training and testing scores for different training set sizes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A learning curve allows us to check whether the addition of training data leads to a benefit. It also allows us to estimate the contribution deriving from variance error and bias error. If the validation score and the training score converge with the size of the training set too low, we will not benefit from further training data.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.model_selection.validation_curve</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html</a></li>
<li class="mce-root"><em>Learning curve</em> (from scikit-learn's official documentation): <a href="https://scikit-learn.org/stable/modules/learning_curve.html">https://scikit-learn.org/stable/modules/learning_curve.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating the income bracket</h1>
                </header>
            
            <article>
                
<p>We will build a classifier to estimate the income bracket of a person based on 14 attributes. The possible output classes are higher than 50,000 or lower than or equal to 50,000. There is a slight twist in this dataset, in the sense that each datapoint is a mixture of numbers and strings. Numerical data is valuable, and we cannot use a label encoder in these situations. We need to design a system that can deal with numerical and non-numerical data at the same time. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We will use the census income dataset available at </span><a href="https://archive.ics.uci.edu/ml/datasets/census+income">https://archive.ics.uci.edu/ml/datasets/Census+Income</a><span>. </span></p>
<p>The dataset has the following characteristics:</p>
<ul>
<li>Number of instances: 48,842</li>
<li>Number of attributes: 14</li>
</ul>
<p>The following is a list of attributes:</p>
<ul>
<li>Age: continuous</li>
<li>Workclass: text</li>
<li>fnlwgt: continuous</li>
<li>Education: text</li>
<li>Education-num: continuous</li>
<li>Marital-status: text</li>
<li>Occupation: text</li>
<li>Relationship: text</li>
<li>Race: text</li>
<li>Sex: female or male</li>
<li>Capital-gain: continuous</li>
<li>Capital-loss: continuous</li>
<li>Hours-per-week: continuous</li>
<li>Native-country: text</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to estimate the income bracket:</p>
<ol>
<li>We will use the <kbd>income.py</kbd> file, already provided to you as a reference. We will use a Naive Bayes classifier to achieve this. Let's import a couple of packages:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from sklearn import preprocessing<br/>from sklearn.naive_bayes import GaussianNB</pre>
<ol start="2">
<li>Let's load the dataset:</li>
</ol>
<pre style="padding-left: 60px">input_file = 'adult.data.txt'<br/># Reading the data<br/>X = []<br/>y = []<br/>count_lessthan50k = 0<br/>count_morethan50k = 0<br/>num_images_threshold = 10000</pre>
<ol start="3">
<li>We will use 20,000 datapoints from the datasets—10,000 for each class to avoid class imbalance. During training, if you use many datapoints that belong to a single class, the classifier tends to get biased toward that class. Therefore, it's better to use the same number of datapoints for each class:</li>
</ol>
<pre style="padding-left: 60px">with open(input_file, 'r') as f:<br/>    for line in f.readlines():<br/>        if '?' in line:<br/>            continue<br/>        data = line[:-1].split(', ')<br/>        if data[-1] == '&lt;=50K' and count_lessthan50k &lt; num_images_threshold:<br/>            X.append(data)<br/>            count_lessthan50k = count_lessthan50k + 1<br/>        elif data[-1] == '&gt;50K' and count_morethan50k &lt; num_images_threshold:<br/>            X.append(data)<br/>            count_morethan50k = count_morethan50k + 1<br/>        if count_lessthan50k &gt;= num_images_threshold and count_morethan50k &gt;= num_images_threshold:<br/>            break<br/>X = np.array(X)</pre>
<p style="padding-left: 60px">It's a comma-separated file again. We just loaded the data in the <kbd>X</kbd> variable just as before.</p>
<ol start="4">
<li>We need to convert string attributes to numerical data while leaving out the original numerical data:</li>
</ol>
<pre style="padding-left: 60px"># Convert string data to numerical data<br/>label_encoder = []<br/>X_encoded = np.empty(X.shape)<br/>for i,item in enumerate(X[0]):<br/>    if item.isdigit():<br/>        X_encoded[:, i] = X[:, i]<br/>    else:<br/>        label_encoder.append(preprocessing.LabelEncoder())<br/>        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])<br/>X = X_encoded[:, :-1].astype(int)<br/>y = X_encoded[:, -1].astype(int)</pre>
<p style="padding-left: 60px">The <kbd>isdigit()</kbd> function helps us to identify numerical data. We converted string data to numerical data and stored all the label encoders in a list so that we can use it when we want to classify unknown data.</p>
<ol start="5">
<li>Let's train the classifier:</li>
</ol>
<pre style="padding-left: 60px"># Build a classifier<br/>classifier_gaussiannb = GaussianNB()<br/>classifier_gaussiannb.fit(X, y)</pre>
<ol start="6">
<li>Let's split the data into training and testing to extract performance metrics:</li>
</ol>
<pre style="padding-left: 60px"># Cross validation<br/>from sklearn import model_selection<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)<br/>classifier_gaussiannb = GaussianNB()<br/>classifier_gaussiannb.fit(X_train, y_train)<br/>y_test_pred = classifier_gaussiannb.predict(X_test)</pre>
<p class="mce-root"/>
<ol start="7">
<li>Let's extract performance metrics:</li>
</ol>
<pre style="padding-left: 60px"># compute F1 score of the classifier<br/>f1 = model_selection.cross_val_score(classifier_gaussiannb,<br/>        X, y, scoring='f1_weighted', cv=5)<br/>print("F1 score: " + str(round(100*f1.mean(), 2)) + "%")</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>F1 score: 75.9%</strong></pre>
<ol start="8">
<li>Let's see how to classify a single datapoint. We need to convert the datapoint into something that our classifier can understand:</li>
</ol>
<pre style="padding-left: 60px"># Testing encoding on single data instance<br/>input_data = ['39', 'State-gov', '77516', 'Bachelors', '13', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', '2174', '0', '40', 'United-States']<br/>count = 0<br/>input_data_encoded = [-1] * len(input_data)<br/>for i,item in enumerate(input_data):<br/>    if item.isdigit():<br/>        input_data_encoded[i] = int([input_data[i]])<br/>    else:<br/>        input_data_encoded[i] = int(label_encoder[count].transform([input_data[i]]))<br/>        count = count + 1 <br/>input_data_encoded = np.array(input_data_encoded)</pre>
<ol start="9">
<li>We are now ready to classify it:</li>
</ol>
<pre style="padding-left: 60px"># Predict and print output for a particular datapoint<br/>output_class = classifier_gaussiannb.predict([input_data_encoded])<br/>print(label_encoder[-1].inverse_transform(output_class)[0])</pre>
<p style="padding-left: 60px">Just as before, we use the <kbd>predict</kbd> method to get the <kbd>output</kbd> class and the <kbd>inverse_transform</kbd> method to convert this label back to its original form to print it out on the Terminal. <span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>&lt;=50K</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The underlying principle<span> of a Bayesian classifier is that some individuals belong to a class of interest with a given probability based on some observations. This probability is based on the assumption that the characteristics observed can be dependent or independent from one another; in the second case, the Bayesian classifier is called <em>naive</em> because it assumes that the presence or absence of a particular characteristic in a given class of interest is not related to the presence or absence of other characteristics, greatly simplifying the calculation. Let's go ahead and build a Naive Bayes classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The concept of Bayes applied to classification is very intuitive: if I look at a particular measurable feature, I can estimate the probability that this feature represents a certain class after the observation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>The official documentation of the <kbd>sklearn.naive_bayes.GaussianNB</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting the quality of wine</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will predict the quality of wine based on the chemical properties <span>of wines grown. The code uses a wine dataset, which contains a DataFrame with 177 rows and 13 columns; the first column contains the class labels. This data is obtained from the chemical analyses of wines grown in the same region in Italy (Piemonte) but derived from three different cultivars—namely, the Nebbiolo, Barberas, and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">The data consists of the amounts of several constituents found in each of the three types of wines, as well as some spectroscopic variables. <span>The attributes are as follows:</span></p>
<ul>
<li><span>Alcohol </span></li>
<li><span>Malic acid </span></li>
<li><span>Ash </span></li>
<li><span>Alcalinity of ash </span></li>
<li><span>Magnesium </span></li>
<li><span>Total phenols </span></li>
<li><span>Flavanoids </span></li>
<li><span>Nonflavanoid phenols </span></li>
<li><span>Proanthocyanins </span></li>
<li><span>Color intensity </span></li>
<li><span>Hue </span></li>
<li><span>OD280/OD315 of diluted wines </span></li>
<li><span>Proline </span></li>
</ul>
<p>The first column of the DataFrame contains <span>the class which indicates one of three types of wine as (0, 1, or 2).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how to predict the quality of wine:</p>
<ol>
<li><span><span>We will use the <kbd>wine.quality.py</kbd> file, already provided to you as a reference. We start, as always, by importing the NumPy library and loading the data (<kbd>wine.txt</kbd>):</span></span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><span>import numpy as np</span><br/>input_file = 'wine.txt'<br/>X = []<br/>y = []<br/>with open(input_file, 'r') as f:<br/>  for line in f.readlines():<br/>     data = [float(x) for x in line.split(',')]<br/>     X.append(data[1:])<br/>     y.append(data[0])<br/>X = np.array(X)<br/>y = np.array(y)</pre>
<p style="padding-left: 60px">Two arrays are returned: <kbd><span><span>X</span></span></kbd> (input data), and <kbd>y</kbd> (target). </p>
<ol start="2">
<li>Now we need to separate our data <span>into two groups: a </span>training<span> dataset and a </span>testing<span><span> dataset. The training dataset will be used to build the model, and the testing dataset will be used to see how this trained model performs on unknown data:</span></span></li>
</ol>
<pre style="padding-left: 60px">from sklearn import model_selection<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)</pre>
<p>Four arrays are returned: <kbd>X_train</kbd>, <kbd>X_test</kbd>, <kbd>y_train</kbd>, and <kbd>y_test</kbd>. This data will be used to train and validate the model.</p>
<ol start="3">
<li>Let's train the classifier:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.tree import DecisionTreeClassifier<br/><br/>classifier_DecisionTree = DecisionTreeClassifier()<br/>classifier_DecisionTree.fit(X_train, y_train)</pre>
<p>To train the model, a decision tree algorithm has been used. A decision tree algorithm is based on a non-parametric supervised learning method used for classification and regression. The aim is to build a model that predicts the value of a target variable using decision rules inferred from the data features.</p>
<ol start="4">
<li>Now it's time to the compute accuracy of the classifier:</li>
</ol>
<pre style="padding-left: 60px">y_test_pred = classifier_DecisionTree.predict(X_test)<br/><br/>accuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]<br/>print("Accuracy of the classifier =", round(accuracy, 2), "%")</pre>
<p>The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier = 91.11 %</strong></pre>
<ol start="5">
<li>Finally, a confusion matrix will be calculated to compute the model performance:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import confusion_matrix<br/><br/>confusion_mat = confusion_matrix(y_test, y_test_pred)<br/>print(confusion_mat)</pre>
<p class="mce-root"/>
<p><span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>[[17  2  0]</strong><br/><strong> [ 1 12  1]</strong><br/><strong> [ 0  0 12]]</strong></pre>
<p>Values not present on the diagonals represent classification errors. So, only four errors were committed by the classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, the quality of wine based on the chemical properties </span><span>of wines grown was predicted. To do this, a decision tree algorithm was used. A decision tree shows graphically the choices made or proposed. It does not happen so often that things are so clear that the choice between two solutions is immediate. Often, a decision is determined by a series of cascading conditions. Representing this concept with tables and numbers is difficult. In fact, even if a table represents a phenomenon, it may confuse the reader because the justification for the choice is not obvious.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A tree structure allows us to extract the information with clear legibility by highlighting the branch we have inserted to determine the choice or evaluation. Decision tree technology is useful for identifying a strategy or pursuing a goal by creating a model with probable results. The decision tree graph immediately orients the reading of the result. A plot is much more eloquent than a table full of numbers. The human mind prefers to see a solution first and then go back to understand a justification of the solution, instead of a series of algebraic descriptions, percentages, and data to describe a result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <kbd>sklearn.tree.DecisionTreeClassifier()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier</a></li>
<li class="mce-root"><em>Decision Trees</em> (from the University of Hildesheim, Germany): <a href="https://www.ismll.uni-hildesheim.de/lehre/ml-06w/skript/ml-4up-04-decisiontrees.pdf">https://www.ismll.uni-hildesheim.de/lehre/ml-06w/skript/ml-4up-04-decisiontrees.pdf</a></li>
<li class="mce-root"><em>Decision Trees</em> (from scikit-learn's official documentation): <a href="https://scikit-learn.org/stable/modules/tree.html#tree">https://scikit-learn.org/stable/modules/tree.html#tree</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Newsgroup trending topics classification</h1>
                </header>
            
            <article>
                
<p>Newsgroups are discussion groups on many issues and are made available by news-servers, located all over the world, which collect messages from clients and transmit them, on the one hand, to all their users and, on the other, to other news-servers connected to the network. The success of this technology is due to user interaction in discussions. Everyone has to respect the rules of the group.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will build a classifier that will allow us to classify the membership of a topic into a particular discussion group. This operation will be useful to verify whether the topic is relevant to the discussion group. We will use the data contained in the 20 newsgroups dataset, available at the following URL: <a href="http://qwone.com/~jason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a>.</p>
<p class="mce-root">This is a collection of about 20,000 newsgroup documents, divided into 20 different newsgroups. Originally collected by Ken Lang, and published in <em>Newsweeder paper: Learning to filter netnews,</em> the dataset is particularly useful for dealing with text classification problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to perform newsgroup trending topics classification:</p>
<ol>
<li><span>We will use the </span><kbd>post.classification.py</kbd><span> file, already provided to you as a reference. We start importing the dataset as follows:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>from sklearn.datasets import fetch_20newsgroups</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">This dataset is contained in the <kbd>sklearn.datasets</kbd> library; in this way, it will be very easy for us to recover the data. As anticipated, the dataset contains posts related to 20 newsgroups. We will limit our analysis to only the following two newsgroups:</p>
<pre style="padding-left: 60px">NewsClass = ['rec.sport.baseball', 'rec.sport.hockey']</pre>
<ol start="2">
<li>Download the data:</li>
</ol>
<pre style="padding-left: 60px">DataTrain = fetch_20newsgroups(subset='train',categories=NewsClass, shuffle=True, random_state=42)</pre>
<ol start="3">
<li>The data has two attributes: <kbd>data</kbd> and <kbd>target</kbd>. Obviously, <kbd>data</kbd> represents the input and <kbd>target</kbd> is the output. Let's check which newsgroups have been selected:</li>
</ol>
<pre style="padding-left: 60px">print(DataTrain.target_names)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>['rec.sport.baseball', 'rec.sport.hockey']</strong></pre>
<ol start="4">
<li>Let's check the shape:</li>
</ol>
<pre style="padding-left: 60px">print(len(DataTrain.data))<br/>print(len(DataTrain.target))</pre>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong>1197</strong><br/><strong>1197</strong></pre>
<ol start="5">
<li>To extract features from texts, we will use the <kbd>CountVectorizer()</kbd> function as follows:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer<br/><br/>CountVect = CountVectorizer()<br/>XTrainCounts = CountVect.fit_transform(DataTrain.data)<br/>print(XTrainCounts.shape)</pre>
<p style="padding-left: 60px"><span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>(1197, 18571)</strong></pre>
<p style="padding-left: 60px">In this way, we have made a count of the occurrences of words.</p>
<p class="mce-root"/>
<ol start="6">
<li>Now let's divide the number of occurrences of each word in a document by the total number of words in the document:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import TfidfTransformer<br/><br/>TfTransformer = TfidfTransformer(use_idf=False).fit(XTrainCounts)<br/>XTrainNew = TfTransformer.transform(XTrainCounts)<br/>TfidfTransformer = TfidfTransformer()<br/>XTrainNewidf = TfidfTransformer.fit_transform(XTrainCounts)</pre>
<ol start="7">
<li>Now we can build the classifier:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.naive_bayes import MultinomialNB<br/><br/>NBMultiClassifier = MultinomialNB().fit(XTrainNewidf, DataTrain.target)</pre>
<ol start="8">
<li>Finally, we will <span>compute the accuracy of the classifier</span>:</li>
</ol>
<pre style="padding-left: 60px">NewsClassPred = NBMultiClassifier.predict(XTrainNewidf)<br/><br/>accuracy = 100.0 * (DataTrain.target == NewsClassPred).sum() / XTrainNewidf.shape[0]<br/>print("Accuracy of the classifier =", round(accuracy, 2), "%")</pre>
<p><span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>Accuracy of the classifier = 99.67 %</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we built a classifier to classify the membership of a topic into a particular discussion group. To extract features from the text, a <strong>tokenization</strong> procedure was needed. In the tokenization phase, within each single sentence, atomic elements called <strong>tokens</strong> are identified; based on the token identified, it's possible to carry out an analysis and evaluation of the sentence itself. Once the characteristics of the text had been extracted, a classifier based on the multinomial Naive Bayes algorithm was constructed.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The Naive Bayes multinomial algorithm is used for text and images when features represent the frequency of words (textual or visual) in a document.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">The official documentation of the <em>Dataset loading utilities</em>: <a href="https://scikit-learn.org/stable/datasets/index.html">https://scikit-learn.org/stable/datasets/index.html</a></li>
<li class="mce-root"><span>The o</span>fficial documentation of the <kbd>sklearn.feature_extraction.text.CountVectorizer()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></li>
<li class="mce-root"><span>The o</span>fficial documentation of the <kbd>sklearn.feature_extraction.text.TfidfTransformer()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html</a></li>
<li class="mce-root"><span>The o</span>fficial documentation of the <kbd>sklearn.naive_bayes.MultinomialNB()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html</a></li>
</ul>


            </article>

            
        </section>
    </body></html>