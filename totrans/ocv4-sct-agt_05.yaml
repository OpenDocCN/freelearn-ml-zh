- en: Training a Smart Alarm to Recognize the Villain and His Cat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The naming of cats is a difficult matter."'
  prefs: []
  type: TYPE_NORMAL
- en: – T. S. Eliot, Old Possum's Book of Practical Cats (1939)
  prefs: []
  type: TYPE_NORMAL
- en: '"Blofeld: I''ve taught you to love chickens, to love their flesh, their voice."'
  prefs: []
  type: TYPE_NORMAL
- en: – On Her Majesty's Secret Service (1969)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the date is January 1, 2015\. The balance of world power is shifting
    again. Lithuania is joining the eurozone. Russia, Belarus, Armenia, Kazakhstan,
    and Kyrgyzstan are forming the Eurasian Economic Union. The first edition of *OpenCV
    for Secret Agents* is going to the printers. On this day, if you saw Ernst Stavro
    Blofeld, would you recognize him?
  prefs: []
  type: TYPE_NORMAL
- en: Let me remind you that Blofeld, as the number one man in the **Special Executive
    for Counterintelligence, Terrorism, Revenge, and Extortion** (**SPECTRE**), is
    a super-villain who eludes James Bond countless times before being written out
    of the movies due to an intellectual property dispute. Blofeld last appears as
    an anonymous character in the intro sequence of *For Your Eyes Only* (1981), where
    we see him fall from a helicopter and down a factory's smokestack as he shouts,
    *Mr. Boooooooooond!*
  prefs: []
  type: TYPE_NORMAL
- en: Despite this dramatic exit, the evidence of Blofeld's death is unclear. After
    all, Blofeld is a notoriously difficult man to recognize. His face is seldom caught
    on camera. As early as the 1960s, he was using plastic surgery to change his identity
    and to turn his henchmen into lookalikes of himself. Half a century later, we
    must ask, is Blofeld a dead man or is he just made-over, perhaps as a beautiful
    actress in a Colombian telenovela?
  prefs: []
  type: TYPE_NORMAL
- en: One thing is certain. If Blofeld is alive, he is accompanied by a blue-eyed,
    white Angora cat (preserved by a veterinary miracle or taxidermy). Patting this
    cat is Blofeld's telltale habit in every movie. His face may be different but
    his lap cat is the same. We last see the cat jumping out of Blofeld's lap just
    before the fateful helicopter ride.
  prefs: []
  type: TYPE_NORMAL
- en: Some commentators have noted a resemblance between Blofeld and Dr. Evil, the
    nemesis of Austin Powers. However, by comparing the respective lap cats, we can
    prove that these two villains are not the same.
  prefs: []
  type: TYPE_NORMAL
- en: The moral is that two approaches to recognition are better than one. Though
    we cannot see the man's face, we should not lose sight of his cat.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the suspense ended on October 26, 2015, when Blofeld made his comeback
    in *Spectre*. He and the cat looked remarkably unchanged after 34 years.
  prefs: []
  type: TYPE_NORMAL
- en: To automate the search for villains and their cats, we are going to develop
    a desktop or Raspberry Pi application called `Angora Blue` (an innocent-sounding
    code name that alludes to the blue eyes of Blofeld's cat). `Angora Blue` will
    send us an email alert when it recognizes a specified villain or a specified cat
    with a certain level of confidence. We will also develop a GUI app called `Interactive
    Recognizer`, which will train Angora Blue's recognition model based on camera
    images and names that we provide interactively. To distinguish faces from the
    background, `Interactive Recognizer` depends on a human face detection model that
    comes with OpenCV and a cat face detection model that we are going to train using
    an original script and third-party image databases.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you have heard that OpenCV comes with a set of pretrained cat face detectors.
    This is true! I originally developed them for this book's first edition; I contributed
    them to OpenCV, and I have maintained them with improvements. This chapter covers
    the process that I used to train the latest version of these official OpenCV cat-face
    detectors.
  prefs: []
  type: TYPE_NORMAL
- en: This is a big chapter, but it is rewarding because you will learn a process
    that applies to detecting and recognizing any kind of animal face, and even any
    object!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter''s project has the following software dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Python environment with the following modules**: OpenCV (including opencv_contrib),
    NumPy, SciPy, Requests, wxPython, and optionally PyInstaller'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup instructions are covered in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*. Refer to the setup instructions for any version requirements.
    Basic instructions for running Python code are covered in [Appendix C](c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml),
    *Running with Snakes (or, First Steps with Python)*.
  prefs: []
  type: TYPE_NORMAL
- en: The completed project for this chapter can be found in this book's GitHub repository,
    [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition),
    in the `Chapter003` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning in general
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our work throughout this chapter builds on the techniques of machine learning,
    meaning that the software makes predictions or decisions based on statistical
    models. Particularly, our approach is one of supervised learning, meaning that
    we (programmers and users) provide the software with examples of data and correct
    responses. The software creates the statistical model to extrapolate from these
    examples. The human provided examples are referred to as reference data or training
    data (or reference images or training images in the context of computer vision).
    Conversely, the software's extrapolations pertain to test data (or test images
    or scenes in the context of computer vision).
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is much like the flashcard pedagogy used in early childhood
    education. The teacher shows the child a series of pictures (training images)
    and says,
  prefs: []
  type: TYPE_NORMAL
- en: '"This is a cow. Moo! This is a horse. Neigh!"'
  prefs: []
  type: TYPE_NORMAL
- en: Then, on a field trip to a farm (a scene), the child can hopefully distinguish
    between a horse and a cow. However, I must confess that I once mistook a horse
    for a cow, and I was teased about this misclassification for many years thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from supervised learning, which is widely used in problems of vision and
    semantics, there are two other broad approaches to machine learning—unsupervised
    learning and reinforcement learning. **Unsupervised learning** requires the software
    to find some structure, such as clusters, in data where no meaning or correct
    examples are assigned by a human. Analyzing biological structures, such as genomes,
    is a common problem for unsupervised learning. On the other hand, **reinforcement
    learning** requires the software to experimentally optimize a solution to some
    sequence of problems, where a human assigns the final goal but the software must
    set intermediate goals. Piloting a vehicle and playing a game are common problems
    for reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Besides being a computer vision library, OpenCV offers a general purpose machine
    learning module that can process any kind of data, not necessarily images. For
    more information on this module and the underlying machine learning concepts,
    see the *Machine Learning* section of the official OpenCV-Python tutorials at
    [https://docs.opencv.org/4.0.0-beta/d6/de2/tutorial_py_table_of_contents_ml.html](https://docs.opencv.org/4.0.0-beta/d6/de2/tutorial_py_table_of_contents_ml.html).
    Meanwhile, our chapter proceeds with more specialized machine learning functionality
    and concepts that OpenCV users often apply to face detection and recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the Interactive Recognizer app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin this project with the middle layer, the `Interactive Recognizer`
    app, in order to see how all layers connect. Like Luxocator from the [Chapter
    2](4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml), *Searching for Luxury Accommodations
    Worldwide,* project, `Interactive Recognizer` is a GUI app built with wxPython.
    Refer to the following screenshot, featuring one of my colleagues, Chief Science
    Officer Sanibel San Delphinium Andromeda, high priestess of the Numm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3363886-0e67-42c2-bdcd-a2a3289e91ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The app uses a face detection model, which is loaded from disk, and it maintains
    a face recognition model, which is saved to disk and later loaded back from disk.
    The user may specify the identity of any detected face, and this input is added
    to the face recognition model. A detection result is shown by outlining the face
    in the video feed, while a recognition result is shown by displaying the name
    of the face in the text below. To elaborate, we may say that the app has the following
    flow of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a face detection model from file. The role of the detection model is to
    distinguish faces from the background.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a face recognition model from file if any such model was saved during a
    previous run of `Interactive Recognizer`. Otherwise, if there is no such model
    to load, create a new one. The role of the recognition model is to distinguish
    faces of different individuals from each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capture and display a live video from a camera.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each frame of video, detect the largest face, if any. If a face is detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a rectangle around the face.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Permit the user to enter the face's identity as a short string (up to four characters),
    such as `Joe` or `Puss`. When the user hits the Add to Model button, train the
    model to recognize the face as whomever the user specified (`Joe`, `Puss`, or
    another identity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the recognition model is trained for at least one face, display the recognizer's
    prediction for the current face—that is, display the most probable identity of
    the current face, according to the recognizer. Also, display a measure of distance
    (non-confidence) for this prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the recognition model is trained for at least one face, permit the user to
    hit the Clear Model button to delete the model (including any version saved to
    file) and create a new one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On exit, if the recognition model is trained for at least one face, save the
    model to file so that it can be loaded in subsequent runs of `Interactive Recognizer`
    and `Angora Blue`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We could generalize by using the term object instead of face. Depending on the
    models that it loads, `Interactive Recognizer` could detect and recognize any
    kind of object, not necessarily faces.
  prefs: []
  type: TYPE_NORMAL
- en: We use a type of detection model called a **Haar cascade** and a type of recognition
    model called **Local Binary Patterns** (**LBP**) or **Local Binary Pattern Histograms**
    (**LBPH**). Alternatively, we may use LBPH for both detection and recognition.
    As detection models, LBP cascades are faster but generally less reliable, compared
    to Haar cascades. OpenCV comes with some Haar cascade and LBP cascade files, including
    several face detection models. Command-line tools for generating such files are
    also included with OpenCV. The APIs offer high-level classes for loading and using
    Haar or LBP cascades and for loading, saving, training, and using LBPH recognition
    models. Let's look at the basic concepts of these models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Haar cascades and LBPH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Cookie Monster: Hey, you know what? A round cookie with one bite out of it
    looks like a C. A round donut with one bite out of it also looks like a C! But
    it is not as good as a cookie. Oh, and the moon sometimes looks like a C! But
    you can''t eat that."'
  prefs: []
  type: TYPE_NORMAL
- en: – "C is for Cookie," Sesame Street
  prefs: []
  type: TYPE_NORMAL
- en: Think about cloud-watching. If you lie on the ground and look up at the clouds,
    maybe you imagine that one cloud is shaped like a mound of mashed potatoes on
    a plate. If you board an airplane and fly to this cloud, you will still see some
    resemblance between the cloud's surface and the fluffy, lumpy texture of hearty
    mashed potatoes. However, if you could slice off a piece of cloud and examine
    it under a microscope, you might see ice crystals that do not resemble the microscopic
    structure of mashed potatoes at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in an image made up of pixels, a person or a computer vision algorithm
    can see many distinctive shapes or patterns, partly depending on the level of
    magnification. During the creation of a Haar cascade, various parts of the image
    are cropped and/or scaled so that we consider only a few pixels at a time (though
    these pixels may represent any level of magnification). This sample of the image
    is called a **window**. We subtract some of the grayscale pixel values from others
    in order to measure the window''s similarity to certain common shapes where a
    dark region meets a light region. Examples include an edge, a corner, or a thin
    line, as shown in the following diagram. If a window has a high similarity to
    one of these archetypes, it may be selected as a **feature**. We expect to find
    similar features at similar positions and magnifications relative to each other,
    across all images of the same subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7751bc20-2862-456c-a467-0e2a24720d46.png)'
  prefs: []
  type: TYPE_IMG
- en: Not all features are equally significant. Across a set of images, we can see
    whether a feature is truly typical of images that include our subject (the **positive
    training set**) and atypical of images that exclude our subject (the **negative
    training set**). We give features a different rank or **stage** depending on how
    well they distinguish subjects from non-subjects. Together, a set of stages form
    a **cascade** or a series of comparison criteria. Every stage must be passed in
    order to reach a positive detection result. Conversely, a negative detection result
    can be reached in fewer stages, perhaps only a single stage (an important optimization).
    Like the training images, scenes are examined through various windows, and we
    may end up detecting multiple subjects in one scene.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Haar cascades in OpenCV, see the official documentation
    at [https://docs.opencv.org/4.0.0-beta/d7/d8b/tutorial_py_face_detection.html](https://docs.opencv.org/4.0.0-beta/d7/d8b/tutorial_py_face_detection.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'An LBPH model, as the name suggests, is based on a kind of histogram. For each
    pixel in a window, we note whether each neighboring pixel in a certain radius
    is brighter or darker. Our histogram counts the darker pixels in each neighboring
    position. For example, suppose a window contains the following two neighborhoods
    of a one-pixel radius:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30c8be1f-f92e-4085-9727-4b9f9c797077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Counting these two neighborhoods (and not yet counting other neighborhoods
    in the window), our histogram can be visualized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cefbc6c1-db29-43dd-ad5f-cea6c565ee37.png)'
  prefs: []
  type: TYPE_IMG
- en: If we compute the LBPH of multiple sets of reference images for multiple subjects,
    we can determine which set of LBPH references is least distant from the LBPH of
    a piece of a scene, such as a detected face. Based on the least distant set of
    references, we can predict the identity of the face (or other object) in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: An LBPH model is good at capturing fine texture detail in any subject, not just
    faces. Moreover, it is good for applications where the model needs to be updated,
    such as `Interactive Recognizer`. The histograms for any two images are computed
    independently, so a new reference image can be added without recomputing the rest
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV also implements other models that are popular for face recognition, namely,
    Eigenfaces and Fisherfaces. We use LBPH because it supports updates in real time,
    whereas Eigenfaces and Fisherfaces do not. For more information on these three
    recognition models, see the official documentation at [https://docs.opencv.org/4.0.0-beta/da/d60/tutorial_face_main.html](https://docs.opencv.org/4.0.0-beta/da/d60/tutorial_face_main.html).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, for detection rather than recognition, we can organize LBPH models
    into a cascade of multiple tests, much like a Haar cascade. Unlike an LBPH recognition
    model, an LBP cascade cannot be updated in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Haar cascades, LBP cascades, and LBPH recognition models are not robust with
    respect to rotation or flipping. For example, if we look at a face upside down,
    it will not be detected by a Haar cascade that was trained only with upright faces.
    Similarly, if we had an LBPH recognition model trained for a cat whose face is
    black on the cat's left-hand side and orange on the cat's right-hand side, the
    model might not recognize the same cat in a mirror. The exception is that we could
    include mirror images in the training set, but then we might get a false positive
    recognition for a different cat whose face is orange on the cat's left-hand side
    and black on the cat's right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise noted, we may assume that a Haar cascade or LBPH model is trained
    for an *upright* subject. That is, the subject is not tilted or upside down in
    the image's coordinate space. If a man is standing on his head, we can take an
    upright photo of his face by turning the camera upside down or, equivalently,
    by applying a 180-degree rotation in software.
  prefs: []
  type: TYPE_NORMAL
- en: Some other directional terms are worth noting. A *frontal*, *rear*, or *profile*
    subject has its front, rear, or profile visible in the image. Most computer vision
    people, including the authors of OpenCV, express *left* and *right* in the image's
    coordinate space. For example, if we say *left eye*, for an upright, frontal,
    non-mirrored face, we mean the subject's right-hand eye, since left and right
    in image space are opposite from an upright, frontal, non-mirrored subject's left-hand
    and right-hand directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how we would label the *left eye* and *right
    eye* in a non-mirrored image (left) and mirrored image (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/851a49a3-9db4-4a43-b274-929ce15bacb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Our human and feline detectors deal with upright, frontal faces.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in a real-world photo, we cannot expect a face to be perfectly upright.
    The person's head or the camera might have been slightly tilted. Moreover, we
    cannot expect boundary regions, where a face meets a background, to be similar
    across images. We must take great care to preprocess the training images so that
    the face is rotated to a nearly perfect upright pose and boundary regions are
    cropped off. When cropping, we should place the major features of the face, such
    as eyes, in a consistent position. These considerations are addressed further
    in the *Planning the cat-detection model* section, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If we must detect faces in various rotations, one option is to rotate the scene
    before sending it to the detector. For example, we can try to detect faces in
    the original scene, then in a version of the scene that has been rotated 15 degrees,
    then a version rotated—15 degrees (345 degrees), then a version rotated 30 degrees,
    and so on. Similarly, we can send mirrored versions of the scene to the detector.
    Depending on how many variations of the scene are tested, such an approach may
    be too slow for real-time use, and thus we do not use it in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Interactive Recognizer app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a new folder, where we will store this chapter''s project, including
    the following subfolders and files that are relevant to `Interactive Recognizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalface_alt.xml`: A detection model for a frontal
    human face. It should be included with OpenCV at a path such as `<opencv_unzip_destination>/data/haarcascades/haarcascade_frontalface_alt.xml`
    or for a MacPorts installation at `/opt/local/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml`.
    Copy or link to that version. (Alternatively, get it from this book''s GitHub
    repository).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/lbpcascade_frontalface.xml`: An alternative (faster but less reliable)
    detection model for a frontal human face. It should be included with OpenCV at
    a path such as `<opencv_unzip_destination>/data/lbpcascades/lbpcascade_frontalface.xml`
    or for a MacPorts installation at `/opt/local/share/OpenCV/lbpcascades/lbpcascade_frontalface.xml`.
    Copy or link to that version. Alternatively, get it from this book''s GitHub repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalcatface.xml`: A detection model for a frontal,
    feline face. We will build it later in this chapter. Alternatively, you may get
    a prebuilt version from this book''s GitHub repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalcatface_extended.xml`: An alternative detection
    model for a frontal feline face. This version is sensitive to diagonal patterns,
    potentially including whiskers and ears. We will build it later in this chapter.
    (Alternatively, you may get a prebuilt version from this book''s GitHub repository.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/lbpcascade_frontalcatface.xml`: Another alternative (faster but less
    reliable) detection model for a frontal feline face. We will build it later in
    this chapter. (Alternatively, you may get a prebuilt version from this book''s
    GitHub repository.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognizers/lbph_human_faces.xml`: A recognition model for the faces of certain
    human individuals. It is generated by `InteractiveHumanFaceRecognizer.py`, as
    follows later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognizers/lbph_cat_faces.xml`: A recognition model for the faces of certain
    feline individuals. It is generated by `InteractiveCatFaceRecognizer.py`, as follows
    later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResizeUtils.py`: Utility functions for resizing images. It copies or links
    to the previous chapter''s version of `ResizeUtils.py`. We will add a function
    to resize the camera capture dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WxUtils.py`: Utility functions for wxPython GUI applications. It copies or
    links to the [Chapter 2](4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml), *Searching
    for Luxury Accommodations Worldwide*, version of `WxUtils.py`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinasciiUtils.py`: Utility functions for converting human-readable identifiers
    into numbers and back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InteractiveRecognizer.py`: A class that encapsulates the `Interactive Recognizer`
    app and exposes certain variables for configuration. We will implement it in this
    section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InteractiveHumanFaceRecognizer.py`: A script to launch a version of Interactive
    Recognizer that is configured for frontal human faces. We will implement it in
    this section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InteractiveCatFaceRecognizer.py`: A script to launch a version of `Interactive
    Recognizer` that is configured for frontal feline faces. We will implement it
    in this section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with an addition to our existing `ResizeUtils` module. We want
    to be able to specify the resolution at which a camera captures images. Camera
    input is represented by an OpenCV class called `VideoCapture`, with the `get`
    and `set` methods that pertain to various camera parameters, including resolution.
    (Incidentally, `VideoCapture` can also represent a video file.) There is no guarantee
    that a given capture resolution is supported by a given camera. We need to check
    the success or failure of any attempt to set the capture resolution. Accordingly,
    let''s add the following utility function to `ResizeUtils.py` to attempt to set
    a capture resolution and to return the actual capture resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s consider the requirements for our new `BinasciiUtils` module. OpenCV''s
    recognizers use 32-bit integers as identifiers. For a GUI, asking the user to
    give a face a number, instead of a name, is not very friendly. We could keep a
    dictionary that maps numbers to names, and we could save this dictionary to disk,
    alongside the recognition model, but here is my lazier solution. Four or fewer
    ASCII characters can be cast to a 32-bit integer (and vice versa). For example,
    consider the name *Puss*, in which the letter ASCII codes are *80*, *117*, *115*,
    and *115*, respectively. Remembering that each letter is one byte or 8 bits, we
    can apply bitshift operations to the ASCII codes to get the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/057dc3ba-57dd-475e-b121-7f32de7ac5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will let the user enter names of up to four characters, and, behind the
    scenes, we will convert to and from the 32-bit integers that the model stores.
    Let''s create `BinasciiUtils.py`, and put the following imports and conversion
    functions in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s proceed to write `InteractiveRecognizer.py`. It should start with
    the following `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `InteractiveRecognizer` application class accepts several arguments that
    allow us to create different variants of the app with different titles, highlight
    colors, recognition models, detection models, and tweaks to the detection behavior.
    Let''s look at the initializer''s declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The initializer''s arguments are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`recognizerPath`: This is the file containing the recognition model. This file
    does not need to exist when the app starts. Rather, the recognition model (if
    any) is saved here when the app exits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascadePath`: This is the file containing the detection model. This file does
    need to exist when the app starts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaleFactor`: The detector searches for faces at several different scales.
    This argument specifies the ratio of each scale to the next smaller scale. A bigger
    ratio implies a faster search but fewer detections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minNeighbors`: If the detector encounters two overlapping regions that both
    might pass detection as faces, they are called neighbors. The `minNeighbors` argument
    specifies the minimum number of neighbors that a face must have in order to pass
    detection. Where `minNeighbors>0`, the rationale is that a true face could be
    cropped in several alternative places and still look like a face. A greater number
    of required neighbors implies fewer detections and a lower proportion of false
    positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minSizeProportional`: A face''s minimum width and height are expressed as
    a proportion of the camera''s vertical resolution or horizontal resolution, whichever
    is less. For example, if the camera resolution is *640 x 480* and `minSizeProportional=(0.25,
    0.25)`, the face must measure at least *120 x 120* (in pixels) in order to pass
    detection. A bigger minimum size implies a faster search but fewer detections.
    The `(0.25, 0.25)` default value is appropriate for a face that is close to a
    webcam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rectColor`: This is for the color of the rectangle outlining a detected face.
    Like most color tuples in OpenCV, it is specified in **blue, green, and red**
    (**BGR**) order (not RGB).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cameraDeviceID`: The is the device ID of the camera that should be used for
    input. Typically, webcams are numbered starting from `0`, with any connected external
    webcams coming before any internal webcams. Some camera drivers reserve fixed
    device IDs. For example, OpenNI reserves `900` for Kinect and `910` for Asus Xtion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imageSize`: The preferred resolution for captured images. If the camera does
    not support this resolution, another resolution is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`title`: The app title, as seen in the window title bar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also provide a public Boolean variable to configure whether or not the camera
    feed is mirrored. By default, it is mirrored because users find a mirrored image
    of themselves to be more intuitive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Another Boolean tracks whether the app should still be running or whether it
    is closing. This information is relevant to cleaning up a background thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using an OpenCV class called `cv2.VideoCapture`, we open a camera feed and
    get its resolution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We define variables to store the images that we will capture, process, and
    display. Initially, these are `None`. In order to capture and process images on
    one thread, and then draw them to the screen on another thread, we will use a
    pattern known as **double buffering**. While one frame (the **back buffer**) is
    being prepared on one thread, another frame (the **front buffer**) will be being
    drawn on a second thread. When both threads have done a round of work, we will
    swap the buffers so that the old back buffer becomes the new front buffer and
    vice versa (by simply changing references, without copying data). To accomplish
    this in a thread-safe manner, we need to declare a **mutual exclusion lock** (also
    called a **mutex**), which represents a permission or resource (in this case,
    access to the front buffer) that only one thread can acquire at a time. We will
    see the lock in use later in this section, in the `_onVideoPanelPaint` and `_runCaptureLoop` methods.
    For now, here are the initial declarations of the images and lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up variables related to detection and recognition. Many of these
    variables just store initialization arguments for later use. Also, we keep a reference
    to the currently detected face, which is initially `None`. We initialize an LBPH
    recognizer and load any recognition model that we may have saved on a previous
    run on the app. Likewise, we initialize a detector by loading a Haar cascade or
    LBP cascade from file. Here is the relevant code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Having set up the variables that are relevant to computer vision, we proceed
    to the GUI implementation, which is mostly boilerplate code. First, in the following
    snippet, we set up the window with a certain style, size, title, and background
    color, and we bind a handler for its close event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set a callback for the *Escape* key. Since a key is not a GUI widget,
    there is no `Bind` method directly associated with a key, and we need to set up
    the callback a bit differently than we have previously seen with wxWidgets. We
    bind a new menu event and callback to the `InteractiveRecognizer` instance, and
    we map a keyboard shortcut to the menu event using a class called `wx.AcceleratorTable`.
    (Note, however, that our app actually has no menu, nor is an actual menu item
    required for the keyboard shortcut to work.) Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code initializes the GUI widgets (including a video panel, text
    field, buttons, and label) and sets their event callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to Luxocator (the [Chapter 2](4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml), *Searching
    for Luxury Accommodations Worldwide,* project), `Interactive Recognizer` lays
    out the image on top and a row of controls on the bottom. Here is the layout code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the initializer starts a background thread that performs image capture
    and image processing, including detection and recognition. It is important to
    perform the intensive computer vision work on a background thread so that it doesn''t
    stall the handling of GUI events. Here is the code that starts the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With a variety of input events and background work, `InteractiveRecognizer`
    has many methods that run in an indeterminate order. We will look at input event
    handlers first, before proceeding to the image pipeline (capture, processing,
    and display), which partly runs on the background thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the window is closed, we ensure that the background thread stops. Then,
    if the recognition model is trained, we save it to file. Here is the implementation
    of the relevant callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides closing the window when its standard *X* button is clicked, we also
    close it in the `_onQuitCommand` callback, which we linked to the *Esc* button.
    The callback''s implementation is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We handle the video panel''s erase event by doing nothing because we simply
    want to draw over the old video frame instead of erasing it. We handle the video
    panel''s draw event by acquiring the lock that gives us thread-safe access to
    the front image buffer, converting the image into a wxPython bitmap, and then
    drawing the bitmap to the panel. Here are the implementations of the two relevant
    callbacks in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When the user adds or deletes text in the text field, our `_onReferenceTextCtrlKeyUp`
    callback (as follows) calls a helper method to check whether the Add to Model
    button should be enabled or disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When the Add to Model button is clicked, its callback provides new training
    data to the recognition model. If the LBPH model has no prior training data, we
    must use the recognizer''s `train` method; otherwise, we must use its `update`
    method. Both methods accept two arguments—a list of images (the faces) and a NumPy
    array of integers (the face identifiers). We train or update the model with just
    one image at a time so that the user can interactively test the effect of each
    incremental change to the model. The image is the most recently detected face,
    and the identifier is converted from the text in the text field using our `BinasciiUtils.fourCharsToInt`
    function. Here is the implementation of the Add to Model button''s callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When the Clear Model button is clicked, its callback deletes the recognition
    model (including any version that has been saved to disk) and creates a new one.
    Also, we record that the model is untrained and we disable the Clear Model button
    until the model is retrained. Here is the implementation in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Our background thread runs a loop. On each iteration, we capture an image using
    the `VideoCapture` object''s `read` method. Along with the image, the `read` method
    returns a `success` flag, which we do not need because instead we just check whether
    the image is `None`. If the image is not `None`, we call a helper method named
    `_detectAndRecognize`, and then we may mirror the image for display. We also acquire
    the lock to perform a thread-safe swap of the front and back image buffers. After
    the swap, we tell the video panel to refresh itself by drawing the bitmap from
    the new front buffer. Here is the implementation of the loop in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By calling `self._capture.read(self._image)`, we are telling OpenCV to reuse
    the image buffer in `self._image` (if `self.image` is not `None` and is the right
    size) so that new memory doesn't have to be allocated every time we capture a
    new frame. Alternatively, it would be valid, but less efficient, to call `self._capture.read()`
    without arguments; in this case, new memory would be allocated every time we captured
    a new frame.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the loop ends after our `_onCloseWindow` callback sets `_running`
    to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_detectAndRecognize` helper method is also running on the background thread.
    It begins by creating an equalized grayscale version of the image. An **equalized**
    image has an approximately uniform histogram; that is to say, for some bin size,
    the number of pixels in each bin of gray values is approximately equal. It is
    a kind of contrast adjustment that makes a subject''s appearance more predictable,
    despite different lighting conditions and exposure settings in different images;
    thus, it aids detection or recognition. We pass the equalized image to the classifier''s
    `detectMultiScale` method, also using the `scaleFactor`, `minNeighbors`, and `minSize` arguments
    that were specified during initialization of `InteractiveRecognizer`. As the return
    value from `detectMultiScale`, we get a list of rectangle measurements, describing
    the bounds of the detected faces. For display, we draw green outlines around these
    faces. If at least one face is detected, we store an equalized grayscale version
    of the first face in the `_currDetectedObject` member variable. Here is the implementation
    of this first portion of the `_detectAndRecognize` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we perform equalization separately on the detected face region after
    we crop it. This enables us to get an equalization result that is better adapted
    to the local contrast of the face, instead of the global contrast of the whole
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a face is currently detected and the recognition model is trained for at
    least one individual, we can proceed to predict the identity of the face. We pass
    the equalized face to the `predict` method of the recognizer and get two return
    values—an integer identifier and a measure of distance (non-confidence). Using
    our `BinasciiUtils.intToFourChars` function, we convert the integer into a string
    (of at most four characters), which will be one of the face names that the user
    previously entered. We show the name and distance. If an error occurs (for example,
    if an invalid model was loaded from file), we delete and recreate the model. If
    the model is not yet trained, we show instructions about training the model. Here
    is the implementation of this middle portion of the `_detectAndRecognize` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If no face was detected, we set `_currDetectedObject` to `None` and show either
    the instructions (if the model hasn''t been trained yet) or no descriptive text,
    otherwise. Under all conditions, we end the `_detectAndRecognize` method by ensuring
    that the Add to Model button is enabled or disabled, as appropriate. Here is this
    final portion of the method''s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The Add to Model button should be enabled only when a face is detected and
    the text field is not empty. We can implement this logic in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set the label''s text under several different conditions, we use the
    following helper functions to reduce repetition of code, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of the `wx.CallAfter` function to ensure that the label is updated
    on the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is all the functionality of `Interactive Recognizer`. Now, we just need
    to write the `main` functions for the two variants of the app, starting with the
    `Interactive Human Face Recognizer`. As arguments to the initializer of `InteractiveRecognizer`,
    we provide the app''s title and PyInstaller-compatible paths to the relevant detection
    model and recognition model. We run the app. Here is the implementation, which
    we may put in `InteractiveHumanFaceRecognizer.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Remember that `cascades/haarcascade_frontalface_alt.xml` or `cascades/lpbcascade_frontalface.xml`
    needs to be obtained from the OpenCV samples or from this book's GitHub repository.
    Feel free to test `Interactive Human Face Recognizer` now!
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second variant of the app, `Interactive Cat Face Recognizer`, uses very
    similar code. We change the app''s title and the paths of the detection and recognition
    models. Also, we lower the `scaleFactor` value to `1.2`, the `minNeighbors` value
    to `1`, and the `minSizeProportional` value to (`0.125`, `0.125`) to make the
    detector a little more sensitive. (A cat face is smaller than a human face, and
    our cat face detection model turns out to be less prone to false positives than
    our human face detection model, so these adjustments are appropriate.) Here is
    the implementation, which we may put in `InteractiveCatFaceRecognizer.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, `Interactive Cat Face Recognizer` will not run properly because
    `cascades/haarcascade_frontalcatface.xml`, `cascades/haarcascade_frontalcatface_extended.xml`,
    or `cascades/lpbcascade_frontalcatface.xml` does not exist (unless you copied
    the prebuilt version from this book's GitHub repository). Soon, we will create
    it!
  prefs: []
  type: TYPE_NORMAL
- en: Planning the cat-detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I said *soon*, I meant in a day or two. Training a Haar cascade takes a
    lot of processing time. Training an LBP cascade is relatively quick. However,
    in either case, we need to download some big collections of images before we even
    start. Settle down with a reliable internet connection, a power outlet, at least
    4 GB of free disk space, and the fastest CPU and biggest RAM you can find. Do
    not attempt this segment of the project on a Raspberry Pi. Keep the computer away
    from external heat sources or things that might block its fans. My processing
    time for Haar cascade training was 24 hours (or more for the whisker-friendly
    version that is sensitive to diagonal patterns), with 100% usage on four cores,
    on a MacBook Pro with a 2.6 GHz Intel Core i7 CPU and 16 GB RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the following sets of images, which are freely available for research
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: The PASCAL **Visual Object Classes Challenge 2007** (**VOC2007**) dataset. VOC2007
    contains 10,000 images of diverse subjects against diverse backgrounds, under
    diverse lighting conditions, so it is suitable as the basis of our negative training
    set. The images come with annotation data, including a count of cats for each
    image (most often 0). Thus, in building our negative training set, we can easily
    omit images that contain cats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontal face dataset from the **California Institute of Technology** (**Caltech**)
    Faces 1999\. This set contains 450 images of frontal human faces under diverse
    lighting conditions and against diverse backgrounds. These images make a useful
    addition to our negative training set because our frontal cat face detector may
    be deployed in places where frontal human faces are also likely to be present.
    None of the images contain cats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Urtho negative training set, which was originally part of a face- and eye-detection
    project called **Urtho**. This set contains 3,000 images of diverse backgrounds.
    None of the images contain cats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cat head dataset from Microsoft Research (*Microsoft Cat Dataset 2008*)
    has 10,000 images of cats against diverse backgrounds and under diverse lighting
    conditions. The rotation of the cat''s head varies, but in all cases the nose,
    mouth, both eyes, and both ears are clearly visible. Thus, we may say that all
    the images include frontal faces and are suitable for use as our positive training
    set. Each image comes with annotation data, indicating coordinates of the center
    of the mouth, center of the eyes, and corners of the hollow of the ear (three
    corners per ear). Based on the annotation data, we can straighten and crop the
    cat''s face in order to make the positive training images more similar to each
    other, as shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4ea6b541-c4f7-4b19-b7ed-f2887149e46e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The author of the Urtho negative training set is unknown. The other annotated
    datasets are generously provided by the following authors, as part of the following
    publications:'
  prefs: []
  type: TYPE_NORMAL
- en: Everingham, M. and Van Gool, L. and Williams, C. K. I. and Winn, J., and Zisserman,
    A. *The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weber, Markus. *Frontal face dataset*. California Institute of Technology, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weiwei Zhang, Jian Sun, and Xiaoou Tang. *Cat Head Detection - How to Effectively
    Exploit Shape and Texture Features*, *Proc. of European Conf. Computer Vision*,
    vol. 4, pp. 802-816, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will preprocess the images and generate files describing the positive and
    negative training sets. After preprocessing, all the training images are in equalized
    grayscale format, and the positive training images are upright and cropped. The
    description files conform to certain formats expected by the OpenCV training tools.
    With the training sets prepared, we will run the OpenCV training tools with the
    appropriate parameters. The output will be a Haar cascade file for detecting upright
    frontal cat faces.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the training script for the cat-detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Praline: I''ve never seen so many aerials in me life. The man told me, their
    equipment could pinpoint a purr at 400 yards and Eric, being such a happy cat,
    was a piece of cake."'
  prefs: []
  type: TYPE_NORMAL
- en: – Fish License sketch, Monty Python's Flying Circus, Episode 23 (1970)
  prefs: []
  type: TYPE_NORMAL
- en: 'This segment of the project uses tens of thousands of files, including images,
    annotation files, scripts, and intermediate and final outputs of the training
    process. Let''s organize all of this new material by giving our project a subfolder,
    `cascade_training`, which will ultimately have the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cascade_training/CAT_DATASET_01`: This is the first half of the Microsoft
    Cat Dataset 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/CAT_DATASET_02`: This is the second half of the Microsoft
    Cat Dataset 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/faces`: This is the Caltech Faces 1999 dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/urtho_negatives`: This is the Urtho negatives dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/VOC2007`: This is the VOC2007 dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/describe.py`: A script to preprocess and describe the positive
    and negative training sets. As outputs, it creates new images in the previous
    dataset directories and in the following text description files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/negative_description.txt`: This is a generated text file
    describing the negative training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/positive_description.txt`: This is a generated text file
    describing the positive training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/train.bat` (Windows) or `cascade_training/train.sh` (Mac
    or Linux): This is a script to run the OpenCV cascade training tools with appropriate
    parameters. As input, it uses the previous text description files. As output,
    it generates a not-yet mentioned binary description file and cascade files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/binary_description`: This is a generated binary file describing
    the positive training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/lbpcascade_frontalcatface/*.xml`: This gives the intermediate
    and final results of the LBP cascade training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/lbpcascade_frontalcatface.xml`: This is a copy of the final result
    of the LBP cascade training, in a location where our apps expect it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascade_training/haarcascade_frontalcatface/*.xml`: This shows the intermediate
    and final results of the Haar cascade training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalcatface.xml`: This is a copy of the final result
    of the Haar cascade training, in a location where our apps expect it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For up-to-date instructions on obtaining and extracting the Microsoft Cat Dataset
    2008, the Caltech Faces 1999 dataset, the Urtho negatives dataset, and the VOC2007
    dataset, refer to the README on this book's GitHub web page at [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/).
    Over time, some of the datasets' original websites and mirrors have gone down
    permanently, yet other mirrors continue to come online.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the datasets are downloaded and decompressed to the proper locations,
    let''s write `describe.py`. It needs to start with the following shebang line
    and imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'All our source images need some preprocessing to optimize them as training
    images. We need to save the preprocessed versions, so let''s globally define an
    extension that we will use for these files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To give our training images a more predictable appearance despite differences
    in lighting conditions and exposure settings, we need to create equalized grayscale
    images at several points in this script. Let''s write the following helper function
    for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we need to append to the negative description file at more than
    one point in the script. Each line in the negative description is just an image
    path. Let''s add the following helper method, which accepts an image path and
    a file object for the negative description, loads the image and saves an equalized
    version, and appends the equalized version''s path to the description file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write the `describeNegative` function that calls `describeNegativeHelper`.
    It begins by opening a file in write mode so that we can write the negative description.
    Then, we iterate over all the image paths in the Caltech Faces 1999 set, which
    contains no cats. We skip any paths to output images that were written on a previous
    call of this function. We pass the remaining image paths, along with the newly
    opened negative description file, to `describeNegativeHelper`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For every image in the Urtho negative training set, we pass the file path to `describeNegativeHelper`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The remainder of the `describeNegative` function is responsible for passing
    relevant file paths from the VOC2007 image set to `describeNegativeHelper`. Some
    images in VOC2007 do contain cats. An annotation file, `VOC2007/ImageSets/Main/cat_test.txt`,
    lists image IDs and a flag indicating whether any cats are present in the image.
    The flag may be—`1` (no cats), `0` (one or more cats as background or secondary
    subjects of the image), or `1` (one or more cats as foreground or foreground subjects
    of the image). We parse this annotation data and, if an image contains no cats,
    we pass its path and the description file to `describeNegativeHelper`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on to helper functions for generating the positive description.
    When rotating a face to straighten it, we also need to rotate a list of coordinate
    pairs representing features of the face. The following helper function accepts
    such a list, along with a center of rotation and angle of rotation, and returns
    a new list of the rotated coordinate pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s write a long helper function to preprocess a single positive training
    image. This function accepts two arguments—a list of coordinate pairs (which is
    named `coords`) and an OpenCV image. Refer to the diagram of feature points on
    a cat face. The numbering of the points signifies their order in a line of annotation
    data and in `coords`. To begin the function, we get the coordinates for the eyes
    and mouth. If the face is upside down (not an uncommon pose in playful or sleepy
    cats), we swap our definitions of left and right eyes to be consistent with an
    upright pose. (In determining whether the face is upside down, we rely in part
    on the position of the mouth relative to the eyes.) Then, we find the angle between
    the eyes and we rotate the image so that the face becomes upright. An OpenCV function
    called `cv2.getRotationMatrix2D` is used to define the rotation, and another function
    called `cv2.warpAffine` is used to apply it. As a result of rotating border regions,
    some blank regions are introduced into the image. We may specify a fill color
    for these regions as an argument to `cv2.warpAffine`. We use 50% gray, since it
    has the least tendency to bias the equalization of the image. Here is the implementation
    of this first part of the `preprocessCatFace` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As well as straightening the image, we call `rotateCoords` to make feature
    coordinates that match the straightened image. Here is the code for this function
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, the image and feature coordinates are transformed so that the
    cat''s eyes are level and upright. Next, let''s crop the image to eliminate most
    of the background and to standardize the eyes'' position relative to the bounds.
    Arbitrarily, we define the cropped face to be a square region, as wide as the
    distance between the outer base points of the cat''s ears. This square is positioned
    so that half its area lies to the left of the midpoint between the cat''s eyes,
    half lies to the right, 40% lies above, and 60% lies below. For an ideal frontal
    cat face, this crop excludes all background regions, but includes the eyes, chin,
    and several fleshy regions—the nose, mouth, and part of the inside of the ears.
    We equalize and return the cropped image. Accordingly, the implementation of `preprocessCatFace`
    proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: During cropping, we usually eliminate the blank border region that was introduced
    during rotation. However, if the cat face was close to the border of the original
    image, some of the rotated gray border region may remain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pair of screenshots is an example of input and output for the
    `processCatFace` function. First, there''s the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f23403bc-aabd-42a5-82d9-1dcd5a023e9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output is displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/943b2738-7ab0-45be-b71c-419354f919b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To generate the positive description file, we iterate over all the images in
    the Microsoft Cat Dataset 2008\. For each image, we parse the cat feature coordinates
    from the corresponding `.cat` file and we generate the straightened, cropped,
    and equalized image by passing the coordinates and original image to our `processCatFace`
    function. We append each processed image path and measurements to the positive
    description file. Here is the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, let''s take note of the format of a positive description file. Each line
    contains a path to a training image, followed by a series of numbers indicating
    the count of positive objects in the image and the measurements (x, y, width,
    and height) of rectangles containing those objects. In our case, there is always
    one cat face filling the entire cropped image, so we get lines such as the following,
    which is for a *64 x 64* image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Hypothetically, if the image had two *8 x 8* pixel cat faces in opposite corners,
    its line in the description file would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The main function of `describe.py` simply calls our `describeNegative` and
    `describePositive` functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Run `describe.py` and then feel free to have a look at the generated files,
    including `negative_description.txt`, `positive_description.txt`, and the cropped
    cat faces whose filenames follow the `CAT_DATASET_*/CAT_*/*.out.jpg` pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will use two of OpenCV's command-line tools. We will refer to them
    as `<opencv_createsamples>` and `<opencv_traincascade>`. They are responsible
    for converting the positive description to a binary format and generating the
    Haar cascade in an XML format, respectively. On Windows, these executables are
    named `opencv_createsamples.exe` and `opencv_traincascade.exe`. On Mac or Linux,
    the executables are named `opencv_createsamples` and `opencv_traincascade`.
  prefs: []
  type: TYPE_NORMAL
- en: For up-to-date instructions on obtaining `<opencv_createsamples>` and `<opencv_traincascade>`,
    refer to the README on this book's GitHub web page at [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/).
    At the time of writing, there is not yet an OpenCV 4.x version of these two command-line
    tools, but the OpenCV 3.4 version of them is forward-compatible, and work on a
    4.x version has been proposed for summer 2019.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many flags can be used to provide arguments to `<opencv_createsamples>` and
    `<opencv_traincascade>`, as described in the official documentation at [https://docs.opencv.org/master/dc/d88/tutorial_traincascade.html](https://docs.opencv.org/master/dc/d88/tutorial_traincascade.html).
    We use the following flags and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vec`: This is the path to a binary description of the positive training images.
    This file is generated by `<opencv_createsamples>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: This is the path to a text description of the positive training images.
    We generated this file using `describe.py`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bg`: The path to a text description of the negative training images. We generated
    this file using `describe.py`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num`: The number of positive training images in `info`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numStages`: The number of stages in the cascade. As we discussed earlier in
    *Conceptualizing Haar cascades and LBPH*, each stage is a test that is applied
    to an image region. If the region passes all tests, it is classified as a frontal
    cat face (or whatever class of object the positive training set represents). We
    use `20`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numPos`: The number of positive training images used in each stage. It should
    be significantly smaller than `num`. (Otherwise, the trainer will fail, complaining
    that it has run out of new images to use in new stages.) We use 90% of `num`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numNeg`: The number of negative training images used in each stage. We use
    90% of the number of negative training images in `bg`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minHitRate`: The **hit rate** is also called the **sensitivity**, **recall**,
    or **true positive rate**. In our case, it is the proportion of cat faces that
    are correctly classified as such. The `minHitRate` parameter specifies the minimum
    hit rate that *each* stage must achieve. A higher proportion implies a longer
    training time but a better fit between the model and the training data. (A better
    fit is normally a good thing, though it is possible to **overfit** so that the
    model does not make correct extrapolations beyond the training data.) We use `0.995`. With
    20 stages, this implies an overall hit rate of *0.995 ^ 20* or approximately 99%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxFalseAlarmRate`: The **false alarm rate** is also called the **miss rate**
    or **false positive rate**. In our case, it is the proportion of backgrounds or
    non-cat faces that are misclassified as cat faces. The `maxFalseAlarmRate` parameter
    specifies the maximum false alarm rate for *each* stage. We use `0.5`. With 20
    stages, this implies an overall false alarm rate of *0.5 ^ 20* or approximately
    one in a million.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featureType`: The type of features used, either `HAAR` (the default) or `LBP`.
    As we discussed previously, Haar cascades tend to be more reliable but are much
    slower to train and somewhat slower at runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: This is the subset of Haar features used. (For LBP, this flag has no
    effect.) The valid options are `BASIC` (the default), `CORE`, and `ALL`. The `CORE`
    option makes the model slower to train and run, but the benefit is to make the
    model sensitive to little dots and thick lines. The `ALL` option goes further,
    making the model even slower to train and run but adding sensitivity to diagonal
    patterns (whereas `BASIC` and `CORE` are only sensitive to horizontal and vertical
    patterns). The `ALL` option has nothing to do with detecting non-upright subjects.
    Rather, it relates to detecting subjects that contain diagonal patterns. For example,
    a cat''s whiskers and ears might qualify as diagonal patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s write a shell script to run `<opencv_createsamples>` and `<opencv_traincascade>`
    with the appropriate flags and to copy the resulting Haar cascade to the path
    where `Interactive Cat Face Recognizer` expects it. On Windows, let''s call our
    script `train.bat` and implement it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'On Mac or Linux, let''s instead call our script `train.sh` and implement it
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The preceding versions of the training script are configured to use basic Haar
    features and will take a long, long time to run, perhaps more than a day. By commenting
    out the variables related to a basic Haar configuration and uncommenting the variables
    related to an LBP configuration, we can cut the training time down to several
    minutes. As a third alternative, variables for an extended Haar configuration
    (sensitive to diagonal patterns) are also present but currently commented out.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training is done, feel free to have a look at the generated files,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For basic Haar features, `cascades/haarcascade_frontalcatface.xml` and `cascade_training/haarcascade_frontalcatface/*`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For extended Haar features, `cascades/haarcascade_frontalcatface_extended.xml`
    and `cascade_training/haarcascade_frontalcatface_extended/*`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For LBP, `cascades/lbpcascade_frontalcatface.xml` and `cascade_training/lbpcascade_frontalcatface/*`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, let's run `InteractiveCatFaceRecognizer.py` to test our cascade!
  prefs: []
  type: TYPE_NORMAL
- en: Remember that our detector is designed for frontal upright cat faces. The cat
    should be facing the camera and might need some incentive to hold that pose. For
    example, you could ask the cat to settle on a blanket or in your lap, and you
    could pat or comb the cat. See the following screenshot of my colleague, Chancellor
    Josephine (*Little Jo*) Antoinette Puddingcat, GRL (Grand Rock of Lambda), sitting
    for a test.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not have a cat (or even a human) who is willing to participate, then
    you can simply print a few images of a given cat (or human) from the web. Use
    heavy, matte paper and hold the print so that it faces the camera. Use prints
    of some images for training the recognizer and prints of other images for testing
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1496173d-fee5-46a8-ae44-f33cdfda9fe4.png)'
  prefs: []
  type: TYPE_IMG
- en: Our detector is pretty good at finding frontal cat faces. However, I encourage
    you to experiment further, make it better, and share your results! The current
    version sometimes mistakes the center of a frontal human face for a frontal cat
    face. Perhaps we should have used more databases of human faces as negative training
    images. Alternatively, if we had used faces of several mammal species as positive
    training images, could we have created a more general mammal face detector? Let
    me know what you discover!
  prefs: []
  type: TYPE_NORMAL
- en: Planning the Angora Blue app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Angora Blue` reuses the same detection and recognition models that we created
    earlier. It is a relatively linear and simple app because it has no GUI and does
    not modify any models. It just loads the detection and recognition models from
    file and then silently runs a camera until a face is recognized with a certain
    level of confidence. After recognizing a face, the app sends an email alert and
    exits. To elaborate, we may say the app has the following flow of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Load face detection and face recognition models from file for both human and
    feline subjects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Capture a live video from a camera. For each frame of video, it can do the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect all human faces in the frame. Perform recognition on each human face.
    If a face is recognized with a certain level of confidence, it sends an email
    alert and exits the app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect all cat faces in the frame. Discard any cat faces that intersect with
    human faces. (We assume that such cat faces are false positives, since our cat
    detector sometimes mistakes human faces for cat faces.) For each remaining cat
    face, it performs recognition. If a face is recognized with a certain level of
    confidence, it sends an email alert and exits the app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Angora Blue` is capable of running on a Raspberry Pi. The Pi''s small size
    makes it a nice platform for a hidden alarm system! Make sure that the Pi or other
    machine is connected to the internet in order to send email messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Angora Blue app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Angora Blue` app uses three new files—`GeomUtils.py`, `MailUtils.py`,
    and `AngoraBlue.py`, which should all be in our project''s top folder. Given the
    app''s dependencies on our previous work, the following files are relevant to
    `Angora Blue`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalface_alt.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cascades/haarcascade_frontalcatface.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognizers/lbph_human_faces.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognizers/lbph_cat_faces.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResizeUtils.py`: A utility function for resizing images, including camera
    capture dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GeomUtils.py`: A utility function for geometric operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MailUtils.py`: A utility function for sending emails'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AngoraBlue.py`: The application that sends an email alert when a person or
    cat is recognized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let''s create `GeomUtils.py`. It doesn''t need any import statements.
    Let''s add the following `intersects` function, which accepts two rectangles as
    arguments and returns either `True` (if they intersect) or `False` (otherwise),
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `intersects` function, let''s write the following `difference` function,
    which accepts two lists of rectangles, `rects0` and `rects1`, and returns a new
    list containing the rectangles in `rects0` that don''t intersect with any rectangle
    in `rects1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Later, we will use the `difference` function to filter out cat faces that intersect
    with human faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create `MailUtils.py`. It needs the following `import` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For the task of sending an email, let''s copy the following function from Rosetta
    Code, a free wiki that offers utility functions in many programming languages,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `sendEmail` function uses Gmail. By specifying the optional
    `smtpServer` argument, we can use a different service.
  prefs: []
  type: TYPE_NORMAL
- en: Since July 2014, the default security settings on Google accounts require apps
    to use not only SMTP authentication but also OAuth authentication in order to
    send an email through Gmail. Our `sendEmail` function uses a secure TLS connection
    but handles SMTP authentication only (as this is sufficient for most email services
    other than Gmail). To reconfigure your Google account for compatibility with our
    function, log in to your account, go to [https://www.google.com/settings/security/lesssecureapps](https://www.google.com/settings/security/lesssecureapps),
    select the Enable option, and click Done. For best security, you might wish to
    create a dummy Google account for this project and apply the custom security setting
    to this dummy account only. Alternatively, most email services besides Gmail should
    not require special configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to implement `AngoraBlue.py`. It starts with the following
    shebang line and imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`Angora Blue` simply uses a `main` function and one helper function, `recognizeAndReport`.
    This helper function begins as follows, by iterating over a given list of face
    rectangles and using a given recognizer (be it a human recognizer or a cat recognizer)
    to get a label and distance (non-confidence) for each face, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'For testing, it is useful to log the recognition results here. However, we
    comment out the logging in the final version, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If any of the faces is recognized with a certain level of confidence (based
    on a `maxDistance` argument), we attempt to send an email alert. If the alert
    is sent successfully, the function returns `True`, meaning it did recognize and
    report a face. Otherwise, it returns `False`. Here is the remainder of the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function starts by defining paths to the detection and recognition
    models. If either recognition model does not exist (because it has not been trained),
    we print an error and exit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We prompt the user to enter email credentials and recipients, and we store
    the user responses in local variables, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As in `Interactive Recognizer`, we start capturing video from a camera and
    we store the video''s resolution in order to calculate the relative, minimum size
    of a face. Here is the relevant code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We load detectors and recognizers from file and set a minimum face size for
    detection and maximum distance (non-confidence) for recognition. We specify the
    values separately for human and feline subjects. You may need to tweak the values
    based on your particular camera setup and models. The code proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We read frames from the camera continuously until an email alert is sent as
    a result of face recognition. Each frame is converted into grayscale and equalized.
    Next, we detect and recognize human faces and possibly send an alert, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'If no alert has been sent, we continue to cat-detection and recognition. For
    cat-detection, we make extra efforts to eliminate false positives by specifying
    a higher `minNeighbors` value and by filtering out any cat faces that intersect
    human faces. Here is this final part of Angora Blue''s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Before testing `Angora Blue`, ensure that the two recognition models are trained
    using `Interactive Human Face Recognizer` and `Interactive Cat Face Recognizer`.
    Preferably, each model should contain two or more individuals. Then, set up a
    computer and webcam in a place where frontal human faces and frontal cat faces
    will be encountered. Try to get your friends and pets to participate in the following
    test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: A human, who is unknown to the model, looks into the camera. Nothing should
    happen. If you get an email alert, increase `humanMaxDistance` and try again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cat, who is unknown to the model, looks into the camera. Nothing should happen.
    If you get an email alert, increase `catMaxDistance` and try again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A human, who is known to the model, looks into the camera. You should get an
    email alert. If not, decrease `humanMaxDistance` or rerun `Interactive Human Face
    Recognizer` to add more samples of the given human face. Try `Angora Blue` again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cat, who is known to the model, looks into the camera. You should get an email
    alert. If not, decrease `catMaxDistance` or rerun `Interactive Cat Face Recognizer`
    to add more samples of the given cat face. Try `Angora Blue` again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, if you don't have enough human or feline volunteers, just get some heavy,
    matte paper and print faces from the web. Hold a print so that it is visible (and
    upright) from the camera's perspective, but ensure that you stay out of view so
    that the recognizer runs only on the print, not on you.
  prefs: []
  type: TYPE_NORMAL
- en: Once the recognition model and `Angora Blue` are tweaked, we are ready to deploy
    our alarm system to a vast network of webcam-enabled computers! Let the search
    for the blue-eyed Angora begin!
  prefs: []
  type: TYPE_NORMAL
- en: Building Angora Blue for distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use PyInstaller to bundle `Angora Blue`, along with detection and recognition
    models, for distribution. Since the build scripts should be quite similar to the
    ones we used for Luxocator (the [Chapter 2](4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml), *Searching
    for Luxury Accommodations Worldwide,* project), we will not discuss their implementation
    here. However, they are included in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Further fun with finding felines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kittydar** (short for **kitty radar**), by Heather Arthur, is an open source,
    JavaScript library for detecting upright frontal cat faces. You can find its demo
    application at [http://harthur.github.io/kittydar/](http://harthur.github.io/kittydar/)
    and its source code at [https://github.com/harthur/kittydar](https://github.com/harthur/kittydar).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another detector for upright frontal cat faces was developed by Microsoft Research
    using the Microsoft Cat Dataset 2008\. The detector is described in the following
    research paper, but no demo application or source code has been released:'
  prefs: []
  type: TYPE_NORMAL
- en: Weiwei Zhang, Jian Sun, and Xiaoou Tang. Cat Head Detection - How to Effectively
    Exploit Shape and Texture Features, *Proc. of European Conf. Computer Vision*,
    vol. 4, pp. 802-816, 2008.
  prefs: []
  type: TYPE_NORMAL
- en: If you know of other work on cat detectors, recognizers, or datasets, please
    write to tell me about it!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the previous chapter, this chapter has dealt with classification tasks,
    as well as interfaces among OpenCV, a source of images, and a GUI. This time,
    our classification labels have more objective meanings (a species or an individual's
    identity), so the classifier's success or failure is more obvious. To meet the
    challenge, we used much bigger sets of training images, we preprocessed the training
    images for greater consistency, and we applied two tried-and-true classification
    techniques in sequence (either Haar cascades or LBP cascades for detection and
    then LBPH for recognition).
  prefs: []
  type: TYPE_NORMAL
- en: The methodology presented in this chapter, as well as the entire `Interactive
    Recognizer` app and some of the other code, generalizes well to other original
    work in detection and recognition. With the right training images, you could detect
    and recognize many more animals in many poses. You could even detect an object
    such as a car and recognize the Batmobile!
  prefs: []
  type: TYPE_NORMAL
- en: For our next project, we turn our attention to a moving target, literally. We
    will try to detect a person who is in motion and then recognize particular gestures.
  prefs: []
  type: TYPE_NORMAL
