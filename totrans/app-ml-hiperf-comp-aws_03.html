<html><head></head><body>
		<div id="_idContainer051">
			<h1 id="_idParaDest-51" class="chapter-number"><a id="_idTextAnchor050"/>3</h1>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Compute and Networking</h1>
			<p>Several large and small organizations run workloads on AWS using AWS compute. Here, AWS <strong class="bold">Compute</strong> refers to a set of services on AWS that help you build and deploy your own solutions and services; this can include workloads as diverse as websites, data analytics engines, <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>), <strong class="bold">High-Performance Computing</strong> (<strong class="bold">HPC</strong>), and more. Being one of the first services to be released, <strong class="bold">Amazon Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) is sometimes used synonymously with the term <em class="italic">compute</em> and offers a wide variety of instance types, processors, memory, and storage configurations for <span class="No-Break">your workloads.</span></p>
			<p>Apart from EC2, compute services that are suited to some specific types of workloads include Amazon <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>), <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), Batch, Lambda, Wavelength, and Outposts. <strong class="bold">Networking on AWS</strong> refers to foundational networking services, including Amazon <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>), AWS Transit Gateway, and AWS PrivateLink. These services, along with the various compute services, enable you to build solutions with the most secure and performant networked systems at a global scale. AWS compute and networking concepts are two broad topics and are important to understand many concepts that will be discussed in the <span class="No-Break">following chapters.</span></p>
			<p>Compute and networking also form two important pillars of HPC, along with data management, which was discussed in the last chapter. Every application of HPC is generally optimized for high levels of distributed compute, which depends <span class="No-Break">on networking.</span></p>
			<p>In this chapter, you will learn about the different services AWS offers for compute and networking, how these services are used for different types of computing workloads, and lastly, best practices for the HPC type of workloads on AWS, which goes beyond the AWS <span class="No-Break">Well-Architected Framework.</span></p>
			<p>Specifically, in this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introducing the AWS <span class="No-Break">compute ecosystem</span></li>
				<li>Networking <span class="No-Break">on AWS</span></li>
				<li>Selecting the right compute for <span class="No-Break">HPC workloads</span></li>
				<li>Best practices for <span class="No-Break">HPC workloads</span></li>
			</ul>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Introducing the AWS compute ecosystem</h1>
			<p>Compute lies <a id="_idIndexMarker143"/>at the foundation of every HPC application that you will read about in and outside of this book. In AWS and other clouds in general, compute refers to a group of services that offer the basic building blocks of performing a computation or some business logic. This can range from basic data computations <span class="No-Break">to ML.</span></p>
			<p>The basic units of measuring compute power on AWS (regardless of the service we are talking about) are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><em class="italic">Processing units</em> – this can be measured as the number of <strong class="bold">Central Processing Units</strong> (<strong class="bold">CPUs</strong>), <strong class="bold">Virtual CPUs</strong> (<strong class="bold">vCPUs</strong>), or <strong class="bold">Graphics Processing </strong><span class="No-Break"><strong class="bold">Units</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GPUs</strong></span><span class="No-Break">)</span></li>
				<li><em class="italic">Memory</em> – this is the total requested or allocated memory for the application measured in units <span class="No-Break">of bytes</span></li>
			</ul>
			<p>Typical HPC applications access multiple instances and hence can take advantage of pooled compute and memory resources for <span class="No-Break">larger workloads.</span></p>
			<p>The foundational service that provides compute resources for customers to build their applications on AWS is called Amazon EC2. Amazon EC2 provides customers with a choice of about 500 instance types (at the time of writing this book and according to public documentation). Customers can then tailor the right combination of instance types for their <span class="No-Break">business applications.</span></p>
			<p>Amazon EC2 <a id="_idIndexMarker144"/>provides five types <span class="No-Break">of instances:</span></p>
			<ul>
				<li>General <span class="No-Break">purpose instances</span></li>
				<li>Compute <span class="No-Break">optimized instances</span></li>
				<li>Accelerated <span class="No-Break">computing instances</span></li>
				<li>Memory <span class="No-Break">optimized instances</span></li>
				<li>Storage <span class="No-Break">optimized instances</span></li>
			</ul>
			<p>Each of the instance types listed here is actually a family of instances, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B18493_03_001.jpg" alt="Figure 3.1 – Amazon EC2 instance types"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Amazon EC2 instance types</p>
			<p>In the following<a id="_idIndexMarker145"/> section, we will highlight some important facts about these <span class="No-Break">instance types.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>General purpose instances</h2>
			<p>General purpose <a id="_idIndexMarker146"/>instances can be used for a <a id="_idIndexMarker147"/>variety of workloads. They have the right balance of compute, memory, and storage for most typical applications that customers have on AWS. On AWS, there <a id="_idIndexMarker148"/>are several types of general <span class="No-Break">purpose instances:</span></p>
			<ul>
				<li><strong class="bold">T-type instances</strong>: T instances, for<a id="_idIndexMarker149"/> example, the<a id="_idIndexMarker150"/> T2 instance, are <em class="italic">burstable</em> instances that provide a basic level of compute for low compute- and memory-footprint workloads. The following figure shows what a typical workload that is suited for T-type instances might look like – most of the time is spent under the baseline CPU utilization level, with a need to <em class="italic">burst</em> above this baseline occasionally. With non-burstable instances, application owners need to over-provision for the burst CPU levels, and therefore pay more while utilizing very little. With burstable T instances, credits are accrued for utilization under the baseline (white areas below the baseline), and these credits can be used when the application is experiencing higher loads; see<a id="_idIndexMarker151"/> the gray <a id="_idIndexMarker152"/>filled-in areas in the <span class="No-Break">following graph:</span></li>
			</ul>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18493_03_002.jpg" alt="Figure 3.2 – CPU utilization versus time for burstable T-type instances on AWS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – CPU utilization versus time for burstable T-type instances on AWS</p>
			<ul>
				<li><strong class="bold">M-type instances</strong>: M instances (like the M4, M5, and M6) can be used for a variety of <a id="_idIndexMarker153"/>workloads <a id="_idIndexMarker154"/>that need a balance of compute, memory, and networking, including (but not limited to) web and application servers, and small to medium-sized database<a id="_idIndexMarker155"/> workloads. Special versions of M5 and M6 instances are offered that are suitable for certain workloads. For example, the M5zn instance types can be used for applications that demand extremely high single-threaded performance, such as HPC, simulations, <span class="No-Break">and gaming.</span></li>
				<li><strong class="bold">A-type instances</strong>: A1 instances <a id="_idIndexMarker156"/>are used<a id="_idIndexMarker157"/> to run <strong class="bold">Advanced RISC Machine</strong> or <strong class="bold">ARM</strong>-based<a id="_idIndexMarker158"/> applications, such as microservices and web servers powered by the AWS Graviton <span class="No-Break">ARM processors.</span></li>
				<li><strong class="bold">Mac-type instances</strong>: Mac1 instances<a id="_idIndexMarker159"/> are<a id="_idIndexMarker160"/> powered by Apple’s Mac Mini computers and provide very high network and storage bandwidth. They are typically used for building and<a id="_idIndexMarker161"/> testing Apple applications for the iPhone, Mac, and <span class="No-Break">so on.</span></li>
			</ul>
			<p>In the following section, we will discuss compute optimized instances <span class="No-Break">on AWS.</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Compute optimized instances</h2>
			<p>Many HPC <a id="_idIndexMarker162"/>applications that will be <a id="_idIndexMarker163"/>described in this book take advantage of the high-performance, compute optimized instance types <span class="No-Break">on AWS.</span></p>
			<p>There are several types of<a id="_idIndexMarker164"/> compute <span class="No-Break">optimized instances:</span></p>
			<ul>
				<li><strong class="bold">C5 instances</strong>: C5 and C5n instances <a id="_idIndexMarker165"/>provide<a id="_idIndexMarker166"/> low-cost, high-performance compute for typical HPC, gaming, batch processing, and modeling. C5 instances use Intel Xeon processors (first and second generation) and provide upward of 3.4 GHz clock speeds on a single core. C5a instances also provide AMD processors for high performance at an even lower cost. C5n instances are well suited to HPC applications since they <a id="_idIndexMarker167"/>support <strong class="bold">Elastic Fabric Adapter</strong> (<strong class="bold">EFA</strong>) and can deliver up to 100 gigabits per second of networking throughput. For more information about EFA, please <span class="No-Break">visit </span><a href="https://aws.amazon.com/hpc/efa/"><span class="No-Break">https://aws.amazon.com/hpc/efa/</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">C6 instances</strong>: C6g instances are <a id="_idIndexMarker168"/>ARM-based<a id="_idIndexMarker169"/> instances based on the AWS Graviton processor. They are ideal for running HPC workloads, ad serving, and game servers. C6g instances<a id="_idIndexMarker170"/> are available with local <strong class="bold">Non-Volatile Memory express</strong> or <strong class="bold">NVMe</strong>-based high performance, low latency SSD storage with 100 gigabits per second networking, and support for EFA. On the other hand, the C6i class of instances is Intel Xeon-based and can provide up to 128 vCPUs per instance for typical <span class="No-Break">HPC workloads.</span></li>
				<li><strong class="bold">HPC instances</strong>: The HPC6a <a id="_idIndexMarker171"/>instance<a id="_idIndexMarker172"/> type is powered by third-generation AMD processors for lower cost-to-performance ratios for typical HPC workloads. These instance types also provide 96 CPU cores and 384 GB of RAM for memory-intensive applications. HPC6a instances also support EFA-based networking for up to 100 gigabits per second <span class="No-Break">of throughput.</span></li>
			</ul>
			<p>In the following section, we will discuss accelerated compute instances <span class="No-Break">on AWS.</span></p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Accelerated compute instances</h2>
			<p>Accelerated<a id="_idIndexMarker173"/> computing instances use co-processors <a id="_idIndexMarker174"/>such as GPUs to accelerate performance for workloads such as floating point number calculations useful for ML, deep learning, and <span class="No-Break">graphics processing.</span></p>
			<p>Accelerated compute instances use hardware-based compute accelerators such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break">GPUs</span></li>
				<li><strong class="bold">Field Programmable Gate </strong><span class="No-Break"><strong class="bold">Arrays</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FPGAs</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">AWS Inferentia</span></li>
			</ul>
			<h3>GPUs</h3>
			<p>GPUs were <a id="_idIndexMarker175"/>originally used for 3D graphics but are now being used as general-purpose co-processors for various applications such as HPC and deep learning. HPC applications are computation and bandwidth-heavy. Several types of NVIDIA GPUs are available on AWS, and detailed information can be found at the following <span class="No-Break">link, </span><a href="https://aws.amazon.com/nvidia/"><span class="No-Break">https://aws.amazon.com/nvidia/</span></a><span class="No-Break">.</span></p>
			<p>Let’s dive into the basics of how GPUs help with compute-heavy calculations. Imagine adding a list of numbers to another list of the same size. Visually, this looks like the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18493_03_003.jpg" alt="Figure 2.3 – Adding two arrays"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Adding two arrays</p>
			<p>The naïve way of adding these to arrays is to loop through all elements of each array and add each corresponding number from the top and bottom arrays. This may be fine for small arrays, but what about arrays that are millions of elements long? To do this on a GPU, we first allocate memory for these two very long arrays and then use <em class="italic">threads</em> to parallelize these computations. Adding these arrays using a single thread on a single GPU is the same as our earlier naïve approach. Using multiple threads (say 256) can help parallelize this operation by allocating a part of the work to each thread. For example, the first few elements (the total size divided by 256 in this case) will be done by the first thread, and so<a id="_idIndexMarker176"/> on. This speeds up the operation by letting each thread focus on a smaller portion of the work and do each of these split-up addition operations in parallel; see the shaded region in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B18493_03_004.jpg" alt="Figure 3.4 – Multiple threads handling a portion of the computation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Multiple threads handling a portion of the computation</p>
			<p>GPUs today are architected in a way that allows even higher levels of parallelism – multiple processing threads make up a block, and there are usually multiple blocks in a GPU. Each block can run<a id="_idIndexMarker177"/> concurrently in a <strong class="bold">Streaming Multiprocessor</strong> (<strong class="bold">SM</strong>) and process the same set of computations or <em class="italic">kernels</em>. Visually, this looks like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18493_03_005.jpg" alt="Figure 3.5 – Multiple blocks in a GPU"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Multiple blocks in a GPU</p>
			<p>To give you an idea of what you can access on AWS, consider the <strong class="bold">P4d.24xlarge</strong> instance. This instance has eight GPUs, as seen in the following figure, each of which is an NVIDIA A100 housing 108 SMs, with each SM capable of running 2,048 threads <span class="No-Break">in parallel:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18493_03_006.jpg" alt="Figure 3.6 – A single instance with multiple GPUs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – A single instance with multiple GPUs</p>
			<p>On AWS, P4d <a id="_idIndexMarker178"/>instances can be used to provision a supercomputer or an EC2 Ultracluster with more than 4,000 A100 GPUs, Petabit-scale networking, and scalable, shared high throughput storage on Amazon FSx for Lustre (<a href="https://aws.amazon.com/fsx/lustre/">https://aws.amazon.com/fsx/lustre/</a>). Application and package developers use the NVIDIA CUDA library to build massively parallel applications for HPC and deep learning. For example, PyTorch, a popular ML library, uses NVIDIA’s CUDA GPU programming library for training<a id="_idIndexMarker179"/> large-scale models. Another example is Ansys Fluent, a popular <strong class="bold">Computational Fluid Dynamics</strong> (<strong class="bold">CFD</strong>) simulation software that uses GPU cores to accelerate fluid <span class="No-Break">flow computations.</span></p>
			<p>On AWS, there are several families of <span class="No-Break">GPU instances:</span></p>
			<ul>
				<li><strong class="bold">G family of instances</strong>: G2, G3, G4, and G5 type instances on AWS provide cost-effective access to GPU resources. Each G-type instance mentioned here comes with a different NVIDIA GPU – for example, the latest G5 instances come with NVIDIA A10G GPUs, G5g instances with NVIDIA T4G GPUs, and G4Dn with the NVIDIA Tesla GPUs. AMD-based GPUs are also available – for example, the G4ad instances use the AMD Radeon Pro <span class="No-Break">V520 GPUs.</span></li>
				<li><strong class="bold">P family of instances</strong>: These instances provide extremely high-performance GPUs for single-instance and distributed applications. The P2 instances provide access to the NVIDIA K80 GPUs, P3 instances have the NVIDIA Tesla V100 GPUs, and the P4d instances have the <span class="No-Break">A10 GPUs.</span></li>
				<li><strong class="bold">VT1 instances</strong>: These provide access to the Xilinx Alveo U30 media accelerator cards that are primarily used for video transcoding applications. More information about VT1 instances can be found <span class="No-Break">here: </span><a href="https://aws.amazon.com/ec2/instance-types/vt1/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/vt1/</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">AWS Inferentia</strong>: These instances are specifically designed to provide cost-effective and low latency ML inference capability and are a custom-made chip created by AWS. A typical workflow that customers could follow using these <em class="italic">Inf1</em> instances is to use an ML framework like TensorFlow to train a model on another EC2 instance <a id="_idIndexMarker180"/>or SageMaker training instances, then use Amazon SageMaker’s compilation feature <em class="italic">Neo</em> to compile the model for use with the Inf1 instances. You can also make use of the AWS Neuron SDK to profile and deploy deep learning models onto <span class="No-Break"><em class="italic">Inf1</em></span><span class="No-Break"> instances.</span></li>
			</ul>
			<h3>FPGA instances</h3>
			<p>Amazon EC2 F1 instances <a id="_idIndexMarker181"/>allow you to develop and deploy hardware-accelerated applications easily on the cloud. Example applications include (but are not limited to) big data analytics, genomics, and simulation-related applications. Developers can use high-level C/C++ code to program their applications, register the FPGA as an <strong class="bold">Amazon FPGA Image</strong> (<strong class="bold">AFI</strong>), and deploy the application to an F1 instance. For more information on F1 instances, please refer to the links in the Reference<a id="_idIndexMarker182"/> section at the end of <span class="No-Break">this chapter.</span></p>
			<p>In the following section, we will discuss memory optimized compute instances <span class="No-Break">on AWS.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Memory optimized instances</h2>
			<p>Memory optimized<a id="_idIndexMarker183"/> instances <a id="_idIndexMarker184"/>on AWS are suited to run applications that require storage of extremely large data in memory. Typical applications that fall into this category are in-memory databases, HPC applications, simulation, and <strong class="bold">Electronic Design Automation</strong> (<strong class="bold">EDA</strong>) applications. On AWS, there are several <a id="_idIndexMarker185"/>types of<a id="_idIndexMarker186"/> memory <span class="No-Break">optimized instances:</span></p>
			<ul>
				<li><strong class="bold">R5 instances</strong>: The R5 family<a id="_idIndexMarker187"/> of instances (such as the R5, R5a, R5b, and R5n instance types) is a great choice for relational databases such as MySQL, MongoDB, and Cassandra, for in-memory databases such as Redis and Memcached, and business intelligence applications, such as SAP HANA and HPC applications. The R5 metal instance type also provides direct access to processors and memory on the physical server that the instance is <span class="No-Break">based on.</span></li>
				<li><strong class="bold">R6 instances</strong>: R6 instances<a id="_idIndexMarker188"/> such as R6g and R6gd are based on ARM-based AWS Gravitron2 processors and can provide better price-to-performance ratios compared to R5 instances. Application developers can use these instance types to develop or support ARM-based applications that need a high <span class="No-Break">memory footprint.</span></li>
				<li><strong class="bold">U instances</strong>: These instances<a id="_idIndexMarker189"/> are extremely high memory instances – they can offer anywhere from 6 to 24 TB of memory per instance. They are typically used to run large in-memory applications such as databases and SAP HANA and are powered by Intel Xeon Platinum <span class="No-Break">8176M processors.</span></li>
				<li><strong class="bold">X instances</strong>: X-type<a id="_idIndexMarker190"/> instances (such as X1, X1e, and X1gd instance types) are designed for large-scale in-memory applications in the cloud. Each X1 instance is powered by four Intel Xeon E8880 processors with up to 128 vCPUs, and up to 1,952 GB of memory. X1 instances are picked by developers for their low price-to-performance ratio<a id="_idIndexMarker191"/> given the amount of memory provided, compared to other families of instances. X1e instances provide even higher memory (up to 3,904 GB) and support production-grade SAP workloads. Both X1 and X1e instance types provide up to 25 gigabits per second of network bandwidth when<a id="_idIndexMarker192"/> used with an <strong class="bold">Elastic Network Adapter</strong> (<strong class="bold">ENA</strong>). Finally, X1gd and X2gd are AWS Graviton2-based ARM instances that provide better price performance compared to x86-based <span class="No-Break">X1 instances.</span></li>
				<li><strong class="bold">Z instance types</strong>: The Z1d instance <a id="_idIndexMarker193"/>type provides both high performance and high memory for typical data analytics, financial services, and HPC applications. Z1d instances are well suited for applications that need very high single-threaded performance, with an added dependence on high memory. Z1d instances come in seven different sizes, and up to 48 vCPUs and 384 GB of RAM, so customers can choose the right instance size for <span class="No-Break">their application.</span></li>
			</ul>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Storage optimized instances</h2>
			<p>Storage <a id="_idIndexMarker194"/>optimized<a id="_idIndexMarker195"/> instances are well suited for applications that need frequent, sequential reads<a id="_idIndexMarker196"/> and writes from local storage by providing very high <strong class="bold">I/O</strong> <strong class="bold">Operations Per Second</strong> (<strong class="bold">IOPS</strong>). There <a id="_idIndexMarker197"/>are several storage optimized instances <span class="No-Break">on AWS:</span></p>
			<ul>
				<li><strong class="bold">D-type instances</strong>: Instances <a id="_idIndexMarker198"/>such as D2, D3, and D3en <a id="_idIndexMarker199"/>provide high-performance local storage and can be used for MapReduce-style operations (with Hadoop or Spark), log processing, and other big data workloads that do not require all the data to be held in memory<a id="_idIndexMarker200"/> but<a id="_idIndexMarker201"/> require very fast, on-demand access to <span class="No-Break">this data.</span></li>
				<li><strong class="bold">H-type instances</strong>: The H1 instance<a id="_idIndexMarker202"/> is typically <a id="_idIndexMarker203"/>used for MapReduce applications and distributed file storage, and other <span class="No-Break">data-intensive applications.</span></li>
				<li><strong class="bold">I-type instances</strong>: I type<a id="_idIndexMarker204"/> instances such <a id="_idIndexMarker205"/>as I3 and I3en are well suited for relational and non-relational databases, in-memory caches, and other big data applications. The I3 instances<a id="_idIndexMarker206"/> are NVMe <strong class="bold">Solid State Drive</strong> (<strong class="bold">SSD</strong>) -based instances that can provide up to 25 GB of network bandwidth and 14 gigabits per <a id="_idIndexMarker207"/>second of dedicated bandwidth to attached <strong class="bold">Elastic Block Store</strong> (<strong class="bold">EBS</strong>) volumes. The Im4gn and Is4gen type instances can be used for relational and <em class="italic">NoSQL</em> databases, streaming <a id="_idIndexMarker208"/>applications, and distributed <span class="No-Break">file applications.</span></li>
			</ul>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Amazon Machine Images (AMIs)</h2>
			<p>Now that we <a id="_idIndexMarker209"/>have discussed different instance types that you can choose for your applications on AWS, we can move on to the topic of <strong class="bold">Amazon Machine Images</strong> (<strong class="bold">AMIs</strong>). AMIs contain all the information needed to launch an instance. This includes <span class="No-Break">the following:</span></p>
			<ul>
				<li>The description of the operating system to use, the architecture (32 or 64-bit), any applications to be included along with an application server, and EBS snapshots to be attached <span class="No-Break">before launch</span></li>
				<li>Block-device mapping that defines which volumes to attach to the instance <span class="No-Break">on launch</span></li>
				<li>Launch permissions that control which AWS accounts can use <span class="No-Break">this AMI</span></li>
			</ul>
			<p>You can create your own AMI, or buy, share, or sell your AMIs on the AWS Marketplace. AWS maintains Amazon Linux-based AMIs that are stable and secure, updated and maintained on a regular basis, and includes several AWS tools and packages. Furthermore, these<a id="_idIndexMarker210"/> AMIs are provided free of charge to <span class="No-Break">AWS customers.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Containers on AWS</h2>
			<p>In the previous section, we<a id="_idIndexMarker211"/> spoke about AMIs on AWS that can help isolate and replicate applications across several instances and instance types. Containers can be used to further isolate and launch one or more applications onto instances. The most popular flavor of containers is called Docker. <strong class="bold">Docker</strong> is an<a id="_idIndexMarker212"/> open platform for developing, shipping, and running applications. Docker provides the ability to package and run an application in a loosely isolated environment called a container. Docker containers are definitions of runnable images, and these images can be run locally on your computer, on virtual machines, or in the cloud. Docker containers can be run on any host operating system, and as such are extremely portable, as long as Docker is running on the <span class="No-Break">host system.</span></p>
			<p>A Docker container contains everything that is needed to run the applications that are defined inside it – this includes configuration information, directory structure, software dependencies, binaries, and packages. This may sound complicated, but it is actually very easy to define a Docker image; this is done in a Dockerfile that may look similar <span class="No-Break">to this:</span></p>
			<pre class="source-code">
FROM python:3.7-alpine
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD ["gunicorn", "-w 4", "main:app"]</pre>
			<p>The preceding file named <strong class="source-inline">Dockerfile</strong> defines the Docker image to run a sample Python application using the popular <strong class="source-inline">Gunicorn</strong> package (see the last line in the file). Before we can run the application, we tell Docker to use the Python-3.7 base image (<strong class="source-inline">FROM python:3.7-alpine</strong>), copy all the required files from the host system to a folder called <strong class="source-inline">app</strong>, and install requirements or dependencies for that application to run successfully (<strong class="source-inline">RUN pip install -r requirements.txt</strong>). Now you can test out this application locally before deploying it at scale on <span class="No-Break">the cloud.</span></p>
			<p>On AWS, you can run containers on EC2 instances of your choice or make use of the many container <span class="No-Break">services available:</span></p>
			<ul>
				<li>When you need to run containers with server-level control, you can directly run the images that you define on EC2. Furthermore, running these containers on EC2 <em class="italic">Spot</em> instances can save you up to 90% of the cost over on-demand instances. For more information on <em class="italic">Spot</em> instances please refer <span class="No-Break">to </span><a href="https://aws.amazon.com/ec2/spot/"><span class="No-Break">https://aws.amazon.com/ec2/spot/</span></a><span class="No-Break">.</span></li>
				<li>At the opposite end of the spectrum, you can use a service like AWS Fargate to run containers without managing servers. Fargate removes all the operational overhead of maintaining server-level software so you can focus on just the application at hand. With Fargate, you only pay for what you use – for example, if you create an<a id="_idIndexMarker213"/> application that downloads data files from Amazon S3, processes these files, and writes output files back to S3, and this process takes 30 minutes to finish, you only pay for the time and resources (vCPUs) used to complete <span class="No-Break">the task.</span></li>
				<li>When you have multiple, complex, container-based applications, managing and orchestrating these applications is an important task. On AWS, container management <a id="_idIndexMarker214"/>and orchestration can be achieved <a id="_idIndexMarker215"/>using services like Amazon <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), Amazon <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>), and <a id="_idIndexMarker216"/>Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>). A detailed discussion about these services is outside the scope of this book but if you are interested, you can refer to the links mentioned in the <em class="italic">References</em> section to <span class="No-Break">learn more.</span></li>
			</ul>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Serverless compute on AWS</h2>
			<p>In the previous<a id="_idIndexMarker217"/> section, you <a id="_idIndexMarker218"/>read about AWS Fargate, which lets you run applications and code based on Docker containers, without the need to manage infrastructure. This is an example of a serverless service on AWS. AWS offers serverless services that have the following features <span class="No-Break">in common:</span></p>
			<ul>
				<li>No infrastructure <span class="No-Break">to manage</span></li>
				<li><span class="No-Break">Automatic scaling</span></li>
				<li>Built-in <span class="No-Break">high availability</span></li>
				<li><span class="No-Break">Pay-per-use billing</span></li>
			</ul>
			<p>Serverless compute technologies on AWS are AWS Lambda and Fargate. AWS Lambda is a serverless computing service that lets you run any code that can be triggered by over 200 services and SaaS applications. Code can be written in popular languages such as Python, <a href="http://Node.js">Node.js</a>, Go, and Java or can be packaged as Docker containers, as described earlier. With AWS Lambda, you only pay for the number of milliseconds that your code runs, beyond a very generous free tier of over a million free requests. AWS Lambda supports the creation of a wide variety of applications including file processing, streaming, web applications, IoT backend applications, and mobile <span class="No-Break">app backends.</span></p>
			<p>For<a id="_idIndexMarker219"/> more<a id="_idIndexMarker220"/> information on serverless computing on AWS, please refer to the links included in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<p>In the next section, we will cover basic concepts around networking <span class="No-Break">on AWS.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Networking on AWS</h1>
			<p>Networking <a id="_idIndexMarker221"/>on AWS is <a id="_idIndexMarker222"/>a vast topic that is out of the scope of this book. However, in order to easily explain some of the sections and chapters that follow, we will attempt to provide a brief overview here. First, AWS has a concept<a id="_idIndexMarker223"/> called <strong class="bold">regions</strong>, which are physical areas around the world where AWS places clusters of data centers. Each region contains multiple logically separated, groups of data centers called <strong class="bold">availability zones</strong>. Each <a id="_idIndexMarker224"/>availability zone has independent power, cooling, and physical security. Availability zones are connected via redundant and ultra-low latency AWS Networks. At the time of writing this chapter, AWS has 26 regions and 84 <span class="No-Break">availability zones.</span></p>
			<p>The next foundational concept we will <a id="_idIndexMarker225"/>discuss here is a <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>). A VPC is a logical partition that lets you launch and group AWS resources. In the following diagram, we can see that a region has multiple availability zones that can span <span class="No-Break">multiple VPCs:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B18493_03_007.jpg" alt="Figure 3.7 – Relationship between regions, VPCs, and availability zones"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Relationship between regions, VPCs, and availability zones</p>
			<p>A <strong class="bold">subnet</strong> is a range<a id="_idIndexMarker226"/> of IP addresses associated with the VPC you have defined. A <strong class="bold">route table</strong> is a set <a id="_idIndexMarker227"/>of rules that determine how traffic will flow within the VPC. Every subnet you create in a VPC is<a id="_idIndexMarker228"/> automatically associated with the main route table of the <a id="_idIndexMarker229"/>VPC. A <strong class="bold">VPC endpoint</strong> lets<a id="_idIndexMarker230"/> you connect resources from one VPC to another and to <span class="No-Break">other services.</span></p>
			<p>Next, we will discuss <strong class="bold">Classless Inter-Domain Routing</strong> (<strong class="bold">CIDR</strong>) blocks <span class="No-Break">and routing.</span></p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>CIDR blocks and routing</h2>
			<p>CIDR is a set of <a id="_idIndexMarker231"/>standards that is useful for assigning IP addresses to a device or group of devices. A CIDR block looks like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
10.0.0.0/16</pre>
			<p>This defines the starting IP, and the number of IP addresses in the block. Here, the 16 means that there are 2^(32-16) or 65,536 unique addresses. When you create a CIDR block, you have to make sure that all IP addresses are contiguous, the block size is a power of 2, and IPs range from <strong class="source-inline">0.0.0.0</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">256.256.256.256</strong></span><span class="No-Break">.</span></p>
			<p>For example, the CIDR block <strong class="source-inline">10.117.50.0/22</strong> has a total of 2^(32-22), or 1,024 addresses. Now, if we would like to partition this network into four more networks with 256 addresses each, we could use the following <span class="No-Break">CIDR blocks:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="4">
							<p><span class="No-Break"><strong class="source-inline">10.117.50.0.22</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">10.117.50.0/24</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">256 addresses</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">10.117.51.0/24</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">256 addresses</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">10.117.52.0/24</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">256 addresses</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">10.117.53.0/24</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">256 addresses</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Example of using CIDR blocks to create four partitions on the network</p>
			<p>Great, now<a id="_idIndexMarker232"/> that we know how CIDR blocks work, let us apply the same to VPCs <span class="No-Break">and subnets.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>Networking for HPC workloads</h2>
			<p>Referring<a id="_idIndexMarker233"/> back <a id="_idIndexMarker234"/>to <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em>, we have made a few modifications to show CIDR blocks that define two subnets within <strong class="bold">VPC1</strong> in the<a id="_idIndexMarker235"/> <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B18493_03_009.jpg" alt="Figure 3.9 – CIDR blocks used to define two subnets within VPC1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – CIDR blocks used to define two subnets within VPC1</p>
			<p>As we can see in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.9</em>, VPC 1 has a CIDR block of <strong class="source-inline">10.0.0.0/16</strong> (amounting to 65,536 addresses), and the two subnets (<strong class="source-inline">/24</strong>) have allocated 256 addresses each. As you have already noticed, there are several unallocated addresses in this VPC, which can be used in the future for more subnets. Routing decisions are defined using a route table, as shown in the figure. Here, each subnet is considered to be private, as traffic originating from within the VPC cannot leave the VPC. This also means that resources within this VPC cannot, by default, access the internet. One way to allow resources from within a subnet to access the internet is to add an internet gateway. For allowing only outbound internet connection from a private subnet, you can use an NAT gateway. This is often a requirement for security-sensitive workloads. This modification results in the following change <a id="_idIndexMarker236"/>to<a id="_idIndexMarker237"/> our <span class="No-Break">network diagram:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B18493_03_010.jpg" alt="Figure 3.10 – Adding an internet gateway to Subnet 1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Adding an internet gateway to Subnet 1</p>
			<p>The main route table is associated with all subnets in the VPC, but we can also define custom route tables for each subnet. This defines whether the subnet is private, public, or VPN only. Now, if we need resources in <strong class="bold">Subnet 2</strong> to only access VPN resources in a corporate <a id="_idIndexMarker238"/>network via a <strong class="bold">Virtual Private Gateway</strong> (<strong class="bold">VGW</strong>) in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.11</em>, we can create two route tables and associate them with <strong class="bold">Subnet 1</strong> and <strong class="bold">Subnet 2</strong>, as shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B18493_03_011.jpg" alt="Figure 3.11 – Adding a VGW to connect to on-premises resources"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Adding a VGW to connect to on-premises resources</p>
			<p>A<a id="_idIndexMarker239"/> feature<a id="_idIndexMarker240"/> called <strong class="bold">VPC peering</strong> can be <a id="_idIndexMarker241"/>used in order to privately access resources in another VPC on AWS. With VPC peering, you can use a private networking connection between two VPCs to enable communication between them. For more information, you can visit <a href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a>. As shown in the following diagram, VPC peering allows resources in <strong class="bold">VPC 1</strong> and <strong class="bold">VPC 2</strong> to communicate with each other as though they are in the <span class="No-Break">same network:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B18493_03_012.jpg" alt="Figure 3.12 – Adding VPC peering and VPC endpoints"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Adding VPC peering and VPC endpoints</p>
			<p>VPC peering can<a id="_idIndexMarker242"/> be <a id="_idIndexMarker243"/>done within VPCs in the same region or VPCs in different regions. A VPC endpoint allows resources from within a VPC (here, VPC 2) to access AWS services <a id="_idIndexMarker244"/>privately. Here, an <strong class="bold">EC2</strong> instance<a id="_idIndexMarker245"/> can make private API calls to services such as <strong class="bold">Amazon</strong> <strong class="bold">S3</strong>, <strong class="bold">Kinesis</strong>, or <strong class="bold">SageMaker</strong>. These<a id="_idIndexMarker246"/> are<a id="_idIndexMarker247"/> called <strong class="bold">interface-type endpoints</strong>. Gateway-type VPC endpoints are also available for Amazon S3 and DynamoDB, where you can further customize access control using policies (for example, bucket policies for <span class="No-Break">Amazon S3).</span></p>
			<p>Large enterprise customers with workloads that run on-premises, as well as on the cloud, may have a setup similar to <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B18493_03_013.jpg" alt="Figure 3.13 – Enterprise network architecture example"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Enterprise network architecture example</p>
			<p>Each corporate location<a id="_idIndexMarker248"/> may <a id="_idIndexMarker249"/>be connected to AWS<a id="_idIndexMarker250"/> by using <strong class="bold">Direct Connect</strong> (a service for creating dedicated network connections to AWS with a VPN backup. Private subnets may host single or clusters of EC2 instances for large, permanent workloads. The cluster of EC2 instances is placed in a multi-AZ autoscaling group so that the workload can recover from the unlikely event of an AZ failure, and a minimum number of EC2 instances <span class="No-Break">is maintained.</span></p>
			<p>For ephemeral workloads, managed services such as EKS, Glue, or SageMaker can be used. In the preceding diagram, a private <strong class="bold">EKS</strong> cluster is placed in VPC 2. Since internet access is disabled by default, all container images must be local to the VPC or copied onto an ECR repository; that is, you cannot use an image from Docker Hub. To publish logs and save checkpoints, VPC endpoints are required in VPC 2 to connect to the Amazon S3 and CloudWatch services. Data stores and databases are not discussed in this diagram but are important considerations in hybrid architectures. This is because some data cannot leave the corporate network but may be anonymized and replicated on <span class="No-Break">AWS temporarily.</span></p>
			<p>Typically, this temporary data on AWS is used for analytics purposes before getting deleted. Lastly, hybrid architectures may<a id="_idIndexMarker251"/> also involve <strong class="bold">AWS Outposts</strong>, which is a fully managed service<a id="_idIndexMarker252"/> that <a id="_idIndexMarker253"/>extends AWS<a id="_idIndexMarker254"/> services, such as EC2, <a id="_idIndexMarker255"/>ECS, <a id="_idIndexMarker256"/>EKS, S3, EMR, <strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) and<a id="_idIndexMarker257"/> so on, <span class="No-Break">to on-premises.</span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor064"/>Selecting the right compute for HPC workloads</h1>
			<p>Now that you have<a id="_idIndexMarker258"/> learned about the foundations of compute and network on AWS, we are ready to explore some typical architectural patterns for compute <span class="No-Break">on AWS.</span></p>
			<p>Selecting the right compute for HPC and ML applications involves considering the rest of the architecture you are designing, and therefore involves all aspects of the <span class="No-Break">Well-Architected </span><span class="No-Break"><a id="_idIndexMarker259"/></span><span class="No-Break">Framework:</span></p>
			<ul>
				<li><span class="No-Break">Operational excellence</span></li>
				<li><span class="No-Break">Security</span></li>
				<li><span class="No-Break">Reliability</span></li>
				<li><span class="No-Break">Performance efficiency</span></li>
				<li><span class="No-Break">Cost optimization</span></li>
			</ul>
			<p>We cover best practices across these pillars at the end of this section, but first, we will start with the most basic pattern of computing on AWS and add complexity as <span class="No-Break">we progress.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Pattern 1 – a standalone instance</h2>
			<p>Many HPC <a id="_idIndexMarker260"/>applications that are built for simulations, financial <a id="_idIndexMarker261"/>services, CFD, or genomics can run on a single EC2 instance as long as the right instance type is selected. We discussed many of these instance-type options in the <em class="italic">Introducing AWS compute ecosystem</em> section. As shown in the following<a id="_idIndexMarker262"/> diagram, a <strong class="bold">CloudFormation Template</strong> can be used to <a id="_idIndexMarker263"/>launch an <strong class="bold">EC2 Instance</strong> in a VPC, and <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) access <a id="_idIndexMarker264"/>can be provided to the user for installing and using software on <span class="No-Break">this instance:</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B18493_03_014.jpg" alt="Figure 3.14 – CloudFormation Template used to launch an EC2 Instance inside a VPC"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – CloudFormation Template used to launch an EC2 Instance inside a VPC</p>
			<p>Next, we will describe a pattern that uses <span class="No-Break">AWS ParallelCluster.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Pattern 2 – using AWS ParallelCluster</h2>
			<p><strong class="bold">AWS ParallelCluster</strong> can be<a id="_idIndexMarker265"/> used to provision a cluster with head and worker nodes for massive-scale parallel processing or HPC. ParallelCluster, once launched, will be similar to on-premises HPC clusters with the added benefits of security and scalability in the cloud. These clusters can be permanent or provisioned and de-provisioned on an as-needed basis. On AWS, a user can use the AWS ParallelCluster <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) to<a id="_idIndexMarker266"/> create a cluster of EC2 instances on the fly. AWS CloudFormation is used to launch the infrastructure, including required networking, storage, and AMI configurations. As the user (or multiple users) submit jobs through the job scheduler, more instances are provisioned and de-provisioned in the autoscaling group, as shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B18493_03_015.jpg" alt="Figure 3.15 – Using AWS ParallelCluster for distributed workloads on AWS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Using AWS ParallelCluster for distributed workloads on AWS</p>
			<p>Once the user is done with using the cluster for their HPC workloads, they can use the CLI or CloudFormation APIs to delete all resources created. As a modification to what is suggested in the following architecture, you can replace the <strong class="bold">head/master</strong> EC2 node with an Amazon <a id="_idIndexMarker267"/>SQS queue to get a queue-based<a id="_idIndexMarker268"/> architecture for typical <span class="No-Break">HPC workloads.</span></p>
			<p>Next, we will discuss how you can use <span class="No-Break">AWS Batch.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Pattern 3 – using AWS Batch</h2>
			<p>AWS Batch <a id="_idIndexMarker269"/>helps run HPC and big data-based applications that are based on unconnected input configurations or files without the need to manage infrastructure. To submit a job to AWS batch, you package your application as a container and use the CLI or supported APIs to define and submit a job. With AWS Batch, you can get started quickly by using default job configurations, a built-in job queue, and integration with workflow services such as AWS Step Functions <span class="No-Break">and Luigi.</span></p>
			<p>As you can see in the following screenshot, the user <a id="_idIndexMarker270"/>first defines a <strong class="bold">Docker image</strong> (much like the image we discussed in the section on containers) and then registers this image<a id="_idIndexMarker271"/> with <strong class="bold">Amazon ECR</strong>. Then, the user can create a job definition in <strong class="bold">AWS Batch</strong> and submit one or more jobs to the job queue. Input data can be pulled from <strong class="bold">Amazon S3</strong>, and output data can be written to a different location on <span class="No-Break">Amazon S3:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B18493_03_016.jpg" alt="Figure 3.16 – Using AWS Batch along with AWS EC2 instances for batch workloads"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Using AWS Batch along with AWS EC2 instances for batch workloads</p>
			<p>Next, we will<a id="_idIndexMarker272"/> discuss patterns that help with hybrid <a id="_idIndexMarker273"/>architectures <span class="No-Break">on AWS.</span></p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Pattern 4 – hybrid architecture</h2>
			<p>Customers who <a id="_idIndexMarker274"/>have already invested in large on-premises clusters, and who also want to make use of the on-demand, highly scalable, and secure AWS environment for their jobs, generally opt for a hybrid approach. In this approach, organizations decide to do one of <span class="No-Break">the following:</span></p>
			<ul>
				<li>Run a particular job type on AWS and keep the <span class="No-Break">rest on-premises</span></li>
				<li>Use AWS for <span class="No-Break">overflow/excess capacity</span></li>
				<li>Use on-premises as primary data storage for security reasons, or place only the scheduler or job monitors on-premises with all of the compute being done <span class="No-Break">on AWS</span></li>
				<li>Run small, test, or development jobs on-premises, but larger production jobs using high-performance or high-memory instances <span class="No-Break">on AWS</span></li>
			</ul>
			<p>On-premises data can be transferred to Amazon S3 using a software agent called DataSync (see <a href="https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html">https://docs.aws.amazon.com/datasync/latest/userguide/working-with-agents.html</a>). Clusters that use Lustre’s shared high-performance file system on-premises can make use of Amazon FSx for Lustre on AWS (for more information, see <a href="https://aws.amazon.com/fsx/lustre/">https://aws.amazon.com/fsx/lustre/</a>). The following diagram is a reference architecture for <span class="No-Break">hybrid workloads:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18493_03_017.jpg" alt="Figure 3.17 – Using FSx, S3, and AWS DataSync for hybrid architectures"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Using FSx, S3, and AWS DataSync for hybrid architectures</p>
			<p>Next, we will<a id="_idIndexMarker275"/> discuss patterns for container-based <span class="No-Break">distributed</span><span class="No-Break"><a id="_idIndexMarker276"/></span><span class="No-Break"> processing</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Pattern 5 – Container-based distributed processing</h2>
			<p>The following<a id="_idIndexMarker277"/> diagram is a reference architecture for container-based distributed processing workflows that are suited for HPC and other <span class="No-Break">related applications:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18493_03_018.jpg" alt="Figure 3.18 – EKS-based architecture for distributed computing"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – EKS-based architecture for distributed computing</p>
			<p>Admins can use command-line tools such as <em class="italic">eksctl</em> or CloudFormation to provision resources. Pods that are one or more containers can be run on managed EC2 nodes of your choice or via the AWS Fargate service. EMR on EKS can also be used to run open source, big data applications (for example, based on Spark) directly on EKS-managed nodes. In all of the preceding cases, containers that are provided by AWS can be used as a baseline, or completely custom containers that you build and push to ECR may be used. Applications<a id="_idIndexMarker278"/> running in EKS pods can access data from Amazon S3, Redshift, DynamoDB, or a host of other services and applications. To learn more about EKS, Fargate, or EMR on EKS, please take a look at the links provided in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Pattern 6 – serverless architecture</h2>
			<p>The following<a id="_idIndexMarker279"/> diagram<a id="_idIndexMarker280"/> is an example of serverless architecture that can be used for real-time, serverless processing, analytics, and <span class="No-Break">business intelligence:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18493_03_019.jpg" alt="Figure 3.19 – Architecture for real-time, serverless processing and business analytics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Architecture for real-time, serverless processing and business analytics</p>
			<p>First, Kinesis Data Streams captures data from one or more data producers. Next, <strong class="bold">Kinesis Data Analytics</strong> can <a id="_idIndexMarker281"/>be used to build real-time applications for transforming this incoming data using SQL, Java, Python, or Scala. Data can also be interactively processed using<a id="_idIndexMarker282"/> managed <strong class="bold">Apache Zeppelin</strong> notebooks (<a href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a>). In this case, a <strong class="bold">Lambda Function</strong> is being<a id="_idIndexMarker283"/> used to continuously post-process the output of the <strong class="bold">Kinesis Analytics</strong> application before dropping a filtered set of results into the serverless, NoSQL <a id="_idIndexMarker284"/><span class="No-Break">database </span><span class="No-Break"><strong class="bold">DynamoDB</strong></span><span class="No-Break">.</span></p>
			<p>Simultaneously, the <strong class="bold">Kinesis Firehose</strong> component<a id="_idIndexMarker285"/> is being used to save incoming data into S3, which is then <a id="_idIndexMarker286"/>processed by several <a id="_idIndexMarker287"/>other serverless components such as <strong class="bold">AWS Glu </strong>and <strong class="bold">AWS Lambda</strong>, and<a id="_idIndexMarker288"/> orchestrated using <strong class="bold">AWS Step Functions</strong>. With <a id="_idIndexMarker289"/>AWS Glue, you can run serverless <strong class="bold">Extract-Transform-Load</strong> (<strong class="bold">ETL</strong>) applications <a id="_idIndexMarker290"/>that are written in familiar languages such as SQL or Spark. You can then save the output of Glue transform jobs to data stores such as Amazon S3 or Amazon Redshift. ML applications that run on Amazon SageMaker can also make use of the output data from real-time <span class="No-Break">streaming analytics.</span></p>
			<p>Once the data is transformed, it is ready to be queried interactively using <strong class="bold">Amazon Athena</strong>. Amazon <a id="_idIndexMarker291"/>Athena makes it possible for you to query data that resides in Amazon S3 using standard SQL commands. Athena is also directly integrated with the Glue Data Catalog, which makes it much easier to work with these two services without the additional burden of writing ETL jobs or scripts to enable this connection. Athena is built on the open source<a id="_idIndexMarker292"/> library <strong class="bold">Presto</strong> (<a href="https://prestodb.io/">https://prestodb.io/</a>) and can be used to query a variety of standard formats such as CSV, JSON, Parquet, and Avro. With Athena Federated data sources, you can use a visualization tool <a id="_idIndexMarker293"/>such as <strong class="bold">Amazon QuickSight</strong> to run complex <span class="No-Break">SQL queries.</span></p>
			<p>Rather than using a dataset to visualize outputs, QuickSight, when configured correctly, can directly send these SQL queries to Athena. The results of the query can then be directly visualized interactively using multiple chart types and organized into a dashboard. These<a id="_idIndexMarker294"/> dashboards can then be shared with <a id="_idIndexMarker295"/>business analysts for <span class="No-Break">further research.</span></p>
			<p>In this section, we have covered various patterns around the topic of compute on AWS. Although this is not an exhaustive list of patterns, this should give you a basic idea of the components or services used and how these components are connected to each other to achieve different requirements. Next, we will describe some best practices related to HPC <span class="No-Break">on AWS.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Best practices for HPC workloads</h1>
			<p>The AWS Well-Architected <a id="_idIndexMarker296"/>Framework helps with the architecting of secure, cost-effective, resilient, and high-performing applications and workloads on the cloud. It is the go-to reference when building any application. Details about the AWS Well-Architected Framework can be obtained at <a href="https://aws.amazon.com/architecture/well-architected/">https://aws.amazon.com/architecture/well-architected/</a>. However, applications in certain domains and verticals require further scrutiny and have details that need to be handled differently from the generic guidance that the AWS Well-Architected Framework provides. Thus, we have many other documents called <em class="italic">lenses</em> that provide best practice guidance; some of these lenses that are relevant to our current discussion are listed <span class="No-Break">as follows:</span></p>
			<ul>
				<li><em class="italic">Data Analytics Lens</em> – well-architected lens for data analytics <span class="No-Break">workloads (</span><a href="https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&amp;trk=wp_card"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&amp;trk=wp_card</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Serverless Lens</em> – focusing on architecting serverless applications on <span class="No-Break">AWS (</span><a href="https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/welcome.html</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">ML Lens</em> – for ML workloads on <span class="No-Break">AWS (</span><a href="https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&amp;trk=wp_card"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html?did=wp_card&amp;trk=wp_card</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">HPC Lens</em> – focusing on HPC workloads on <span class="No-Break">AWS (</span><a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&amp;trk=wp_card"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html?did=wp_card&amp;trk=wp_card</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>While it is out of the<a id="_idIndexMarker297"/> scope of this book to go over best practices from the generic AWS Well-Architected Framework, as well as these individual lenses, we will list some common, important design considerations that are relevant to our current topic of HPC <span class="No-Break">and ML:</span></p>
			<ul>
				<li>Both HPC and ML applications evolve over time. Organizations that freeze an architecture for several years in advance tend to be the ones that resist change and are later impacted by even larger costs to accommodate new requirements. In general, it is best practice to avoid static architectures, as the original requirements may evolve quickly. When there is a need to run more training jobs, or more HPC simulations, the architecture must allow scaling out and increase overall performance, but also return back to a steady, low-cost state when the demand <span class="No-Break">is lower.</span></li>
			</ul>
			<p>On AWS, compute clusters can be right-sized at any given point in time, and the use of managed services can help with provisioning resources on the fly. For example, Amazon SageMaker allows users to provision various instance types for training without the undifferentiated heavy lifting of maintaining clusters or infrastructure. Customers only need to choose the framework of interest, point to training data in Amazon S3, and use the APIs to start, monitor, and stop training jobs. Customers only pay for what they use and don’t pay for any <span class="No-Break">idle time.</span></p>
			<ul>
				<li>Architecting to encourage and enable collaboration can make a significant difference to the productivity of the team running HPC and ML workloads on AWS. With teams that are becoming remote and global, the importance of effective collaboration <a id="_idIndexMarker298"/>cannot be understated. To improve collaboration, it is important to do <span class="No-Break">the following:</span><ul><li>Track experiments using an experiment <span class="No-Break">tracking tool.</span></li><li>Enable sharing of resources such as configuration files, CloudFormation templates, pipeline definitions, code, notebooks, <span class="No-Break">and data.</span></li><li>Enable automation and use tools for continuous integration, continuous delivery, continuous monitoring, continuous training, and <span class="No-Break">continuous improvement.</span></li><li>Make sure that work is reproducible – this means that the inputs, environmental configuration, and packages can be easily reused and the outputs of a batch process can be verified. This helps to track changes, with audits, and to maintain <span class="No-Break">high standards.</span></li></ul></li>
				<li>Use ephemeral resources from managed services when possible. Again, this is applicable to both HPC and ML. When considering hybrid architectures or when migrating an on-premises workload to AWS, it is no longer necessary to completely replicate the workload on AWS. For example, running Spark-based workloads can be done on AWS Glue without the need to provision an entire EMR cluster. Similarly, you can run ML training or inference without handling the underlying clusters using Amazon <span class="No-Break">SageMaker APIs.</span></li>
				<li>Consider both performance and cost when right-sizing your resources. For workloads that are not time-sensitive, using Spot instance on AWS is the simplest cost optimization strategy to follow. For HPC applications, running workloads on EC2 spot instances or using spot fleets for containerized workloads on EKS or Fargate can provide a discount of up to 90% over on-demand instances of the <span class="No-Break">same type.</span></li>
			</ul>
			<p>On SageMaker, using Spot instances is very simple – you just need to pass an argument to supported training APIs. On the other hand, for high-performance workloads, it is important to prioritize on-demand instances over spot instances so that the results of simulations or ML training jobs can be returned and analyzed in a timely manner. When choosing services or applications to use for your HPC or ML workloads, prefer pay-as-you-go pricing over licensing and <span class="No-Break">upfront costs.</span></p>
			<ul>
				<li>Consider cost<a id="_idIndexMarker299"/> optimization and performance for the entire pipeline. It is typical for both HPC and ML applications to be designed over a pipeline – for example, data transfer, pre-processing, training or simulation, post-processing, and visualization. It is possible that some steps require less compute than others. Also, making a decision upfront about data formats or locations may force downstream steps to be more expensive in terms of time, processing resources, <span class="No-Break">or cost.</span></li>
				<li>Focus on making small and frequent changes, building modular components, and testing in an automated fashion. Reducing the level of manual intervention and architecting the workload so that it does not require any downtown for maintenance is a <span class="No-Break">best practice.</span></li>
				<li>For both HPC and ML, use software packages and tools that provide good documentation and support. This choice needs to be made carefully upfront, since several architectural, design, and team decisions may need to change based on this. For example, when choosing an ML framework such as PyTorch, it is important to be familiar with services on AWS that support this framework and hire a team that is well-versed in this particular framework to <span class="No-Break">ensure success.</span></li>
			</ul>
			<p>Similarly in HPC, the choice of software that does molecular dynamics simulations will decide the scale of simulations that can be done, which services are compatible with the package on AWS, and which team members are trained and ready to make use of this software set up <span class="No-Break">on AWS.</span></p>
			<ul>
				<li>Prioritize security and establish security best practices before beginning to develop multiple workloads or applications. These best practice areas under security are discussed in great detail in the AWS Well-Architected Framework and several of the<a id="_idIndexMarker300"/> lenses. Here, we outline the major sub-topics <span class="No-Break">for completeness:</span><ul><li><strong class="bold">Identity and Access </strong><span class="No-Break"><strong class="bold">Management</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IAM</strong></span><span class="No-Break">)</span></li><li><span class="No-Break">Detective</span><span class="No-Break"><a id="_idIndexMarker301"/></span><span class="No-Break"> controls</span></li><li><span class="No-Break">Preventive controls</span></li><li><span class="No-Break">Infrastructure protection</span></li><li><span class="No-Break">Data protection</span></li><li><span class="No-Break">Incident response</span></li></ul></li>
				<li>It is normal to expect a complex architecture system to fail, but the best practice is to respond quickly and recover from these failures. Checkpointing is a common feature that is built into HPC and ML applications. A common idea is to checkpoint data or progress (or both) to a remote S3 location where a simulation or a training job can pick up after a failure. Checkpointing becomes even more important when using spot instances. When managing infrastructure on your own, you have the flexibility to deploy the application to multiple availability zones when extremely low latency requirements do not need to be met. Managed services take care of maintaining and updating instances and containers that run on <span class="No-Break">these instances.</span></li>
				<li>Make sure the cluster is dynamic, can be used by multiple users simultaneously, and is designed to work over large amounts of data. In order to design the cluster successfully, use cloud native technologies to test applications and packages over a meaningful use case and not a toy problem. With the cloud, you have the ability to spin up and spin down ephemeral clusters to test out your use case at a<a id="_idIndexMarker302"/> low cost, while also making sure that a production-sized workload will work smoothly and <span class="No-Break">as expected.</span></li>
			</ul>
			<p>In this section, we have listed some best practices for HPC workloads <span class="No-Break">on AWS.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Summary</h1>
			<p>In this chapter, we first described the AWS Compute ecosystem, including the various types of EC2 instances, as well as container-based services (Fargate, ECS, and EKS), and serverless compute options (AWS Lambda). We then introduced networking concepts on AWS and applied them to typical workloads using a visual walk-through. To help guide you through selecting the right compute for HPC workloads, we described several typical patterns including standalone, self-managed instances, AWS ParallelCluster, AWS Batch, hybrid architectures, container-based architectures, and completely serverless architectures for HPC. Lastly, we discussed various best practices that may further help you right-size your instances and clusters and apply the Well-Architected Framework to <span class="No-Break">your workloads.</span></p>
			<p>In the next chapter, we will outline the various storage services that can be used on AWS for HPC and <span class="No-Break">ML workloads.</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>References</h1>
			<p>For additional information on the topics covered in this chapter, please navigate to the <span class="No-Break">following pages:</span></p>
			<ul>
				<li><a href="https://aws.amazon.com/products/compute/"><span class="No-Break">https://aws.amazon.com/products/compute/</span></a></li>
				<li><a href="https://aws.amazon.com/what-is/compute/"><span class="No-Break">https://aws.amazon.com/what-is/compute/</span></a></li>
				<li><a href="https://aws.amazon.com/hpc/?pg=ln&amp;sec=uc"><span class="No-Break">https://aws.amazon.com/hpc/?pg=ln&amp;sec=uc</span></a></li>
				<li><a href="https://www.amazonaws.cn/en/ec2/instance-types/"><span class="No-Break">https://www.amazonaws.cn/en/ec2/instance-types/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-explorer/"><span class="No-Break">https://aws.amazon.com/ec2/instance-explorer/</span></a></li>
				<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/hpc6/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/hpc6/</span></a></li>
				<li><a href="https://aws.amazon.com/hpc/parallelcluster/"><span class="No-Break">https://aws.amazon.com/hpc/parallelcluster/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/c5/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/c5/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/c6g/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/c6g/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/c6i/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/c6i/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/m5/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/m5/</span></a></li>
				<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html#gpu-instances"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html#gpu-instances</span></a></li>
				<li><a href="https://github.com/aws/aws-neuron-sdk"><span class="No-Break">https://github.com/aws/aws-neuron-sdk</span></a></li>
				<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/a1/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/a1/</span></a></li>
				<li><a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/"><span class="No-Break">https://developer.nvidia.com/blog/even-easier-introduction-cuda/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/p4/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/p4/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/p4/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/p4/</span></a></li>
				<li><a href="https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/ansys-fluent/"><span class="No-Break">https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/ansys-fluent/</span></a></li>
				<li><a href="https://www.nvidia.com/en-in/data-center/a100/"><span class="No-Break">https://www.nvidia.com/en-in/data-center/a100/</span></a></li>
				<li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf"><span class="No-Break">https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf</span></a></li>
				<li><a href="https://pytorch.org/docs/stable/notes/cuda.html"><span class="No-Break">https://pytorch.org/docs/stable/notes/cuda.html</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/f1/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/f1/</span></a></li>
				<li><a href="https://github.com/aws/aws-fpga"><span class="No-Break">https://github.com/aws/aws-fpga</span></a></li>
				<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/i3/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/i3/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/r6g/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/r6g/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/high-memory/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/high-memory/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/x1/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/x1/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/x1e/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/x1e/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/x2g/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/x2g/</span></a></li>
				<li><a href="https://www.hpcworkshops.com/"><span class="No-Break">https://www.hpcworkshops.com/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/instance-types/z1d/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/z1d/</span></a></li>
				<li><a href="https://aws.amazon.com/containers/"><span class="No-Break">https://aws.amazon.com/containers/</span></a></li>
				<li><a href="https://aws.amazon.com/ec2/?c=cn&amp;sec=srv"><span class="No-Break">https://aws.amazon.com/ec2/?c=cn&amp;sec=srv</span></a></li>
				<li><a href="https://aws.amazon.com/containers/?nc1=f_cc"><span class="No-Break">https://aws.amazon.com/containers/?nc1=f_cc</span></a></li>
				<li><a href="https://aws.amazon.com/fargate/?c=cn&amp;sec=srv"><span class="No-Break">https://aws.amazon.com/fargate/?c=cn&amp;sec=srv</span></a></li>
				<li><a href="https://aws.amazon.com/serverless/?nc2=h_ql_prod_serv"><span class="No-Break">https://aws.amazon.com/serverless/?nc2=h_ql_prod_serv</span></a></li>
				<li><a href="https://aws.amazon.com/eks/?c=cn&amp;sec=srv"><span class="No-Break">https://aws.amazon.com/eks/?c=cn&amp;sec=srv</span></a></li>
				<li><a href="https://aws.amazon.com/ecs/?c=cn&amp;sec=srv"><span class="No-Break">https://aws.amazon.com/ecs/?c=cn&amp;sec=srv</span></a></li>
				<li><a href="https://aws.amazon.com/lambda/"><span class="No-Break">https://aws.amazon.com/lambda/</span></a></li>
				<li><a href="https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/"><span class="No-Break">https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/</span></a></li>
				<li><a href="https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc"><span class="No-Break">https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc</span></a></li>
				<li><a href="https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html"><span class="No-Break">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html</span></a></li>
				<li><a href="https://aws.amazon.com/kinesis/data-analytics/"><span class="No-Break">https://aws.amazon.com/kinesis/data-analytics/</span></a></li>
				<li><a href="https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc"><span class="No-Break">https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc</span></a></li>
				<li><a href="https://aws.amazon.com/emr/features/eks/"><span class="No-Break">https://aws.amazon.com/emr/features/eks/</span></a></li>
				<li><a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/pod-templates.html"><span class="No-Break">https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/pod-templates.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html"><span class="No-Break">https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html</span></a></li>
				<li><a href="https://aws.amazon.com/blogs/big-data/orchestrate-an-amazon-emr-on-amazon-eks-spark-job-with-aws-step-functions/"><span class="No-Break">https://aws.amazon.com/blogs/big-data/orchestrate-an-amazon-emr-on-amazon-eks-spark-job-with-aws-step-functions/</span></a></li>
				<li><a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/"><span class="No-Break">https://aws.amazon.com/about-aws/global-infrastructure/regions_az/</span></a></li>
				<li><a href="https://d1.awsstatic.com/whitepapers/computational-fluid-dynamics-on-aws.pdf?cmptd_hpc3"><span class="No-Break">https://d1.awsstatic.com/whitepapers/computational-fluid-dynamics-on-aws.pdf?cmptd_hpc3</span></a></li>
				<li><a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/general-design-principles.html"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/general-design-principles.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-design-principles.html"><span class="No-Break">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-design-principles.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html"><span class="No-Break">https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html"><span class="No-Break">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html"><span class="No-Break">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html</span></a></li>
				<li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/security.html"><span class="No-Break">https://docs.aws.amazon.com/vpc/latest/userguide/security.html</span></a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer052" class="IMG---Figure">
			</div>
		</div>
	</body></html>