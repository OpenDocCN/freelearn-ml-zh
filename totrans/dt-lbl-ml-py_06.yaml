- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Image Data Using Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to label image data using data augmentation
    for semi-supervised machine learning. We will use the CIFAR-10 dataset and the
    MNIST dataset of handwritten digits to generate labels using data augmentation.
    From there we will build an image classification machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation plays a crucial role in data labeling by enhancing the diversity,
    size, and quality of the dataset. Data augmentation techniques generate additional
    samples by applying transformations to existing data. This effectively increases
    the size of the dataset, providing more examples for training and improving the
    model’s ability to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to prepare training data with image data augmentation and implement support
    vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement convolutional neural networks with augmented image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will use the CIFAR-10 dataset, which is a publicly available
    image dataset consisting of 60,000 32x32 color images in 10 classes ([http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)),
    along with the famous MNIST handwritten digits dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training support vector machines with augmented image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**) are widely used in machine learning
    to solve classification problems. SVMs are known for their high accuracy and ability
    to handle complex datasets. One of the challenges in training SVMs is the availability
    of large and diverse datasets. In this section, we will discuss the importance
    of data augmentation in training SVMs for image classification problems. We will
    also provide Python code examples for each technique.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – SVM separates class A and class B with largest margin
  prefs: []
  type: TYPE_NORMAL
- en: SVMs are a type of supervised learning algorithm used for classification and
    regression analysis. SVMs can be used for outlier detection. SVMs were originally
    designed for classification tasks, but can also be adapted for anomaly or outlier
    detection as well.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of SVMs is to find the hyperplane that maximizes the margin between
    two classes of data. The hyperplane is defined as the decision boundary that separates
    the data points of two classes. The margin is the distance between the hyperplane
    and the nearest data point of each class.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs use something called the *kernel trick*. Let’s understand what this is
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say you have data points on a sheet of paper, and you want to separate
    them into two groups. Imagine you have a magic wand (i.e., the kernel trick) that
    allows you to lift the points off the paper into the air. In the air, you can
    easily draw a line or a curve to separate the floating points.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when you’re satisfied with the separation in the air, you use the magic
    wand again to bring everything back down to the paper. Miraculously, the separation
    you drew in the air translates to a more complex decision boundary on the paper
    that effectively separates your original data points.
  prefs: []
  type: TYPE_NORMAL
- en: In the SVM world, this “magic wand” is the kernel trick. It allows SVMs to implicitly
    work in a higher-dimensional space, making it possible to find more intricate
    decision boundaries that weren’t achievable in the original space. The key is
    that you don’t have to explicitly compute the coordinates of the higher-dimensional
    space; the kernel trick does this for you.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the kernel trick lifts your data into a higher-dimensional space,
    where SVMs can find more sophisticated ways to separate different classes. It’s
    a powerful tool for handling complex data scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs leverage the kernel trick to transform the input data into a higher-dimensional
    space, where a linear decision boundary can be found. The kernel function plays
    a crucial role in this process, mapping the input data into a feature space where
    the relationships between variables may be more easily separable.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used kernel functions are the linear kernel, which represents
    a linear decision boundary, the polynomial kernel, which introduces non-linearity
    with higher-order polynomial features, and the **radial basis function** (**RBF**)
    kernel, which allows for a more flexible and non-linear decision boundary. The
    choice of the kernel function and its parameters significantly influences the
    SVM’s ability to model complex relationships in the data.
  prefs: []
  type: TYPE_NORMAL
- en: As we now have a basic idea about SVMs, let us next understand data augmentation,
    image data augmentation, and the various techniques used for this.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data augmentation is the process of creating new data points from the existing
    data points by applying various transformations such as rotation, translation,
    and scaling. Data augmentation is used to increase the size of the training dataset
    and improve the generalizability and accuracy of the model by helping the model
    to learn more features and patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Image data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image data augmentation is a technique of augmenting the image dataset to improve
    the accuracy of the model. The following is a selection of the techniques that
    can be used for image data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Image rotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image rotation is a technique where an image is rotated by a certain angle.
    This technique is used to increase the size of the training dataset and improve
    the model’s ability to recognize objects from different angles. The Python code
    for image rotation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we load the image from the image path and rotate it by
    a given number of degrees. This creates a new dataset for the same image from
    different angles and improves model training.
  prefs: []
  type: TYPE_NORMAL
- en: Image translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image translation is a technique where an image is shifted horizontally or
    vertically by a certain amount of pixels. This technique is used to increase the
    size of the training dataset and improve the model’s ability to recognize objects
    in different positions. The Python code for image translation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define a Python function that shifts the image by
    a certain amount of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Image scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image scaling is an augmentation technique where an image is scaled up or down
    by a certain factor. This technique is used to increase the size of the training
    dataset and improve the model’s ability to recognize objects at different scales.
    The Python code for image scaling is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we change the image size by multiplying the image by
    a scale factor in a Python function. Next, let’s see how to implement an SVM with
    data augmentation using the CIFAR-10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an SVM with data augmentation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide a step-by-step guide to implement an SVM with
    data augmentation in Python using the CIFAR-10 dataset. We will start by introducing
    the CIFAR-10 dataset and then move on to loading the dataset in Python. We will
    then preprocess the data for SVM training and implement an SVM with the default
    hyperparameters and dataset. Next, we train and evaluate the performance of the
    SVM with an augmented dataset, showing that the performance of the SVM improves
    on the augmented dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the CIFAR-10 dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CIFAR-10 dataset is a commonly used image classification dataset that consists
    of 60,000 32x32 color images in 10 classes. The classes are: airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is divided into
    50,000 training images and 10,000 testing images. The dataset is preprocessed
    in a way that the training set and test set have an equal number of images from
    each class.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the CIFAR-10 dataset in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load the CIFAR-10 dataset in Python, we will use the `cifar10` module from
    the Keras library. If you don’t have Keras installed, you can install it using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have installed Keras, you can load the CIFAR-10 dataset using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cifar10.load_data()` function returns two tuples: `(x_train, y_train)`
    and `(x_test, y_test)`. The `x_train` and `x_test` tuples contain the input images,
    while the `y_train` and `y_test` tuples contain the corresponding class labels
    for the input images.'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data for SVM training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will first convert the input images from 3D matrices to
    2D matrices. We will also normalize the pixel values of the input images to be
    between 0 and 1\. Finally, we will reshape the input images and convert the class
    labels to one-hot encoded vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `reshape()` function is used to reshape the input images from 3D matrices
    to 2D matrices. The `-1` argument tells the function to infer the number of columns
    based on the number of rows and the size of each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The pixel values of the input images are normalized to be between 0 and 1 by
    dividing them by 255, which is the maximum pixel value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `to_categorical()` function is used to convert the class labels to one-hot
    encoded vectors. The `num_classes` variable is set to `10`, which is the number
    of classes in the CIFAR-10 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Implementing an SVM with the default hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperparameters in SVMs are parameters that are not learned from the data but
    are set prior to the training process. They control the behavior of the SVM model
    and can significantly impact its performance. Here are some important hyperparameters
    in SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel**: The kernel function determines the type of decision boundary used
    by the SVM. Common kernel functions include the linear, polynomial, **radial basis
    function** (**RBF**), and sigmoid function. The choice of kernel depends on the
    data and problem at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization parameter (C)**: Regularization is a technique used to prevent
    overfitting or underfitting of the model. Regularization methods help to control
    the complexity of the model and improve its generalization on unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a binary classification problem, the decision boundary is a hyperplane that
    separates data into two classes. The margin is the distance between this hyperplane
    and the nearest data point from either class. The “width” of the margin is the
    actual numerical value of the distance or gap between the decision boundary and
    the nearest data point.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A wider margin implies a larger separation between classes, providing more room
    for potential misclassifications without affecting the decision boundary. The
    regularization parameter, often denoted as C, controls the trade-off between achieving
    a low training error rate and maintaining a wide margin. A smaller C value allows
    for more misclassifications but results in a larger margin, while a larger C value
    tries to minimize misclassifications at the cost of a narrower margin.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Gamma (for RBF kernels)**: The gamma parameter influences the shape of the
    decision boundary for SVMs with the RBF kernel. It determines the reach of each
    training sample and affects the smoothness of the decision boundary. Higher gamma
    values tend to result in more complex decision boundaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Degree (for polynomial kernels)**: The degree parameter specifies the degree
    of the polynomial kernel function. It determines the nonlinearity of the decision
    boundary. Higher degree values allow for more complex decision boundaries but
    may increase the risk of overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These hyperparameters need to be carefully tuned to achieve the best performance
    of the SVM model. Grid search, random search, or other optimization techniques
    can be employed to explore different combinations of hyperparameter values and
    select the optimal set.
  prefs: []
  type: TYPE_NORMAL
- en: To implement an SVM with the default hyperparameters, we will use the `svm.SVC`
    class from the scikit-learn library. We will first create an instance of the `SVC`
    class and then fit the training data to the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance of the `SVC` class is created using `svm.SVC()`. By not specifying
    any hyperparameters, it uses the default values for the kernel, regularization
    parameter (C), and other relevant parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit()` function is used to fit the training data to the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating SVM on the original dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We evaluate the performance of the original dataset to compare the performance
    with the augmented dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the performance of SVM on the original dataset, we will use the
    `predict()` function to predict the class labels of the test data and then use
    the `accuracy_score()` function from the scikit-learn library to calculate the
    accuracy of the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `predict()` function is used to predict the class labels of the test data.
    The `accuracy_score()` function is used to calculate the accuracy of the classifier
    by comparing the predicted class labels to the actual class labels.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the SVM model on the test dataset is around `47.97%`, which
    is not very good. This indicates that the SVM model is not able to learn all the
    important features and patterns in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an SVM with an augmented dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement SVM with data augmentation, we will use the `ImageDataGenerator`
    class from the Keras library to generate new training data. We will first create
    an instance of the `ImageDataGenerator` class and then use the `flow()` function
    to generate new batches of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `ImageDataGenerator()` function creates an instance of the `ImageDataGenerator`
    class. The `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`,
    `zoom_range`, and `horizontal_flip` arguments are used to specify the types of
    data augmentation to be applied to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: The `flow()` function is used to generate new batches of training data from
    the original training data and the `ImageDataGenerator` object.
  prefs: []
  type: TYPE_NORMAL
- en: Training the SVM on augmented data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train SVM on augmented data, we will use the `partial_fit()` function of
    the `SVC` class to train the classifier on each batch of training data generated
    by the `ImageDataGenerator` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `classes` argument is used to specify the unique classes in the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the SVM’s performance on the augmented dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the performance of the SVM on the augmented dataset, we will again
    use the `predict()` function to predict the class labels of the test data and
    then use the `accuracy_score()` function to calculate the accuracy of the classifier
    by comparing the predicted class labels to the actual class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of the SVM model on the augmented test dataset is around `54.75%`,
    which is better than the previous accuracy. This indicates that the SVM model
    is able to learn more important features and patterns in the augmented dataset,
    and is able to generalize better to new data.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section, we have discussed the importance of data augmentation
    in training SVMs for image classification. We have used the CIFAR-10 dataset to
    illustrate the impact of data augmentation on the performance of the SVM model.
    We have also provided Python code examples for loading the CIFAR-10 dataset, training
    an SVM model on the original dataset, and training an SVM model on the augmented
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The results show that data augmentation can improve the performance of SVM models
    on image classification tasks. By applying random rotations, translations, and
    scaling, we can generate new images that the SVM model can use to learn more features
    and patterns. This enables the SVM model to generalize better to new data and
    achieve better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to implement the SVM with data augmentation
    using the MNIST handwritten digits dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification using the SVM with data augmentation on the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us see how we can apply data augmentation for image classification using
    an SVM with the MNIST dataset. All the steps are similar to the previous example
    with the CIFAR-10 dataset, except the dataset itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As stated, this code is similar to the previous example, except that we are
    now using the MNIST dataset and the images are grayscale and of size 28x28\. We
    have also modified the input shape of the SVM model and the image data generator
    to accommodate the new image size and color channel.
  prefs: []
  type: TYPE_NORMAL
- en: The results show that data augmentation can also improve the performance of
    SVM models on the MNIST dataset. By applying random rotations, translations, and
    scaling, we can generate new images that the SVM model can use to learn more features
    and patterns. This enables the SVM model to generalize better to new data and
    achieve better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we have used grid search to find the optimal hyperparameters for
    the SVM model. This is important because the performance of SVM models is highly
    dependent on the choice of hyperparameters. By tuning the hyperparameters using
    grid search, we can improve the accuracy of the SVM model even further.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this example code demonstrates the effectiveness of data augmentation
    in improving the performance of SVM models on image classification tasks. It also
    highlights the importance of hyperparameter tuning using grid search to achieve
    the best possible accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, data augmentation is a powerful technique for improving the performance
    of machine learning models on image classification tasks. By generating new images
    that the model can use to learn more features and patterns, we can improve the
    generalization ability of the model and achieve better accuracy. SVM models are
    particularly well suited for image classification tasks and can benefit greatly
    from data augmentation. With the help of Python libraries such as scikit-learn
    and TensorFlow, we can easily implement SVM models with data augmentation and
    achieve state-of-the-art performance on image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us see how to implement convolutional neural networks with augmented
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks using augmented image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) have revolutionized the field
    of computer vision by demonstrating exceptional performance in various image-related
    tasks such as object detection, image classification, and segmentation. However,
    the availability of large, annotated datasets for training CNNs is often a challenge.
    Fortunately, one effective approach to overcome this limitation is through the
    use of **image data** **augmentation** techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start from scratch and explain what CNNs are and how they work. Imagine
    you have a picture, say a photo of a cat, and you want to teach a computer how
    to recognize that it’s a cat. CNNs are like a special type of computer program
    that helps computers understand and recognize things in images, just like how
    you recognize objects in photos.
  prefs: []
  type: TYPE_NORMAL
- en: An image is made up of tiny dots called pixels. Each pixel has a color, and
    when you put them all together, you get an image. The more pixels you have, the
    more detailed the image is. When you look at a picture, your brain doesn’t try
    to understand it all at once. Instead, it focuses on small parts, like the shape
    of an ear or the color of an eye. This is how we recognize things. We break the
    big picture into small pieces and understand them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs work a bit like the human brain, breaking images down into small parts.
    These small parts are called “features” or “filters.” Imagine these filters as
    tiny windows that move across the picture. These windows look at a small part
    of the image at a time and learn what’s important in it.
  prefs: []
  type: TYPE_NORMAL
- en: How CNNs work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us understand how CNNs work for the desired output:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution**: This is the first step. It’s like moving the small window
    (filter) over the picture. The filter checks the colors and shapes in the area
    it’s looking at and learns what’s important.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pooling**: After the different parts of the image have been looked at, the
    CNN doesn’t need all the details. Pooling is like taking a summary of what it’s
    seen. It simplifies things, but we don’t lose the important parts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fully connected layers**: After looking at the many small parts and summarizing
    them, everything is connected together next. It’s like putting the pieces of a
    puzzle together to see the whole picture. This helps the CNN understand the entire
    image and make a final decision on what is depicted in it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the convolutional layers have processed the image by extracting various
    features and patterns, the fully connected layers play a crucial role in bringing
    all the information together to make a comprehensive decision about the content
    of the image. This process is akin to assembling the pieces of a puzzle, where
    each piece corresponds to a specific feature detected by the convolutional layers.
    By connecting these pieces, the network gains a holistic understanding of the
    image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, as powerful as fully connected layers are, there is a risk of overfitting,
    a situation where the model becomes too specialized in the training data and performs
    poorly on new, unseen data. To mitigate this, regularization techniques are often
    employed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regularization in fully connected layers**: Regularization is a set of techniques
    used to prevent overfitting and enhance the generalization capabilities of a model.
    In the context of fully connected layers, regularization methods are applied to
    control the complexity of the model and avoid relying too heavily on specific
    features present in the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training a CNN**: To teach a CNN to recognize cats, you’d show it lots of
    cat pictures. It looks at them, learns the important features, and gets better
    over time at recognizing them. It also needs to see pictures of things that are
    not cats, so it can tell the difference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Making predictions**: Once a CNN is trained, you can show it a new picture,
    and it will try to find the important features just like it learned. If it finds
    enough cat-like features, it will say, “Hey, that’s a cat!”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, in simple terms, a CNN is like a computer program that learns to recognize
    things in pictures by looking at small parts of the image, finding important features,
    and making decisions based on those features.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen, a CNN’s architecture constitutes convolution, pooling, and fully
    connected layers. The architecture specifies how the model is structured, including
    the number of layers, the size of filters, and the connections between neurons.
    The architecture guides how the learned weights and features are used to process
    images and make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the final model is, in essence, a combination of the architecture, the
    learned weights, and the learned features. Let’s break down a couple of these
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learned weights**: These are the parameters that the CNN has learned during
    the training process. The model adjusts these weights to make accurate predictions.
    These weights are essentially the “knowledge” the model gains during training.
    They represent how important certain features are for making decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learned features**: Features, in the context of a CNN, are visual patterns
    and characteristics of images. They are representations of important information
    within the image. These features are not directly visible to us but are learned
    by the network through the layers of convolution and pooling. Features are abstract
    representations of the image that help the model recognize patterns and objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, these learned weights and features are stored in the model’s parameters.
    When you save a trained CNN model, you are saving these parameters, which can
    be used to make predictions on new, unseen images. The model takes an image as
    input, processes it through its layers, and uses the learned weights and features
    to make predictions, such as classifying objects in the image or detecting specific
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: We will now delve into the powerful combination of CNNs and image data augmentation.
    By artificially augmenting the data, CNNs can be exposed to a broader range of
    variations during training to help them generalize better to unseen images.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the benefits and considerations of using image data augmentation are
    reducing overfitting, enhancing model robustness, and improving generalization
    performance. Whether you are a beginner or an experienced practitioner, this section
    serves as a comprehensive guide to understanding and implementing image data augmentation
    in the context of CNNs, assisting you in taking your computer vision projects
    to new heights.
  prefs: []
  type: TYPE_NORMAL
- en: Practical example of a CNN using data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us see how to implement image data augmentation on a CNN. To do so, you
    can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Start by importing the necessary libraries, including Keras and
    NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`ImageDataGenerator` object and specify the desired data augmentation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `ImageDataGenerator` object will generate batches of augmented data using
    the specified data augmentation techniques. In this example, we are using rotation,
    width and height shifts, and horizontal flipping.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: Load the original dataset and split it into training and validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the `flow_from_directory()` function to load the original
    dataset from the specified directory. We also specify the target size of the images,
    the batch size, and the class mode (categorical in this case). We split the data
    into training and validation sets using the `subset` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the provided code snippet, the `flow_from_directory` function is used to
    generate a data generator to load images from a directory. Let’s break down the
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''/path/to/dataset''`: This is the path to the directory containing the dataset.
    The function will look for subdirectories inside this directory, where each subdirectory
    represents a different class or category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_size=(224, 224)`: `target_size` is the size to which all images will
    be resized during loading. In this case, each image will be resized as a square
    with dimensions of 224x224 pixels. Standardizing the image size is important for
    consistency and compatibility with neural network models, especially when using
    pre-trained models that expect a specific input size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size=32`: `batch_size` determines the number of images loaded and processed
    in each iteration during training or validation. A larger batch size can lead
    to faster training but may require more memory. Smaller batch sizes are often
    used when memory is limited or for fine-tuning models. It also affects the gradient
    update during training, impacting the stability and convergence of the training
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_mode=''categorical''`: `class_mode` specifies how the target classes
    are represented. In this case, it is set to `categorical`, indicating that the
    labels are one-hot encoded (a binary matrix representation of class membership).
    Other possible values include `binary` for binary classification, `sparse` for
    integer-encoded class labels, and `None` for no labels (used for test datasets).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset=''validation''`: Subset is used to specify whether the generator is
    for the training set or the validation set. In this case, it is set to `validation`,
    indicating that the generator is for the validation set. When using subset, make
    sure the dataset directory contains subdirectories like `train` and `validation`
    to facilitate the split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, these parameters help configure the data generator to load and preprocess
    images from a directory. The choices made for target size, batch size, and class
    mode are often determined by the requirements of the machine learning model being
    used, the available computing resources, and the characteristics of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: Create a CNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are creating a simple CNN model with four convolutional layers and
    one fully connected layer. We are using ReLU activation for the convolutional
    layers and softmax activation for the output layer. We also compile the model
    with the categorical cross-entropy loss function, the Adam optimizer, and the
    accuracy metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code snippet, a CNN model is being created using the Keras
    library. Let’s break down the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`activation=''relu''` is used for the convolutional and dense layers. ReLU
    is an activation function that introduces non-linearity to the model. It outputs
    the input directly if it is positive; otherwise, it outputs zero. ReLU is preferred
    for CNNs because it helps the model learn complex patterns and relationships in
    data. It is computationally efficient and mitigates the vanishing gradient problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The effect of ReLU**: ReLU introduces non-linearity, enabling the model to
    learn complex features and relationships in the data. It helps address the vanishing
    gradient problem, promoting more efficient training by allowing the model to propagate
    gradients during backpropagation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`activation=''softmax''` is used for the output layer. Softmax is a function
    that converts raw scores (logits) into probabilities. It is often used in the
    output layer of a multi-class classification model. In this binary classification
    case (two classes), the softmax activation function normalizes the output scores
    for each class, assigning a probability to each class. The class with the highest
    probability is considered the model’s prediction. Softmax is useful for producing
    probability distributions over multiple classes, making it suitable for classification
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The effect of Softmax**: Softmax converts raw model outputs into probability
    distributions over classes. It ensures that the predicted probabilities sum to
    1, facilitating a meaningful interpretation of the model’s confidence in each
    class. In binary classification, it is often used in conjunction with categorical
    cross-entropy loss.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why should we use them? ReLU is chosen for its simplicity, computational efficiency,
    and effectiveness in training deep neural networks. Softmax is selected for the
    output layer to obtain class probabilities, which are valuable for interpreting
    and evaluating the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, ReLU and softmax activations contribute to the effectiveness of
    the CNN model by introducing non-linearity, promoting efficient training, and
    producing meaningful probability distributions for classification. They are widely
    used in CNNs for image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the provided code snippet, the model is compiled with three important components
    – categorical cross-entropy loss, the Adam optimizer, and the accuracy metric.
    Let’s delve into each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss=''categorical_crossentropy''`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical cross-entropy is a loss function commonly used for multi-class classification
    problems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this context, the model is designed for binary classification (two classes),
    but it uses categorical cross-entropy to handle a case where there are more than
    two classes. The target labels are expected to be one-hot-encoded.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function measures the dissimilarity between the predicted probabilities
    (obtained from the softmax activation in the output layer) and the true class
    labels.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal during training is to minimize this loss, effectively improving the
    model’s ability to make accurate class predictions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer=''adam''`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive Moment Estimation** (**Adam**) is an optimization algorithm widely
    used to train neural networks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It combines ideas from two other optimization algorithms – **Root Mean Square
    Propagation** (RMSprop) and Momentum.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam adapts the learning rates of each parameter individually, making it well-suited
    for a variety of optimization problems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is known for its efficiency and effectiveness in training deep neural networks
    and is often a default choice for many applications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics=[''accuracy'']`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy is a metric used to evaluate the performance of a classification model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of binary classification, accuracy measures the proportion of
    correctly classified instances (both true positives and true negatives) among
    all instances.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy metric is essential for assessing how well the model performs on
    the training and validation datasets.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While accuracy is a commonly used metric, it might not be sufficient for imbalanced
    datasets, where one class is much more prevalent than the other. In such cases,
    additional metrics such as precision, recall, or F1 score may be considered.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the choice of categorical cross-entropy loss, the Adam optimizer,
    and the accuracy metric during compilation reflects the best practices for training
    a binary classification model. These choices are based on their effectiveness
    in optimizing the model parameters, handling multi-class scenarios, and providing
    a straightforward evaluation of classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5**: Train the model using the augmented dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We use the `fit()` function to train the model on the augmented dataset. We
    specify the training and validation generators, the number of steps per epoch,
    the validation steps, and the number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code snippet, the `fit()` function is used to train the model on an
    augmented dataset. Let’s break down the key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_generator`):The training generator is an instance of a data generator
    that generates batches of training data with augmentation on the fly in *Step
    3*. A data generator is a way to efficiently load and preprocess data in chunks
    during training rather than loading the entire dataset into memory. `train_generator`
    is responsible for providing the model with batches of augmented training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val_generator`): Similar to the training generator, the validation generator
    is an instance of a data generator that generates batches of validation data.
    The validation generator provides a separate set of data that the model has not
    seen during training. It helps assess the model’s generalization to unseen examples
    and prevents overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps_per_epoch=train_generator.samples // 32`): `steps_per_epoch` specifies
    the number of batches of data to process in each epoch of training. It is calculated
    as the total number of samples in the training dataset divided by the batch size
    (`32` in this case). Each step involves a forward pass (prediction) and a backward
    pass (gradient computation and parameter updates) on a batch of data. A smaller
    `steps_per_epoch` value means that the model will see fewer batches in each epoch,
    potentially leading to faster training but with less exposure to the entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_steps=val_generator.samples // 32`): `validation_steps` is similar
    to `steps_per_epoch` but for the validation dataset. It determines the number
    of batches processed during each validation epoch. Like `steps_per_epoch`, it
    is calculated based on the total number of samples in the validation dataset divided
    by the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs=10`): Epoch specifies the number of times the entire dataset is processed
    during training. Training for more epochs allows the model to learn from the data
    over multiple passes, potentially improving performance. However, training for
    too many epochs may lead to overfitting, where the model memorizes the training
    data but fails to generalize to new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting the batch size, steps per epoch, and validation steps can impact the
    training speed and memory requirements. A larger batch size and more steps per
    epoch may lead to slower training but can be more memory-efficient. The number
    of epochs should be chosen carefully to balance model training and prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the settings provided to `fit()` control how the model is trained,
    the data it sees in each epoch, and the evaluation of the validation set. Properly
    tuning these settings is crucial to achieving good model performance and preventing
    issues such as overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: By following these steps, you can implement supervised CNNs using image data
    augmentation in Keras. This can help improve the performance of your model and
    make it more robust to variations in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: CNN using image data augmentation with the CIFAR-10 dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us see some example Python code for a supervised CNN using image data augmentation
    with the CIFAR-10 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code defines the architecture of a CNN using the Keras library.
    Let’s go through each line to understand the purpose and functionality of each
    component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line creates a sequential model, which allows us to stack layers
    on top of each other sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet adds a 2D convolutional layer to the model. It has
    32 filters, a filter size of `(3, 3)`, the ReLU activation function, and the `''same''`
    padding. The `input_shape` parameter is set to the shape of the input data (`x_train`)
    without the batch dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down the following CNN code snippet to understand it more in depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**2D convolutional layer addition**: In deep learning for image processing,
    convolutional layers are crucial to learning hierarchical features from input
    images. Convolutional layers are used to detect local patterns in the input data.
    Each filter in the convolutional layer learns to recognize different features
    or patterns. The code adds a layer to the neural network model, and specifically,
    it’s a 2D convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolutional layer has the following configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filters**: There are 32 filters. Filters are small grids that slide over
    the input data to detect patterns or features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter size**: Each filter has a size of (3, 3). This means it considers
    a 3x3 grid of pixels at some point during the convolution operation capturing
    local information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: The ReLU activation function is applied element-wise
    to the output of each convolutional operation. ReLU introduces non-linearity,
    allowing the model to learn complex patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Same` padding is used. Padding is a technique to preserve spatial dimensions
    after convolution preventing information loss at the edges of the image. `Same`
    padding pads the input so that the output has the same spatial dimensions as the
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape` parameter is set to the shape of the input data (`x_train`) without
    the batch dimension. The input shape determines the size of the input data that
    the layer will process. In this case, it is set to the shape of the training data
    `x_train` without considering the batch dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, this code snippet adds a convolutional layer to the neural network
    model, configuring it with specific parameters for filter size, number of filters,
    activation function, and padding. The convolutional layer plays a crucial role
    in learning hierarchical features from input images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line adds another 2D convolutional layer with the same specifications
    as the previous one, but without specifying the input shape. The model will infer
    the input shape based on the previous layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line adds a max-pooling layer with a pool size of (2, 2), which
    reduces the spatial dimensions of the input by taking the maximum value within
    each pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line adds a dropout layer with a rate of 0.25, which randomly
    sets 25% of the input units to 0 during training. Dropout helps prevent overfitting
    by introducing randomness and reducing the reliance on specific features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The code continues adding more convolutional layers, max-pooling layers, and
    dropout layers, and finally ends with fully connected (dense) layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line flattens the previous layer’s output to a 1D tensor, preparing
    it to be connected to a dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line adds a dense layer with 512 units and ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line adds a dropout layer with a rate of 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line adds a final dense layer with 10 units and softmax activation,
    which produces a probability distribution over the 10 classes for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code initializes an instance of the `ImageDataGenerator` class
    from Keras, which is used for data augmentation in image datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a CNN with two convolutional layers, two max-pooling layers,
    and three fully connected layers. Data augmentation is performed using the `ImageDataGenerator`
    class, which randomly applies various transformations to the training images to
    generate more training data. The model is trained for 100 epochs using the `fit`
    method with the data generator as the input. Finally, the model is evaluated on
    the test set using the `evaluate` method.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a variety of image data augmentation techniques.
    We learned how to implement an SVM with data augmentation in Python using the
    scikit-learn and Keras libraries. We first implemented SVM with the default hyperparameters
    and evaluated the performance of the classifier on the original dataset. We then
    implemented an SVM with data augmentation and trained the classifier on each batch
    of training data generated by the `ImageDataGenerator` object. Finally, we evaluated
    the performance of the classifier on the augmented dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how to implement a CNN using augmentation with the CIFAR-10 dataset.
    Using data augmentation, we were able to improve the accuracy of the classifier
    on the augmented dataset. This demonstrates the effectiveness of data augmentation
    in improving the performance of machine learning models, especially in cases where
    the available dataset is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation can reduce the need for manual annotation by creating variations
    of existing labeled data. Instead of labeling each transformed image separately,
    augmentation techniques allow for the generation of additional labeled samples
    without the need for additional human annotation efforts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to label text data using generative
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Labeling Text, Audio, and Video Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part of the book, you will explore how to read text, audio, and video
    data using Python, analyze the data, and extract features. The content delves
    into various methods for programmatically labeling text, video, and audio data
    in Python, leveraging OpenAI’s large language models, as well as semi-supervised
    and unsupervised techniques such as K-means clustering. Additionally, this section
    aids in understanding different open source data annotation tools such as Label
    Studio, CVAT, pyOpenAnnotate, and Azure Machine Learning for image, video, audio,
    and text data, providing a comprehensive comparison between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18944_07.xhtml#_idTextAnchor147), *Labeling Text Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18944_08.xhtml#_idTextAnchor176), *Exploring Video Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18944_09.xhtml#_idTextAnchor204), *Labeling Video Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18944_10.xhtml#_idTextAnchor221), *Exploring Audio Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18944_11.xhtml#_idTextAnchor248), *Labeling Audio Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18944_12.xhtml#_idTextAnchor267), *Hands-On Exploring Data
    Labeling Tools*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
