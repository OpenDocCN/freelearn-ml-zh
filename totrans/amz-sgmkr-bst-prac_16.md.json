["```py\n    from sagemaker.spark.processing import PySparkProcessor \n    spark_processor = PySparkProcessor( \n        base_job_name=\"spark-preprocessor\", \n        framework_version=\"3.0\", \n        role=role, \n        instance_count=15, \n        instance_type=\"ml.m5.4xlarge\", \n        max_runtime_in_seconds=7200, ) \n    configuration = [ \n        { \n        \"Classification\": \"spark-defaults\", \n        \"Properties\": {\"spark.executor.memory\": \"18g\",\n                  \"spark.yarn.executor.memoryOverhead\": \"3g\", \n                       \"spark.driver.memory\": \"18g\", \n                    \"spark.yarn.driver.memoryOverhead\": \"3g\", \n                       \"spark.executor.cores\": \"5\", \n                       \"spark.driver.cores\": \"5\", \n                     \"spark.executor.instances\": \"44\",      \n                     \"spark.default.parallelism\": \"440\", \n                   \"spark.dynamicAllocation.enabled\": \"false\" \n                    }, \n         }, \n         { \n         \"Classification\": \"yarn-site\", \n          \"Properties\": {\"yarn.nodemanager.vmem-check-enabled\": \"false\", \n          \"yarn.nodemanager.mmem-check-enabled\": \"false\"}, \n          } \n    ]\n    ```", "```py\n    from sagemaker.workflow.steps import CacheConfig\n    cache_config = CacheConfig(enable_caching=True, expire_after=\"T360m\")\n    ```", "```py\n    from sagemaker.processing import ProcessingInput, ProcessingOutput\n    run_args = pyspark_processor.get_run_args(\n        \"preprocess.py\",\n        submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n        spark_event_logs_s3_uri=spark_event_logs_s3_uri,\n        configuration=configuration,\n        outputs=[ \\ \n            ProcessingOutput(output_name=\"validation\", destination=validation_data_out, source=\"/opt/ml/processing/validation\"),\n            ProcessingOutput(output_name=\"train\", destination=train_data_out, source=\"/opt/ml/processing/train\"),\n            ProcessingOutput(output_name=\"test\", destination=test_data_out, source=\"/opt/ml/processing/test\"),\n         ],\n        arguments=[\n            '--s3_input_bucket', s3_bucket,\n            '--s3_input_key_prefix', s3_prefix_parquet,\n            '--s3_output_bucket', s3_bucket,\n            '--s3_output_key_prefix', s3_output_prefix+'/prepared-data/'+timestamp\n        ]\n    )\n    ```", "```py\n    from sagemaker.workflow.steps import ProcessingStep\n    step_process = ProcessingStep(\n        name=\"DataPreparation\",\n        processor=pyspark_processor,\n        inputs=run_args.inputs,\n        outputs=run_args.outputs,\n        job_arguments=run_args.arguments,\n        code=\"modelbuild/pipelines/preprocess.py\",\n    )\n    ```", "```py\n    # initialize hyperparameters\n    hyperparameters = {\n            \"max_depth\":\"5\",\n            \"eta\":\"0.2\",\n            \"gamma\":\"4\",\n            \"min_child_weight\":\"6\",\n            \"subsample\":\"0.7\",\n            \"objective\":\"reg:squarederror\",\n            \"num_round\":\"5\"}\n    # set an output path where the trained model will be saved\n    m_prefix = 'pipeline/model'\n    output_path = 's3://{}/{}/{}/output'.format(s3_bucket, m_prefix, 'xgboost')\n    # this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n    # specify the repo_version depending on your preference.\n    image_uri = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n    # construct a SageMaker estimator that calls the xgboost-container\n    xgb_estimator = sagemaker.estimator.Estimator(image_uri=image_uri, \n                             hyperparameters=hyperparameters,\n                         role=sagemaker.get_execution_role(),\n                             instance_count=1, \n                             instance_type='ml.m5.12xlarge', \n                             volume_size=200, # 5 GB \n                             output_path=output_path)\n    ```", "```py\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.workflow.steps import TrainingStep\nstep_train = TrainingStep(\n    name=\"ModelTrain\",\n    estimator=xgb_estimator,\n    cache_config=cache_config,\n    inputs={\n        \"train\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n            content_type=\"text/csv\",\n        ),\n        \"validation\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n            content_type=\"text/csv\",\n        ),\n    },\n)\n```", "```py\n    from sagemaker.processing import ScriptProcessor\n    script_eval = ScriptProcessor(\n        image_uri=image_uri,\n        command=[\"python3\"],\n        instance_type=processing_instance_type,\n        instance_count=1,\n        base_job_name=\"script-weather-eval\",\n        role=role,\n    )\n    ```", "```py\nfrom sagemaker.workflow.properties import PropertyFile\nevaluation_report = PropertyFile(\n    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\nstep_eval = ProcessingStep(\n    name=\"WeatherEval\",\n    processor=script_eval,\n    cache_config = cache_config,\n    inputs=[\n        ProcessingInput(\n            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            destination=\"/opt/ml/processing/model\",\n        ),\n        ProcessingInput(\n          source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,  destination=\"/opt/ml/processing/test\",\n        ),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n    ],\n    code=\"modelbuild/pipelines/evaluation.py\",\n    property_files=[evaluation_report],\n)\n```", "```py\nfrom sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\nfrom sagemaker.workflow.condition_step import (\n    ConditionStep,\n    JsonGet\n)\ncond_lte = ConditionLessThanOrEqualTo(\n    left=JsonGet(\n        step=step_eval,\n        property_file=evaluation_report,\n        json_path=\"regression_metrics.mse.value\"\n    ),\n    right=6.0\n)\nstep_cond = ConditionStep(\n    name=\"MSECondition\",\n    conditions=[cond_lte],\n    if_steps=[step_register, step_create_model],\n    else_steps=[]\n)\n```", "```py\nfrom sagemaker.model_metrics import MetricsSource, ModelMetrics\nfrom sagemaker.workflow.step_collections import RegisterModel\nmodel_metrics = ModelMetrics(\n    model_statistics=MetricsSource(\n        s3_uri=\"{}/evaluation.json\".format(\nstep_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n        ),\n        content_type=\"application/json\",\n    )\n)\nstep_register = RegisterModel(\n    name=\"RegisterModel\",\n    estimator=xgb_train,\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[\"text/csv\"],\n    response_types=[\"text/csv\"],\n    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n    transform_instances=[\"ml.m5.xlarge\"],\n    model_package_group_name=model_package_group_name,\n    approval_status=model_approval_status,\n    model_metrics=model_metrics,\n)\n```", "```py\nfrom sagemaker.workflow.pipeline import Pipeline\npipeline_name = f\"WeatherPipeline\"\npipeline = Pipeline(\n    name=pipeline_name,\n    parameters=[\n        processing_instance_type,\n        processing_instance_count,\n        training_instance_type,\n        model_approval_status,\n        input_data\n    ],\n    steps=[step_process, step_train, step_eval, step_cond],\n)\n```", "```py\nimport json\njson.loads(pipeline.definition())\npipeline.upsert(role_arn=role)\n```", "```py\nexecution = pipeline.start()\n```"]