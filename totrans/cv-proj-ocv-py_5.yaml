- en: Handwritten Digit Recognition with scikit-learn and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how machine learning can be applied to
    computer vision projects, using a couple of different Python modules. We will
    also create and train a support vector machine that will actually perform our
    digit classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring and processing MNIST digit data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and training a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the support vector machine to new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TensorFlow with digit classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquiring and processing MNIST digit data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, we will be covering handwritten digit recognition with scikit-learn
    and TensorFlow. Here, we're going to learn how machine learning can be applied
    to computer vision projects, and we're going to learn a couple of different ways
    and models, using a couple of different Python modules. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: You have probably heard about machine learning. Here, we will be particularly
    talking about supervised machine learning, where we have a bunch of examples that
    we want to accomplish. So, rather than explicitly telling the computer what we
    want, we give an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take the case of the handwritten digits 0 through 9, which have labels
    that are created by humans indicating what those digits are supposed to be. So,
    rather than hand-coding features and explicitly telling the computer what the
    algorithm is, we are going to construct a model where we take those inputs, optimize
    some functions like a set of variables, and then train the computer to put the
    outputs to be what we want them to be.
  prefs: []
  type: TYPE_NORMAL
- en: So, we will go through handwritten digits, starting with 0, 1, 2, 3, and so
    on. That's the general paradigm of machine learning, and we're going to cover
    three different algorithms here.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's start running some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up your Jupyter Notebook and, as we did in the previous chapter, let''s
    start fresh in this chapter. As you can observe in the following code, we will
    be importing our essential modules, such as `numpy`, which is the foundation of
    numerical computing in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we are importing `pyplot`, so that we
    can visualize what we are doing. We will also use a little bit of OpenCV for converting
    some images. We will also be using scikit-learn, which is abbreviated as `sklearn` in
    the actual module, while importing a support vector machine, as well as some tools
    that will give us our metrics. This will tell us how well things have actually
    worked. We will also be importing TensorFlow, with the abbreviation as `tf`, as
    we will be obtaining our data from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'One main advantage of scikit-learn and TensorFlow is that they have built-in
    functionality for getting digit recognition, which is such a common thing in computer
    vision and machine learning packages. So, you don''t need to go to websites and
    download it, and then write the lines yourself. It will be taken care of for you.
    Hence, scikit-learn actually has a good number of inbuilt datasets, some for computer
    vision, some for other tasks. It has a digit example, and we can then choose which
    datasets are available from the inbuilt datasets by writing `datasets` and then
    pressing *Tab*, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e26c33b4-1d2f-4905-827a-a886bd59b62c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have a list of inbuilt datasets. For example, you want to know `california_housing`
    prices; that is, you want estimated housing prices based on things like square
    footage and the number of bedrooms in the house—there''s a dataset for that. Some
    of this is image data, some is not. So, this might be something you want to check
    out if you want to experiment with different machine learning techniques, but
    for the `dataset.load_digits()` one, we have the following code that shows what
    it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break it down and understand the code. Firstly, we load an example image,
    just the first image in the set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The data is actually stored in images and it's an array of examples where each
    one is an 8 x 8 handwritten digit image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we plot the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6d7d2c1-ee8a-456f-adcc-ef9f4da83ae8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But I like to work with a slightly higher resolution example that we''re going
    to see from MNIST later. The lower resolution images are a little computationally
    faster to use because they''re smaller images. If we want to preprocess these
    images, these are stored as 8 x 8, and we need to convert each of them to a 1D
    array. We can do that easily using the `reshape` function, which we have used
    in our previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide us with an output where, instead of an 8 x 8 array, we get
    a 1 x 64 array, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f1f2016-6359-475b-8c87-cc232bc53330.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we are going to use the MNIST data that is available from the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a fairly standard dataset. TensorFlow is nice enough to provide some
    functionality for getting that data, so you don''t have to go to the website and
    manually download it. We need to define `data_dir` and specify a location to save
    the data to. So, just create this `/tmp/tensorflow/mnist/input_data` directory
    and this will be fine, regardless of the operating system you''re running, and
    then we have some `input_data` that we imported from `tensorflow` and `read_data_sets`.
    Now, let''s run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c828acb-7b21-4136-b8fe-34bce96b048e.png)'
  prefs: []
  type: TYPE_IMG
- en: If you don't have the files, the code will download the gzip files and, if you
    do already have them, it just reads the existing gzip files and stores them in
    the `mnist` variable. `one_hot=True` ensures you get the labels, in terms of vectors,
    which means instead of being labeled with an American numerals like zero, one,
    two, three, four, and so on, it's going to be an array of mostly zeros. It's going
    to be an array of length 10, where everything is 0 except for one thing, which
    will be 1\. So, if we have, for example, 0, 1, 0, 0, 0, 0, and so on, that would
    represent a 1 and, if it was a 9, it would be all zeros until the last one, which
    would be a 1\. So, it's one useful way for machine learning to label an output. This
    is the way we got the data and we're going to be using it; it's more helpful for
    when we actually use TensorFlow, but for scikit-learn it actually does need the
    numerics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the data before we dive in and do some actual machine learning.
    We have the `mnist` variable, and it''s already separated into training and testing
    data. With machine learning, you don''t want to train on all of your data; you
    don''t want to build your model with all of your data because then you won''t
    know how well it''s going to handle new data examples that it hasn''t seen before.
    What you want to do is split it into training data and testing data. So, the training
    data is going to build a model, and the test data is going to validate it. So,
    the splitting of the data is already done for us, just with the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's break down the code for better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we load `train_data` from `train.images`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to see what the shape is to understand it using `.shape`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to know the number of samples, we can extract that from the `shape`
    output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Again, it is a NumPy array, so all NumPy functions and features are there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, execute the following code for `train_labels`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we just see where the `train.label` equals `1` and we extract that to
    create an array of those values, which will give us our `train_labels`. So, a
    1D array corresponds to the number of examples where it contains the actual output
    of each one. We'll see just an example; let's take `1000` out of `55000` training
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e3c2879-f179-4de7-94b3-99c325736b94.png)'
  prefs: []
  type: TYPE_IMG
- en: '`784` is the number of pixels in the image and that''s because they are `28`
    by `28` squares, *28 x 28 = 784*. So, we have, `55000` examples `784` pixels,
    or we call them features, and then the `train_labels` is going to be of length
    `55000` and the other dimension is just `1`. Here''s an example. This data already
    comes in a 1D array, so that was why we used the `reshape` function and passed
    the `28` by `28` value, in order to convert it to an actual image that we can
    see.'
  prefs: []
  type: TYPE_NORMAL
- en: Great, our data is loaded and processed and is ready to be used, so we can begin
    actual machine learning. Now that our data is set up and ready to go, we can move
    on to our next section, in which we will create and train our support vector machine
    and perform our digit classification.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and training a support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to create and train a support vector machine that
    will actually perform our digit classification.
  prefs: []
  type: TYPE_NORMAL
- en: In the very first example, we're going to use scikit-learn, and we're going
    to use what's called a support vector machine, which is a very powerful, very
    versatile classic machine learning technique that can learn all kinds of functions
    and all kinds of mappings from inputs to outputs. We're going to do classification,
    which is mapping inputs as an array of pixels, and in our case we're going to
    classify each input into one of ten classes, corresponding to ten digits. But
    we can classify different kinds of things as continuous ordered functions, which
    is called regression, and that can be useful, for example, if you want to extract
    position or an area of volume where it doesn't just fit into a neat category.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we''re going to be doing primarily classification. So, scikit-learn
    makes it very easy to create such a model. A support vector classifier can be
    called with `svm.SVC`, in which support vector machine came from `sklearn` package and
    we have this meta parameter for the model called `gamma`, just kind of an inverse
    radius, the area of influence of the sport vectors, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How the support vector machine works is not covered here, as there is plenty
    of literature available on that subject, and it's not absolutely necessary to
    understand it fully in order to learn it. Now, we're just going to see how we
    can apply this for some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The `gamma` parameter is something I recommend you experiment with as an exercise.
    We're going to start with a known `gamma` parameter that will work well for our
    case, namely `.001`, but you should learn about the other parameters that are
    available. I recommend going to [http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) and
    again I recommend playing with this to see how it affects execution time and accuracy. But,
    what's important to take away here is that we can create our model with just one
    line. It defines the model but we haven't actually trained it. We haven't actually
    given any data and made it fit its parameters such that it will actually produce
    a desirable output. Now, if we feed it an image of five, it will say, that's OK,
    this is a 5\. So, in order to do that, we have to fit it.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we have created our classifier and it's very simple: `classifier.
    fit`. We give it the `train_data` and the `train_labels` that we got from our
    previous code execution. Just a heads up, this execution is going to take a few
    minutes; it generally does. Usually, the training process is the slowest part
    of machine learning. That's typically the case but this shouldn't be too bad.
    This only takes a couple of minutes and, again, we're just using your training
    data so that we can verify that this will generalize to unseen cases.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've seen our support vector machine and it's actually been trained,
    we can move on to our next section, where we apply the support vector machine
    to new data that it was not trained upon.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the support vector machine to new data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our trained support vector machine, we can actually apply the
    support vector machine to new data that hasn't been seen and see that our digit
    classifier is actually working.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the cell has executed successfully and if everything worked correctly,
    we should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b80dd4a3-bab1-4049-b30c-1781e67d40e6.png)'
  prefs: []
  type: TYPE_IMG
- en: This is just the output from creating the support vector classifier. This just
    gives information about the metadata parameters that we used; we used what's known
    as a radial basis function kernel, and fitting the data did not produce any error
    messages. So, that means the code has worked. So, now we have our trained model,
    we want to see how well it's going to work on data that it hasn't seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to get our test data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We get our `mnist.test.images`, which is equal to `mnist.train.images`, and
    extract the labels the same way, by calling the `expected` variable, and then
    we're going to compute `predicted` from the `classifier` model, using `classifier.predict(test_data)`.
    So, this is going to take just a little bit of time to execute. After execution,
    there should be no error messages, which indicates that our prediction ran successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now we can see how well we did. We''re going to use the built-in metrics
    functions from scikit-learn. We''re going to record some of the metrics, such
    as *precision* and *recall*, and if you want to understand what those mean, I
    recommend the following Wikipedia article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Precision_and_recall_to_understand_metric_definitions](https://en.wikipedia.org/wiki/Precision_and_recall_to_understand_metric_definitions)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just in short, they are different metrics for evaluating how well your machine
    learning algorithm did. Accuracy is probably the most common. It''s simple: the
    correct data points divided by the total. But there''s also precision recall that
    weighs the pros and cons with true positives, true negatives, false positives,
    and false negatives, and which one is the best depends on your application. It
    depends on which is worse between the false positive and the false negative and
    so forth and, on top of that, we''re going to output what''s known as a confusion
    matrix, which tells you which ones were successful and which ones were misclassified.
    Let''s run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It should give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/634bd8ef-8432-49da-b72a-381338a357ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OK, so we get the classification reports and we can see `precision`, `recall`,
    and another metric called `f1-score` that you can read about in that same Wikipedia
    article. In short, zero is the worst case and one is the best case. In the preceding
    screenshot, we can see `precision`, `recall`, and `f1-score` for the different
    digits and we can see we''re in the 90% range; it varies, which is OK. It depends
    on your application, but that might be good enough or that might be abysmally
    bad. It depends. We''re actually going to see how we can do better a little later
    on using a more powerful model. We can see that it generally worked. We look at
    the confusion matrix here, where the columns tell you what the actual value is
    and the rows tell you what the predicted value is. Ideally, we would see all large
    values along the diagonal and all zeroes otherwise. There''s always going to be
    some errors, we''re human beings so that''s going to happen but like I said, we''re
    going to see if we can do a little bit better, in vast majority of cases it did
    work. Now, we can see some example random outputs, where we had some digits as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1677df97-8bed-4199-9ac3-bb9ac2f3ae4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, all of the predictions are correct according to their images.
    OK, that''s all well and good but I kind of feel like I''m taking the computer''s
    word for it at this point. I''d like to throw my own data at it. I''d like to
    see how well this is really working, and this is something generally recommended
    with machine learning. You want to test it with your own data to really know if
    it''s working and, if nothing else, it''s much more satisfying. So, here is a
    little snippet of code that''s going to use Jupyter''s widget capabilities, its
    interactive capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So, now we're actually going to create a little drawing widget. It's going to
    let us produce our own digits. Let's look at the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import `Line2D` from `matpllotlib.line`, this is going to let us draw
    individual lines, like creating a kind of a vector image based on our mouse movements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute `%pylab notebook`; the percent sign indicates the following magic
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It's kind of a meta command within Jupyter and Pylab Notebook, and it loads
    a bunch of stuff into your namespace for plotting and numerics. It's not necessary
    because we already did that with NumPy and Matplotlib, but to enable the widgets,
    we use this command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, create this `Annotator` class, which contains call back for what happens
    if we move a mouse over our displayed image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We don't have to understand the `Annotator` class, but this might be something
    useful in the future if you want to make an annotation or draw something, and
    to seize a full snippet of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''re going to create a blank image, the same size as our images. It''s
    just going to be three RGBs for the time being. It just looks a little bit nicer,
    even though we''re going to make it black and white in the end, because that''s
    what our data is. Create the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a plot, show it, and hook up our `annotator` functions to that,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e873b37-046c-4f77-aaaf-caad496ca0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s draw, say, the numeral three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e1d18ae-a7b8-44d6-a4d5-27d8be21a696.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, that''s sluggish you know, and isn''t exactly going to replace Photoshop,
    but this still beats going into a separate program and creating your image file,
    making sure it''s in the right format, saving it, and then writing code to load
    it and get it right. So, this will allow us to quickly play and experiment with
    our models. We just created a kind of array of lines, so we need to rasterize
    that and process it so that it looks more like actual handwritten digits, something
    that would come from either a scanned pencil drawing or a pressure-sensitive tablet.
    The following will do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the code. First, we create a blank image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate over the `xy` pairs that came from our `annotator` and then we''re
    going to draw lines there in raster format on our rasterized images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we convert the image to a `float`, from range `0` to `1`, just like our
    input data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we are going to bring it a little bit closer to `1`, because that''s
    just what our input images look like and what our model would be expecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have our two-dimensional image but, of course, to run it through our
    model, we need to flatten it into `1` x `784`, so that''s what this `reshape`
    function does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''re going to run that through our `classifier` and print the output
    as well. We will create a figure where we can see what our rasterized image looks
    like, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64e0f2af-31ab-4e3d-8457-e65f326b3aaa.png)'
  prefs: []
  type: TYPE_IMG
- en: We drew a three and we predicted a `3`. Excellent. Let's try something else.
    Clear the previous output by hitting *Ctrl* + *Enter*, and we get a warning message;
    it's just telling us that it has clobbered some of the variables that were created.
    That's not a big deal. You can safely ignore that. Just a fair warning, your mileage
    may vary on this, depending on what your handwriting is like and what was in your
    training data. If you wanted this to work perfectly every time, or as close to
    that as possible, you want to train it probably on your own handwriting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54ef84b4-a1ba-4453-b443-57ad38f50a2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/980d11b9-8013-4631-8cba-0c53e53a8f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, here you can see an example of it not working. The prediction is supposed
    to be zero but the model predicted a three for some reason. It's possible that
    if you redraw it, it might work. So, again, your mileage may vary. Experiment
    with it. You can also play with the preprocessing, although this, as far as I
    can tell, works pretty well. But anyway, we can see that our model is, at least
    for the most part, working. So, that's going to be it for the scikit-learn support
    vector machines. Now, in our next section, we're going to introduce TensorFlow
    and perform digit classification with that.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorFlow with digit classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to see TensorFlow in action and see how we can perform digit classification
    with a tractable amount of code. TensorFlow is Google's machine learning library,
    for numerical analysis in general. It is called TensorFlow because it supposedly
    flows tensors, where tensors are defined to be arrays of *n* dimensions. Tensors
    have a real geometric meaning that just multidimensional arrays don't necessarily
    classify, but we're just going to use that term. A tensor is just a multidimensional
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''re going to do a simple `softmax` example. It''s a very simple model;
    you can visit TensorFlow''s own website ([https://www.tensorflow.org/get_started/mnist/beginners](https://www.tensorflow.org/get_started/mnist/beginners))
    for more information. Let''s have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In short, you're going to take your input data and you're going to multiply
    it by a matrix. The data has `784` points. Each point is going to have a matrix
    value and, for each of the `10` classes, you're going to compute an inner product
    by multiplying 784 × 784 and sum them up. There will be `10` outputs. It will
    be a 1 by 10 array, and you're going to add a bias variable to the output of the
    array and run it through the `softmax` function, which will convert it to something.
    The output of the matrix plus the bias will compute something in the range of
    `0` to `1`, which loosely corresponds to the probability of that data being in
    that class. For example, there might be a 0.4% probability or 40% probability
    of being a `1`, a 2% probability of being it `2`, and 90% probability of it being
    a `9`, and the output is going to be the maximum output of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow is very sophisticated. There''s a little bit more setup here than
    there was with the scikit-learn example. You can learn more about that on their
    own website. Now, let''s go through the following code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We've already done this in the previous example. Now, we're going to get the
    data from `data_dir`; make sure it's in our `mnist` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create the model where `x` corresponds to our input data and, although
    we''re not loading the data just yet, we just need to create a placeholder so
    TensorFlow knows where stuff is. We don''t need to know how many examples there
    are, and that''s what the `None` dimension corresponds to, but we do need to know
    how big each example is, which in this case is `784`. `W` is the matrix that''s
    going to multiply `x` classes to the inner product over the image, `784` *dot*
    `784`, and you do that `10` times. So, that corresponds to a 784/10 matrix, `10`
    being the number of classes; then, you add the `b` bias variable to that. The
    values of `W` and `b` are what TensorFlow is going to produce for us based on
    our inputs, and `y` defines the actual operation that''s going to be performed
    on our data from the matrix multiplication. We add the `b` bias variable to it
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a placeholder for our labeled data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to do machine learning, you need a `loss` function or a `fitness`
    function, which tells you how well your model is doing given learning parameters
    like those given in `W` and `b`. Hence, we''re going to use something called `cross-entropy`;
    we''ll not go into much detail about `cross-entropy` but that''s going to give
    us some criteria for letting us know that we''re getting closer to a working model,
    as shown in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As we add more and more data, we're going to use what's known as `GradientDescentOptimizer`
    in order to minimize the error, minimize the cross entropy, and make our model
    fit as well as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we''re actually going to start by creating an interactive
    session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We want to make it an interactive session so that we can use our model and buy
    new data to it afterwards. We're going to initialize `run()` and then we're going
    to compute the data in batches. TensorFlow is a very powerful program and it allows
    you to break up your data. We're not going to do it here, but you can run parallelized
    code fairly easily with it. Here, we're just going to iterate `1000` times and
    compete stuff in batches feeding in our training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this runs, we''re going to see how well we did and just see where our
    predicted data is equal to the given labels to it. We can compute the `accuracy`
    by just seeing how many of the predictions were correct on average. Then, print
    that data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d8cf5fb-5bb4-4041-8402-1629d6cee9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Being a really simple model, it runs a lot faster and we can see that we got
    a little less than 92% accuracy. The code executed a lot faster, but a little
    bit less accurately than our **Support Vector Machine** (**SVM**), but that's
    OK. This code just provides a very simple example of how TensorFlow works.
  prefs: []
  type: TYPE_NORMAL
- en: 'You get a little bit more advanced momentarily, but let''s test the following
    code the way we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1af2c010-209e-41b9-913d-923e77aec493.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our annotator is initiated and let''s put in a digit. Try a `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15a25209-0eed-4937-9a5a-7256ffe39d97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''re going to preprocess the drawn digit and it''s almost the same code
    as before, which is going to go through the classes and possible classes for our
    data and see which one the TensorFlow `softmax` model thought was the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we''ll run the preceding block and, as shown, it predicts a `3` from a
    `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a71f2e2c-549b-42f8-9450-31496778a708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes it might not predict correctly, and that is unfortunately going to
    happen. So, there are two ways to improve that: train on your own handwriting
    or use a better model.'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to move on to the most powerful model in this section. Here, we're
    going to just briefly touch on deep learning with, **convolutional neural networks**
    (**CNNs**). We are not covering the theory here. There's a lot to know about deep
    learning and multi-layered neural networks in general. Deep learning is a deep
    subject but, for this chapter, we're just going to see how we can actually apply
    state-of-the-art machine learning techniques to digit recognition with a relatively
    simple block of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have a `deepnn(x)` function here that creates our deep neural network,
    finds our hidden layers or convolutional layers, pools, and so forth, and defines
    all our necessary stuff from our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`deepnn` builds the graph for a deep net for classifying digits and `reshape` function
    is to be used within a convolutional neural net. The arguments used here are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: An input tensor with the dimensions (`N_examples`, `784`), where `784`
    is the number of pixels in a standard MNIST image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y`: A tensor of shape (`N_examples`, `10`), with values equal to the logic''s
    of classifying the digit into one of 10 classes (the digits 0-9). `keep_prob`
    is a scalar placeholder for the probability of dropout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This returns a tuple (`y`, `keep_prob`). The last dimension is for *features*—there
    is only one here, since images are grayscale—it would be 3 for an RGB image, 4
    for RGBA, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first convolutional layer maps one grayscale image to `32` feature maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We have our functions that do convolutions, weight variables, bias variables,
    and so forth. Then, we have our main code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `mnist` variable gets the data in case we don't already have it. We define
    our placeholder for the input, and the outputs build the graph. We then define
    the `fitness` function and `cross-entropy` and create our graph. We have to be
    careful while creating the session; in the example on their website, they just
    created a normal session. We want an interactive session here, so that we can
    apply our model to our own generated data, and we're going to break this up into
    batches. We'll run it, and every `100` iterations it's going to tell us what exactly
    it's doing and then, at the end, it will tell us our accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the code and extract the data, and you can see the statistics as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/820ee86a-10a9-4c9d-bac7-d4e93158c64e.png)'
  prefs: []
  type: TYPE_IMG
- en: It starts off with very bad training accuracy but it quickly gets up to over
    90% and shoots up to `1`. It's not exactly 100%, but that usually means it's something
    like 99%, so pretty close to `1`. This usually takes a few minutes. OK, now we
    have created our TensorFlow classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we finished training, as we can see from the following screenshot, we
    get a result of over 99%, so that is significantly better than what we got with
    `softmax` or our SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4eabb374-6994-4a4b-a598-fa2b2b9b38bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning is probably the most powerful machine learning technique, due
    to its ability to learn very complex pattern recognition. It's just dominating
    everything else for advanced computer vision, speech processing, and more—stuff
    that conventional machine learning techniques haven't been all that successful
    at. However, that doesn't necessarily mean that you want to use deep learning
    for everything. Deep learning generally acquires a large number of examples—many
    thousands, if not millions sometimes—and it can also be very computationally expensive.
    So, it's not always the best solution, although it is very powerful, as we have
    seen right here. So, 99% is about as good as you can get.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to draw digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code rasterizes and preprocesses the handwritten digit image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s test it again on our handwritten digit `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1880122f-8b2d-4199-b02f-e02ef3c0a43b.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, we have similar code for processing the vectorized image, and we're getting
    the output and raster form and running it through our model, doing `accuracy.eval`
    here. As we can see in the preceding screenshot, we've got a zero as expected,
    and that's perfect. So,  we're going to be talking more about deep learning with
    CNNs in the next chapters, but we've already seen how powerful it is with relatively
    little code and we were able to fly it towards our particular problem of digit
    recognition. Alright, so, with that, we're going to move on to our next chapter,
    which is [Chapter 6](c59fb392-c966-4da6-987a-625378474e71.xhtml), *Facial Feature
    Tracking and Classification with dlib*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to perform digit classification with TensorFlow
    using `softmax`. We learned how to acquire and process MNIST digit data. We then
    learned how to create and train a support vector machine, and apply it to new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn facial feature tracking and classification
    using dlib.
  prefs: []
  type: TYPE_NORMAL
