["```py\nRecordReader recordReader = new  ] CSVRecordReader(numLinesToSkip,delimiter);\nrecordReader.initialize(new FileSplit(new ClassPathResource(fileName).getFile()));\nDataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses);\nDataSet allData = iterator.next();\nallData.shuffle();\nSplitTestAndTrain testAndTrain = allData.splitTestAndTrain(trainPercent); \nDataSet trainingData = testAndTrain.getTrain();\nDataSet testData = testAndTrain.getTest();\n```", "```py\nDataSetIterator mnistTrain = new MnistDataSetIterator(batchSize, true, randomSeed);\nDataSetIterator mnistTest = new MnistDataSetIterator(batchSize, false, randomSeed);\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() .seed(randomSeed) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // use SGD\n.iterations(m)//iterations\n.activation(Activation.RELU)//activation function\n.weightInit(WeightInit.XAVIER)//weight initialization\n.learningRate(rate) //specify the learning rate\n.updater(Updater.NESTEROVS).momentum(momentum)//momentum\n.regularization(true).l2(rate * regularization) // \n.list()\n.layer(0, \nnew DenseLayer.Builder() //create the first input layer.\n.nIn(numRows * numColumns)\n.nOut(firstOutput)\n.build())\n.layer(1, new DenseLayer.Builder() //create the second input layer\n.nIn(secondInput)\n.nOut(secondOutput)\n.build())\n.layer(2, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) //create hidden layer\n.activation(Activation.SOFTMAX)\n.nIn(thirdInput)\n.nOut(numberOfOutputClasses)\n.build())\n.pretrain(false).backprop(true) //use backpropagation to adjust weights\n.build();\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\nmodel.setListeners(new ScoreIterationListener(5));  //print the score with every iteration\n//Initialize the user interface backend\nUIServer uiServer = UIServer.getInstance();\n//Configure where the network information (gradients, activations, score vs. time etc) is to be stored\n//Then add the StatsListener to collect this information from the network, as it trains\nStatsStorage statsStorage = new InMemoryStatsStorage();             //Alternative: new FileStatsStorage(File) - see UIStorageExample\nint listenerFrequency = 1;\nnet.setListeners(new StatsListener(statsStorage, listenerFrequency));\n//Attach the StatsStorage instance to the UI: this allows the contents of the StatsStorage to be visualized\nuiServer.attach(statsStorage);\nlog.info(\"\"Train model....\"\");\nfor( int i=0; i<numEpochs; i++ ){\nlog.info(\"\"Epoch \"\" + i);\nmodel.fit(mnistTrain);\n        }\nlog.info(\"\"Evaluate model....\"\");\nEvaluation eval = new Evaluation(numberOfOutputClasses); \nwhile(mnistTest.hasNext()){\nDataSet next = mnistTest.next();\nINDArray output = model.output(next.getFeatureMatrix()); //get the networks prediction\neval.eval(next.getLabels(), output); //check the prediction against the true class\n        }\nlog.info(eval.stats());\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n.seed(seed)\n.iterations(iterations) .regularization(true).l2(regularization)\n.learningRate(learningRate)\n.weightInit(WeightInit.XAVIER) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) .updater(Updater.NESTEROVS).momentum(momentum)\n.list()\n.layer(0, new ConvolutionLayer.Builder(xsize, ysize)\n.nIn(nChannels)\n.stride(x,y)\n.nOut(nOut0)\n.activation(Activation.IDENTITY)\n.build())\n.layer(1, new SubsamplingLayer\n.Builder(SubsamplingLayer.PoolingType.MAX)\n.kernelSize(width, height)\n.stride(sx,sy)\n.build())\n.layer(2, new ConvolutionLayer.Builder(xsize, ysize)\n.stride(x,y)\n.nOut(nOut2)\n.activation(Activation.IDENTITY)\n.build())\n.layer(3, new SubsamplingLayer\n.Builder(SubsamplingLayer.PoolingType.MAX)\n.kernelSize(width, height)\n.stride(sx,sy)\n.build())\n.layer(4, new DenseLayer.Builder()\n.activation(Activation.RELU)\n.nOut(nOut4).build())\n.layer(5, new OutputLayer. Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n.nOut(outputNum)\n.activation(Activation.SOFTMAX)\n.build())\n.setInputType(InputType.convolutionalFlat(numRows,numColumns,1)) \n.backprop(true).pretrain(false).build();\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n.seed(rngSeed)\n.iterations(iterations)\n.optimizationAlgo(\nOptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n.learningRate(learningRate)\n.updater(Updater.RMSPROP).rmsDecay(rmmsDecay)\n.weightInit(WeightInit.XAVIER)\n.regularization(true).l2(regulaization)\n.list()\n.layer(0, new VariationalAutoencoder.Builder()\n.activation(Activation.LEAKYRELU)\n                .encoderLayerSizes(vaeEncoder1, vaeEncoder2)        //2 encoder layers\n                .decoderLayerSizes(vaeDecoder1, vaeDecoder2)        //2 decoder layers\n.pzxActivationFunction(\"\"identity\"\")  //p(z|data) activation function\n.reconstructionDistribution(new BernoulliReconstructionDistribution(Activation.SIGMOID.getActivationFunction()))     //Bernoulli distribution for p(data|z) (binary or 0 to 1 data only)\n.nIn(numRows * numColumns) //Input size                      \n.nOut(latentVarSpaceSize) //Size of the latent variable space: p(z|x).\n.build())\n.layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX)\n.nIn(latentVarSpaceSize).nOut(outputNum).build())\n.pretrain(true).backprop(true).build();\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n.seed(seed)\n.gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\n.gradientNormalizationThreshold(1.0)\n.iterations(iterations)\n.updater(Updater.NESTEROVS)\n.momentum(momentum)\n.optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)\n.list()\n.layer(0, new RBM.Builder().nIn(numRows*numColumns).nOut(nOut0)\n.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)\n.visibleUnit(RBM.VisibleUnit.BINARY)\n.hiddenUnit(RBM.HiddenUnit.BINARY)\n.build())\n.layer(1, new RBM.Builder().nIn(nIn1).nOut(nOut1)\n.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)\n.visibleUnit(RBM.VisibleUnit.BINARY)\n.hiddenUnit(RBM.HiddenUnit.BINARY)\n.build())\n.layer(2, new RBM.Builder().nIn(nIn2).nOut(nOut2)\n.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)\n.visibleUnit(RBM.VisibleUnit.BINARY)\n.hiddenUnit(RBM.HiddenUnit.BINARY)\n.build())\n.layer(3, new OutputLayer.Builder().nIn(nIn3).nOut(outputNum)\n.weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX)\n.build())\n.pretrain(true).backprop(true)\n.build();\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\nmodel.setListeners(new ScoreIterationListener(listenerFreq));\n```", "```py\nParameterSpace<Double> learningRateHyperparam = new ContinuousParameterSpace(0.0001, 0.1);  //Values will be generated uniformly at random between 0.0001 and 0.1 (inclusive)\nParameterSpace<Integer> layerSizeHyperparam = new IntegerParameterSpace(16,256);            //Integer values will be generated uniformly at random between 16 and 256 (inclusive)\nMultiLayerSpace hyperparameterSpace = new MultiLayerSpace.Builder()\n//These next few options: fixed values for all models\n.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n.iterations(1)\n.regularization(true)\n.l2(0.0001)\n//Learning rate: this is something we want to test different values for\n.learningRate(learningRateHyperparam)\n.addLayer( new DenseLayerSpace.Builder()\n//Fixed values for this layer:\n.nIn(784)  //Fixed input: 28x28=784 pixels for MNIST\n.activation(\"\"relu\"\")\n//One hyperparameter to infer: layer size\n.nOut(layerSizeHyperparam)\n.build())\n.addLayer( new OutputLayerSpace.Builder()\n//nIn: set the same hyperparemeter as the nOut for the last layer.\n.nIn(layerSizeHyperparam)\n//The remaining hyperparameters: fixed for the output layer\n.nOut(10)\n.activation(\"\"softmax\"\")\n.lossFunction(LossFunctions.LossFunction.MCXENT)\n.build())\n.pretrain(false).backprop(true).build();\n```"]