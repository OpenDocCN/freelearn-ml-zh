<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer076">
<h1 class="chapter-number" id="_idParaDest-151"><a id="_idTextAnchor154"/>9</h1>
<h1 id="_idParaDest-152"><a id="_idTextAnchor155"/>Exploring Diffusion Models for Synthetic Data</h1>
<p>This chapter introduces you to diffusion models, which are cutting-edge approaches to synthetic data generation. We will highlight the pros and cons of this novel synthetic data generation approach. This will help you to make informed decisions about the best methods to utilize for your own problems. We will highlight the opportunities and challenges of diffusion models. Moreover, this chapter is enriched with a comprehensive practical example, providing hands-on experience in both generating and effectively employing synthetic data for a real-world ML application. As you go through diffusion models, you will learn about the main ethical issues and concerns around utilizing this synthetic data approach in practice. In addition to that, we will review some state-of-the-art research on this topic. Thus, this chapter will equip you with the necessary knowledge to thoroughly understand this novel synthetic data <span class="No-Break">generation approach.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>An introduction to <span class="No-Break">diffusion models</span></li>
<li>Diffusion models – the pros <span class="No-Break">and cons</span></li>
<li>Hands-on diffusion models <span class="No-Break">in practice</span></li>
<li>Diffusion models – <span class="No-Break">ethical issues</span></li>
</ul>
<h1 id="_idParaDest-153"><a id="_idTextAnchor156"/>Technical requirements</h1>
<p>Any code used in this chapter will be available under the corresponding chapter folder at this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning"><span class="No-Break">https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor157"/>An introduction to diffusion models</h1>
<p>In this section, we will <a id="_idIndexMarker404"/>explore diffusion models. We will <a id="_idIndexMarker405"/>compare them to <strong class="bold">Variational Autoencoders</strong> (<strong class="bold">VAEs</strong>) and <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), which we covered in <a href="B18494_07.xhtml#_idTextAnchor120"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. This will help you to gain a holistic and comprehensive understanding of generative models. Additionally, it will make comparing and contrasting the architectures, training procedures, and data flow of these methods straightforward. Furthermore, we will also learn how to train a typical <span class="No-Break">diffusion model.</span></p>
<p><strong class="bold">Diffusion Models</strong> (<strong class="bold">DMs</strong>) are generative <a id="_idIndexMarker406"/>models that were recently proposed as a clever solution to generate images, audio, videos, time series, and texts. DMs are excellent at modeling complex probability distributions, structures, temporal dependencies, and correlations in data. The initial mathematical model behind DMs was first proposed and applied in the field of statistical mechanics to study the random motion of particles in gases and liquids. As we will see later, it is essential and crucial to learn about DMs, as they are powerful generative models that can usually generate higher-quality and more privacy-preserving synthetic data compared to other approaches. Additionally, DMs rely on strong mathematical and <span class="No-Break">theoretical foundations.</span></p>
<p>One of the first works to show that DMs can be utilized to generate photorealistic images was <em class="italic">Denoising Diffusion Probabilistic Models</em> (<a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a>), which was proposed by researchers from UC Berkeley. This pioneering work was followed by another work by OpenAI titled <em class="italic">Diffusion Models Beat GANs on Image Synthesis</em> (<a href="https://arxiv.org/pdf/2105.05233.pdf">https://arxiv.org/pdf/2105.05233.pdf</a>), showing that DMs are better at generating photorealistic synthetic images. Then, other researchers started to explore the potential of these DMs in different fields and compare them to VAEs <span class="No-Break">and GANs.</span></p>
<p><strong class="bold">Variational Autoencoders</strong> (<strong class="bold">VAEs</strong>) are one of the earliest solutions for generating synthetic data. They <a id="_idIndexMarker407"/>are based on using an encoder to encode data from a high-dimensional space (such as RGB images) into a latent low-dimensional space. Then, the decoder is used to reconstruct these encoded samples from the latent space to the original high-dimensional space. In the training process, the VAE is forced to minimize the loss between the original training sample and the reconstructed one by the decoder. Assuming the model was trained on a sufficient number of training samples, it can then be used to generate new synthetic data by sampling points from the latent space and using the decoder to decode them, from the latent low-dimensional space to the high-dimensional one, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor158"/>The training process of DMs</h2>
<p>DMs are used to generate synthetic data with a distribution close to the probability distribution <a id="_idIndexMarker408"/>of the training data. Thus, an important task is to learn the distribution of our training data and then leverage our knowledge of the real data to generate an unlimited number of synthetic data samples. Usually, we would want to generate high-quality and diverse synthetic data using a fast-generation method. However, each generation method has its own advantages and disadvantages, as we will see <span class="No-Break">later on.</span></p>
<p>As illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em>, given a training image, <img alt="" height="36" src="image/B18494_F09_001.png" width="43"/>, from the real data, the DM adds <strong class="bold">Gaussian noise</strong> to this image to become <img alt="" height="42" src="image/B18494_F09_002.png" width="46"/>. The process is repeated until the image simply becomes an <a id="_idIndexMarker409"/>image of random noise, <img alt="" height="24" src="image/B18494_F09_003.png" width="23"/>. This process is called <strong class="bold">forward diffusion</strong>. Following this, the model starts the denoising process, in which the DM takes the random noise, <img alt="" height="27" src="image/B18494_F09_004.png" width="24"/>, and reverses the previous process (i.e., forward diffusion) to reconstruct <a id="_idIndexMarker410"/>the training image. This process is known as <span class="No-Break"><strong class="bold">reverse diffusion</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="Figure 9.1 – The training process and architectures of the main generative models – VAEs, GANs, and DMs" height="887" src="image/Figure_09_01_B18494.jpg" width="895"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – The training process and architectures of the main generative models – VAEs, GANs, and DMs</p>
<p>As we can <a id="_idIndexMarker411"/>see from <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em>, the forward diffusion <a id="_idIndexMarker412"/>process is a <strong class="bold">Markov chain</strong>. Each step of the process is a stochastic event, and each event depends only on the previous event or state. Thus, they form a sequence of stochastic events and, consequently, a <span class="No-Break">Markov chain.</span></p>
<p>A key idea of the DMs’ training process is using a neural network such as <em class="italic">U-Net</em> (<a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a>) to predict the amount of noise that was added to a given noisy image from the training data. This is crucial to reverse the noising process and learn how to generate a synthetic sample given <span class="No-Break">random noise.</span></p>
<p>When the diffusion model converges after a successful training process, we can give it random noise, <img alt="" height="26" src="image/B18494_F09_005.png" width="26"/>, and the DM, using the reverse diffusion path, will give us a synthetic sample based on the provided <img alt="" height="24" src="image/B18494_F09_006.png" width="23"/>. Thus, we can now generate an unlimited number of new synthetic samples from the same training data <span class="No-Break">probability distribution.</span></p>
<p>Please note the <a id="_idIndexMarker413"/>number of diffusion steps, <img alt="" height="24" src="image/B18494_F09_007.png" width="24"/>, in the noising/denoising process depends on how smooth you want the training process to be. In other words, a higher value of <img alt="" height="22" src="image/B18494_F09_008.png" width="22"/> means less abrupt and more gradual noise will be added in the training process. Thus, the model’s weights will update steadily and the optimization loss will decrease smoothly. However, higher values of <img alt="" height="28" src="image/B18494_F09_009.png" width="28"/> will make the process slower (usually, <img alt="" height="33" src="image/B18494_F09_010.png" width="154"/> ).</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor159"/>Applications of DMs</h2>
<p>DMs have been <a id="_idIndexMarker414"/>utilized for a wide range of applications in various domains such as computer vision, natural language processing, finance, and healthcare. Let’s briefly discuss some of <span class="No-Break">these applications.</span></p>
<ul>
<li><span class="No-Break"><strong class="bold">Computer vision</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Image generation</strong>: Generating diverse photorealistic images is one of the ultimate <a id="_idIndexMarker415"/>aims of DMs. However, there is usually a trade-off between diversity and photorealism. Thus, to maintain <a id="_idIndexMarker416"/>good fidelity, the DMs are usually guided or conditioned on textual data. For more information, please refer to <em class="italic">GLIDE: Towards Photorealistic Image Generation and</em> <em class="italic">Editing with Text-Guided Diffusion Models</em> (<a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a>). As you can imagine, the generated synthetic images can be used for various applications, including data augmentation, creative art, prototyping, <span class="No-Break">and visualization.</span></li><li><strong class="bold">Video prediction</strong>: Predicting the next frame has many useful applications, as it facilitates <a id="_idIndexMarker417"/>predicting future events, which is essential for planning and decision-making in fields such as robotics. Simultaneously, it has huge applications in fields such as surveillance and security. Video prediction ML models can be utilized to anticipate and forecast possible threats, hazards, and potential risks. Additionally, predicting future frames accurately can be utilized to generate synthetic videos to complement real training data. For more details, refer to <em class="italic">Diffusion Models for Video Prediction and Infilling</em> (<a href="https://arxiv.org/pdf/2206.07696.pdf">https://arxiv.org/pdf/2206.07696.pdf</a>), <em class="italic">Video Diffusion Models</em> (<a href="https://arxiv.org/pdf/2204.03458.pdf">https://arxiv.org/pdf/2204.03458.pdf</a>), and <em class="italic">Imagen Video: High-Definition Video Generation with Diffusion </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2210.02303.pdf"><span class="No-Break">https://arxiv.org/pdf/2210.02303.pdf</span></a><span class="No-Break">).</span></li><li><strong class="bold">Image inpainting</strong>: This is simply filling in or restoring missing or damaged parts in images. It is <a id="_idIndexMarker418"/>a fundamental task in areas such as historical archiving, privacy protection, and entertainment. Furthermore, it has also been recently utilized for synthetic data generation. For example, DMs were utilized recently to generate synthetic brain MRIs, with the ability to control tumoral and non-tumoral tissues. It was shown that using generated synthetic data can boost performance considerably. For more details, please refer to <em class="italic">Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report</em> (<a href="https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf">https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf</a>) and <em class="italic">RePaint: Inpainting using Denoising Diffusion Probabilistic </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2201.09865"><span class="No-Break">https://arxiv.org/abs/2201.09865</span></a><span class="No-Break">).</span></li><li><strong class="bold">Image colorization</strong>: This is <a id="_idIndexMarker419"/>the task of transferring grayscale images into colored ones. For example, this is essential to improve the photorealism of historical or old photographs. Adding color is important to make these photos more appealing and more emotionally engaging. DMs were shown to perform very well under this task. For further information, please read <em class="italic">Palette: Image-to-Image Diffusion </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2111.05826.pdf"><span class="No-Break">https://arxiv.org/pdf/2111.05826.pdf</span></a><span class="No-Break">).</span></li></ul></li>
<li><strong class="bold">Natural </strong><span class="No-Break"><strong class="bold">language processing</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Text generation</strong>: Virtual assistants, chatbots, and similar conversational text-based <a id="_idIndexMarker420"/>systems rely on text generation. DMs have been utilized recently for this task to <a id="_idIndexMarker421"/>improve the quality and diversity, as they are more capable of capturing complex distributions. For an example, please refer to <em class="italic">DiffuSeq: Sequence to Sequence Text Generation with Diffusion </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2210.08933"><span class="No-Break">https://arxiv.org/abs/2210.08933</span></a><span class="No-Break">).</span></li><li><strong class="bold">Text-to-speech synthesis</strong>: This is the process of transforming text into audio. While <a id="_idIndexMarker422"/>it has many <a id="_idIndexMarker423"/>applications in fields such as <strong class="bold">Human-Computer Interaction</strong> (<strong class="bold">HCI</strong>), education and learning, and video games, it has been recently utilized to make textual content accessible to individuals with visual impairment. For more details about DM and text-to-speech synthesis, please refer to <em class="italic">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</em> (<a href="https://arxiv.org/pdf/2104.01409.pdf">https://arxiv.org/pdf/2104.01409.pdf</a>) and <em class="italic">Prodiff: Progressive fast diffusion model for high-quality text-to-speech</em> (<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547855">https://dl.acm.org/doi/abs/10.1145/3503161.3547855</a>). Additionally, for a survey of recent text-to-speech DM-based methods, please refer to <em class="italic">A Survey on Audio Diffusion Models: Text to Speech Synthesis and Enhancement in Generative </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2303.13336.pdf"><span class="No-Break">https://arxiv.org/pdf/2303.13336.pdf</span></a><span class="No-Break">).</span></li><li><strong class="bold">Text-driven image generation</strong>: This is another promising field where the aim is <a id="_idIndexMarker424"/>to generate visual content, such as <a id="_idIndexMarker425"/>images based on textual input. It has various applications in content generation, marketing, data augmentation, and assisted data-labeling tools. As expected, DMs are excellent at modeling complex data distribution and very powerful at generating diverse and appealing images. Thus, they have been utilized for text-driven image generation. For more details, please refer to <em class="italic">Text2Human: text-driven controllable human image </em><span class="No-Break"><em class="italic">generation</em></span><span class="No-Break"> (</span><a href="https://dl.acm.org/doi/abs/10.1145/3528223.3530104"><span class="No-Break">https://dl.acm.org/doi/abs/10.1145/3528223.3530104</span></a><span class="No-Break">).</span></li></ul></li>
<li><span class="No-Break"><strong class="bold">Other applications</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Privacy in healthcare</strong>: Real <strong class="bold">Electronic Health Records</strong> (<strong class="bold">EHRs</strong>) of patients <a id="_idIndexMarker426"/>carry rich and very useful <a id="_idIndexMarker427"/>information that ML models can leverage to help in disease diagnosis, predictive analytics, decision-making, and the optimization and management of resources. DMs were shown to generate high-quality and large-scale EHRs that can be leveraged in enormous applications, such as ML model training, healthcare research, and medical education. To delve into more details, please read <em class="italic">MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion </em><span class="No-Break"><em class="italic">Model</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2302.04355.pdf"><span class="No-Break">https://arxiv.org/pdf/2302.04355.pdf</span></a><span class="No-Break">).</span></li><li><strong class="bold">Anomaly detection</strong>: This is the task of detecting or identifying patterns and instances that are not in line with the expected behavior and distribution. It has a myriad of applications in cybersecurity, fraud detection, telecommunication, and manufacturing. DMs are usually robust to noise and more stable, which makes them ideal for these applications. For an example and more details about utilizing DMs for anomaly detection in healthcare, please refer to <em class="italic">Diffusion models for medical anomaly </em><span class="No-Break"><em class="italic">detection</em></span><span class="No-Break"> (</span><a href="https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4"><span class="No-Break">https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4</span></a><span class="No-Break">).</span></li><li><strong class="bold">Text-to-motion</strong>: Generating animation or motion from textual input has many applications in the training, education, media, and entertainment sectors. DMs have shown promising results by producing high-quality animations of human motion. For more details, please refer to <em class="italic">Human Motion Diffusion </em><span class="No-Break"><em class="italic">Model</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2209.14916"><span class="No-Break">https://arxiv.org/abs/2209.14916</span></a><span class="No-Break">).</span></li></ul></li>
</ul>
<p>Now that we have an idea of the domains where diffusion models are used, in the next section, we closely examine the pros and cons <span class="No-Break">of DMs.</span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor160"/>Diffusion models – the pros and cons</h1>
<p>In this section, you will learn about and examine the main pros and cons of using DMs for synthetic data <a id="_idIndexMarker428"/>generation. This will help you to weigh the advantages and disadvantages of each synthetic data generation method. Consequently, it will give you the wisdom to select the best approach for your <span class="No-Break">own problems.</span></p>
<p>As we learned in <a href="B18494_07.xhtml#_idTextAnchor120"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, GANs work very well for certain applications, such as style transfer and image-to-image translation, but they are usually very hard to train and unstable. Additionally, the generated synthetic samples are usually less diverse and photorealistic. Conversely, recent papers have shown that DM-based synthetic data generation approaches surpass GANs on many benchmarks. For more details, please refer to <em class="italic">Diffusion Models Beat GANs on Image Synthesis </em>(<a href="https://arxiv.org/pdf/2105.05233.pdf">https://arxiv.org/pdf/2105.05233.pdf</a>). Like any other synthetic data generation approach, DMs have pros and cons. Thus, you need to consider them carefully for your particular application or problem. Then, you can select the best approach to generate the synthetic data that you want. With that aim in mind, we will examine the key advantages and disadvantages of <span class="No-Break">using DMs.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor161"/>The pros of using DMs</h2>
<p>In general, DMs are excellent at modeling complex data probability distributions and capturing temporal dependencies and hidden patterns. This is possible with DMs because they use a diffusion process to model data distributions, using a sequence of conditional distributions. Thus, we can highlight the main strengths and merits of DMs <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Generalizability and applicability to a wide range of problems</strong>: Unlike other generative methods, which are limited to image and video generation, DMs can be utilized to generate images, audio, videos, texts, molecular structures, and many other data types <span class="No-Break">and modalities</span></li>
<li><strong class="bold">Stability in the training process</strong>: The architecture, the training process, and the optimization technique of DMs make them more stable compared to other generative models such <span class="No-Break">as GANs</span></li>
<li><strong class="bold">High-quality synthetic data generation</strong>: Due to their distinctive architecture <a id="_idIndexMarker429"/>and innovative gradual and iterative training process, DMs generate high-quality synthetic data that surpasses other generative models such as VAEs <span class="No-Break">and GANs</span></li>
</ul>
<h2 id="_idParaDest-159"><a id="_idTextAnchor162"/>The cons of using DMS</h2>
<p>The two <a id="_idIndexMarker430"/>main limitations and shortcomings of using DMs can be described in terms of the computational complexity of the training and inference process, as well as the large-scale training datasets required <span class="No-Break">by DMs:</span></p>
<ul>
<li><strong class="bold">Computational complexity</strong>: DMs are computationally heavy. They are usually slow compared to other generative models, as the forward and reverse diffusion processes are composed of hundreds of steps (usually, the number of steps, <img alt="" height="26" src="image/B18494_F09_011.png" width="26"/>, is close <span class="No-Break">to <img alt="" height="33" src="image/B18494_F09_012.png" width="82"/>).</span></li>
<li><strong class="bold">More training data is required</strong>: DMs require large-scale training datasets to converge. Obtaining such datasets is not suitable for certain fields, which limits the usability of DMs for <span class="No-Break">certain applications.</span></li>
</ul>
<p>Now, we can clearly identify the pros and cons of using DMs for synthetic data generation. Let’s practice utilizing DMs to generate synthetic data to train <span class="No-Break">ML models.</span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor163"/>Hands-on diffusion models in practice</h1>
<p>Let’s study a practical example that demonstrates the usability of synthetic data in the computer <a id="_idIndexMarker431"/>vision field. For that aim, we will generate and prepare our dataset, build our ML model from scratch, train it, and evaluate its performance. The dataset is available at <em class="italic">Kaggle</em> (<a href="https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih">https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih</a>). The full code, the trained model, and the results are available on GitHub under the corresponding chapter folder in the <span class="No-Break">book’s repository.</span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor164"/>Context</h2>
<p>We want <a id="_idIndexMarker432"/>to build an ML model that can classify car images into two distinct categories – images depicting car accidents and those that do not. As you can imagine, curating such a real dataset is time-consuming and error-prone. It could be easy to collect car images without accidents. However, collecting images of cars with accidents, collisions, fires, and other dangerous scenarios is extremely hard. To solve this problem and to prove the usability of synthetic data, let’s first generate our training dataset. We can use a single synthetic data generation <a id="_idIndexMarker433"/>approach for that aim. However, let’s combine different methods and tools to collect more diverse data and practice different approaches as well. In this example, we will use the <span class="No-Break">following methods:</span></p>
<ul>
<li>The DALL·E 2 <span class="No-Break">image generator</span></li>
<li>The DeepAI <span class="No-Break">text-to-image generator</span></li>
<li>A simulator built using a game engine such <span class="No-Break">as Silver</span></li>
</ul>
<h2 id="_idParaDest-162"><a id="_idTextAnchor165"/>Dataset</h2>
<p>First, let’s <a id="_idIndexMarker434"/>leverage the remarkable capabilities of recent generative models such as <em class="italic">DALL·E 2</em> (<a href="https://openai.com/dall-e-2">https://openai.com/dall-e-2</a>) to generate some car <span class="No-Break">accident images.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<img alt="Figure 9.2 – Generating synthetic images using the DALL·E 2 web application" height="398" src="image/Figure_09_02_B18494.jpg" width="1172"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Generating synthetic images using the DALL·E 2 web application</p>
<p>We can simply use the following prompts to generate these images (see <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">):</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">Car accidents</strong></span></li>
<li><strong class="source-inline">White </strong><span class="No-Break"><strong class="source-inline">car accidents</strong></span></li>
<li><strong class="source-inline">Red </strong><span class="No-Break"><strong class="source-inline">car accidents</strong></span></li>
<li><strong class="source-inline">Blue </strong><span class="No-Break"><strong class="source-inline">car accidents</strong></span></li>
<li><strong class="source-inline">Ambulance </strong><span class="No-Break"><strong class="source-inline">car accident</strong></span></li>
</ul>
<p>As you can see from <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em>, the generated images look photorealistic and diverse, which is exactly what we need to train our <span class="No-Break">ML model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="Figure 9.3 – Car accident images generated using DALL·E 2" height="770" src="image/Figure_09_03_B18494.jpg" width="885"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Car accident images generated using DALL·E 2</p>
<p>We also <a id="_idIndexMarker435"/>use images collected from the <em class="italic">car-accident(resnet)</em> dataset, licensed under CC BY 4.0 (<a href="https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei">https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei</a>) using <a id="_idIndexMarker436"/>Roboflow (<a href="https://roboflow.com">https://roboflow.com</a>). We chose this dataset as the images are similar to what they would be if they were sourced from a video game, such as <em class="italic">BeamNG Drive Crashes</em> (<a href="https://www.beamng.com/game">https://www.beamng.com/game</a>). We will add these images to our dataset to further improve its diversity. A sample of these images is shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 9.4 – Car accident images collected from a video game" height="768" src="image/Figure_09_04_B18494.jpg" width="1114"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Car accident images collected from a video game</p>
<p>Then, we <a id="_idIndexMarker437"/>need to generate similar images for the other category, images of cars with no accidents. This time, let’s use the <em class="italic">DeepAI</em> tool to generate these images. As did earlier, we can simply use the following prompts to get the <span class="No-Break">required images:</span></p>
<ul>
<li><strong class="source-inline">Car </strong><span class="No-Break"><strong class="source-inline">under rain</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">White car</strong></span></li>
<li><strong class="source-inline">Car </strong><span class="No-Break"><strong class="source-inline">under fog</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">Blue car</strong></span></li>
</ul>
<p>As we can see in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em>, we effortlessly obtained another 30 car images with <span class="No-Break">no accidents.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 9.5 – Intact car images generated using DeepAI" height="715" src="image/Figure_09_05_B18494.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Intact car images generated using DeepAI</p>
<p>Now, we have 60 synthetic images, but we still need more images to appropriately train our ML <a id="_idIndexMarker438"/>model. We can generate any number of images using the previous generative models, but let’s explore another way – using the <em class="italic">Silver</em> <span class="No-Break">simulator (</span><a href="https://github.com/lsmcolab/Silver"><span class="No-Break">https://github.com/lsmcolab/Silver</span></a><span class="No-Break">).</span></p>
<p>By specifying the number of images that we want, we can generate the following images for this category (<span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 9.6 – Intact car images generated using Silver" height="712" src="image/Figure_09_06_B18494.jpg" width="824"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Intact car images generated using Silver</p>
<p>At this <a id="_idIndexMarker439"/>point, we have collected <strong class="source-inline">600</strong> synthetic images. Now, to assess the performance of our trained model, we should test it on real car images. Let’s collect real images using the <em class="italic">Unsplash</em> website (<a href="https://unsplash.com">https://unsplash.com</a>). To further improve our dataset, let’s also manually add images using Roboflow. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em> shows a sample of <span class="No-Break">these images.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 9.7 – Sample images of our real dataset" height="709" src="image/Figure_09_07_B18494.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Sample images of our real dataset</p>
<p>Finally, our <a id="_idIndexMarker440"/>dataset is composed of <strong class="source-inline">600</strong> synthetic images and <strong class="source-inline">250</strong> real ones, as shown in <span class="No-Break"><em class="italic">Table 9.1</em></span><span class="No-Break">.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Split</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Synthetic</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Real</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Training</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">540</span></p>
</td>
<td class="No-Table-Style">
<p>-</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Validation</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">62</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Test</span></p>
</td>
<td class="No-Table-Style">
<p>-</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">188</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Total</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">600</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">250</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – Our final dataset’s splits and number of images</p>
<p>Note that we will train only on synthetic data and test only on real data. Also, note in the validation split that we used both synthetic and real data because we are training and testing on two different domains – synthetic and real. Thus, a balanced mixture of data from both domains is necessary to give a good understanding of our model’s learning of the synthetic data and generalizability to the <span class="No-Break">real domain.</span></p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor166"/>ML model</h2>
<p>Our ML <a id="_idIndexMarker441"/>model is composed of four convolutional and <a id="_idIndexMarker442"/>three fully connected layers, max pooling, dropout, and batch <a id="_idIndexMarker443"/>normalization. We will use the <strong class="bold">Adam</strong> optimizer, which is an extension of <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>). To learn more about this optimization technique, please refer to <em class="italic">Adam: A Method for Stochastic Optimization</em> (<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>). Additionally, since the problem (as mentioned in the <em class="italic">Context</em> section) is a binary classification, we will deploy <strong class="bold">Binary Cross Entropy</strong> (<strong class="bold">BCE</strong>) loss. Refer <a id="_idIndexMarker444"/>to the <strong class="source-inline">train.py</strong> file in the corresponding chapter’s folder of the book’s <span class="No-Break">GitHub repository.</span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor167"/>Training</h2>
<p>We train <a id="_idIndexMarker445"/>our model from scratch on the training split shown in <em class="italic">Table 9.1</em> for <strong class="source-inline">30</strong> epochs. Then, we will select the best model using the validation split. The training loss is shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.8</em>. The loss is smoothly decreasing, as expected, which means that our model trains well on our synthetic <span class="No-Break">training dataset.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 9.8 – The training loss during the training stage" height="807" src="image/Figure_09_08_B18494.jpg" width="1016"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – The training loss during the training stage</p>
<p>From the <a id="_idIndexMarker446"/>validation accuracy shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.9</em>, we can see that our model achieved the best results on the validation set at epoch <strong class="source-inline">27</strong>. Thus, we will use this model <span class="No-Break">for testing.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 9.9 – Validation accuracy during the training stage" height="810" src="image/Figure_09_09_B18494.jpg" width="1021"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Validation accuracy during the training stage</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor168"/>Testing</h2>
<p>Let’s see <a id="_idIndexMarker447"/>the performance of our best model on the test split. As you can see from the confusion matrix shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.10</em>, our model classifies no-accident cars with an accuracy of <em class="italic">84%</em>, while it classifies accidents with an accuracy of <em class="italic">74%</em>. Our model achieved a total accuracy of <em class="italic">80%</em>, which is excellent given that our model was trained only on <span class="No-Break">synthetic data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 9.10 – The confusion matrix illustrating classification results" height="182" src="image/Figure_09_10_B18494.jpg" width="200"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – The confusion matrix illustrating classification results</p>
<p>We have <a id="_idIndexMarker448"/>provided a detailed example of how to generate and leverage synthetic data for training ML models. Now, let’s delve into DMs and the main <span class="No-Break">ethical issues.</span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor169"/>Diffusion models – ethical issues</h1>
<p>In this section, you will learn about the main ethical issues associated with using DMs for synthetic <span class="No-Break">data generation.</span></p>
<p>Diffusion-based <a id="_idIndexMarker449"/>generative models are emerging and powerful technologies. Thus, their pros and cons need to be considered carefully. Their advantages are huge for businesses, industry, and research. However, they possess dangerous capabilities that can be leveraged to cause harm to individuals, businesses, societies, and <span class="No-Break">so on.</span></p>
<p>Let’s list the main ethical issues usually associated with generative models and, <span class="No-Break">especially, DMs:</span></p>
<ul>
<li><span class="No-Break">Copyright</span></li>
<li><span class="No-Break">Bias</span></li>
<li><span class="No-Break">Inappropriate content</span></li>
<li><span class="No-Break">Responsibility issues</span></li>
<li><span class="No-Break">Privacy issues</span></li>
<li>Fraud and <span class="No-Break">identity theft</span></li>
</ul>
<p>Now, let’s delve <a id="_idIndexMarker450"/>into some of the main ethical issues behind using DMs <span class="No-Break">in practice.</span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor170"/>Copyright</h2>
<p>DMs are usually trained on large-scale real datasets. For example, DALL E 2 was trained on more <a id="_idIndexMarker451"/>than 650 million text-image pairs. Thus, obtaining permission from the owners of these data samples, such as images, artworks, video clips, and others, is not feasible. Additionally, DMs may misrepresent or represent copyrighted and intellectual properties with minor changes. For example, <em class="italic">Universal Music</em> asked streaming services and companies to prevent AI/ML companies from accessing their songs for training (<a href="https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs">https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs</a>). Thus, it creates many difficult questions for regulators to address and regulations for companies to comply with. Taking into account the huge, rapid progress in this field, the regulations may not be able to provide an ethical frame to control <span class="No-Break">copyright issues.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor171"/>Bias</h2>
<p>As we have <a id="_idIndexMarker452"/>outlined in this chapter, DMs learn to generate synthetic data following the same distribution of the training data. Thus, if the training data is biased, the DM will also generate biased synthetic data. What is more difficult with these complex models is assessing this bias compared to examining the raw training data, such as images and videos. It becomes even more complex and severe when these models are leveraged in decision-making by non-experts. For example, a professional comic artist may identify gender, political, and age biases within a comic book. However, training a DM on millions of comic books and then assessing the biases of the generated comic books, using this model, may not <span class="No-Break">be possible!</span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor172"/>Inappropriate content</h2>
<p>DMs may <a id="_idIndexMarker453"/>generate inappropriate content without users’ consent. It may include inappropriate language, sexual and violent content, hate speech, and racism. Thus, DMs need to utilize an effective filtering mechanism to discard unsuitable data. Simultaneously, they need to utilize a safeguarding procedure to prevent generating <span class="No-Break">inappropriate content.</span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor173"/>Responsibility</h2>
<p>Utilizing generative <a id="_idIndexMarker454"/>models such as DMs in decision-making will be almost unavoidable in the future. However, it is not possible, with current DMs, to understand why a certain decision was made. Thus, it is challenging to understand who is responsible for wrong decisions made by DMs that may cause death, injuries, or damage to properties. Thus, we need to develop suitable mechanisms to ensure transparency, accountability, and tracking in the process <span class="No-Break">of decision-making.</span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor174"/>Privacy</h2>
<p>DMs may <a id="_idIndexMarker455"/>reveal sensitive information about individuals and organizations since the generated synthetic data still follows the statistical properties of the original real training data. These models may disclose information that could be utilized by a third party to cause harm, loss, or unwanted consequences. For example, ChatGPT was banned in Italy due to <span class="No-Break">privacy issues.</span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor175"/>Fraud and identity theft</h2>
<p>DMs are <a id="_idIndexMarker456"/>powerful and capable of mimicking the human voice, photos, and videos. Thus, generated fake media can be exploited for many fraudulent purposes, such as accessing personal information, money laundering, credit card fraud, and cybercrime. Furthermore, DMs can be used to impersonate a celebrity, activist, or politician to get access to private information, classified material, <span class="No-Break">and documents.</span></p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor176"/>Summary</h1>
<p>In this chapter, we introduced a novel and powerful method to generate synthetic data – using DMs. We compared DMs to other state-of-the-art generative models, and then, we highlighted the training process of DMs. Furthermore, we discussed the pros and cons of utilizing DMs. Additionally, we learned how to generate and utilize synthetic data in practice. We also examined the main ethical considerations usually raised when deploying DMs for synthetic data generation. You developed a comprehensive understanding of generative models, and you learned about standard DM architecture, the training process, and the main advantages, benefits, and limitations of utilizing DMs in practice. In the next chapter, we will shed light on several case studies, highlighting how synthetic data has been successfully utilized to improve computer vision solutions in practice. The chapter aims to inspire and motivate you to explore the potential of synthetic data in your <span class="No-Break">own applications.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer077">
<h1 id="_idParaDest-174" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor177"/>Part 4:Case Studies and Best Practices</h1>
<p>In this part, you will be introduced to rich and diverse case studies in three cutting-edge areas in the field of ML: <strong class="bold">Computer Vision</strong> (<strong class="bold">CV</strong>), <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>), and <strong class="bold">Predictive Analytics</strong> (<strong class="bold">PA</strong>). You will comprehend the benefits of employing synthetic data in these fields and identify the main challenges and issues. Following this, you will learn about best practices that improve the usability of synthetic data <span class="No-Break">in practice.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18494_10.xhtml#_idTextAnchor178"><em class="italic">Chapter 10</em></a>, <em class="italic">Case Study 1 – Computer Vision</em></li>
<li><a href="B18494_11.xhtml#_idTextAnchor188"><em class="italic">Chapter 11</em></a>, <em class="italic">Case Study 2 – Natural Language Processing</em></li>
<li><a href="B18494_12.xhtml#_idTextAnchor203"><em class="italic">Chapter 12</em></a>, <em class="italic">Case Study 3 – Predictive Analytics</em></li>
<li><a href="B18494_13.xhtml#_idTextAnchor216"><em class="italic">Chapter 13</em></a>, <em class="italic">Best Practices for Applying Synthetic Data</em></li>
</ul>
</div>
<div>
<div id="_idContainer078">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer079">
</div>
</div>
</div></body></html>