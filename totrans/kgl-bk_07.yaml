- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Competition Tasks and Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a competition, you start by examining the target metric. Understanding how
    your model’s errors are evaluated is key for scoring highly in every competition.
    When your predictions are submitted to the Kaggle platform, they are compared
    to a ground truth based on the target metric.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the *Titanic* competition ([https://www.kaggle.com/c/titanic/](https://www.kaggle.com/c/titanic/)),
    all your submissions are evaluated based on *accuracy*, the percentage of surviving
    passengers you correctly predict. The organizers decided upon this metric because
    the aim of the competition is to find a model that estimates the probability of
    survival of a passenger under similar circumstances. In another knowledge competition,
    *House Prices - Advanced Regression Techniques* ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)),
    your work will be evaluated based on an *average difference* between your prediction
    and the ground truth. This involves computing the logarithm, squaring, and taking
    the square root, because the model is expected to be able to quantify as correctly
    as possible the order of the price of a house on sale.
  prefs: []
  type: TYPE_NORMAL
- en: In real-world data science, target metrics are also key for the success of your
    project, though there are certainly differences between the real world and a Kaggle
    competition. We could easily summarize by saying that there are more complexities
    in the real world. In real-world projects, you will often have not just one but
    multiple metrics that your model will be evaluated against. Frequently, some of
    the evaluation metrics won’t even be related to how your predictions perform against
    the ground truth you are using for testing. For instance, the domain of knowledge
    you are working in, the scope of the project, the number of features considered
    by your model, the overall memory usage, any requirement for special hardware
    (such as a GPU, for instance), the latency of the prediction process, the complexity
    of the predicting model, and many other aspects may end up counting more than
    the mere predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world problems are indeed dominated by business and tech infrastructure
    concerns much more than you may imagine before being involved in any of them.
  prefs: []
  type: TYPE_NORMAL
- en: Yet you cannot escape the fact that the basic principle at the core of both
    real-world projects and Kaggle competitions is the same. Your work will be evaluated
    according to some criteria, and understanding the details of such criteria, optimizing
    the fit of your model in a smart way, or selecting its parameters according to
    the criteria will bring you success. If you can learn more about how model evaluation
    occurs in Kaggle, your real-world data science job will also benefit from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to detail how evaluation metrics for certain
    kinds of problems strongly influence the way you can operate when building your
    model solution in a data science competition. We also address the variety of metrics
    available in Kaggle competitions to give you an idea of what matters most and,
    in the margins, we discuss the different effects of metrics on predictive performance
    and how to correctly translate them into your projects. We will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics and objective functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic types of tasks: regression, classification, and ordinal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Meta Kaggle dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling never-before-seen metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for regression (standard and ordinal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for binary classification (label prediction and probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for multi-class classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for object detection problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for multi-label classification and recommendation problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation metrics and objective functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a Kaggle competition, you can find the evaluation metric in the left menu
    on the **Overview** page of the competition. By selecting the **Evaluation** tab,
    you will get details about the evaluation metric. Sometimes you will find the
    metric formula, the code to reproduce it, and some discussion of the metric. On
    the same page, you will also get an explanation about the submission file format,
    providing you with the header of the file and a few example rows.
  prefs: []
  type: TYPE_NORMAL
- en: The association between the evaluation metric and the submission file is important,
    because you have to consider that the metric works essentially after you have
    trained your model and produced some predictions. Consequently, as a first step,
    you will have to think about the difference between an **evaluation metric** and
    an **objective function**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boiling everything down to the basics, an objective function serves your model
    during training because it is involved in the process of error minimization (or
    score maximization, depending on the problem). In contrast, an evaluation metric
    serves your model *after* it has been trained by providing a score. Therefore,
    it cannot influence how the model fits the data, but does influence it in an indirect
    way: by helping you to select the most well-performing hyperparameter settings
    within a model, and the best models among competing ones. Before proceeding with
    the rest of the chapter, which will show you how this can affect a Kaggle competition
    and why the analysis of the Kaggle evaluation metric should be your first act
    in a competition, let’s first discuss some terminology that you may encounter
    in the discussion forums.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will often hear talk about objective functions, cost functions, and loss
    functions, sometimes interchangeably. They are not exactly the same thing, however,
    and we explain the distinction here:'
  prefs: []
  type: TYPE_NORMAL
- en: A **loss function** is a function that is defined on a single data point, and,
    considering the prediction of the model and the ground truth for the data point,
    computes a penalty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **cost function** takes into account the whole dataset used for training (or
    a batch from it), computing a sum or average over the loss penalties of its data
    points. It can comprise further constraints, such as the L1 or L2 penalties, for
    instance. The cost function directly affects how the training happens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An **objective function** is the most general (and safe-to-use) term related
    to the scope of optimization during machine learning training: it comprises cost
    functions, but it is not limited to them. An objective function, in fact, can
    also take into account goals that are not related to the target: for instance,
    requiring sparse coefficients of the estimated model or a minimization of the
    coefficients’ values, such as in L1 and L2 regularizations. Moreover, whereas
    loss and cost functions imply an optimization based on minimization, an objective
    function is neutral and can imply either a maximization or a minimization activity
    performed by the learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Likewise, when it comes to evaluation metrics, you’ll hear about scoring functions
    and error functions. Distinguishing between them is easy: a **scoring function**
    suggests better prediction results if scores from the function are higher, implying
    a maximization process.'
  prefs: []
  type: TYPE_NORMAL
- en: An **error function** instead suggests better predictions if smaller error quantities
    are reported by the function, implying a minimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Basic types of tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Not all objective functions are suitable for all problems. From a general point
    of view, you’ll find two kinds of problems in Kaggle competitions: **regression**
    tasks and **classification** tasks. Recently, there have also been **reinforcement
    learning** (**RL**) tasks, but RL doesn’t use metrics for evaluation; instead,
    it relies on a ranking derived from direct match-ups against other competitors
    whose solutions are assumed to be as well-performing as yours (performing better
    in this match-up than your peers will raise your ranking, performing worse will
    lower it). Since RL doesn’t use metrics, we will keep on referring to the regression-classification
    dichotomy, though **ordinal** tasks, where you predict ordered labels represented
    by integer numbers, may elude such categorization and can be dealt with successfully
    either using a regression or classification approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Regression** requires you to build a model that can predict a real number;
    often a positive number, but there have been examples of negative number prediction
    too. A classic example of a regression problem is *House Prices - Advanced Regression
    Techniques*, because you have to guess the value of a house. The evaluation of
    a regression task involves computing a distance between your predictions and the
    values of the ground truth. This difference can be evaluated in different ways,
    for instance by squaring it in order to punish larger errors, or by applying a
    log to it in order to penalize predictions of the wrong scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When facing a **classification** task on Kaggle, there are more nuances to take
    into account. The classification, in fact, could be **binary**, **multi-class**,
    or **multi-label**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In **binary** problems, you have to guess if an example should be classified
    or not into a specific class (usually called the *positive* class and compared
    to the *negative* one). Here, the evaluation could involve the straightforward
    prediction of the class ownership itself, or an estimation of the probability
    of such ownership. A typical example is the *Titanic* competition, where you have
    to guess a binary outcome: survival or not-survival. In this case, the requirement
    of the competition is just the prediction, but in many cases, it is necessary
    to provide a probability because in certain fields, especially for medical applications,
    it is necessary to rank positive predictions across different options and situations
    in order to make the best decision.'
  prefs: []
  type: TYPE_NORMAL
- en: Though counting the exact number of correct matches in a binary classification
    may seem a valid approach, this won’t actually work well when there is an imbalance,
    that is, a different number of examples, between the positive and the negative
    class. Classification based on an imbalanced distribution of classes requires
    evaluation metrics that take the imbalance into account, if you want to correctly
    track improvements on your model.
  prefs: []
  type: TYPE_NORMAL
- en: When you have more than two classes, you have a **multi-class** prediction problem.
    This also requires the use of suitable functions for evaluation, since it is necessary
    to keep track of the overall performance of the model, but also to ensure that
    the performance across the classes is comparable (for instance, your model could
    underperform with respect to certain classes). Here, each case can be in one class
    exclusively, and not in any others. A good example is *Leaf Classification* ([https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification)),
    where each image of a leaf specimen has to be associated with the correct plant
    species.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when your class predictions are not exclusive and you can predict multiple
    class ownership for each example, you have a **multi-label** problem that requires
    further evaluations in order to control whether your model is predicting the correct
    classes, as well as the correct number and mix of classes. For instance, in *Greek
    Media Monitoring Multilabel Classification (WISE 2014)* ([https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)),
    you had to associate each article with all the topics it deals with.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a problem involving a prediction on an ordinal scale, you have to guess integer
    numeric labels, which are naturally ordered. As an example, the magnitude of an
    earthquake is on an ordinal scale. In addition, data from marketing research questionnaires
    is often recorded on ordinal scales (for instance, consumers’ preferences or opinion
    agreement). Since an ordinal scale is made of ordered values, ordinal tasks can
    be considered somewhat halfway between regression and classification, and you
    can solve them in both ways.
  prefs: []
  type: TYPE_NORMAL
- en: The most common way is to treat your ordinal task as a **multi-class** problem.
    In this case, you will get a prediction of an integer value (the class label)
    but the prediction will not take into account that the classes have a certain
    order. You can get a feeling that there is something wrong with approaching the
    problem as a multi-class problem if you look at the prediction probability for
    the classes. Often, probabilities will be distributed across the entire range
    of possible values, depicting a multi-modal and often asymmetric distribution
    (whereas you should expect a Gaussian distribution around the maximum probability
    class).
  prefs: []
  type: TYPE_NORMAL
- en: The other way to solve the ordinal prediction problem is to treat it as a **regression**
    problem and then post-process your result. In this way, the order among classes
    will be taken into consideration, though the prediction output won’t be immediately
    useful for scoring on the evaluation metric. In fact, in a regression you get
    a float number as an output, not an integer representing an ordinal class; moreover,
    the result will include the full range of values between the integers of your
    ordinal distribution and possibly also values outside of it. Cropping the output
    values and casting them into integers by unit rounding may do the trick, but this
    might lead to inaccuracies requiring some more sophisticated post-processing (we’ll
    discuss more on this later in the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Now, you may be wondering what kind of evaluation you should master in order
    to succeed in Kaggle. Clearly, you always have to master the evaluation metric
    of the competition you have taken on. However, some metrics are more common than
    others, which is information you can use to your advantage. What are the most
    common metrics? How can we figure out where to look for insights in competitions
    that have used similar evaluation metrics? The answer is to consult the Meta Kaggle
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Meta Kaggle dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    is a collection of rich data about Kaggle’s community and activity, published
    by Kaggle itself as a public dataset. It contains CSV tables filled with public
    activity from Competitions, Datasets, Notebooks, and Discussions. All you have
    to do is to start a Kaggle Notebook (as you saw in *Chapters 2* and *3*), add
    to it the Meta Kaggle dataset, and start analyzing the data. The CSV tables are
    updated daily, so you’ll have to refresh your analysis often, but that’s worth
    it given the insights you can extract.
  prefs: []
  type: TYPE_NORMAL
- en: We will sometimes refer to the Meta Kaggle dataset in this book, both as inspiration
    for many interesting examples of the dynamics in a competition and as a way to
    pick up useful examples for your learning and competition strategies. Here, we
    are going to use it in order to figure out what evaluation metrics have been used
    most frequently for competitions in the last seven years. By looking at the most
    common ones in this chapter, you’ll be able to start any competition from solid
    ground and then refine your knowledge of the metric, picking up competition-specific
    nuances using the discussion you find in the forums.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we introduce the code necessary to produce a data table of metrics and
    their counts per year. It is designed to run directly on the Kaggle platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we read in the CSV table containing the data relating to the competitions.
    We focus on the columns representing the evaluation and the columns informing
    us about the competition name, start date, and type. We limit the rows to those
    competitions held since 2015 and that are of the Featured or Research type (which
    are the most common ones). We complete the analysis by creating a pandas pivot
    table, combining the evaluation algorithm with the year, and counting the number
    of competitions using it. We just display the top 20 algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the resulting table (at the time of writing):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **year** | **2015** | **2016** | **2017** | **2018** | **2019** | **2020**
    | **2021** | **Tot** |'
  prefs: []
  type: TYPE_TB
- en: '| **Evaluation Algorithm** |'
  prefs: []
  type: TYPE_TB
- en: '| AUC | 4 | 4 | 1 | 3 | 3 | 2 | 0 | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| LogLoss | 2 | 2 | 5 | 2 | 3 | 2 | 0 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| MAP@{K} | 1 | 3 | 0 | 4 | 1 | 0 | 1 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| CategorizationAccuracy | 1 | 0 | 4 | 0 | 1 | 2 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| MulticlassLoss | 2 | 3 | 2 | 0 | 1 | 0 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| RMSLE | 2 | 1 | 3 | 1 | 1 | 0 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| QuadraticWeightedKappa | 3 | 0 | 0 | 1 | 2 | 1 | 0 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanFScoreBeta | 1 | 0 | 1 | 2 | 1 | 2 | 0 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanBestErrorAtK | 0 | 0 | 2 | 2 | 1 | 1 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| MCRMSLE | 0 | 0 | 1 | 0 | 0 | 5 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| MCAUC | 1 | 0 | 1 | 0 | 0 | 3 | 0 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | 1 | 1 | 0 | 3 | 0 | 0 | 0 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Dice | 0 | 1 | 1 | 0 | 2 | 1 | 0 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| GoogleGlobalAP | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| MacroFScore | 0 | 0 | 0 | 1 | 0 | 2 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Score | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| CRPS | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenImagesObjectDetectionAP | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanFScore | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| RSNAObjectDetectionAP | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Using the same variables we just instantiated in order to generate the table,
    you can also check the data to find the competitions where the metric of your
    choice has been adopted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the above snippet, we decided to represent the competitions that have been
    using the AUC metric. You just have to change the string representing the chosen
    metric and the resulting list will be updated accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the table generated, we can examine the most popular evaluation
    metrics used in competitions hosted on Kaggle:'
  prefs: []
  type: TYPE_NORMAL
- en: The two top metrics are closely related to each other and to binary probability
    classification problems. The **AUC** metric helps to measure if your model’s predicted
    probabilities tend to predict positive cases with high probabilities, and the
    **Log Loss** helps to measure how far your predicted probabilities are from the
    ground truth (and as you optimize for Log Loss, you also optimize for the AUC
    metric).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 3^(rd) position, we find **MAP@{K}**, which is a common metric in recommender
    systems and search engines. In Kaggle competitions, this metric has been used
    mostly for information retrieval evaluations, such as in the *Humpback Whale Identification*
    competition ([https://www.kaggle.com/c/humpback-whale-identification](https://www.kaggle.com/c/humpback-whale-identification)),
    where you have to precisely identify a whale and you have five possible guesses.
    Another example of MAP@{K} usage is in the *Quick, Draw! Doodle Recognition Challenge*
    ([https://www.kaggle.com/c/quickdraw-doodle-recognition/](https://www.kaggle.com/c/quickdraw-doodle-recognition/)),
    where your goal is to guess the content of a drawn sketch and you are allowed
    three attempts. In essence, when MAP@{K} is the evaluation metric, you can score
    not just if you can guess correctly, but also if your correct guess is among a
    certain number (the “K” in the name of the function) of other incorrect predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only in 6^(th) position can we find a regression metric, the **RMSLE**, or **Root
    Mean Squared Logarithmic Error**, and in 7^(th) place the **Quadratic Weighted
    Kappa**, a metric particularly useful for estimating model performance on problems
    that involve guessing a progressive integer number (an ordinal scale problem).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you skim through the list of top metrics, you will keep on finding metrics
    that are commonly discussed in machine learning textbooks. In the next few sections,
    after first discussing what to do when you meet a never-before-seen metric (something
    that happens in Kaggle competitions more frequently than you may expect), we will
    revise some of the most common metrics found in regression and classification
    competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Handling never-before-seen metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before proceeding, we have to consider that the top 20 table doesn’t cover all
    the metrics used in competitions. We should be aware that there are metrics that
    have only been used once in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s keep on using the results from the previous code to find out what they
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we get the following table showing, for each year, how many competitions
    used a metric that has never been used afterward (`n_comps`), and the proportion
    of these competitions per year (`pct_comps`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Observing the relative share of competitions with a never-to-be-seen-afterward
    metric, we immediately notice how it is growing year by year and that it reached
    the 25%-30% level in recent years, implying that typically one competition out
    of every three or four requires you to study and understand a metric from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the list of such metrics that have occurred in the past with a
    simple code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing the code, you will get a list similar to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By close inspection, you can find many metrics relating to deep learning and
    reinforcement learning competitions.
  prefs: []
  type: TYPE_NORMAL
- en: What do you do when you meet a metric that has never been used before? Of course,
    you can rely on the discussions in the Kaggle discussion forums, where you can
    always find good inspiration and many Kagglers who will help you. However, if
    you want to build up your own knowledge about the metric, aside from Googling
    it, we advise that you try to experiment with it by coding the evaluation function
    by yourself, even in an imperfect way, and trying to simulate how the metric reacts
    to different types of error produced by the model. You could also directly test
    how it functions on a sample from the competition training data or synthetic data
    that you have prepared.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quote a few examples of this approach as used by Kagglers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Carlo Lepelaars* with Spearman’s Rho: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho](https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlo Lepelaars with Quadratic Weighted Kappa: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa](https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rohan Rao* with Laplace Log Likelihood: [https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood](https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can give you increased insight into the evaluation and an advantage over
    other competitors relying only on answers from Googling and Kaggle forums.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Rohan_Rao.png)'
  prefs: []
  type: TYPE_IMG
- en: Rohan Rao
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/rohanrao](https://www.kaggle.com/rohanrao)'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start exploring different metrics, let’s catch up with Rohan Rao (aka
    Vopani) himself, Quadruple Grandmaster and Senior Data Scientist at `H2O.ai`,
    about his successes on Kaggle and the wisdom he has to share with us.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your speciality on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I like to dabble with different types of competitions, but my favorite would
    certainly be time series ones. I don’t quite like the typical approaches to and
    concepts of time series in the industry, so I tend to innovate and think out of
    the box by building solutions in an unorthodox way, which has ended up being very
    successful for me.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*For any Kaggle competition, my typical workflow would look like this:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Understand the problem statement and read all the information related to rules,
    format, timelines, datasets, metrics, and deliverables.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dive deep into the data. Slice and dice it in every way possible and explore/visualize
    it to be able to answer any question about it.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Build a simple pipeline with a baseline model and make a submission to confirm
    the process works.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Engineer features, tune hyperparameters, and* *experiment with multiple models
    to get a sense of what’s generally working and what’s not.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Constantly go back to analyzing the data, reading discussions on the forum,
    and tweaking the features and models to the fullest. Maybe team up at some point.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ensemble multiple models and decide which submissions to make as final.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*In my day-to-day work in data science, most of this happens too. But there
    are two crucial elements that are additionally required:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Curating and preparing datasets for the problem statement.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deploying the final model or solution into production.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The majority of my time has been spent in these two activities for most of
    the projects I’ve worked on in the past.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*The vast majority of everything I’ve learned in machine learning has come
    from Kaggle. The community, the platform, and the content are pure gold and there
    is an incredible amount of stuff you can learn.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*What has benefitted me the most is the experience of competing in Kaggle competitions;
    it has given me immense confidence in understanding, structuring, and solving
    problems across domains, which I have been able to apply successfully in many
    of the companies and projects I worked on outside Kaggle.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Many recruiters have contacted me for opportunities looking at my Kaggle achievements,
    primarily in Competitions. It gives a fairly good indication of a candidate’s
    ability in solving data science problems and hence it is a great platform to showcase
    your skills* *and build a portfolio.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*I’ve made some mistake in every competition! That’s how you learn and improve.
    Sometimes it’s a coding bug, sometimes a flawed validation setup, sometimes an
    incorrect submission selection!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*What’s important is to learn from these and ensure you don’t repeat them.
    Iterating over this process automatically helps to improve your overall performance
    on Kaggle.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis/machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*I strongly believe in never marrying a technology. Use whatever works best,
    whatever is most comfortable and effective, but constantly be open to learning
    new tools and libraries.*'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for regression (standard and ordinal)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with regression problems, that is, problems that involve estimating
    a continuous value (that could range from minus infinity to infinity), the most
    commonly used error measures are **RMSE** (**root mean squared error**) and **MAE**
    (**mean absolute error**), but you can also find slightly different error measures
    useful, such as RMSLE or MCRMSLE.
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error (MSE) and R squared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The root mean squared error is the root of the **mean squared error** (**MSE**),
    which is nothing else but the mean of the good old **sum of squared errors** (**SSE**)
    that you learned about when you studied how a regression works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the formula for the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s start by explaining how the formula works. First of all, *n* indicates
    the number of cases, ![](img/B17574_05_002.png) is the ground truth, and ![](img/B17574_05_003.png)
    the prediction. You first get the difference between your predictions and your
    real values. You square the differences (so they become positive or simply zero),
    then you sum them all, resulting in your SSE. Then you just have to divide this
    measure by the number of predictions to obtain the average value, the MSE. Usually,
    all regression models minimize the SSE, so you won’t have great problems trying
    to minimize MSE or its direct derivatives such as **R squared** (also called the
    **coefficient of determination**), which is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, SSE (the sum of squared errors) is compared to the **sum of squares total**
    (**SST**), which is just the variance of the response. In statistics, in fact,
    SST is defined as the squared difference between your target values and their
    mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: To put it another way, R squared compares the squared errors of the model against
    the squared errors from the simplest model possible, the average of the response.
    Since both SSE and SST have the same scale, R squared can help you to determine
    whether transforming your target is helping to obtain better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Please remember that linear transformations, such as minmax ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))
    or standardization ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),
    do not change the performance of any regressor, since they are linear transformations
    of the target. **Non-linear** transformations, such as the square root, the cubic
    root, the logarithm, the exponentiation, and their combinations, should instead
    definitely modify the performance of your regression model on the evaluation metric
    (hopefully for the better, if you decide on the right transformation).
  prefs: []
  type: TYPE_NORMAL
- en: MSE is a great instrument for comparing regression models applied to the same
    problem. The bad news is that the MSE is seldom used in Kaggle competitions, since
    RMSE is preferred. In fact, by taking the root of MSE, its value will resemble
    the original scale of your target and it will be easier at a glance to figure
    out if your model is doing a good job or not. In addition, if you are considering
    the same regression model across different data problems (for instance, across
    various datasets or data competitions), R squared is better because it is perfectly
    correlated with MSE and its values range between 0 and 1, making all comparisons
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error (RMSE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RMSE is just the square root of MSE, but this implies some subtle change. Here
    is its formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_006.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. In MSE, large
    prediction errors are greatly penalized because of the squaring activity. In RMSE,
    this dominance is lessened because of the root effect (however, you should always
    pay attention to outliers; they can affect your model performance a lot, no matter
    whether you are evaluating based on MSE or RMSE).
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, depending on the problem, you can get a better fit with an algorithm
    using MSE as an objective function by first applying the square root to your target
    (if possible, because it requires positive values), then squaring the results.
    Functions such as the `TransformedTargetRegressor` in Scikit-learn help you to
    appropriately transform your regression target in order to get better-fitting
    results with respect to your evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent competitions where RMSE has been used include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Avito Demand Prediction Challenge*: [https://www.kaggle.com/c/avito-demand-prediction](https://www.kaggle.com/c/avito-demand-prediction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Analytics Customer Revenue Prediction*: [https://www.kaggle.com/c/ga-customer-revenue-prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elo Merchant Category Recommendation* [https://www.kaggle.com/c/elo-merchant-category-recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root mean squared log error (RMSLE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another common transformation of MSE is **root mean squared log error** (**RMSLE**).
    MCRMSLE is just a variant made popular by the COVID-19 forecasting competitions,
    and it is the column-wise average of the RMSLE values of each single target when
    there are multiple ones. Here is the formula for RMSLE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_009.png)'
  prefs: []
  type: TYPE_IMG
- en: In the formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. Since you
    are applying a logarithmic transformation to your predictions and your ground
    truth before all the other squaring, averaging, and rooting operations, you don’t
    penalize huge differences between the predicted and the actual values, especially
    when both are large numbers. In other words, what you care the most about when
    using RMSLE is *the scale of your predictions with respect to the scale of the
    ground truth*. As with RMSE, machine learning algorithms for regression can better
    optimize for RMSLE if you apply a logarithmic transformation to the target before
    fitting it (and then reverse the effect using the exponential function).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent competitions using RMSLE as an evaluation metric are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ASHRAE - Great Energy Predictor III*: [https://www.kaggle.com/c/ashrae-energy-prediction](https://www.kaggle.com/c/ashrae-energy-prediction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Santander Value Prediction Challenge*: [https://www.kaggle.com/c/santander-value-prediction-challenge](https://www.kaggle.com/c/santander-value-prediction-challenge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mercari Price Suggestion Challenge*: [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sberbank Russian Housing Market*: [https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market](https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recruit Restaurant Visitor Forecasting*: [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By far, at the moment, RMSLE is the most used evaluation metric for regression
    in Kaggle competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error (MAE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **MAE** (**mean absolute error**) evaluation metric is the absolute value
    of the difference between the predictions and the targets. Here is the formulation
    of MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_012.png)'
  prefs: []
  type: TYPE_IMG
- en: In the formula, *n* stands for the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. MAE is not
    particularly sensitive to outliers (unlike MSE, where errors are squared), hence
    you may find it is an evaluation metric in many competitions whose datasets present
    outliers. Moreover, you can easily work with it since many algorithms can directly
    use it as an objective function; otherwise, you can optimize for it indirectly
    by just training on the square root of your target and then squaring the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of downside, using MAE as an objective function results in much slower
    convergence, since you are actually optimizing for predicting the median of the
    target (also called the L1 norm), instead of the mean (also called the L2 norm),
    as occurs by MSE minimization. This results in more complex computations for the
    optimizer, so the training time can even grow exponentially based on your number
    of training cases (see, for instance, this Stack Overflow question: [https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to](https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notable recent competitions that used MAE as an evaluation metric are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LANL Earthquake Prediction*: [https://www.kaggle.com/c/LANL-Earthquake-Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How Much Did It Rain? II*: [https://www.kaggle.com/c/how-much-did-it-rain-ii](https://www.kaggle.com/c/how-much-did-it-rain-ii)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having mentioned the ASHRAE competition earlier, we should also mention that
    regression evaluation measures are quite relevant to forecasting competitions.
    For instance, the M5 forecasting competition was held recently ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))
    and data from all the other M competitions is available too. If you are interested
    in forecasting competitions, of which there are a few on Kaggle, please see [https://robjhyndman.com/hyndsight/forecasting-competitions/](https://robjhyndman.com/hyndsight/forecasting-competitions/)
    for an overview about M competitions and how valuable Kaggle is for obtaining
    better practical and theoretical results from such competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, forecasting competitions do not require a very different evaluation
    to regression competitions. When dealing with forecasting tasks, it is true that
    you can get some unusual evaluation metrics such as the **Weighted Root Mean Squared
    Scaled Error** ([https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation))
    or the **symmetric mean absolute percentage error**, better known as **sMAPE**
    ([https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation](https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation)).
    However, in the end they are just variations of the usual RMSE or MAE that you
    can handle using the right target transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for classification (label prediction and probability)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having discussed the metrics for regression problems, we are going now to illustrate
    the metrics for classification problems, starting from the binary classification
    problems (when you have to predict between two classes), moving to the multi-class
    (when you have more than two classes), and then to the multi-label (when the classes
    overlap).
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When analyzing the performance of a binary classifier, the most common and
    accessible metric that is used is **accuracy**. A misclassification error is when
    your model predicts the wrong class for an example. The accuracy is just the complement
    of the misclassification error and it can be calculated as the ratio between the
    number of correct numbers divided by the number of answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_015.png)'
  prefs: []
  type: TYPE_IMG
- en: This metric has been used, for instance, in *Cassava Leaf Disease Classification*
    ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification))
    and *Text Normalization Challenge - English Language* ([https://www.kaggle.com/c/text-normalization-challenge-english-language](https://www.kaggle.com/c/text-normalization-challenge-english-language)),
    where you scored a correct prediction only if your predicted text matched the
    actual string.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a metric, the accuracy is focused strongly on the effective performance
    of the model in a real setting: it tells you if the model works as expected. However,
    if your purpose is to evaluate and compare and have a clear picture of how effective
    your approach really is, you have to be cautious when using the accuracy because
    it can lead to wrong conclusions when the classes are imbalanced (when they have
    different frequencies). For instance, if a certain class makes up just 10% of
    the data, a predictor that predicts nothing but the majority class will be 90%
    accurate, proving itself quite useless in spite of the high accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can you spot such a problem? You can do this easily by using a **confusion
    matrix**. In a confusion matrix, you create a two-way table comparing the actual
    classes on the rows against the predicted classes on the columns. You can create
    a straightforward one using the Scikit-learn `confusion_matrix` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Providing the `y_true` and `y_pred` vectors will suffice to return you a meaningful
    table, but you can also provide row/column labels and sample weights for the examples
    in consideration, and normalize (set the marginals to sum to 1) over the true
    examples (the rows), the predicted examples (the columns), or all the examples.
    A perfect classifier will have all the cases on the principal diagonal of the
    matrix. Serious problems with the validity of the predictor are highlighted if
    there are few or no cases on one of the cells of the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to give you a better idea of how it works, you can try the graphical
    example offered by Scikit-learn at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Confusion matrix, with each cell normalized to 1.00, to represent
    the share of matches'
  prefs: []
  type: TYPE_NORMAL
- en: You can attempt to improve the usability of the accuracy by considering the
    accuracy relative to each of the classes and averaging them, but you will find
    it more useful to rely on other metrics such as **precision**, **recall**, and
    the **F1-score**.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To obtain the precision and recall metrics, we again start from the confusion
    matrix. First, we have to name each of the cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted** |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **Negative** | **Positive** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual** | **Negative** | True Negative | False Positive |'
  prefs: []
  type: TYPE_TB
- en: '| **Positive** | False Negative | True Positive |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Confusion matrix with cell names'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we define the cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TP** (**true positives**): These are located in the upper-left cell, containing
    examples that have correctly been predicted as positive ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP** (**false positives**): These are located in the upper-right cell, containing
    examples that have been predicted as positive but are actually negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN** (**false negatives**): These are located in the lower-left cell, containing
    examples that have been predicted as negative but are actually positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TN** (**true negatives**): These are located in the lower-right cell, containing
    examples that have been correctly predicted as negative ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these cells, you can actually get more precise information about how
    your classifier works and how you can tune your model better. First, we can easily
    revise the accuracy formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the first informative metric is called **precision** (or **specificity**)
    and it is actually the accuracy of the positive cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_017.png)'
  prefs: []
  type: TYPE_IMG
- en: In the computation, only the number of true positives and the number of false
    positives are involved. In essence, the metric tells you how often you are correct
    when you predict a positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, your model could get high scores by predicting positives for only
    the examples it has high confidence in. That is actually the purpose of the measure:
    to force models to predict a positive class only when they are sure and it is
    safe to do so.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if it is in your interest also to predict as many positives as possible,
    then you’ll also need to watch over the **recall** (or **coverage** or **sensitivity**
    or even **true positive rate**) metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_018.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you will also need to know about false negatives. The interesting thing
    about these two metrics is that, since they are based on examples classification,
    and a classification is actually based on probability (which is usually set between
    the positive and negative class at the `0.5` threshold), you can change the threshold
    and have one of the two metrics improved at the expense of the other.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you increase the threshold, you will get more precision (the
    classifier is more confident of the prediction) but less recall. If you decrease
    the threshold, you get less precision but more recall. This is also called the
    **precision/recall trade-off**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scikit-learn website offers a simple and practical overview of this trade-off
    ([https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)),
    helping you to trace a **precision/recall curve** and thus understand how these
    two measures can be exchanged to obtain a result that better fits your needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: A two-class precision-recall curve with its characteristic steps'
  prefs: []
  type: TYPE_NORMAL
- en: One metric associated with the precision/recall trade-off is the **average precision**.
    Average precision computes the mean precision for recall values from 0 to 1 (basically,
    as you vary the threshold from 1 to 0). Average precision is very popular for
    tasks related to object detection, which we will discuss a bit later on, but it
    is also very useful for classification in tabular data. In practice, it proves
    valuable when you want to monitor model performance on a very rare class (when
    the data is extremely imbalanced) in a more precise and exact way, which is often
    the case with fraud detection problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more specific insights on this, read *Gael Varoquaux’s* discussion: [http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision](http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision).'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, you have probably already figured out that using precision or
    recall as an evaluation metric is not an ideal choice because you can only optimize
    one at the expense of the other. For this reason, there are no Kaggle competitions
    that use only one of the two metrics. You should combine them (as in the average
    precision). A single metric, the **F1 score**, which is the harmonic mean of precision
    and recall, is commonly considered to be the best solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_019.png)'
  prefs: []
  type: TYPE_IMG
- en: If you get a high *F*1 score, it is because your model has improved in precision
    or recall or in both. You can find a fine example of the usage of this metric
    in the *Quora* *Insincere Questions Classification* competition ([https://www.kaggle.com/c/quora-insincere-questions-classification](https://www.kaggle.com/c/quora-insincere-questions-classification)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In some competitions, you also get the **F-beta** score. This is simply the
    weighted harmonic mean between precision and recall, and beta decides the weight
    of the recall in the combined score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we have already introduced the concept of threshold and classification
    probability, we can now discuss the log loss and ROC-AUC, both quite common classification
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Log loss and ROC-AUC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the **log loss**, which is also known as **cross-entropy**
    in deep learning models. The log loss is the difference between the predicted
    probability and the ground truth probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_021.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above formula, *n* stands for the number of examples, ![](img/B17574_05_002.png)
    is the ground truth for the *i*^(th) case, and ![](img/B17574_05_008.png) the
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: If a competition uses the log loss, it is implied that the objective is to estimate
    as correctly as possible the probability of an example being of a positive class.
    You can actually find the log loss in quite a lot of competitions.
  prefs: []
  type: TYPE_NORMAL
- en: We suggest you have a look, for instance, at the recent *Deepfake Detection
    Challenge* ([https://www.kaggle.com/c/deepfake-detection-challenge](https://www.kaggle.com/c/deepfake-detection-challenge))
    or at the older *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ROC curve**, or **receiver operating characteristic curve**, is a graphical
    chart used to evaluate the performance of a binary classifier and to compare multiple
    classifiers. It is the building block of the ROC-AUC metric, because the metric
    is simply the area delimited under the ROC curve. The ROC curve consists of the
    true positive rate (the recall) plotted against the false positive rate (the ratio
    of negative instances that are incorrectly classified as positive ones). It is
    equivalent to one minus the true negative rate (the ratio of negative examples
    that are correctly classified). Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Different ROC curves and their AUCs'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a ROC curve of a well-performing classifier should quickly climb up
    the true positive rate (recall) at low values of the false positive rate. A ROC-AUC
    between 0.9 to 1.0 is considered very good.
  prefs: []
  type: TYPE_NORMAL
- en: A bad classifier can be spotted by the ROC curve appearing very similar, if
    not identical, to the diagonal of the chart, which represents the performance
    of a purely random classifier, as in the top left of the figure above; ROC-AUC
    scores near 0.5 are considered to be almost random results. If you are comparing
    different classifiers, and you are using the **area under the curve** (**AUC**),
    the classifier with the higher area is the more performant one.
  prefs: []
  type: TYPE_NORMAL
- en: If the classes are balanced, or not too imbalanced, increases in the AUC are
    proportional to the effectiveness of the trained model and they can be intuitively
    thought of as the ability of the model to output higher probabilities for true
    positives. We also think of it as the ability to order the examples more properly
    from positive to negative. However, when the positive class is rare, the AUC starts
    high and its increments may mean very little in terms of predicting the rare class
    better. As we mentioned before, in such a case, average precision is a more helpful
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'AUC has recently been used for quite a lot of different competitions. We suggest
    you have a look at these three:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IEEE-CIS Fraud Detection*: [https://www.kaggle.com/c/ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Riiid Answer Correctness Prediction*: [https://www.kaggle.com/c/riiid-test-answer-prediction](https://www.kaggle.com/c/riiid-test-answer-prediction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jigsaw Multilingual Toxic Comment Classification*: [https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read a detailed treatise in the following paper: Su, W., Yuan, Y.,
    and Zhu, M. *A relationship between the average precision and the area under the
    ROC curve.* Proceedings of the 2015 International Conference on The Theory of
    Information Retrieval. 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: Matthews correlation coefficient (MCC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We complete our overview of binary classification metrics with the **Matthews
    correlation coefficient** (**MCC**), which made its appearance in *VSB Power Line
    Fault Detection* ([https://www.kaggle.com/c/vsb-power-line-fault-detection](https://www.kaggle.com/c/vsb-power-line-fault-detection))
    and *Bosch Production Line Performance* ([https://www.kaggle.com/c/bosch-production-line-performance](https://www.kaggle.com/c/bosch-production-line-performance)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for the MCC is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_024.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above formula, *TP* stands for true positives, *TN* for true negatives,
    *FP* for false positives, and *FN* for false negatives. It is the same nomenclature
    as we met when discussing precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Behaving as a correlation coefficient, in other words, ranging from +1 (perfect
    prediction) to -1 (inverse prediction), this metric can be considered a measure
    of the quality of the classification even when the classes are quite imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In spite of its complexity, the formula can be reformulated and simplified,
    as demonstrated by Neuron Engineer ([https://www.kaggle.com/ratthachat](https://www.kaggle.com/ratthachat))
    in his Notebook: [www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc](https://www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The work done by Neuron Engineer in understanding the ratio of the evaluation
    metric is indeed exemplary. In fact, his reformulated MCC becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where each element of the formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_026.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_027.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_028.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_029.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The reformulation helps to clarify, in a more intelligible form than the original,
    that you can get higher performance from improving both positive and negative
    class precision, but that’s not enough: you also have to have positive and negative
    predictions in proportion to the ground truth, or your submission will be greatly
    penalized.'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for multi-class classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When moving to multi-class classification, you simply use the binary classification
    metrics that we have just seen, applied to each class, and then you summarize
    them using some of the averaging strategies that are commonly used for multi-class
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you want to evaluate your solution based on the *F*1 score,
    you have three possible averaging choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Macro averaging**: Simply calculate the *F*1 score for each class and then
    average all the results. In this way, each class will count as much the others,
    no matter how frequent its positive cases are or how important they are for your
    problem, resulting therefore in equal penalizations when the model doesn’t perform
    well with any class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_031.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Micro averaging**: This approach will sum all the contributions from each
    class to compute an aggregated *F*1 score. It results in no particular favor to
    or penalization of any class, since all the computations are made regardless of
    each class, so it can more accurately account for class imbalances:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_032.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Weighting**: As with macro averaging, you first calculate the *F*1 score
    for each class, but then you make a weighted average mean of all of them using
    a weight that depends on the number of true labels of each class. By using such
    a set of weights, you can take into account the frequency of positive cases from
    each class or the relevance of that class for your problem. This approach clearly
    favors the majority classes, which will be weighted more in the computations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_033.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Common multi-class metrics that you may encounter in Kaggle competitions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiclass accuracy (weighted)**: *Bengali.AI Handwritten Grapheme Classification*
    ([https://www.kaggle.com/c/bengaliai-cv19](https://www.kaggle.com/c/bengaliai-cv19))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass log loss (MeanColumnwiseLogLoss)**: *Mechanisms of Action (MoA)*
    *Prediction* ([https://www.kaggle.com/c/lish-moa/](https://www.kaggle.com/c/lish-moa/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macro-F1** and **Micro-F1 (NQMicroF1)**: *University of Liverpool - Ion Switching*
    ([https://www.kaggle.com/c/liverpool-ion-switching](https://www.kaggle.com/c/liverpool-ion-switching)),
    *Human Protein Atlas Image Classification* ([https://www.kaggle.com/c/human-protein-atlas-image-classification/](https://www.kaggle.com/c/human-protein-atlas-image-classification/)),
    *TensorFlow* *2.0 Question* *Answering* ([https://www.kaggle.com/c/tensorflow2-question-answering](https://www.kaggle.com/c/tensorflow2-question-answering))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean-F1**: *Shopee - Price Match Guarantee* ([https://www.kaggle.com/c/shopee-product-matching/](https://www.kaggle.com/c/shopee-product-matching/)).
    Here, the *F*1 score is calculated for every predicted row, then averaged, whereas
    the Macro-F1 score is defined as the mean of class-wise/label-wise *F*1 scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then there is also **Quadratic Weighted Kappa**, which we will explore later
    on as a smart evaluation metric for ordinal prediction problems. In its simplest
    form, the **Cohen Kappa** score, it just measures the agreement between your predictions
    and the ground truth. The metric was actually created for measuring **inter-annotation
    agreement**, but it is really versatile and has found even better uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is inter-annotation agreement? Let’s imagine that you have a labeling
    task: classifying some photos based on whether they contain an image of a cat,
    a dog, or neither. If you ask a set of people to do the task for you, you may
    incur some erroneous labels because someone (called the *judge* in this kind of
    task) may misinterpret a dog as a cat or vice versa. The smart way to do this
    job correctly is to divide the work among multiple judges labeling the same photos,
    and then measure their level of agreement based on the Cohen Kappa score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the Cohen Kappa is devised as a score expressing the level of agreement
    between two annotators on a labeling (classification) problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Using the confusion
    matrix nomenclature, this can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_036.png)'
  prefs: []
  type: TYPE_IMG
- en: The interesting aspect of this formula is that the score takes into account
    the empirical probability that the agreement has happened just by chance, so the
    measure has a correction for all the most probable classifications. The metric
    ranges from 1, meaning complete agreement, to -1, meaning the judges completely
    oppose each other (total disagreement).
  prefs: []
  type: TYPE_NORMAL
- en: Values around 0 signify that agreement and disagreement among the judges is
    happening by mere chance. This helps you figure out if the model is really performing
    better than chance in most situations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Andrey_Lukyanenko_-_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: Andrey Lukyanenko
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/artgor](https://www.kaggle.com/artgor)'
  prefs: []
  type: TYPE_NORMAL
- en: Our second interview of the chapter is with Andrey Lukyanenko, a Notebooks and
    Discussions Grandmaster and Competitions Master. In his day job, he is a Machine
    Learning Engineer and TechLead at MTS Group. He had many interesting things to
    say about his Kaggle experiences!
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques, solving
    approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I prefer competitions where solutions can be general enough to be transferable
    to other datasets/domains. I’m interested in trying various neural net architectures,
    state-of-the-art approaches, and post-processing tricks. I don’t favor those competitions
    that require reverse engineering or creating some “golden features,” as these
    approaches won’t be applicable in other datasets.*'
  prefs: []
  type: TYPE_NORMAL
- en: While you were competing on Kaggle, you also became a Grandmaster in Notebooks
    (and ranked number one) and Discussions. Have you invested in these two objectives?
  prefs: []
  type: TYPE_NORMAL
- en: '*I have invested a lot of time and effort into writing Notebooks, but the Discussion
    Grandmaster rank happened kind of on its own.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s start with the Notebook ranking.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*There was a special competition in 2018 called DonorsChoose.org Application
    Screening. DonorsChoose is a fund that empowers public school teachers from across
    the country to request much-needed materials and experiences for their students.
    It organized a competition, where the winning solutions were based not on the
    score on the leaderboard, but on the number of the upvotes on the Notebook. This
    looked interesting and I wrote a Notebook for the competition. Many participants
    advertised their analysis on social media and I did the same. As a result, I reached
    second place and won a Pixelbook (I’m still using it!).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I was very motivated by this success and continued writing Notebooks. At first,
    I simply wanted to share my analysis and get feedback, because I wanted to try
    to compare my analytics and visualization skills with other people to see what
    I could do and what people thought of it. People started liking my kernels and
    I wanted to improve my skills even further. Another motivation was a desire to
    improve my skill at making a quick MVP (minimum viable product). When a new competition
    starts, many people begin writing Notebooks, and if you want to be one of the
    first, you have to be able to do it fast without sacrificing quality. This is
    challenging, but fun and rewarding.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I was able to get the Notebook Grandmaster rank in the February of 2019; after
    some time, I reached first place and held it for more than a year. Now I write
    Notebooks less frequently, but I still enjoy doing it.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*As for discussions, I think it kind of happened on its own. I answered the
    comments on my Notebooks, and shared and discussed ideas about competitions in
    which I took part, and my discussion ranking steadily increased.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: '*It was the* Predicting Molecular Properties *competition. I have written a
    blog post about it in more detail here (*[https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1](https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1)*).
    It was a domain-specific competition aimed at predicting interactions between
    atoms in molecules. Nuclear Magnetic Resonance (NMR) is a technology that uses
    principles similar to MRI to understand the structure and dynamics of proteins
    and molecules. Researchers around the world conduct NMR experiments to further
    understand the structure and dynamics of molecules, across areas like environmental
    science, pharmaceutical science, and materials science. In this competition, we
    tried to predict the magnetic interaction between two atoms in a molecule (the
    scalar coupling constant). State-of-the-art methods from quantum mechanics can
    calculate these coupling constants given only a 3D molecular structure as input.
    But these calculations are very resource-intensive, so can’t be always used. If
    machine learning approaches could predict these values, it would really help medicinal
    chemists to gain structural insights faster and more cheaply.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I usually write EDA kernels for new Kaggle competitions, and this one was
    no exception. A common approach for tabular data in Kaggle competitions is extensive
    feature engineering and using gradient boosting models. I used LGBM too in my
    early attempts, but knew that there should be better ways to work with graphs.
    I realized that domain expertise would provide a serious advantage, so I hunted
    for every piece of such information. Of course, I noticed that there were several
    active experts, who wrote on the forum and created kernels, so I read everything
    from them. And one day I received an e-mail from an expert in this domain who
    thought that our skills could complement each other. Usually, I prefer to work
    on competitions by myself for some time, but in this case, combining forces seemed
    to be a good idea to me. And this decision turned out to be a great one! With
    time we were able to gather an amazing team.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*After some time, we noticed a potential for neural nets in the competition:
    a well-known Kaggler, Heng, posted an example of an MPNN (Message Passing Neural
    Network) model. After some time, I was even able to run it, but the results were
    worse compared to our models. Nevertheless, our team knew that we would need to
    work with these Neural Nets if we wanted to aim high. It was amazing to see how
    Christof was able to build new neural nets extremely fast. Soon, we focused only
    on developing those models.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*After that, my role switched to a support one. I did a lot of experiments
    with our neural nets: trying various hyperparameters, different architectures,
    various little tweaks to training schedules, and so on. Sometimes I did EDA on
    our predictions to find our interesting or wrong cases, and later we used this
    information to improve our models even further.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*We got the 8*^(th) *place and I learned a lot during this competition.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle definitely helped me a lot, especially with my skills and my personal
    brand. Writing and publishing Kaggle Notebooks taught me not only EDA and ML skills,
    but it forced me to become adaptable, to be able to understand new topics and
    tasks quickly, to iterate more efficiently between approaches. At the same time,
    it provided a measure of visibility for me, because people appreciated my work.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*My first portfolio (*[https://erlemar.github.io/](https://erlemar.github.io/)*)
    had a lot of different Notebooks, and half of them were based on old Kaggle competitions.
    It was definitely helpful in getting my first jobs. My Kaggle achievements also
    helped me attract recruiters from good companies, sometimes even to skip steps
    of the interview process, and even led me to several consulting gigs.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*I think we need to separate inexperienced Kagglers into two groups: those
    who are inexperienced in data science in general and those who are inexperienced
    on Kaggle.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Those who are inexperienced in general make a number of different mistakes
    (and it is okay, everyone started somewhere):*'
  prefs: []
  type: TYPE_NORMAL
- en: '*One of the most serious problems: lack of critical thinking and not knowing
    how to do their own research;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not knowing when and what tools/approaches to use;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Blindly taking public Notebooks and using them without understanding how they
    work;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fixating on a certain idea and spending too much time pursuing it, even when
    it doesn’t work;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Despairing and losing motivation when their experiments fail.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*As for those people who have experience in data science but don’t have experience
    with Kaggle, I’d say that the most serious thing they overlook is that they underestimate
    Kaggle’s difficulty. They don’t expect Kaggle to be very competitive, that you
    need to try many different things to succeed, that there are a lot of tricks that
    work only in competitions, that there are people who professionally participate
    in competitions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, people often overestimate domain expertise. I admit that there were
    a number of competitions when the teams with domain experts in them won gold medals
    and prizes, but in most cases experienced Kagglers triumph.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, I have seen the following situation many times: some person proclaims
    that winning Kaggle is easy, and that he (or his group of people) will get a gold
    medal or many gold medals in the recent future. In most cases, they silently fail.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*Not* *enough looking in the data. Sometimes I wasn’t able to generate better
    features or apply better postprocessing due to this. And reserve engineering and
    “golden features” is a whole additional topic.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spending too much time on a single idea because I hoped it would work. This
    is called sunk-cost fallacy.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not enough experiments. The effort pays off – if you don’t spend enough time
    and resources on the competition, you won’t get a high place on a leaderboard.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entering “wrong” competitions. There were competitions with leaks, reverse
    engineering, etc. There were competitions with an unreasonable split between public
    and private test data and a shake-up ensured. There were competitions that weren’t
    interesting enough for me and I shouldn’t have started participating in them.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Teaming up with the wrong people. There were cases when my teammates weren’t
    as active as I expected a**nd it led to a worse team score.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*I think it is important to remember your goal, know what are you ready to
    invest into this competition, and think about the possible outcomes. There are
    many possible goals that people have while entering a competition:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Win**ning money or getting a medal;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting new skills or improving existing ones;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with a new task/domain;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Networking;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PR;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*etc;*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Of course, it is possible to have multiple motivations.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*As for what are you ready to invest, it is usually about the amount of time
    and effort you are ready to spend as well as the hardware that you have.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*When I speak about the outcomes, I mean what will happen when the competition
    ends. It is possible that you will invest a lot in this competition and win, but
    you could also lose. Are you ready for this reality? Is winning a particular competition
    critical to you? Maybe you need to be prepared to invest more effort; on the other
    hand, maybe you have long-term goals and one failed competition won’t hurt much.*'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for object detection problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent years, deep learning competitions have become more and more common
    on Kaggle. Most of these competitions, focused on image recognition or on natural
    language processing tasks, have not required the use of evaluation metrics much
    different from the ones we have explored up to now. However, a couple of specific
    problems have required some special metric to be evaluated correctly: those relating
    to **object detection** and **segmentation**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Computer vision tasks. (Source: https://cocodataset.org/#explore?id=38282,
    https://cocodataset.org/#explore?id=68717)'
  prefs: []
  type: TYPE_NORMAL
- en: In **object detection**, you don’t have to classify an image, but instead find
    relevant portions of a picture and label them accordingly. For instance, in *Figure
    5.4*, an object detection classifier has been entrusted to locate within a photo
    the portions of the picture where either dogs or cats are present and classify
    each of them with a proper label. The example on the left shows the localization
    of a cat using a rectangular box (called a **bounding box**). The example on the
    right presents how multiple cats and dogs are detected in the picture by bounding
    boxes and then correctly classified (the blue bounding boxes are for dogs, the
    red ones for cats).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to describe the spatial location of an object, in object detection
    we use **bounding boxes**, which define a rectangular area in which the object
    lies. A bounding box is usually specified using two (*x*, *y*) coordinates: the
    upper-left and lower-right corners. In terms of a machine learning algorithm,
    finding the coordinates of bounding boxes corresponds to applying a regression
    problem to multiple targets. However, you probably won’t frame the problem from
    scratch but rely on pre-built and often pre-trained models such as Mask R-CNN
    ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)), RetinaNet
    ([https://arxiv.org/abs/2106.05624v1](https://arxiv.org/abs/2106.05624v1)), FPN
    ([https://arxiv.org/abs/1612.03144v2](https://arxiv.org/abs/1612.03144v2)), YOLO
    ([https://arxiv.org/abs/1506.02640v1](https://arxiv.org/abs/1506.02640v1)), Faster
    R-CNN ([https://arxiv.org/abs/1506.01497v1](https://arxiv.org/abs/1506.01497v1)),
    or SDD ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In **segmentation**, you instead have a classification at the *pixel* level,
    so if you have a 320x200 image, you actually have to make 64,000 pixel classifications.
    Depending on the task, you can have a **semantic segmentation** where you have
    to classify every pixel in a photo, or an **instance segmentation** where you
    only have to classify the pixels representing objects of a certain type of interest
    (for instance, a cat as in our example in *Figure 5.5* below):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Semantic segmentation and instance segmentation on the same image.
    (Source: https://cocodataset.org/#explore?id=338091)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an overview of the specific metrics for these tasks, metrics
    that can work well for both problems, since, in both cases, you are predicting
    entire areas (rectangular ones in object detection, polygonal ones in segmentation)
    of a picture and you have to compare your predictions against a ground truth,
    which is, again, expressed as areas. On the side of segmentation, the easiest
    metric is the **pixel accuracy**, which, as the name suggests, is the accuracy
    on the pixel classification.
  prefs: []
  type: TYPE_NORMAL
- en: It is not a great metric because, as happens with accuracy on binary and multi-class
    problems, your score may look great if the relevant pixels do not take up very
    much of the image (you just predict the majority claim, thus you don’t segment).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, there are two metrics that are used much more, especially in competitions:
    the **intersection over union** and the **dice coefficient**.'
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over union (IoU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **intersection over union** (**IoU**) is also known as the **Jaccard index**.
    When used in segmentation problems, using IoU implies that you have two images
    to compare: one is your prediction and the other is the mask revealing the ground
    truth, which is usually a binary matrix where the value 1 stands for the ground
    truth and 0 otherwise. In the case of multiple objects, you have multiple masks,
    each one labeled with the class of the object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When used in object detection problems, you have the boundaries of two rectangular
    areas (those of the prediction and the ground truth), expressed by the coordinates
    of their vertices. For each classified class, you compute the area of overlap
    between your prediction and the ground truth mask, and then you divide this by
    the area of the union between your prediction and the ground truth, a sum that
    takes into account any overlap. In this way, you are proportionally penalized
    both if you predict a larger area than what it should be (the denominator will
    be larger) or a smaller one (the numerator will be smaller):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Visual representation of the IoU calculation'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.6* you can see a visual representation of the areas involved in
    the computation. By imagining the squares overlapping more, you can figure out
    how the metric efficiently penalizes your solution when your prediction, even
    if covering the ground truth, exceeds it (the area of union becomes larger).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of competitions where IoU has been used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TGS Salt Identification Challenge* ([https://www.kaggle.com/c/tgs-salt-identification-challenge/](https://www.kaggle.com/c/tgs-salt-identification-challenge/))
    with Intersection Over Union Object Segmentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iMaterialist (Fashion) 2019 at FGVC6* ([https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6))
    with Intersection Over Union Object Segmentation With Classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Airbus Ship Detection Challenge* ([https://www.kaggle.com/c/airbus-ship-detection](https://www.kaggle.com/c/airbus-ship-detection))
    with Intersection Over Union Object Segmentation Beta'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The other useful metric is the **Dice coefficient**, which is the area of overlap
    between the prediction and ground truth doubled and then divided by the sum of
    the prediction and ground truth areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Visual representation of the Dice calculation'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, with respect to the Jaccard index, you do not take into account
    the overlap of the prediction with the ground truth in the denominator. Here,
    the expectation is that, as you maximize the area of overlap, you predict the
    correct area size. Again, you are penalized if you predict areas larger than you
    should be predicting. In fact, the two metrics are positively correlated and they
    produce almost the same results for a single classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: The differences actually arise when you are working with multiple classes. In
    fact, both with IoU and the Dice coefficient, when you have multiple classes you
    average the result of all of them. However, in doing so, the IoU metric tends
    to penalize the overall average more if a single class prediction is wrong, whereas
    the Dice coefficient is more lenient and tends to represent the average performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of Kaggle competitions using the Dice coefficient (it is often encountered
    in competitions with medical purposes, but not necessarily only there, because
    it can also be used for clouds and cars):'
  prefs: []
  type: TYPE_NORMAL
- en: '*HuBMAP - Hacking the Kidney*: [https://www.kaggle.com/c/hubmap-kidney-segmentation](https://www.kaggle.com/c/hubmap-kidney-segmentation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ultrasound Nerve Segmentation*: [https://www.kaggle.com/c/ultrasound-nerve-segmentation](https://www.kaggle.com/c/ultrasound-nerve-segmentation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding Clouds from Satellite Images*: [https://www.kaggle.com/c/understanding_cloud_organization](https://www.kaggle.com/c/understanding_cloud_organization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Carvana Image Masking Challenge*: [https://www.kaggle.com/c/carvana-image-masking-challenge](https://www.kaggle.com/c/carvana-image-masking-challenge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoU and Dice constitute the basis for all the more complex metrics in segmentation
    and object detection. By choosing an appropriate threshold level for IoU or Dice
    (usually 0.5), you can decide whether or not to confirm a detection, therefore
    a classification. At this point, you can use previously discussed metrics for
    classification, such as precision, recall, and *F*1, such as is done in popular
    object detection and segmentation challenges such as Pascal VOC ([http://host.robots.ox.ac.uk/pascal/VOC/voc2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012))
    or COCO ([https://cocodataset.org](https://cocodataset.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for multi-label classification and recommendation problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recommender systems are one of the most popular applications of data analysis
    and machine learning, and there are quite a few competitions on Kaggle that have
    used the recommendation approach. For instance, the *Quick, Draw! Doodle Recognition
    Challenge* was a prediction evaluated as a recommender system. Some other competitions
    on Kaggle, however, truly strived to build effective recommender systems (such
    as *Expedia Hotel Recommendations*: [https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations))
    and RecSYS, the conference on recommender systems ([https://recsys.acm.org/](https://recsys.acm.org/)),
    even hosted one of its yearly contests on Kaggle (*RecSYS 2013*: [https://www.kaggle.com/c/yelp-recsys-2013](https://www.kaggle.com/c/yelp-recsys-2013)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Average Precision at K** (**MAP@{K}**) is typically the metric of choice
    for evaluating the performance of recommender systems, and it is the most common
    metric you will encounter on Kaggle in all the competitions that try to build
    or approach a problem as a recommender system.'
  prefs: []
  type: TYPE_NORMAL
- en: There are also some other metrics, such as the **precision at k**, or **P@K**,
    and the **average precision at k**, or **AP@K**, which are loss functions, in
    other words, computed at the level of each single prediction. Understanding how
    they work can help you better understand the MAP@K and how it can perform both
    in recommendations and in multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, analogous to recommender systems, multi-label classifications imply
    that your model outputs a series of class predictions. Such results could be evaluated
    using some average of some binary classification metrics (such as in *Greek Media
    Monitoring Multilabel Classification (WISE 2014)*, which used the mean *F*1 score:
    [https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)) as well
    as metrics that are more typical of recommender systems, such as MAP@K. In the
    end, you can deal with both recommendations and multi-label predictions as *ranking
    tasks*, which translates into a set of ranked suggestions in a recommender system
    and into a set of labels (without a precise order) in multi-label classification.'
  prefs: []
  type: TYPE_NORMAL
- en: MAP@{K}
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MAP@K is a complex metric and it derives from many computations. In order to
    understand the MAP@K metric fully, let’s start with its simplest component, the
    **precision at** **k** (**P@K**). In this case, since the prediction for an example
    is a ranked sequence of predictions (from the most probable to the least), the
    function takes into account only the top *k* predictions, then it computes how
    many matches it got with respect to the ground truth and divides that number by
    *k*. In a few words, it is quite similar to an accuracy measure averaged over
    *k* predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A bit more complex in terms of computation, but conceptually simple, the **average
    precision at** **k** (**AP@K**) is the average of P@K computed over all the values
    ranging from *1* to *k*. In this way, the metric evaluates how well the prediction
    works overall, using the top prediction, then the top two predictions, and so
    on until the top *k* predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, **MAP@K** is the mean of the AP@K for the entire predicted sample,
    and it is a metric because it comprises all the predictions in its evaluation.
    Here is the MAP@5 formulation you can find in the *Expedia Hotel Recommendations*
    competition ([https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_037.png)'
  prefs: []
  type: TYPE_IMG
- en: In the formula, ![](img/B17574_05_038.png) is the number of user recommendations,
    *P(k)* is the precision at cutoff *k*, and *n* is the number of predicted hotel
    clusters (you could predict up to 5 hotels for each recommendation).
  prefs: []
  type: TYPE_NORMAL
- en: It is clearly a bit more daunting than our explanation, but the formula just
    expresses that the MAP@K is the mean of all the AP@K evaluations over all the
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed this overview of specific metrics for different regression
    and classification metrics, let’s discuss how to deal with evaluation metrics
    in a Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summing up what we have discussed so far, an objective function is a function
    inside your learning algorithm that measures how well the algorithm’s internal
    model is fitting the provided data. The objective function also provides feedback
    to the algorithm in order for it to improve its fit across successive iterations.
    Clearly, since the entire algorithm’s efforts are recruited to perform well based
    on the objective function, if the Kaggle evaluation metric perfectly matches the
    objective function of your algorithm, you will get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this is not frequently the case. Often, the evaluation metric
    provided can only be approximated by existing objective functions. Getting a good
    approximation, or striving to get your predictions performing better with respect
    to the evaluation criteria, is the secret to performing well in Kaggle competitions.
    When your objective function does not match your evaluation metric, you have a
    few alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify your learning algorithm and have it incorporate an objective function
    that matches your evaluation metric, though this is not possible for all algorithms
    (for instance, algorithms such as LightGBM and XGBoost allow you to set custom
    objective functions, but most Scikit-learn models don’t allow this).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune your model’s hyperparameters, choosing the ones that make the result shine
    the most when using the evaluation metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Post-process your results so they match the evaluation criteria more closely.
    For instance, you could code an optimizer that performs transformations on your
    predictions (probability calibration algorithms are an example, and we will discuss
    them at the end of the chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having the competition metric incorporated into your machine learning algorithm
    is really the most effective method to achieve better predictions, though only
    a few algorithms can be hacked into using the competition metric as your objective
    function. The second approach is therefore the more common one, and many competitions
    end up in a struggle to get the best hyperparameters for your models to perform
    on the evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: If you already have your evaluation function coded, then doing the right cross-validation
    or choosing the appropriate test set plays the lion share. If you don’t have the
    coded function at hand, you have to first code it in a suitable way, following
    the formulas provided by Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invariably, doing the following will make the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking for all the relevant information about the evaluation metric and its
    coded function on a search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Browsing through the most common packages (such as Scikit-learn: [https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)
    or TensorFlow: [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Browsing GitHub projects (for instance, *Ben Hammer’s* Metrics project: [https://github.com/benhamner/Metrics](https://github.com/benhamner/Metrics))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking or looking around in the forums and available Kaggle Notebooks (both
    for the current competition and for similar competitions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, as we mentioned before, querying the Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    and looking in the **Competitions** table will help you find out which other Kaggle
    competitions used that same evaluation metric, and immediately provides you with
    useful code and ideas to try out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss in greater detail the alternatives you have when your evaluation
    metric doesn’t match your algorithm’s objective function. We’ll start by exploring
    custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Custom metrics and custom objective functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a first option when your objective function does not match your evaluation
    metric, we learned above that you can solve this by creating your own custom objective
    function, but that only a few algorithms can easily be modified to incorporate
    a specific objective function.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that the few algorithms that allow this are among the most
    effective ones in Kaggle competitions and data science projects. Of course, creating
    your own custom objective function may sound a little bit tricky, but it is an
    incredibly rewarding approach to increasing your score in a competition. For instance,
    there are options to do this when using gradient boosting algorithms such as XGBoost,
    CatBoost, and LightGBM, as well as with all deep learning models based on TensorFlow
    or PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find great tutorials for custom metrics and objective functions in
    TensorFlow and PyTorch here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279](https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://petamind.com/advanced-keras-custom-loss-functions/](https://petamind.com/advanced-keras-custom-loss-functions/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/](https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These will provide you with the basic function templates and some useful suggestions
    about how to code a custom objective or evaluation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want just to get straight to the custom objective function you need,
    you can try this Notebook by RNA ([https://www.kaggle.com/bigironsphere](https://www.kaggle.com/bigironsphere)):
    [https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook).
    It contains a large range of custom loss functions for both TensorFlow and PyTorch
    that have appeared in different competitions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you need to create a custom loss in LightGBM, XGBoost, or CatBoost, as indicated
    in their respective documentation, you have to code a function that takes as inputs
    the prediction and the ground truth, and that returns as outputs the gradient
    and the hessian.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can consult this post on Stack Overflow for a better understanding of what
    a gradient and a hessian are: [https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based](https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a code implementation perspective, all you have to do is to create a function,
    using closures if you need to pass more parameters beyond just the vector of predicted
    labels and true labels. Here is a simple example of a **focal loss** (a loss that
    aims to heavily weight the minority class in the loss computations as described
    in Lin, T-Y. et al. *Focal loss for dense object detection*: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002))
    function that you can use as a model for your own custom functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another interesting aspect of the example is that it really makes it easy to
    compute the gradient and the hessian of the cost function by means of the derivative
    function from SciPy. If your cost function is differentiable, you don’t have to
    worry about doing any calculations by hand. However, creating a custom objective
    function requires some mathematical knowledge and quite a lot of effort to make
    sure it works properly for your purposes. You can read about the difficulties
    that *Max Halford* experienced while implementing a focal loss for the LightGBM
    algorithm, and how he overcame them, here: [https://maxhalford.github.io/blog/lightgbm-focal-loss/](https://maxhalford.github.io/blog/lightgbm-focal-loss/).
    Despite the difficulty, being able to conjure up a custom loss can really determine
    your success in a Kaggle competition where you have to extract the maximum possible
    result from your model.'
  prefs: []
  type: TYPE_NORMAL
- en: If building your own objective function isn’t working out, you can simply lower
    your ambitions, give up building your function as an objective function used by
    the optimizer, and instead code it as a custom *evaluation metric*. Though your
    model won’t be directly optimized to perform against this function, you can still
    improve its predictive performance with hyperparameter optimization based on it.
    This is the second option we talked about in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just remember, if you are writing a metric from scratch, sometimes you may
    need to abide by certain code conventions for your function to work properly.
    For instance, if you use Scikit-learn, you have to convert your functions using
    the `make_scorer` function. The `make_scorer` function is actually a wrapper that
    makes your evaluation function suitable for working with the Scikit-learn API.
    It will wrap your function while considering some meta-information, such as whether
    to use probability estimates or predictions, whether you need to specify a threshold
    for prediction, and, last but not least, the directionality of the optimization,
    that is, whether you want to maximize or minimize the score it returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, you prepare a scorer based on the average precision metric,
    specifying that it should use a weighted computation when dealing with multi-class
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: If you are optimizing for your evaluation metric, you can apply grid search,
    random search, or some more sophisticated optimization such as Bayesian optimization
    and find the set of parameters that makes your algorithm perform optimally for
    your evaluation metric, even if it works with a different cost function. We will
    explore how to best arrange parameter optimization and obtain the best results
    on Kaggle competitions after having discussed model validation, specifically in
    the chapter dealing with tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing your predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-processing tuning implies that your predictions are transformed, by means
    of a function, into something else in order to present a better evaluation. After
    building your custom loss or optimizing for your evaluation metric, you can also
    improve your results by leveraging the characteristics of your evaluation metric
    using a specific function applied to your predictions. Let’s take the Quadratic
    Weighted Kappa, for instance. We mentioned previously that this metric is useful
    when you have to deal with the prediction of an ordinal value. To recap, the original
    Kappa coefficient is a chance-adjusted index of agreement between the algorithm
    and the ground truth. It is a kind of accuracy measurement corrected by the probability
    that the match between the prediction and the ground truth is due to a fortunate
    chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the original version of the Kappa coefficient, as seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Here, you need just
    two matrices, the one with the observed scores and the one with the expected scores
    based on chance agreement. When the Kappa coefficient is weighted, you also consider
    a weight matrix and the formula turns into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_040.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix *p*[p] contains the penalizations to weight errors differently, which
    is very useful for ordinal predictions since this matrix can penalize much more
    when the predictions deviate further from the ground truths. Using the quadratic
    form, that is, squaring the resulting *k*, makes the penalization even more severe.
    However, optimizing for such a metric is really not easy, since it is very difficult
    to implement it as a cost function. Post-processing can help you.
  prefs: []
  type: TYPE_NORMAL
- en: An example can be found in the *PetFinder.my Adoption Prediction* competition
    ([https://www.kaggle.com/c/petfinder-adoption-prediction](https://www.kaggle.com/c/petfinder-adoption-prediction)).
    In this competition, given that the results could have 5 possible ratings (0,
    1, 2, 3, or 4), you could deal with them either using a classification or a regression.
    If you used a regression, a post-processing transformation of the regression output
    could improve the model’s performance against the Quadratic Weighted Kappa metric,
    outperforming the results you could get from a classification directly outputting
    discrete predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the PetFinder competition, the post-processing consisted of
    an optimization process that started by transforming the regression results into
    integers, first using the boundaries [0.5, 1.5, 2.5, 3.5] as thresholds and, by
    an iterative fine-tuning, finding a better set of boundaries that maximized the
    performance. The fine-tuning of the boundaries required the computations of an
    optimizer such as SciPy’s `optimize.minimize`, which is based on the Nelder-Mead
    algorithm. The boundaries found by the optimizer were validated by a cross-validation
    scheme. You can read more details about this post-processing directly from the
    post made by *Abhishek Thakur* during the competition: [https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from the PetFinder competition, many other competitions have demonstrated
    that smart post-processing can lead to improved results and rankings. We’ll point
    out a few examples here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw](https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage](https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization](https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, post-processing is often very dependent on the metric you are
    using (understanding the metric is imperative for devising any good post-processing)
    and often also data-specific, for instance, in the case of time series data and
    leakages. Hence, it is very difficult to generalize any procedure for figuring
    out the right post-processing for any competition. Nevertheless, always be aware
    of this possibility and be on the lookout in a competition for any hint that post-processing
    results is favorable. You can always get hints about post-processing from previous
    competitions that have been similar, and by forum discussion – eventually, someone
    will raise the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Predicted probability and its adjustment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To complete the above discussion on metrics optimization (post-processing of
    predictions), we will discuss situations where it is paramount to predict correct
    probabilities, but you are not sure if the algorithm you are using is doing a
    good job. As we detailed previously, classification probabilities concern both
    binary and multiclass classification problems, and they are commonly evaluated
    using the logarithmic loss (aka log loss or logistic loss or cross-entropy loss)
    in its binary or multi-class version (for more details, see the previous sections
    on *Metrics for classification (label prediction and probability)* and *Metrics
    for multi-class classification*).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, evaluating or optimizing for the log loss may not prove enough. The
    main problems to be on the lookout for when striving to achieve correct probabilistic
    predictions with your model are:'
  prefs: []
  type: TYPE_NORMAL
- en: Models that do not return a truly probabilistic estimate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unbalanced distribution of classes in your problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different class distribution between your training data and your test data (on
    both public and private leaderboards)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point alone provides reason to check and verify the quality of classification
    predictions in terms of modeled uncertainty. In fact, even if many algorithms
    are provided in the Scikit-learn package together with a `predict_proba` method,
    this is a very weak assurance that they will return a true probability.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take, for instance, decision trees, which are the basis of many effective
    methods to model tabular data. The probability outputted by a classification decision
    tree ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))
    is based on terminal leaves; that is, it depends on the distribution of classes
    on the leaf that contains the case to be predicted. If the tree is fully grown,
    it is highly likely that the case is in a small leaf with very few other cases,
    so the predicted probability will be very high. If you change parameters such
    as `max_depth`, `max_leaf_nodes`, or `min_samples_leaf`, the resulting probability
    will drastically change from higher values to lower ones depending on the growth
    of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are the most common base model for ensembles such as bagging
    models and random forests, as well as boosted models such as gradient boosting
    (with its high-performing implementations XGBoost, LightGBM, and CatBoost). But,
    for the same reasons – probability estimates that are not truly based on solid
    probabilistic estimations – the problem affects many other commonly used models,
    such as support-vector machines and *k*-nearest neighbors. Such aspects were mostly
    unknown to Kagglers until the *Otto Group Product Classification Challenge* ([https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/](https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/)),
    when it was raised by *Christophe Bourguignat* and others during the competition
    (see [https://www.kaggle.com/cbourguignat/why-calibration-works](https://www.kaggle.com/cbourguignat/why-calibration-works)),
    and easily solved at the time using the calibration functions that had recently
    been added to Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the model you will be using, the presence of imbalance between classes
    in your problem may also result in models that are not at all reliable. Hence,
    a good approach in the case of unbalanced classification problems is to rebalance
    the classes using undersampling or oversampling strategies, or different custom
    weights for each class to be applied when the loss is computed by the algorithm.
    All these strategies may render your model more performant; however, they will
    surely distort the probability estimates and you may have to adjust them in order
    to obtain an even better model score on the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a third point of concern is related to how the test set is distributed.
    This kind of information is usually concealed, but there are often ways to estimate
    it and figure it out (for instance, by trial and error based on the public leaderboard
    results, as we mentioned in *Chapter 1*, *Introducing Kaggle and Other Data Science
    Competitions*).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, this happened in the *iMaterialist Furniture Challenge* ([https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/))
    and the more popular *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
    Both competitions gave rise to various discussions on how to post-process in order
    to adjust probabilities to test expectations (see [https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/](https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/)
    and [https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb](https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb)
    for more details on the method used). From a general point of view, assuming that
    you do not have an idea of the test distribution of classes to be predicted, it
    is still very beneficial to correctly predict probability based on the priors
    you get from the training data (and until you get evidence to the contrary, that
    is the probability distribution that your model should mimic). In fact, it will
    be much easier to correct your predicted probabilities if your predicted probability
    distribution matches those in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution, when your predicted probabilities are misaligned with the training
    distribution of the target, is to use the **calibration function** provided by
    Scikit-learn, `CalibratedClassifierCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of the calibration function is to apply a post-processing function
    to your predicted probabilities in order to make them adhere more closely to the
    empirical probabilities seen in the ground truth. Provided that your model is
    a Scikit-learn model or behaves similarly to one, the function will act as a wrapper
    for your model and directly pipe its predictions into a post-processing function.
    You have the choice between using two methods for post-processing. The first is
    the **sigmoid** method (also called Plat’s scaling), which is nothing more than
    a logistic regression. The second is the **isotonic regression**, which is a non-parametric
    regression; beware that it tends to overfit if there are few examples.
  prefs: []
  type: TYPE_NORMAL
- en: You also have to choose how to fit this calibrator. Remember that it is a model
    that is applied to the results of your model, so you have to avoid overfitting
    by systematically reworking predictions. You could use a **cross-validation**
    (more on this in the following chapter on *Designing Good Validation*) and then
    produce a number of models that, once averaged, will provide your predictions
    (`ensemble=True`). Otherwise, and this is our usual choice, resort to an **out-of-fold
    prediction** (more on this in the following chapters) and calibrate on that using
    all the data available (`ensemble=False`).
  prefs: []
  type: TYPE_NORMAL
- en: Even if `CalibratedClassifierCV` can handle most situations, you can also figure
    out some empirical way to fix probability estimates for the best performance at
    test time. You can use any transformation function, from a handmade one to a sophisticated
    one derived by genetic algorithms, for instance. Your only limit is simply that
    you should cross-validate it and possibly have a good final result from the public
    leaderboard (but not necessarily, because you should trust your local cross-validation
    score more, as we are going to discuss in the next chapter). A good example of
    such a strategy is provided by Silogram ([https://www.kaggle.com/psilogram](https://www.kaggle.com/psilogram)),
    who, in the *Microsoft Malware Classification Challenge*, found out a way to tune
    the unreliable probabilistic outputs of random forests into probabilistic ones
    simply by raising the output to a power determined by grid search (see [https://www.kaggle.com/c/malware-classification/discussion/13509](https://www.kaggle.com/c/malware-classification/discussion/13509)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Sudalai_Rajkumar.png)'
  prefs: []
  type: TYPE_IMG
- en: Sudalai Rajkumar
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/sudalairajkumar](https://www.kaggle.com/sudalairajkumar)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our final interview of the chapter, we speak to Sudalai Rajkumar, SRK, a
    Grandmaster in Competitions, Datasets, and Notebooks, and a Discussion Master.
    He is ranked #1 in the Analytics Vidhya data science platform, and works as an
    AI/ML advisor for start-ups.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*My favorite kinds of competition are ones that involve a good amount of feature
    engineering. I think that is my strength as well. I am generally interested in
    data exploration to get a deep understanding of the data (which you can infer
    from my series of simple exploration Notebooks (*[https://www.kaggle.com/sudalairajkumar/code](https://www.kaggle.com/sudalairajkumar/code)*))
    and then creating features based on it.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*The framework for a competition involves data exploration, finding the right
    validation method, feature engineering, model building, and ensembling/stacking.
    All these are involved in my day job as well. But in addition to this, there is
    a good amount of stakeholder discussion, data collection, data tagging, model
    deployment, model monitoring, and data storytelling that is involved in my daily
    job.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: Santander Product Recommendation *is a memorable competition that we entered.
    Rohan & I did a lot of feature engineering and built multiple models. When we
    did final ensembling, we used different weights for different products and some
    of them did not add up to 1\. From the data exploration and understanding, we
    hand-picked these weights, which helped us. This made us realise the domain/data
    importance in solving problems and how data science is an art as much as science.*
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle played a very important role in my career. I was able to secure my
    last two jobs mainly because of Kaggle. Also, the success from Kaggle helps to
    connect with other stalwarts in the data science field easily and learn from them.
    It also helps a lot in my current role as AI / ML advisor for start-ups, as it
    gives credibility.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding the data in depth. Often this is overlooked, and people get
    into model-building right away. Exploring the data plays a very important role
    in the success of any Kaggle competition. This helps to create proper cross validation
    and to create better features and to extract more value from the data.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*It is a very big list, and I would say that they are learning opportunities.
    In every competition, out of 20-30 ideas that I try, only 1 may work. These mistakes/failures
    give much more learning than the actual success or things that worked. For example,
    I learnt about overfitting the very hard way by falling from top deciles to bottom
    deciles in one of my very first competitions. But that learning stayed with me
    forever thereafter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Are there any particular tools or libraries that you would recommend using
    for data analysis/machine learning?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I primarily use XGBoost/LightGBM in the case of tabular data. I also use open
    source AutoML libraries and Driverless AI to get early benchmarks these days.
    I use Keras, Transformers, and PyTorch for deep learning models.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*Consistency is the key. Each competition will have its own ups and downs.
    There will be multiple days without any progress, but we should not give up and
    keep trying. I think this is applicable for anything and not just Kaggle competitions.*'
  prefs: []
  type: TYPE_NORMAL
- en: Do you use other competition platforms? How do they compare to Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I have also taken part on other platforms like the Analytics Vidhya DataHack
    platform, Driven Data, CrowdAnalytix etc. They are good too, but Kaggle is more
    widely adopted and global in nature, so the amount of competition on Kaggle is
    much higher compared to other platforms.*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed evaluation metrics in Kaggle competitions.
    First, we explained how an evaluation metric can differ from an objective function.
    We also remarked on the differences between regression and classification problems.
    For each type of problem, we analyzed the most common metrics that you can find
    in a Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we discussed the metrics that have never previously been seen in
    a competition and that you won’t likely see again. Finally, we explored and studied
    different common metrics, giving examples of where they have been used in previous
    Kaggle competitions. We then proposed a few strategies for optimizing an evaluation
    metric. In particular, we recommended trying to code your own custom cost functions
    and provided suggestions on possible useful post-processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: You should now have grasped the role of an evaluation metric in a Kaggle competition.
    You should also have a strategy to deal with every common or uncommon metric,
    by retracing past competitions and by gaining a full understanding of the way
    a metric works. In the next chapter, we are going to discuss how to use evaluation
    metrics and properly estimate the performance of your Kaggle solution by means
    of a validation strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code40480600921811704671.png)'
  prefs: []
  type: TYPE_IMG
