- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Competition Tasks and Metrics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 竞赛任务和指标
- en: In a competition, you start by examining the target metric. Understanding how
    your model’s errors are evaluated is key for scoring highly in every competition.
    When your predictions are submitted to the Kaggle platform, they are compared
    to a ground truth based on the target metric.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛中，你首先检查目标指标。理解你的模型错误是如何评估的是在每一场竞赛中取得高分的关键。当你的预测提交到Kaggle平台时，它们将与基于目标指标的真实数据进行比较。
- en: For instance, in the *Titanic* competition ([https://www.kaggle.com/c/titanic/](https://www.kaggle.com/c/titanic/)),
    all your submissions are evaluated based on *accuracy*, the percentage of surviving
    passengers you correctly predict. The organizers decided upon this metric because
    the aim of the competition is to find a model that estimates the probability of
    survival of a passenger under similar circumstances. In another knowledge competition,
    *House Prices - Advanced Regression Techniques* ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)),
    your work will be evaluated based on an *average difference* between your prediction
    and the ground truth. This involves computing the logarithm, squaring, and taking
    the square root, because the model is expected to be able to quantify as correctly
    as possible the order of the price of a house on sale.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*泰坦尼克号*竞赛([https://www.kaggle.com/c/titanic/](https://www.kaggle.com/c/titanic/))中，你所有的提交都将根据*准确率*进行评估，即正确预测的幸存乘客百分比。组织者选择这个指标是因为竞赛的目的是找到一个模型，该模型可以估计在类似情况下乘客生存的概率。在另一个知识竞赛*房价
    - 高级回归技术*([https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques))中，你的工作将根据你的预测与真实数据之间的*平均差异*进行评估。这涉及到计算对数、平方和开方，因为模型预计能够尽可能准确地量化待售房屋价格的高低顺序。
- en: In real-world data science, target metrics are also key for the success of your
    project, though there are certainly differences between the real world and a Kaggle
    competition. We could easily summarize by saying that there are more complexities
    in the real world. In real-world projects, you will often have not just one but
    multiple metrics that your model will be evaluated against. Frequently, some of
    the evaluation metrics won’t even be related to how your predictions perform against
    the ground truth you are using for testing. For instance, the domain of knowledge
    you are working in, the scope of the project, the number of features considered
    by your model, the overall memory usage, any requirement for special hardware
    (such as a GPU, for instance), the latency of the prediction process, the complexity
    of the predicting model, and many other aspects may end up counting more than
    the mere predictive performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的数据科学中，目标指标也是你项目成功的关键，尽管现实世界与Kaggle竞赛之间肯定存在差异。我们可以简单地总结说，现实世界中有更多的复杂性。在现实世界的项目中，你将经常面对不止一个而是多个指标，你的模型将根据这些指标进行评估。通常，一些评估指标甚至与你的预测与用于测试的真实数据之间的表现无关。例如，你正在工作的知识领域，项目的范围，你的模型考虑的特征数量，整体内存使用量，对特殊硬件（例如GPU）的任何要求，预测过程的延迟，预测模型的复杂性，以及许多其他方面，最终可能比单纯的预测性能更重要。
- en: Real-world problems are indeed dominated by business and tech infrastructure
    concerns much more than you may imagine before being involved in any of them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的问题确实比你在参与其中之前想象的要更多地受到商业和技术基础设施的关注。
- en: Yet you cannot escape the fact that the basic principle at the core of both
    real-world projects and Kaggle competitions is the same. Your work will be evaluated
    according to some criteria, and understanding the details of such criteria, optimizing
    the fit of your model in a smart way, or selecting its parameters according to
    the criteria will bring you success. If you can learn more about how model evaluation
    occurs in Kaggle, your real-world data science job will also benefit from it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你无法回避这样一个事实：无论是现实世界项目还是Kaggle竞赛的核心基本原则都是相同的。你的工作将根据某些标准进行评估，理解这些标准的细节，以智能的方式优化你模型的拟合度，或者根据这些标准选择其参数，这些都将为你带来成功。如果你能更多地了解Kaggle中模型评估的流程，你的现实世界数据科学工作也将从中受益。
- en: 'In this chapter, we are going to detail how evaluation metrics for certain
    kinds of problems strongly influence the way you can operate when building your
    model solution in a data science competition. We also address the variety of metrics
    available in Kaggle competitions to give you an idea of what matters most and,
    in the margins, we discuss the different effects of metrics on predictive performance
    and how to correctly translate them into your projects. We will cover the following
    topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细说明某些类型问题的评估指标如何强烈影响你在数据科学竞赛中构建模型解决方案时的操作方式。我们还讨论了Kaggle竞赛中可用的各种指标，以给你一个关于什么最重要的概念，并在边缘讨论了指标对预测性能的不同影响以及如何正确地将它们转化为你的项目。我们将涵盖以下主题：
- en: Evaluation metrics and objective functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标和目标函数
- en: 'Basic types of tasks: regression, classification, and ordinal'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本任务类型：回归、分类和序型
- en: The Meta Kaggle dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta Kaggle 数据集
- en: Handling never-before-seen metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理未见过的指标
- en: Metrics for regression (standard and ordinal)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归指标（标准型和序型）
- en: Metrics for binary classification (label prediction and probability)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类指标（标签预测和概率）
- en: Metrics for multi-class classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类指标
- en: Metrics for object detection problems
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测问题的指标
- en: Metrics for multi-label classification and recommendation problems
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类和推荐问题的指标
- en: Optimizing evaluation metrics
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化评估指标
- en: Evaluation metrics and objective functions
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标和目标函数
- en: In a Kaggle competition, you can find the evaluation metric in the left menu
    on the **Overview** page of the competition. By selecting the **Evaluation** tab,
    you will get details about the evaluation metric. Sometimes you will find the
    metric formula, the code to reproduce it, and some discussion of the metric. On
    the same page, you will also get an explanation about the submission file format,
    providing you with the header of the file and a few example rows.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，你可以在竞赛的**概述**页面的左侧菜单中找到评估指标。通过选择**评估**选项卡，你可以获得关于评估指标的具体信息。有时你会找到指标公式、重现它的代码以及一些关于指标的讨论。在同一页面上，你还可以获得关于提交文件格式的说明，提供文件的标题和一些示例行。
- en: The association between the evaluation metric and the submission file is important,
    because you have to consider that the metric works essentially after you have
    trained your model and produced some predictions. Consequently, as a first step,
    you will have to think about the difference between an **evaluation metric** and
    an **objective function**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标与提交文件之间的关联很重要，因为你必须考虑到指标在训练模型并产生一些预测后基本上才会工作。因此，作为第一步，你必须考虑**评估指标**和**目标函数**之间的区别。
- en: 'Boiling everything down to the basics, an objective function serves your model
    during training because it is involved in the process of error minimization (or
    score maximization, depending on the problem). In contrast, an evaluation metric
    serves your model *after* it has been trained by providing a score. Therefore,
    it cannot influence how the model fits the data, but does influence it in an indirect
    way: by helping you to select the most well-performing hyperparameter settings
    within a model, and the best models among competing ones. Before proceeding with
    the rest of the chapter, which will show you how this can affect a Kaggle competition
    and why the analysis of the Kaggle evaluation metric should be your first act
    in a competition, let’s first discuss some terminology that you may encounter
    in the discussion forums.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将一切简化到基本原理，目标函数在训练过程中服务于你的模型，因为它涉及到错误最小化（或根据问题进行分数最大化）。相比之下，评估指标在模型**训练后**提供服务，通过提供一个分数。因此，它不能影响模型如何拟合数据，但以间接方式影响它：通过帮助你选择模型中最出色的超参数设置，以及竞争中的最佳模型。在继续本章的其余部分之前，这部分将向你展示这如何影响Kaggle竞赛以及为什么分析Kaggle评估指标应该是你在竞赛中的首要行动，让我们首先讨论一些你可能在讨论论坛中遇到的术语。
- en: 'You will often hear talk about objective functions, cost functions, and loss
    functions, sometimes interchangeably. They are not exactly the same thing, however,
    and we explain the distinction here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会听到关于目标函数、损失函数和损失函数的讨论，有时可以互换使用。然而，它们并不完全相同，我们在这里解释了区别：
- en: A **loss function** is a function that is defined on a single data point, and,
    considering the prediction of the model and the ground truth for the data point,
    computes a penalty.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**是在单个数据点上定义的函数，它考虑了模型的预测和该数据点的真实值，计算一个惩罚。'
- en: A **cost function** takes into account the whole dataset used for training (or
    a batch from it), computing a sum or average over the loss penalties of its data
    points. It can comprise further constraints, such as the L1 or L2 penalties, for
    instance. The cost function directly affects how the training happens.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代价函数**考虑了用于训练的整个数据集（或其一部分批次），通过计算其数据点的损失惩罚的总和或平均值。它可以包括进一步的约束，例如L1或L2惩罚，例如。代价函数直接影响到训练过程。'
- en: 'An **objective function** is the most general (and safe-to-use) term related
    to the scope of optimization during machine learning training: it comprises cost
    functions, but it is not limited to them. An objective function, in fact, can
    also take into account goals that are not related to the target: for instance,
    requiring sparse coefficients of the estimated model or a minimization of the
    coefficients’ values, such as in L1 and L2 regularizations. Moreover, whereas
    loss and cost functions imply an optimization based on minimization, an objective
    function is neutral and can imply either a maximization or a minimization activity
    performed by the learning algorithm.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数**是与机器学习训练中优化范围最通用（且安全使用）的术语：它包括代价函数，但不仅限于它们。实际上，目标函数还可以考虑与目标无关的目标：例如，要求估计模型的稀疏系数或系数值的极小化，如在L1和L2正则化中。此外，虽然损失和代价函数暗示基于最小化的优化过程，但目标函数是中立的，可以暗示学习算法执行的是最大化或最小化活动。'
- en: 'Likewise, when it comes to evaluation metrics, you’ll hear about scoring functions
    and error functions. Distinguishing between them is easy: a **scoring function**
    suggests better prediction results if scores from the function are higher, implying
    a maximization process.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当涉及到评估指标时，你会听到关于评分函数和误差函数的讨论。区分它们很容易：一个**评分函数**如果函数的分数更高，则暗示更好的预测结果，这意味着一个最大化过程。
- en: An **error function** instead suggests better predictions if smaller error quantities
    are reported by the function, implying a minimization process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**误差函数**则暗示如果函数报告较小的误差量，则会有更好的预测，这意味着一个最小化过程。'
- en: Basic types of tasks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本任务类型
- en: 'Not all objective functions are suitable for all problems. From a general point
    of view, you’ll find two kinds of problems in Kaggle competitions: **regression**
    tasks and **classification** tasks. Recently, there have also been **reinforcement
    learning** (**RL**) tasks, but RL doesn’t use metrics for evaluation; instead,
    it relies on a ranking derived from direct match-ups against other competitors
    whose solutions are assumed to be as well-performing as yours (performing better
    in this match-up than your peers will raise your ranking, performing worse will
    lower it). Since RL doesn’t use metrics, we will keep on referring to the regression-classification
    dichotomy, though **ordinal** tasks, where you predict ordered labels represented
    by integer numbers, may elude such categorization and can be dealt with successfully
    either using a regression or classification approach.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有目标函数都适用于所有问题。从一般的角度来看，你会在Kaggle竞赛中找到两种类型的问题：**回归**任务和**分类**任务。最近，也出现了**强化学习**（**RL**）任务，但RL不使用指标进行评估；相反，它依赖于与其他竞争对手的直接对抗产生的排名，假设这些竞争对手的解决方案与你的表现相当（在这个对抗中表现优于你的同伴将提高你的排名，表现较差将降低它）。由于RL不使用指标，我们仍将继续提到回归-分类的二分法，尽管**序数**任务，其中你预测由整数表示的有序标签，可能逃避这种分类，并且可以使用回归或分类方法成功处理。
- en: Regression
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: '**Regression** requires you to build a model that can predict a real number;
    often a positive number, but there have been examples of negative number prediction
    too. A classic example of a regression problem is *House Prices - Advanced Regression
    Techniques*, because you have to guess the value of a house. The evaluation of
    a regression task involves computing a distance between your predictions and the
    values of the ground truth. This difference can be evaluated in different ways,
    for instance by squaring it in order to punish larger errors, or by applying a
    log to it in order to penalize predictions of the wrong scale.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**需要你构建一个可以预测实数的模型；通常是一个正数，但也有预测负数的例子。一个经典的回归问题示例是 *房价 - 高级回归技术*，因为你需要猜测房屋的价值。回归任务的评估涉及计算你的预测与真实值之间的距离。这种差异可以通过不同的方式来评估，例如通过平方它来惩罚较大的错误，或者通过对其应用对数来惩罚错误尺度的预测。'
- en: Classification
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: When facing a **classification** task on Kaggle, there are more nuances to take
    into account. The classification, in fact, could be **binary**, **multi-class**,
    or **multi-label**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 Kaggle 面对分类任务时，需要考虑更多的细微差别。实际上，分类可以是 **二元**、**多类** 或 **多标签**。
- en: 'In **binary** problems, you have to guess if an example should be classified
    or not into a specific class (usually called the *positive* class and compared
    to the *negative* one). Here, the evaluation could involve the straightforward
    prediction of the class ownership itself, or an estimation of the probability
    of such ownership. A typical example is the *Titanic* competition, where you have
    to guess a binary outcome: survival or not-survival. In this case, the requirement
    of the competition is just the prediction, but in many cases, it is necessary
    to provide a probability because in certain fields, especially for medical applications,
    it is necessary to rank positive predictions across different options and situations
    in order to make the best decision.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **二元**问题中，你必须猜测一个示例是否应该被分类到特定的类别中（通常称为 *正* 类，并与 *负* 类进行比较）。在这里，评估可能包括对类归属的直接预测，或者对这种归属概率的估计。一个典型的例子是
    *泰坦尼克号* 比赛，你必须猜测一个二元结果：是否幸存。在这种情况下，比赛的要求仅仅是预测，但在许多情况下，提供概率是必要的，因为在某些领域，特别是医疗应用中，需要对不同选项和情况下的阳性预测进行排序，以便做出最佳决策。
- en: Though counting the exact number of correct matches in a binary classification
    may seem a valid approach, this won’t actually work well when there is an imbalance,
    that is, a different number of examples, between the positive and the negative
    class. Classification based on an imbalanced distribution of classes requires
    evaluation metrics that take the imbalance into account, if you want to correctly
    track improvements on your model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在二元分类中计算正确匹配的确切数量似乎是一个有效的方法，但当正负类之间存在不平衡时，这实际上并不会很好地工作，也就是说，正负类中的示例数量不同。基于类别不平衡分布的分类需要考虑不平衡的评估指标，如果你想正确跟踪模型上的改进。
- en: When you have more than two classes, you have a **multi-class** prediction problem.
    This also requires the use of suitable functions for evaluation, since it is necessary
    to keep track of the overall performance of the model, but also to ensure that
    the performance across the classes is comparable (for instance, your model could
    underperform with respect to certain classes). Here, each case can be in one class
    exclusively, and not in any others. A good example is *Leaf Classification* ([https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification)),
    where each image of a leaf specimen has to be associated with the correct plant
    species.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有超过两个类别时，你面临的是一个 **多类** 预测问题。这也需要使用合适的评估函数，因为需要跟踪模型的总体性能，同时也要确保跨类别的性能是可比的（例如，你的模型可能在某些类别上表现不佳）。在这里，每个案例只能属于一个类别，而不能属于其他任何类别。一个很好的例子是
    *叶分类* ([https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification))，其中每张叶片样本的图像都必须与正确的植物物种相关联。
- en: Finally, when your class predictions are not exclusive and you can predict multiple
    class ownership for each example, you have a **multi-label** problem that requires
    further evaluations in order to control whether your model is predicting the correct
    classes, as well as the correct number and mix of classes. For instance, in *Greek
    Media Monitoring Multilabel Classification (WISE 2014)* ([https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)),
    you had to associate each article with all the topics it deals with.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当你的类别预测不是互斥的，并且你可以为每个示例预测多个类别所有权时，你就遇到了一个**多标签**问题，这需要进一步的评估来控制你的模型是否正在预测正确的类别，以及正确的类别数量和组合。例如，在*希腊媒体监测多标签分类（WISE
    2014）*（[https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)）中，你必须将每篇文章与它所涉及的所有主题关联起来。
- en: Ordinal
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有序
- en: In a problem involving a prediction on an ordinal scale, you have to guess integer
    numeric labels, which are naturally ordered. As an example, the magnitude of an
    earthquake is on an ordinal scale. In addition, data from marketing research questionnaires
    is often recorded on ordinal scales (for instance, consumers’ preferences or opinion
    agreement). Since an ordinal scale is made of ordered values, ordinal tasks can
    be considered somewhat halfway between regression and classification, and you
    can solve them in both ways.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个涉及对有序尺度预测的问题中，你必须猜测有序的整数数值标签，这些标签自然是按顺序排列的。例如，地震的震级就属于有序尺度。此外，市场调研问卷中的数据通常也记录在有序尺度上（例如，消费者的偏好或意见一致性）。由于有序尺度由有序值组成，因此有序任务可以被视为介于回归和分类之间的一种任务，你可以用这两种方式来解决它们。
- en: The most common way is to treat your ordinal task as a **multi-class** problem.
    In this case, you will get a prediction of an integer value (the class label)
    but the prediction will not take into account that the classes have a certain
    order. You can get a feeling that there is something wrong with approaching the
    problem as a multi-class problem if you look at the prediction probability for
    the classes. Often, probabilities will be distributed across the entire range
    of possible values, depicting a multi-modal and often asymmetric distribution
    (whereas you should expect a Gaussian distribution around the maximum probability
    class).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的方法是将你的有序任务视为一个**多类**问题。在这种情况下，你将得到一个整数值（类别标签）的预测，但预测不会考虑这些类别有一定的顺序。如果你查看类别的预测概率，你会感觉到将问题作为多类问题处理可能存在问题。通常，概率将分布在所有可能值的整个范围内，描绘出一个多模态且通常不对称的分布（而你应该期望最大概率类周围有一个高斯分布）。
- en: The other way to solve the ordinal prediction problem is to treat it as a **regression**
    problem and then post-process your result. In this way, the order among classes
    will be taken into consideration, though the prediction output won’t be immediately
    useful for scoring on the evaluation metric. In fact, in a regression you get
    a float number as an output, not an integer representing an ordinal class; moreover,
    the result will include the full range of values between the integers of your
    ordinal distribution and possibly also values outside of it. Cropping the output
    values and casting them into integers by unit rounding may do the trick, but this
    might lead to inaccuracies requiring some more sophisticated post-processing (we’ll
    discuss more on this later in the chapter).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解决有序预测问题的另一种方法是将它视为一个**回归**问题，然后对结果进行后处理。这样，类之间的顺序将被考虑在内，尽管预测输出不会立即用于评估指标上的评分。实际上，在回归中，你得到的是一个浮点数作为输出，而不是表示有序类的整数；此外，结果将包括你有序分布中的整数之间的全部值，甚至可能还包括它之外的值。通过裁剪输出值并将它们通过单位舍入转换为整数可能可行，但这可能会导致一些需要更复杂后处理的误差（我们将在本章后面进一步讨论这个问题）。
- en: Now, you may be wondering what kind of evaluation you should master in order
    to succeed in Kaggle. Clearly, you always have to master the evaluation metric
    of the competition you have taken on. However, some metrics are more common than
    others, which is information you can use to your advantage. What are the most
    common metrics? How can we figure out where to look for insights in competitions
    that have used similar evaluation metrics? The answer is to consult the Meta Kaggle
    dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道为了在Kaggle上取得成功，你应该掌握哪种类型的评估。显然，你必须掌握你所参加的竞赛的评估指标。然而，一些指标比其他指标更常见，这是你可以利用的信息。最常见的指标是什么？我们如何确定在使用了类似评估指标的竞赛中寻找洞察力的地方？答案是查阅Meta
    Kaggle数据集。
- en: The Meta Kaggle dataset
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    is a collection of rich data about Kaggle’s community and activity, published
    by Kaggle itself as a public dataset. It contains CSV tables filled with public
    activity from Competitions, Datasets, Notebooks, and Discussions. All you have
    to do is to start a Kaggle Notebook (as you saw in *Chapters 2* and *3*), add
    to it the Meta Kaggle dataset, and start analyzing the data. The CSV tables are
    updated daily, so you’ll have to refresh your analysis often, but that’s worth
    it given the insights you can extract.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: We will sometimes refer to the Meta Kaggle dataset in this book, both as inspiration
    for many interesting examples of the dynamics in a competition and as a way to
    pick up useful examples for your learning and competition strategies. Here, we
    are going to use it in order to figure out what evaluation metrics have been used
    most frequently for competitions in the last seven years. By looking at the most
    common ones in this chapter, you’ll be able to start any competition from solid
    ground and then refine your knowledge of the metric, picking up competition-specific
    nuances using the discussion you find in the forums.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we introduce the code necessary to produce a data table of metrics and
    their counts per year. It is designed to run directly on the Kaggle platform:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we read in the CSV table containing the data relating to the competitions.
    We focus on the columns representing the evaluation and the columns informing
    us about the competition name, start date, and type. We limit the rows to those
    competitions held since 2015 and that are of the Featured or Research type (which
    are the most common ones). We complete the analysis by creating a pandas pivot
    table, combining the evaluation algorithm with the year, and counting the number
    of competitions using it. We just display the top 20 algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the resulting table (at the time of writing):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '| **year** | **2015** | **2016** | **2017** | **2018** | **2019** | **2020**
    | **2021** | **Tot** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| **Evaluation Algorithm** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| AUC | 4 | 4 | 1 | 3 | 3 | 2 | 0 | 17 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| LogLoss | 2 | 2 | 5 | 2 | 3 | 2 | 0 | 16 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| MAP@{K} | 1 | 3 | 0 | 4 | 1 | 0 | 1 | 10 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| CategorizationAccuracy | 1 | 0 | 4 | 0 | 1 | 2 | 0 | 8 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| MulticlassLoss | 2 | 3 | 2 | 0 | 1 | 0 | 0 | 8 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| RMSLE | 2 | 1 | 3 | 1 | 1 | 0 | 0 | 8 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| QuadraticWeightedKappa | 3 | 0 | 0 | 1 | 2 | 1 | 0 | 7 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| MeanFScoreBeta | 1 | 0 | 1 | 2 | 1 | 2 | 0 | 7 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| MeanBestErrorAtK | 0 | 0 | 2 | 2 | 1 | 1 | 0 | 6 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| MCRMSLE | 0 | 0 | 1 | 0 | 0 | 5 | 0 | 6 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| MCAUC | 1 | 0 | 1 | 0 | 0 | 3 | 0 | 5 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| RMSE | 1 | 1 | 0 | 3 | 0 | 0 | 0 | 5 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| Dice | 0 | 1 | 1 | 0 | 2 | 1 | 0 | 5 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| GoogleGlobalAP | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 5 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| MacroFScore | 0 | 0 | 0 | 1 | 0 | 2 | 1 | 4 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| Score | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 3 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| CRPS | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 3 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| OpenImagesObjectDetectionAP | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 3 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| OpenImagesObjectDetectionAP | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 3 |'
- en: '| MeanFScore | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 3 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| MeanFScore | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 3 |'
- en: '| RSNAObjectDetectionAP | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 2 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| RSNAObjectDetectionAP | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 2 |'
- en: 'Using the same variables we just instantiated in order to generate the table,
    you can also check the data to find the competitions where the metric of your
    choice has been adopted:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚实例化的相同变量来生成表格，你还可以检查数据以找到采用你选择指标的竞赛：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the above snippet, we decided to represent the competitions that have been
    using the AUC metric. You just have to change the string representing the chosen
    metric and the resulting list will be updated accordingly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的片段中，我们决定表示使用 AUC 指标的竞赛。你只需更改表示所选指标的字符串，结果列表将相应更新。
- en: 'Coming back to the table generated, we can examine the most popular evaluation
    metrics used in competitions hosted on Kaggle:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回到生成的表格，我们可以检查在 Kaggle 主办的竞赛中最受欢迎的评估指标：
- en: The two top metrics are closely related to each other and to binary probability
    classification problems. The **AUC** metric helps to measure if your model’s predicted
    probabilities tend to predict positive cases with high probabilities, and the
    **Log Loss** helps to measure how far your predicted probabilities are from the
    ground truth (and as you optimize for Log Loss, you also optimize for the AUC
    metric).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个最重要的指标彼此之间以及与二进制概率分类问题密切相关。**AUC** 指标有助于衡量你的模型预测概率是否倾向于以高概率预测阳性案例，而 **Log
    Loss** 指标有助于衡量你的预测概率与真实值之间的差距（并且当你优化 Log Loss 时，你也在优化 AUC 指标）。
- en: In 3^(rd) position, we find **MAP@{K}**, which is a common metric in recommender
    systems and search engines. In Kaggle competitions, this metric has been used
    mostly for information retrieval evaluations, such as in the *Humpback Whale Identification*
    competition ([https://www.kaggle.com/c/humpback-whale-identification](https://www.kaggle.com/c/humpback-whale-identification)),
    where you have to precisely identify a whale and you have five possible guesses.
    Another example of MAP@{K} usage is in the *Quick, Draw! Doodle Recognition Challenge*
    ([https://www.kaggle.com/c/quickdraw-doodle-recognition/](https://www.kaggle.com/c/quickdraw-doodle-recognition/)),
    where your goal is to guess the content of a drawn sketch and you are allowed
    three attempts. In essence, when MAP@{K} is the evaluation metric, you can score
    not just if you can guess correctly, but also if your correct guess is among a
    certain number (the “K” in the name of the function) of other incorrect predictions.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 3 位，我们发现 **MAP@{K}**，这是推荐系统和搜索引擎中常用的一个指标。在 Kaggle 竞赛中，这个指标主要用于信息检索评估，例如在
    *座头鲸识别* 竞赛（[https://www.kaggle.com/c/humpback-whale-identification](https://www.kaggle.com/c/humpback-whale-identification)）中，你需要在五个可能的猜测中精确识别一只鲸鱼。**MAP@{K}**
    的另一个应用示例是在 *快速绘画！涂鸦识别挑战*（[https://www.kaggle.com/c/quickdraw-doodle-recognition/](https://www.kaggle.com/c/quickdraw-doodle-recognition/)）中，你的目标是猜测所画草图的内容，并且你有三次尝试的机会。本质上，当
    MAP@{K} 是评估指标时，你可以评分的不仅仅是能否正确猜测，还包括你的正确猜测是否在一定的其他错误预测数量（函数名称中的“K”）中。
- en: Only in 6^(th) position can we find a regression metric, the **RMSLE**, or **Root
    Mean Squared Logarithmic Error**, and in 7^(th) place the **Quadratic Weighted
    Kappa**, a metric particularly useful for estimating model performance on problems
    that involve guessing a progressive integer number (an ordinal scale problem).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在第 6 位，我们才能找到一个回归指标，即 **RMSLE** 或 **Root Mean Squared Logarithmic Error**，在第
    7 位是 **Quadratic Weighted Kappa**，这是一个特别有用的指标，用于估计模型在涉及猜测递增整数数（有序尺度问题）的问题上的性能。
- en: As you skim through the list of top metrics, you will keep on finding metrics
    that are commonly discussed in machine learning textbooks. In the next few sections,
    after first discussing what to do when you meet a never-before-seen metric (something
    that happens in Kaggle competitions more frequently than you may expect), we will
    revise some of the most common metrics found in regression and classification
    competitions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当你浏览顶级指标列表时，你将不断发现这些指标在机器学习教科书中经常被讨论。在接下来的几节中，首先讨论当你遇到从未见过的指标时应该做什么（这种情况在 Kaggle
    竞赛中发生的频率可能比你预期的要高），然后我们将回顾回归和分类竞赛中最常见的指标。
- en: Handling never-before-seen metrics
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理从未见过的指标
- en: Before proceeding, we have to consider that the top 20 table doesn’t cover all
    the metrics used in competitions. We should be aware that there are metrics that
    have only been used once in recent years.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须考虑的是，前20名表格并没有涵盖所有比赛中使用的指标。我们应该意识到，近年来有一些指标只被使用过一次。
- en: 'Let’s keep on using the results from the previous code to find out what they
    are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用之前代码的结果来找出它们是什么：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a result, we get the following table showing, for each year, how many competitions
    used a metric that has never been used afterward (`n_comps`), and the proportion
    of these competitions per year (`pct_comps`):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了以下表格，展示了每年有多少比赛使用了此后从未再被使用的指标（`n_comps`），以及这些比赛在每年中的比例（`pct_comps`）：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Observing the relative share of competitions with a never-to-be-seen-afterward
    metric, we immediately notice how it is growing year by year and that it reached
    the 25%-30% level in recent years, implying that typically one competition out
    of every three or four requires you to study and understand a metric from scratch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 观察从未再被使用过的指标的竞赛相对份额，我们立即注意到它逐年增长，并在近年来达到了25%-30%的水平，这意味着通常每三到四个竞赛中就有一个需要你从头开始研究和理解一个指标。
- en: 'You can get the list of such metrics that have occurred in the past with a
    simple code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过一个简单的代码片段获取过去发生过的此类指标列表：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By executing the code, you will get a list similar to this one:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行代码，你会得到一个类似这样的列表：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By close inspection, you can find many metrics relating to deep learning and
    reinforcement learning competitions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细检查，你可以找到许多与深度学习和强化学习比赛相关的指标。
- en: What do you do when you meet a metric that has never been used before? Of course,
    you can rely on the discussions in the Kaggle discussion forums, where you can
    always find good inspiration and many Kagglers who will help you. However, if
    you want to build up your own knowledge about the metric, aside from Googling
    it, we advise that you try to experiment with it by coding the evaluation function
    by yourself, even in an imperfect way, and trying to simulate how the metric reacts
    to different types of error produced by the model. You could also directly test
    how it functions on a sample from the competition training data or synthetic data
    that you have prepared.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你遇到一个以前从未使用过的指标时，你会怎么做？当然，你可以依赖Kaggle讨论论坛中的讨论，在那里你总能找到好的灵感，以及许多愿意帮助你的Kagglers。然而，如果你想建立自己对这一指标的知识，除了在谷歌上搜索之外，我们建议你尝试通过自己编写评估函数来实验它，即使是不完美的，并尝试模拟指标对模型产生的不同类型错误的反应。你也可以直接测试它在比赛训练数据样本或你准备好的合成数据上的功能。
- en: 'We can quote a few examples of this approach as used by Kagglers:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以引用一些Kagglers使用此方法的一些例子：
- en: '*Carlo Lepelaars* with Spearman’s Rho: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho](https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卡洛·莱佩拉尔斯* 使用斯皮尔曼相关系数：[https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho](https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho)'
- en: 'Carlo Lepelaars with Quadratic Weighted Kappa: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa](https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡洛·莱佩拉尔斯使用二次加权Kappa：[https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa](https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa)
- en: '*Rohan Rao* with Laplace Log Likelihood: [https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood](https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*罗汉·拉奥* 使用拉普拉斯对数似然：[https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood](https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood)'
- en: This can give you increased insight into the evaluation and an advantage over
    other competitors relying only on answers from Googling and Kaggle forums.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以让你对评估有更深入的了解，并比那些只依赖谷歌搜索和Kaggle论坛答案的竞争对手有优势。
- en: '![](img/Rohan_Rao.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Rohan_Rao.png)'
- en: Rohan Rao
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 罗汉·拉奥
- en: '[https://www.kaggle.com/rohanrao](https://www.kaggle.com/rohanrao)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/rohanrao](https://www.kaggle.com/rohanrao)'
- en: Before we start exploring different metrics, let’s catch up with Rohan Rao (aka
    Vopani) himself, Quadruple Grandmaster and Senior Data Scientist at `H2O.ai`,
    about his successes on Kaggle and the wisdom he has to share with us.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索不同的指标之前，让我们了解一下罗汉·拉奥（又名Vopani）本人，他是`H2O.ai`的超级大师级高级数据科学家，关于他在Kaggle上的成功以及他要与我们分享的智慧。
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your speciality on Kaggle?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在Kaggle上，你在技术和解决方法方面有什么专长？
- en: '*I like to dabble with different types of competitions, but my favorite would
    certainly be time series ones. I don’t quite like the typical approaches to and
    concepts of time series in the industry, so I tend to innovate and think out of
    the box by building solutions in an unorthodox way, which has ended up being very
    successful for me.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢尝试不同类型的比赛，但我的最爱无疑是时间序列比赛。我不太喜欢行业中对时间序列的典型方法和概念，所以我倾向于通过以非常规的方式构建解决方案来创新和跳出思维定势，这对我的成功非常有帮助。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何处理Kaggle比赛？这种处理方式与你的日常工作有何不同？
- en: '*For any Kaggle competition, my typical workflow would look like this:*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于任何Kaggle比赛，我的典型工作流程如下：*'
- en: '*Understand the problem statement and read all the information related to rules,
    format, timelines, datasets, metrics, and deliverables.*'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解问题陈述，并阅读所有与规则、格式、时间表、数据集、指标和交付成果相关的信息。*'
- en: '*Dive deep into the data. Slice and dice it in every way possible and explore/visualize
    it to be able to answer any question about it.*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深入数据。以任何可能的方式切割和分解它，并探索/可视化它，以便能够回答关于它的任何问题。*'
- en: '*Build a simple pipeline with a baseline model and make a submission to confirm
    the process works.*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用基线模型构建一个简单的流水线，并提交以确认流程是否有效。*'
- en: '*Engineer features, tune hyperparameters, and* *experiment with multiple models
    to get a sense of what’s generally working and what’s not.*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工程师特征，调整超参数，并且* *尝试多个模型以了解哪些通常有效，哪些无效。*'
- en: '*Constantly go back to analyzing the data, reading discussions on the forum,
    and tweaking the features and models to the fullest. Maybe team up at some point.*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不断地回到分析数据，阅读论坛上的讨论，并尽可能调整特征和模型。也许在某个时刻可以组建团队。*'
- en: '*Ensemble multiple models and decide which submissions to make as final.*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成多个模型，并决定哪些提交作为最终版本。*'
- en: '*In my day-to-day work in data science, most of this happens too. But there
    are two crucial elements that are additionally required:*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的数据科学日常工作中，这些事情也经常发生。但还有两个额外需要的关键要素：*'
- en: '*Curating and preparing datasets for the problem statement.*'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为问题陈述准备和整理数据集。*'
- en: '*Deploying the final model or solution into production.*'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将最终模型或解决方案部署到生产环境中。*'
- en: '*The majority of my time has been spent in these two activities for most of
    the projects I’ve worked on in the past.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在过去参与的大多数项目中，大部分时间都花在这两个活动上。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你的职业生涯？如果是，是如何帮助的？
- en: '*The vast majority of everything I’ve learned in machine learning has come
    from Kaggle. The community, the platform, and the content are pure gold and there
    is an incredible amount of stuff you can learn.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在机器学习中学到的绝大多数东西都来自Kaggle。社区、平台和内容都是纯金，你可以学到的东西非常多。*'
- en: '*What has benefitted me the most is the experience of competing in Kaggle competitions;
    it has given me immense confidence in understanding, structuring, and solving
    problems across domains, which I have been able to apply successfully in many
    of the companies and projects I worked on outside Kaggle.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*对我最有益的是参加Kaggle比赛的经验；它使我在理解、构建和解决跨领域问题方面有了巨大的信心，我能够成功地将其应用于Kaggle之外的公司和项目中。*'
- en: '*Many recruiters have contacted me for opportunities looking at my Kaggle achievements,
    primarily in Competitions. It gives a fairly good indication of a candidate’s
    ability in solving data science problems and hence it is a great platform to showcase
    your skills* *and build a portfolio.*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*许多招聘人员联系我，寻求查看我的Kaggle成就的机会，主要是在竞赛方面。它相当好地表明了候选人在解决数据科学问题方面的能力，因此这是一个展示你的技能和建立作品集的绝佳平台。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*I’ve made some mistake in every competition! That’s how you learn and improve.
    Sometimes it’s a coding bug, sometimes a flawed validation setup, sometimes an
    incorrect submission selection!*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在每一场比赛中都犯过一些错误！这就是你学习和进步的方式。有时是编码错误，有时是验证设置有缺陷，有时是提交选择不正确！*'
- en: '*What’s important is to learn from these and ensure you don’t repeat them.
    Iterating over this process automatically helps to improve your overall performance
    on Kaggle.*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*重要的是要从这些经验中学习，并确保你不会重复它们。自动迭代这个过程有助于提高你在Kaggle上的整体表现。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis/machine learning?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析/机器学习？
- en: '*I strongly believe in never marrying a technology. Use whatever works best,
    whatever is most comfortable and effective, but constantly be open to learning
    new tools and libraries.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*我坚信永远不要与一项技术结婚。使用最好的，最舒适和最有效的，但始终开放学习新的工具和库。*'
- en: Metrics for regression (standard and ordinal)
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归的度量（标准和序数）
- en: When working with regression problems, that is, problems that involve estimating
    a continuous value (that could range from minus infinity to infinity), the most
    commonly used error measures are **RMSE** (**root mean squared error**) and **MAE**
    (**mean absolute error**), but you can also find slightly different error measures
    useful, such as RMSLE or MCRMSLE.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理回归问题时，即涉及估计一个连续值（可能从负无穷大到正无穷大）的问题时，最常用的误差度量是**RMSE**（根均方误差）和**MAE**（平均绝对误差），但你也可以发现一些稍微不同的误差度量很有用，例如RMSLE或MCRMSLE。
- en: Mean squared error (MSE) and R squared
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均方误差（MSE）和R平方
- en: The root mean squared error is the root of the **mean squared error** (**MSE**),
    which is nothing else but the mean of the good old **sum of squared errors** (**SSE**)
    that you learned about when you studied how a regression works.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根均方误差是**均方误差**（MSE）的平方根，这实际上就是你在学习回归如何工作时学到的**平方误差和**（SSE）的平均值。
- en: 'Here is the formula for the MSE:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是MSE的公式：
- en: '![](img/B17574_05_001.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_001.png)'
- en: 'Let’s start by explaining how the formula works. First of all, *n* indicates
    the number of cases, ![](img/B17574_05_002.png) is the ground truth, and ![](img/B17574_05_003.png)
    the prediction. You first get the difference between your predictions and your
    real values. You square the differences (so they become positive or simply zero),
    then you sum them all, resulting in your SSE. Then you just have to divide this
    measure by the number of predictions to obtain the average value, the MSE. Usually,
    all regression models minimize the SSE, so you won’t have great problems trying
    to minimize MSE or its direct derivatives such as **R squared** (also called the
    **coefficient of determination**), which is given by:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先解释一下公式是如何工作的。首先，*n*表示案例数量，![](img/B17574_05_002.png)是真实值，![](img/B17574_05_003.png)是预测值。你首先得到你的预测值和真实值之间的差异。然后你对差异进行平方（这样它们就变成正数或简单地为零），然后将它们全部相加，得到你的SSE。然后你只需将这个度量除以预测的数量，以获得平均值，即MSE。通常，所有回归模型都最小化SSE，所以你不会在尝试最小化MSE或其直接导数（如**R平方**，也称为**确定系数**）时遇到太大问题，它由以下公式给出：
- en: '![](img/B17574_05_004.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_004.png)'
- en: 'Here, SSE (the sum of squared errors) is compared to the **sum of squares total**
    (**SST**), which is just the variance of the response. In statistics, in fact,
    SST is defined as the squared difference between your target values and their
    mean:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，SSE（平方误差和）与**平方和总**（SST）进行比较，这实际上是响应的方差。实际上，在统计学中，SST被定义为你的目标值与它们的平均值之间的平方差：
- en: '![](img/B17574_05_005.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_005.png)'
- en: To put it another way, R squared compares the squared errors of the model against
    the squared errors from the simplest model possible, the average of the response.
    Since both SSE and SST have the same scale, R squared can help you to determine
    whether transforming your target is helping to obtain better predictions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，R平方比较了模型平方误差与可能的最简单模型（响应平均值）的平方误差。由于SSE和SST具有相同的尺度，R平方可以帮助你确定转换目标是否有助于获得更好的预测。
- en: Please remember that linear transformations, such as minmax ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))
    or standardization ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),
    do not change the performance of any regressor, since they are linear transformations
    of the target. **Non-linear** transformations, such as the square root, the cubic
    root, the logarithm, the exponentiation, and their combinations, should instead
    definitely modify the performance of your regression model on the evaluation metric
    (hopefully for the better, if you decide on the right transformation).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，线性变换，如minmax（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)）或标准化（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)），不会改变任何回归器的性能，因为它们是目标值的线性变换。**非线性**变换，如平方根、立方根、对数、指数及其组合，则应该肯定地修改你的回归模型在评估指标上的性能（如果你选择了正确的变换，希望是更好的）。
- en: MSE is a great instrument for comparing regression models applied to the same
    problem. The bad news is that the MSE is seldom used in Kaggle competitions, since
    RMSE is preferred. In fact, by taking the root of MSE, its value will resemble
    the original scale of your target and it will be easier at a glance to figure
    out if your model is doing a good job or not. In addition, if you are considering
    the same regression model across different data problems (for instance, across
    various datasets or data competitions), R squared is better because it is perfectly
    correlated with MSE and its values range between 0 and 1, making all comparisons
    easier.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MSE是用于比较应用于同一问题的回归模型的一个很好的工具。坏消息是MSE在Kaggle竞赛中很少使用，因为RMSE更受欢迎。实际上，通过取MSE的根，其值将类似于你的目标原始尺度，这将更容易一眼看出你的模型是否做得很好。此外，如果你正在考虑跨不同数据问题（例如，跨各种数据集或数据竞赛）的相同回归模型，R平方更好，因为它与MSE完美相关，其值介于0和1之间，这使得所有比较都更容易。
- en: Root mean squared error (RMSE)
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根均方误差（RMSE）
- en: 'RMSE is just the square root of MSE, but this implies some subtle change. Here
    is its formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE只是MSE的平方根，但这意味着一些微妙的变化。以下是它的公式：
- en: '![](img/B17574_05_006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_006.png)'
- en: In the above formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. In MSE, large
    prediction errors are greatly penalized because of the squaring activity. In RMSE,
    this dominance is lessened because of the root effect (however, you should always
    pay attention to outliers; they can affect your model performance a lot, no matter
    whether you are evaluating based on MSE or RMSE).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，*n*表示案例数量，![图片](img/B17574_05_002.png)是真实值，![图片](img/B17574_05_008.png)是预测值。在MSE中，由于平方操作，大的预测误差会受到极大的惩罚。在RMSE中，由于根效应，这种主导性减弱（然而，你应该始终注意异常值；无论你是基于MSE还是RMSE进行评估，它们都可能对你的模型性能产生很大影响）。
- en: Consequently, depending on the problem, you can get a better fit with an algorithm
    using MSE as an objective function by first applying the square root to your target
    (if possible, because it requires positive values), then squaring the results.
    Functions such as the `TransformedTargetRegressor` in Scikit-learn help you to
    appropriately transform your regression target in order to get better-fitting
    results with respect to your evaluation metric.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据问题，你可以在将目标值开平方（如果可能的话，因为这需要正值）然后平方结果之后，使用均方误差（MSE）作为目标函数来获得更好的算法拟合。Scikit-learn中的`TransformedTargetRegressor`等函数可以帮助你适当地转换回归目标，以便在评估指标方面获得更好的拟合结果。
- en: 'Recent competitions where RMSE has been used include:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最近使用RMSE的竞赛包括：
- en: '*Avito Demand Prediction Challenge*: [https://www.kaggle.com/c/avito-demand-prediction](https://www.kaggle.com/c/avito-demand-prediction)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Avito Demand Prediction Challenge*：[https://www.kaggle.com/c/avito-demand-prediction](https://www.kaggle.com/c/avito-demand-prediction)'
- en: '*Google Analytics Customer Revenue Prediction*: [https://www.kaggle.com/c/ga-customer-revenue-prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google Analytics Customer Revenue Prediction*：[https://www.kaggle.com/c/ga-customer-revenue-prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction)'
- en: '*Elo Merchant Category Recommendation* [https://www.kaggle.com/c/elo-merchant-category-recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elo商家类别推荐* [https://www.kaggle.com/c/elo-merchant-category-recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)'
- en: Root mean squared log error (RMSLE)
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根均方对数误差 (RMSLE)
- en: 'Another common transformation of MSE is **root mean squared log error** (**RMSLE**).
    MCRMSLE is just a variant made popular by the COVID-19 forecasting competitions,
    and it is the column-wise average of the RMSLE values of each single target when
    there are multiple ones. Here is the formula for RMSLE:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: MSE（均方误差）的另一种常见转换是**根均方对数误差**（**RMSLE**）。MCRMSLE只是由COVID-19预测竞赛推广的一种变体，它是当存在多个单一目标时，每个目标RMSLE值的列平均值。以下是RMSLE的公式：
- en: '![](img/B17574_05_009.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_009.png)'
- en: In the formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. Since you
    are applying a logarithmic transformation to your predictions and your ground
    truth before all the other squaring, averaging, and rooting operations, you don’t
    penalize huge differences between the predicted and the actual values, especially
    when both are large numbers. In other words, what you care the most about when
    using RMSLE is *the scale of your predictions with respect to the scale of the
    ground truth*. As with RMSE, machine learning algorithms for regression can better
    optimize for RMSLE if you apply a logarithmic transformation to the target before
    fitting it (and then reverse the effect using the exponential function).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*n*表示案例数量，![](img/B17574_05_002.png)是真实值，![](img/B17574_05_008.png)是预测值。由于你在所有其他平方、平均和开根运算之前对预测值和真实值应用了对数变换，因此你不会对预测值和实际值之间的大差异进行惩罚，尤其是在两者都是大数的情况下。换句话说，当你使用RMSLE时，你最关心的是*预测值与真实值规模的比例*。与RMSE一样，如果你在拟合之前对目标应用对数变换，机器学习算法可以更好地优化RMSLE（然后使用指数函数逆转效果）。
- en: 'Recent competitions using RMSLE as an evaluation metric are:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最近使用RMSLE作为评估指标的竞赛包括：
- en: '*ASHRAE - Great Energy Predictor III*: [https://www.kaggle.com/c/ashrae-energy-prediction](https://www.kaggle.com/c/ashrae-energy-prediction)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ASHRAE - 伟大能源预测者III*: [https://www.kaggle.com/c/ashrae-energy-prediction](https://www.kaggle.com/c/ashrae-energy-prediction)'
- en: '*Santander Value Prediction Challenge*: [https://www.kaggle.com/c/santander-value-prediction-challenge](https://www.kaggle.com/c/santander-value-prediction-challenge)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Santander价值预测挑战*: [https://www.kaggle.com/c/santander-value-prediction-challenge](https://www.kaggle.com/c/santander-value-prediction-challenge)'
- en: '*Mercari Price Suggestion Challenge*: [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mercari价格建议挑战*: [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)'
- en: '*Sberbank Russian Housing Market*: [https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market](https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sberbank俄罗斯住房市场*: [https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market](https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market)'
- en: '*Recruit Restaurant Visitor Forecasting*: [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Recruit餐厅访客预测*: [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)'
- en: By far, at the moment, RMSLE is the most used evaluation metric for regression
    in Kaggle competitions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，RMSLE（Root Mean Squared Log Error，根均方对数误差）是Kaggle竞赛中回归问题最常用的评估指标。
- en: Mean absolute error (MAE)
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均值绝对误差 (MAE)
- en: 'The **MAE** (**mean absolute error**) evaluation metric is the absolute value
    of the difference between the predictions and the targets. Here is the formulation
    of MAE:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAE**（**平均绝对误差**）评估指标是预测值与目标之间的差异的绝对值。以下是MAE的公式：'
- en: '![](img/B17574_05_012.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_012.png)'
- en: In the formula, *n* stands for the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. MAE is not
    particularly sensitive to outliers (unlike MSE, where errors are squared), hence
    you may find it is an evaluation metric in many competitions whose datasets present
    outliers. Moreover, you can easily work with it since many algorithms can directly
    use it as an objective function; otherwise, you can optimize for it indirectly
    by just training on the square root of your target and then squaring the predictions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*n*代表案例数量，![](img/B17574_05_002.png)是真实值，而![](img/B17574_05_008.png)是预测值。MAE对异常值不敏感（与MSE不同，其中误差是平方的），因此你可能会发现它是在许多竞赛中使用的评估指标，这些竞赛的数据集包含异常值。此外，你可以轻松地与之合作，因为许多算法可以直接将其用作目标函数；否则，你可以通过仅对目标的平方根进行训练，然后对预测值进行平方来间接优化它。
- en: 'In terms of downside, using MAE as an objective function results in much slower
    convergence, since you are actually optimizing for predicting the median of the
    target (also called the L1 norm), instead of the mean (also called the L2 norm),
    as occurs by MSE minimization. This results in more complex computations for the
    optimizer, so the training time can even grow exponentially based on your number
    of training cases (see, for instance, this Stack Overflow question: [https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to](https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下行方面，使用MAE作为目标函数会导致收敛速度大大减慢，因为你实际上是在优化预测目标的中位数（也称为L1范数），而不是均值（也称为L2范数），正如MSE最小化所发生的那样。这导致优化器需要进行更复杂的计算，因此训练时间甚至可以根据你的训练案例数量呈指数增长（例如，参见这个Stack
    Overflow问题：[https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to](https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to))。
- en: 'Notable recent competitions that used MAE as an evaluation metric are:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一些使用MAE作为评估指标的著名竞赛包括：
- en: '*LANL Earthquake Prediction*: [https://www.kaggle.com/c/LANL-Earthquake-Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LANL地震预测*: [https://www.kaggle.com/c/LANL-Earthquake-Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction)'
- en: '*How Much Did It Rain? II*: [https://www.kaggle.com/c/how-much-did-it-rain-ii](https://www.kaggle.com/c/how-much-did-it-rain-ii)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降雨量有多少？II*: [https://www.kaggle.com/c/how-much-did-it-rain-ii](https://www.kaggle.com/c/how-much-did-it-rain-ii)'
- en: Having mentioned the ASHRAE competition earlier, we should also mention that
    regression evaluation measures are quite relevant to forecasting competitions.
    For instance, the M5 forecasting competition was held recently ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))
    and data from all the other M competitions is available too. If you are interested
    in forecasting competitions, of which there are a few on Kaggle, please see [https://robjhyndman.com/hyndsight/forecasting-competitions/](https://robjhyndman.com/hyndsight/forecasting-competitions/)
    for an overview about M competitions and how valuable Kaggle is for obtaining
    better practical and theoretical results from such competitions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到了ASHRAE竞赛之后，我们也应该提到回归评估指标与预测竞赛的相关性。例如，最近举办了M5预测竞赛([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))，而且所有其他M竞赛的数据也是可用的。如果你对预测竞赛感兴趣，其中Kaggle上有几个，请参阅[https://robjhyndman.com/hyndsight/forecasting-competitions/](https://robjhyndman.com/hyndsight/forecasting-competitions/)以了解M竞赛的概述以及Kaggle在从这些竞赛中获得更好的实际和理论结果方面的重要性。
- en: Essentially, forecasting competitions do not require a very different evaluation
    to regression competitions. When dealing with forecasting tasks, it is true that
    you can get some unusual evaluation metrics such as the **Weighted Root Mean Squared
    Scaled Error** ([https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation))
    or the **symmetric mean absolute percentage error**, better known as **sMAPE**
    ([https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation](https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation)).
    However, in the end they are just variations of the usual RMSE or MAE that you
    can handle using the right target transformations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for classification (label prediction and probability)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having discussed the metrics for regression problems, we are going now to illustrate
    the metrics for classification problems, starting from the binary classification
    problems (when you have to predict between two classes), moving to the multi-class
    (when you have more than two classes), and then to the multi-label (when the classes
    overlap).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When analyzing the performance of a binary classifier, the most common and
    accessible metric that is used is **accuracy**. A misclassification error is when
    your model predicts the wrong class for an example. The accuracy is just the complement
    of the misclassification error and it can be calculated as the ratio between the
    number of correct numbers divided by the number of answers:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_015.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: This metric has been used, for instance, in *Cassava Leaf Disease Classification*
    ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification))
    and *Text Normalization Challenge - English Language* ([https://www.kaggle.com/c/text-normalization-challenge-english-language](https://www.kaggle.com/c/text-normalization-challenge-english-language)),
    where you scored a correct prediction only if your predicted text matched the
    actual string.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'As a metric, the accuracy is focused strongly on the effective performance
    of the model in a real setting: it tells you if the model works as expected. However,
    if your purpose is to evaluate and compare and have a clear picture of how effective
    your approach really is, you have to be cautious when using the accuracy because
    it can lead to wrong conclusions when the classes are imbalanced (when they have
    different frequencies). For instance, if a certain class makes up just 10% of
    the data, a predictor that predicts nothing but the majority class will be 90%
    accurate, proving itself quite useless in spite of the high accuracy.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'How can you spot such a problem? You can do this easily by using a **confusion
    matrix**. In a confusion matrix, you create a two-way table comparing the actual
    classes on the rows against the predicted classes on the columns. You can create
    a straightforward one using the Scikit-learn `confusion_matrix` function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Providing the `y_true` and `y_pred` vectors will suffice to return you a meaningful
    table, but you can also provide row/column labels and sample weights for the examples
    in consideration, and normalize (set the marginals to sum to 1) over the true
    examples (the rows), the predicted examples (the columns), or all the examples.
    A perfect classifier will have all the cases on the principal diagonal of the
    matrix. Serious problems with the validity of the predictor are highlighted if
    there are few or no cases on one of the cells of the diagonal.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to give you a better idea of how it works, you can try the graphical
    example offered by Scikit-learn at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Confusion matrix, with each cell normalized to 1.00, to represent
    the share of matches'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You can attempt to improve the usability of the accuracy by considering the
    accuracy relative to each of the classes and averaging them, but you will find
    it more useful to rely on other metrics such as **precision**, **recall**, and
    the **F1-score**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To obtain the precision and recall metrics, we again start from the confusion
    matrix. First, we have to name each of the cells:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '|  |  | **Negative** | **Positive** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| **Actual** | **Negative** | True Negative | False Positive |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| **Positive** | False Negative | True Positive |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Confusion matrix with cell names'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we define the cells:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**TP** (**true positives**): These are located in the upper-left cell, containing
    examples that have correctly been predicted as positive ones.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP** (**false positives**): These are located in the upper-right cell, containing
    examples that have been predicted as positive but are actually negative.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN** (**false negatives**): These are located in the lower-left cell, containing
    examples that have been predicted as negative but are actually positive.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TN** (**true negatives**): These are located in the lower-right cell, containing
    examples that have been correctly predicted as negative ones.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these cells, you can actually get more precise information about how
    your classifier works and how you can tune your model better. First, we can easily
    revise the accuracy formula:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_016.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Then, the first informative metric is called **precision** (or **specificity**)
    and it is actually the accuracy of the positive cases:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_017.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: In the computation, only the number of true positives and the number of false
    positives are involved. In essence, the metric tells you how often you are correct
    when you predict a positive.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, your model could get high scores by predicting positives for only
    the examples it has high confidence in. That is actually the purpose of the measure:
    to force models to predict a positive class only when they are sure and it is
    safe to do so.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if it is in your interest also to predict as many positives as possible,
    then you’ll also need to watch over the **recall** (or **coverage** or **sensitivity**
    or even **true positive rate**) metric:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_018.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Here, you will also need to know about false negatives. The interesting thing
    about these two metrics is that, since they are based on examples classification,
    and a classification is actually based on probability (which is usually set between
    the positive and negative class at the `0.5` threshold), you can change the threshold
    and have one of the two metrics improved at the expense of the other.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you increase the threshold, you will get more precision (the
    classifier is more confident of the prediction) but less recall. If you decrease
    the threshold, you get less precision but more recall. This is also called the
    **precision/recall trade-off**.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scikit-learn website offers a simple and practical overview of this trade-off
    ([https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)),
    helping you to trace a **precision/recall curve** and thus understand how these
    two measures can be exchanged to obtain a result that better fits your needs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_02.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: A two-class precision-recall curve with its characteristic steps'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: One metric associated with the precision/recall trade-off is the **average precision**.
    Average precision computes the mean precision for recall values from 0 to 1 (basically,
    as you vary the threshold from 1 to 0). Average precision is very popular for
    tasks related to object detection, which we will discuss a bit later on, but it
    is also very useful for classification in tabular data. In practice, it proves
    valuable when you want to monitor model performance on a very rare class (when
    the data is extremely imbalanced) in a more precise and exact way, which is often
    the case with fraud detection problems.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'For more specific insights on this, read *Gael Varoquaux’s* discussion: [http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision](http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, you have probably already figured out that using precision or
    recall as an evaluation metric is not an ideal choice because you can only optimize
    one at the expense of the other. For this reason, there are no Kaggle competitions
    that use only one of the two metrics. You should combine them (as in the average
    precision). A single metric, the **F1 score**, which is the harmonic mean of precision
    and recall, is commonly considered to be the best solution:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_019.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: If you get a high *F*1 score, it is because your model has improved in precision
    or recall or in both. You can find a fine example of the usage of this metric
    in the *Quora* *Insincere Questions Classification* competition ([https://www.kaggle.com/c/quora-insincere-questions-classification](https://www.kaggle.com/c/quora-insincere-questions-classification)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'In some competitions, you also get the **F-beta** score. This is simply the
    weighted harmonic mean between precision and recall, and beta decides the weight
    of the recall in the combined score:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_020.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Since we have already introduced the concept of threshold and classification
    probability, we can now discuss the log loss and ROC-AUC, both quite common classification
    metrics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Log loss and ROC-AUC
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the **log loss**, which is also known as **cross-entropy**
    in deep learning models. The log loss is the difference between the predicted
    probability and the ground truth probability:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_021.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: In the above formula, *n* stands for the number of examples, ![](img/B17574_05_002.png)
    is the ground truth for the *i*^(th) case, and ![](img/B17574_05_008.png) the
    prediction.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: If a competition uses the log loss, it is implied that the objective is to estimate
    as correctly as possible the probability of an example being of a positive class.
    You can actually find the log loss in quite a lot of competitions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: We suggest you have a look, for instance, at the recent *Deepfake Detection
    Challenge* ([https://www.kaggle.com/c/deepfake-detection-challenge](https://www.kaggle.com/c/deepfake-detection-challenge))
    or at the older *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ROC curve**, or **receiver operating characteristic curve**, is a graphical
    chart used to evaluate the performance of a binary classifier and to compare multiple
    classifiers. It is the building block of the ROC-AUC metric, because the metric
    is simply the area delimited under the ROC curve. The ROC curve consists of the
    true positive rate (the recall) plotted against the false positive rate (the ratio
    of negative instances that are incorrectly classified as positive ones). It is
    equivalent to one minus the true negative rate (the ratio of negative examples
    that are correctly classified). Here are a few examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_03.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Different ROC curves and their AUCs'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a ROC curve of a well-performing classifier should quickly climb up
    the true positive rate (recall) at low values of the false positive rate. A ROC-AUC
    between 0.9 to 1.0 is considered very good.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: A bad classifier can be spotted by the ROC curve appearing very similar, if
    not identical, to the diagonal of the chart, which represents the performance
    of a purely random classifier, as in the top left of the figure above; ROC-AUC
    scores near 0.5 are considered to be almost random results. If you are comparing
    different classifiers, and you are using the **area under the curve** (**AUC**),
    the classifier with the higher area is the more performant one.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: If the classes are balanced, or not too imbalanced, increases in the AUC are
    proportional to the effectiveness of the trained model and they can be intuitively
    thought of as the ability of the model to output higher probabilities for true
    positives. We also think of it as the ability to order the examples more properly
    from positive to negative. However, when the positive class is rare, the AUC starts
    high and its increments may mean very little in terms of predicting the rare class
    better. As we mentioned before, in such a case, average precision is a more helpful
    metric.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'AUC has recently been used for quite a lot of different competitions. We suggest
    you have a look at these three:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '*IEEE-CIS Fraud Detection*: [https://www.kaggle.com/c/ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Riiid Answer Correctness Prediction*: [https://www.kaggle.com/c/riiid-test-answer-prediction](https://www.kaggle.com/c/riiid-test-answer-prediction)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jigsaw Multilingual Toxic Comment Classification*: [https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read a detailed treatise in the following paper: Su, W., Yuan, Y.,
    and Zhu, M. *A relationship between the average precision and the area under the
    ROC curve.* Proceedings of the 2015 International Conference on The Theory of
    Information Retrieval. 2015.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Matthews correlation coefficient (MCC)
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We complete our overview of binary classification metrics with the **Matthews
    correlation coefficient** (**MCC**), which made its appearance in *VSB Power Line
    Fault Detection* ([https://www.kaggle.com/c/vsb-power-line-fault-detection](https://www.kaggle.com/c/vsb-power-line-fault-detection))
    and *Bosch Production Line Performance* ([https://www.kaggle.com/c/bosch-production-line-performance](https://www.kaggle.com/c/bosch-production-line-performance)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for the MCC is:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_024.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: In the above formula, *TP* stands for true positives, *TN* for true negatives,
    *FP* for false positives, and *FN* for false negatives. It is the same nomenclature
    as we met when discussing precision and recall.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Behaving as a correlation coefficient, in other words, ranging from +1 (perfect
    prediction) to -1 (inverse prediction), this metric can be considered a measure
    of the quality of the classification even when the classes are quite imbalanced.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'In spite of its complexity, the formula can be reformulated and simplified,
    as demonstrated by Neuron Engineer ([https://www.kaggle.com/ratthachat](https://www.kaggle.com/ratthachat))
    in his Notebook: [www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc](https://www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The work done by Neuron Engineer in understanding the ratio of the evaluation
    metric is indeed exemplary. In fact, his reformulated MCC becomes:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_025.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: 'Where each element of the formula is:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_026.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_027.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_028.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_029.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_030.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'The reformulation helps to clarify, in a more intelligible form than the original,
    that you can get higher performance from improving both positive and negative
    class precision, but that’s not enough: you also have to have positive and negative
    predictions in proportion to the ground truth, or your submission will be greatly
    penalized.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for multi-class classification
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When moving to multi-class classification, you simply use the binary classification
    metrics that we have just seen, applied to each class, and then you summarize
    them using some of the averaging strategies that are commonly used for multi-class
    situations.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you want to evaluate your solution based on the *F*1 score,
    you have three possible averaging choices:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '**Macro averaging**: Simply calculate the *F*1 score for each class and then
    average all the results. In this way, each class will count as much the others,
    no matter how frequent its positive cases are or how important they are for your
    problem, resulting therefore in equal penalizations when the model doesn’t perform
    well with any class:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_031.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: '**Micro averaging**: This approach will sum all the contributions from each
    class to compute an aggregated *F*1 score. It results in no particular favor to
    or penalization of any class, since all the computations are made regardless of
    each class, so it can more accurately account for class imbalances:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_032.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: '**Weighting**: As with macro averaging, you first calculate the *F*1 score
    for each class, but then you make a weighted average mean of all of them using
    a weight that depends on the number of true labels of each class. By using such
    a set of weights, you can take into account the frequency of positive cases from
    each class or the relevance of that class for your problem. This approach clearly
    favors the majority classes, which will be weighted more in the computations:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17574_05_033.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_05_034.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Common multi-class metrics that you may encounter in Kaggle competitions are:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiclass accuracy (weighted)**: *Bengali.AI Handwritten Grapheme Classification*
    ([https://www.kaggle.com/c/bengaliai-cv19](https://www.kaggle.com/c/bengaliai-cv19))'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass log loss (MeanColumnwiseLogLoss)**: *Mechanisms of Action (MoA)*
    *Prediction* ([https://www.kaggle.com/c/lish-moa/](https://www.kaggle.com/c/lish-moa/))'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macro-F1** and **Micro-F1 (NQMicroF1)**: *University of Liverpool - Ion Switching*
    ([https://www.kaggle.com/c/liverpool-ion-switching](https://www.kaggle.com/c/liverpool-ion-switching)),
    *Human Protein Atlas Image Classification* ([https://www.kaggle.com/c/human-protein-atlas-image-classification/](https://www.kaggle.com/c/human-protein-atlas-image-classification/)),
    *TensorFlow* *2.0 Question* *Answering* ([https://www.kaggle.com/c/tensorflow2-question-answering](https://www.kaggle.com/c/tensorflow2-question-answering))'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean-F1**: *Shopee - Price Match Guarantee* ([https://www.kaggle.com/c/shopee-product-matching/](https://www.kaggle.com/c/shopee-product-matching/)).
    Here, the *F*1 score is calculated for every predicted row, then averaged, whereas
    the Macro-F1 score is defined as the mean of class-wise/label-wise *F*1 scores.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then there is also **Quadratic Weighted Kappa**, which we will explore later
    on as a smart evaluation metric for ordinal prediction problems. In its simplest
    form, the **Cohen Kappa** score, it just measures the agreement between your predictions
    and the ground truth. The metric was actually created for measuring **inter-annotation
    agreement**, but it is really versatile and has found even better uses.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'What is inter-annotation agreement? Let’s imagine that you have a labeling
    task: classifying some photos based on whether they contain an image of a cat,
    a dog, or neither. If you ask a set of people to do the task for you, you may
    incur some erroneous labels because someone (called the *judge* in this kind of
    task) may misinterpret a dog as a cat or vice versa. The smart way to do this
    job correctly is to divide the work among multiple judges labeling the same photos,
    and then measure their level of agreement based on the Cohen Kappa score.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the Cohen Kappa is devised as a score expressing the level of agreement
    between two annotators on a labeling (classification) problem:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_035.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Using the confusion
    matrix nomenclature, this can be rewritten as:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_036.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: The interesting aspect of this formula is that the score takes into account
    the empirical probability that the agreement has happened just by chance, so the
    measure has a correction for all the most probable classifications. The metric
    ranges from 1, meaning complete agreement, to -1, meaning the judges completely
    oppose each other (total disagreement).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Values around 0 signify that agreement and disagreement among the judges is
    happening by mere chance. This helps you figure out if the model is really performing
    better than chance in most situations.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Andrey_Lukyanenko_-_Copy.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: Andrey Lukyanenko
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/artgor](https://www.kaggle.com/artgor)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Our second interview of the chapter is with Andrey Lukyanenko, a Notebooks and
    Discussions Grandmaster and Competitions Master. In his day job, he is a Machine
    Learning Engineer and TechLead at MTS Group. He had many interesting things to
    say about his Kaggle experiences!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques, solving
    approaches, what is your specialty on Kaggle?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '*I prefer competitions where solutions can be general enough to be transferable
    to other datasets/domains. I’m interested in trying various neural net architectures,
    state-of-the-art approaches, and post-processing tricks. I don’t favor those competitions
    that require reverse engineering or creating some “golden features,” as these
    approaches won’t be applicable in other datasets.*'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: While you were competing on Kaggle, you also became a Grandmaster in Notebooks
    (and ranked number one) and Discussions. Have you invested in these two objectives?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '*I have invested a lot of time and effort into writing Notebooks, but the Discussion
    Grandmaster rank happened kind of on its own.*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s start with the Notebook ranking.*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '*There was a special competition in 2018 called DonorsChoose.org Application
    Screening. DonorsChoose is a fund that empowers public school teachers from across
    the country to request much-needed materials and experiences for their students.
    It organized a competition, where the winning solutions were based not on the
    score on the leaderboard, but on the number of the upvotes on the Notebook. This
    looked interesting and I wrote a Notebook for the competition. Many participants
    advertised their analysis on social media and I did the same. As a result, I reached
    second place and won a Pixelbook (I’m still using it!).*'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '*I was very motivated by this success and continued writing Notebooks. At first,
    I simply wanted to share my analysis and get feedback, because I wanted to try
    to compare my analytics and visualization skills with other people to see what
    I could do and what people thought of it. People started liking my kernels and
    I wanted to improve my skills even further. Another motivation was a desire to
    improve my skill at making a quick MVP (minimum viable product). When a new competition
    starts, many people begin writing Notebooks, and if you want to be one of the
    first, you have to be able to do it fast without sacrificing quality. This is
    challenging, but fun and rewarding.*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '*I was able to get the Notebook Grandmaster rank in the February of 2019; after
    some time, I reached first place and held it for more than a year. Now I write
    Notebooks less frequently, but I still enjoy doing it.*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '*As for discussions, I think it kind of happened on its own. I answered the
    comments on my Notebooks, and shared and discussed ideas about competitions in
    which I took part, and my discussion ranking steadily increased.*'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '*It was the* Predicting Molecular Properties *competition. I have written a
    blog post about it in more detail here (*[https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1](https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1)*).
    It was a domain-specific competition aimed at predicting interactions between
    atoms in molecules. Nuclear Magnetic Resonance (NMR) is a technology that uses
    principles similar to MRI to understand the structure and dynamics of proteins
    and molecules. Researchers around the world conduct NMR experiments to further
    understand the structure and dynamics of molecules, across areas like environmental
    science, pharmaceutical science, and materials science. In this competition, we
    tried to predict the magnetic interaction between two atoms in a molecule (the
    scalar coupling constant). State-of-the-art methods from quantum mechanics can
    calculate these coupling constants given only a 3D molecular structure as input.
    But these calculations are very resource-intensive, so can’t be always used. If
    machine learning approaches could predict these values, it would really help medicinal
    chemists to gain structural insights faster and more cheaply.*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '*I usually write EDA kernels for new Kaggle competitions, and this one was
    no exception. A common approach for tabular data in Kaggle competitions is extensive
    feature engineering and using gradient boosting models. I used LGBM too in my
    early attempts, but knew that there should be better ways to work with graphs.
    I realized that domain expertise would provide a serious advantage, so I hunted
    for every piece of such information. Of course, I noticed that there were several
    active experts, who wrote on the forum and created kernels, so I read everything
    from them. And one day I received an e-mail from an expert in this domain who
    thought that our skills could complement each other. Usually, I prefer to work
    on competitions by myself for some time, but in this case, combining forces seemed
    to be a good idea to me. And this decision turned out to be a great one! With
    time we were able to gather an amazing team.*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '*After some time, we noticed a potential for neural nets in the competition:
    a well-known Kaggler, Heng, posted an example of an MPNN (Message Passing Neural
    Network) model. After some time, I was even able to run it, but the results were
    worse compared to our models. Nevertheless, our team knew that we would need to
    work with these Neural Nets if we wanted to aim high. It was amazing to see how
    Christof was able to build new neural nets extremely fast. Soon, we focused only
    on developing those models.*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '*After that, my role switched to a support one. I did a lot of experiments
    with our neural nets: trying various hyperparameters, different architectures,
    various little tweaks to training schedules, and so on. Sometimes I did EDA on
    our predictions to find our interesting or wrong cases, and later we used this
    information to improve our models even further.*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '*We got the 8*^(th) *place and I learned a lot during this competition.*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle definitely helped me a lot, especially with my skills and my personal
    brand. Writing and publishing Kaggle Notebooks taught me not only EDA and ML skills,
    but it forced me to become adaptable, to be able to understand new topics and
    tasks quickly, to iterate more efficiently between approaches. At the same time,
    it provided a measure of visibility for me, because people appreciated my work.*'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '*My first portfolio (*[https://erlemar.github.io/](https://erlemar.github.io/)*)
    had a lot of different Notebooks, and half of them were based on old Kaggle competitions.
    It was definitely helpful in getting my first jobs. My Kaggle achievements also
    helped me attract recruiters from good companies, sometimes even to skip steps
    of the interview process, and even led me to several consulting gigs.*'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '*I think we need to separate inexperienced Kagglers into two groups: those
    who are inexperienced in data science in general and those who are inexperienced
    on Kaggle.*'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '*Those who are inexperienced in general make a number of different mistakes
    (and it is okay, everyone started somewhere):*'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '*One of the most serious problems: lack of critical thinking and not knowing
    how to do their own research;*'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not knowing when and what tools/approaches to use;*'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Blindly taking public Notebooks and using them without understanding how they
    work;*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fixating on a certain idea and spending too much time pursuing it, even when
    it doesn’t work;*'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Despairing and losing motivation when their experiments fail.*'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*As for those people who have experience in data science but don’t have experience
    with Kaggle, I’d say that the most serious thing they overlook is that they underestimate
    Kaggle’s difficulty. They don’t expect Kaggle to be very competitive, that you
    need to try many different things to succeed, that there are a lot of tricks that
    work only in competitions, that there are people who professionally participate
    in competitions.*'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, people often overestimate domain expertise. I admit that there were
    a number of competitions when the teams with domain experts in them won gold medals
    and prizes, but in most cases experienced Kagglers triumph.*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, I have seen the following situation many times: some person proclaims
    that winning Kaggle is easy, and that he (or his group of people) will get a gold
    medal or many gold medals in the recent future. In most cases, they silently fail.*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '*Not* *enough looking in the data. Sometimes I wasn’t able to generate better
    features or apply better postprocessing due to this. And reserve engineering and
    “golden features” is a whole additional topic.*'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spending too much time on a single idea because I hoped it would work. This
    is called sunk-cost fallacy.*'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not enough experiments. The effort pays off – if you don’t spend enough time
    and resources on the competition, you won’t get a high place on a leaderboard.*'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entering “wrong” competitions. There were competitions with leaks, reverse
    engineering, etc. There were competitions with an unreasonable split between public
    and private test data and a shake-up ensured. There were competitions that weren’t
    interesting enough for me and I shouldn’t have started participating in them.*'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Teaming up with the wrong people. There were cases when my teammates weren’t
    as active as I expected a**nd it led to a worse team score.*'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '*I think it is important to remember your goal, know what are you ready to
    invest into this competition, and think about the possible outcomes. There are
    many possible goals that people have while entering a competition:*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '*Win**ning money or getting a medal;*'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting new skills or improving existing ones;*'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with a new task/domain;*'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Networking;*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PR;*'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*etc;*'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Of course, it is possible to have multiple motivations.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '*As for what are you ready to invest, it is usually about the amount of time
    and effort you are ready to spend as well as the hardware that you have.*'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '*When I speak about the outcomes, I mean what will happen when the competition
    ends. It is possible that you will invest a lot in this competition and win, but
    you could also lose. Are you ready for this reality? Is winning a particular competition
    critical to you? Maybe you need to be prepared to invest more effort; on the other
    hand, maybe you have long-term goals and one failed competition won’t hurt much.*'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for object detection problems
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent years, deep learning competitions have become more and more common
    on Kaggle. Most of these competitions, focused on image recognition or on natural
    language processing tasks, have not required the use of evaluation metrics much
    different from the ones we have explored up to now. However, a couple of specific
    problems have required some special metric to be evaluated correctly: those relating
    to **object detection** and **segmentation**.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_04.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Computer vision tasks. (Source: https://cocodataset.org/#explore?id=38282,
    https://cocodataset.org/#explore?id=68717)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: In **object detection**, you don’t have to classify an image, but instead find
    relevant portions of a picture and label them accordingly. For instance, in *Figure
    5.4*, an object detection classifier has been entrusted to locate within a photo
    the portions of the picture where either dogs or cats are present and classify
    each of them with a proper label. The example on the left shows the localization
    of a cat using a rectangular box (called a **bounding box**). The example on the
    right presents how multiple cats and dogs are detected in the picture by bounding
    boxes and then correctly classified (the blue bounding boxes are for dogs, the
    red ones for cats).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to describe the spatial location of an object, in object detection
    we use **bounding boxes**, which define a rectangular area in which the object
    lies. A bounding box is usually specified using two (*x*, *y*) coordinates: the
    upper-left and lower-right corners. In terms of a machine learning algorithm,
    finding the coordinates of bounding boxes corresponds to applying a regression
    problem to multiple targets. However, you probably won’t frame the problem from
    scratch but rely on pre-built and often pre-trained models such as Mask R-CNN
    ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)), RetinaNet
    ([https://arxiv.org/abs/2106.05624v1](https://arxiv.org/abs/2106.05624v1)), FPN
    ([https://arxiv.org/abs/1612.03144v2](https://arxiv.org/abs/1612.03144v2)), YOLO
    ([https://arxiv.org/abs/1506.02640v1](https://arxiv.org/abs/1506.02640v1)), Faster
    R-CNN ([https://arxiv.org/abs/1506.01497v1](https://arxiv.org/abs/1506.01497v1)),
    or SDD ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'In **segmentation**, you instead have a classification at the *pixel* level,
    so if you have a 320x200 image, you actually have to make 64,000 pixel classifications.
    Depending on the task, you can have a **semantic segmentation** where you have
    to classify every pixel in a photo, or an **instance segmentation** where you
    only have to classify the pixels representing objects of a certain type of interest
    (for instance, a cat as in our example in *Figure 5.5* below):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_05.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Semantic segmentation and instance segmentation on the same image.
    (Source: https://cocodataset.org/#explore?id=338091)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an overview of the specific metrics for these tasks, metrics
    that can work well for both problems, since, in both cases, you are predicting
    entire areas (rectangular ones in object detection, polygonal ones in segmentation)
    of a picture and you have to compare your predictions against a ground truth,
    which is, again, expressed as areas. On the side of segmentation, the easiest
    metric is the **pixel accuracy**, which, as the name suggests, is the accuracy
    on the pixel classification.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: It is not a great metric because, as happens with accuracy on binary and multi-class
    problems, your score may look great if the relevant pixels do not take up very
    much of the image (you just predict the majority claim, thus you don’t segment).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, there are two metrics that are used much more, especially in competitions:
    the **intersection over union** and the **dice coefficient**.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over union (IoU)
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **intersection over union** (**IoU**) is also known as the **Jaccard index**.
    When used in segmentation problems, using IoU implies that you have two images
    to compare: one is your prediction and the other is the mask revealing the ground
    truth, which is usually a binary matrix where the value 1 stands for the ground
    truth and 0 otherwise. In the case of multiple objects, you have multiple masks,
    each one labeled with the class of the object.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'When used in object detection problems, you have the boundaries of two rectangular
    areas (those of the prediction and the ground truth), expressed by the coordinates
    of their vertices. For each classified class, you compute the area of overlap
    between your prediction and the ground truth mask, and then you divide this by
    the area of the union between your prediction and the ground truth, a sum that
    takes into account any overlap. In this way, you are proportionally penalized
    both if you predict a larger area than what it should be (the denominator will
    be larger) or a smaller one (the numerator will be smaller):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_06.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Visual representation of the IoU calculation'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.6* you can see a visual representation of the areas involved in
    the computation. By imagining the squares overlapping more, you can figure out
    how the metric efficiently penalizes your solution when your prediction, even
    if covering the ground truth, exceeds it (the area of union becomes larger).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of competitions where IoU has been used:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '*TGS Salt Identification Challenge* ([https://www.kaggle.com/c/tgs-salt-identification-challenge/](https://www.kaggle.com/c/tgs-salt-identification-challenge/))
    with Intersection Over Union Object Segmentation'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iMaterialist (Fashion) 2019 at FGVC6* ([https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6))
    with Intersection Over Union Object Segmentation With Classification'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Airbus Ship Detection Challenge* ([https://www.kaggle.com/c/airbus-ship-detection](https://www.kaggle.com/c/airbus-ship-detection))
    with Intersection Over Union Object Segmentation Beta'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dice
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The other useful metric is the **Dice coefficient**, which is the area of overlap
    between the prediction and ground truth doubled and then divided by the sum of
    the prediction and ground truth areas:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_07.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Visual representation of the Dice calculation'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: In this case, with respect to the Jaccard index, you do not take into account
    the overlap of the prediction with the ground truth in the denominator. Here,
    the expectation is that, as you maximize the area of overlap, you predict the
    correct area size. Again, you are penalized if you predict areas larger than you
    should be predicting. In fact, the two metrics are positively correlated and they
    produce almost the same results for a single classification problem.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: The differences actually arise when you are working with multiple classes. In
    fact, both with IoU and the Dice coefficient, when you have multiple classes you
    average the result of all of them. However, in doing so, the IoU metric tends
    to penalize the overall average more if a single class prediction is wrong, whereas
    the Dice coefficient is more lenient and tends to represent the average performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of Kaggle competitions using the Dice coefficient (it is often encountered
    in competitions with medical purposes, but not necessarily only there, because
    it can also be used for clouds and cars):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '*HuBMAP - Hacking the Kidney*: [https://www.kaggle.com/c/hubmap-kidney-segmentation](https://www.kaggle.com/c/hubmap-kidney-segmentation)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ultrasound Nerve Segmentation*: [https://www.kaggle.com/c/ultrasound-nerve-segmentation](https://www.kaggle.com/c/ultrasound-nerve-segmentation)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding Clouds from Satellite Images*: [https://www.kaggle.com/c/understanding_cloud_organization](https://www.kaggle.com/c/understanding_cloud_organization)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Carvana Image Masking Challenge*: [https://www.kaggle.com/c/carvana-image-masking-challenge](https://www.kaggle.com/c/carvana-image-masking-challenge)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoU and Dice constitute the basis for all the more complex metrics in segmentation
    and object detection. By choosing an appropriate threshold level for IoU or Dice
    (usually 0.5), you can decide whether or not to confirm a detection, therefore
    a classification. At this point, you can use previously discussed metrics for
    classification, such as precision, recall, and *F*1, such as is done in popular
    object detection and segmentation challenges such as Pascal VOC ([http://host.robots.ox.ac.uk/pascal/VOC/voc2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012))
    or COCO ([https://cocodataset.org](https://cocodataset.org)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for multi-label classification and recommendation problems
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recommender systems are one of the most popular applications of data analysis
    and machine learning, and there are quite a few competitions on Kaggle that have
    used the recommendation approach. For instance, the *Quick, Draw! Doodle Recognition
    Challenge* was a prediction evaluated as a recommender system. Some other competitions
    on Kaggle, however, truly strived to build effective recommender systems (such
    as *Expedia Hotel Recommendations*: [https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations))
    and RecSYS, the conference on recommender systems ([https://recsys.acm.org/](https://recsys.acm.org/)),
    even hosted one of its yearly contests on Kaggle (*RecSYS 2013*: [https://www.kaggle.com/c/yelp-recsys-2013](https://www.kaggle.com/c/yelp-recsys-2013)).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Average Precision at K** (**MAP@{K}**) is typically the metric of choice
    for evaluating the performance of recommender systems, and it is the most common
    metric you will encounter on Kaggle in all the competitions that try to build
    or approach a problem as a recommender system.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: There are also some other metrics, such as the **precision at k**, or **P@K**,
    and the **average precision at k**, or **AP@K**, which are loss functions, in
    other words, computed at the level of each single prediction. Understanding how
    they work can help you better understand the MAP@K and how it can perform both
    in recommendations and in multi-label classification.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, analogous to recommender systems, multi-label classifications imply
    that your model outputs a series of class predictions. Such results could be evaluated
    using some average of some binary classification metrics (such as in *Greek Media
    Monitoring Multilabel Classification (WISE 2014)*, which used the mean *F*1 score:
    [https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)) as well
    as metrics that are more typical of recommender systems, such as MAP@K. In the
    end, you can deal with both recommendations and multi-label predictions as *ranking
    tasks*, which translates into a set of ranked suggestions in a recommender system
    and into a set of labels (without a precise order) in multi-label classification.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: MAP@{K}
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MAP@K is a complex metric and it derives from many computations. In order to
    understand the MAP@K metric fully, let’s start with its simplest component, the
    **precision at** **k** (**P@K**). In this case, since the prediction for an example
    is a ranked sequence of predictions (from the most probable to the least), the
    function takes into account only the top *k* predictions, then it computes how
    many matches it got with respect to the ground truth and divides that number by
    *k*. In a few words, it is quite similar to an accuracy measure averaged over
    *k* predictions.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: A bit more complex in terms of computation, but conceptually simple, the **average
    precision at** **k** (**AP@K**) is the average of P@K computed over all the values
    ranging from *1* to *k*. In this way, the metric evaluates how well the prediction
    works overall, using the top prediction, then the top two predictions, and so
    on until the top *k* predictions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, **MAP@K** is the mean of the AP@K for the entire predicted sample,
    and it is a metric because it comprises all the predictions in its evaluation.
    Here is the MAP@5 formulation you can find in the *Expedia Hotel Recommendations*
    competition ([https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations)):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_037.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: In the formula, ![](img/B17574_05_038.png) is the number of user recommendations,
    *P(k)* is the precision at cutoff *k*, and *n* is the number of predicted hotel
    clusters (you could predict up to 5 hotels for each recommendation).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: It is clearly a bit more daunting than our explanation, but the formula just
    expresses that the MAP@K is the mean of all the AP@K evaluations over all the
    predictions.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Having completed this overview of specific metrics for different regression
    and classification metrics, let’s discuss how to deal with evaluation metrics
    in a Kaggle competition.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing evaluation metrics
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summing up what we have discussed so far, an objective function is a function
    inside your learning algorithm that measures how well the algorithm’s internal
    model is fitting the provided data. The objective function also provides feedback
    to the algorithm in order for it to improve its fit across successive iterations.
    Clearly, since the entire algorithm’s efforts are recruited to perform well based
    on the objective function, if the Kaggle evaluation metric perfectly matches the
    objective function of your algorithm, you will get the best results.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this is not frequently the case. Often, the evaluation metric
    provided can only be approximated by existing objective functions. Getting a good
    approximation, or striving to get your predictions performing better with respect
    to the evaluation criteria, is the secret to performing well in Kaggle competitions.
    When your objective function does not match your evaluation metric, you have a
    few alternatives:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Modify your learning algorithm and have it incorporate an objective function
    that matches your evaluation metric, though this is not possible for all algorithms
    (for instance, algorithms such as LightGBM and XGBoost allow you to set custom
    objective functions, but most Scikit-learn models don’t allow this).
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune your model’s hyperparameters, choosing the ones that make the result shine
    the most when using the evaluation metric.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Post-process your results so they match the evaluation criteria more closely.
    For instance, you could code an optimizer that performs transformations on your
    predictions (probability calibration algorithms are an example, and we will discuss
    them at the end of the chapter).
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having the competition metric incorporated into your machine learning algorithm
    is really the most effective method to achieve better predictions, though only
    a few algorithms can be hacked into using the competition metric as your objective
    function. The second approach is therefore the more common one, and many competitions
    end up in a struggle to get the best hyperparameters for your models to perform
    on the evaluation metric.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: If you already have your evaluation function coded, then doing the right cross-validation
    or choosing the appropriate test set plays the lion share. If you don’t have the
    coded function at hand, you have to first code it in a suitable way, following
    the formulas provided by Kaggle.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Invariably, doing the following will make the difference:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Looking for all the relevant information about the evaluation metric and its
    coded function on a search engine
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Browsing through the most common packages (such as Scikit-learn: [https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)
    or TensorFlow: [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses))'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Browsing GitHub projects (for instance, *Ben Hammer’s* Metrics project: [https://github.com/benhamner/Metrics](https://github.com/benhamner/Metrics))'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking or looking around in the forums and available Kaggle Notebooks (both
    for the current competition and for similar competitions)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, as we mentioned before, querying the Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    and looking in the **Competitions** table will help you find out which other Kaggle
    competitions used that same evaluation metric, and immediately provides you with
    useful code and ideas to try out
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss in greater detail the alternatives you have when your evaluation
    metric doesn’t match your algorithm’s objective function. We’ll start by exploring
    custom metrics.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Custom metrics and custom objective functions
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a first option when your objective function does not match your evaluation
    metric, we learned above that you can solve this by creating your own custom objective
    function, but that only a few algorithms can easily be modified to incorporate
    a specific objective function.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that the few algorithms that allow this are among the most
    effective ones in Kaggle competitions and data science projects. Of course, creating
    your own custom objective function may sound a little bit tricky, but it is an
    incredibly rewarding approach to increasing your score in a competition. For instance,
    there are options to do this when using gradient boosting algorithms such as XGBoost,
    CatBoost, and LightGBM, as well as with all deep learning models based on TensorFlow
    or PyTorch.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find great tutorials for custom metrics and objective functions in
    TensorFlow and PyTorch here:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279](https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://petamind.com/advanced-keras-custom-loss-functions/](https://petamind.com/advanced-keras-custom-loss-functions/)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/](https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These will provide you with the basic function templates and some useful suggestions
    about how to code a custom objective or evaluation function.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want just to get straight to the custom objective function you need,
    you can try this Notebook by RNA ([https://www.kaggle.com/bigironsphere](https://www.kaggle.com/bigironsphere)):
    [https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook).
    It contains a large range of custom loss functions for both TensorFlow and PyTorch
    that have appeared in different competitions.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: If you need to create a custom loss in LightGBM, XGBoost, or CatBoost, as indicated
    in their respective documentation, you have to code a function that takes as inputs
    the prediction and the ground truth, and that returns as outputs the gradient
    and the hessian.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'You can consult this post on Stack Overflow for a better understanding of what
    a gradient and a hessian are: [https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based](https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'From a code implementation perspective, all you have to do is to create a function,
    using closures if you need to pass more parameters beyond just the vector of predicted
    labels and true labels. Here is a simple example of a **focal loss** (a loss that
    aims to heavily weight the minority class in the loss computations as described
    in Lin, T-Y. et al. *Focal loss for dense object detection*: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002))
    function that you can use as a model for your own custom functions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another interesting aspect of the example is that it really makes it easy to
    compute the gradient and the hessian of the cost function by means of the derivative
    function from SciPy. If your cost function is differentiable, you don’t have to
    worry about doing any calculations by hand. However, creating a custom objective
    function requires some mathematical knowledge and quite a lot of effort to make
    sure it works properly for your purposes. You can read about the difficulties
    that *Max Halford* experienced while implementing a focal loss for the LightGBM
    algorithm, and how he overcame them, here: [https://maxhalford.github.io/blog/lightgbm-focal-loss/](https://maxhalford.github.io/blog/lightgbm-focal-loss/).
    Despite the difficulty, being able to conjure up a custom loss can really determine
    your success in a Kaggle competition where you have to extract the maximum possible
    result from your model.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: If building your own objective function isn’t working out, you can simply lower
    your ambitions, give up building your function as an objective function used by
    the optimizer, and instead code it as a custom *evaluation metric*. Though your
    model won’t be directly optimized to perform against this function, you can still
    improve its predictive performance with hyperparameter optimization based on it.
    This is the second option we talked about in the previous section.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Just remember, if you are writing a metric from scratch, sometimes you may
    need to abide by certain code conventions for your function to work properly.
    For instance, if you use Scikit-learn, you have to convert your functions using
    the `make_scorer` function. The `make_scorer` function is actually a wrapper that
    makes your evaluation function suitable for working with the Scikit-learn API.
    It will wrap your function while considering some meta-information, such as whether
    to use probability estimates or predictions, whether you need to specify a threshold
    for prediction, and, last but not least, the directionality of the optimization,
    that is, whether you want to maximize or minimize the score it returns:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the above example, you prepare a scorer based on the average precision metric,
    specifying that it should use a weighted computation when dealing with multi-class
    classification problems.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: If you are optimizing for your evaluation metric, you can apply grid search,
    random search, or some more sophisticated optimization such as Bayesian optimization
    and find the set of parameters that makes your algorithm perform optimally for
    your evaluation metric, even if it works with a different cost function. We will
    explore how to best arrange parameter optimization and obtain the best results
    on Kaggle competitions after having discussed model validation, specifically in
    the chapter dealing with tabular data problems.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing your predictions
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-processing tuning implies that your predictions are transformed, by means
    of a function, into something else in order to present a better evaluation. After
    building your custom loss or optimizing for your evaluation metric, you can also
    improve your results by leveraging the characteristics of your evaluation metric
    using a specific function applied to your predictions. Let’s take the Quadratic
    Weighted Kappa, for instance. We mentioned previously that this metric is useful
    when you have to deal with the prediction of an ordinal value. To recap, the original
    Kappa coefficient is a chance-adjusted index of agreement between the algorithm
    and the ground truth. It is a kind of accuracy measurement corrected by the probability
    that the match between the prediction and the ground truth is due to a fortunate
    chance.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the original version of the Kappa coefficient, as seen before:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_039.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Here, you need just
    two matrices, the one with the observed scores and the one with the expected scores
    based on chance agreement. When the Kappa coefficient is weighted, you also consider
    a weight matrix and the formula turns into this:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_05_040.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: The matrix *p*[p] contains the penalizations to weight errors differently, which
    is very useful for ordinal predictions since this matrix can penalize much more
    when the predictions deviate further from the ground truths. Using the quadratic
    form, that is, squaring the resulting *k*, makes the penalization even more severe.
    However, optimizing for such a metric is really not easy, since it is very difficult
    to implement it as a cost function. Post-processing can help you.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: An example can be found in the *PetFinder.my Adoption Prediction* competition
    ([https://www.kaggle.com/c/petfinder-adoption-prediction](https://www.kaggle.com/c/petfinder-adoption-prediction)).
    In this competition, given that the results could have 5 possible ratings (0,
    1, 2, 3, or 4), you could deal with them either using a classification or a regression.
    If you used a regression, a post-processing transformation of the regression output
    could improve the model’s performance against the Quadratic Weighted Kappa metric,
    outperforming the results you could get from a classification directly outputting
    discrete predictions.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the PetFinder competition, the post-processing consisted of
    an optimization process that started by transforming the regression results into
    integers, first using the boundaries [0.5, 1.5, 2.5, 3.5] as thresholds and, by
    an iterative fine-tuning, finding a better set of boundaries that maximized the
    performance. The fine-tuning of the boundaries required the computations of an
    optimizer such as SciPy’s `optimize.minimize`, which is based on the Nelder-Mead
    algorithm. The boundaries found by the optimizer were validated by a cross-validation
    scheme. You can read more details about this post-processing directly from the
    post made by *Abhishek Thakur* during the competition: [https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from the PetFinder competition, many other competitions have demonstrated
    that smart post-processing can lead to improved results and rankings. We’ll point
    out a few examples here:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw](https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage](https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage)'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization](https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization)'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, post-processing is often very dependent on the metric you are
    using (understanding the metric is imperative for devising any good post-processing)
    and often also data-specific, for instance, in the case of time series data and
    leakages. Hence, it is very difficult to generalize any procedure for figuring
    out the right post-processing for any competition. Nevertheless, always be aware
    of this possibility and be on the lookout in a competition for any hint that post-processing
    results is favorable. You can always get hints about post-processing from previous
    competitions that have been similar, and by forum discussion – eventually, someone
    will raise the topic.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Predicted probability and its adjustment
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To complete the above discussion on metrics optimization (post-processing of
    predictions), we will discuss situations where it is paramount to predict correct
    probabilities, but you are not sure if the algorithm you are using is doing a
    good job. As we detailed previously, classification probabilities concern both
    binary and multiclass classification problems, and they are commonly evaluated
    using the logarithmic loss (aka log loss or logistic loss or cross-entropy loss)
    in its binary or multi-class version (for more details, see the previous sections
    on *Metrics for classification (label prediction and probability)* and *Metrics
    for multi-class classification*).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'However, evaluating or optimizing for the log loss may not prove enough. The
    main problems to be on the lookout for when striving to achieve correct probabilistic
    predictions with your model are:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Models that do not return a truly probabilistic estimate
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unbalanced distribution of classes in your problem
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different class distribution between your training data and your test data (on
    both public and private leaderboards)
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point alone provides reason to check and verify the quality of classification
    predictions in terms of modeled uncertainty. In fact, even if many algorithms
    are provided in the Scikit-learn package together with a `predict_proba` method,
    this is a very weak assurance that they will return a true probability.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take, for instance, decision trees, which are the basis of many effective
    methods to model tabular data. The probability outputted by a classification decision
    tree ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))
    is based on terminal leaves; that is, it depends on the distribution of classes
    on the leaf that contains the case to be predicted. If the tree is fully grown,
    it is highly likely that the case is in a small leaf with very few other cases,
    so the predicted probability will be very high. If you change parameters such
    as `max_depth`, `max_leaf_nodes`, or `min_samples_leaf`, the resulting probability
    will drastically change from higher values to lower ones depending on the growth
    of the tree.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are the most common base model for ensembles such as bagging
    models and random forests, as well as boosted models such as gradient boosting
    (with its high-performing implementations XGBoost, LightGBM, and CatBoost). But,
    for the same reasons – probability estimates that are not truly based on solid
    probabilistic estimations – the problem affects many other commonly used models,
    such as support-vector machines and *k*-nearest neighbors. Such aspects were mostly
    unknown to Kagglers until the *Otto Group Product Classification Challenge* ([https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/](https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/)),
    when it was raised by *Christophe Bourguignat* and others during the competition
    (see [https://www.kaggle.com/cbourguignat/why-calibration-works](https://www.kaggle.com/cbourguignat/why-calibration-works)),
    and easily solved at the time using the calibration functions that had recently
    been added to Scikit-learn.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the model you will be using, the presence of imbalance between classes
    in your problem may also result in models that are not at all reliable. Hence,
    a good approach in the case of unbalanced classification problems is to rebalance
    the classes using undersampling or oversampling strategies, or different custom
    weights for each class to be applied when the loss is computed by the algorithm.
    All these strategies may render your model more performant; however, they will
    surely distort the probability estimates and you may have to adjust them in order
    to obtain an even better model score on the leaderboard.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a third point of concern is related to how the test set is distributed.
    This kind of information is usually concealed, but there are often ways to estimate
    it and figure it out (for instance, by trial and error based on the public leaderboard
    results, as we mentioned in *Chapter 1*, *Introducing Kaggle and Other Data Science
    Competitions*).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: For instance, this happened in the *iMaterialist Furniture Challenge* ([https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/))
    and the more popular *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
    Both competitions gave rise to various discussions on how to post-process in order
    to adjust probabilities to test expectations (see [https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/](https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/)
    and [https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb](https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb)
    for more details on the method used). From a general point of view, assuming that
    you do not have an idea of the test distribution of classes to be predicted, it
    is still very beneficial to correctly predict probability based on the priors
    you get from the training data (and until you get evidence to the contrary, that
    is the probability distribution that your model should mimic). In fact, it will
    be much easier to correct your predicted probabilities if your predicted probability
    distribution matches those in the training set.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution, when your predicted probabilities are misaligned with the training
    distribution of the target, is to use the **calibration function** provided by
    Scikit-learn, `CalibratedClassifierCV`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The purpose of the calibration function is to apply a post-processing function
    to your predicted probabilities in order to make them adhere more closely to the
    empirical probabilities seen in the ground truth. Provided that your model is
    a Scikit-learn model or behaves similarly to one, the function will act as a wrapper
    for your model and directly pipe its predictions into a post-processing function.
    You have the choice between using two methods for post-processing. The first is
    the **sigmoid** method (also called Plat’s scaling), which is nothing more than
    a logistic regression. The second is the **isotonic regression**, which is a non-parametric
    regression; beware that it tends to overfit if there are few examples.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: You also have to choose how to fit this calibrator. Remember that it is a model
    that is applied to the results of your model, so you have to avoid overfitting
    by systematically reworking predictions. You could use a **cross-validation**
    (more on this in the following chapter on *Designing Good Validation*) and then
    produce a number of models that, once averaged, will provide your predictions
    (`ensemble=True`). Otherwise, and this is our usual choice, resort to an **out-of-fold
    prediction** (more on this in the following chapters) and calibrate on that using
    all the data available (`ensemble=False`).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Even if `CalibratedClassifierCV` can handle most situations, you can also figure
    out some empirical way to fix probability estimates for the best performance at
    test time. You can use any transformation function, from a handmade one to a sophisticated
    one derived by genetic algorithms, for instance. Your only limit is simply that
    you should cross-validate it and possibly have a good final result from the public
    leaderboard (but not necessarily, because you should trust your local cross-validation
    score more, as we are going to discuss in the next chapter). A good example of
    such a strategy is provided by Silogram ([https://www.kaggle.com/psilogram](https://www.kaggle.com/psilogram)),
    who, in the *Microsoft Malware Classification Challenge*, found out a way to tune
    the unreliable probabilistic outputs of random forests into probabilistic ones
    simply by raising the output to a power determined by grid search (see [https://www.kaggle.com/c/malware-classification/discussion/13509](https://www.kaggle.com/c/malware-classification/discussion/13509)).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Sudalai_Rajkumar.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: Sudalai Rajkumar
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/sudalairajkumar](https://www.kaggle.com/sudalairajkumar)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'In our final interview of the chapter, we speak to Sudalai Rajkumar, SRK, a
    Grandmaster in Competitions, Datasets, and Notebooks, and a Discussion Master.
    He is ranked #1 in the Analytics Vidhya data science platform, and works as an
    AI/ML advisor for start-ups.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '*My favorite kinds of competition are ones that involve a good amount of feature
    engineering. I think that is my strength as well. I am generally interested in
    data exploration to get a deep understanding of the data (which you can infer
    from my series of simple exploration Notebooks (*[https://www.kaggle.com/sudalairajkumar/code](https://www.kaggle.com/sudalairajkumar/code)*))
    and then creating features based on it.*'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '*The framework for a competition involves data exploration, finding the right
    validation method, feature engineering, model building, and ensembling/stacking.
    All these are involved in my day job as well. But in addition to this, there is
    a good amount of stakeholder discussion, data collection, data tagging, model
    deployment, model monitoring, and data storytelling that is involved in my daily
    job.*'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Santander Product Recommendation *is a memorable competition that we entered.
    Rohan & I did a lot of feature engineering and built multiple models. When we
    did final ensembling, we used different weights for different products and some
    of them did not add up to 1\. From the data exploration and understanding, we
    hand-picked these weights, which helped us. This made us realise the domain/data
    importance in solving problems and how data science is an art as much as science.*
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle played a very important role in my career. I was able to secure my
    last two jobs mainly because of Kaggle. Also, the success from Kaggle helps to
    connect with other stalwarts in the data science field easily and learn from them.
    It also helps a lot in my current role as AI / ML advisor for start-ups, as it
    gives credibility.*'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding the data in depth. Often this is overlooked, and people get
    into model-building right away. Exploring the data plays a very important role
    in the success of any Kaggle competition. This helps to create proper cross validation
    and to create better features and to extract more value from the data.*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '*It is a very big list, and I would say that they are learning opportunities.
    In every competition, out of 20-30 ideas that I try, only 1 may work. These mistakes/failures
    give much more learning than the actual success or things that worked. For example,
    I learnt about overfitting the very hard way by falling from top deciles to bottom
    deciles in one of my very first competitions. But that learning stayed with me
    forever thereafter.*'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '*Are there any particular tools or libraries that you would recommend using
    for data analysis/machine learning?*'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '*I primarily use XGBoost/LightGBM in the case of tabular data. I also use open
    source AutoML libraries and Driverless AI to get early benchmarks these days.
    I use Keras, Transformers, and PyTorch for deep learning models.*'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '*Consistency is the key. Each competition will have its own ups and downs.
    There will be multiple days without any progress, but we should not give up and
    keep trying. I think this is applicable for anything and not just Kaggle competitions.*'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '*I have also taken part on other platforms like the Analytics Vidhya DataHack
    platform, Driven Data, CrowdAnalytix etc. They are good too, but Kaggle is more
    widely adopted and global in nature, so the amount of competition on Kaggle is
    much higher compared to other platforms.*'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed evaluation metrics in Kaggle competitions.
    First, we explained how an evaluation metric can differ from an objective function.
    We also remarked on the differences between regression and classification problems.
    For each type of problem, we analyzed the most common metrics that you can find
    in a Kaggle competition.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: After that, we discussed the metrics that have never previously been seen in
    a competition and that you won’t likely see again. Finally, we explored and studied
    different common metrics, giving examples of where they have been used in previous
    Kaggle competitions. We then proposed a few strategies for optimizing an evaluation
    metric. In particular, we recommended trying to code your own custom cost functions
    and provided suggestions on possible useful post-processing steps.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: You should now have grasped the role of an evaluation metric in a Kaggle competition.
    You should also have a strategy to deal with every common or uncommon metric,
    by retracing past competitions and by gaining a full understanding of the way
    a metric works. In the next chapter, we are going to discuss how to use evaluation
    metrics and properly estimate the performance of your Kaggle solution by means
    of a validation strategy.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
