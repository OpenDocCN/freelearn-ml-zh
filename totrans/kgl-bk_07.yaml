- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Competition Tasks and Metrics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 竞赛任务和指标
- en: In a competition, you start by examining the target metric. Understanding how
    your model’s errors are evaluated is key for scoring highly in every competition.
    When your predictions are submitted to the Kaggle platform, they are compared
    to a ground truth based on the target metric.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛中，你首先检查目标指标。理解你的模型错误是如何评估的是在每一场竞赛中取得高分的关键。当你的预测提交到Kaggle平台时，它们将与基于目标指标的真实数据进行比较。
- en: For instance, in the *Titanic* competition ([https://www.kaggle.com/c/titanic/](https://www.kaggle.com/c/titanic/)),
    all your submissions are evaluated based on *accuracy*, the percentage of surviving
    passengers you correctly predict. The organizers decided upon this metric because
    the aim of the competition is to find a model that estimates the probability of
    survival of a passenger under similar circumstances. In another knowledge competition,
    *House Prices - Advanced Regression Techniques* ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)),
    your work will be evaluated based on an *average difference* between your prediction
    and the ground truth. This involves computing the logarithm, squaring, and taking
    the square root, because the model is expected to be able to quantify as correctly
    as possible the order of the price of a house on sale.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*泰坦尼克号*竞赛([https://www.kaggle.com/c/titanic/](https://www.kaggle.com/c/titanic/))中，你所有的提交都将根据*准确率*进行评估，即正确预测的幸存乘客百分比。组织者选择这个指标是因为竞赛的目的是找到一个模型，该模型可以估计在类似情况下乘客生存的概率。在另一个知识竞赛*房价
    - 高级回归技术*([https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques))中，你的工作将根据你的预测与真实数据之间的*平均差异*进行评估。这涉及到计算对数、平方和开方，因为模型预计能够尽可能准确地量化待售房屋价格的高低顺序。
- en: In real-world data science, target metrics are also key for the success of your
    project, though there are certainly differences between the real world and a Kaggle
    competition. We could easily summarize by saying that there are more complexities
    in the real world. In real-world projects, you will often have not just one but
    multiple metrics that your model will be evaluated against. Frequently, some of
    the evaluation metrics won’t even be related to how your predictions perform against
    the ground truth you are using for testing. For instance, the domain of knowledge
    you are working in, the scope of the project, the number of features considered
    by your model, the overall memory usage, any requirement for special hardware
    (such as a GPU, for instance), the latency of the prediction process, the complexity
    of the predicting model, and many other aspects may end up counting more than
    the mere predictive performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的数据科学中，目标指标也是你项目成功的关键，尽管现实世界与Kaggle竞赛之间肯定存在差异。我们可以简单地总结说，现实世界中有更多的复杂性。在现实世界的项目中，你将经常面对不止一个而是多个指标，你的模型将根据这些指标进行评估。通常，一些评估指标甚至与你的预测与用于测试的真实数据之间的表现无关。例如，你正在工作的知识领域，项目的范围，你的模型考虑的特征数量，整体内存使用量，对特殊硬件（例如GPU）的任何要求，预测过程的延迟，预测模型的复杂性，以及许多其他方面，最终可能比单纯的预测性能更重要。
- en: Real-world problems are indeed dominated by business and tech infrastructure
    concerns much more than you may imagine before being involved in any of them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的问题确实比你在参与其中之前想象的要更多地受到商业和技术基础设施的关注。
- en: Yet you cannot escape the fact that the basic principle at the core of both
    real-world projects and Kaggle competitions is the same. Your work will be evaluated
    according to some criteria, and understanding the details of such criteria, optimizing
    the fit of your model in a smart way, or selecting its parameters according to
    the criteria will bring you success. If you can learn more about how model evaluation
    occurs in Kaggle, your real-world data science job will also benefit from it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你无法回避这样一个事实：无论是现实世界项目还是Kaggle竞赛的核心基本原则都是相同的。你的工作将根据某些标准进行评估，理解这些标准的细节，以智能的方式优化你模型的拟合度，或者根据这些标准选择其参数，这些都将为你带来成功。如果你能更多地了解Kaggle中模型评估的流程，你的现实世界数据科学工作也将从中受益。
- en: 'In this chapter, we are going to detail how evaluation metrics for certain
    kinds of problems strongly influence the way you can operate when building your
    model solution in a data science competition. We also address the variety of metrics
    available in Kaggle competitions to give you an idea of what matters most and,
    in the margins, we discuss the different effects of metrics on predictive performance
    and how to correctly translate them into your projects. We will cover the following
    topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细说明某些类型问题的评估指标如何强烈影响你在数据科学竞赛中构建模型解决方案时的操作方式。我们还讨论了Kaggle竞赛中可用的各种指标，以给你一个关于什么最重要的概念，并在边缘讨论了指标对预测性能的不同影响以及如何正确地将它们转化为你的项目。我们将涵盖以下主题：
- en: Evaluation metrics and objective functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标和目标函数
- en: 'Basic types of tasks: regression, classification, and ordinal'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本任务类型：回归、分类和序型
- en: The Meta Kaggle dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta Kaggle 数据集
- en: Handling never-before-seen metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理未见过的指标
- en: Metrics for regression (standard and ordinal)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归指标（标准型和序型）
- en: Metrics for binary classification (label prediction and probability)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类指标（标签预测和概率）
- en: Metrics for multi-class classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类指标
- en: Metrics for object detection problems
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测问题的指标
- en: Metrics for multi-label classification and recommendation problems
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类和推荐问题的指标
- en: Optimizing evaluation metrics
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化评估指标
- en: Evaluation metrics and objective functions
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标和目标函数
- en: In a Kaggle competition, you can find the evaluation metric in the left menu
    on the **Overview** page of the competition. By selecting the **Evaluation** tab,
    you will get details about the evaluation metric. Sometimes you will find the
    metric formula, the code to reproduce it, and some discussion of the metric. On
    the same page, you will also get an explanation about the submission file format,
    providing you with the header of the file and a few example rows.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，你可以在竞赛的**概述**页面的左侧菜单中找到评估指标。通过选择**评估**选项卡，你可以获得关于评估指标的具体信息。有时你会找到指标公式、重现它的代码以及一些关于指标的讨论。在同一页面上，你还可以获得关于提交文件格式的说明，提供文件的标题和一些示例行。
- en: The association between the evaluation metric and the submission file is important,
    because you have to consider that the metric works essentially after you have
    trained your model and produced some predictions. Consequently, as a first step,
    you will have to think about the difference between an **evaluation metric** and
    an **objective function**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标与提交文件之间的关联很重要，因为你必须考虑到指标在训练模型并产生一些预测后基本上才会工作。因此，作为第一步，你必须考虑**评估指标**和**目标函数**之间的区别。
- en: 'Boiling everything down to the basics, an objective function serves your model
    during training because it is involved in the process of error minimization (or
    score maximization, depending on the problem). In contrast, an evaluation metric
    serves your model *after* it has been trained by providing a score. Therefore,
    it cannot influence how the model fits the data, but does influence it in an indirect
    way: by helping you to select the most well-performing hyperparameter settings
    within a model, and the best models among competing ones. Before proceeding with
    the rest of the chapter, which will show you how this can affect a Kaggle competition
    and why the analysis of the Kaggle evaluation metric should be your first act
    in a competition, let’s first discuss some terminology that you may encounter
    in the discussion forums.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将一切简化到基本原理，目标函数在训练过程中服务于你的模型，因为它涉及到错误最小化（或根据问题进行分数最大化）。相比之下，评估指标在模型**训练后**提供服务，通过提供一个分数。因此，它不能影响模型如何拟合数据，但以间接方式影响它：通过帮助你选择模型中最出色的超参数设置，以及竞争中的最佳模型。在继续本章的其余部分之前，这部分将向你展示这如何影响Kaggle竞赛以及为什么分析Kaggle评估指标应该是你在竞赛中的首要行动，让我们首先讨论一些你可能在讨论论坛中遇到的术语。
- en: 'You will often hear talk about objective functions, cost functions, and loss
    functions, sometimes interchangeably. They are not exactly the same thing, however,
    and we explain the distinction here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会听到关于目标函数、损失函数和损失函数的讨论，有时可以互换使用。然而，它们并不完全相同，我们在这里解释了区别：
- en: A **loss function** is a function that is defined on a single data point, and,
    considering the prediction of the model and the ground truth for the data point,
    computes a penalty.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**是在单个数据点上定义的函数，它考虑了模型的预测和该数据点的真实值，计算一个惩罚。'
- en: A **cost function** takes into account the whole dataset used for training (or
    a batch from it), computing a sum or average over the loss penalties of its data
    points. It can comprise further constraints, such as the L1 or L2 penalties, for
    instance. The cost function directly affects how the training happens.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代价函数**考虑了用于训练的整个数据集（或其一部分批次），通过计算其数据点的损失惩罚的总和或平均值。它可以包括进一步的约束，例如L1或L2惩罚，例如。代价函数直接影响到训练过程。'
- en: 'An **objective function** is the most general (and safe-to-use) term related
    to the scope of optimization during machine learning training: it comprises cost
    functions, but it is not limited to them. An objective function, in fact, can
    also take into account goals that are not related to the target: for instance,
    requiring sparse coefficients of the estimated model or a minimization of the
    coefficients’ values, such as in L1 and L2 regularizations. Moreover, whereas
    loss and cost functions imply an optimization based on minimization, an objective
    function is neutral and can imply either a maximization or a minimization activity
    performed by the learning algorithm.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数**是与机器学习训练中优化范围最通用（且安全使用）的术语：它包括代价函数，但不仅限于它们。实际上，目标函数还可以考虑与目标无关的目标：例如，要求估计模型的稀疏系数或系数值的极小化，如在L1和L2正则化中。此外，虽然损失和代价函数暗示基于最小化的优化过程，但目标函数是中立的，可以暗示学习算法执行的是最大化或最小化活动。'
- en: 'Likewise, when it comes to evaluation metrics, you’ll hear about scoring functions
    and error functions. Distinguishing between them is easy: a **scoring function**
    suggests better prediction results if scores from the function are higher, implying
    a maximization process.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当涉及到评估指标时，你会听到关于评分函数和误差函数的讨论。区分它们很容易：一个**评分函数**如果函数的分数更高，则暗示更好的预测结果，这意味着一个最大化过程。
- en: An **error function** instead suggests better predictions if smaller error quantities
    are reported by the function, implying a minimization process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**误差函数**则暗示如果函数报告较小的误差量，则会有更好的预测，这意味着一个最小化过程。'
- en: Basic types of tasks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本任务类型
- en: 'Not all objective functions are suitable for all problems. From a general point
    of view, you’ll find two kinds of problems in Kaggle competitions: **regression**
    tasks and **classification** tasks. Recently, there have also been **reinforcement
    learning** (**RL**) tasks, but RL doesn’t use metrics for evaluation; instead,
    it relies on a ranking derived from direct match-ups against other competitors
    whose solutions are assumed to be as well-performing as yours (performing better
    in this match-up than your peers will raise your ranking, performing worse will
    lower it). Since RL doesn’t use metrics, we will keep on referring to the regression-classification
    dichotomy, though **ordinal** tasks, where you predict ordered labels represented
    by integer numbers, may elude such categorization and can be dealt with successfully
    either using a regression or classification approach.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有目标函数都适用于所有问题。从一般的角度来看，你会在Kaggle竞赛中找到两种类型的问题：**回归**任务和**分类**任务。最近，也出现了**强化学习**（**RL**）任务，但RL不使用指标进行评估；相反，它依赖于与其他竞争对手的直接对抗产生的排名，假设这些竞争对手的解决方案与你的表现相当（在这个对抗中表现优于你的同伴将提高你的排名，表现较差将降低它）。由于RL不使用指标，我们仍将继续提到回归-分类的二分法，尽管**序数**任务，其中你预测由整数表示的有序标签，可能逃避这种分类，并且可以使用回归或分类方法成功处理。
- en: Regression
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: '**Regression** requires you to build a model that can predict a real number;
    often a positive number, but there have been examples of negative number prediction
    too. A classic example of a regression problem is *House Prices - Advanced Regression
    Techniques*, because you have to guess the value of a house. The evaluation of
    a regression task involves computing a distance between your predictions and the
    values of the ground truth. This difference can be evaluated in different ways,
    for instance by squaring it in order to punish larger errors, or by applying a
    log to it in order to penalize predictions of the wrong scale.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**需要你构建一个可以预测实数的模型；通常是一个正数，但也有预测负数的例子。一个经典的回归问题示例是 *房价 - 高级回归技术*，因为你需要猜测房屋的价值。回归任务的评估涉及计算你的预测与真实值之间的距离。这种差异可以通过不同的方式来评估，例如通过平方它来惩罚较大的错误，或者通过对其应用对数来惩罚错误尺度的预测。'
- en: Classification
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: When facing a **classification** task on Kaggle, there are more nuances to take
    into account. The classification, in fact, could be **binary**, **multi-class**,
    or **multi-label**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 Kaggle 面对分类任务时，需要考虑更多的细微差别。实际上，分类可以是 **二元**、**多类** 或 **多标签**。
- en: 'In **binary** problems, you have to guess if an example should be classified
    or not into a specific class (usually called the *positive* class and compared
    to the *negative* one). Here, the evaluation could involve the straightforward
    prediction of the class ownership itself, or an estimation of the probability
    of such ownership. A typical example is the *Titanic* competition, where you have
    to guess a binary outcome: survival or not-survival. In this case, the requirement
    of the competition is just the prediction, but in many cases, it is necessary
    to provide a probability because in certain fields, especially for medical applications,
    it is necessary to rank positive predictions across different options and situations
    in order to make the best decision.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **二元**问题中，你必须猜测一个示例是否应该被分类到特定的类别中（通常称为 *正* 类，并与 *负* 类进行比较）。在这里，评估可能包括对类归属的直接预测，或者对这种归属概率的估计。一个典型的例子是
    *泰坦尼克号* 比赛，你必须猜测一个二元结果：是否幸存。在这种情况下，比赛的要求仅仅是预测，但在许多情况下，提供概率是必要的，因为在某些领域，特别是医疗应用中，需要对不同选项和情况下的阳性预测进行排序，以便做出最佳决策。
- en: Though counting the exact number of correct matches in a binary classification
    may seem a valid approach, this won’t actually work well when there is an imbalance,
    that is, a different number of examples, between the positive and the negative
    class. Classification based on an imbalanced distribution of classes requires
    evaluation metrics that take the imbalance into account, if you want to correctly
    track improvements on your model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在二元分类中计算正确匹配的确切数量似乎是一个有效的方法，但当正负类之间存在不平衡时，这实际上并不会很好地工作，也就是说，正负类中的示例数量不同。基于类别不平衡分布的分类需要考虑不平衡的评估指标，如果你想正确跟踪模型上的改进。
- en: When you have more than two classes, you have a **multi-class** prediction problem.
    This also requires the use of suitable functions for evaluation, since it is necessary
    to keep track of the overall performance of the model, but also to ensure that
    the performance across the classes is comparable (for instance, your model could
    underperform with respect to certain classes). Here, each case can be in one class
    exclusively, and not in any others. A good example is *Leaf Classification* ([https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification)),
    where each image of a leaf specimen has to be associated with the correct plant
    species.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有超过两个类别时，你面临的是一个 **多类** 预测问题。这也需要使用合适的评估函数，因为需要跟踪模型的总体性能，同时也要确保跨类别的性能是可比的（例如，你的模型可能在某些类别上表现不佳）。在这里，每个案例只能属于一个类别，而不能属于其他任何类别。一个很好的例子是
    *叶分类* ([https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification))，其中每张叶片样本的图像都必须与正确的植物物种相关联。
- en: Finally, when your class predictions are not exclusive and you can predict multiple
    class ownership for each example, you have a **multi-label** problem that requires
    further evaluations in order to control whether your model is predicting the correct
    classes, as well as the correct number and mix of classes. For instance, in *Greek
    Media Monitoring Multilabel Classification (WISE 2014)* ([https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)),
    you had to associate each article with all the topics it deals with.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当你的类别预测不是互斥的，并且你可以为每个示例预测多个类别所有权时，你就遇到了一个**多标签**问题，这需要进一步的评估来控制你的模型是否正在预测正确的类别，以及正确的类别数量和组合。例如，在*希腊媒体监测多标签分类（WISE
    2014）*（[https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)）中，你必须将每篇文章与它所涉及的所有主题关联起来。
- en: Ordinal
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有序
- en: In a problem involving a prediction on an ordinal scale, you have to guess integer
    numeric labels, which are naturally ordered. As an example, the magnitude of an
    earthquake is on an ordinal scale. In addition, data from marketing research questionnaires
    is often recorded on ordinal scales (for instance, consumers’ preferences or opinion
    agreement). Since an ordinal scale is made of ordered values, ordinal tasks can
    be considered somewhat halfway between regression and classification, and you
    can solve them in both ways.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个涉及对有序尺度预测的问题中，你必须猜测有序的整数数值标签，这些标签自然是按顺序排列的。例如，地震的震级就属于有序尺度。此外，市场调研问卷中的数据通常也记录在有序尺度上（例如，消费者的偏好或意见一致性）。由于有序尺度由有序值组成，因此有序任务可以被视为介于回归和分类之间的一种任务，你可以用这两种方式来解决它们。
- en: The most common way is to treat your ordinal task as a **multi-class** problem.
    In this case, you will get a prediction of an integer value (the class label)
    but the prediction will not take into account that the classes have a certain
    order. You can get a feeling that there is something wrong with approaching the
    problem as a multi-class problem if you look at the prediction probability for
    the classes. Often, probabilities will be distributed across the entire range
    of possible values, depicting a multi-modal and often asymmetric distribution
    (whereas you should expect a Gaussian distribution around the maximum probability
    class).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的方法是将你的有序任务视为一个**多类**问题。在这种情况下，你将得到一个整数值（类别标签）的预测，但预测不会考虑这些类别有一定的顺序。如果你查看类别的预测概率，你会感觉到将问题作为多类问题处理可能存在问题。通常，概率将分布在所有可能值的整个范围内，描绘出一个多模态且通常不对称的分布（而你应该期望最大概率类周围有一个高斯分布）。
- en: The other way to solve the ordinal prediction problem is to treat it as a **regression**
    problem and then post-process your result. In this way, the order among classes
    will be taken into consideration, though the prediction output won’t be immediately
    useful for scoring on the evaluation metric. In fact, in a regression you get
    a float number as an output, not an integer representing an ordinal class; moreover,
    the result will include the full range of values between the integers of your
    ordinal distribution and possibly also values outside of it. Cropping the output
    values and casting them into integers by unit rounding may do the trick, but this
    might lead to inaccuracies requiring some more sophisticated post-processing (we’ll
    discuss more on this later in the chapter).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解决有序预测问题的另一种方法是将它视为一个**回归**问题，然后对结果进行后处理。这样，类之间的顺序将被考虑在内，尽管预测输出不会立即用于评估指标上的评分。实际上，在回归中，你得到的是一个浮点数作为输出，而不是表示有序类的整数；此外，结果将包括你有序分布中的整数之间的全部值，甚至可能还包括它之外的值。通过裁剪输出值并将它们通过单位舍入转换为整数可能可行，但这可能会导致一些需要更复杂后处理的误差（我们将在本章后面进一步讨论这个问题）。
- en: Now, you may be wondering what kind of evaluation you should master in order
    to succeed in Kaggle. Clearly, you always have to master the evaluation metric
    of the competition you have taken on. However, some metrics are more common than
    others, which is information you can use to your advantage. What are the most
    common metrics? How can we figure out where to look for insights in competitions
    that have used similar evaluation metrics? The answer is to consult the Meta Kaggle
    dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道为了在Kaggle上取得成功，你应该掌握哪种类型的评估。显然，你必须掌握你所参加的竞赛的评估指标。然而，一些指标比其他指标更常见，这是你可以利用的信息。最常见的指标是什么？我们如何确定在使用了类似评估指标的竞赛中寻找洞察力的地方？答案是查阅Meta
    Kaggle数据集。
- en: The Meta Kaggle dataset
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta Kaggle数据集
- en: The Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    is a collection of rich data about Kaggle’s community and activity, published
    by Kaggle itself as a public dataset. It contains CSV tables filled with public
    activity from Competitions, Datasets, Notebooks, and Discussions. All you have
    to do is to start a Kaggle Notebook (as you saw in *Chapters 2* and *3*), add
    to it the Meta Kaggle dataset, and start analyzing the data. The CSV tables are
    updated daily, so you’ll have to refresh your analysis often, but that’s worth
    it given the insights you can extract.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Meta Kaggle数据集([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))是Kaggle社区和活动的丰富数据集合，由Kaggle本身作为公共数据集发布。它包含CSV表格，其中充满了来自竞赛、数据集、笔记本和讨论的公共活动。你只需要开始一个Kaggle笔记本（如你在第2章和第3章中看到的），添加Meta
    Kaggle数据集，并开始分析数据。CSV表格每天都会更新，因此你将不得不经常刷新你的分析，但考虑到你可以从中获得的见解，这是值得的。
- en: We will sometimes refer to the Meta Kaggle dataset in this book, both as inspiration
    for many interesting examples of the dynamics in a competition and as a way to
    pick up useful examples for your learning and competition strategies. Here, we
    are going to use it in order to figure out what evaluation metrics have been used
    most frequently for competitions in the last seven years. By looking at the most
    common ones in this chapter, you’ll be able to start any competition from solid
    ground and then refine your knowledge of the metric, picking up competition-specific
    nuances using the discussion you find in the forums.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时会在这本书中提到Meta Kaggle数据集，既作为许多有趣竞赛动态的灵感来源，也是为了收集有用的例子用于你的学习和竞赛策略。在这里，我们将用它来确定在过去七年里最常用于竞赛的评价指标。通过查看本章中最常见的指标，你将能够从稳固的起点开始任何竞赛，然后通过在论坛中找到的讨论来细化你对指标的了解，并掌握竞赛特有的细微差别。
- en: 'Below, we introduce the code necessary to produce a data table of metrics and
    their counts per year. It is designed to run directly on the Kaggle platform:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们介绍了生成指标及其每年计数的表格所需代码。它设计为可以直接在Kaggle平台上运行：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we read in the CSV table containing the data relating to the competitions.
    We focus on the columns representing the evaluation and the columns informing
    us about the competition name, start date, and type. We limit the rows to those
    competitions held since 2015 and that are of the Featured or Research type (which
    are the most common ones). We complete the analysis by creating a pandas pivot
    table, combining the evaluation algorithm with the year, and counting the number
    of competitions using it. We just display the top 20 algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们读取包含竞赛相关数据的CSV表格。我们关注代表评估的列以及告诉我们竞赛名称、开始日期和类型的列。我们将行限制在自2015年以来举行且为特色或研究类型的竞赛（这些是最常见的类型）。我们通过创建pandas交叉表，结合评估算法和年份，并计算使用该算法的竞赛数量来完成分析。我们只显示前20个算法。
- en: 'Here is the resulting table (at the time of writing):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的表格（截至写作时）：
- en: '| **year** | **2015** | **2016** | **2017** | **2018** | **2019** | **2020**
    | **2021** | **Tot** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **年份** | **2015** | **2016** | **2017** | **2018** | **2019** | **2020**
    | **2021** | **总计** |'
- en: '| **Evaluation Algorithm** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **评估算法** |'
- en: '| AUC | 4 | 4 | 1 | 3 | 3 | 2 | 0 | 17 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| AUC | 4 | 4 | 1 | 3 | 3 | 2 | 0 | 17 |'
- en: '| LogLoss | 2 | 2 | 5 | 2 | 3 | 2 | 0 | 16 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| LogLoss | 2 | 2 | 5 | 2 | 3 | 2 | 0 | 16 |'
- en: '| MAP@{K} | 1 | 3 | 0 | 4 | 1 | 0 | 1 | 10 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| MAP@{K} | 1 | 3 | 0 | 4 | 1 | 0 | 1 | 10 |'
- en: '| CategorizationAccuracy | 1 | 0 | 4 | 0 | 1 | 2 | 0 | 8 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| CategorizationAccuracy | 1 | 0 | 4 | 0 | 1 | 2 | 0 | 8 |'
- en: '| MulticlassLoss | 2 | 3 | 2 | 0 | 1 | 0 | 0 | 8 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| MulticlassLoss | 2 | 3 | 2 | 0 | 1 | 0 | 0 | 8 |'
- en: '| RMSLE | 2 | 1 | 3 | 1 | 1 | 0 | 0 | 8 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| RMSLE | 2 | 1 | 3 | 1 | 1 | 0 | 0 | 8 |'
- en: '| QuadraticWeightedKappa | 3 | 0 | 0 | 1 | 2 | 1 | 0 | 7 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| QuadraticWeightedKappa | 3 | 0 | 0 | 1 | 2 | 1 | 0 | 7 |'
- en: '| MeanFScoreBeta | 1 | 0 | 1 | 2 | 1 | 2 | 0 | 7 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| MeanFScoreBeta | 1 | 0 | 1 | 2 | 1 | 2 | 0 | 7 |'
- en: '| MeanBestErrorAtK | 0 | 0 | 2 | 2 | 1 | 1 | 0 | 6 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| MeanBestErrorAtK | 0 | 0 | 2 | 2 | 1 | 1 | 0 | 6 |'
- en: '| MCRMSLE | 0 | 0 | 1 | 0 | 0 | 5 | 0 | 6 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MCRMSLE | 0 | 0 | 1 | 0 | 0 | 5 | 0 | 6 |'
- en: '| MCAUC | 1 | 0 | 1 | 0 | 0 | 3 | 0 | 5 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| MCAUC | 1 | 0 | 1 | 0 | 0 | 3 | 0 | 5 |'
- en: '| RMSE | 1 | 1 | 0 | 3 | 0 | 0 | 0 | 5 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | 1 | 1 | 0 | 3 | 0 | 0 | 0 | 5 |'
- en: '| Dice | 0 | 1 | 1 | 0 | 2 | 1 | 0 | 5 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Dice | 0 | 1 | 1 | 0 | 2 | 1 | 0 | 5 |'
- en: '| GoogleGlobalAP | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 5 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GoogleGlobalAP | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 5 |'
- en: '| MacroFScore | 0 | 0 | 0 | 1 | 0 | 2 | 1 | 4 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| MacroFScore | 0 | 0 | 0 | 1 | 0 | 2 | 1 | 4 |'
- en: '| Score | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 3 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 分数 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 3 |'
- en: '| CRPS | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 3 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| CRPS | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 3 |'
- en: '| OpenImagesObjectDetectionAP | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 3 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| OpenImagesObjectDetectionAP | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 3 |'
- en: '| MeanFScore | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 3 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| MeanFScore | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 3 |'
- en: '| RSNAObjectDetectionAP | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 2 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| RSNAObjectDetectionAP | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 2 |'
- en: 'Using the same variables we just instantiated in order to generate the table,
    you can also check the data to find the competitions where the metric of your
    choice has been adopted:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚实例化的相同变量来生成表格，你还可以检查数据以找到采用你选择指标的竞赛：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the above snippet, we decided to represent the competitions that have been
    using the AUC metric. You just have to change the string representing the chosen
    metric and the resulting list will be updated accordingly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的片段中，我们决定表示使用 AUC 指标的竞赛。你只需更改表示所选指标的字符串，结果列表将相应更新。
- en: 'Coming back to the table generated, we can examine the most popular evaluation
    metrics used in competitions hosted on Kaggle:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回到生成的表格，我们可以检查在 Kaggle 主办的竞赛中最受欢迎的评估指标：
- en: The two top metrics are closely related to each other and to binary probability
    classification problems. The **AUC** metric helps to measure if your model’s predicted
    probabilities tend to predict positive cases with high probabilities, and the
    **Log Loss** helps to measure how far your predicted probabilities are from the
    ground truth (and as you optimize for Log Loss, you also optimize for the AUC
    metric).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个最重要的指标彼此之间以及与二进制概率分类问题密切相关。**AUC** 指标有助于衡量你的模型预测概率是否倾向于以高概率预测阳性案例，而 **Log
    Loss** 指标有助于衡量你的预测概率与真实值之间的差距（并且当你优化 Log Loss 时，你也在优化 AUC 指标）。
- en: In 3^(rd) position, we find **MAP@{K}**, which is a common metric in recommender
    systems and search engines. In Kaggle competitions, this metric has been used
    mostly for information retrieval evaluations, such as in the *Humpback Whale Identification*
    competition ([https://www.kaggle.com/c/humpback-whale-identification](https://www.kaggle.com/c/humpback-whale-identification)),
    where you have to precisely identify a whale and you have five possible guesses.
    Another example of MAP@{K} usage is in the *Quick, Draw! Doodle Recognition Challenge*
    ([https://www.kaggle.com/c/quickdraw-doodle-recognition/](https://www.kaggle.com/c/quickdraw-doodle-recognition/)),
    where your goal is to guess the content of a drawn sketch and you are allowed
    three attempts. In essence, when MAP@{K} is the evaluation metric, you can score
    not just if you can guess correctly, but also if your correct guess is among a
    certain number (the “K” in the name of the function) of other incorrect predictions.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 3 位，我们发现 **MAP@{K}**，这是推荐系统和搜索引擎中常用的一个指标。在 Kaggle 竞赛中，这个指标主要用于信息检索评估，例如在
    *座头鲸识别* 竞赛（[https://www.kaggle.com/c/humpback-whale-identification](https://www.kaggle.com/c/humpback-whale-identification)）中，你需要在五个可能的猜测中精确识别一只鲸鱼。**MAP@{K}**
    的另一个应用示例是在 *快速绘画！涂鸦识别挑战*（[https://www.kaggle.com/c/quickdraw-doodle-recognition/](https://www.kaggle.com/c/quickdraw-doodle-recognition/)）中，你的目标是猜测所画草图的内容，并且你有三次尝试的机会。本质上，当
    MAP@{K} 是评估指标时，你可以评分的不仅仅是能否正确猜测，还包括你的正确猜测是否在一定的其他错误预测数量（函数名称中的“K”）中。
- en: Only in 6^(th) position can we find a regression metric, the **RMSLE**, or **Root
    Mean Squared Logarithmic Error**, and in 7^(th) place the **Quadratic Weighted
    Kappa**, a metric particularly useful for estimating model performance on problems
    that involve guessing a progressive integer number (an ordinal scale problem).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在第 6 位，我们才能找到一个回归指标，即 **RMSLE** 或 **Root Mean Squared Logarithmic Error**，在第
    7 位是 **Quadratic Weighted Kappa**，这是一个特别有用的指标，用于估计模型在涉及猜测递增整数数（有序尺度问题）的问题上的性能。
- en: As you skim through the list of top metrics, you will keep on finding metrics
    that are commonly discussed in machine learning textbooks. In the next few sections,
    after first discussing what to do when you meet a never-before-seen metric (something
    that happens in Kaggle competitions more frequently than you may expect), we will
    revise some of the most common metrics found in regression and classification
    competitions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当你浏览顶级指标列表时，你将不断发现这些指标在机器学习教科书中经常被讨论。在接下来的几节中，首先讨论当你遇到从未见过的指标时应该做什么（这种情况在 Kaggle
    竞赛中发生的频率可能比你预期的要高），然后我们将回顾回归和分类竞赛中最常见的指标。
- en: Handling never-before-seen metrics
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理从未见过的指标
- en: Before proceeding, we have to consider that the top 20 table doesn’t cover all
    the metrics used in competitions. We should be aware that there are metrics that
    have only been used once in recent years.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须考虑的是，前20名表格并没有涵盖所有比赛中使用的指标。我们应该意识到，近年来有一些指标只被使用过一次。
- en: 'Let’s keep on using the results from the previous code to find out what they
    are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用之前代码的结果来找出它们是什么：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a result, we get the following table showing, for each year, how many competitions
    used a metric that has never been used afterward (`n_comps`), and the proportion
    of these competitions per year (`pct_comps`):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了以下表格，展示了每年有多少比赛使用了此后从未再被使用的指标（`n_comps`），以及这些比赛在每年中的比例（`pct_comps`）：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Observing the relative share of competitions with a never-to-be-seen-afterward
    metric, we immediately notice how it is growing year by year and that it reached
    the 25%-30% level in recent years, implying that typically one competition out
    of every three or four requires you to study and understand a metric from scratch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 观察从未再被使用过的指标的竞赛相对份额，我们立即注意到它逐年增长，并在近年来达到了25%-30%的水平，这意味着通常每三到四个竞赛中就有一个需要你从头开始研究和理解一个指标。
- en: 'You can get the list of such metrics that have occurred in the past with a
    simple code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过一个简单的代码片段获取过去发生过的此类指标列表：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By executing the code, you will get a list similar to this one:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行代码，你会得到一个类似这样的列表：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By close inspection, you can find many metrics relating to deep learning and
    reinforcement learning competitions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细检查，你可以找到许多与深度学习和强化学习比赛相关的指标。
- en: What do you do when you meet a metric that has never been used before? Of course,
    you can rely on the discussions in the Kaggle discussion forums, where you can
    always find good inspiration and many Kagglers who will help you. However, if
    you want to build up your own knowledge about the metric, aside from Googling
    it, we advise that you try to experiment with it by coding the evaluation function
    by yourself, even in an imperfect way, and trying to simulate how the metric reacts
    to different types of error produced by the model. You could also directly test
    how it functions on a sample from the competition training data or synthetic data
    that you have prepared.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你遇到一个以前从未使用过的指标时，你会怎么做？当然，你可以依赖Kaggle讨论论坛中的讨论，在那里你总能找到好的灵感，以及许多愿意帮助你的Kagglers。然而，如果你想建立自己对这一指标的知识，除了在谷歌上搜索之外，我们建议你尝试通过自己编写评估函数来实验它，即使是不完美的，并尝试模拟指标对模型产生的不同类型错误的反应。你也可以直接测试它在比赛训练数据样本或你准备好的合成数据上的功能。
- en: 'We can quote a few examples of this approach as used by Kagglers:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以引用一些Kagglers使用此方法的一些例子：
- en: '*Carlo Lepelaars* with Spearman’s Rho: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho](https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卡洛·莱佩拉尔斯* 使用斯皮尔曼相关系数：[https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho](https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho)'
- en: 'Carlo Lepelaars with Quadratic Weighted Kappa: [https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa](https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡洛·莱佩拉尔斯使用二次加权Kappa：[https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa](https://www.kaggle.com/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa)
- en: '*Rohan Rao* with Laplace Log Likelihood: [https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood](https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*罗汉·拉奥* 使用拉普拉斯对数似然：[https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood](https://www.kaggle.com/rohanrao/osic-understanding-laplace-log-likelihood)'
- en: This can give you increased insight into the evaluation and an advantage over
    other competitors relying only on answers from Googling and Kaggle forums.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以让你对评估有更深入的了解，并比那些只依赖谷歌搜索和Kaggle论坛答案的竞争对手有优势。
- en: '![](img/Rohan_Rao.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Rohan_Rao.png)'
- en: Rohan Rao
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 罗汉·拉奥
- en: '[https://www.kaggle.com/rohanrao](https://www.kaggle.com/rohanrao)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/rohanrao](https://www.kaggle.com/rohanrao)'
- en: Before we start exploring different metrics, let’s catch up with Rohan Rao (aka
    Vopani) himself, Quadruple Grandmaster and Senior Data Scientist at `H2O.ai`,
    about his successes on Kaggle and the wisdom he has to share with us.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索不同的指标之前，让我们了解一下罗汉·拉奥（又名Vopani）本人，他是`H2O.ai`的超级大师级高级数据科学家，关于他在Kaggle上的成功以及他要与我们分享的智慧。
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your speciality on Kaggle?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在Kaggle上，你在技术和解决方法方面有什么专长？
- en: '*I like to dabble with different types of competitions, but my favorite would
    certainly be time series ones. I don’t quite like the typical approaches to and
    concepts of time series in the industry, so I tend to innovate and think out of
    the box by building solutions in an unorthodox way, which has ended up being very
    successful for me.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢尝试不同类型的比赛，但我的最爱无疑是时间序列比赛。我不太喜欢行业中对时间序列的典型方法和概念，所以我倾向于通过以非常规的方式构建解决方案来创新和跳出思维定势，这对我的成功非常有帮助。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何处理Kaggle比赛？这种处理方式与你的日常工作有何不同？
- en: '*For any Kaggle competition, my typical workflow would look like this:*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于任何Kaggle比赛，我的典型工作流程如下：*'
- en: '*Understand the problem statement and read all the information related to rules,
    format, timelines, datasets, metrics, and deliverables.*'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解问题陈述，并阅读所有与规则、格式、时间表、数据集、指标和交付成果相关的信息。*'
- en: '*Dive deep into the data. Slice and dice it in every way possible and explore/visualize
    it to be able to answer any question about it.*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深入数据。以任何可能的方式切割和分解它，并探索/可视化它，以便能够回答关于它的任何问题。*'
- en: '*Build a simple pipeline with a baseline model and make a submission to confirm
    the process works.*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用基线模型构建一个简单的流水线，并提交以确认流程是否有效。*'
- en: '*Engineer features, tune hyperparameters, and* *experiment with multiple models
    to get a sense of what’s generally working and what’s not.*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工程师特征，调整超参数，并且* *尝试多个模型以了解哪些通常有效，哪些无效。*'
- en: '*Constantly go back to analyzing the data, reading discussions on the forum,
    and tweaking the features and models to the fullest. Maybe team up at some point.*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不断地回到分析数据，阅读论坛上的讨论，并尽可能调整特征和模型。也许在某个时刻可以组建团队。*'
- en: '*Ensemble multiple models and decide which submissions to make as final.*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成多个模型，并决定哪些提交作为最终版本。*'
- en: '*In my day-to-day work in data science, most of this happens too. But there
    are two crucial elements that are additionally required:*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的数据科学日常工作中，这些事情也经常发生。但还有两个额外需要的关键要素：*'
- en: '*Curating and preparing datasets for the problem statement.*'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为问题陈述准备和整理数据集。*'
- en: '*Deploying the final model or solution into production.*'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将最终模型或解决方案部署到生产环境中。*'
- en: '*The majority of my time has been spent in these two activities for most of
    the projects I’ve worked on in the past.*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在过去参与的大多数项目中，大部分时间都花在这两个活动上。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你的职业生涯？如果是，是如何帮助的？
- en: '*The vast majority of everything I’ve learned in machine learning has come
    from Kaggle. The community, the platform, and the content are pure gold and there
    is an incredible amount of stuff you can learn.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在机器学习中学到的绝大多数东西都来自Kaggle。社区、平台和内容都是纯金，你可以学到的东西非常多。*'
- en: '*What has benefitted me the most is the experience of competing in Kaggle competitions;
    it has given me immense confidence in understanding, structuring, and solving
    problems across domains, which I have been able to apply successfully in many
    of the companies and projects I worked on outside Kaggle.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*对我最有益的是参加Kaggle比赛的经验；它使我在理解、构建和解决跨领域问题方面有了巨大的信心，我能够成功地将其应用于Kaggle之外的公司和项目中。*'
- en: '*Many recruiters have contacted me for opportunities looking at my Kaggle achievements,
    primarily in Competitions. It gives a fairly good indication of a candidate’s
    ability in solving data science problems and hence it is a great platform to showcase
    your skills* *and build a portfolio.*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*许多招聘人员联系我，寻求查看我的Kaggle成就的机会，主要是在竞赛方面。它相当好地表明了候选人在解决数据科学问题方面的能力，因此这是一个展示你的技能和建立作品集的绝佳平台。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*I’ve made some mistake in every competition! That’s how you learn and improve.
    Sometimes it’s a coding bug, sometimes a flawed validation setup, sometimes an
    incorrect submission selection!*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在每一场比赛中都犯过一些错误！这就是你学习和进步的方式。有时是编码错误，有时是验证设置有缺陷，有时是提交选择不正确！*'
- en: '*What’s important is to learn from these and ensure you don’t repeat them.
    Iterating over this process automatically helps to improve your overall performance
    on Kaggle.*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*重要的是要从这些经验中学习，并确保你不会重复它们。自动迭代这个过程有助于提高你在Kaggle上的整体表现。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis/machine learning?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析/机器学习？
- en: '*I strongly believe in never marrying a technology. Use whatever works best,
    whatever is most comfortable and effective, but constantly be open to learning
    new tools and libraries.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*我坚信永远不要与一项技术结婚。使用最好的，最舒适和最有效的，但始终开放学习新的工具和库。*'
- en: Metrics for regression (standard and ordinal)
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归的度量（标准和序数）
- en: When working with regression problems, that is, problems that involve estimating
    a continuous value (that could range from minus infinity to infinity), the most
    commonly used error measures are **RMSE** (**root mean squared error**) and **MAE**
    (**mean absolute error**), but you can also find slightly different error measures
    useful, such as RMSLE or MCRMSLE.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理回归问题时，即涉及估计一个连续值（可能从负无穷大到正无穷大）的问题时，最常用的误差度量是**RMSE**（根均方误差）和**MAE**（平均绝对误差），但你也可以发现一些稍微不同的误差度量很有用，例如RMSLE或MCRMSLE。
- en: Mean squared error (MSE) and R squared
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均方误差（MSE）和R平方
- en: The root mean squared error is the root of the **mean squared error** (**MSE**),
    which is nothing else but the mean of the good old **sum of squared errors** (**SSE**)
    that you learned about when you studied how a regression works.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根均方误差是**均方误差**（MSE）的平方根，这实际上就是你在学习回归如何工作时学到的**平方误差和**（SSE）的平均值。
- en: 'Here is the formula for the MSE:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是MSE的公式：
- en: '![](img/B17574_05_001.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_001.png)'
- en: 'Let’s start by explaining how the formula works. First of all, *n* indicates
    the number of cases, ![](img/B17574_05_002.png) is the ground truth, and ![](img/B17574_05_003.png)
    the prediction. You first get the difference between your predictions and your
    real values. You square the differences (so they become positive or simply zero),
    then you sum them all, resulting in your SSE. Then you just have to divide this
    measure by the number of predictions to obtain the average value, the MSE. Usually,
    all regression models minimize the SSE, so you won’t have great problems trying
    to minimize MSE or its direct derivatives such as **R squared** (also called the
    **coefficient of determination**), which is given by:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先解释一下公式是如何工作的。首先，*n*表示案例数量，![](img/B17574_05_002.png)是真实值，![](img/B17574_05_003.png)是预测值。你首先得到你的预测值和真实值之间的差异。然后你对差异进行平方（这样它们就变成正数或简单地为零），然后将它们全部相加，得到你的SSE。然后你只需将这个度量除以预测的数量，以获得平均值，即MSE。通常，所有回归模型都最小化SSE，所以你不会在尝试最小化MSE或其直接导数（如**R平方**，也称为**确定系数**）时遇到太大问题，它由以下公式给出：
- en: '![](img/B17574_05_004.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_004.png)'
- en: 'Here, SSE (the sum of squared errors) is compared to the **sum of squares total**
    (**SST**), which is just the variance of the response. In statistics, in fact,
    SST is defined as the squared difference between your target values and their
    mean:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，SSE（平方误差和）与**平方和总**（SST）进行比较，这实际上是响应的方差。实际上，在统计学中，SST被定义为你的目标值与它们的平均值之间的平方差：
- en: '![](img/B17574_05_005.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_005.png)'
- en: To put it another way, R squared compares the squared errors of the model against
    the squared errors from the simplest model possible, the average of the response.
    Since both SSE and SST have the same scale, R squared can help you to determine
    whether transforming your target is helping to obtain better predictions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，R平方比较了模型平方误差与可能的最简单模型（响应平均值）的平方误差。由于SSE和SST具有相同的尺度，R平方可以帮助你确定转换目标是否有助于获得更好的预测。
- en: Please remember that linear transformations, such as minmax ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))
    or standardization ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),
    do not change the performance of any regressor, since they are linear transformations
    of the target. **Non-linear** transformations, such as the square root, the cubic
    root, the logarithm, the exponentiation, and their combinations, should instead
    definitely modify the performance of your regression model on the evaluation metric
    (hopefully for the better, if you decide on the right transformation).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，线性变换，如minmax（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)）或标准化（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)），不会改变任何回归器的性能，因为它们是目标值的线性变换。**非线性**变换，如平方根、立方根、对数、指数及其组合，则应该肯定地修改你的回归模型在评估指标上的性能（如果你选择了正确的变换，希望是更好的）。
- en: MSE is a great instrument for comparing regression models applied to the same
    problem. The bad news is that the MSE is seldom used in Kaggle competitions, since
    RMSE is preferred. In fact, by taking the root of MSE, its value will resemble
    the original scale of your target and it will be easier at a glance to figure
    out if your model is doing a good job or not. In addition, if you are considering
    the same regression model across different data problems (for instance, across
    various datasets or data competitions), R squared is better because it is perfectly
    correlated with MSE and its values range between 0 and 1, making all comparisons
    easier.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MSE是用于比较应用于同一问题的回归模型的一个很好的工具。坏消息是MSE在Kaggle竞赛中很少使用，因为RMSE更受欢迎。实际上，通过取MSE的根，其值将类似于你的目标原始尺度，这将更容易一眼看出你的模型是否做得很好。此外，如果你正在考虑跨不同数据问题（例如，跨各种数据集或数据竞赛）的相同回归模型，R平方更好，因为它与MSE完美相关，其值介于0和1之间，这使得所有比较都更容易。
- en: Root mean squared error (RMSE)
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根均方误差（RMSE）
- en: 'RMSE is just the square root of MSE, but this implies some subtle change. Here
    is its formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE只是MSE的平方根，但这意味着一些微妙的变化。以下是它的公式：
- en: '![](img/B17574_05_006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_006.png)'
- en: In the above formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. In MSE, large
    prediction errors are greatly penalized because of the squaring activity. In RMSE,
    this dominance is lessened because of the root effect (however, you should always
    pay attention to outliers; they can affect your model performance a lot, no matter
    whether you are evaluating based on MSE or RMSE).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，*n*表示案例数量，![图片](img/B17574_05_002.png)是真实值，![图片](img/B17574_05_008.png)是预测值。在MSE中，由于平方操作，大的预测误差会受到极大的惩罚。在RMSE中，由于根效应，这种主导性减弱（然而，你应该始终注意异常值；无论你是基于MSE还是RMSE进行评估，它们都可能对你的模型性能产生很大影响）。
- en: Consequently, depending on the problem, you can get a better fit with an algorithm
    using MSE as an objective function by first applying the square root to your target
    (if possible, because it requires positive values), then squaring the results.
    Functions such as the `TransformedTargetRegressor` in Scikit-learn help you to
    appropriately transform your regression target in order to get better-fitting
    results with respect to your evaluation metric.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据问题，你可以在将目标值开平方（如果可能的话，因为这需要正值）然后平方结果之后，使用均方误差（MSE）作为目标函数来获得更好的算法拟合。Scikit-learn中的`TransformedTargetRegressor`等函数可以帮助你适当地转换回归目标，以便在评估指标方面获得更好的拟合结果。
- en: 'Recent competitions where RMSE has been used include:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最近使用RMSE的竞赛包括：
- en: '*Avito Demand Prediction Challenge*: [https://www.kaggle.com/c/avito-demand-prediction](https://www.kaggle.com/c/avito-demand-prediction)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Avito Demand Prediction Challenge*：[https://www.kaggle.com/c/avito-demand-prediction](https://www.kaggle.com/c/avito-demand-prediction)'
- en: '*Google Analytics Customer Revenue Prediction*: [https://www.kaggle.com/c/ga-customer-revenue-prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google Analytics Customer Revenue Prediction*：[https://www.kaggle.com/c/ga-customer-revenue-prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction)'
- en: '*Elo Merchant Category Recommendation* [https://www.kaggle.com/c/elo-merchant-category-recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elo商家类别推荐* [https://www.kaggle.com/c/elo-merchant-category-recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)'
- en: Root mean squared log error (RMSLE)
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根均方对数误差 (RMSLE)
- en: 'Another common transformation of MSE is **root mean squared log error** (**RMSLE**).
    MCRMSLE is just a variant made popular by the COVID-19 forecasting competitions,
    and it is the column-wise average of the RMSLE values of each single target when
    there are multiple ones. Here is the formula for RMSLE:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: MSE（均方误差）的另一种常见转换是**根均方对数误差**（**RMSLE**）。MCRMSLE只是由COVID-19预测竞赛推广的一种变体，它是当存在多个单一目标时，每个目标RMSLE值的列平均值。以下是RMSLE的公式：
- en: '![](img/B17574_05_009.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_009.png)'
- en: In the formula, *n* indicates the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. Since you
    are applying a logarithmic transformation to your predictions and your ground
    truth before all the other squaring, averaging, and rooting operations, you don’t
    penalize huge differences between the predicted and the actual values, especially
    when both are large numbers. In other words, what you care the most about when
    using RMSLE is *the scale of your predictions with respect to the scale of the
    ground truth*. As with RMSE, machine learning algorithms for regression can better
    optimize for RMSLE if you apply a logarithmic transformation to the target before
    fitting it (and then reverse the effect using the exponential function).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*n*表示案例数量，![](img/B17574_05_002.png)是真实值，![](img/B17574_05_008.png)是预测值。由于你在所有其他平方、平均和开根运算之前对预测值和真实值应用了对数变换，因此你不会对预测值和实际值之间的大差异进行惩罚，尤其是在两者都是大数的情况下。换句话说，当你使用RMSLE时，你最关心的是*预测值与真实值规模的比例*。与RMSE一样，如果你在拟合之前对目标应用对数变换，机器学习算法可以更好地优化RMSLE（然后使用指数函数逆转效果）。
- en: 'Recent competitions using RMSLE as an evaluation metric are:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最近使用RMSLE作为评估指标的竞赛包括：
- en: '*ASHRAE - Great Energy Predictor III*: [https://www.kaggle.com/c/ashrae-energy-prediction](https://www.kaggle.com/c/ashrae-energy-prediction)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ASHRAE - 伟大能源预测者III*: [https://www.kaggle.com/c/ashrae-energy-prediction](https://www.kaggle.com/c/ashrae-energy-prediction)'
- en: '*Santander Value Prediction Challenge*: [https://www.kaggle.com/c/santander-value-prediction-challenge](https://www.kaggle.com/c/santander-value-prediction-challenge)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Santander价值预测挑战*: [https://www.kaggle.com/c/santander-value-prediction-challenge](https://www.kaggle.com/c/santander-value-prediction-challenge)'
- en: '*Mercari Price Suggestion Challenge*: [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mercari价格建议挑战*: [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)'
- en: '*Sberbank Russian Housing Market*: [https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market](https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sberbank俄罗斯住房市场*: [https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market](https://www.kaggle.com/olgabelitskaya/sberbank-russian-housing-market)'
- en: '*Recruit Restaurant Visitor Forecasting*: [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Recruit餐厅访客预测*: [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)'
- en: By far, at the moment, RMSLE is the most used evaluation metric for regression
    in Kaggle competitions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，RMSLE（Root Mean Squared Log Error，根均方对数误差）是Kaggle竞赛中回归问题最常用的评估指标。
- en: Mean absolute error (MAE)
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均值绝对误差 (MAE)
- en: 'The **MAE** (**mean absolute error**) evaluation metric is the absolute value
    of the difference between the predictions and the targets. Here is the formulation
    of MAE:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAE**（**平均绝对误差**）评估指标是预测值与目标之间的差异的绝对值。以下是MAE的公式：'
- en: '![](img/B17574_05_012.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_012.png)'
- en: In the formula, *n* stands for the number of cases, ![](img/B17574_05_002.png)
    is the ground truth, and ![](img/B17574_05_008.png) the prediction. MAE is not
    particularly sensitive to outliers (unlike MSE, where errors are squared), hence
    you may find it is an evaluation metric in many competitions whose datasets present
    outliers. Moreover, you can easily work with it since many algorithms can directly
    use it as an objective function; otherwise, you can optimize for it indirectly
    by just training on the square root of your target and then squaring the predictions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*n*代表案例数量，![](img/B17574_05_002.png)是真实值，而![](img/B17574_05_008.png)是预测值。MAE对异常值不敏感（与MSE不同，其中误差是平方的），因此你可能会发现它是在许多竞赛中使用的评估指标，这些竞赛的数据集包含异常值。此外，你可以轻松地与之合作，因为许多算法可以直接将其用作目标函数；否则，你可以通过仅对目标的平方根进行训练，然后对预测值进行平方来间接优化它。
- en: 'In terms of downside, using MAE as an objective function results in much slower
    convergence, since you are actually optimizing for predicting the median of the
    target (also called the L1 norm), instead of the mean (also called the L2 norm),
    as occurs by MSE minimization. This results in more complex computations for the
    optimizer, so the training time can even grow exponentially based on your number
    of training cases (see, for instance, this Stack Overflow question: [https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to](https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下行方面，使用MAE作为目标函数会导致收敛速度大大减慢，因为你实际上是在优化预测目标的中位数（也称为L1范数），而不是均值（也称为L2范数），正如MSE最小化所发生的那样。这导致优化器需要进行更复杂的计算，因此训练时间甚至可以根据你的训练案例数量呈指数增长（例如，参见这个Stack
    Overflow问题：[https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to](https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to))。
- en: 'Notable recent competitions that used MAE as an evaluation metric are:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一些使用MAE作为评估指标的著名竞赛包括：
- en: '*LANL Earthquake Prediction*: [https://www.kaggle.com/c/LANL-Earthquake-Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LANL地震预测*: [https://www.kaggle.com/c/LANL-Earthquake-Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction)'
- en: '*How Much Did It Rain? II*: [https://www.kaggle.com/c/how-much-did-it-rain-ii](https://www.kaggle.com/c/how-much-did-it-rain-ii)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降雨量有多少？II*: [https://www.kaggle.com/c/how-much-did-it-rain-ii](https://www.kaggle.com/c/how-much-did-it-rain-ii)'
- en: Having mentioned the ASHRAE competition earlier, we should also mention that
    regression evaluation measures are quite relevant to forecasting competitions.
    For instance, the M5 forecasting competition was held recently ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))
    and data from all the other M competitions is available too. If you are interested
    in forecasting competitions, of which there are a few on Kaggle, please see [https://robjhyndman.com/hyndsight/forecasting-competitions/](https://robjhyndman.com/hyndsight/forecasting-competitions/)
    for an overview about M competitions and how valuable Kaggle is for obtaining
    better practical and theoretical results from such competitions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到了ASHRAE竞赛之后，我们也应该提到回归评估指标与预测竞赛的相关性。例如，最近举办了M5预测竞赛([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))，而且所有其他M竞赛的数据也是可用的。如果你对预测竞赛感兴趣，其中Kaggle上有几个，请参阅[https://robjhyndman.com/hyndsight/forecasting-competitions/](https://robjhyndman.com/hyndsight/forecasting-competitions/)以了解M竞赛的概述以及Kaggle在从这些竞赛中获得更好的实际和理论结果方面的重要性。
- en: Essentially, forecasting competitions do not require a very different evaluation
    to regression competitions. When dealing with forecasting tasks, it is true that
    you can get some unusual evaluation metrics such as the **Weighted Root Mean Squared
    Scaled Error** ([https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation))
    or the **symmetric mean absolute percentage error**, better known as **sMAPE**
    ([https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation](https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation)).
    However, in the end they are just variations of the usual RMSE or MAE that you
    can handle using the right target transformations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，预测竞赛并不需要与回归竞赛非常不同的评估方法。在处理预测任务时，确实可以获取一些不寻常的评估指标，例如**加权均方根误差**（[https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation)）或**对称平均绝对百分比误差**，更广为人知的缩写为**sMAPE**（[https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation](https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation)）。然而，最终它们只是常规RMSE或MAE的变体，你可以通过正确的目标转换来处理。
- en: Metrics for classification (label prediction and probability)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类（标签预测和概率）的指标
- en: Having discussed the metrics for regression problems, we are going now to illustrate
    the metrics for classification problems, starting from the binary classification
    problems (when you have to predict between two classes), moving to the multi-class
    (when you have more than two classes), and then to the multi-label (when the classes
    overlap).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了回归问题的指标之后，我们现在将说明分类问题的指标，从二元分类问题（当你需要预测两个类别之间）开始，然后到多类别（当你有超过两个类别），最后到多标签（当类别重叠时）。
- en: Accuracy
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确率
- en: 'When analyzing the performance of a binary classifier, the most common and
    accessible metric that is used is **accuracy**. A misclassification error is when
    your model predicts the wrong class for an example. The accuracy is just the complement
    of the misclassification error and it can be calculated as the ratio between the
    number of correct numbers divided by the number of answers:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析二元分类器的性能时，最常用且易于获取的指标是**准确率**。误分类错误是指你的模型对一个例子预测了错误的类别。准确率仅仅是误分类错误的补数，它可以计算为正确数字的数量除以答案数量的比率：
- en: '![](img/B17574_05_015.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_015.png)'
- en: This metric has been used, for instance, in *Cassava Leaf Disease Classification*
    ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification))
    and *Text Normalization Challenge - English Language* ([https://www.kaggle.com/c/text-normalization-challenge-english-language](https://www.kaggle.com/c/text-normalization-challenge-english-language)),
    where you scored a correct prediction only if your predicted text matched the
    actual string.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标已经在例如*木薯叶病分类*（[https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)）和*文本规范化挑战
    - 英语语言*（[https://www.kaggle.com/c/text-normalization-challenge-english-language](https://www.kaggle.com/c/text-normalization-challenge-english-language)）中使用过，在这些挑战中，只有当你的预测文本与实际字符串匹配时，你才被认为做出了正确的预测。
- en: 'As a metric, the accuracy is focused strongly on the effective performance
    of the model in a real setting: it tells you if the model works as expected. However,
    if your purpose is to evaluate and compare and have a clear picture of how effective
    your approach really is, you have to be cautious when using the accuracy because
    it can lead to wrong conclusions when the classes are imbalanced (when they have
    different frequencies). For instance, if a certain class makes up just 10% of
    the data, a predictor that predicts nothing but the majority class will be 90%
    accurate, proving itself quite useless in spite of the high accuracy.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作为指标，准确率强烈关注模型在实际环境中的有效性能：它告诉你模型是否按预期工作。然而，如果你的目的是评估和比较，并清楚地了解你的方法实际上有多有效，那么在使用准确率时你必须谨慎，因为它可能导致错误的结论，尤其是在类别不平衡（它们有不同的频率）时。例如，如果一个特定的类别仅占数据的10%，那么一个只预测多数类别的预测器将会有90%的准确率，尽管准确率很高，但它实际上相当无用。
- en: 'How can you spot such a problem? You can do this easily by using a **confusion
    matrix**. In a confusion matrix, you create a two-way table comparing the actual
    classes on the rows against the predicted classes on the columns. You can create
    a straightforward one using the Scikit-learn `confusion_matrix` function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如何发现这样的问题？您可以通过使用**混淆矩阵**轻松做到这一点。在混淆矩阵中，您创建一个双向表，比较行上的实际类别与列上的预测类别。您可以使用Scikit-learn的`confusion_matrix`函数创建一个简单的混淆矩阵：
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Providing the `y_true` and `y_pred` vectors will suffice to return you a meaningful
    table, but you can also provide row/column labels and sample weights for the examples
    in consideration, and normalize (set the marginals to sum to 1) over the true
    examples (the rows), the predicted examples (the columns), or all the examples.
    A perfect classifier will have all the cases on the principal diagonal of the
    matrix. Serious problems with the validity of the predictor are highlighted if
    there are few or no cases on one of the cells of the diagonal.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 提供`y_true`和`y_pred`向量就足以返回一个有意义的表格，但您也可以提供行/列标签和考虑中的示例的样本权重，并在真实示例（行）、预测示例（列）或所有示例上归一化（将边缘设置为求和为1）。一个完美的分类器将所有案例都位于矩阵的主对角线上。如果对角线上的某个单元格中案例很少或没有，则突出显示预测器的有效性问题。
- en: 'In order to give you a better idea of how it works, you can try the graphical
    example offered by Scikit-learn at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您更好地了解其工作原理，您可以尝试Scikit-learn提供的图形示例，网址为[https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)：
- en: '![](img/B17574_05_01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_01.png)'
- en: 'Figure 5.1: Confusion matrix, with each cell normalized to 1.00, to represent
    the share of matches'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：混淆矩阵，每个单元格归一化到1.00，以表示匹配的份额
- en: You can attempt to improve the usability of the accuracy by considering the
    accuracy relative to each of the classes and averaging them, but you will find
    it more useful to rely on other metrics such as **precision**, **recall**, and
    the **F1-score**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过考虑每个类别的相对精度并取平均值来尝试提高准确性的可用性，但您会发现依赖其他指标如**精确度**、**召回率**和**F1分数**更有用。
- en: Precision and recall
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: 'To obtain the precision and recall metrics, we again start from the confusion
    matrix. First, we have to name each of the cells:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得精确度和召回率指标，我们再次从混淆矩阵开始。首先，我们必须命名每个单元格：
- en: '|  | **Predicted** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测** |'
- en: '|  |  | **Negative** | **Positive** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **负** | **正** |'
- en: '| **Actual** | **Negative** | True Negative | False Positive |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **实际** | **负** | 真阴性 | 假阳性 |'
- en: '| **Positive** | False Negative | True Positive |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **正** | 假阴性 | 真阳性 |'
- en: 'Table 5.1: Confusion matrix with cell names'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1：带有单元格名称的混淆矩阵
- en: 'Here is how we define the cells:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们定义单元格的方式：
- en: '**TP** (**true positives**): These are located in the upper-left cell, containing
    examples that have correctly been predicted as positive ones.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TP**（**真阳性**）：这些位于左上角单元格中，包含被正确预测为正例的示例。'
- en: '**FP** (**false positives**): These are located in the upper-right cell, containing
    examples that have been predicted as positive but are actually negative.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP**（**假阳性**）：这些位于右上角单元格中，包含被预测为正但实际上是负的示例。'
- en: '**FN** (**false negatives**): These are located in the lower-left cell, containing
    examples that have been predicted as negative but are actually positive.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FN**（**假阴性**）：这些位于左下角单元格中，包含被预测为负但实际上是正的示例。'
- en: '**TN** (**true negatives**): These are located in the lower-right cell, containing
    examples that have been correctly predicted as negative ones.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TN**（**真阴性**）：这些位于右下角单元格中，包含被正确预测为负例的示例。'
- en: 'Using these cells, you can actually get more precise information about how
    your classifier works and how you can tune your model better. First, we can easily
    revise the accuracy formula:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些单元格，您可以实际上获取更多关于您的分类器如何工作以及如何更好地调整模型的信息。首先，我们可以轻松地修改准确度公式：
- en: '![](img/B17574_05_016.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_016.png)'
- en: 'Then, the first informative metric is called **precision** (or **specificity**)
    and it is actually the accuracy of the positive cases:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，第一个有意义的度量指标被称为**精度**（或**特异性**），它实际上是正例的准确率：
- en: '![](img/B17574_05_017.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_017.png)'
- en: In the computation, only the number of true positives and the number of false
    positives are involved. In essence, the metric tells you how often you are correct
    when you predict a positive.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算中，只涉及真正例的数量和假正例的数量。本质上，这个度量指标告诉你，当你预测正例时，你有多正确。
- en: 'Clearly, your model could get high scores by predicting positives for only
    the examples it has high confidence in. That is actually the purpose of the measure:
    to force models to predict a positive class only when they are sure and it is
    safe to do so.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你的模型可以通过只对它有高置信度的示例预测正例来获得高分。这实际上是这个度量指标的目的：迫使模型只在它们确定并且这样做是安全的时候预测正类。
- en: 'However, if it is in your interest also to predict as many positives as possible,
    then you’ll also need to watch over the **recall** (or **coverage** or **sensitivity**
    or even **true positive rate**) metric:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你还希望尽可能多地预测出正例，那么你还需要关注**召回率**（或**覆盖率**、**灵敏度**甚至**真正例率**）指标：
- en: '![](img/B17574_05_018.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_018.png)'
- en: Here, you will also need to know about false negatives. The interesting thing
    about these two metrics is that, since they are based on examples classification,
    and a classification is actually based on probability (which is usually set between
    the positive and negative class at the `0.5` threshold), you can change the threshold
    and have one of the two metrics improved at the expense of the other.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你还需要了解假阴性。这两个度量指标有趣的地方在于，由于它们基于示例分类，而分类实际上是基于概率（通常在正负类之间设置在`0.5`阈值），你可以改变阈值，其中一个度量指标会得到改善，而另一个则会付出代价。
- en: For instance, if you increase the threshold, you will get more precision (the
    classifier is more confident of the prediction) but less recall. If you decrease
    the threshold, you get less precision but more recall. This is also called the
    **precision/recall trade-off**.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你提高阈值，你会得到更高的精度（分类器对预测更有信心），但召回率会降低。如果你降低阈值，你会得到较低的精度，但召回率会提高。这也被称为**精度/召回率权衡**。
- en: 'The Scikit-learn website offers a simple and practical overview of this trade-off
    ([https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)),
    helping you to trace a **precision/recall curve** and thus understand how these
    two measures can be exchanged to obtain a result that better fits your needs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn网站提供了一个简单实用的概述，介绍了这个权衡：[https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)，帮助你绘制**精度/召回率曲线**，从而理解这两个度量指标如何相互交换以获得更适合你需求的结果：
- en: '![](img/B17574_05_02.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_02.png)'
- en: 'Figure 5.2: A two-class precision-recall curve with its characteristic steps'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：具有特征步骤的两类精度-召回率曲线
- en: One metric associated with the precision/recall trade-off is the **average precision**.
    Average precision computes the mean precision for recall values from 0 to 1 (basically,
    as you vary the threshold from 1 to 0). Average precision is very popular for
    tasks related to object detection, which we will discuss a bit later on, but it
    is also very useful for classification in tabular data. In practice, it proves
    valuable when you want to monitor model performance on a very rare class (when
    the data is extremely imbalanced) in a more precise and exact way, which is often
    the case with fraud detection problems.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与精度/召回率权衡相关的一个度量指标是**平均精度**。平均精度计算从0到1的召回值对应的平均精度（基本上，当你将阈值从1变到0时）。平均精度在涉及对象检测的任务中非常受欢迎，我们稍后会讨论这一点，但它对于表格数据的分类也非常有用。在实践中，当你想要以更精确和准确的方式监控模型在非常罕见的类别（当数据极度不平衡时）上的性能时，它非常有价值，这在欺诈检测问题中通常是这种情况。
- en: 'For more specific insights on this, read *Gael Varoquaux’s* discussion: [http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision](http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这方面的更具体见解，请阅读*Gael Varoquaux*的讨论：[http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision](http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision)。
- en: The F1 score
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F1分数
- en: 'At this point, you have probably already figured out that using precision or
    recall as an evaluation metric is not an ideal choice because you can only optimize
    one at the expense of the other. For this reason, there are no Kaggle competitions
    that use only one of the two metrics. You should combine them (as in the average
    precision). A single metric, the **F1 score**, which is the harmonic mean of precision
    and recall, is commonly considered to be the best solution:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经意识到使用精确度或召回率作为评估指标并不是一个理想的选择，因为你在优化一个的同时必须牺牲另一个。因此，没有Kaggle竞赛只使用这两个指标中的任何一个。你应该将它们结合起来（如平均精确度）。一个单一的指标，**F1分数**，即精确度和召回率的调和平均数，通常被认为是最佳解决方案：
- en: '![](img/B17574_05_019.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_019.png)'
- en: If you get a high *F*1 score, it is because your model has improved in precision
    or recall or in both. You can find a fine example of the usage of this metric
    in the *Quora* *Insincere Questions Classification* competition ([https://www.kaggle.com/c/quora-insincere-questions-classification](https://www.kaggle.com/c/quora-insincere-questions-classification)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到一个高的**F**1分数，那是因为你的模型在精确度或召回率或两者方面都有所提高。你可以在*Quora* *Insincere Questions
    Classification*竞赛中找到一个使用此指标的精彩示例（[https://www.kaggle.com/c/quora-insincere-questions-classification](https://www.kaggle.com/c/quora-insincere-questions-classification)）。
- en: 'In some competitions, you also get the **F-beta** score. This is simply the
    weighted harmonic mean between precision and recall, and beta decides the weight
    of the recall in the combined score:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些竞赛中，你还会得到**F-beta**分数。这实际上是精确度和召回率之间的加权调和平均数，而beta决定了召回率在组合分数中的权重：
- en: '![](img/B17574_05_020.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_020.png)'
- en: Since we have already introduced the concept of threshold and classification
    probability, we can now discuss the log loss and ROC-AUC, both quite common classification
    metrics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经介绍了阈值和分类概率的概念，我们现在可以讨论对数损失和ROC-AUC，这两个都是相当常见的分类指标。
- en: Log loss and ROC-AUC
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数损失和ROC-AUC
- en: 'Let’s start with the **log loss**, which is also known as **cross-entropy**
    in deep learning models. The log loss is the difference between the predicted
    probability and the ground truth probability:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**对数损失**开始，它在深度学习模型中也被称为**交叉熵**。对数损失是预测概率与真实概率之间的差异：
- en: '![](img/B17574_05_021.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_021.png)'
- en: In the above formula, *n* stands for the number of examples, ![](img/B17574_05_002.png)
    is the ground truth for the *i*^(th) case, and ![](img/B17574_05_008.png) the
    prediction.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，*n* 代表示例数量，![](img/B17574_05_002.png) 是第 *i* 个案例的真实值，而 ![](img/B17574_05_008.png)
    是预测值。
- en: If a competition uses the log loss, it is implied that the objective is to estimate
    as correctly as possible the probability of an example being of a positive class.
    You can actually find the log loss in quite a lot of competitions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个竞赛使用对数损失，这意味着目标是尽可能准确地估计示例属于正类的概率。你实际上可以在很多竞赛中找到对数损失。
- en: We suggest you have a look, for instance, at the recent *Deepfake Detection
    Challenge* ([https://www.kaggle.com/c/deepfake-detection-challenge](https://www.kaggle.com/c/deepfake-detection-challenge))
    or at the older *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你看看最近的*Deepfake Detection Challenge*（[https://www.kaggle.com/c/deepfake-detection-challenge](https://www.kaggle.com/c/deepfake-detection-challenge)）或较老的*Quora
    Question Pairs*（[https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)）。
- en: 'The **ROC curve**, or **receiver operating characteristic curve**, is a graphical
    chart used to evaluate the performance of a binary classifier and to compare multiple
    classifiers. It is the building block of the ROC-AUC metric, because the metric
    is simply the area delimited under the ROC curve. The ROC curve consists of the
    true positive rate (the recall) plotted against the false positive rate (the ratio
    of negative instances that are incorrectly classified as positive ones). It is
    equivalent to one minus the true negative rate (the ratio of negative examples
    that are correctly classified). Here are a few examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC曲线**，或**受试者工作特征曲线**，是一种用于评估二元分类器性能并比较多个分类器的图表。它是ROC-AUC指标的基础，因为该指标简单地是ROC曲线下方的面积。ROC曲线由真正例率（召回率）与假正例率（错误地将负例分类为正例的负例比例）的对比组成。它等同于1减去真正例率（正确分类的负例比例）。以下是一些示例：'
- en: '![](img/B17574_05_03.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_03.png)'
- en: 'Figure 5.3: Different ROC curves and their AUCs'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：不同的ROC曲线及其AUC值
- en: Ideally, a ROC curve of a well-performing classifier should quickly climb up
    the true positive rate (recall) at low values of the false positive rate. A ROC-AUC
    between 0.9 to 1.0 is considered very good.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，表现良好的分类器的ROC曲线应该在低假阳性率的情况下迅速上升真阳性率（召回率）。ROC-AUC在0.9到1.0之间被认为是非常好的。
- en: A bad classifier can be spotted by the ROC curve appearing very similar, if
    not identical, to the diagonal of the chart, which represents the performance
    of a purely random classifier, as in the top left of the figure above; ROC-AUC
    scores near 0.5 are considered to be almost random results. If you are comparing
    different classifiers, and you are using the **area under the curve** (**AUC**),
    the classifier with the higher area is the more performant one.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不好的分类器可以通过ROC曲线看起来非常相似，如果不是完全相同，与图表的对角线相似来识别，这代表了一个纯粹随机分类器的性能，如图中左上角所示；ROC-AUC分数接近0.5被认为是几乎随机的结果。如果您正在比较不同的分类器，并且您正在使用**曲线下面积**（**AUC**），则面积更大的分类器性能更好。
- en: If the classes are balanced, or not too imbalanced, increases in the AUC are
    proportional to the effectiveness of the trained model and they can be intuitively
    thought of as the ability of the model to output higher probabilities for true
    positives. We also think of it as the ability to order the examples more properly
    from positive to negative. However, when the positive class is rare, the AUC starts
    high and its increments may mean very little in terms of predicting the rare class
    better. As we mentioned before, in such a case, average precision is a more helpful
    metric.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别是平衡的，或者不平衡程度不是太高，AUC 的增加与训练模型的效率成正比，并且可以直观地认为这是模型输出更高概率的正例的能力。我们也可以将其视为从正例到负例更正确地排序示例的能力。然而，当正例类别很少时，AUC
    的起始值很高，其增量在预测稀有类别方面可能意义不大。正如我们之前提到的，在这种情况下，平均精度是一个更有帮助的指标。
- en: 'AUC has recently been used for quite a lot of different competitions. We suggest
    you have a look at these three:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，AUC 已经被用于许多不同的竞赛。我们建议您查看以下三个：
- en: '*IEEE-CIS Fraud Detection*: [https://www.kaggle.com/c/ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IEEE-CIS 欺诈检测*：[https://www.kaggle.com/c/ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection)'
- en: '*Riiid Answer Correctness Prediction*: [https://www.kaggle.com/c/riiid-test-answer-prediction](https://www.kaggle.com/c/riiid-test-answer-prediction)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Riiid 回答正确性预测*：[https://www.kaggle.com/c/riiid-test-answer-prediction](https://www.kaggle.com/c/riiid-test-answer-prediction)'
- en: '*Jigsaw Multilingual Toxic Comment Classification*: [https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jigsaw 多语言有毒评论分类*：[https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/)'
- en: 'You can read a detailed treatise in the following paper: Su, W., Yuan, Y.,
    and Zhu, M. *A relationship between the average precision and the area under the
    ROC curve.* Proceedings of the 2015 International Conference on The Theory of
    Information Retrieval. 2015.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下论文中阅读详细的论述：Su, W., Yuan, Y., and Zhu, M. *平均精度与ROC曲线下面积之间的关系.* 2015年国际信息检索理论会议论文集。2015。
- en: Matthews correlation coefficient (MCC)
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马修斯相关系数（MCC）
- en: We complete our overview of binary classification metrics with the **Matthews
    correlation coefficient** (**MCC**), which made its appearance in *VSB Power Line
    Fault Detection* ([https://www.kaggle.com/c/vsb-power-line-fault-detection](https://www.kaggle.com/c/vsb-power-line-fault-detection))
    and *Bosch Production Line Performance* ([https://www.kaggle.com/c/bosch-production-line-performance](https://www.kaggle.com/c/bosch-production-line-performance)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过介绍**马修斯相关系数**（**MCC**）来完成对二元分类指标的概述，它在*VSB 电力线故障检测* ([https://www.kaggle.com/c/vsb-power-line-fault-detection](https://www.kaggle.com/c/vsb-power-line-fault-detection))
    和 *Bosch 生产线性能* ([https://www.kaggle.com/c/bosch-production-line-performance](https://www.kaggle.com/c/bosch-production-line-performance))
    中首次出现。
- en: 'The formula for the MCC is:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 的公式是：
- en: '![](img/B17574_05_024.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_024.png)'
- en: In the above formula, *TP* stands for true positives, *TN* for true negatives,
    *FP* for false positives, and *FN* for false negatives. It is the same nomenclature
    as we met when discussing precision and recall.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，*TP* 代表真阳性，*TN* 代表真阴性，*FP* 代表假阳性，*FN* 代表假阴性。这与我们在讨论精确率和召回率时遇到的命名法相同。
- en: Behaving as a correlation coefficient, in other words, ranging from +1 (perfect
    prediction) to -1 (inverse prediction), this metric can be considered a measure
    of the quality of the classification even when the classes are quite imbalanced.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 作为相关系数的行为，换句话说，从+1（完美预测）到-1（反向预测），这个指标可以被认为是分类质量的一个度量，即使类别相当不平衡。
- en: 'In spite of its complexity, the formula can be reformulated and simplified,
    as demonstrated by Neuron Engineer ([https://www.kaggle.com/ratthachat](https://www.kaggle.com/ratthachat))
    in his Notebook: [www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc](https://www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管公式复杂，但它可以被重新表述和简化，正如神经元工程师([https://www.kaggle.com/ratthachat](https://www.kaggle.com/ratthachat))在他的笔记本中所展示的：[www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc](https://www.kaggle.com/ratthachat/demythifying-matthew-correlation-coefficients-mcc)。
- en: 'The work done by Neuron Engineer in understanding the ratio of the evaluation
    metric is indeed exemplary. In fact, his reformulated MCC becomes:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元工程师在理解评估指标比率方面所做的工作确实值得称赞。事实上，他重新表述的MCC变为：
- en: '![](img/B17574_05_025.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_025.png)'
- en: 'Where each element of the formula is:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 公式中的每个元素是：
- en: '![](img/B17574_05_026.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_026.png)'
- en: '![](img/B17574_05_027.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_027.png)'
- en: '![](img/B17574_05_028.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_028.png)'
- en: '![](img/B17574_05_029.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_029.png)'
- en: '![](img/B17574_05_030.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_030.png)'
- en: 'The reformulation helps to clarify, in a more intelligible form than the original,
    that you can get higher performance from improving both positive and negative
    class precision, but that’s not enough: you also have to have positive and negative
    predictions in proportion to the ground truth, or your submission will be greatly
    penalized.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 重新表述有助于更清晰地阐明，通过提高正负类别的精确度，你可以获得更高的性能，但这还不够：你还需要有与真实标签成比例的正负预测，否则你的提交将受到严重惩罚。
- en: Metrics for multi-class classification
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类别分类的指标
- en: When moving to multi-class classification, you simply use the binary classification
    metrics that we have just seen, applied to each class, and then you summarize
    them using some of the averaging strategies that are commonly used for multi-class
    situations.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当转向多类别分类时，你只需将我们刚刚看到的二进制分类指标应用于每个类别，然后使用一些常用的多类别情况下的平均策略来总结它们。
- en: 'For instance, if you want to evaluate your solution based on the *F*1 score,
    you have three possible averaging choices:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想根据 *F*1 分数评估你的解决方案，你有三种可能的平均选择：
- en: '**Macro averaging**: Simply calculate the *F*1 score for each class and then
    average all the results. In this way, each class will count as much the others,
    no matter how frequent its positive cases are or how important they are for your
    problem, resulting therefore in equal penalizations when the model doesn’t perform
    well with any class:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宏平均**：简单地对每个类别的 *F*1 分数进行计算，然后对所有结果进行平均。这样，每个类别的权重与其他类别相同，无论其正例出现的频率如何，或者它们对于你的问题的重要性如何，因此当模型在任一类别上表现不佳时，将产生相等的惩罚：'
- en: '![](img/B17574_05_031.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_031.png)'
- en: '**Micro averaging**: This approach will sum all the contributions from each
    class to compute an aggregated *F*1 score. It results in no particular favor to
    or penalization of any class, since all the computations are made regardless of
    each class, so it can more accurately account for class imbalances:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微平均**：这种方法将每个类别的所有贡献相加来计算一个综合的 *F*1 分数。由于所有计算都是不考虑每个类别的，因此它不会特别偏向或惩罚任何类别，因此它可以更准确地反映类别不平衡：'
- en: '![](img/B17574_05_032.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_032.png)'
- en: '**Weighting**: As with macro averaging, you first calculate the *F*1 score
    for each class, but then you make a weighted average mean of all of them using
    a weight that depends on the number of true labels of each class. By using such
    a set of weights, you can take into account the frequency of positive cases from
    each class or the relevance of that class for your problem. This approach clearly
    favors the majority classes, which will be weighted more in the computations:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权**：与宏平均一样，你首先计算每个类别的 *F*1 分数，但然后使用一个依赖于每个类别的真实标签数量的权重，对所有这些分数进行加权平均。通过使用这样一套权重，你可以考虑每个类别的正例频率或该类别对于你的问题的相关性。这种方法明显偏向多数类别，在计算中将给予更多的权重：'
- en: '![](img/B17574_05_033.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_033.png)'
- en: '![](img/B17574_05_034.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_034.png)'
- en: 'Common multi-class metrics that you may encounter in Kaggle competitions are:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在Kaggle竞赛中遇到的常见多类度量包括：
- en: '**Multiclass accuracy (weighted)**: *Bengali.AI Handwritten Grapheme Classification*
    ([https://www.kaggle.com/c/bengaliai-cv19](https://www.kaggle.com/c/bengaliai-cv19))'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类准确率（加权）**：*孟加拉文手写图形分类* ([https://www.kaggle.com/c/bengaliai-cv19](https://www.kaggle.com/c/bengaliai-cv19))'
- en: '**Multiclass log loss (MeanColumnwiseLogLoss)**: *Mechanisms of Action (MoA)*
    *Prediction* ([https://www.kaggle.com/c/lish-moa/](https://www.kaggle.com/c/lish-moa/))'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类对数损失（平均列对数损失）**：*作用机制（MoA）预测* ([https://www.kaggle.com/c/lish-moa/](https://www.kaggle.com/c/lish-moa/))'
- en: '**Macro-F1** and **Micro-F1 (NQMicroF1)**: *University of Liverpool - Ion Switching*
    ([https://www.kaggle.com/c/liverpool-ion-switching](https://www.kaggle.com/c/liverpool-ion-switching)),
    *Human Protein Atlas Image Classification* ([https://www.kaggle.com/c/human-protein-atlas-image-classification/](https://www.kaggle.com/c/human-protein-atlas-image-classification/)),
    *TensorFlow* *2.0 Question* *Answering* ([https://www.kaggle.com/c/tensorflow2-question-answering](https://www.kaggle.com/c/tensorflow2-question-answering))'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宏-F1**和**微-F1（NQMicroF1）**：*利物浦大学 - 离子开关* ([https://www.kaggle.com/c/liverpool-ion-switching](https://www.kaggle.com/c/liverpool-ion-switching))，*人类蛋白质图谱图像分类*
    ([https://www.kaggle.com/c/human-protein-atlas-image-classification/](https://www.kaggle.com/c/human-protein-atlas-image-classification/))，*TensorFlow*
    *2.0问答* ([https://www.kaggle.com/c/tensorflow2-question-answering](https://www.kaggle.com/c/tensorflow2-question-answering))'
- en: '**Mean-F1**: *Shopee - Price Match Guarantee* ([https://www.kaggle.com/c/shopee-product-matching/](https://www.kaggle.com/c/shopee-product-matching/)).
    Here, the *F*1 score is calculated for every predicted row, then averaged, whereas
    the Macro-F1 score is defined as the mean of class-wise/label-wise *F*1 scores.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均-F1**：*Shopee - 价格匹配保证* ([https://www.kaggle.com/c/shopee-product-matching/](https://www.kaggle.com/c/shopee-product-matching/))。在这里，*F*1得分是对每一预测行计算的，然后取平均值，而宏-F1得分定义为类/标签*F*1得分的平均值。'
- en: Then there is also **Quadratic Weighted Kappa**, which we will explore later
    on as a smart evaluation metric for ordinal prediction problems. In its simplest
    form, the **Cohen Kappa** score, it just measures the agreement between your predictions
    and the ground truth. The metric was actually created for measuring **inter-annotation
    agreement**, but it is really versatile and has found even better uses.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 然后还有**二次加权Kappa**，我们稍后会探讨它作为序数预测问题的智能评估指标。在其最简单形式中，**Cohen Kappa**得分只是衡量你的预测与真实值之间的一致性。该度量实际上是为了测量**互标注一致性**而创建的，但它非常灵活，并且已经找到了更好的用途。
- en: 'What is inter-annotation agreement? Let’s imagine that you have a labeling
    task: classifying some photos based on whether they contain an image of a cat,
    a dog, or neither. If you ask a set of people to do the task for you, you may
    incur some erroneous labels because someone (called the *judge* in this kind of
    task) may misinterpret a dog as a cat or vice versa. The smart way to do this
    job correctly is to divide the work among multiple judges labeling the same photos,
    and then measure their level of agreement based on the Cohen Kappa score.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 互标注一致性是什么？让我们想象一下，你有一个标注任务：根据照片中是否包含猫、狗或两者都不是的图像来对照片进行分类。如果你要求一组人帮你完成任务，你可能会因为有人（在这种任务中被称为*评委*）可能将狗误认为是猫或反之，而得到一些错误的标签。正确完成这项工作的聪明方法是让多个评委对同一照片进行标注，然后根据Cohen
    Kappa得分衡量他们的协议水平。
- en: 'Therefore, the Cohen Kappa is devised as a score expressing the level of agreement
    between two annotators on a labeling (classification) problem:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Cohen Kappa被设计为一个表示两个标注者在标注（分类）问题上一致程度的得分：
- en: '![](img/B17574_05_035.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_035.png)'
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Using the confusion
    matrix nomenclature, this can be rewritten as:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*p*[0]是评分者之间观察到的相对一致性，而*p*[e]是偶然一致性的假设概率。使用混淆矩阵的命名法，这可以重写为：
- en: '![](img/B17574_05_036.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_036.png)'
- en: The interesting aspect of this formula is that the score takes into account
    the empirical probability that the agreement has happened just by chance, so the
    measure has a correction for all the most probable classifications. The metric
    ranges from 1, meaning complete agreement, to -1, meaning the judges completely
    oppose each other (total disagreement).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的有趣之处在于，得分考虑了协议仅仅是通过偶然发生的经验概率，因此该度量对所有最可能的分类都有修正。该度量范围从1，表示完全一致，到-1，表示评委完全对立（完全不一致）。
- en: Values around 0 signify that agreement and disagreement among the judges is
    happening by mere chance. This helps you figure out if the model is really performing
    better than chance in most situations.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 值在0附近的表示评委之间的同意和不同意完全是偶然发生的。这有助于你判断模型是否在大多数情况下真的比随机情况表现更好。
- en: '![](img/Andrey_Lukyanenko_-_Copy.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Andrey_Lukyanenko_-_Copy.png)'
- en: Andrey Lukyanenko
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 安德烈·卢克扬诺夫
- en: '[https://www.kaggle.com/artgor](https://www.kaggle.com/artgor)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/artgor](https://www.kaggle.com/artgor)'
- en: Our second interview of the chapter is with Andrey Lukyanenko, a Notebooks and
    Discussions Grandmaster and Competitions Master. In his day job, he is a Machine
    Learning Engineer and TechLead at MTS Group. He had many interesting things to
    say about his Kaggle experiences!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二次采访是与安德烈·卢克扬诺夫，笔记本和讨论大师以及比赛大师。在他的日常工作岗位上，他是MTS集团的机器学习工程师和技术负责人。他对自己的Kaggle经历有很多有趣的事情要说！
- en: What’s your favourite kind of competition and why? In terms of techniques, solving
    approaches, what is your specialty on Kaggle?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在技术、解决方法方面，你在Kaggle上的专长是什么？
- en: '*I prefer competitions where solutions can be general enough to be transferable
    to other datasets/domains. I’m interested in trying various neural net architectures,
    state-of-the-art approaches, and post-processing tricks. I don’t favor those competitions
    that require reverse engineering or creating some “golden features,” as these
    approaches won’t be applicable in other datasets.*'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '*我更喜欢那些解决方案足够通用，可以转移到其他数据集/领域的比赛。我对尝试各种神经网络架构、最先进的方法和后处理技巧感兴趣。我不喜欢那些需要逆向工程或创建某些“黄金特征”的比赛，因为这些方法在其他数据集中不适用。*'
- en: While you were competing on Kaggle, you also became a Grandmaster in Notebooks
    (and ranked number one) and Discussions. Have you invested in these two objectives?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在Kaggle上竞争时，你也成为了笔记本（排名第一）和讨论的大师。你在这两个目标上投入了吗？
- en: '*I have invested a lot of time and effort into writing Notebooks, but the Discussion
    Grandmaster rank happened kind of on its own.*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在编写笔记本上投入了大量的时间和精力，但讨论大师排名似乎是自然而然发生的。*'
- en: '*Let’s start with the Notebook ranking.*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们从笔记本排名开始。*'
- en: '*There was a special competition in 2018 called DonorsChoose.org Application
    Screening. DonorsChoose is a fund that empowers public school teachers from across
    the country to request much-needed materials and experiences for their students.
    It organized a competition, where the winning solutions were based not on the
    score on the leaderboard, but on the number of the upvotes on the Notebook. This
    looked interesting and I wrote a Notebook for the competition. Many participants
    advertised their analysis on social media and I did the same. As a result, I reached
    second place and won a Pixelbook (I’m still using it!).*'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '*2018年有一个特别的比赛叫做DonorsChoose.org应用筛选。DonorsChoose是一个基金，它赋予全国各地的公立学校教师请求他们学生所需的大量材料和体验的能力。它组织了一场比赛，获胜的解决方案不是基于排行榜上的分数，而是基于笔记本的点赞数。这看起来很有趣，我为比赛写了一个笔记本。许多参与者都在社交媒体上宣传他们的分析，我也是这样做的。结果，我获得了第二名，赢得了一台Pixelbook（我还在使用它！）*'
- en: '*I was very motivated by this success and continued writing Notebooks. At first,
    I simply wanted to share my analysis and get feedback, because I wanted to try
    to compare my analytics and visualization skills with other people to see what
    I could do and what people thought of it. People started liking my kernels and
    I wanted to improve my skills even further. Another motivation was a desire to
    improve my skill at making a quick MVP (minimum viable product). When a new competition
    starts, many people begin writing Notebooks, and if you want to be one of the
    first, you have to be able to do it fast without sacrificing quality. This is
    challenging, but fun and rewarding.*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*这次成功让我非常兴奋，我继续写笔记本。起初，我只是想分享我的分析并得到反馈，因为我想要尝试比较我的分析和可视化技能与其他人，看看我能做什么，人们对此有何看法。人们开始喜欢我的内核，我想进一步提高我的技能。另一个动力是提高制作快速MVP（最小可行产品）的技能。当一个新的比赛开始时，许多人开始写笔记本，如果你想成为其中之一，你必须能够快速完成而不牺牲质量。这很有挑战性，但很有趣，也很值得。*'
- en: '*I was able to get the Notebook Grandmaster rank in the February of 2019; after
    some time, I reached first place and held it for more than a year. Now I write
    Notebooks less frequently, but I still enjoy doing it.*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*我能在2019年2月获得笔记本大师排名；过了一段时间，我达到了第一名，并保持了超过一年的记录。现在我不太频繁地写笔记本，但我仍然喜欢这样做。*'
- en: '*As for discussions, I think it kind of happened on its own. I answered the
    comments on my Notebooks, and shared and discussed ideas about competitions in
    which I took part, and my discussion ranking steadily increased.*'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*至于讨论，我认为它似乎自然而然地发生了。我回答了Notebooks上的评论，并分享和讨论了我参与的比赛中的一些想法，我的讨论排名稳步上升。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请告诉我们您参加的一个特别具有挑战性的比赛，以及您使用了哪些见解来应对这项任务。
- en: '*It was the* Predicting Molecular Properties *competition. I have written a
    blog post about it in more detail here (*[https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1](https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1)*).
    It was a domain-specific competition aimed at predicting interactions between
    atoms in molecules. Nuclear Magnetic Resonance (NMR) is a technology that uses
    principles similar to MRI to understand the structure and dynamics of proteins
    and molecules. Researchers around the world conduct NMR experiments to further
    understand the structure and dynamics of molecules, across areas like environmental
    science, pharmaceutical science, and materials science. In this competition, we
    tried to predict the magnetic interaction between two atoms in a molecule (the
    scalar coupling constant). State-of-the-art methods from quantum mechanics can
    calculate these coupling constants given only a 3D molecular structure as input.
    But these calculations are very resource-intensive, so can’t be always used. If
    machine learning approaches could predict these values, it would really help medicinal
    chemists to gain structural insights faster and more cheaply.*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一场*预测分子性质*的比赛。我在这里详细写了一篇博客文章（[https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1](https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-competition-things-done-and-lessons-learned-c269d9c233d1)）。这是一场针对预测分子中原子之间相互作用的特定领域比赛。核磁共振（NMR）是一种使用与MRI类似原理的技术，用于理解蛋白质和分子的结构和动态。全球的研究人员通过进行NMR实验来进一步了解分子的结构和动态，涉及环境科学、药理学和材料科学等领域。在这场比赛中，我们试图预测分子中两个原子之间的磁相互作用（标量耦合常数）。量子力学中最先进的方法可以在仅提供3D分子结构作为输入的情况下计算出这些耦合常数。但这些计算非常资源密集，因此不能总是使用。如果机器学习方法能够预测这些值，这将真正帮助药物化学家更快、更便宜地获得结构洞察。*'
- en: '*I usually write EDA kernels for new Kaggle competitions, and this one was
    no exception. A common approach for tabular data in Kaggle competitions is extensive
    feature engineering and using gradient boosting models. I used LGBM too in my
    early attempts, but knew that there should be better ways to work with graphs.
    I realized that domain expertise would provide a serious advantage, so I hunted
    for every piece of such information. Of course, I noticed that there were several
    active experts, who wrote on the forum and created kernels, so I read everything
    from them. And one day I received an e-mail from an expert in this domain who
    thought that our skills could complement each other. Usually, I prefer to work
    on competitions by myself for some time, but in this case, combining forces seemed
    to be a good idea to me. And this decision turned out to be a great one! With
    time we were able to gather an amazing team.*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*我通常为新的Kaggle比赛编写EDA内核，这次也不例外。在Kaggle比赛中，表格数据的常见方法是进行广泛的特征工程和使用梯度提升模型。我在早期的尝试中也使用了LGBM，但我知道应该有更好的方法来处理图。我意识到领域专业知识将提供重大优势，所以我寻找了所有这样的信息。当然，我也注意到了几位活跃的专家，他们在论坛上撰写文章并创建了内核，所以我阅读了他们的一切。有一天，我收到了一位该领域专家的电子邮件，他认为我们的技能可以互补。通常，我更喜欢自己独立工作一段时间，但在这个情况下，联合力量似乎是一个好主意。而且这个决定证明是一个非常好的决定！随着时间的推移，我们能够聚集一个惊人的团队。*'
- en: '*After some time, we noticed a potential for neural nets in the competition:
    a well-known Kaggler, Heng, posted an example of an MPNN (Message Passing Neural
    Network) model. After some time, I was even able to run it, but the results were
    worse compared to our models. Nevertheless, our team knew that we would need to
    work with these Neural Nets if we wanted to aim high. It was amazing to see how
    Christof was able to build new neural nets extremely fast. Soon, we focused only
    on developing those models.*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*经过一段时间，我们注意到在比赛中神经网络有潜力：一位知名的Kaggler，Heng，发布了一个MPNN（消息传递神经网络）模型的示例。过了一段时间，我甚至能够运行它，但结果比我们的模型要差。尽管如此，我们的团队知道，如果我们想取得好成绩，我们就需要与这些神经网络合作。看到Christof能够极快地构建新的神经网络，真是令人惊叹。很快，我们就只专注于开发这些模型。*'
- en: '*After that, my role switched to a support one. I did a lot of experiments
    with our neural nets: trying various hyperparameters, different architectures,
    various little tweaks to training schedules, and so on. Sometimes I did EDA on
    our predictions to find our interesting or wrong cases, and later we used this
    information to improve our models even further.*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*在那之后，我的角色转变为支持性角色。我对我们的神经网络进行了大量实验：尝试各种超参数、不同的架构、训练计划的各种小调整等等。有时我对我们的预测进行EDA，以找到有趣或错误的案例，后来我们使用这些信息进一步改进了我们的模型。*'
- en: '*We got the 8*^(th) *place and I learned a lot during this competition.*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们获得了第8名，我在这次比赛中学到了很多。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进展？如果是的话，是如何帮助的？
- en: '*Kaggle definitely helped me a lot, especially with my skills and my personal
    brand. Writing and publishing Kaggle Notebooks taught me not only EDA and ML skills,
    but it forced me to become adaptable, to be able to understand new topics and
    tasks quickly, to iterate more efficiently between approaches. At the same time,
    it provided a measure of visibility for me, because people appreciated my work.*'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle确实在很大程度上帮助了我，特别是在我的技能和个人品牌方面。撰写和发布Kaggle笔记本不仅教会了我EDA和ML技能，而且迫使我变得适应性强，能够快速理解新的主题和任务，在方法之间更有效地迭代。同时，它也为我提供了一定程度的可见性，因为人们欣赏我的工作。*'
- en: '*My first portfolio (*[https://erlemar.github.io/](https://erlemar.github.io/)*)
    had a lot of different Notebooks, and half of them were based on old Kaggle competitions.
    It was definitely helpful in getting my first jobs. My Kaggle achievements also
    helped me attract recruiters from good companies, sometimes even to skip steps
    of the interview process, and even led me to several consulting gigs.*'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的第一个作品集（[https://erlemar.github.io/](https://erlemar.github.io/)）有很多不同的笔记本，其中一半是基于旧Kaggle比赛的。这无疑有助于我获得第一份工作。我的Kaggle成就也帮助我吸引了来自好公司的招聘人员，有时甚至可以跳过面试流程的某些步骤，甚至让我获得了几个咨询项目。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，缺乏经验的Kaggler通常忽略了什么？你现在知道什么，而当你刚开始时希望知道的呢？
- en: '*I think we need to separate inexperienced Kagglers into two groups: those
    who are inexperienced in data science in general and those who are inexperienced
    on Kaggle.*'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为我们需要将缺乏经验的Kaggler分为两组：那些在数据科学方面缺乏经验的人和那些在Kaggle上缺乏经验的人。*'
- en: '*Those who are inexperienced in general make a number of different mistakes
    (and it is okay, everyone started somewhere):*'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*那些在一般情况下缺乏经验的人会犯许多不同的错误（这是可以理解的，每个人都是从某个地方开始的）：*'
- en: '*One of the most serious problems: lack of critical thinking and not knowing
    how to do their own research;*'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最严重的问题之一：缺乏批判性思维和不知道如何进行自己的研究；*'
- en: '*Not knowing when and what tools/approaches to use;*'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不知道何时以及使用哪些工具/方法；*'
- en: '*Blindly taking public Notebooks and using them without understanding how they
    work;*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*盲目地使用公开的笔记本，而不知道它们是如何工作的；*'
- en: '*Fixating on a certain idea and spending too much time pursuing it, even when
    it doesn’t work;*'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*专注于某个想法并花费太多时间追求它，即使它不起作用；*'
- en: '*Despairing and losing motivation when their experiments fail.*'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当他们的实验失败时感到绝望和失去动力。*'
- en: '*As for those people who have experience in data science but don’t have experience
    with Kaggle, I’d say that the most serious thing they overlook is that they underestimate
    Kaggle’s difficulty. They don’t expect Kaggle to be very competitive, that you
    need to try many different things to succeed, that there are a lot of tricks that
    work only in competitions, that there are people who professionally participate
    in competitions.*'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*至于那些在数据科学方面有经验但没有Kaggle经验的人，我认为他们最严重的问题是低估了Kaggle的难度。他们没有预料到Kaggle竞争激烈，需要尝试许多不同的事情才能成功，有很多只在比赛中有效的技巧，还有专业参加比赛的人。*'
- en: '*Also, people often overestimate domain expertise. I admit that there were
    a number of competitions when the teams with domain experts in them won gold medals
    and prizes, but in most cases experienced Kagglers triumph.*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，人们往往高估了领域专业知识。我承认，有一些比赛是拥有领域专家的团队赢得了金牌和奖项，但在大多数情况下，经验丰富的Kagglers取得了胜利。*'
- en: '*Also, I have seen the following situation many times: some person proclaims
    that winning Kaggle is easy, and that he (or his group of people) will get a gold
    medal or many gold medals in the recent future. In most cases, they silently fail.*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，我多次见过以下情况：有些人宣称赢得Kaggle很容易，并且他（或他的团队）将在不久的将来获得金牌或许多金牌。在大多数情况下，他们默默失败。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*Not* *enough looking in the data. Sometimes I wasn’t able to generate better
    features or apply better postprocessing due to this. And reserve engineering and
    “golden features” is a whole additional topic.*'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数据的观察不足。有时我无法生成更好的特征或应用更好的后处理，就是由于这一点。而且逆向工程和“黄金特征”是一个完全额外的主题。*'
- en: '*Spending too much time on a single idea because I hoped it would work. This
    is called sunk-cost fallacy.*'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*因为希望它能够成功，所以在单一想法上花费太多时间。这被称为沉没成本谬误。*'
- en: '*Not enough experiments. The effort pays off – if you don’t spend enough time
    and resources on the competition, you won’t get a high place on a leaderboard.*'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实验不足。努力会得到回报——如果你不花足够的时间和资源在比赛中，你不会在排行榜上获得高分。*'
- en: '*Entering “wrong” competitions. There were competitions with leaks, reverse
    engineering, etc. There were competitions with an unreasonable split between public
    and private test data and a shake-up ensured. There were competitions that weren’t
    interesting enough for me and I shouldn’t have started participating in them.*'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参加“错误”的比赛。有些比赛存在泄露、逆向工程等问题。有些比赛的公共和私有测试数据分配不合理，导致动荡。有些比赛对我来说不够有趣，我不应该开始参加。*'
- en: '*Teaming up with the wrong people. There were cases when my teammates weren’t
    as active as I expected a**nd it led to a worse team score.*'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与错误的人组队。有些情况下，我的队友没有像我预期的那样活跃，这导致了更差的团队得分。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住什么最重要的事情或做什么？
- en: '*I think it is important to remember your goal, know what are you ready to
    invest into this competition, and think about the possible outcomes. There are
    many possible goals that people have while entering a competition:*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为重要的是记住你的目标，了解你愿意为这次比赛投入什么，并考虑可能的结果。人们在参加比赛时有许多可能的目标：*'
- en: '*Win**ning money or getting a medal;*'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*赢得金钱或获得奖牌；*'
- en: '*Getting new skills or improving existing ones;*'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取新技能或提高现有技能；*'
- en: '*Working with a new task/domain;*'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理新的任务/领域；*'
- en: '*Networking;*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*建立人脉；*'
- en: '*PR;*'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共关系；*'
- en: '*etc;*'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*等等；*'
- en: '*Of course, it is possible to have multiple motivations.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然，有多种动机是可能的。*'
- en: '*As for what are you ready to invest, it is usually about the amount of time
    and effort you are ready to spend as well as the hardware that you have.*'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*至于你准备投入什么，通常是指你愿意投入的时间和精力，以及你拥有的硬件。*'
- en: '*When I speak about the outcomes, I mean what will happen when the competition
    ends. It is possible that you will invest a lot in this competition and win, but
    you could also lose. Are you ready for this reality? Is winning a particular competition
    critical to you? Maybe you need to be prepared to invest more effort; on the other
    hand, maybe you have long-term goals and one failed competition won’t hurt much.*'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我提到结果时，我的意思是比赛结束时会发生什么。你可能会在这个比赛中投入很多并赢得胜利，但你也可能失败。你准备好接受这个现实了吗？赢得某个特定的比赛对你来说至关重要吗？也许你需要准备投入更多的努力；另一方面，也许你有长期目标，一次失败的比赛不会造成太大的伤害。*'
- en: Metrics for object detection problems
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测问题的指标
- en: 'In recent years, deep learning competitions have become more and more common
    on Kaggle. Most of these competitions, focused on image recognition or on natural
    language processing tasks, have not required the use of evaluation metrics much
    different from the ones we have explored up to now. However, a couple of specific
    problems have required some special metric to be evaluated correctly: those relating
    to **object detection** and **segmentation**.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在Kaggle上，深度学习竞赛越来越普遍。大多数这些竞赛，专注于图像识别或自然语言处理任务，并没有要求使用与我们迄今为止探索的不同的评估指标。然而，一些特定的问题需要一些特殊的指标来正确评估：与**目标检测**和**分割**相关的问题。
- en: '![](img/B17574_05_04.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_04.png)'
- en: 'Figure 5.4: Computer vision tasks. (Source: https://cocodataset.org/#explore?id=38282,
    https://cocodataset.org/#explore?id=68717)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：计算机视觉任务。（来源：https://cocodataset.org/#explore?id=38282, https://cocodataset.org/#explore?id=68717）
- en: In **object detection**, you don’t have to classify an image, but instead find
    relevant portions of a picture and label them accordingly. For instance, in *Figure
    5.4*, an object detection classifier has been entrusted to locate within a photo
    the portions of the picture where either dogs or cats are present and classify
    each of them with a proper label. The example on the left shows the localization
    of a cat using a rectangular box (called a **bounding box**). The example on the
    right presents how multiple cats and dogs are detected in the picture by bounding
    boxes and then correctly classified (the blue bounding boxes are for dogs, the
    red ones for cats).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在**目标检测**中，你不需要对图像进行分类，而是需要找到图片中的相关部分并相应地进行标记。例如，在图5.4中，一个目标检测分类器被委托在照片中定位到有狗或猫存在的部分，并对每个部分使用适当的标签进行分类。左边的例子展示了使用矩形框（称为**边界框**）定位猫的位置。右边的例子展示了如何通过边界框检测图片中的多只猫和狗，并正确地进行分类（蓝色边界框用于狗，红色边界框用于猫）。
- en: 'In order to describe the spatial location of an object, in object detection
    we use **bounding boxes**, which define a rectangular area in which the object
    lies. A bounding box is usually specified using two (*x*, *y*) coordinates: the
    upper-left and lower-right corners. In terms of a machine learning algorithm,
    finding the coordinates of bounding boxes corresponds to applying a regression
    problem to multiple targets. However, you probably won’t frame the problem from
    scratch but rely on pre-built and often pre-trained models such as Mask R-CNN
    ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)), RetinaNet
    ([https://arxiv.org/abs/2106.05624v1](https://arxiv.org/abs/2106.05624v1)), FPN
    ([https://arxiv.org/abs/1612.03144v2](https://arxiv.org/abs/1612.03144v2)), YOLO
    ([https://arxiv.org/abs/1506.02640v1](https://arxiv.org/abs/1506.02640v1)), Faster
    R-CNN ([https://arxiv.org/abs/1506.01497v1](https://arxiv.org/abs/1506.01497v1)),
    or SDD ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述物体的空间位置，在目标检测中，我们使用**边界框**，它定义了一个矩形区域，其中包含该物体。边界框通常使用两个(*x*, *y*)坐标来指定：左上角和右下角。从机器学习算法的角度来看，找到边界框的坐标相当于将回归问题应用于多个目标。然而，你可能不会从头开始构建问题，而是依赖于预先构建的、通常是预先训练的模型，例如Mask
    R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))、RetinaNet
    ([https://arxiv.org/abs/2106.05624v1](https://arxiv.org/abs/2106.05624v1))、FPN
    ([https://arxiv.org/abs/1612.03144v2](https://arxiv.org/abs/1612.03144v2))、YOLO
    ([https://arxiv.org/abs/1506.02640v1](https://arxiv.org/abs/1506.02640v1))、Faster
    R-CNN ([https://arxiv.org/abs/1506.01497v1](https://arxiv.org/abs/1506.01497v1))或SDD
    ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325))。
- en: 'In **segmentation**, you instead have a classification at the *pixel* level,
    so if you have a 320x200 image, you actually have to make 64,000 pixel classifications.
    Depending on the task, you can have a **semantic segmentation** where you have
    to classify every pixel in a photo, or an **instance segmentation** where you
    only have to classify the pixels representing objects of a certain type of interest
    (for instance, a cat as in our example in *Figure 5.5* below):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分割**中，你实际上是在**像素**级别进行分类，所以如果你有一个320x200的图像，你实际上需要进行64,000个像素分类。根据任务的不同，你可能需要进行**语义分割**，即对照片中的每个像素进行分类，或者进行**实例分割**，你只需要对代表特定类型感兴趣对象的像素进行分类（例如，如图5.5所示，我们例子中的猫）：
- en: '![](img/B17574_05_05.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_05.png)'
- en: 'Figure 5.5: Semantic segmentation and instance segmentation on the same image.
    (Source: https://cocodataset.org/#explore?id=338091)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：同一图像上的语义分割和实例分割。（来源：https://cocodataset.org/#explore?id=338091）
- en: Let’s start with an overview of the specific metrics for these tasks, metrics
    that can work well for both problems, since, in both cases, you are predicting
    entire areas (rectangular ones in object detection, polygonal ones in segmentation)
    of a picture and you have to compare your predictions against a ground truth,
    which is, again, expressed as areas. On the side of segmentation, the easiest
    metric is the **pixel accuracy**, which, as the name suggests, is the accuracy
    on the pixel classification.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这些任务的特定度量指标概述开始，这些指标可以很好地适用于这两个问题，因为在两种情况下，你都在预测整个区域（在目标检测中是矩形区域，在分割中是多边形区域），并且你必须将你的预测与真实情况进行比较，真实情况再次表示为区域。在分割方面，最简单的度量指标是**像素精度**，正如其名所示，是像素分类的准确性。
- en: It is not a great metric because, as happens with accuracy on binary and multi-class
    problems, your score may look great if the relevant pixels do not take up very
    much of the image (you just predict the majority claim, thus you don’t segment).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 它不是一个很好的度量指标，因为，就像在二元和多类问题上的准确性一样，如果你的相关像素在图像中占的面积不大（你只是预测了主要主张，因此你没有分割），你的分数可能看起来很好。
- en: 'Therefore, there are two metrics that are used much more, especially in competitions:
    the **intersection over union** and the **dice coefficient**.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有两种度量指标被使用得更多，尤其是在竞赛中：**交并比**和**dice系数**。
- en: Intersection over union (IoU)
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交并比（IoU）
- en: 'The **intersection over union** (**IoU**) is also known as the **Jaccard index**.
    When used in segmentation problems, using IoU implies that you have two images
    to compare: one is your prediction and the other is the mask revealing the ground
    truth, which is usually a binary matrix where the value 1 stands for the ground
    truth and 0 otherwise. In the case of multiple objects, you have multiple masks,
    each one labeled with the class of the object.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**交并比**（**IoU**）也被称为**Jaccard指数**。在分割问题中使用IoU意味着你有两个图像要比较：一个是你的预测，另一个是揭示真实情况的掩码，这通常是一个二元矩阵，其中值1代表真实情况，否则为0。在多个对象的情况下，你有多个掩码，每个掩码都标记了对象的类别。'
- en: 'When used in object detection problems, you have the boundaries of two rectangular
    areas (those of the prediction and the ground truth), expressed by the coordinates
    of their vertices. For each classified class, you compute the area of overlap
    between your prediction and the ground truth mask, and then you divide this by
    the area of the union between your prediction and the ground truth, a sum that
    takes into account any overlap. In this way, you are proportionally penalized
    both if you predict a larger area than what it should be (the denominator will
    be larger) or a smaller one (the numerator will be smaller):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当用于目标检测问题时，你有两个矩形区域的边界（预测和真实情况的边界），由它们的顶点坐标表示。对于每个分类类别，你计算你的预测和真实情况掩码之间的重叠面积，然后你将这个面积除以你的预测和真实情况之间的并集面积，这个总和考虑了任何重叠。这样，如果你预测的面积大于应有的面积（分母将更大）或更小（分子将更小），你都会按比例受到惩罚：
- en: '![](img/B17574_05_06.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_06.png)'
- en: 'Figure 5.6: Visual representation of the IoU calculation'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：IoU计算的视觉表示
- en: In *Figure 5.6* you can see a visual representation of the areas involved in
    the computation. By imagining the squares overlapping more, you can figure out
    how the metric efficiently penalizes your solution when your prediction, even
    if covering the ground truth, exceeds it (the area of union becomes larger).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.6*中，你可以看到涉及计算的区域视觉表示。通过想象方块重叠得更多，你可以弄清楚当你的预测，即使覆盖了真实情况，也超过了它（并集的面积变得更大）时，度量如何有效地惩罚你的解决方案。
- en: 'Here are some examples of competitions where IoU has been used:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些使用IoU的竞赛示例：
- en: '*TGS Salt Identification Challenge* ([https://www.kaggle.com/c/tgs-salt-identification-challenge/](https://www.kaggle.com/c/tgs-salt-identification-challenge/))
    with Intersection Over Union Object Segmentation'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TGS盐识别挑战* ([https://www.kaggle.com/c/tgs-salt-identification-challenge/](https://www.kaggle.com/c/tgs-salt-identification-challenge/))
    使用交并比对象分割'
- en: '*iMaterialist (Fashion) 2019 at FGVC6* ([https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6))
    with Intersection Over Union Object Segmentation With Classification'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*iMaterialist (时尚) 2019 at FGVC6* ([https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6))
    使用交并比对象分割及分类'
- en: '*Airbus Ship Detection Challenge* ([https://www.kaggle.com/c/airbus-ship-detection](https://www.kaggle.com/c/airbus-ship-detection))
    with Intersection Over Union Object Segmentation Beta'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*空中客车船舶检测挑战赛* ([https://www.kaggle.com/c/airbus-ship-detection](https://www.kaggle.com/c/airbus-ship-detection))
    使用交集与并集对象分割Beta版'
- en: Dice
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dice
- en: 'The other useful metric is the **Dice coefficient**, which is the area of overlap
    between the prediction and ground truth doubled and then divided by the sum of
    the prediction and ground truth areas:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的度量标准是**Dice系数**，它是预测和真实值重叠区域的面积加倍，然后除以预测和真实值区域的总和：
- en: '![](img/B17574_05_07.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_05_07.png)'
- en: 'Figure 5.7: Visual representation of the Dice calculation'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：Dice计算的视觉表示
- en: In this case, with respect to the Jaccard index, you do not take into account
    the overlap of the prediction with the ground truth in the denominator. Here,
    the expectation is that, as you maximize the area of overlap, you predict the
    correct area size. Again, you are penalized if you predict areas larger than you
    should be predicting. In fact, the two metrics are positively correlated and they
    produce almost the same results for a single classification problem.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，与Jaccard指数相比，你不会在分母中考虑预测与真实值之间的重叠。这里，期望的是，随着你最大化重叠区域，你预测正确的区域大小。再次强调，如果你预测的区域大于你应该预测的区域，你会受到惩罚。事实上，这两个度量标准是正相关，对于单个分类问题，它们产生几乎相同的结果。
- en: The differences actually arise when you are working with multiple classes. In
    fact, both with IoU and the Dice coefficient, when you have multiple classes you
    average the result of all of them. However, in doing so, the IoU metric tends
    to penalize the overall average more if a single class prediction is wrong, whereas
    the Dice coefficient is more lenient and tends to represent the average performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，差异出现在你处理多个类别时。实际上，无论是使用IoU还是Dice系数，当你有多个类别时，你会平均所有类别的结果。然而，在这样做的时候，IoU度量标准往往会因为单个类别的预测错误而更多地惩罚整体平均值，而Dice系数则更为宽容，更倾向于表示平均性能。
- en: 'Examples of Kaggle competitions using the Dice coefficient (it is often encountered
    in competitions with medical purposes, but not necessarily only there, because
    it can also be used for clouds and cars):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Dice系数的Kaggle竞赛示例（它常在具有医疗目的的竞赛中出现，但不仅限于此，因为它也可以用于云层和汽车）：
- en: '*HuBMAP - Hacking the Kidney*: [https://www.kaggle.com/c/hubmap-kidney-segmentation](https://www.kaggle.com/c/hubmap-kidney-segmentation)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HuBMAP - 激活肾脏*: [https://www.kaggle.com/c/hubmap-kidney-segmentation](https://www.kaggle.com/c/hubmap-kidney-segmentation)'
- en: '*Ultrasound Nerve Segmentation*: [https://www.kaggle.com/c/ultrasound-nerve-segmentation](https://www.kaggle.com/c/ultrasound-nerve-segmentation)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超声神经分割*: [https://www.kaggle.com/c/ultrasound-nerve-segmentation](https://www.kaggle.com/c/ultrasound-nerve-segmentation)'
- en: '*Understanding Clouds from Satellite Images*: [https://www.kaggle.com/c/understanding_cloud_organization](https://www.kaggle.com/c/understanding_cloud_organization)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从卫星图像中理解云层*: [https://www.kaggle.com/c/understanding_cloud_organization](https://www.kaggle.com/c/understanding_cloud_organization)'
- en: '*Carvana Image Masking Challenge*: [https://www.kaggle.com/c/carvana-image-masking-challenge](https://www.kaggle.com/c/carvana-image-masking-challenge)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Carvana图像遮罩挑战*: [https://www.kaggle.com/c/carvana-image-masking-challenge](https://www.kaggle.com/c/carvana-image-masking-challenge)'
- en: IoU and Dice constitute the basis for all the more complex metrics in segmentation
    and object detection. By choosing an appropriate threshold level for IoU or Dice
    (usually 0.5), you can decide whether or not to confirm a detection, therefore
    a classification. At this point, you can use previously discussed metrics for
    classification, such as precision, recall, and *F*1, such as is done in popular
    object detection and segmentation challenges such as Pascal VOC ([http://host.robots.ox.ac.uk/pascal/VOC/voc2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012))
    or COCO ([https://cocodataset.org](https://cocodataset.org)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: IoU和Dice构成了分割和目标检测中所有更复杂度量的基础。通过为IoU或Dice选择一个适当的阈值水平（通常为0.5），你可以决定是否确认一个检测，即分类。在这个时候，你可以使用之前讨论的分类度量，如精确度、召回率和*F*1，就像在流行的目标检测和分割挑战赛（如Pascal
    VOC [http://host.robots.ox.ac.uk/pascal/VOC/voc2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012)）或COCO
    [https://cocodataset.org](https://cocodataset.org)）中所做的那样。
- en: Metrics for multi-label classification and recommendation problems
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签分类和推荐问题的度量标准
- en: 'Recommender systems are one of the most popular applications of data analysis
    and machine learning, and there are quite a few competitions on Kaggle that have
    used the recommendation approach. For instance, the *Quick, Draw! Doodle Recognition
    Challenge* was a prediction evaluated as a recommender system. Some other competitions
    on Kaggle, however, truly strived to build effective recommender systems (such
    as *Expedia Hotel Recommendations*: [https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations))
    and RecSYS, the conference on recommender systems ([https://recsys.acm.org/](https://recsys.acm.org/)),
    even hosted one of its yearly contests on Kaggle (*RecSYS 2013*: [https://www.kaggle.com/c/yelp-recsys-2013](https://www.kaggle.com/c/yelp-recsys-2013)).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是数据分析与机器学习最受欢迎的应用之一，在Kaggle上有很多使用推荐方法的竞赛。例如，*Quick, Draw! Doodle Recognition
    Challenge*被评估为推荐系统的一个预测。然而，Kaggle上的一些其他竞赛真正致力于构建有效的推荐系统（例如*Expedia Hotel Recommendations*：[https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations)）和RecSYS，推荐系统会议([https://recsys.acm.org/](https://recsys.acm.org/))，甚至在其年度竞赛中在Kaggle上举办了一次（*RecSYS
    2013*：[https://www.kaggle.com/c/yelp-recsys-2013](https://www.kaggle.com/c/yelp-recsys-2013)）。
- en: '**Mean Average Precision at K** (**MAP@{K}**) is typically the metric of choice
    for evaluating the performance of recommender systems, and it is the most common
    metric you will encounter on Kaggle in all the competitions that try to build
    or approach a problem as a recommender system.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**K值平均平均精度**（**MAP@{K}**）通常是评估推荐系统性能的指标，也是你在Kaggle上遇到的最常见指标，在所有试图将问题作为推荐系统构建或处理的竞赛中。'
- en: There are also some other metrics, such as the **precision at k**, or **P@K**,
    and the **average precision at k**, or **AP@K**, which are loss functions, in
    other words, computed at the level of each single prediction. Understanding how
    they work can help you better understand the MAP@K and how it can perform both
    in recommendations and in multi-label classification.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些其他指标，例如**k值精度**，或**P@K**，以及**k值平均精度**，或**AP@K**，这些都是损失函数，换句话说，是在每个单个预测的层面上计算的。了解它们是如何工作的可以帮助你更好地理解MAP@K以及它在推荐系统和多标签分类中的表现。
- en: 'In fact, analogous to recommender systems, multi-label classifications imply
    that your model outputs a series of class predictions. Such results could be evaluated
    using some average of some binary classification metrics (such as in *Greek Media
    Monitoring Multilabel Classification (WISE 2014)*, which used the mean *F*1 score:
    [https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)) as well
    as metrics that are more typical of recommender systems, such as MAP@K. In the
    end, you can deal with both recommendations and multi-label predictions as *ranking
    tasks*, which translates into a set of ranked suggestions in a recommender system
    and into a set of labels (without a precise order) in multi-label classification.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，与推荐系统类似，多标签分类意味着你的模型输出一系列类别预测。这些结果可以使用一些二元分类指标的均值（例如在*希腊媒体监控多标签分类（WISE 2014）*中，使用了平均*F*1分数：[https://www.kaggle.com/c/wise-2014](https://www.kaggle.com/c/wise-2014)）以及更典型的推荐系统指标进行评估，例如MAP@K。最终，你可以将推荐和多标签预测都视为**排序任务**，这在推荐系统中转化为一系列排序建议，在多标签分类中转化为一系列标签（没有精确的顺序）。
- en: MAP@{K}
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MAP@{K}
- en: MAP@K is a complex metric and it derives from many computations. In order to
    understand the MAP@K metric fully, let’s start with its simplest component, the
    **precision at** **k** (**P@K**). In this case, since the prediction for an example
    is a ranked sequence of predictions (from the most probable to the least), the
    function takes into account only the top *k* predictions, then it computes how
    many matches it got with respect to the ground truth and divides that number by
    *k*. In a few words, it is quite similar to an accuracy measure averaged over
    *k* predictions.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: MAP@K是一个复杂的指标，它源于许多计算。为了完全理解MAP@K指标，让我们从其最简单的组成部分开始，即**k值精度**（**P@K**）。在这种情况下，由于一个示例的预测是一个从最可能到最不可能的排序预测序列，该函数只考虑了前*k*个预测，然后计算它与真实情况的匹配数量，并将该数字除以*k*。简而言之，它与平均精度度量在*k*个预测上的平均相当。
- en: A bit more complex in terms of computation, but conceptually simple, the **average
    precision at** **k** (**AP@K**) is the average of P@K computed over all the values
    ranging from *1* to *k*. In this way, the metric evaluates how well the prediction
    works overall, using the top prediction, then the top two predictions, and so
    on until the top *k* predictions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算上稍微复杂一些，但在概念上很简单，**平均精度** **@** **k** （**AP@K**）是所有从 *1* 到 *k* 的值上计算的 P@K
    的平均值。这样，该指标评估了预测的整体效果，使用最顶部的预测，然后是前两个预测，以此类推，直到前 *k* 个预测。
- en: 'Finally, **MAP@K** is the mean of the AP@K for the entire predicted sample,
    and it is a metric because it comprises all the predictions in its evaluation.
    Here is the MAP@5 formulation you can find in the *Expedia Hotel Recommendations*
    competition ([https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations)):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**MAP@K** 是整个预测样本的 AP@K 的平均值，因为它包含了评估中的所有预测，所以它是一个指标。以下是你在 *Expedia Hotel
    Recommendations* 竞赛（[https://www.kaggle.com/c/expedia-hotel-recommendations](https://www.kaggle.com/c/expedia-hotel-recommendations)）中可以找到的
    MAP@5 公式：
- en: '![](img/B17574_05_037.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_037.png)'
- en: In the formula, ![](img/B17574_05_038.png) is the number of user recommendations,
    *P(k)* is the precision at cutoff *k*, and *n* is the number of predicted hotel
    clusters (you could predict up to 5 hotels for each recommendation).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，![图片](img/B17574_05_038.png) 是用户推荐的数量，*P(k)* 是截止 *k* 的精度，*n* 是预测的酒店集群数量（你可以为每个推荐预测最多
    5 家酒店）。
- en: It is clearly a bit more daunting than our explanation, but the formula just
    expresses that the MAP@K is the mean of all the AP@K evaluations over all the
    predictions.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然比我们的解释要复杂一些，但公式只是表达了 MAP@K 是所有预测的 AP@K 评估的平均值。
- en: Having completed this overview of specific metrics for different regression
    and classification metrics, let’s discuss how to deal with evaluation metrics
    in a Kaggle competition.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了对不同回归和分类指标的特定指标的概述之后，让我们讨论如何在 Kaggle 竞赛中处理评估指标。
- en: Optimizing evaluation metrics
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化评估指标
- en: Summing up what we have discussed so far, an objective function is a function
    inside your learning algorithm that measures how well the algorithm’s internal
    model is fitting the provided data. The objective function also provides feedback
    to the algorithm in order for it to improve its fit across successive iterations.
    Clearly, since the entire algorithm’s efforts are recruited to perform well based
    on the objective function, if the Kaggle evaluation metric perfectly matches the
    objective function of your algorithm, you will get the best results.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们到目前为止所讨论的内容，目标函数是学习算法内部的一个函数，用于衡量算法的内部模型如何拟合提供的数据。目标函数还向算法提供反馈，以便它在后续迭代中改进其拟合。显然，由于整个算法的努力都是为了根据目标函数表现良好，如果
    Kaggle 评估指标完美匹配你的算法的目标函数，你将获得最佳结果。
- en: 'Unfortunately, this is not frequently the case. Often, the evaluation metric
    provided can only be approximated by existing objective functions. Getting a good
    approximation, or striving to get your predictions performing better with respect
    to the evaluation criteria, is the secret to performing well in Kaggle competitions.
    When your objective function does not match your evaluation metric, you have a
    few alternatives:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种情况并不常见。通常，提供的评估指标只能通过现有的目标函数来近似。获得一个好的近似，或者努力使你的预测在评估标准上表现得更好，是 Kaggle
    竞赛中表现良好的秘诀。当你的目标函数与评估指标不匹配时，你有几种替代方案：
- en: Modify your learning algorithm and have it incorporate an objective function
    that matches your evaluation metric, though this is not possible for all algorithms
    (for instance, algorithms such as LightGBM and XGBoost allow you to set custom
    objective functions, but most Scikit-learn models don’t allow this).
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改你的学习算法，使其包含一个与你的评估指标相匹配的目标函数，尽管并非所有算法都允许这样做（例如，LightGBM 和 XGBoost 算法允许你设置自定义目标函数，但大多数
    Scikit-learn 模型不允许这样做）。
- en: Tune your model’s hyperparameters, choosing the ones that make the result shine
    the most when using the evaluation metric.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整你模型的超参数，选择那些在使用评估指标时使结果最耀眼的选择。
- en: Post-process your results so they match the evaluation criteria more closely.
    For instance, you could code an optimizer that performs transformations on your
    predictions (probability calibration algorithms are an example, and we will discuss
    them at the end of the chapter).
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后处理您的结果，使其更接近评估标准。例如，您可以为预测执行转换的优化器编写代码（概率校准算法是一个例子，我们将在本章末尾讨论它们）。
- en: Having the competition metric incorporated into your machine learning algorithm
    is really the most effective method to achieve better predictions, though only
    a few algorithms can be hacked into using the competition metric as your objective
    function. The second approach is therefore the more common one, and many competitions
    end up in a struggle to get the best hyperparameters for your models to perform
    on the evaluation metric.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 将竞赛指标纳入您的机器学习算法中，实际上是最有效的提高预测准确性的方法，尽管只有少数算法可以被修改为使用竞赛指标作为目标函数。因此，第二种方法更为常见，许多竞赛最终都陷入了一场为了使模型在评估指标上表现最佳而争夺最佳超参数的斗争。
- en: If you already have your evaluation function coded, then doing the right cross-validation
    or choosing the appropriate test set plays the lion share. If you don’t have the
    coded function at hand, you have to first code it in a suitable way, following
    the formulas provided by Kaggle.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经编写了评估函数的代码，那么进行正确的交叉验证或选择合适的测试集就占据了主导地位。如果您手头没有编码函数，您必须首先以合适的方式编写它，遵循 Kaggle
    提供的公式。
- en: 'Invariably, doing the following will make the difference:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 不容置疑，执行以下操作将产生差异：
- en: Looking for all the relevant information about the evaluation metric and its
    coded function on a search engine
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在搜索引擎上查找有关评估指标及其编码函数的所有相关信息
- en: 'Browsing through the most common packages (such as Scikit-learn: [https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)
    or TensorFlow: [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses))'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览最常用的包（例如 Scikit-learn：[https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)
    或 TensorFlow：[https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)）
- en: 'Browsing GitHub projects (for instance, *Ben Hammer’s* Metrics project: [https://github.com/benhamner/Metrics](https://github.com/benhamner/Metrics))'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览 GitHub 项目（例如，*本·哈默的* 指标项目：[https://github.com/benhamner/Metrics](https://github.com/benhamner/Metrics)）
- en: Asking or looking around in the forums and available Kaggle Notebooks (both
    for the current competition and for similar competitions)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在论坛上提问或在可用的 Kaggle 笔记本（包括当前竞赛和类似竞赛）中寻找
- en: In addition, as we mentioned before, querying the Meta Kaggle dataset ([https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle))
    and looking in the **Competitions** table will help you find out which other Kaggle
    competitions used that same evaluation metric, and immediately provides you with
    useful code and ideas to try out
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，正如我们之前提到的，查询 Meta Kaggle 数据集（[https://www.kaggle.com/kaggle/meta-kaggle](https://www.kaggle.com/kaggle/meta-kaggle)）并在
    **竞赛** 表中查找将帮助您找出哪些其他 Kaggle 竞赛使用了相同的评估指标，并立即为您提供有用的代码和尝试的想法
- en: Let’s discuss in greater detail the alternatives you have when your evaluation
    metric doesn’t match your algorithm’s objective function. We’ll start by exploring
    custom metrics.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论当评估指标与算法的目标函数不匹配时您可以选择的替代方案。我们将从探索自定义指标开始。
- en: Custom metrics and custom objective functions
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义指标和自定义目标函数
- en: As a first option when your objective function does not match your evaluation
    metric, we learned above that you can solve this by creating your own custom objective
    function, but that only a few algorithms can easily be modified to incorporate
    a specific objective function.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标函数与评估指标不匹配时，作为第一个选项，我们上面已经了解到您可以通过创建自己的自定义目标函数来解决这个问题，但只有少数算法可以轻松修改以包含特定的目标函数。
- en: The good news is that the few algorithms that allow this are among the most
    effective ones in Kaggle competitions and data science projects. Of course, creating
    your own custom objective function may sound a little bit tricky, but it is an
    incredibly rewarding approach to increasing your score in a competition. For instance,
    there are options to do this when using gradient boosting algorithms such as XGBoost,
    CatBoost, and LightGBM, as well as with all deep learning models based on TensorFlow
    or PyTorch.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，允许这样做的一些算法在Kaggle竞赛和数据科学项目中是最有效的。当然，创建自己的自定义目标函数可能听起来有点棘手，但这是提高竞赛分数的一种非常有益的方法。例如，当使用XGBoost、CatBoost和LightGBM等梯度提升算法，以及所有基于TensorFlow或PyTorch的深度学习模型时，都有这样的选项。
- en: 'You can find great tutorials for custom metrics and objective functions in
    TensorFlow and PyTorch here:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到关于TensorFlow和PyTorch中自定义指标和目标函数的精彩教程：
- en: '[https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279](https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279](https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-are-to-use-in-tensorflow2-2-6d079c2ca279)'
- en: '[https://petamind.com/advanced-keras-custom-loss-functions/](https://petamind.com/advanced-keras-custom-loss-functions/)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://petamind.com/advanced-keras-custom-loss-functions/](https://petamind.com/advanced-keras-custom-loss-functions/)'
- en: '[https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/](https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/](https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/)'
- en: These will provide you with the basic function templates and some useful suggestions
    about how to code a custom objective or evaluation function.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将为您提供基本的功能模板以及一些关于如何编写自定义目标或评估函数的有用建议。
- en: 'If you want just to get straight to the custom objective function you need,
    you can try this Notebook by RNA ([https://www.kaggle.com/bigironsphere](https://www.kaggle.com/bigironsphere)):
    [https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook).
    It contains a large range of custom loss functions for both TensorFlow and PyTorch
    that have appeared in different competitions.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想直接获取所需的自定义目标函数，可以尝试这个RNA的Notebook（[https://www.kaggle.com/bigironsphere](https://www.kaggle.com/bigironsphere)）：[https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook）。它包含了一系列适用于TensorFlow和PyTorch的自定义损失函数，这些函数在不同的竞赛中都有出现。
- en: If you need to create a custom loss in LightGBM, XGBoost, or CatBoost, as indicated
    in their respective documentation, you have to code a function that takes as inputs
    the prediction and the ground truth, and that returns as outputs the gradient
    and the hessian.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在LightGBM、XGBoost或CatBoost中创建自定义损失，如它们各自的文档中所述，您必须编写一个函数，该函数接受预测和真实值作为输入，并返回梯度和对角线作为输出。
- en: 'You can consult this post on Stack Overflow for a better understanding of what
    a gradient and a hessian are: [https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based](https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考Stack Overflow上的这篇帖子，以更好地理解梯度和对角线是什么：[https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based](https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based)。
- en: 'From a code implementation perspective, all you have to do is to create a function,
    using closures if you need to pass more parameters beyond just the vector of predicted
    labels and true labels. Here is a simple example of a **focal loss** (a loss that
    aims to heavily weight the minority class in the loss computations as described
    in Lin, T-Y. et al. *Focal loss for dense object detection*: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002))
    function that you can use as a model for your own custom functions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码实现的角度来看，您只需创建一个函数，如果您需要传递除预测标签和真实标签向量之外的更多参数，可以使用闭包。以下是一个简单的**focal loss**（一个旨在在损失计算中为少数类赋予较大权重的损失函数，如Lin,
    T-Y. 等人在 *Focal loss for dense object detection* 中所述：[https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)）函数示例，您可以用它作为自己自定义函数的模板：
- en: '[PRE7]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another interesting aspect of the example is that it really makes it easy to
    compute the gradient and the hessian of the cost function by means of the derivative
    function from SciPy. If your cost function is differentiable, you don’t have to
    worry about doing any calculations by hand. However, creating a custom objective
    function requires some mathematical knowledge and quite a lot of effort to make
    sure it works properly for your purposes. You can read about the difficulties
    that *Max Halford* experienced while implementing a focal loss for the LightGBM
    algorithm, and how he overcame them, here: [https://maxhalford.github.io/blog/lightgbm-focal-loss/](https://maxhalford.github.io/blog/lightgbm-focal-loss/).
    Despite the difficulty, being able to conjure up a custom loss can really determine
    your success in a Kaggle competition where you have to extract the maximum possible
    result from your model.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的另一个有趣方面是，它确实使得通过SciPy的导数函数计算成本函数的梯度和对偶函数变得容易。如果你的成本函数是可微分的，你不必担心手动进行任何计算。然而，创建一个自定义目标函数需要一些数学知识，并且需要相当多的努力来确保它适用于你的目的。你可以阅读关于*Max
    Halford*在实现LightGBM算法的焦点损失时遇到的困难以及他是如何克服它们的，这里：[https://maxhalford.github.io/blog/lightgbm-focal-loss/](https://maxhalford.github.io/blog/lightgbm-focal-loss/)。尽管困难重重，但能够创造出自定义损失函数确实可以决定你在Kaggle竞赛中的成功，在那里你必须从你的模型中提取最大可能的结果。
- en: If building your own objective function isn’t working out, you can simply lower
    your ambitions, give up building your function as an objective function used by
    the optimizer, and instead code it as a custom *evaluation metric*. Though your
    model won’t be directly optimized to perform against this function, you can still
    improve its predictive performance with hyperparameter optimization based on it.
    This is the second option we talked about in the previous section.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你构建自己的目标函数不起作用，你可以简单地降低你的雄心，放弃将你的函数作为优化器使用的目标函数，而是将其编码为自定义*评估指标*。尽管你的模型不会直接优化以执行此函数，但你仍然可以通过基于它的超参数优化来提高其预测性能。这是我们在上一节中讨论的第二个选项。
- en: 'Just remember, if you are writing a metric from scratch, sometimes you may
    need to abide by certain code conventions for your function to work properly.
    For instance, if you use Scikit-learn, you have to convert your functions using
    the `make_scorer` function. The `make_scorer` function is actually a wrapper that
    makes your evaluation function suitable for working with the Scikit-learn API.
    It will wrap your function while considering some meta-information, such as whether
    to use probability estimates or predictions, whether you need to specify a threshold
    for prediction, and, last but not least, the directionality of the optimization,
    that is, whether you want to maximize or minimize the score it returns:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住，如果你是从头开始编写一个指标，有时你可能需要遵守某些代码约定以确保你的函数正常工作。例如，如果你使用Scikit-learn，你必须使用`make_scorer`函数来转换你的函数。`make_scorer`函数实际上是一个包装器，它使你的评估函数适合与Scikit-learn
    API一起工作。它将在考虑一些元信息的同时包装你的函数，例如是否使用概率估计或预测，是否需要指定预测的阈值，以及最后但同样重要的是，优化的方向性，即你是否希望最大化或最小化返回的分数：
- en: '[PRE8]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the above example, you prepare a scorer based on the average precision metric,
    specifying that it should use a weighted computation when dealing with multi-class
    classification problems.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，你基于平均精度指标准备了一个评分器，指定它应该在使用多类分类问题时使用加权计算。
- en: If you are optimizing for your evaluation metric, you can apply grid search,
    random search, or some more sophisticated optimization such as Bayesian optimization
    and find the set of parameters that makes your algorithm perform optimally for
    your evaluation metric, even if it works with a different cost function. We will
    explore how to best arrange parameter optimization and obtain the best results
    on Kaggle competitions after having discussed model validation, specifically in
    the chapter dealing with tabular data problems.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在优化你的评估指标，你可以应用网格搜索、随机搜索或更复杂的优化，如贝叶斯优化，并找到使你的算法在评估指标上表现最优的参数集，即使它与不同的成本函数一起工作。在讨论了模型验证之后，我们将探讨如何在Kaggle竞赛中最佳地安排参数优化并获得最佳结果，特别是在处理表格数据问题的章节中。
- en: Post-processing your predictions
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理你的预测结果
- en: Post-processing tuning implies that your predictions are transformed, by means
    of a function, into something else in order to present a better evaluation. After
    building your custom loss or optimizing for your evaluation metric, you can also
    improve your results by leveraging the characteristics of your evaluation metric
    using a specific function applied to your predictions. Let’s take the Quadratic
    Weighted Kappa, for instance. We mentioned previously that this metric is useful
    when you have to deal with the prediction of an ordinal value. To recap, the original
    Kappa coefficient is a chance-adjusted index of agreement between the algorithm
    and the ground truth. It is a kind of accuracy measurement corrected by the probability
    that the match between the prediction and the ground truth is due to a fortunate
    chance.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理调优意味着你的预测通过一个函数转换成其他东西，以便更好地展示评估结果。在构建你自定义的损失函数或优化你的评估指标之后，你也可以通过应用一个特定的函数到你的预测上，利用你评估指标的特征来提高你的结果。以二次加权Kappa为例。我们之前提到，这个指标在处理有序值预测时很有用。为了回顾，原始的Kappa系数是算法与真实值之间一致性的调整概率指数。它是一种经过概率校正的准确度测量方法，该概率是指预测与真实值之间的匹配是由于幸运的机会。
- en: 'Here is the original version of the Kappa coefficient, as seen before:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是之前提到的Kappa系数的原始版本：
- en: '![](img/B17574_05_039.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_039.png)'
- en: 'In the formula, *p*[0] is the relative observed agreement among raters, and
    *p*[e] is the hypothetical probability of chance agreement. Here, you need just
    two matrices, the one with the observed scores and the one with the expected scores
    based on chance agreement. When the Kappa coefficient is weighted, you also consider
    a weight matrix and the formula turns into this:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，*p*[0] 是评分者之间观察到的相对一致性，而 *p*[e] 是假设的偶然一致性概率。在这里，你需要两个矩阵，一个是观察到的分数矩阵，另一个是基于偶然一致性预期的分数矩阵。当Kappa系数加权时，你还需要考虑一个权重矩阵，公式变为：
- en: '![](img/B17574_05_040.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_05_040.png)'
- en: The matrix *p*[p] contains the penalizations to weight errors differently, which
    is very useful for ordinal predictions since this matrix can penalize much more
    when the predictions deviate further from the ground truths. Using the quadratic
    form, that is, squaring the resulting *k*, makes the penalization even more severe.
    However, optimizing for such a metric is really not easy, since it is very difficult
    to implement it as a cost function. Post-processing can help you.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *p*[p] 包含了不同错误权重的惩罚，这对于有序预测非常有用，因为当预测偏离真实值更远时，这个矩阵可以施加更多的惩罚。使用二次形式，即对结果 *k*
    进行平方，使得惩罚更加严重。然而，优化这样一个指标确实不容易，因为它很难将其实现为一个成本函数。后处理可以帮助你。
- en: An example can be found in the *PetFinder.my Adoption Prediction* competition
    ([https://www.kaggle.com/c/petfinder-adoption-prediction](https://www.kaggle.com/c/petfinder-adoption-prediction)).
    In this competition, given that the results could have 5 possible ratings (0,
    1, 2, 3, or 4), you could deal with them either using a classification or a regression.
    If you used a regression, a post-processing transformation of the regression output
    could improve the model’s performance against the Quadratic Weighted Kappa metric,
    outperforming the results you could get from a classification directly outputting
    discrete predictions.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可以在 *PetFinder.my 预测领养比赛* ([https://www.kaggle.com/c/petfinder-adoption-prediction](https://www.kaggle.com/c/petfinder-adoption-prediction))
    中找到。在这个比赛中，由于结果可能有5个可能的评级（0、1、2、3或4），你可以通过分类或回归来处理它们。如果你使用回归，对回归输出的后处理变换可以提高模型在二次加权Kappa度量标准上的性能，从而超越直接输出离散预测的分类方法。
- en: 'In the case of the PetFinder competition, the post-processing consisted of
    an optimization process that started by transforming the regression results into
    integers, first using the boundaries [0.5, 1.5, 2.5, 3.5] as thresholds and, by
    an iterative fine-tuning, finding a better set of boundaries that maximized the
    performance. The fine-tuning of the boundaries required the computations of an
    optimizer such as SciPy’s `optimize.minimize`, which is based on the Nelder-Mead
    algorithm. The boundaries found by the optimizer were validated by a cross-validation
    scheme. You can read more details about this post-processing directly from the
    post made by *Abhishek Thakur* during the competition: [https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在PetFinder竞赛的情况下，后处理包括一个优化过程，该过程首先将回归结果转换为整数，首先使用边界[0.5, 1.5, 2.5, 3.5]作为阈值，并通过迭代微调，找到一组更好的边界，以最大化性能。边界的微调需要计算优化器，如SciPy的`optimize.minimize`，它基于Nelder-Mead算法。优化器找到的边界通过交叉验证方案进行了验证。你可以直接从竞赛期间*Abhishek
    Thakur*发布的帖子中了解更多关于这种后处理细节：[https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107)。
- en: 'Aside from the PetFinder competition, many other competitions have demonstrated
    that smart post-processing can lead to improved results and rankings. We’ll point
    out a few examples here:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 除了PetFinder竞赛之外，许多其他竞赛已经证明，智能后处理可以提高结果和排名。我们在这里将指出几个例子：
- en: '[https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw](https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw](https://www.kaggle.com/khoongweihao/post-processing-technique-c-f-1st-place-jigsaw)'
- en: '[https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage](https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage)'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage](https://www.kaggle.com/tomooinubushi/postprocessing-based-on-leakage)'
- en: '[https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization](https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization)'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization](https://www.kaggle.com/saitodevel01/indoor-post-processing-by-cost-minimization)'
- en: Unfortunately, post-processing is often very dependent on the metric you are
    using (understanding the metric is imperative for devising any good post-processing)
    and often also data-specific, for instance, in the case of time series data and
    leakages. Hence, it is very difficult to generalize any procedure for figuring
    out the right post-processing for any competition. Nevertheless, always be aware
    of this possibility and be on the lookout in a competition for any hint that post-processing
    results is favorable. You can always get hints about post-processing from previous
    competitions that have been similar, and by forum discussion – eventually, someone
    will raise the topic.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，后处理通常非常依赖于你使用的度量标准（理解度量标准对于设计任何有效的后处理至关重要）并且通常也针对特定数据，例如在时间序列数据和泄露的情况下。因此，为任何竞赛概括任何确定正确后处理程序的步骤都非常困难。尽管如此，始终要意识到这种可能性，并在竞赛中寻找任何表明后处理结果有利的线索。你可以从之前类似竞赛中获得有关后处理的提示，并通过论坛讨论——最终，有人会提出这个话题。
- en: Predicted probability and its adjustment
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测概率及其调整
- en: To complete the above discussion on metrics optimization (post-processing of
    predictions), we will discuss situations where it is paramount to predict correct
    probabilities, but you are not sure if the algorithm you are using is doing a
    good job. As we detailed previously, classification probabilities concern both
    binary and multiclass classification problems, and they are commonly evaluated
    using the logarithmic loss (aka log loss or logistic loss or cross-entropy loss)
    in its binary or multi-class version (for more details, see the previous sections
    on *Metrics for classification (label prediction and probability)* and *Metrics
    for multi-class classification*).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述关于度量标准优化（预测的后处理）的讨论完成后，我们将讨论在预测正确概率至关重要，但你不确定你使用的算法是否做得好的情况下。正如我们之前详细说明的，分类概率涉及二分类和多分类问题，并且通常使用对数损失（也称为log
    loss或逻辑损失或交叉熵损失）的二元或多元版本进行评估（更多细节，请参阅关于*分类度量标准（标签预测和概率）*和*多分类度量标准*的先前章节）。
- en: 'However, evaluating or optimizing for the log loss may not prove enough. The
    main problems to be on the lookout for when striving to achieve correct probabilistic
    predictions with your model are:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，评估或优化对数损失可能并不足够。在努力使用你的模型实现正确的概率预测时，需要注意的主要问题包括：
- en: Models that do not return a truly probabilistic estimate
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不返回真正概率估计的模型
- en: Unbalanced distribution of classes in your problem
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在问题中的类别分布不平衡
- en: Different class distribution between your training data and your test data (on
    both public and private leaderboards)
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和测试数据之间的类别分布不同（在公共和私人排行榜上）
- en: The first point alone provides reason to check and verify the quality of classification
    predictions in terms of modeled uncertainty. In fact, even if many algorithms
    are provided in the Scikit-learn package together with a `predict_proba` method,
    this is a very weak assurance that they will return a true probability.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的第一点就提供了检查和验证分类预测质量（就建模不确定性而言）的理由。事实上，即使Scikit-learn包中提供了许多算法以及`predict_proba`方法，这也并不能保证它们会返回真正的概率。
- en: Let’s take, for instance, decision trees, which are the basis of many effective
    methods to model tabular data. The probability outputted by a classification decision
    tree ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))
    is based on terminal leaves; that is, it depends on the distribution of classes
    on the leaf that contains the case to be predicted. If the tree is fully grown,
    it is highly likely that the case is in a small leaf with very few other cases,
    so the predicted probability will be very high. If you change parameters such
    as `max_depth`, `max_leaf_nodes`, or `min_samples_leaf`, the resulting probability
    will drastically change from higher values to lower ones depending on the growth
    of the tree.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 以决策树为例，它是许多有效建模表格数据的有效方法的基石。分类决策树输出的概率基于终端叶子；也就是说，它依赖于包含待预测案例的叶子上类别的分布。如果树完全生长，案例很可能位于一个包含非常少其他案例的小叶子上，因此预测的概率会非常高。如果你改变`max_depth`、`max_leaf_nodes`或`min_samples_leaf`等参数，结果概率将随着树的生长从高值急剧变化到低值。
- en: Decision trees are the most common base model for ensembles such as bagging
    models and random forests, as well as boosted models such as gradient boosting
    (with its high-performing implementations XGBoost, LightGBM, and CatBoost). But,
    for the same reasons – probability estimates that are not truly based on solid
    probabilistic estimations – the problem affects many other commonly used models,
    such as support-vector machines and *k*-nearest neighbors. Such aspects were mostly
    unknown to Kagglers until the *Otto Group Product Classification Challenge* ([https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/](https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/)),
    when it was raised by *Christophe Bourguignat* and others during the competition
    (see [https://www.kaggle.com/cbourguignat/why-calibration-works](https://www.kaggle.com/cbourguignat/why-calibration-works)),
    and easily solved at the time using the calibration functions that had recently
    been added to Scikit-learn.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是集成模型（如bagging模型和随机森林）以及提升模型（如梯度提升，包括其高性能实现XGBoost、LightGBM和CatBoost）中最常见的基模型。但是，由于同样的原因——概率估计并非真正基于坚实的概率估计——这个问题影响了许多其他常用的模型，例如支持向量机和*k*最近邻。这些方面在Kagglers中大多鲜为人知，直到在*Otto
    Group Product Classification Challenge*（[https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/](https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/））中，由*Christophe
    Bourguignat*和其他人在比赛中提出（参见[https://www.kaggle.com/cbourguignat/why-calibration-works](https://www.kaggle.com/cbourguignat/why-calibration-works)），当时使用最近添加到Scikit-learn中的校准函数可以轻松解决。
- en: Aside from the model you will be using, the presence of imbalance between classes
    in your problem may also result in models that are not at all reliable. Hence,
    a good approach in the case of unbalanced classification problems is to rebalance
    the classes using undersampling or oversampling strategies, or different custom
    weights for each class to be applied when the loss is computed by the algorithm.
    All these strategies may render your model more performant; however, they will
    surely distort the probability estimates and you may have to adjust them in order
    to obtain an even better model score on the leaderboard.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你将使用的模型之外，你的问题中类别之间的不平衡也可能导致模型完全不可靠。因此，在处理不平衡分类问题时，一个好的方法是通过使用欠采样或过采样策略重新平衡类别，或者当算法计算损失时为每个类别应用不同的自定义权重。所有这些策略都可能使你的模型表现更佳；然而，它们肯定会扭曲概率估计，你可能需要调整它们以在排行榜上获得更好的模型分数。
- en: Finally, a third point of concern is related to how the test set is distributed.
    This kind of information is usually concealed, but there are often ways to estimate
    it and figure it out (for instance, by trial and error based on the public leaderboard
    results, as we mentioned in *Chapter 1*, *Introducing Kaggle and Other Data Science
    Competitions*).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三个需要关注的问题是关于测试集的分布。这类信息通常被隐藏，但通常有方法可以估计它并找出它（例如，通过基于公开排行榜结果的试错法，正如我们在*第一章*，*介绍Kaggle和其他数据科学竞赛*中提到的）。
- en: For instance, this happened in the *iMaterialist Furniture Challenge* ([https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/))
    and the more popular *Quora Question Pairs* ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)).
    Both competitions gave rise to various discussions on how to post-process in order
    to adjust probabilities to test expectations (see [https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/](https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/)
    and [https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb](https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb)
    for more details on the method used). From a general point of view, assuming that
    you do not have an idea of the test distribution of classes to be predicted, it
    is still very beneficial to correctly predict probability based on the priors
    you get from the training data (and until you get evidence to the contrary, that
    is the probability distribution that your model should mimic). In fact, it will
    be much easier to correct your predicted probabilities if your predicted probability
    distribution matches those in the training set.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这种情况发生在*iMaterialist Furniture Challenge*([https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/))和更受欢迎的*Quora
    Question Pairs*([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs))竞赛中。这两场竞赛都引发了关于如何进行后处理以调整概率以符合测试预期的各种讨论（参见[https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/](https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/)和[https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb](https://www.kaggle.com/dowakin/probability-calibration-0-005-to-lb)以获取关于所用方法的更多详细信息）。从一般的角度来看，即使你不知道要预测的类别的测试分布，根据从训练数据中获得的先验概率来正确预测概率仍然非常有好处（并且直到你得到相反的证据，这就是你的模型应该模仿的概率分布）。实际上，如果你的预测概率分布与训练集中的分布相匹配，那么纠正你的预测概率将会容易得多。
- en: 'The solution, when your predicted probabilities are misaligned with the training
    distribution of the target, is to use the **calibration function** provided by
    Scikit-learn, `CalibratedClassifierCV`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的预测概率与目标训练分布不匹配时，解决方案是使用Scikit-learn提供的**校准函数**，`CalibratedClassifierCV`：
- en: '[PRE9]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The purpose of the calibration function is to apply a post-processing function
    to your predicted probabilities in order to make them adhere more closely to the
    empirical probabilities seen in the ground truth. Provided that your model is
    a Scikit-learn model or behaves similarly to one, the function will act as a wrapper
    for your model and directly pipe its predictions into a post-processing function.
    You have the choice between using two methods for post-processing. The first is
    the **sigmoid** method (also called Plat’s scaling), which is nothing more than
    a logistic regression. The second is the **isotonic regression**, which is a non-parametric
    regression; beware that it tends to overfit if there are few examples.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 校准函数的目的是对你的预测概率应用一个后处理函数，以便使其更接近于在真实数据中看到的经验概率。假设你的模型是Scikit-learn模型或类似行为，该函数将作为你的模型包装器，并将其预测直接传递到后处理函数。你可以在两种后处理方法之间进行选择。第一种是**sigmoid**方法（也称为Plat的缩放），这实际上就是逻辑回归。第二种是**等调回归**，这是一种非参数回归；注意，如果示例很少，它往往会过拟合。
- en: You also have to choose how to fit this calibrator. Remember that it is a model
    that is applied to the results of your model, so you have to avoid overfitting
    by systematically reworking predictions. You could use a **cross-validation**
    (more on this in the following chapter on *Designing Good Validation*) and then
    produce a number of models that, once averaged, will provide your predictions
    (`ensemble=True`). Otherwise, and this is our usual choice, resort to an **out-of-fold
    prediction** (more on this in the following chapters) and calibrate on that using
    all the data available (`ensemble=False`).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须选择如何拟合这个校准器。记住，它是一个应用于你模型结果的模型，因此你必须通过系统地重新工作预测来避免过拟合。你可以使用**交叉验证**（关于这一点，下一章将详细介绍*设计良好的验证*），然后生成多个模型，一旦平均，将提供你的预测（`ensemble=True`）。否则，这是我们通常的选择，求助于**折叠外预测**（关于这一点，下一章将详细介绍）并使用所有可用数据对其进行校准（`ensemble=False`）。
- en: Even if `CalibratedClassifierCV` can handle most situations, you can also figure
    out some empirical way to fix probability estimates for the best performance at
    test time. You can use any transformation function, from a handmade one to a sophisticated
    one derived by genetic algorithms, for instance. Your only limit is simply that
    you should cross-validate it and possibly have a good final result from the public
    leaderboard (but not necessarily, because you should trust your local cross-validation
    score more, as we are going to discuss in the next chapter). A good example of
    such a strategy is provided by Silogram ([https://www.kaggle.com/psilogram](https://www.kaggle.com/psilogram)),
    who, in the *Microsoft Malware Classification Challenge*, found out a way to tune
    the unreliable probabilistic outputs of random forests into probabilistic ones
    simply by raising the output to a power determined by grid search (see [https://www.kaggle.com/c/malware-classification/discussion/13509](https://www.kaggle.com/c/malware-classification/discussion/13509)).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`CalibratedClassifierCV`可以处理大多数情况，你还可以找出一些经验方法来调整测试时的概率估计，以获得最佳性能。你可以使用任何转换函数，从手工制作的到由遗传算法推导出的复杂函数，例如。你唯一的限制是应该交叉验证它，并可能从公共排行榜中获得一个好的最终结果（但不一定，因为你应该更信任你的本地交叉验证分数，正如我们将在下一章中讨论的那样）。这样的策略的一个好例子是由Silogram
    ([https://www.kaggle.com/psilogram](https://www.kaggle.com/psilogram))提供的，他在*Microsoft
    Malware Classification Challenge*中找到了一种方法，通过将输出提升到由网格搜索确定的幂来调整随机森林不可靠的概率输出，使其成为概率输出（见[https://www.kaggle.com/c/malware-classification/discussion/13509](https://www.kaggle.com/c/malware-classification/discussion/13509))）。
- en: '![](img/Sudalai_Rajkumar.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![Sudalai_Rajkumar](img/Sudalai_Rajkumar.png)'
- en: Sudalai Rajkumar
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Sudalai Rajkumar
- en: '[https://www.kaggle.com/sudalairajkumar](https://www.kaggle.com/sudalairajkumar)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/sudalairajkumar](https://www.kaggle.com/sudalairajkumar)'
- en: 'In our final interview of the chapter, we speak to Sudalai Rajkumar, SRK, a
    Grandmaster in Competitions, Datasets, and Notebooks, and a Discussion Master.
    He is ranked #1 in the Analytics Vidhya data science platform, and works as an
    AI/ML advisor for start-ups.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一次采访中，我们与Sudalai Rajkumar，SRK，这位在竞赛、数据集和笔记本方面的围棋大师，以及讨论大师进行了交谈。他在Analytics
    Vidhya数据科学平台上排名#1，并为初创公司担任AI/ML顾问。
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*My favorite kinds of competition are ones that involve a good amount of feature
    engineering. I think that is my strength as well. I am generally interested in
    data exploration to get a deep understanding of the data (which you can infer
    from my series of simple exploration Notebooks (*[https://www.kaggle.com/sudalairajkumar/code](https://www.kaggle.com/sudalairajkumar/code)*))
    and then creating features based on it.*'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '*我最喜欢的竞赛类型是那些涉及大量特征工程的竞赛。我认为这也是我的优势。我通常对数据探索感兴趣，以深入理解数据（你可以从我的系列简单探索笔记本中推断出来[*https://www.kaggle.com/sudalairajkumar/code*]），然后基于它创建特征。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种处理方式与你在日常工作中所做的是如何不同的？
- en: '*The framework for a competition involves data exploration, finding the right
    validation method, feature engineering, model building, and ensembling/stacking.
    All these are involved in my day job as well. But in addition to this, there is
    a good amount of stakeholder discussion, data collection, data tagging, model
    deployment, model monitoring, and data storytelling that is involved in my daily
    job.*'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '*竞赛的框架包括数据探索、找到合适的验证方法、特征工程、模型构建和集成/堆叠。所有这些都在我的日常工作中涉及。但除此之外，还有大量的利益相关者讨论、数据收集、数据标记、模型部署、模型监控和数据故事讲述。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你参加的一个特别具有挑战性的竞赛，以及你使用了哪些见解来应对这项任务。
- en: Santander Product Recommendation *is a memorable competition that we entered.
    Rohan & I did a lot of feature engineering and built multiple models. When we
    did final ensembling, we used different weights for different products and some
    of them did not add up to 1\. From the data exploration and understanding, we
    hand-picked these weights, which helped us. This made us realise the domain/data
    importance in solving problems and how data science is an art as much as science.*
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Santander产品推荐*是我们参加的一个令人难忘的竞赛。我和Rohan进行了大量的特征工程，并构建了多个模型。在最终的集成中，我们为不同的产品使用了不同的权重，其中一些权重加起来并不等于1。从数据探索和理解中，我们手动挑选了这些权重，这帮了我们。这让我们意识到领域/数据在解决问题中的重要性，以及数据科学既是艺术也是科学。*
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进步？如果是的话，你是如何做到的？
- en: '*Kaggle played a very important role in my career. I was able to secure my
    last two jobs mainly because of Kaggle. Also, the success from Kaggle helps to
    connect with other stalwarts in the data science field easily and learn from them.
    It also helps a lot in my current role as AI / ML advisor for start-ups, as it
    gives credibility.*'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle在我的职业生涯中发挥了非常重要的作用。我能够获得我最后两个工作主要是因为Kaggle。此外，Kaggle的成功帮助我轻松地与其他数据科学领域的杰出人物建立联系，并向他们学习。这也极大地帮助我在当前作为初创公司AI/ML顾问的角色中，因为它增加了我的可信度。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kagglers通常忽视了什么？你现在知道什么，而当你刚开始时希望知道的呢？
- en: '*Understanding the data in depth. Often this is overlooked, and people get
    into model-building right away. Exploring the data plays a very important role
    in the success of any Kaggle competition. This helps to create proper cross validation
    and to create better features and to extract more value from the data.*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*深入理解数据。这往往被忽视，人们直接进入模型构建阶段。探索数据在Kaggle竞赛的成功中扮演着非常重要的角色。这有助于创建适当的交叉验证，创建更好的特征，并从数据中提取更多价值。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去竞赛中犯过哪些错误？
- en: '*It is a very big list, and I would say that they are learning opportunities.
    In every competition, out of 20-30 ideas that I try, only 1 may work. These mistakes/failures
    give much more learning than the actual success or things that worked. For example,
    I learnt about overfitting the very hard way by falling from top deciles to bottom
    deciles in one of my very first competitions. But that learning stayed with me
    forever thereafter.*'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一个非常长的列表，我认为这些都是学习机会。在每一场竞赛中，我尝试了20-30个想法，可能只有1个会成功。这些错误/失败比实际的成功或有效的事物能带来更多的学习。例如，我在我参加的第一个竞赛中，由于过度拟合，从顶尖的十分之一跌到了最底部的十分之一，这是我对过度拟合的非常艰难的学习方式。但这次学习一直伴随着我。*'
- en: '*Are there any particular tools or libraries that you would recommend using
    for data analysis/machine learning?*'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '*有没有任何特定的工具或库，你会推荐用于数据分析/机器学习？*'
- en: '*I primarily use XGBoost/LightGBM in the case of tabular data. I also use open
    source AutoML libraries and Driverless AI to get early benchmarks these days.
    I use Keras, Transformers, and PyTorch for deep learning models.*'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '*我主要在处理表格数据时使用XGBoost/LightGBM。如今，我也使用开源的AutoML库和Driverless AI来获取早期的基准测试。对于深度学习模型，我使用Keras、Transformers和PyTorch。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加竞赛时，他们应该记住或做最重要的事情是什么？
- en: '*Consistency is the key. Each competition will have its own ups and downs.
    There will be multiple days without any progress, but we should not give up and
    keep trying. I think this is applicable for anything and not just Kaggle competitions.*'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '*一致性是关键。每个竞赛都会有起有落。可能会有多天没有任何进展，但我们不应该放弃，继续尝试。我认为这适用于任何事，而不仅仅是Kaggle竞赛。*'
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用其他竞赛平台吗？它们与Kaggle相比如何？
- en: '*I have also taken part on other platforms like the Analytics Vidhya DataHack
    platform, Driven Data, CrowdAnalytix etc. They are good too, but Kaggle is more
    widely adopted and global in nature, so the amount of competition on Kaggle is
    much higher compared to other platforms.*'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '*我也在其他平台如Analytics Vidhya DataHack平台、Driven Data、CrowdAnalytix等参与过。它们也很好，但Kaggle的采用范围更广，具有全球性质，因此与其他平台相比，Kaggle上的竞争量要大得多。*'
- en: Summary
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed evaluation metrics in Kaggle competitions.
    First, we explained how an evaluation metric can differ from an objective function.
    We also remarked on the differences between regression and classification problems.
    For each type of problem, we analyzed the most common metrics that you can find
    in a Kaggle competition.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Kaggle竞赛中的评估指标。首先，我们解释了评估指标如何与目标函数不同。我们还提到了回归问题和分类问题之间的区别。对于每种类型的问题，我们分析了在Kaggle竞赛中可以找到的最常见的指标。
- en: After that, we discussed the metrics that have never previously been seen in
    a competition and that you won’t likely see again. Finally, we explored and studied
    different common metrics, giving examples of where they have been used in previous
    Kaggle competitions. We then proposed a few strategies for optimizing an evaluation
    metric. In particular, we recommended trying to code your own custom cost functions
    and provided suggestions on possible useful post-processing steps.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们讨论了在竞赛中从未见过且不太可能再次出现的指标。最后，我们探索并研究了不同的常见指标，给出了它们在以前的Kaggle竞赛中使用的例子。然后，我们提出了一些优化评估指标的策略。特别是，我们建议尝试编写自己的自定义成本函数，并提供了可能的实用后处理步骤的建议。
- en: You should now have grasped the role of an evaluation metric in a Kaggle competition.
    You should also have a strategy to deal with every common or uncommon metric,
    by retracing past competitions and by gaining a full understanding of the way
    a metric works. In the next chapter, we are going to discuss how to use evaluation
    metrics and properly estimate the performance of your Kaggle solution by means
    of a validation strategy.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该已经掌握了在Kaggle竞赛中评估指标的作用。你也应该有一个策略来处理每一个常见或不常见的指标，通过回顾过去的竞赛并全面理解指标的工作方式。在下一章中，我们将讨论如何通过验证策略使用评估指标，并正确估计你的Kaggle解决方案的性能。
- en: Join our book’s Discord space
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord工作空间，参加每月的“问我任何问题”活动，与作者交流：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaggle Discord链接](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code40480600921811704671.png)'
