- en: Natural Language Processing Using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll study and implement common algorithms that are used
    in NLP, which can help us develop machines that are capable of automatically analyzing
    and understanding human text and speech in context. Specifically, we will study
    and implement the following classes of computer science algorithms related to
    NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature transformers, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature extractors, including the following :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency–inverse document frequency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental concept behind natural language processing is treating human
    text and speech as data—just like the structured and unstructured numerical and
    categorical data sources we have encountered in this book thus far—while preserving
    its *context*. However, natural language is notoriously difficult to understand,
    even for humans, let alone machines! Not only does natural language consist of
    hundreds of different spoken languages, with different writing systems, but it
    also poses other challenges, such as different tones, inflections, slang, abbreviations,
    metaphors, and sarcasm. Writing systems and communication platforms in particular
    provide us with text that may contain spelling mistakes, unconventional grammar,
    and sentences that are loosely structured.
  prefs: []
  type: TYPE_NORMAL
- en: Our first challenge, therefore, is to convert natural language into data that
    can be used by a machine while preserving its underlying context. Furthermore,
    when applied to machine learning, we also need to convert natural language into
    feature vectors in order to train machine learning models. Well, there are two
    broad classes of computer science algorithms that help us with these challenges—**feature
    extractors**, which help us extract relevant features from the natural language
    data, and **feature transformers**, which help us scale, convert, and/or modify
    these features in preparation for subsequent modelling. In this subsection, we
    will discuss feature transformers and how they can help us convert our natural
    language data into structures that are easier to process. First, let's introduce
    some common definitions within NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Document
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In NLP, a document represents a logical container of text. The container itself
    can be anything that makes sense within the context of your use case. For example,
    one document could refer to a single article, record, social media posting, or
    tweet.
  prefs: []
  type: TYPE_NORMAL
- en: Corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have defined what your document represents, a corpus is defined as
    a logical collection of documents. Using the previous examples, a corpus could
    represent a collection of articles (for example, a magazine or blog) or a collection
    of tweets (for example, tweets with a particular hashtag).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the basic tasks involved in NLP is the preprocessing of your documents
    in an attempt to standardize the text from different sources as much as possible.
    Not only does preprocessing help us to standardize text, it often reduces the
    size of the raw text, thereby reducing the computational complexity of subsequent
    processes and models. The following subsections describe common preprocessing
    techniques that may constitute a typical ordered preprocessing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization refers to the technique of splitting up your text into individual
    *tokens* or terms. Formally, a token is defined as a sequence of characters that
    represents a subset of the original text. Informally, tokens are typically just
    the different words that make up the original text, and that have been segmented
    using the whitespace and other punctuation characters. For example, the sentence
    "Machine Learning with Apache Spark" may result in a collection of tokens persisted
    in an array or list expressed as `["Machine", "Learning", "with", "Apache", "Spark"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stop words are common words in a given language that are used to structure a
    sentence grammatically, but that are not necessarily helpful in determining its
    underlying meaning or sentiment. For example, in the English language, common
    stop words include *and*, *I*, *there*, *this*, and *with*. A common preprocessing
    technique is to therefore remove these words from the collection of tokens by
    filtering based on a language-specific lookup of stop words. Using our previous
    example, our filtered list of tokens would be `["Machine", "Learning", "Apache",
    "Spark]`.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming refers to the technique of reducing words to a common base or *stem*.
    For example, the words "connection", "connections", "connective", "connected",
    and "connecting" can all be reduced to their common stem of "connect". Stemming
    is not a perfect process, and stemming algorithms are liable to make mistakes.
    However, for the purposes of reducing the size of a dataset in order to train
    a machine learning model, it is a valuable technique. Using our previous example,
    our filtered list of stems would be `["Machin", "Learn", "Apach", "Spark"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While stemming quickly reduces words to a base form, it does not take into account
    the context, and can therefore not differentiate between words that have different
    meanings depending on their position within a sentence or context. Lemmatization
    does not crudely reduce words purely based on a common stem, but instead aims
    to remove inflectional endings only in order to return a dictionary form of a
    word called its *lemma*. For example, the words *am*, *is*, *being*, and *was*
    can be reduced to the lemma *be*, while a stemmer would not be able to infer this
    contextual meaning.
  prefs: []
  type: TYPE_NORMAL
- en: While lemmatization can be used to preserve context and meaning to a better
    extent, it comes at the cost of additional computational complexity and processing
    time. Using our previous example, our filtered list of lemmas may therefore look
    like `["Machine", "Learning", "Apache", "Spark"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, normalization refers to a wide variety of common techniques that are
    used to standardize text. Typical normalization techniques include converting
    all text to lowercase, removing selected characters, punctuation and other sequences
    of characters (typically using regular expressions), and expanding abbreviations
    by applying language-specific dictionaries of common abbreviations and slang terms.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.1* illustrates a typical ordered preprocessing pipeline that is used
    to standardize raw written text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2166c5b8-2ae4-494c-aeff-0addd02bdefa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Typical preprocessing pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extractors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how feature transformers allow us to convert, modify, and standardize
    our documents using a preprocessing pipeline, resulting in the conversion of raw
    text into a collection of tokens. *Feature extractors* take these tokens and generate
    feature vectors from them that may then be used to train machine learning models.
    Two common examples of typical feature extractors that are used in NLP are the
    **bag of words** and **term frequency–inverse document frequency** (**TF–IDF**)
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *bag of words* approach simply counts the number of occurrences of each
    unique word in the raw or tokenized text. For example, given the text "Machine
    Learning with Apache Spark, Apache Spark''s MLlib and Apache Kafka", the bag of
    words approach would provide us with the following numerical feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Machine | Learning | with | Apache | Spark | MLlib | Kafka |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 3 | 2 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Note that each unique word is a feature or dimension, and that the bag of words
    approach is a simple technique that is often employed as a baseline model with
    which to compare the performance of more advanced feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency–inverse document frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TF–IDF** aims to improve upon the bag of words approach by providing an indication
    of how *important* each word is, taking into account how often that word appears
    across the entire corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us use *TF(t, d)* to denote the **term frequency**, which is the number
    of times that a term, *t*, appears in a document, *d*. Let''s also use *DF(t,
    D)* to denote the **document frequency**, which is the number of documents in
    our corpus, *D*, that contain the term *t*. We can then define the **inverse document
    frequency** *IDF(t, D)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a66d7f42-032c-4cd8-8362-92c147274cbc.png)'
  prefs: []
  type: TYPE_IMG
- en: The IDF provides us with a measure of how important a term is, taking into account
    how often that term appears across the entire corpus, where *|D|* is the total
    number of documents in our corpus, *D*. Terms that are less common across the
    corpus have a higher IDF metric. Note, however, that because of the use of the
    logarithm, if a term appears in all documents, its IDF becomes 0—that is, *log(1)*.
    IDF, therefore, provides a metric whereby more value is placed on rarer terms
    that are important in describing documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to calculate the TF–IDF measure, we simply multiply the term frequency
    by the inverse document frequency as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc877716-e3b7-4aba-a75e-9a744a014d5c.png)'
  prefs: []
  type: TYPE_IMG
- en: This implies that the TF–IDF measure increases proportionally with the number
    of times that a word appears in a document, offset by the frequency of the word
    across the entire corpus. This is important because the term frequency alone may
    highlight words such as "a", "I", and "the" that appear very often in a given
    document but that do not help us determine the underlying meaning or sentiment
    of the text. By employing TF–IDF, we can reduce the impact of these types of words
    on our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now apply these feature transformers and feature extractors to a very
    modern real-world use case—sentiment analysis. In sentiment analysis, the goal
    is to classify the underlying human sentiment—for example, whether the writer
    is positive, neutral, or negative towards the subject of a text. To many organizations,
    sentiment analysis is an important technique that is used to better understand
    their customers and target markets. For example, sentiment analysis can be used
    by retailers to gauge the public's reaction to a particular product, or by politicians
    to assess public mood towards a policy or news item. In our case study, we will
    examine tweets about airlines in order to predict whether customers are saying
    positive or negative things about them. Our analysis could then be used by airlines
    in order to improve their customer service by focusing on those tweets that have
    been classified as negative in sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: The corpus of tweets that we will use for our case study has been downloaded
    from **Figure Eight**, a company that provides businesses with high-quality training
    datasets for real-world machine learning. Figure Eight also provides a Data for
    Everyone platform containing open datasets that are available for download by
    the public, and which may be found at [https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open `twitter-data/airline-tweets-labelled-corpus.csv` in any text editor
    from either the GitHub repository accompanying this book or from Figure Eight''s
    Data for Everyone platform, you will find a collection of 14,872 tweets about
    major airlines that were scraped from Twitter in February 2015\. These tweets
    have also been pre-labelled for us, with a sentiment classification of positive,
    negative, or neutral. The pertinent columns in this dataset are described in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column Name** | **Data Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `unit_id` | `Long` | Unique identifier (primary key) |'
  prefs: []
  type: TYPE_TB
- en: '| `airline_sentiment` | `String` | Sentiment classification—positive, neutral,
    or negative |'
  prefs: []
  type: TYPE_TB
- en: '| `airline` | `String` | Name of the airline |'
  prefs: []
  type: TYPE_TB
- en: '| `text` | `String` | Textual content of the tweet |'
  prefs: []
  type: TYPE_TB
- en: Our goal will be to use this corpus of tweets in order to train a machine learning
    model to predict whether future tweets about a given airline are positive or negative
    in sentiment towards that airline.
  prefs: []
  type: TYPE_NORMAL
- en: NLP pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we look at the Python code for our case study, let''s visualize the
    end-to-end NLP pipeline that we will construct. Our NLP pipeline for this case
    study is illustrated in *Figure 6.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e322457d-6d42-4f82-999a-a2a446d5862e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: End-to-end NLP pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: NLP in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of Spark 2.3.2, tokenization and stop-word removal feature transformers (among
    a wide variety of others), and the TF–IDF feature extractor is available natively
    in `MLlib`. Although stemming, lemmatization, and standardization can be achieved
    indirectly through transformations on Spark dataframes in Spark 2.3.2 (via **user-defined
    functions** (**UDFs**) and map functions that are applied to RDDs), we will be
    using a third-party Spark library called `spark-nlp` to perform these feature
    transformations. This third-party library has been designed to extend the features
    already available in `MLlib` by providing an easy-to-use API for distributed NLP
    annotations on Spark dataframes at scale. To learn more about `spark-nlp`, please
    visit [https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/). Finally,
    we will use the estimators and transformers that are already available natively
    in `MLlib`—as we have seen in previous chapters—to train our final machine learning
    classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that by using feature transformers and extractors native to `MLlib` followed
    by feature transformers provided by the third-party `spark-nlp` library, before
    finally applying native `MLlib` estimators, we will be required to explicitly
    define and develop data transformation stages in our pipeline in order to conform
    to the underlying data structures expected by the two different libraries. While
    this is not recommended for production-grade pipelines because of its inefficiencies,
    one of the purposes of this section is to demonstrate how to use both libraries
    for NLP. Readers will then be in an informed position to choose a suitable library,
    depending on the requirements of the use case in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your environment setup, there are a few methods that are available
    that can be used to install `spark-nlp`, as described at [https://nlp.johnsnowlabs.com/quickstart.html](https://nlp.johnsnowlabs.com/quickstart.html).
    However, based on the local development environment that we provisioned in [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml),* Setting Up a Local Development
    Environment*, we will install `spark-nlp` using `pip`, which is another commonly
    used Python package manager that comes bundled with the Anaconda distribution
    that we have already installed (at the time of writing, `spark-nlp` is not available
    via the `conda` repositories, and so we shall use `pip` instead). To install `spark-nlp`
    for our Python environment, simply execute the following command, which will install
    version 1.7.0 of `spark-nlp` (which is the latest version as of writing, and which
    is compatible with Spark 2.x):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to tell Spark where it can find the `spark-nlp` library. We can
    do this either by defining an additional parameter in `{SPARK_HOME}/conf/spark-defaults.conf`
    or by setting the `spark.jars` configuration within our code when instantiating
    a Spark context, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please refer to [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, for further details regarding defining the
    configuration for Apache Spark. Note that in a multinode Spark cluster, all third-party
    Python packages either need to be installed on all Spark nodes or your Spark application
    itself needs to be packaged into a self-contained file containing all third-party
    dependencies. This self-contained file is then distributed to all nodes in the
    Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to develop our NLP pipeline in Apache Spark in order to perform
    sentiment analysis on our corpus of airline tweets! Let''s go through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp06-01-natural-language-processing.ipynb`.
    It can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'As well as importing the standard PySpark dependencies, we also need to import
    the relevant `spark-nlp` dependencies, including its `Tokenizer`, `Stemmer`, and
    `Normalizer` classes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a `SparkContext` as usual. Note, however, that in this
    case, we explicitly tell Spark where to find the `spark-nlp` library using the
    `spark-jars` configuration parameter. We can then invoke the `getConf()` method
    on our `SparkContext` instance to review the current Spark configuration, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading our corpus of airline tweets from `twitter-data/airline-tweets-labelled-corpus.csv`
    into a Spark dataframe called `airline_tweets_df`, we generate a new label column.
    The existing dataset already contains a label column called `airline_sentiment`,
    which is either `"positive"`, `"neutral"`, or `"negative"` based on a manual pre-classification.
    Although positive messages are naturally always welcome, in reality, the most
    useful messages are usually the negative ones. By automatically identifying and
    studying the negative messages, organizations can focus more efficiently on how
    to improve their products and services based on negative feedback. Therefore,
    we will create a new label column called `negative_sentiment_label` that is `"true"`
    if the underlying sentiment has been classified as `"negative"` and `"false"`
    otherwise, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build and apply our preprocessing pipeline to our corpus
    of raw tweets! Here, we demonstrate how to utilize the feature transformers native
    to Spark''s `MLlib`, namely its `Tokenizer` and `StopWordsRemover` transformers.
    First, we tokenize the raw textual content of each tweet using the `Tokenizer`
    transformer, resulting in a new column containing a list of parsed tokens. We
    then pass this column containing the tokens to the `StopWordsRemover` transformer,
    which removes English language (default) stop words from this list, resulting
    in a new column containing the list of filtered tokens. In the next cell, we will
    demonstrate how to utilize the feature transformers available in the `spark-nlp` third-party
    library. However, `spark-nlp` requires a column of a `string` type as its initial
    input, not a list of tokens. Therefore, the final statement concatenates the list
    of filtered tokens back into a whitespace-delimited `string` column, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now demonstrate how to utilize the feature transformers and annotators
    available in the `spark-nlp` third-party library, namely its `DocumentAssember`
    transformer and `Tokenizer`, `Stemmer` , and `Normalizer` annotators. First, we
    create annotated documents from our string column that are required as the initial
    input into the `spark-nlp` pipelines. Then, we apply the `spark-nlp` `Tokenizer`
    and `Stemmer` annotators to convert our filtered list of tokens into a list of
    *stems*. Finally, we apply its `Normalizer` annotator, which converts the stems
    into lowercase by default. All of these stages are defined within a *pipeline*, which,
    as we saw in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, is an ordered list of machine learning and data
    transformation steps that is executed on a Spark dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We execute our pipeline on our dataset, resulting in a new dataframe called
    `preprocessed_df` from which we keep only the relevant columns that are required
    for subsequent analysis and modelling, namely `unit_id` (unique record identifier),
    `text` (original raw textual content of the tweet), `negative_sentiment_label` (our
    new label), and `normalised_stems` (a `spark-nlp` array of filtered, stemmed,
    and normalized tokens as a result of our preprocessing pipeline), as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can create feature vectors from our array of stemmed tokens using
    `MLlib`''s native feature extractors, there is one final preprocessing step. The
    column containing our stemmed tokens, namely `normalised_stems`, persists these
    tokens in a specialized `spark-nlp` array structure. We need to convert this `spark-nlp`
    array back into a standard list of tokens so that we may apply `MLlib`''s native
    TF–IDF algorithms to it. We achieve this by first exploding the `spark-nlp` array
    structure, which has the effect of creating a new dataframe observation for every
    element in this array. We then group our Spark dataframe by `unit_id`, which is
    the primary key for each unique tweet, before aggregating the stems using the
    whitespace delimiter into a new string column called `tokens`. Finally, we apply
    the `split` function to this column to convert the aggregated string into a list
    of strings or tokens, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to generate feature vectors from our list of filtered, stemmed,
    and normalized tokens! As discussed, we will be using the TF–IDF feature extractor
    to generate feature vectors rather than the basic bag of words approach. The TF–IDF
    feature extractor is native to `MLlib` and comes in two parts. First, we generate the **term
    frequency** (**TF**) feature vectors by passing our list of tokens into `MLlib`''s
    `HashingTF` transformer. We then *fit* `MLlib`''s **inverse document frequency**
    (**IDF**) estimator to our dataframe containing the term frequency feature vectors,
    as shown in the following code. The result is a new Spark dataframe with our TF–IDF
    feature vectors contained in a column called `features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, since our label column is categorical in nature,
    we need to apply `MLlib`''s `StringIndexer` to it in order to identify and index
    all possible classifications. The result is a new Spark dataframe with an indexed
    label column called `"label"`, which is 0.0 if `negative_sentiment_label` is `true`,
    and 1.0 if `negative_sentiment_label` is `false`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to create training and test dataframes in order to train and
    evaluate subsequent machine learning models. We achieve this as normal, using
    the `randomSplit` method (as shown in the following code), but in this case, 90%
    of all observations will go into our training dataframe, with the remaining 10%
    going into our test dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we will be training a supervised decision tree classifier
    (see [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised Learning
    Using Apache Spark*) in order to help us classify whether a given tweet is positive
    or negative in sentiment. As in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml),
    *Supervised Learning Using Apache Spark*, we fit `MLlib`''s `DecisionTreeClassifier`
    estimator to our training dataframe in order to train our classification tree,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a trained classification tree, we can apply it to our test
    dataframe in order to classify test tweets. As we did in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, we apply our trained classification tree to the
    test dataframe using the `transform()` method (as shown in the following code),
    and afterwards study its predicted classifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, our decision tree classifier has predicted that the following
    tweets from our test dataframe are negative in sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '"I need you...to be a better airline. ^LOL"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"if you can''t guarantee parents will sit with their children, don''t sell
    tickets with that promise"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"resolved and im sick and tired of waiting on you. I want my refund and I''d
    like to speak to someone about it."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"I would have loved to respond to your website until I saw the really long
    form. In business the new seats are bad"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A human would also probably classify these tweets as negative in sentiment!
    But more importantly, airlines can use this model and the tweets that it identifies
    to focus on areas for improvement. Based on this sample of tweets, such areas
    would include website usability, ticket marketing, and the time taken to process
    refunds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in order to quantify the accuracy of our trained classification tree,
    let''s compute its confusion matrix on the test data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting confusion matrix looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predict *y *= 0 (Negative)** | **Predict *y* = 1 (Non-Negative)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual *y* = 0****(Negative)** | 725 | 209 |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual *y* = 1****(Non-Negative)** | 244 | 325 |'
  prefs: []
  type: TYPE_TB
- en: 'We can interpret this confusion matrix as follows—out of a total of 1,503 test
    tweets, our model exhibits the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly classifies 725 tweets as negative in sentiment that are actually negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correctly classifies 325 tweets as non-negative in sentiment that are actually
    non-negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrectly classifies 209 tweets as non-negative in sentiment that are actually
    negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrectly classifies 244 tweets as negative in sentiment that are actually
    non-negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall accuracy = 70%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall error rate = 30%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitivity = 57%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity = 78%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, based on a default threshold value of 0.5 (which in this case study is fine
    because we have no preference over what type of error is better), our decision
    tree classifier has an overall accuracy rate of 70%, which is quite good!
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of completeness, let''s train a decision tree classifier, but
    using the feature vectors that are derived from the bag of words algorithm. Note
    that we already computed these feature vectors when we applied the `HashingTF`
    transformer to our preprocessed corpus to calculate the term frequency (TF) feature
    vectors. Therefore, we can just repeat our machine learning pipeline, but based
    only on the output of the `HashingTF` transformer instead, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that the resulting confusion matrix is exactly the same as when we applied
    our decision tree classifier that had been trained on the *scaled* feature vectors
    using the `IDF` estimator (given the same random split seed and size of the training
    dataframe). This is because of the fact that our corpus of tweets is relatively
    small at 14,872 documents, and therefore the effect of scaling the term frequency
    (`TF`) feature vectors based on the frequency across the corpus will have a negligible
    impact on the predictive quality of this specific model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very useful feature provided by `MLlib` is the ability to save trained machine
    learning models to disk for later use. We can take advantage of this feature by
    saving our trained decision tree classifier to the local disk of our single-node
    development environment. In multi-node clusters, trained models may also be saved
    to a distributed file system, such as the Apache Hadoop Distributed File system
    (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*)
    by simply using the relevant file system prefix (for example `hdfs://<HDFS NameNode
    URL>/<HDFS Path>`), as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Our trained decision tree classifier for performing sentiment analysis of airline
    tweets has also been pushed to the GitHub repository accompanying this book, and
    may be found in `chapter06/models/airline-sentiment-analysis-decision-tree-classifier`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have studied, implemented, and evaluated common algorithms
    that are used in natural language processing. We have preprocessed a corpus of
    documents using feature transformers and generated feature vectors from the resulting
    processed corpus using feature extractors. We have also applied these common NLP
    algorithms to machine learning. We trained and tested a sentiment analysis model
    that we used to predict the underlying sentiment of tweets so that organizations
    may improve their product and service offerings. In [Chapter 8](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml),
    *Real-Time Machine Learning Using Apache Spark*, we will extend our sentiment
    analysis model to operate in real time using Spark Streaming and Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a hands-on exploration through the exciting
    and cutting-edge world of deep learning!
  prefs: []
  type: TYPE_NORMAL
