<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using Spark with IBM Watson Studio</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will discuss <strong>Machine Learning (ML)</strong> pipelines and provide guidelines for creating and deploying a Spark machine learning pipeline within IBM Watson Studio.</p>
<p>We will divide this chapter into the following areas:</p>
<ul>
<li>Introduction to Apache Spark</li>
<li>Creating a Spark pipeline in Watson Studio</li>
<li>Data preparation</li>
<li>A data analysis and visualization example</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Apache Spark</h1>
                </header>
            
            <article>
                
<p>Before we get going on creating any kind of a pipeline, we should take a minute to familiarize ourselves with what Spark is and what it offers us.</p>
<p>Spark, built for both speed and ease of use, is a superfast open source engine that was designed with the large-scale processing of data in mind.</p>
<p>Through the advanced <strong>Directed Acyclic Graph</strong> (<strong><span>DAG</span></strong>) execution engine that supports <strong>cyclic data flow</strong> and in-memory computing, programs and scripts can run up to 100 times faster than Hadoop MapReduce in memory or 10 times faster on disk.</p>
<p class="mce-root"/>
<p>Spark consists of the following components:</p>
<ul>
<li><strong>Spark Core</strong>: This is the underlying engine of Spark, utilizing the fundamental programming abstraction called <strong>Resilient Distributed Datasets</strong> (<strong>RDDs</strong>). RDDs are <span>small logical chunks of data</span> Spark uses as "object collections".</li>
<li><strong>Spark SQL</strong>: This provides a new data abstraction called DataFrames for structured data processing using a distributed SQL query engine. It enables unmodified Hadoop Hive queries to run up to 100x faster on existing deployments and data.</li>
<li><strong>MLlib</strong>: This is Spark's built-in library of algorithms for mining big data, common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, and dimensionality reduction, as well as underlying optimization primitives that best support Spark.</li>
<li><strong>Streaming</strong>: This extends Spark's fast scheduling capability to perform real-time analysis on continuous streams of new data.</li>
<li><strong>GraphX</strong>: This is the graph processing framework for the analysis of graph structured data.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Watson Studio and Spark</h1>
                </header>
            
            <article>
                
<p>IBM Watson Studio offers certain Spark environments that you can use as default Spark environment definitions to quickly get started with Spark in Watson Studio without having to take the time to create your own Spark environment definitions. This saves setup time and allows you to spend your time creating solutions rather than administering an environment.</p>
<div class="packt_infobox">
<p>Spark environments are available by default for all Watson Studio users. You don't have to provision or associate any external Spark service with your Watson Studio project. You simply select the hardware and software configuration of the Spark runtime service you need to run your tool and then when you start the tool with the environment definition, a runtime instance is created based on your configuration specifications. The Spark compute resources are dedicated to your tool alone and not shared with collaborators—<a href="https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668">https://medium.com/ibm-watson/ibm-watson-studio-spark-environments-generally-available-f3dda78d3668</a>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Spark-enabled notebook</h1>
                </header>
            
            <article>
                
<p>To use Spark in Watson Studio, you need to create a notebook and associate a Spark version with it by performing the following steps:</p>
<ol>
<li>The steps to create the notebook are the same as we have followed in previous chapters. First, from within the project, locate the <span class="packt_screen">Notebook</span> section and click on <span class="packt_screen">New Notebook</span>. On the <span class="packt_screen">New notebook</span> page, provide a name and description:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/919a523f-e088-4d39-bbd3-aa153fbaa426.png" style=""/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Notice that, in the preceding screenshot, <span class="packt_screen">Python 3.5</span> is the selected language—this is fine but then if we scroll down, we will see <span class="packt_screen">Spark version*</span>. From the drop-down list, you can select the runtime environment for the notebook. For our example, we can select <span class="packt_screen">Default Spark Python 3.5 XS (Driver with 1 vCPU and 4GB, 2 executors with 1 vCPU and 4 GB RAM each)</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7c5ad35d-f1f4-4fa0-ad1c-fbb1f9b96b07.png" style=""/></div>
<ol start="3">
<li>Once you click on <span class="packt_screen">Create Notebook</span><span>, the notebook environment will be instanced and you will be ready to begin entering</span> Spark commands.</li>
<li>Once your Spark-enabled notebook is created, you can run Python commands and execute Spark jobs to process Spark SQL queries using DataFrame abstractions as a data source, as shown in the following example:</li>
</ol>
<pre style="padding-left: 60px">df_data_2.createOrReplaceTempView("station")<br/>sqlDF = spark.sql("SELECT * FROM station where VALUE &gt; 200")<br/>sqlDF.show()</pre>
<p>Don't pay too much attention to the actual code in the preceding example at this point as, in the next sections, we will use our Spark-enabled notebook to create a <strong>Spark ML pipeline</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Spark pipeline in Watson Studio</h1>
                </header>
            
            <article>
                
<p>So, let's start by understanding just what it is that we mean when we say <em>pipeline</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is a pipeline?</h1>
                </header>
            
            <article>
                
<p>An ML pipeline is characteristically used to <strong>automate</strong> ML workflows, essentially enabling sets of data to be transformed and correlated in a model that can then be tested and evaluated to achieve or estimate an outcome.</p>
<p>Such a workflow consists of four basic areas:</p>
<ul>
<li>Data preparation</li>
<li>Training set generation</li>
<li>Algorithm training/evaluation/selection </li>
<li>Deployment/monitoring</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipeline objectives</h1>
                </header>
            
            <article>
                
<p>A pipeline consists of a sequence of stages. There are two basic types of pipeline stages: <strong>transformer</strong> and <strong>estimator</strong>. As hinted in the <em>What is a pipeline?</em> section, a transformer takes a dataset as input and produces an augmented dataset as output, while an estimator abstracts the concept of a learning algorithm and implements a method that accepts data and produces a model.</p>
<p>Even more simply put, a pipeline executes a workflow that can repeatedly prepare new data (for transformation), transform the prepared data, and then train a model (on the prepared data). Another way to summarize is to think of a pipeline as the running of a sequence of algorithms to process and learn from data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking down a pipeline example</h1>
                </header>
            
            <article>
                
<p>We will start with stepping through the key steps in a Spark pipeline example available within the <strong>I</strong><strong>BM Watson Studio Community</strong> (<em>Use Spark and Python to predict equipment purchase</em>, submitted by Melanie Manley, July 26, 2018).</p>
<p class="mce-root"/>
<p>In this particular example, we see an existing Spark-enabled notebook that contains steps to load data, create a predictive model, and score data.</p>
<p>The example uses Spark commands to accomplish the tasks of loading data, performing data cleaning and exploration, creating a pipeline, training a model, persisting a model, deploying a model, and scoring a model; however, we will focus here only on the steps that create the Spark ML model. The reader may choose to view the entire example online.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>In this step, the data in a DataFrame object is split (using the Spark <kbd>randomSplit</kbd> command) into three—a <strong>training set</strong> (to be used to train a model), a <strong>testing set</strong> (to be used for model evaluation and testing the assumptions of the model), and a <strong>prediction set</strong> (used for prediction) and then a record count is printed for each set:</p>
<pre>splitted_data=df_data.randomSplit([0.8,0.18,0.02],24)<br/>train_data=splitted_data[0]<br/>test_data=splitted_data[1]<br/>predict_data=splitted_data[2]<br/>print("Number of training records: " + str(train_data.count())) print("Number of testing records : " + str(test_data.count())) print("Number of prediction records : " + str(predict_data.count()))</pre>
<p class="mce-root"/>
<p>Executing the preceding commands within the notebook is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0a8c51a4-5f62-4fac-a670-5757088da28f.png"/></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The pipeline</h1>
                </header>
            
            <article>
                
<p>Here, after we have created our three datasets, the Apache Spark ML pipeline will be created, and the model is trained by performing the following steps:</p>
<ol>
<li>First, you need to import the Apache Spark ML packages that will be needed in the subsequent steps:</li>
</ol>
<pre style="padding-left: 60px">frompyspark.ml.featureimportOneHotEncoder,StringIndexer,IndexToString,VectorAssembler<br/>frompyspark.ml.classification importRandomForestClassifier<br/>frompyspark.ml.evaluationimportMulticlassClassificationEvaluator<br/>from pyspark.ml import Pipeline, Model</pre>
<ol start="2">
<li>Next, the example uses the <kbd>StringIndexer</kbd> function as a transformer to convert all of the string columns into numeric ones:</li>
</ol>
<pre style="padding-left: 60px">stringIndexer_label=StringIndexer(inputCol="PRODUCT_LINE",outputCol="label").fit(df_data)<br/>stringIndexer_prof=StringIndexer(inputCol="PROFESSION",outputCol="PROFESSION_IX")<br/>stringIndexer_gend=StringIndexer(inputCol="GENDER",outputCol="GENDER_IX")<br/>stringIndexer_mar = StringIndexer(inputCol="MARITAL_STATUS", outputCol="MARITAL_STATUS_IX")</pre>
<ol start="3">
<li>In the following step, the example creates a feature vector by combining all features together:</li>
</ol>
<pre style="padding-left: 60px">vectorAssembler_features = VectorAssembler(inputCols=["GENDER_IX", "AGE", "MARITAL_STATUS_IX", "PROFESSION_IX"], outputCol="features")</pre>
<ol start="4">
<li>Next, the estimators that you want to use for classification are defined (<strong>random forest</strong> is used):</li>
</ol>
<pre style="padding-left: 60px">rf = RandomForestClassifier(labelCol="label", featuresCol="features")</pre>
<ol start="5">
<li>And finally, convert the indexed labels back into original labels:</li>
</ol>
<pre style="padding-left: 60px">labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=stringIndexer_label.labels)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>Now the actual pipeline is built:</li>
</ol>
<pre style="padding-left: 60px">pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter])</pre>
<p>At this point in the example, you are ready to train the random forest model by using the pipeline and training data you have just built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A data analysis and visualization example</h1>
                </header>
            
            <article>
                
<p>One of the most exciting advantages of using a Spark-enabled notebook within an IBM Watson Studio project is that all of the data explorations and subsequent visualizations can frequently be accomplished using just a few lines of (interactively written) code. In addition, the notebook interface allows a trial and error approach to running queries and commands, reviewing the results, and perhaps adjusting (the queries) and rerunning until you are satisfied (with the results).</p>
<p>Finally, notebooks and Spark can easily scale to deal with massive (GB and TB) datasets.</p>
<p>In this section, our objective is to use a Spark-enabled notebook to illustrate how certain tasks can be accomplished, such as loading data into the notebook, performing some simple data explorations, running queries (on the data), plotting, and then saving the results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setup</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's take a look at the following sections to understand the setup process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>First things first. We need data. Rather than make some up, we'll follow what several other working examples in the Watson Studio Community have done and download some collected data available publicly from the NOAA <strong>National Climatic Data Center</strong> (<strong>NCDC</strong>): <a href="http://www.ncdc.noaa.gov/data-access/quick-links">www.ncdc.noaa.gov/data-access/quick-links</a>.</p>
<p>Here's how to get the raw data from the NCDC:</p>
<ol>
<li>From the <strong>National Oceanic and Atmospheric Administration</strong> (<strong>NOAA</strong>) site, click on <span class="packt_screen">Global Historical Climatology Network (GHCN)</span>.</li>
<li>Click on <span class="packt_screen">GHCN-Daily FTP Access</span>.</li>
</ol>
<ol start="3">
<li>Click on the <span class="packt_screen">by_year/</span> folder.</li>
<li>Scroll to the bottom and click on <span class="packt_screen">2015.csv.gz</span> to download the dataset.</li>
<li>After the file has downloaded, extract it to an easily accessible location.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>Now we have a file of (although somewhat structured) still raw data. One typical first task when preparing data for analysis is to add column headings. If the file is of reasonable size, you can use a programmer's text editor to open and add a heading row, but if not, you can accomplish this directly in your Spark notebook.</p>
<p>Assuming you've loaded the file into your Watson project (using the process that we have shown in previous chapters), you can then click on <span class="packt_screen">Insert to code</span> and then select <span class="packt_screen">Insert pandas DataFrame</span> object as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b7b2adab-7b85-475e-8eef-9b2ec7533713.png" style=""/></div>
<p>When you click on <span class="packt_screen">Insert pandas DataFrame</span>, code is generated and added to the notebook for you. The generated code imports any required packages, accesses the data file (with the appropriate credentials), and loads the data into a DataFrame. You can then modify the <kbd>pd.read_csv</kbd> command (within the code) to include the <kbd>names</kbd> parameter (as shown in the following code).</p>
<p>This will create a heading row at the top of the file, using the provided column names:</p>
<pre>df_data_1 = pd.read_csv(body, sep=',',names = ['STATION', 'DATE', 'METRIC', 'VALUE', 'C5', 'C6', 'C7', 'C8'])</pre>
<p>Running the code in the cell is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/878e18f9-9149-4cf5-a1ef-f9a18bec8780.png"/></div>
<p>The raw data in the base file has the format shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6b370e3d-fcb2-4145-8c3f-d5fa86ca9942.png" style=""/></div>
<p>Hopefully, you can see that each column contains a weather station identifier, a date, a metric that is collected (such as precipitation, daily maximum and minimum temperatures, temperature at the time of observation, snowfall, snow depth, and so on) and some additional columns (note that missing values may show as <span class="packt_screen">NaN,</span> meaning <em>Not a Number</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploration</h1>
                </header>
            
            <article>
                
<p>As we demonstrated in <a href="630e47b4-11b7-4be9-a881-8be2cb492314.xhtml">Chapter 5</a>,<a href="630e47b4-11b7-4be9-a881-8be2cb492314.xhtml"/> <em>Machine Learning Workouts on IBM Cloud</em>, there is plenty of essential functionality common to the <kbd>pandas</kbd> data structures to support preprocessing and analysis of your data. In this example though, we are going look at examples of data explorations again but this time using Spark DataFrame methods.</p>
<p>For example, earlier we loaded a data file using <span class="packt_screen">Insert pandas DataFrame</span>; this time, we can reload that file using the same steps, but this time selecting <span class="packt_screen">Insert SparkSession DataFrame</span>. The code generated will include the <kbd>import ibmos2spark</kbd> and <kbd>from pyspark.sql import SparkSession</kbd> commands and load the data into <kbd>SparkSession DataFrame</kbd> (rather than a <kbd>pandas</kbd> DataFrame):</p>
<pre>import ibmos2spark<br/># @hidden_cell<br/>credentials = {<br/>   'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',<br/>    'service_id': 'iam-ServiceId-f9f1f892-3a72-4bdd-9d12-32b5a616dbfa',<br/>   'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',<br/>   'api_key': 'D2NjbuA02Ra3Pq6OueNW0JZZU6S3MKXOookVfQsKfH3L'<br/>}<br/>configuration_name = 'os_f20250362df648648ee81858c2a341b5_configs'<br/>cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')<br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.getOrCreate()<br/>df_data_2 = spark.read\<br/> .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\<br/> .option('header', 'true')\<br/> .load(cos.url('2015.CSV', 'chapter6-donotdelete-pr-qy3imqdyi8jv3w'))<br/>df_data_2.take(5)</pre>
<p>Running the cell initiates Spark jobs, shows a progress/status for those jobs, and, eventually, the output generated by the <kbd>.take(5)</kbd> command:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-412 image-border" src="assets/165781b8-c1a0-43bd-80b1-3f8d1a091f7a.png" style=""/></div>
<div class="packt_infobox"><kbd>SparkSession</kbd><span> is the entry point to Spark SQL. It is one of the very first objects you create while developing a Spark SQL application. As a Spark developer, you create </span><kbd>SparkSession</kbd><span> using the </span><kbd>SparkSession.builder</kbd><span> method (which gives you access to the </span><strong>Builder API</strong><span> that you use to configure the session).</span></div>
<p>Of course, we can also use <kbd>count()</kbd>, <kbd>first</kbd> as well as other statements:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-413 image-border" src="assets/1e19951a-7e73-4d01-8c19-cea09caeca9f.png" style=""/></div>
<p class="mce-root"/>
<p>Another interesting and handy analysis method is to show the schema of a DataFrame. You can use the <kbd>printSchema()</kbd> function to print out the schema for a <kbd>SparkR</kbd> DataFrame in a tree format, as follows:</p>
<pre>df_data_2.printSchema()</pre>
<p>The preceding command yields the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/65a0b7cd-fdc2-440d-bf7c-606ddf996efe.png" style=""/></div>
<div class="packt_infobox">
<p>A <strong>schema</strong> is the description of the structure of the data. A schema is described using <kbd>StructType</kbd>, which is a collection of the <kbd>StructField</kbd> objects (that in turn are tuples of names, types, and nullability classifiers).</p>
</div>
<p>Using a Spark DataFrame also provides you with the ability to navigate through the data and apply logic. For example, it's not unreasonable or unexpected to want to look at the first two (or first few) rows of your data by running the <kbd>print</kbd> command; however, for readability, you might want to add a row of asterisks in between the data rows by using the following code:</p>
<pre>for row in df_data_2.take(2):<br/>    print(row)<br/>    print( "*" * 104)</pre>
<p class="mce-root"/>
<p>The preceding code generates the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8355e0a9-4edb-4525-bd63-21e3e543a8e2.png"/></div>
<p>Suppose you are interested in using your SQL skills to perform your analysis?</p>
<p>No problem! You can use <strong>SparkSQL</strong> with your <kbd>SparkSession</kbd> DataFrame object.</p>
<p>However, all SQL statements must be run against a table, so you need to define a table that acts like a <strong>pointer</strong> to the DataFrame (after you import the <kbd>SQLContext</kbd> module):</p>
<pre>from pyspark.sql import SQLContext<br/>sqlContext = SQLContext(sc)<br/>df_data_2.registerTempTable("MyWeather")</pre>
<p>Additionally, you'll need to define a new DataFrame object to hold the results of your SQL query and put the SQL statement inside the <kbd>sqlContext.sql()method</kbd>. Let's see how that works.</p>
<p>You can run the following cell to select all columns from the table we just created and then print information about the resulting DataFrame and schema of the data:</p>
<pre>temp_df =  sqlContext.sql("select * from MyWeather")<br/>print (type(temp_df))<br/>print ("*" * 104)<br/>print (temp_df)</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7ef6afc8-439e-4a1a-8bfc-7e6afed09500.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extraction</h1>
                </header>
            
            <article>
                
<p>Now let's move on to the concept of <strong>extraction</strong>. The <kbd>print </kbd>command doesn't really show the data in a very useful format. So, instead of using our Spark DataFrame, we could use the <kbd>pandas</kbd> open source data analytics library to create a <kbd>pandas</kbd> DataFrame that shows the data in a table.</p>
<p>Now we can look at an example that will make our SQL coders happy.</p>
<p>Import the <kbd>pandas</kbd> library and use the <kbd>.toPandas()</kbd> method to show the SQL query results:</p>
<pre>import pandas as pd<br/>sqlContext.sql("select STATION, METRIC from MyWeather limit 2").toPandas()</pre>
<p>Running the preceding commands results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/acbb7e69-3b8a-48c8-b6b7-8199d99608b3.png"/></div>
<p>Here is another example of simple SQL query execution, this time counting the number of metrics recorded for each weather station and then creating a list of the weather stations ordered by the number of metric records for the weather station:</p>
<pre>query = """<br/>select<br/>    STATION ,<br/>    count(*) as metric_count<br/>from MyWeather<br/>group by STATION<br/>order by count(*) desc<br/>"""<br/>sqlContext.sql(query).toPandas()</pre>
<p class="mce-root"/>
<p>The preceding code gives us the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/61c37792-fcfd-473a-a56e-eca4c3e377be.png"/></div>
<p>You are encouraged to experiment with additional variations of SQL statements and then review the results in real time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting</h1>
                </header>
            
            <article>
                
<p>So, let's move along!</p>
<p>We will now take a look at plotting some of the data we collected in our Spark DataFrame. You can use <kbd>matplotlib</kbd> and <kbd>pandas</kbd> to create almost an endless number of visualizations (once you understand your data well enough).</p>
<p>You may even find that, once you reach this point, generating visualizations is quite easy but then you can spend an almost endless amount of time getting them clean and ready to share with others.</p>
<p>We will now look at a simple example of how this process might go.</p>
<p class="mce-root"/>
<p>Starting with the Spark DataFrame from the previous section, suppose that we think that it would be nice to generate a simple bar chart based upon the <kbd>DATE</kbd> field within our data. So, to get going, we can use the following code to come up with a count by <kbd>DATE</kbd>:</p>
<pre>df_data_2.groupBy("DATE").count().show()<br/>df_data_2.groupBy("DATE").count().collect()</pre>
<p>The results of running the preceding code are shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/83893c7d-6853-45b7-8fd5-5a0b748607f7.png"/></div>
<p>We might say that the output generated seems somewhat reasonable (at least at first glance), so the next step would be to use the following code to construct a matrix of data formatted in a way that can easily be plotted:</p>
<pre>count = [item[1] for item in df_data_2.groupBy("DATE").count().collect()]<br/>year = [item[0] for item in df_data_2.groupBy("DATE").count().collect()]<br/>number_of_metrics_per_year = {"count":count, "DATE" : year}<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>number_of_metrics_per_year = pd.DataFrame(number_of_metrics_per_year )<br/>number_of_metrics_per_year .head()</pre>
<p class="mce-root"/>
<p>Running this code and looking at the output generated seems perfectly reasonable and in line with our goal:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f30ebc5e-8266-4d63-a266-2ec8aff04c87.png"/></div>
<p>So, great! If we got to this point, we would think that we are ready to plot and visualize the data, so we can go ahead and use the following code to create a visualization:</p>
<pre>number_of_metrics_per_year = number_of_metrics_per_year.sort_values(by = "DATE")<br/>number_of_metrics_per_year.plot(figsize = (20,10), kind = "bar", color = "red", x = "DATE", y = "count", legend = False)<br/>plt.xlabel("", fontsize = 18)<br/>plt.ylabel("Number of Metrics", fontsize = 18)<br/>plt.title("Number of Metrics Per Date", fontsize = 28)<br/>plt.xticks(size = 18)<br/>plt.yticks(size = 18)<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>After running the preceding code, we can see that the code worked (we have generated a visualization based upon our data) yet the output isn't quite as useful as we might have hoped:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/329d03bd-fbbf-4861-8c63-5e05a0df8bf0.png"/></div>
<p>It's pretty messy and not very useful!</p>
<p>So, let's go back and try to reduce the volume of data we are trying to plot. Thankfully, we can reuse some of the code from the previous sections of this chapter.</p>
<p>We can start by again setting up a temporary table that we can query:</p>
<pre>from pyspark.sql import SQLContext<br/>sqlContext = SQLContext(sc)<br/>df_data_2.registerTempTable("MyWeather")</pre>
<p>Then, we can create a temporary DataFrame to hold our results (<kbd>temp_df</kbd>). The query can only load records where <kbd>METRIC</kbd> collected is <kbd>PRCP</kbd> and <kbd>VALUE</kbd> is greater than <kbd>500</kbd>:</p>
<pre>temp_df =  sqlContext.sql("select * from MyWeather where METRIC = 'PRCP' and VALUE&gt;500")<br/>print (temp_df)<br/>temp_df.count()</pre>
<p>This should significantly limit the number of data records to be plotted.</p>
<p>Now we can go back and rerun our codes that we used to create the data matrix to be plotted as well as the actual plotting code but, this time, using the temporary DataFrame:</p>
<pre>temp_df.groupBy("DATE").count().show()<br/>temp_df.groupBy("DATE").count().collect()<br/>count = [item[1] for item in temp_df.groupBy("DATE").count().collect()]<br/>year = [item[0] for item in temp_df.groupBy("DATE").count().collect()]<br/>number_of_metrics_per_year = {"count":count, "DATE" : year}<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>number_of_metrics_per_year = pd.DataFrame(number_of_metrics_per_year )<br/>number_of_metrics_per_year .head()<br/>number_of_metrics_per_year = number_of_metrics_per_year.sort_values(by = "DATE")<br/>number_of_metrics_per_year.plot(figsize = (20,10), kind = "bar", color = "red", x = "DATE", y = "count", legend = False)<br/>plt.xlabel("", fontsize = 18)<br/>plt.ylabel("Number of Metrics", fontsize = 18)<br/>plt.title("Number of Metrics Per Date", fontsize = 28)<br/>plt.xticks(size = 18)<br/>plt.yticks(size = 18)<br/>plt.show()</pre>
<p>So now we have a different, maybe somewhat better, result but one that is probably still not ready to be shared:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a196b381-1732-4c62-91de-ddba4755af39.png" style=""/></div>
<p>If we continue using the preceding strategy, we could again modify the SQL query to further restrict or filter the data as follows:</p>
<pre>temp_df =  sqlContext.sql("select * from MyWeather where METRIC = 'PRCP' and VALUE &gt; 2999")<br/>print (temp_df)<br/>temp_df.count()</pre>
<p>And then we can review the resulting temporary DataFrame and see that now it has a lot fewer records:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2102e25-730f-443a-b89b-69db68df2976.png"/></div>
<p class="mce-root"/>
<p>If we now proceed with rerunning the rest of the plotting code, we see that it yields a slightly better (but still not acceptable) plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8fc00e82-e2a0-447c-9e5c-c21316eb123d.png"/></div>
<p>We could, in fact, continue this process of trial and error by modifying the SQL, rerunning the code, and then reviewing the latest results until we are happy with what we see, but you should have the general idea, so we will move on at this point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving</h1>
                </header>
            
            <article>
                
<p>Just like when working in Microsoft Word or Microsoft Excel, it is always a good idea to periodically save your work.</p>
<p>In fact, you may even want to save multiple versions of your work, as you continue to evolve it, in case you want to revert back at some point. While evolving your scripts, you can click on <span class="packt_screen">File</span>, then <span class="packt_screen">Save</span> or <span class="packt_screen">Save Version</span> to keep appropriate copies of your notebook:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ba01e425-d796-4454-bf20-5dae9a3372fc.png"/></div>
<p>You can also save and share read-only copies of your notebooks <em>even outside of Watson Studio</em> so that others who aren't collaborators in your Watson Studio projects can see and download them. You can do this in the following ways:</p>
<ul>
<li><strong>Share a URL on social media</strong>:<strong> </strong>You can create a URL to share the last saved version of a notebook on social media or with people outside of Watson Studio.</li>
<li><strong>Publish on GitHub</strong>:<strong> </strong>To support collaboration with stakeholders and the data science community at large, you can publish your notebooks in GitHub repositories.</li>
<li><strong>Publish as a gist</strong>:<strong> </strong>All project collaborators who have administrator or editor permission can share notebooks or parts of a notebook as gists. The latest saved version of your notebook is published as a gist.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading your notebook</h1>
                </header>
            
            <article>
                
<p>You can download your notebook as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f55f4c74-259e-43f8-8e97-95d2c8c74c61.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced both Spark and how to create a Spark-enabled notebook in IBM Watson Studio, as well as the concept of an ML pipeline.</p>
<p>Finally, we finished up with a practical example of using Spark to perform data analysis and create a visualization to tell a better story about the data.</p>
<p>In the next chapter, we will provide an introduction to deep learning and neural networks on IBM Cloud.</p>


            </article>

            
        </section>
    </body></html>