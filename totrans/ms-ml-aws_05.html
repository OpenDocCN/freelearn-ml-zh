<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting House Value with Regression Algorithms</h1>
                </header>
            
            <article>
                
<p>This chapter will introduce the basics of regression algorithms and apply them to predict the price of houses given a number of features. We'll also introduce how to use logistic regression for classification problems. Examples in SageMaker Notebooks for scikit-learn,  Apache Spark, and SageMaker's linear learner will be provided.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Predicting the price of houses</li>
<li>Understanding linear regression</li>
<li>Evaluating regression models</li>
<li>Implementing linear regression through scikit-learn</li>
<li><span>Implementing </span>linear regression through Apache Spark</li>
<li><span>Implementing </span>linear regression through SageMaker's linear learner</li>
<li><span>Understanding </span>logistic regression</li>
<li>Pros and cons of linear models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting the price of houses</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will consider the problem of trying to predict the value of houses in Boston's suburbs based on a number of variables, such as the number of rooms and house age. The details of the dataset can be found here: <a href="https://www.kaggle.com/c/boston-housing/">https://www.kaggle.com/c/boston-housing/</a>. This problem is different to the one we considered in the last chapter, as the variable we're trying to predict (price in dollars) is continuous. Models that are able to predict continuous quantities are called <strong>regressors</strong>, or <strong>regression algorithms</strong>. There are many such algorithms, but in this chapter, we will focus on the simplest (but very popular) kind, linear regressors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding linear regression</h1>
                </header>
            
            <article>
                
<p>Regression algorithms are an important algorithm in a data scientist's toolkit as they can be used for various non-binary prediction tasks. The linear regression algorithm models the relationship between a dependent variable that we are trying to predict with a vector of independent variables. The vector of variables is also called the regressor in the context of regression algorithms. Linear regression assumes that there is a linear relationship between the vector of independent variables and the dependent variable that we are trying to predict. Hence, linear regression models learn the unknown variables and constants of a  linear function using the training data, such that the linear function best fits the training data. </p>
<p>Linear regression can be applied in cases where the goal is to predict or forecast the dependent variable based on the regressor variables. We will use an example to explain how linear regression trains based on data.</p>
<p>The following table shows a sample dataset where the goal is to predict the price of a house based on three variables:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Floor Size</strong></td>
<td><strong>Number of Bedrooms</strong></td>
<td><strong>Number of Bathrooms</strong></td>
<td><strong>House Price</strong></td>
</tr>
<tr>
<td>2500</td>
<td>4</td>
<td>2</td>
<td>6,00,000</td>
</tr>
<tr>
<td>2800</td>
<td>4</td>
<td>2</td>
<td>6,50,000</td>
</tr>
<tr>
<td>2700</td>
<td>4</td>
<td>3</td>
<td>6,50,000</td>
</tr>
<tr>
<td>4500</td>
<td>6</td>
<td>4</td>
<td>8,00,000</td>
</tr>
<tr>
<td>3500</td>
<td>4</td>
<td>2</td>
<td>7,50,000</td>
</tr>
<tr>
<td>3000</td>
<td>5</td>
<td>4</td>
<td>7,60,000</td>
</tr>
<tr>
<td>2000</td>
<td>3</td>
<td>2</td>
<td>5,00,000</td>
</tr>
<tr>
<td>4100</td>
<td>4</td>
<td>3</td>
<td>8,10,000</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"/>
<p>In this dataset, the variables <kbd><span>Floor</span><span> </span><span>Size</span></kbd><span>, <kbd>Number of Bedrooms</kbd>, and <kbd>Number of Bathrooms</kbd></span> <span>are assumed as independent in linear regression. Our goal is to predict the <kbd>House Price</kbd> value based on the variables. </span></p>
<p>Let's simplify this problem. Let's only consider the <kbd>Floor Size</kbd> variable to predict the house price. Creating linear regression from only one variable or regressor is referred to as a <strong>simple linear regression</strong>. If we create a scatterplot from the two columns, we can observe that there is a relationship between these two variables:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-693 image-border" src="assets/1cb29401-fe91-43e0-a51a-402e0429c16c.png" style="width:111.33em;height:57.92em;"/></p>
<p>Although there is not an exact linear relationship between the two variables, we can create an approximate line that represents the trend. The aim of the modeling algorithm is to minimize the error in creating this approximate line. </p>
<p>As we know, a straight line can be represented by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/390d4767-5122-40f8-a3c8-476d8a0a0cad.png" style="width:7.92em;height:1.50em;"/>                              </p>
<p>Hence, the approximately linear relationship in the preceding diagram can also be represented using the same formula, and the task of the linear regression model is to learn the value of <img class="fm-editor-equation" src="assets/61c5c848-e1f1-4789-8178-51a9ddf7f18b.png" style="width:1.58em;height:1.58em;"/> and <img class="fm-editor-equation gr-progress" src="assets/e922631d-d30b-4eb4-ab0b-26fe5423638e.png" style="width:1.58em;height:1.58em;"/>. Moreover, since we know that the relationship between the predicted variable and the regressors is not strictly linear, we can add a random error variable to the equation that models the noise in the dataset. The following formula represents how the simple linear regression model is represented:</p>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="assets/74c7bb58-49d6-4576-b25d-85441bc46446.png" style="width:9.50em;height:1.42em;"/>                     </span></p>
<p>Now, let's consider the dataset with multiple regressors. Instead of just representing the linear relationship between one variable <img class="fm-editor-equation" src="assets/29cd6f71-77bd-4a79-86e3-e33d692957a3.png" style="width:0.75em;height:0.75em;"/> and <img class="fm-editor-equation" src="assets/72700c1f-4a44-4d77-951b-289c1b39b237.png" style="width:0.58em;height:1.00em;"/>, we will represent a set of regressors as <img class="fm-editor-equation" src="assets/5d2d71b2-4d09-4ef3-aa4b-4e81e2f823f6.png" style="width:8.33em;height:1.25em;"/>. We will assume that a linear relationship between the dependent variable <img class="fm-editor-equation" src="assets/0512f8a2-dfe9-4c7d-9962-3bb9fd40740c.png" style="width:0.42em;height:0.67em;"/> and the regressors <img class="fm-editor-equation" src="assets/74710d03-6a1e-4ce6-b62d-4a67656f1032.png" style="width:0.83em;height:0.83em;"/> is linear.  Thus, a linear regression model with multiple regressors is represented by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4b62e289-abd2-47cd-bf76-aa49a39f1167.png" style="width:10.50em;height:3.00em;"/>       </p>
<p>Linear regression, as we've already discussed, assumes that there is a linear relationship between the regressors and the dependent variable that we are trying to predict. This is an important assumption that may not hold true in all datasets. Hence, for a data scientist, using linear regression may look attractive due to its fast training time. However, if the dataset variables do not have a linear relationship with the dependent variable, it may lead to significant errors. In such cases, data scientists may also try algorithms such as Bernoulli regression, Poisson regression, or multinomial regression to improve prediction precision. We will also discuss logistic regression later in this chapter, which is used when the dependent variable is binary. </p>
<p>During the training phase, linear regression can use various techniques for parameter estimation to learn the values of <img class="fm-editor-equation" src="assets/66b07061-2818-4fee-a27a-9611da35a28a.png" style="width:1.17em;height:1.17em;"/>, <img class="fm-editor-equation" src="assets/10ceb4e2-3cf4-4a4c-9220-a0d395cd4fc4.png" style="width:1.25em;height:1.25em;"/>, and <img class="fm-editor-equation" src="assets/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png" style="width:0.75em;height:1.00em;"/>. We will not go into the details of these techniques in this book. However, we recommend that you try using these parameter estimation techniques in the examples that follow and observe their effect on the training time of the algorithm and the accuracy of prediction. </p>
<p>To fit a linear model to the data, we first need to be able to determine how well a linear model fits the data. There are various models being developed for parameter estimation in linear regression. Parameter estimation is the process of estimating the values of <img class="fm-editor-equation" src="assets/adbf97e1-95a4-4ce9-96f8-cbd27236a730.png" style="width:1.17em;height:1.17em;"/><span>, </span><img class="fm-editor-equation" src="assets/5acf773a-ee58-407c-8123-8e582436f4d4.png" style="width:1.08em;height:1.08em;"/><span>, </span>and <img class="fm-editor-equation" src="assets/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png" style="width:0.75em;height:1.00em;"/>. In the following sections, will briefly explain these two estimation techniques. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear least squares estimation</h1>
                </header>
            
            <article>
                
<p><strong>Linear least squares</strong> (<strong>LLS</strong>) is an estimation approach that's used to estimate parameters based on the given data. The optimization problem of the LLS estimation can be explained as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/361d26cb-e963-4ee3-827e-341a7e63a582.png" style="font-size: 1em;width:18.00em;height:3.75em;"/><span>             </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>LLS is a set of formulations that are used to get solutions to the statistical problem of linear regression by estimating the values of <img class="fm-editor-equation" src="assets/5449879e-b6a5-4394-bfe4-8086ff5b67d7.png" style="width:1.33em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/17003f09-11c2-4ff3-85d4-c07326491423.png" style="width:1.17em;height:1.17em;"/>. LLS is an optimization methodology for getting solutions for linear regression. It uses the observed values of <em>x</em> and <em>y</em> to estimate the values of <img class="fm-editor-equation" src="assets/903e175a-5148-410d-8744-acfac1205906.png" style="width:1.25em;height:1.25em;"/> and <img class="fm-editor-equation" src="assets/b07ede01-7c14-4fd5-a73f-ef2ff3d729b1.png" style="width:1.33em;height:1.33em;"/>. We encourage you to explore LLS solutions to understand how it estimates the linear regression parameters. However, as the focus of this book is to introduce you to these concepts and help you apply them in AWS, we won't go into detail about this methodology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum likelihood estimation</h1>
                </header>
            
            <article>
                
<p><strong>Maximum likelihood estimation</strong> (<strong>MLE</strong>) is a popular model that's used for estimating the parameters of linear regression. MLE is a probabilistic model that can predict what values of the parameters have the maximum likelihood to recreate the observed dataset. This is represented by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c5e14e2a-58d9-4694-b775-dc55ae8e725f.png" style="width:23.33em;height:3.33em;"/>                           </p>
<p>For linear regression, our assumption is that the dependent variable has a linear relationship with the model. MLE assumes that the dependent variable values have a normal distribution. The idea is to predict the parameters for each observed value of <em>X</em> so that it models the value of <em>y</em>. We also estimate the error for each observed value that models how different the linear predicted value of <em>y</em> is from the actual value. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient descent </h1>
                </header>
            
            <article>
                
<p>The <strong>gradient descent algorithm</strong> is also popular for estimating parameters for linear regression. The gradient descent algorithm is used to minimize a function. Based on what we are predicting, we start with a set of initial values for the parameters and iteratively move toward the parameters to minimize the error in the function. The function to iteratively make steps in minimizing error is called <strong>gradient</strong>. The idea is to descend the gradient toward the lowest point in the gradient plane. Different types of gradient descent algorithms include <strong>batch gradient descent</strong>, which looks at all observed examples in each example, and <strong>stochastic gradient descent</strong>, where we iterate with only one observation at a time. For this reason, batch gradient descent is more accurate than stochastic gradient descent, but is much slower and hence not suitable for larger datasets.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There is a vast amount of research being done on regression algorithms as it is very well suited for predicting continuous variables. We encourage you to learn more about linear regression libraries and try different variants that are provided in the library to calculate the efficiency and effectiveness of the test datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating regression models </h1>
                </header>
            
            <article>
                
<p>Unlike the Naive Bayes classification model, the regression model provides a numerical output as a prediction. This output can be used for binary classification by predicting the value for both the events and using the maximum value. However, in examples such as predicting a house value based on regressors, we cannot use evaluation metrics that rely on just predicting whether we got the answer correct or incorrect. When we are predicting a numerical value, the evaluation metrics should also quantify the value of error in prediction. For example, if the house value is 600,000 and model A predicts it as 700,000 and model B predicts it as 1,000,000, metrics such as precision and recall will count both these predictions as false positives. However, for regression models, we need evaluation metrics that can tell us that model A was closer to the actual value than model B. Therefore, in this section, we will present three metrics that are used for such numerical predictions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean absolute error</h1>
                </header>
            
            <article>
                
<p><strong>Mean absolute error</strong> (<strong>MAE</strong>) is the mean of the absolute values of the error. It can be represented with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3bec4000-89bd-4e6b-a10a-813b45e5612b.png" style="width:11.25em;height:3.25em;"/>                              </p>
<p>MAE provides an average error between two vectors. In our case, MAE is the difference between the actual value of <img class="fm-editor-equation" src="assets/5d6873fe-60a7-4217-84a2-18731963b018.png" style="width:0.58em;height:1.00em;"/> and the predicted value <img class="fm-editor-equation" src="assets/013d9a96-4fc4-4978-b441-fa57474b633d.png" style="width:0.67em;height:1.25em;"/>. MAE is used by a lot of researchers since it gives a clear interpretation of the errors in the model's prediction. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p><strong>Mean squared error</strong> (<strong>MSE</strong>) is the mean of squares of the error values and is represented by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d46f802d-b3e3-40e2-ba85-944de7e64c52.png" style="width:12.00em;height:3.33em;"/>                             </p>
<p>MSE is useful in cases where the errors are very small. MSE incorporates both how far the predicted values are from the truth and also the variance in the predicted values. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Root mean squared error</h1>
                </header>
            
            <article>
                
<p><strong>Root mean squared error</strong> (<strong>RMSE</strong>) is the square root of the mean squared errors and is represented by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dd3c8b98-9636-47f9-8c90-3c38d0b29d1c.png" style="width:11.75em;height:3.08em;"/></p>
<p>RMSE, similar to MSE, captures the variance in predictions. However, in RMSE, since we take the square root of the squared error values, the error can be comparable to MSE, and also keep the advantages of MSE. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">R-squared</h1>
                </header>
            
            <article>
                
<p>Another popular metric that's used in regression problems is the R-squared score, or coefficient of determination. This score measures the proportion of the variance in the dependent variable that is predictable from the independent variables:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d15c5172-eac3-4a23-b250-b7e8793556d7.png" style="width:14.92em;height:3.50em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/b783ac00-5d51-4d02-991d-d8798c4930f4.png" style="width:0.58em;height:1.00em;"/> represents the vector of actual values, while <img class="fm-editor-equation" src="assets/ad5e1d74-9a9e-49c0-9446-1be879d533f9.png" style="width:1.25em;height:1.25em;"/> and <img class="fm-editor-equation" src="assets/59ae26e6-30e8-49e1-8f2d-a387f03b4e89.png" style="width:1.17em;height:1.42em;"/><span> </span>represents the vector of predicted values. The mean actual value is <img class="fm-editor-equation" src="assets/2be42d23-e4da-4220-86f9-0720ba123610.png" style="width:0.67em;height:1.17em;"/>. The denominator of the quotient measures how actual values typically differ from the<span> </span>mean, while<span> </span>the numerator measures how actual values differ from predicted values. Note that differences are squared, similar to MSE, and so large differences are penalized heavily.</p>
<p><span> In a perfect regressor, the numerator is 0, so the best possible value for <em>R<sup>2</sup></em> is 1.0. However, we can see arbitrarily large negative values when the prediction errors are significant. </span></p>
<p>All four types of evaluation metrics are implemented in machine learning packages and are demonstrated in the following code examples. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing linear regression through scikit-learn</h1>
                </header>
            
            <article>
                
<p>Like we did in the previous chapter, we will show you how you can quickly use <kbd>scikit-learn</kbd> to train a linear model straight from a SageMaker notebook instance. First, you must create the notebook instance (choosing <kbd>conda_python3</kbd> as the kernel).</p>
<ol>
<li>We will start by loading the training data into a <kbd>pandas</kbd> dataframe:</li>
</ol>
<pre style="padding-left: 60px">housing_df = pd.read_csv(SRC_PATH + 'train.csv')<br/>housing_df.head()</pre>
<p style="padding-left: 60px"><span>The preceding code displays the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-953 image-border" src="assets/2b9aefdc-61cc-4527-bd9d-2f68436c2cb6.png" style="width:44.42em;height:9.58em;"/></p>
<ol start="2">
<li>The last column <kbd>(medv)</kbd> stands for median value and represents the variable that we're trying to predict (dependent variable) based on the values from the remaining columns (independent variables).</li>
</ol>
<p style="padding-left: 60px">As usual, we will split the dataset for training and testing:</p>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/><br/>housing_df_reordered = housing_df[[label] + training_features]<br/><br/>training_df, test_df = train_test_split(housing_df_reordered, <br/>                                        test_size=0.2)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li class="mce-root">Once we have these datasets, we will proceed to construct a linear regressor:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.linear_model import LinearRegression<br/><br/>regression = LinearRegression()<br/><br/>training_features = ['crim', 'zn', 'indus', 'chas', 'nox', <br/>                     'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']<br/><br/>model = regression.fit(training_df[training_features], <br/>                       training_df['medv'])</pre>
<p style="padding-left: 60px">We start by constructing an estimator (in this case, linear regression) and fit the model by providing the matrix of training values, <kbd>(training_df[training_features])</kbd>, and the labels, <kbd>(raining_df['medv'])</kbd>.</p>
<ol start="4">
<li>After fitting the model, we can use it to get predictions for every row in our testing dataset. We do this by appending a new column to our existing testing dataframe:</li>
</ol>
<pre style="padding-left: 60px">test_df['predicted_medv'] = model.predict(test_df[training_features])<br/>test_df.head()</pre>
<p style="padding-left: 60px"><span>The preceding code displays the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-954 image-border" src="assets/ea8ce6ac-3ed3-4055-b45d-3b8a0c45d1e7.png" style="width:129.83em;height:27.58em;"/></p>
<ol start="5">
<li>It's always useful to check our predictions graphically. One way to do this is by plotting the predicted versus actual values as a scatterplot:</li>
</ol>
<pre style="padding-left: 60px">test_df[['medv', 'predicted_medv']].plot(kind='scatter', <br/>                                         x='medv', <br/>                                         y='predicted_medv')</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The preceding code displays the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-955 image-border" src="assets/92701c81-74e6-44c5-bf9c-037824d9288f.png" style="width:28.08em;height:19.25em;"/></p>
<p style="padding-left: 60px">Note how the values are located mostly on the diagonal. This is a good sign, as a perfect regressor would yield all data points exactly on the diagonal (every predicted value would be exactly the same as the actual value).</p>
<ol start="6">
<li>In addition to this graphical verification, we obtain an evaluation metric that tells us how good our model is at predicting the values. In this example, we use R-squared evaluation metrics, as explained in the previous section, which is available in scikit-learn.</li>
</ol>
<p style="padding-left: 60px">Let's look at the following code block:</p>
<pre style="padding-left: 60px">from sklearn.metrics import r2_score<br/><br/>r2_score(test_df['medv'], test_df['predicted_medv'])<br/><br/>0.695</pre>
<p class="mce-root">A value near 0.7 is a decent value. If you want to get a sense of what a good R2 correlation is, we recommend you play this game: <a href="http://guessthecorrelation.com/">http://guessthecorrelation.com/</a>.</p>
<p class="mce-root">Our linear model will create a predicted price by multiplying the value of each feature by a coefficient and adding up all these values, plus an independent term, or intercept.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">We can find the values of these coefficients and intercept by accessing the data members in the model instance variable:</p>
<pre>model.coef_<br/><br/><strong>array([-7.15121101e-02, 3.78566895e-02, -4.47104045e-02, 5.06817970e+00,</strong><br/><strong>      -1.44690998e+01, 3.98249374e+00, -5.88738235e-03, -1.73656446e+00,</strong><br/><strong>       1.01325463e-03, -6.18943939e-01, -6.55278930e-01])</strong><br/><br/>model.intercept_<br/><strong>32.20</strong></pre>
<p class="mce-root">It is usually very convenient to examine the coefficients of the different variables as they can be indicative of the relative importance of the features in terms of their independent predictive ability. </p>
<p class="mce-root">By default, most linear regression algorithms such as <kbd>scikit-learn</kbd> or Spark will automatically do some degree of preprocessing (for example, it will scale the variables to prevent features with large values to introduce bias). Additionally, these algorithms support regularization parameters and provide you with options to choose the optimizer that's used to efficiently search for the coefficients that maximize the R2 score (or minimize some loss function).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing linear regression through Apache Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">You are likely interested in training regression models that can take huge datasets as input, beyond what you can do in <kbd>scikit-learn</kbd>. Apache Spark is a good candidate for this scenario. As we mentioned in the previous chapter, Apache Spark can easily run training algorithms on a cluster of machines using <strong>Elastic MapReduce</strong> (<strong>EMR</strong>) on AWS. We will explain how to set up EMR clusters in the next chapter. In this section, we'll explain how you can use the Spark ML library to train linear regression algorithms.</p>
<ol>
<li class="mce-root">The first step is to create a dataframe from our training data:</li>
</ol>
<pre style="padding-left: 60px">housing_df = sql.read.csv(SRC_PATH + 'train.csv', header=True, inferSchema=True)</pre>
<p class="mce-root" style="padding-left: 60px">The following image shows the first few rows of the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-956 image-border" src="assets/78d39e07-ec5d-43e1-a0a8-ba2db1f2e68a.png" style="width:45.92em;height:8.92em;"/></p>
<ol start="2">
<li class="mce-root">Typically, Apache Spark requires the input dataset to have a single column with a vector of numbers representing all the training features. In <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em><span>,we used the</span> <kbd>CountVectorizer</kbd> <span>to create such a column. In this chapter, since the vector values are already available in our dataset, we just need to construct such a column using a</span> <kbd>VectorAssembler</kbd> <span>transformer:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from pyspark.ml.feature import VectorAssembler<br/><br/>training_features = ['crim', 'zn', 'indus', 'chas', 'nox', <br/>                     'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']<br/><br/>vector_assembler = VectorAssembler(inputCols=training_features, <br/>                                   outputCol="features")<br/><br/>df_with_features_vector = vector_assembler.transform(housing_df)</pre>
<p class="mce-root" style="padding-left: 60px">The following screenshot shows the first few rows of the df_with_features_vector dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-957 image-border" src="assets/9f1ce736-58b2-4102-9548-b7caba902035.png" style="width:155.83em;height:23.17em;"/></p>
<p class="mce-root" style="padding-left: 60px">Note how the vector assembler created a new column called features, which assembles all the features that are used for training as vectors.</p>
<ol start="3">
<li class="mce-root">As usual, we will split our dataframe into testing and training:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">train_df, test_df = df_with_features_vector.randomSplit([0.8, 0.2], <br/>                                                        seed=17)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li class="mce-root">We can now instantiate our regressor and fit a model:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from pyspark.ml.regression import LinearRegression<br/><br/>linear = LinearRegression(featuresCol="features", labelCol="medv")<br/>linear_model = linear.fit(train_df)</pre>
<ol start="5">
<li class="mce-root">By using this model, we find predictions for each value in the test dataset:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">predictions_df = linear_model.transform(test_df)<br/>predictions_df.show(3)</pre>
<p class="mce-root" style="padding-left: 60px">The output of the above <kbd>show()</kbd> command is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-958 image-border" src="assets/b11e61a1-744c-4ce8-9428-1c7b61141f8d.png" style="width:182.58em;height:20.58em;"/></p>
<ol start="6">
<li class="mce-root">We can easily find the <kbd>R2</kbd> value by using a <kbd>RegressionEvaluator</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from pyspark.ml.evaluation import RegressionEvaluator<br/><br/>evaluator = RegressionEvaluator(labelCol="medv", <br/>                                predictionCol="prediction", <br/>                                metricName="r2")<br/>evaluator.evaluate(predictions_df)</pre>
<p class="mce-root" style="padding-left: 60px">In this case, we get an <kbd>R2</kbd> of <kbd>0.688</kbd>, which is a similar result to that of <kbd>scikit-learn</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing linear regression through SageMaker's linear Learner</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another alternative within AWS for training regression models is to use SageMaker's API to build linear models. In the previous chapter, we explained the basics of this service when we considered how to use BlazingText for our text classification problem. Similarly, we will use Linear Learners in this section and go through the same process, which basically entails three steps:</p>
<ol>
<li>Stage the training and testing data in S3</li>
<li>Invoke the API to train the model</li>
<li>Use the model to obtain predictions</li>
</ol>
<p class="mce-root">Unlike what we did in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em>, instead of deploying an endpoint (that is, a web service) to obtain predictions, we will use a batch transformer, which is a service that's capable of obtaining bulk predictions given a model and some data in S3. Let's take a look at the following steps:</p>
<ol>
<li class="mce-root">Assuming that we have prepared the training and testing datasets in a similar way to the previous sections, we will create a SageMaker session and upload our training and testing data to S3:</li>
</ol>
<pre style="padding-left: 60px">import sagemaker<br/>from sagemaker import get_execution_role<br/>import json<br/>import boto3<br/><br/>sess = sagemaker.Session()<br/>role = get_execution_role()<br/><br/>bucket = "mastering-ml-aws"<br/>prefix = "chapter3/linearmodels"<br/><br/>train_path = prefix + '/train'<br/>validation_path = prefix + '/validation'<br/><br/><br/>sess.upload_data(path='training-housing.csv', <br/>                 bucket=bucket, <br/>                 key_prefix=train_path)<br/>sess.upload_data(path='testing-housing.csv', <br/>                 bucket=bucket, <br/>                 key_prefix=validation_path)<br/><br/>s3_train_data = 's3://{}/{}'.format(bucket, train_path)<br/>s3_validation_data = 's3://{}/{}'.format(bucket, validation_path)</pre>
<ol start="2">
<li class="mce-root">Once the data is in S3, we can proceed to instantiate the estimator:</li>
</ol>
<pre style="padding-left: 60px">from sagemaker.amazon.amazon_estimator import get_image_uri<br/>from sagemaker.session import s3_input<br/><br/>container = get_image_uri(boto3.Session().region_name, 'linear-learner')<br/>s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)<br/><br/>linear = sagemaker.estimator.Estimator(container,<br/>                                       role,<br/>                                       train_instance_count=1, <br/>                                       train_instance_type='ml.c4.xlarge',<br/>                                       output_path=s3_output_location,<br/>                                       sagemaker_session=sess)</pre>
<ol start="3">
<li class="mce-root">Next, we need to set the hyperparameters. Linear Learner in SageMaker takes a large set of options, which can be found here <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html">https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html</a>. In <a href="7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml">Chapter 14</a>, <em>Optimizing SageMaker and Spark Machine Learning Models</em>, we will dive into how to find suitable values for these parameters:</li>
</ol>
<pre style="padding-left: 60px">linear.set_hyperparameters(feature_dim=len(training_features),<br/>predictor_type='regressor',<br/>mini_batch_size=1)<br/><br/>linear.fit({'train': s3_input(s3_train_data, <br/>content_type='text/csv'), <br/>'test': s3_input(s3_validation_data, <br/>content_type='text/csv')})</pre>
<ol start="4">
<li class="mce-root">Once we fit the model, we can instantiate a transformer, which is capable of computing predictions for our test dataset in <kbd>S3:</kbd></li>
</ol>
<pre style="padding-left: 60px">transformer = linear.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=s3_output_location)<br/><br/>transformer.transform(s3_validation_data, content_type='text/csv')<br/>transformer.wait()</pre>
<p class="mce-root" style="padding-left: 60px">This will create a file in s3 called <kbd>testing-housing.csv.out</kbd> with the following format:</p>
<pre style="padding-left: 60px"> {"score":18.911674499511719}<br/> {"score":41.916255950927734}<br/> {"score":20.833599090576172}<br/> {"score":38.696208953857422}</pre>
<ol start="5">
<li class="mce-root">We can download this file and build a pandas dataframe with the predictions:</li>
</ol>
<pre style="padding-left: 60px">predictions = pd.read_json('testing-housing.csv.out',lines=True)</pre>
<p class="mce-root" style="padding-left: 60px">The following screenshot shows the first few predictions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-959 image-border" src="assets/4f6ec0ab-6f36-4f1e-aa98-e61662863166.png" style="width:7.42em;height:12.17em;"/></p>
<ol start="6">
<li class="mce-root">Given that these scores follow the exact order found in the testing dataset, we can then proceed to put together the actual and predicted columns by merging the data series:</li>
</ol>
<pre style="padding-left: 60px">evaluation_df = pd.DataFrame({'actual':list(test_df[label]),<br/>                              'predicted':list(predictions['score'])})</pre>
<p class="mce-root" style="padding-left: 60px"><span>The preceding code displays the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-960 image-border" src="assets/34686cb3-6d30-4969-a882-3893f92bc134.png" style="width:10.42em;height:12.00em;"/></p>
<ol start="7">
<li class="mce-root">With this data frame, we can calculate the R2 score:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import r2_score<br/><br/>r2_score(evaluation_df['actual'], evaluation_df['predicted'])</pre>
<p class="mce-root" style="padding-left: 60px">The result was <kbd>0.796</kbd>, which is in line with the previous estimates, with a slight improvement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding logistic regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">Logistic regression is a widely used statistical model that can be used to model a binary dependent variable. In linear regression, we assumed that the dependent variable is a numerical value that we were trying to predict. Consider a case where the binary variable has values of true and false. In logistic regression, instead of calculating the value of numerical output using the formula we used in the <em>Linear regression</em> section, we estimate the log odds of a binary event labeled True using the same formulation. The function that converts log odds to the probability of the event labeled 1 occurring is called the <strong>logistic function</strong>. </p>
<p class="mce-root"><span>The unit of measurement for log-odds scale is called <strong>logit</strong>. </span>Log-odds are calculated using the following formula:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7f307bd6-134d-4d57-a9f1-b08bbc8da42b.png" style="width:18.83em;height:3.25em;"/>                    </p>
<p class="mce-root">Thus, using the same methodology as linear regression, logistic regression is used for binary dependent variables by calculating the odds of the True event occurring. The main difference between linear regression and logistic regression is that linear regression is used to predict the values of the dependent variable, while logistic regression is used to predict the probability of the value of the dependent variable. Hence, as we emphasize in most of this book, data scientists should look at what they want to predict and choose the algorithms accordingly. </p>
<p class="mce-root">The logistic regression algorithm is implemented in most popular machine learning packages, and we will provide an example of how to use it in Spark in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression in Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <kbd>chapter3/train_logistic</kbd> notebook shows how we can instantiate a <kbd>LogisticRegression</kbd> Spark Trainer instead of <kbd>NaiveBayes</kbd> for the Twitter dataset we dealt with in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em> and obtain a model just as good as the one we constructed:</p>
<pre>from pyspark.ml.classification import LogisticRegression<br/>logistic_regression = LogisticRegression(featuresCol="features", <br/>                                         labelCol="label")<br/>logistic_model = logistic_regression.fit(cleaned_training_df)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros and cons of linear models</h1>
                </header>
            
            <article>
                
<p class="mce-root">Regression models are very popular in machine learning and are widely applied in many areas. Linear regression's main advantage is its simplicity to represent the dataset as a simple linear model. Hence, the training time for linear regression is fast. Similarly, the model can be inspected by data scientists to understand which variable is contributing to the decisions of the overall model. Linear regression is recommended in cases where the problem statement is simple and fewer variables are used for predictions. As the complexity of the dataset increases, linear regression may generate significant errors if the data has a lot of noise in it. </p>
<p class="mce-root">Linear regression makes a bold assumption that the dependent variable has a linear relationship with the regressors. If this does not hold true, then the linear regression algorithm may not be able to fit the data well. There are variants such as quadratic regressions that can solve this issue. However, this leads to complexity in the model and hence <span><span>significantly </span></span>increases training time. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we started with the basics of regression algorithms and applied them to predict the price of houses. We then learned how to evaluate regression models, were introduced to linear regression through various libraries such as <span><kbd>scikit-learn</kbd>,  Apache Spark and SageMaker's linear learner, and, finally, we saw </span>how to use logistic regression for classification problems, and the pros and cons of linear models.</p>
<p class="mce-root">In the next chapter, we will predict user behavior with tree-based methods.</p>


            </article>

            
        </section>
    </body></html>