- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing for ML Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Part 2* of the book, we will examine the ML process. We will start from
    the preparation work, which includes ML problem framing to define an ML problem;
    data preparation and feature engineering to get the data ready; followed by the
    ML model development phases, which include model training, model validation, model
    testing, and model deployment. We will end *Part 2* with neural networks and DL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, will discuss the two ML preparation tasks: ML problem framing
    and data preparation. We will address the following questions for the problem
    we are solving:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the business requirements?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is ML the best way to solve the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the inputs and outputs for the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is my data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I measure the success of the ML solution?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data ready?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I collect my data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I transform and construct my data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I select features for the ML model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is very important that we identify the business requirements, understand
    the problem and its inputs/outputs, establish the business success measurements,
    and collect, transform, and construct high-quality datasets before model training
    and deployment. Through this process, we will learn and develop the following
    skills:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining and understanding a business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating it to an ML problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and measuring the success of the business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining high-quality datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming and constructing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s keep these questions and skills in mind as we go through this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from business requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical ML process starts by defining business requirements. Follow the following
    steps to define the business requirements of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly define the business outcome that your ML solution is supposed to achieve,
    among all the stakeholders. For example, for a prediction ML problem, we need
    to define a range of accuracy that is acceptable by the business and agreed upon
    by all the stakeholders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly define the data source of the ML problem. All ML projects are based
    on loads of data. You need to clearly define what the reliable data sources are,
    including training data, evaluation data, testing data, and a feed of regularly
    updated data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly define the frequency of ML model updating (since data distributions
    drift over time), and the strategies for maintaining production during the model
    updating times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly define the financial indications of the ML product or project. Understand
    any limitations such as resource availability and budget planning, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly define the rules, policies, and regulations for the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let us look at an example of the problem and the business requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: A real estate company called Zeellow does great business buying
    and selling properties in the United States. Due to the nature of the business,
    accurately predicting house prices is critical for Zeellow. Over the past few
    years, they have accumulated a large amount of historical data for US houses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the business outcome is accurately predicting house prices in the United
    States. It is agreed by business stakeholders that more than 2% prediction error
    is not acceptable. The data source is defined as the in-house historical property
    database. Due to database updates, the model needs to be updated every month.
    There are two data scientists and two data engineers working full-time on the
    project, and enough funding has been provided. There are no regulations about
    the house data and the ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: Defining ML problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we have identified the business requirements, we need to define the problem
    by identifying the features and target of the problem. For *Example 1*, the house
    price is the target, and features are the house attributes that affect the house
    price, such as the location, the house size (total square footage), the age of
    the house, the number of bedrooms and bathrooms of the house, and so on. *Table
    3.1* shows a small sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 3.1 – Example 1 dataset ](img/Table_3.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 3.1 – Example 1 dataset
  prefs: []
  type: TYPE_NORMAL
- en: The problem is then defined as building a model among the features and the target
    and discovering their relationships. During the problem definition process, we
    will understand the problem better, decide whether ML is the best solution for
    the problem, and to what category the problem belongs.
  prefs: []
  type: TYPE_NORMAL
- en: Is ML the best solution?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When facing a problem, the first thing we need to do is choose the best modeling/solution
    for the problem. For example, given the initial position and speed of a physical
    object, its mass, and the forces acting on it, how can we precisely predict its
    position at any time *t*? For this problem, a traditional mathematical model,
    based on Newton’s laws in the classic mechanical world, works much better than
    any ML models!
  prefs: []
  type: TYPE_NORMAL
- en: 'While scientific modeling provides the mathematical relationship between the
    prediction target and features, there are many problems that are very hard or
    even impossible to build a mathematical model for, and ML may be the best way
    to solve these problems. How do we know whether ML is the best way to solve a
    given problem? There are several conditions that need to be checked when judging
    whether ML is a potentially good solution for a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a pattern among the features and the predicting target. For *Example
    1*, we know the house price will be related to house features such as location,
    the total square footage, age, and so on, and there are patterns between the house
    price and its features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The existing pattern or relationship cannot be modeled using mathematics or
    science. For *Example 1*, the relationships between the price of the house and
    the features cannot be mathematically formulated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is plenty of quality data available. For *Example 1*, Zeellow has accumulated
    a large amount of historical data for US houses, including prices and their features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Example 1*, Zeellow needs to predict house prices. Apparently, there are
    relationships between the house price and the features of the house, but it is
    very difficult to build a mathematical model to describe the relationships. Since
    we have enough historical data, ML is potentially a good way to solve the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at more examples and see whether ML is the best solution for them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2**: Zeellow Mortgage is a subsidiary of Zeellow and is a mortgage
    business in the States. They have also accumulated a large amount of historical
    data on mortgage applicants and are trying to automate the decision process of
    approval or denial for new applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 3**: Zeellow Appraisal is a subsidiary of Zeellow and they evaluate
    the prices of existing houses when they are under contract. One good approximation
    for that is to see how similar properties are priced, and this leads to the grouping
    of properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we examine these two problems and check their conditions, we can decide
    whether ML is the way to solve the problems. Further, we will look at the categories
    of ML problems and what types our three examples belong to.
  prefs: []
  type: TYPE_NORMAL
- en: ML problem categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML problems can be divided into several categories. For *Example 1*, Zeellow
    needs to predict house prices, which is a continuous value, compared to *Example
    2* where ML needs to predict an approval (yes) or denial (no) for a mortgage application.
    An ML problem that outputs a continuous value is called **Regression**, while
    a problem that outputs discrete values (two or more) is called **Classification**.
    If there are two outputs (yes and no), then it’s called **Binary classification**.
    If there are more than two outputs, then it’s called **Multiclass classification**.
  prefs: []
  type: TYPE_NORMAL
- en: In both *Example 1* and *Example 2*, we let the machine learn from the existing
    dataset that is labeled with results. *Example 1*’s datasets are for houses that
    have been sold in the past few years and thus include house location, the number
    of bedrooms and bathrooms, the age of the house and the sale price. *Example 2*’s
    datasets include the mortgage applicant’s gender, age, income, marital status,
    and so on, and whether the application was approved or denied. Since the inputs
    for both examples are labeled, they are called **supervised learning**. Input
    data such as the house’s location, the number of bedrooms and bathrooms, and the
    age of the house in *Example 1*, and the applicant’s gender, age, income, and
    marital status in *Example 2*, are called **features** since they reflect the
    datasets attributes (features).
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning problems**, on the other hand, have inputs that are
    not labeled. For *Example 3*, Zeellow Appraisal needs to divide houses into different
    groups, and each group has similar features. The focus here is not on the house
    prices but to identify meaningful patterns in the data and split the houses into
    groups. Since we do not label the datasets, it is an unsupervised learning problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Another type of machine learning problem is **reinforcement learning** (**RL**).
    In RL, you don’t collect examples with labels. You set up the model (agent) and
    a reward function to reward the agent when it performs a task. With reinforcement
    learning, the agent can learn very quickly how to outperform humans. More details
    can be found on the Wikipedia page ([https://en.wikipedia.org/wiki/Reinforcement_learning](https://en.wikipedia.org/wiki/Reinforcement_learning)).
  prefs: []
  type: TYPE_NORMAL
- en: ML model inputs and outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an ML problem, the inputs are usually datasets, including all kinds of data
    formats/media such as numerical data, images, audio, and so on. Input datasets
    are extremely important and will decide how good the ML model will be.
  prefs: []
  type: TYPE_NORMAL
- en: For supervised learning, labeled data is used to train the model, which is a
    piece of software representing what a machine has learned from the training data.
    Inputs to supervised learning are marked datasets of features and targets, and
    the model learns by comparing the labeled target values with the model outputs
    to find errors. The erroris what we want to optimize. Note we refer to the optimization
    of the error, not the minimization of the error. In other words, minimizing the
    error to zero may not generate the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Within supervised learning, the output for a regression model is a continuous
    numerical value, and for a classification model, the output is a discrete value
    indicating a category (yes/no for binary classifications).
  prefs: []
  type: TYPE_NORMAL
- en: For unsupervised learning, input data is not labeled, and usually, the objective
    is to find the input data patterns and group them into different categories, called
    **clustering** or **grouping**.
  prefs: []
  type: TYPE_NORMAL
- en: For reinforcement learning, the input is a state, and the output is the action
    performed by ML. Between input and output, we have a function that takes a state
    as input and returns an action as output.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring ML solutions and data readiness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we define the problem and conclude that ML is a potentially good solution
    to the problem, we need to set up a way of measuring the problem solution and
    whether it’s ready for production deployment. For *Example 1*, we need to have
    a consensus as to what range is acceptable for the house prediction errors and
    we can use the ML model in production.
  prefs: []
  type: TYPE_NORMAL
- en: ML model performance measurement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the performance of ML solutions, we use ML metrics. For regression
    models, there are three metrics: mean square error, mean absolute error, and r-square.
    For classification models, we use the confusion matrix. We will discuss that more
    in the following chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Is the ML solution ready to be deployed? We need to circle back to the model’s
    original business goals in the ML problem framing:'
  prefs: []
  type: TYPE_NORMAL
- en: For Zeellow, is predicting a house price with 95% accuracy good enough?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Zeellow Mortgage, are we allowed to make a decision with 95% confidence?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Zeellow Appraisal, can we categorize a house into the proper group with
    95% accuracy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we have evaluated the ML model with the testing datasets and confirmed
    that the model reaches the business requirements, we will be ready to deploy it
    into production.
  prefs: []
  type: TYPE_NORMAL
- en: Data readiness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data plays such a significant role in the machine learning process that the
    quality of data has a huge impact on the model performance, the so-called *garbage
    in, garbage out*. An ML model’s accuracy relies on many factors, including the
    size and quality of the dataset. The perception that *The more data, the more
    model accuracy* is not always true. In real time, it is a big challenge to obtain
    a big amount of clean, high-quality data. Often in an ML project, we spend a big
    portion of time collecting and preparing the datasets. Depending on the ML problem
    we need to solve, there are several ways to collect data and sources to collect
    it from. For example, we can go with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Historical data collected by companies, such as user data and media data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publicly available data from research institutes and government agencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much data is enough for my ML model, and how do we measure the quality of
    our data? It depends on the type of machine learning problem you want to solve.
    As part of the problem framing process, we need to check and make sure we have
    enough high-quality data to go with building an ML solution. Next, we will discuss
    more details about data preparation and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data collection** is collecting the source data and storing it in a central
    safe location. In the data collection phase, we try to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the nature of the problem and do we have the right data for it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the data and do we have access to the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can we do to ingest all the data into one central repository?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we safeguard the central data repository?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These questions are crucial in any ML project because, in a real business, data
    is typically spread across many different heterogeneous source systems, and bringing
    all the source data together to form a dataset may involve huge challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common data collection and consolidation process called **Extract, Transform,
    and Load** (**ETL**) has the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract**: Pull the data from the various sources to a single location.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transform**: During data extraction and consolidation, we may need to change
    the data format, modify some data, remove duplicates, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Load**: Data is loaded into a single repository, such as Google Cloud Storage
    or Google BigQuery.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During this ETL process, we also need to address the data size, quality, and
    security:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data size**: How much data do we need to get useful ML results? While the
    answer is dependent on the ML problem, the rule of thumb is that the training
    datasets will be several times more than the model’s trainable parameters. For
    example, a typical regression problem may need ten times as many observations
    as features. A typical image classification problem may require tens of thousands
    of images to create a high-accuracy image classifier. Generally speaking, simple
    models trained with large datasets perform better than complex models with small
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: This can include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: Are the data sources reliable? Is the dataset labeled correctly?
    Is the dataset filtered properly? Are there duplicates or missing values?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature representation**: Does the dataset represent the useful ML features?
    Are there any outliers?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency between training and production data**: Are there any skews that
    exist between the datasets for training and for production?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data security**: Is the data secure? Do we need to encrypt the data? Is there
    any **Personally Identifiable Information** (**PII**) in the dataset? Are there
    any laws or regulatory requirements for accessing the dataset?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the data is collected and stored in a central safe repository, we need
    to construct and transform it into the right format so that it can be used for
    ML model training. We will discuss this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objectives of data engineering are to make sure that the datasets represent
    the real ML problem and have the right format for ML model training. Often, we
    use statistical techniques to sample, balance, and scale datasets, and handle
    missing values and outliers in the datasets. This section covers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling data with sub-datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing dataset classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us start with data sampling and balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data sampling is a statistical analysis technique used to select, manipulate,
    and analyze a representative subset in a larger dataset. Sampling data plays an
    important role in data construction. When sampling data, you need to be very careful
    not to introduce biased factors. For more details, please refer to [https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/sampling](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/sampling).
  prefs: []
  type: TYPE_NORMAL
- en: 'A classification dataset has more than two dataset classes. We call the classes
    that make up a large proportion of the set **majority classes**, and those that
    make up a small proposition **minority classes**. When the dataset has skewed
    class proportions – meaning the proportion of the minority classes is significantly
    less than that of the majority classes, it is an imbalanced dataset, and we need
    to balance it using statistical techniques called **downsampling** and **upweighting**.
    Let’s consider a fraud detection dataset with 1 positive and 200 negatives. The
    model training process will not reflect the real problem since the positive proportion
    is so small. In this case, we will need to process the dataset in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Downsampling**: Extract data examples from the dominant class to balance
    the classes. With a factor of 50 downsampling, the proportion will be 40:1 after
    downsampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upweighting**: Increase the dominant class weight by the same factor of 50
    (the same as the downsampling factor) during ML model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some ML libraries have provided built-in features to facilitate the process.
    For more details about the techniques and why we perform the previous steps, refer
    to [https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data).
  prefs: []
  type: TYPE_NORMAL
- en: Numerical value transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a dataset that has numeric features covering distinctly different ranges
    (for example, the age feature in a mortgage application approval ML model), it
    is strongly recommended to normalize the dataset since it will help algorithms
    such as gradient descent to converge. Common ways to normalize data are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling to a range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clipping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucketing/binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling to a range
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Scaling to a range** normalization is converting floating-point feature values
    from their natural range (for example, the age range of 0-90 ) into a standard
    range (for example, 0 to 1, or -1 to +1). When you know the approximate range
    (upper and lower bounds) of your data, and the data is approximately uniformly
    distributed across that range, then it is a good normalization practice. For example,
    most age values fall between 0 and 90, and every part of the range has a substantial
    number of people, thus normalizing age values is a common practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature clipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Feature clipping** caps all feature values above (or below) a certain value
    to a fixed value. If your dataset contains extreme outliers, feature clipping
    may be a good practice. For example, you could clip all temperature values above
    80 to be exactly 80\. Feature clipping can be applied before or after other normalizations.'
  prefs: []
  type: TYPE_NORMAL
- en: Log scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Log scaling** computes the log of your feature values, thus compressing a
    wide data range to a narrow range. When a handful of the data values have many
    points and most of the other values have few points, *log scaling* becomes a good
    transformation method. For example, movie ratings are good business use cases
    for *log scaling*, since most movies have very few ratings, while a few movies
    have lots of ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: Bucketing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Bucketing** is also called binning. It transforms numeric features into categorical
    features, using a set of thresholds. A good example is transforming house prices
    into low, medium, and high categories, for better modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical value transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Categorical values are discrete values that aren’t in an ordered relationship,
    with each value marking a category. If categorical data doesn’t have any order
    to it, for example, a *color* feature that has values such as red, green, and
    blue, and there is no preference among the categories, if you assign values of
    *1*, *2*, and *3* to represent *red*, *green*, and *blue*, respectively, the model
    might interpret blue as more important than red, since it has a higher numeric
    value. We often encode non-ordinal data into multiple columns or features, called
    **one-hot encoding**. *Figure 3.2* shows the one-hot encoding transformation for
    the color feature – red is 100, blue is 010, and green is 001:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – One-hot encoding for the “color” feature ](img/Figure_3.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – One-hot encoding for the “color” feature
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding transforms the non-ordinal categorical values into numerical
    values without introducing any ordinal bias. It is used widely in ML data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Missing value handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preparing the dataset, we often see missing data. For example, some columns
    in your dataset might be missing because of a data collection error, or data wasn’t
    collected on a particular feature. Missing data can make it difficult to accurately
    interpret the relationship between the feature and the target variable and dealing
    with missing data is an important step in data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Based on what has caused the missing data, the total dataset size, and the proportion
    of missing values, we can either drop the whole feature or impute the missing
    values. For example, if a row or column has a large percentage of missing values,
    *dropping* the entire row or column may be a viable option. If the missing values
    are randomly spread throughout the dataset and it’s only a small portion of its
    rows or columns, then *imputation* may be a better option. For categorical variables,
    we can usually replace missing values with mean, median, or most frequent values.
    For numerical or continuous variables, we typically use the mean or median to
    impute. Sometimes, we also encounter nulls or zeros, but those should be approached
    with care as zero can be a value in a column while an ETL pipeline will replace
    all missing values with zero.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often, we also see outliers – data points that lie at an abnormal distance
    from other values in the dataset. Outliers can make it harder for models to predict
    accurately because they skew values away from the other more normal values that
    are related to that feature. Depending on the causes of the outliers, you want
    to clean them up, or transform them to add richness to your dataset (some algorithms
    have built-in functions to handle outliers):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deleting the outlier or imputing the outlier**: If your outlier is based
    on an artificial error, such as incorrectly entered data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transforming the outlier**: Taking the natural log of a value to reduce the
    outlier’s influence on the overall dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Through the previously shown process of data construction and transformation,
    the dataset is ready. And now it’s time to go to the next step: checking and selecting
    the features (the variables that affect the model target) – the process called
    feature engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Feature engineering** is the process of selecting and transforming the most
    relevant features in ML modeling. It is one of the most important steps in the
    ML learning process. Feature engineering includes **feature selection** and **feature
    synthesis** (transformation).'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For an ML problem that has a lot of features extracted during the initial phase,
    feature selection is used to reduce the number of those features (input variables),
    so that we can focus on the features that are most useful to a model to predict
    the target variable. After you extract features for the problem, you need to use
    feature selection methods to choose the most appropriate features for model training.
    Depending on whether ML training is needed, there are two main types of feature
    selection methods you can use – filter methods and wrapper methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter methods** use statistical techniques to evaluate and score the relationship
    between each input variable and the target variable. The scores are used to compare
    the features and decide the input variables that will be used in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrapper methods** create many models with different subsets of input features
    and perform model training and compare their performances. The feature subsets
    fitting the best-performing model according to a performance metric will be selected.
    Wrapper methods need ML training on different subsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A synthetic feature is created algorithmically, usually with a combination of
    the real features using arithmetic operations such as addition, subtraction, multiplication,
    and division to train machine learning models. **Feature synthesis** provides
    great insights into data patterns and helps model training for some ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: After data collection, construction and transformation, and feature engineering,
    our data is ready for the next stage – modeling development.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed preparations for an ML process. Starting from
    business requirements, you need to understand the problem and see if ML is the
    best solution for it. You then define the ML problem, set up performance measurement,
    and identify the data to be used for ML modeling to make sure we have a high-quality
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data plays such an important role! We have also discussed data preparation and
    feature engineering in this chapter. From data collection and construction to
    data transformation, feature selection, and feature synthesis, data pipelines
    prepare the dataset for ML model training. Mastering these data preparation and
    feature engineering skills will provide us with insights into the data and help
    us in model development. In the next chapter, we will discuss the ML model development
    process, from model training and validation to model testing and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further insights into what you''ve learned in this chapter, you can refer
    to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developers.google.com/machine-learning/problem-framing](https://developers.google.com/machine-learning/problem-framing%20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developers.google.com/machine-learning/data-prep](https://developers.google.com/machine-learning/data-prep)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
