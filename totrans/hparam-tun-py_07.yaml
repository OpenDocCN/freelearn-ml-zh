- en: '*Chapter 6*: Exploring Multi-Fidelity Optimization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Multi-Fidelity Optimization** (**MFO**) is the fourth of four groups of hyperparameter
    tuning methods. The main characteristic of this group is that all methods belonging
    to this group utilize the cheap approximation of the whole hyperparameter tuning
    pipeline so we can have similar performance results with a much lower computational
    cost and faster experiment time. This group is suitable when you have a very large
    model or a very large number of samples, for example, when you are developing
    a neural-network-based model.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss several methods in the MFO group, including
    coarse-to-fine search, successive halving, hyper band, and **Bayesian Optimization
    and Hyperband** (**BOHB**). As in [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)*,
    Exploring Heuristic Search* we will discuss the definition of each method, the
    differences between them, how they work, and the pros and cons of each.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be confident in explaining MFO and its
    variations, and also how they work at a high level and in a technical way. You
    will also be able to tell the differences between them, along with the pros and
    cons of each. You will also experience the crucial benefit of understanding each
    of the methods in practice: being able to configure the method to match your own
    problem and knowing what to do when there are errors or unexpected outputs from
    the method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MFO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding coarse-to-fine search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding successive halving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding hyper band
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding BOHB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing MFO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MFO is a group of hyperparameter tuning methods that work by creating a cheap
    approximation of the whole hyperparameter tuning pipeline so that we can get similar
    performance results with much *lower computational cost* and *faster experiment
    time*. There are many ways to create a cheap approximation. For example, we can
    work only on the subsets of the full data in the first several steps rather than
    directly working on the full data, or we can also try to use fewer epochs when
    training a neural-network-based model before training our model with full epochs.
    In other words, MFO methods work by *combining cheap low-fidelity and expensive
    high-fidelity* evaluations, where usually the proportion of cheaper evaluations
    is much larger than the more expensive evaluations so that we can achieve lower
    computational cost and thus faster experiment time. However, MFO methods can also
    be categorized as part of the **informed search** category since they utilize
    knowledge from previous iterations to have a (hopefully) better search space in
    future.
  prefs: []
  type: TYPE_NORMAL
- en: All of the methods that we have learned in the previous chapters can be categorized
    as **black-box optimization** methods. All black-box optimization methods try
    to perform hyperparameter tuning without utilizing any information from what is
    happening inside the ML model or the data that is used by the model. A black-box
    optimizer will only focus on searching the best set of hyperparameters from the
    defined hyperparameter space and *treat other factors as a black box* (see *Figure
    6.1*). This characteristic has its own good and bad implications. It enables us
    to utilize a black-box optimizer, which is more flexible for various types of
    models or data, but it also costs us more since we do not consider other factors
    that may speed up the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Illustration of black-box optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Illustration of black-box optimizer
  prefs: []
  type: TYPE_NORMAL
- en: The expense of black-box optimization methods means we can’t utilize them when
    we are working with a *very large model* or *big data* that requires a very long
    time for just one training iteration. That’s where the MFO group of hyperparameter
    tuning methods comes into the picture! By considering other factors that are treated
    as black-box by black-box optimizers, we can have a faster process while sacrificing
    a bit of the generality that black-box optimizers have.
  prefs: []
  type: TYPE_NORMAL
- en: Generality
  prefs: []
  type: TYPE_NORMAL
- en: '**Generality** means the model is able to perform on many unseen cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, most of the methods categorized in this group can *utilize parallel
    computational resources* very nicely, which can further boost the speed of the
    hyperparameter tuning process. However, the benefit of faster processes offered
    by MFO methods comes with a cost. We may have *worse performing tuning results*
    since there is a chance we have excluded a better subspace during the cheap low-fidelity
    evaluations step. However, the *speedup is arguably more significant* than the
    estimation error, especially when we are working with a very large model and/or
    big data.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The MFO group of hyperparameter tuning methods is *not* a completely different
    group compared to black-box optimization methods, including exhaustive search,
    Bayesian optimization, and heuristic search. In fact, we can also apply a similar
    procedure done in a multi-fidelity optimization method to a black-box optimizer.
    In other words, *we can combine black-box-and multi-fidelity* models so we can
    get the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can perform hyperparameter tuning with one of the **Bayesian
    Optimization** (**BO**) methods (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036),
    *Exploring Bayesian Optimization*) and also apply the successive halving method
    (see the *Understanding successive halving* section) on top of it. This way, we
    will ensure that we only perform BO on important subspace, rather than letting
    BO explore the whole hyperparameter space by itself. By doing this, we can have
    a faster experiment time with lower computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are aware of what MFO is, how it differs from black-box optimization
    methods, and how it works at a high level, we will dive deeper into several MFO
    methods in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding coarse-to-fine search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Coarse-to-Fine Search** (**CFS**) is a combination of grid and random search
    hyperparameter tuning methods (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*,
    Exploring Exhaustive Search*). Unlike grid and random search, which are categorized
    in the **uninformed search** group of methods, CFS utilizes knowledge from previous
    iterations to have a (hopefully) better search space in the future. In other words,
    CFS is a *combination of sequential and parallel* hyperparameter tuning methods.
    It is indeed a very simple method since it is basically a *combination of two
    other simple methods: grid and random search*.'
  prefs: []
  type: TYPE_NORMAL
- en: CFS can be effectively utilized as a hyperparameter tuning method when you are
    working with a medium-sized model, for example, a shallow neural network (other
    types of models can also work) and a moderate amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of CFS is just to start with a *coarse* random search from the
    whole hyperparameter space, then gradually *refine* the search in more detail,
    either using random or grid search. The following figure summarizes how CFS works
    as a hyperparameter tuning method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Illustration of CFS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Illustration of CFS
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in *Figure 6.2*, CFS starts by performing a random search in
    the whole pre-defined hyperparameter space. Then, it looks for a promising subspace
    based on the first coarse random search evaluation results. The definition of
    a promising subspace may vary and can be adjusted to your own preference. The
    following list shows several definitions of a promising subspace that you can
    adopt:'
  prefs: []
  type: TYPE_NORMAL
- en: Get only the *top N percentiles* of the best set of hyperparameters based on
    the evaluation performed in the previous trial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put a *hard threshold* to filter out the bad set of hyperparameters from the
    previous trial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct a *univariate analysis* to get the best range of values for each hyperparameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter what definition you are using to define the promising subspace, we
    will always get a list of values for each hyperparameter. Then, we can create
    a new hyperparameter space based on the minimum and maximum values in each list
    of hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the promising subspace, we can continue the process by performing
    a grid search or another random search in the smaller area. Note that you can
    also put a condition on when to keep using random search and when to start using
    grid search. Again, it is up to you to choose the appropriate condition. However,
    it is better to perform a random search than a grid search, so that we can have
    *more evaluations based on the cheap low-fidelity approach* compared to the expensive
    high-fidelity approach. We keep repeating this procedure until we reach the stopping
    criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following procedure explains in more detail how CFS works as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into a training set and a test set. (See [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,* *Evaluating Machine Learning Models*.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, `H`, with the accompanied distributions, the
    objective function, `f`, based on the training set, and the stopping criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the grid size for creating the grid search hyperparameter space, `grid_size`,
    and the random search number of iterations, `random_iters`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the criterion of a promising subspace by utilizing the objective function,
    `f`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the criterion of when to start using grid search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial best set of hyperparameters, `best_set`, with the value `None`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a random search on the current hyperparameter space, `H`, for `random_iters`
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select a promising subspace based on the criterion defined in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current best-performing set of hyperparameters is worse than the previous
    `best_set`, add `best_set` to the promising subspace.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current best-performing set of hyperparameters is better than the previous
    `best_set`, update `best_set`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the criterion in `step 5` is met, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the current hyperparameter space, `H`, with the promising subspace selected
    in *step 8*, using unique `grid_size` values for each of the hyperparameters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a grid search on the updated hyperparameter space, `H`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the criterion in *step 5* is not met, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the current hyperparameter space, `H`, with the promising subspace selected
    in *step 8* using the minimum and maximum values for each hyperparameter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a random search on the updated hyperparameter space, `H`, for `random_iters`
    times.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 8 – 10* until the stopping criterion is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the best hyperparameter combination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In CFS, the multi-fidelity characteristic is based neither on the amount of
    data nor the number of training epochs, but on the *granularity of the search*
    performed in the search space during each trial. In other words, we will *keep
    using all of the data* and *all of the training epochs* with a *refined hyperparameter
    space* in each trial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how CFS works as a hyperparameter tuning method on dummy data generated
    by the `make_classification` to create dummy classification data with several
    customizable configurations. In this example, we use the following configurations
    to generate the dummy data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Number of classes*. We set the number of target classes in the data to 2 by
    setting `n_classes=2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Number of samples*. We set the number of samples to 500 by setting `n_samples=500`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Number of features*. We set the number of features or the number of dependent
    variables in the data to 25 by setting `n_features=25`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Number of informative features*. We set the number of features that have high
    importance to distinguish between all of the target classes to 18 by setting `n_informative=18`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Number of redundant features*. We set the number of features that are basically
    just a weighted sum from other features to 5 by setting `n_redundant=5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Random seed*. To ensure reproducibility, we set `random_state=0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We utilize a `12`, which acts as the stopping criterion. We set the number
    of iterations for each random search trial to `20`. Finally, we utilize the *top
    N percentiles* scheme to define the promising subspace in each trial, with `N=50`.
    We define the hyperparameter space as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of neurons in the hidden layer: `hidden_layer_sizes=range(1,51)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initial learning rate: `learning_rate_init=np.linspace(0.001,0.1,50)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure shows how CFS works in each iteration or trial. The *purple
    dots* refer to hyperparameter values tested in the current trial, while the *red
    rectangles* refer to the promising subspace to be searched in the next trial.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Illustration of the CFS process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Illustration of the CFS process
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.3*, we can see clearly how CFS starts by working at the full hyperparameter
    space and then gradually searches in the smaller subspaces. It is also worth noting
    that although we only use random search in this example, we can see that CFS still
    increases its fidelity over the number of trials until we get a final set of hyperparameters
    in the last trial. We can also see the performance of each trial in the following
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Convergence plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Convergence plot
  prefs: []
  type: TYPE_NORMAL
- en: The blue line in *Figure 6.4* reflects the average cross-validation scores from
    all tested hyperparameters (see the purple dots in *Figure 6.3*) at each trial.
    The red line reflects the cross-validation score of the best-performing set of
    hyperparameters at each trial. We can see that the red line has a nice *non-decreasing
    monotonic* characteristic. This happens because we always add back the best set
    of hyperparameters from all previous trials to the promising subspace definition,
    as defined in *step 8* in the previous procedure. We will learn how to implement
    CFS with scikit-learn in *Chapter 7, Hyperparameter Tuning via Scikit*.
  prefs: []
  type: TYPE_NORMAL
- en: The following table summarizes the pros and cons of utilizing CFS as a hyperparameter
    tuning method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Pros and cons of CFS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Pros and cons of CFS
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have discussed CFS, looking at what it is, how it works,
    and the pros and cons. We will discuss another interesting MFO method in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding successive halving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Successive Halving** (**SH**) is an MFO method that is not only able to focus
    on a more promising hyperparameter subspace but can also *allocate computational
    cost wisely* in each trial. Unlike CFS, which utilizes all of the data in each
    trial, SH can utilize less data for a not-too-promising subspace while utilizing
    more data for a more promising subspace. It can be said that SH is a variant of
    CFS with a much clearer algorithm definition and is wiser in spending the computational
    cost. The most effective way to utilize SH as a hyperparameter tuning method is
    when you are working with a large model (for example, a deep neural network) and/or
    working with a large amount of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to CFS, SH also *utilizes grid search or random search* to search for
    the best set of hyperparameters. At the first iteration, SH will perform a grid
    or random search on the whole hyperparameter space with a small amount of **budget**
    or resources, and then it will gradually increase the budget while also removing
    the worst half of the hyperparameters candidates at each iteration. In other words,
    SH performs hyperparameter tuning with a lower budget on a bigger search space
    and a higher budget on a more promising smaller subspace. SH can also be seen
    as a **tournament** between hyperparameter candidates, where only the best candidate
    will survive at the end of the trials.
  prefs: []
  type: TYPE_NORMAL
- en: Budget Definition in SH
  prefs: []
  type: TYPE_NORMAL
- en: In a default hyperparameter tuning setup, the budget is defined as the number
    of samples in the data. However, it is also possible to define the budget in other
    ways. For example, we can also define the budget as the maximum training time,
    number of iterations during XGBoost training steps, number of estimators in a
    random forest, or number of epochs when training a neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: To have a better understanding of SH, let’s look at the following example before
    we discuss how it works in a formal procedure. We utilize the same model and the
    same hyperparameter space definition used in the example in the *Understanding
    CFS* section. We also utilize a similar procedure to generate a dummy classification
    dataset a hundred times bigger in size, meaning we have `50000` samples instead
    of only `500` samples as in the CFS example.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we utilize random search instead of grid search to sample the
    hyperparameter candidates in each trial. The following figure shows the accuracy
    scores of hyperparameter candidates over trials. Each line refers to the trend
    of each hyperparameter candidate’s objective function score, which in this case
    is the *seven-fold cross-validation accuracy score*, over the number of trials.
    The final objective function score, based on the best set of hyperparameters selected
    from the SH tuning process, is `0.984`. We will learn how to implement SH in [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062)*, Hyperparameter Tuning via Scikit*
    and [*Chapter 9*](B18753_09_ePub.xhtml#_idTextAnchor082)*, Hyperparameter Tuning
    via Optuna*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Illustration of the SH process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Illustration of the SH process
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.6*, we can clearly see how SH takes only the top hyperparameter
    candidates (see the orange ovals) from each trial for further evaluation in the
    next trial. In the first iteration, a random search is performed `240` times with
    only `600` out of `50000` of the samples available in the data. This means we
    have `240` hyperparameter candidates, `n_candidates`, in the first iteration.
    Out of those hyperparameter candidates, SF takes only the top `80` candidates
    to be evaluated with a larger number of samples in the second iteration, which
    is `1800` samples. For the third iteration, SF again takes only the top `27` candidates
    and evaluates them on `5400` samples.
  prefs: []
  type: TYPE_NORMAL
- en: This process continues until we *can’t use a larger number of samples* since
    it will be greater than the maximum resources, `max_resources`, defined in the
    first place. In this example, the maximum resources are defined as the number
    of samples that we have in the data. However, it can also be defined as the total
    number of epochs or training steps based on the definition of the budget or resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we stopped at the fourth iteration, where we need to evaluate
    `3` candidates based on `48600` samples. The final hyperparameter candidate chosen
    is the one that has the highest seven-fold cross-validation accuracy score evaluated
    on those `48600` samples.
  prefs: []
  type: TYPE_NORMAL
- en: As you will notice, the gradual increment of the number of samples in each trial
    and the gradual decrement of the number of candidates in each trial follows the
    same multiplier factor, `factor`, which is `3` in this example. That’s why we
    have to stop at the fourth iteration, since if we continue to the fifth iteration,
    we would need `48600*3=145800` samples, while we only have `50000` samples in
    the data. Note that we have to set the value of the multiplier factor ourselves
    before running the SH tuning process. In other words, this multiplier factor is
    the hyperparameter for SH.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplier Factor in SH
  prefs: []
  type: TYPE_NORMAL
- en: The halving term in SH refers to setting the multiplier factor value to two.
    In other words, only the best half of the hyperparameter candidates in each trial
    are passed to the next trial. However, we can also change this with another value.
    For example, when we set the multiplier factor as three, it means we take only
    the top one-third of hyperparameter candidates in each trial. In practice, setting
    the multiplier factor as three usually works better than setting it as two.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the multiplier factor and maximum resources, SH also has other hyperparameters,
    such as the minimum number of resources to be used at the first iteration, `min_resources`,
    and the initial number of candidates to be evaluated at the first iteration, `n_candidates`.
    If grid search is utilized in the SH tuning process, `n_candidates` will equal
    the number of all combinations of hyperparameters in the search space. If a random
    search is utilized, then we have to set the value of `n_candidates` ourselves.
    In our example, where random search is utilized, we set `min_resources=600` and
    `n_candidates=240`.
  prefs: []
  type: TYPE_NORMAL
- en: 'While setting `factor` to be equal to three is the common practice, this is
    not the case for `min_resources` and `n_candidates`. There are many factors to
    be considered before choosing the right values for both the `min_resources` and
    `n_candidates` hyperparameters. In other words, there is a trade-off between them,
    as explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a *bigger* value for `n_candidates` is useful when the bad and good
    hyperparameters can be easily distinguished with a smaller number of samples (a
    smaller value for `min_resources`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a *smaller* value for `n_candidates` is useful when we need a larger
    number of samples (a larger value for `min_resources`) to distinguish between
    the bad and good hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another hyperparameter that SH has is the minimum early stopping rate, `min_early_stopping`.
    This integer-type hyperparameter has a default value of zero. If it is set to
    more than zero, it will reduce the number of iterations while increasing the number
    of resources to be used at the first iteration. In our previous example, we set
    `min_early_stopping=0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, SH as a hyperparameter tuning method works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, `H`, with the accompanied distributions, and
    the objective function, `f`, based on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the budget/resources. Usually, this is defined as the number of samples
    or training epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the maximum amount of resources, `max_resources`. Usually, this is defined
    as the total number of samples in data or the total number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the multiplier factor, `factor`, the minimum amount of resources to be
    used at the first iteration, `min_resources`, and the minimum early stopping rate,
    `min_early_stopping`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the initial number of hyperparameter candidates to be evaluated at the
    first iteration, `n_candidates`. If grid search is utilized, this will be automatically
    parsed from the total number of hyperparameter combinations in the search space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the maximum number of iterations, *n*iter, using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Assert if `n_candidates` ≥ ![](img/Formula_B18753_06_002.png) to ensure there
    is at least one candidate in the last iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Warm up the first iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample `n_candidates` sets of hyperparameters from the hyperparameter space.
    If grid search is utilized, just return all of the hyperparameter combinations
    in the space. This set of candidates is referred to as *candidates1*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all *candidates1* sets of hyperparameters, using `min_resources`, based
    on the objective function, *f*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the *topK* value that will be used to select top candidates for the
    next iteration:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For each iteration, *i*, starting from the second iteration until *n*iter iteration,
    proceed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the current set of candidates, *candidatesi*, by selecting *topK* candidates
    from *candidatesi-1* in terms of the most optimal objective function score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the current allocated resources, *resourcesi*, based on the following
    formula:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluate all *candidatesi* sets of hyperparameters, using *resourcesi*, based
    on the objective function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the *topK* value based on the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Return the best hyperparameter candidate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all candidates in the last iteration using the allocated number of
    resources and the objective function, `f`. Note that it’s possible that the allocated
    resource in the last iteration is less than `max_resources`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the candidate with the optimal objective function score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full train set using the best set of hyperparameters from *step
    11*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the previous example and the stated procedure, we can see that SH performs
    cheap, low-fidelity evaluations on the first several iterations by using a low
    number of resources and starts to perform more expensive high-fidelity evaluations
    on the final several iterations by using a high number of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Other Black-Box Methods
  prefs: []
  type: TYPE_NORMAL
- en: SH can also be utilized along with other black-box hyperparameter tuning methods
    apart from grid and random search. For example, in the **Optuna** (see [*Chapter
    9*](B18753_09_ePub.xhtml#_idTextAnchor082)*, Hyperparameter Tuning via Optuna*)
    package, we can combine TPE (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*) with SH, where SH acts as a **pruner**. Note
    that in Optuna, the budget/resources is defined as the number of training steps
    or epochs instead of the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of the pros and cons of SH as a hyperparameter tuning
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Pros and cons of SH'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Pros and cons of SH
  prefs: []
  type: TYPE_NORMAL
- en: In practice, most of the time, we do not know how to balance the trade-off between
    the number of resources and the number of candidates since there is no clear definition
    of how to distinguish bad and good hyperparameters. One thing that can help us
    to find a sweet spot in this trade-off is leveraging previous similar experiment
    configurations or by performing **meta-learning** based on the available meta-data
    from previous similar experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Now you are aware of SH, how it works, when to use it, and its pros and cons,
    in the next section, we will learn about an extension of this method that attempts
    to overcome the cons of SH.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hyper band
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hyper Band** (**HB**) is an extension of SH that is specifically designed
    to overcome issues inherent in SH (see *Figure 6.7*). Although we can perform
    meta-learning to help us balance the trade-off, most of the time we do not have
    the metadata that’s needed in practice. Furthermore, the possibility of SH removing
    better sets of hyperparameters in the first several iterations is also worrying
    and can’t be solved by just finding a sweet spot from the trade-off. HB tries
    to solve these issues by calling SH several times iteratively.'
  prefs: []
  type: TYPE_NORMAL
- en: Since HB is just an extension of SH, it is suggested that you utilize HB as
    your hyperparameter tuning method when you are working with a large model (for
    example, a deep neural network) and/or working with a large amount of data, just
    like SH. Furthermore, it is even better to utilize HB than SH when you do not
    have the time or metadata needed to help you configure the trade-off between the
    amount of resources and the number of candidates, which is the case most of the
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between HB and SH is in their hyperparameters. HB has the
    same hyperparameters as SH (see the *Understanding SH* section) except for `n_candidates`.
    In HB, we don’t have to choose the best value for `n_candidates` since it is calculated
    automatically within the HB algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, HB works by running SH iteratively with variations of `n_candidates`
    and `min_resources` in each of the `n_candidates` and the lowest possible value
    for `min_resources`, and going to the lowest possible value for `n_candidates`
    and the highest possible value for *resources* (see *Figure 6.8*). It’s like a
    brute-force approach to try *almost* all of the possible combinations of `n_candidates`
    and `min_resources`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates
    and min_resources for bracket-j, respectively'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates
    and min_resources for bracket-j, respectively
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 6.8*, assume that we set `factor=3`, `min_resources=1`,
    `max_resources=27`, and `min_early_stopping=0`. As you can see, HB allocates the
    minimum amount of resources with the maximum number of candidates in the first
    bracket, while it allocates the maximum amount of resources with the minimum number
    of candidates in the last bracket. Again, each bracket refers to each SH run,
    meaning we are running SH four times in this illustration, where the last bracket
    is basically the same as performing random or grid search on a small hyperparameter
    space.
  prefs: []
  type: TYPE_NORMAL
- en: By testing *almost* all of the possible combinations of `n_candidates` and `min_resources`,
    HB is able to remove the trade-off in SH while also reducing the possibility of
    excluding better hyperparameters in the first iterations. However, this groundbreaking
    characteristic of HB *doesn’t ensure that it will be always better than SH*. Why?
    Because HB hasn’t actually tried all the possible combinations. We might find
    a better combination of `n_candidates` and `min_resources` values just by performing
    a single SH than all the possible combinations HB tried. However, this takes time
    and luck since we have to manually select the `n_candidates` and `min_resources`
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Other Black-Box Methods
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper on HB, the authors utilize random search for each SH run.
    However, as with SH, we can also integrate HB with other black-box methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following procedure further states how HB works formally as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the full original dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, `H`, with the accompanied distributions, and
    the objective function, `f`, based on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `budget` resource. This is usually defined as the number of samples
    or training epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the maximum resources, `max_resources`. This is usually defined as the
    total number of samples in the data or the total number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the multiplier factor, `factor`, the minimum early stopping rate, `min_early_stopping`,
    and the minimum number of resources for all brackets, `min_resources`. Usually,
    `min_resources` is set to one, if the budget is defined as the number of samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dictionary, `top_candidates`, that will be utilized to store the best-performing
    set of hyperparameters from each SH run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the number of brackets, *nbrackets*, using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For each bracket-*j*, starting from `j=1` until `j=nbrackets`, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the minimum number of resources to be used at the first iteration
    of SH for bracket-*j*, ![](img/Formula_B18753_06_017.png), using the following
    formula:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate the initial number of hyperparameter candidates to be evaluated at
    the first iteration of SH for bracket-*j*, ![](img/Formula_B18753_06_019.png),
    using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Do *steps 7 – 11* from the SH procedure given in the *Understanding SH* section
    by utilizing ![](img/Formula_B18753_06_021.png) as the `min_resources` and ![](img/Formula_B18753_06_022.png)
    as the `n_candidates` for the current SH run, respectively. Other hyperparameters
    for SH, such as `max_resources`, `min_early_stopping`, and `factor`, are inherited
    from HB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the best set of hyperparameters output from the current SH run, along
    with the objective function score, in the `top_candidates` dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best candidate that has the most optimal objective function score
    from the `top_candidates` dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the best set of hyperparameters from *step
    9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following table summarizes the pros and cons of utilizing HB as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Pros and cons of HB'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – Pros and cons of HB
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that although HB can help us to deal with the trade-off of
    SH, it has a higher computational cost, since we have to run several SH rounds
    iteratively. It is even more costly when we are faced with a case where the bad
    and good hyperparameters cannot be easily distinguished with a small budget value.
    Why? The first several brackets of HB that utilize small budgets will result in
    a noisy estimation, since the relative rankings inside the SH iterations on smaller
    budgets do not reflect the actual relative rankings on higher budgets. In the
    most extreme case, the best set of hyperparameters will result from the last bracket
    (random search). If this is the case, then HB will run *n**brackets* times slower
    compared to random search.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have discussed HB, what it is, how it works, and its pros
    and cons. We will discuss another interesting MFO method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BOHB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayesian Optimization and Hyper Band** (**BOHB**) is an extension of HB that
    is superior to CFS, SH, and HB, in terms of understanding the relationship between
    the hyperparameter candidates and the objective function. If CFS, SH, and HB are
    all part of the informed search group based on random search, BOHB is an informed
    search group that is based on the BO method. This means BOHB is able to decide
    which subspace needs to be searched based on previous experiences rather than
    luck.'
  prefs: []
  type: TYPE_NORMAL
- en: As its name implies, BOHB is the combination of the BO and HB methods. While
    SH and HB can also be utilized with other black-box methods (see the *Understanding
    SH* and *Understanding HB* sections), BOHB is specifically designed to utilize
    a BO method in a way that can support HB. Furthermore, the BO method in BOHB also
    tracks all the previous evaluations on all budgets, so that it can serve as the
    base for future evaluations. Note that the BO method used in BOHB is the **multivariate
    TPE**, which is able to take into account the interdependencies among hyperparameters
    (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring Bayesian
    Optimization*).
  prefs: []
  type: TYPE_NORMAL
- en: The main selling point of BOHB is its ability to achieve both a strong initial
    performance and a strong final performance. This can be easily seen in *Figure
    6.10*, from the original BOHB paper (see the following note for details). BO (without
    performing metalearning) will outperform random search if we have more time to
    let it learn from previous experiences. If we don’t have time, BO will deliver
    a similar or even worse performance compared to random search. On the other hand,
    HB performs much better than random search when we have limited time, but will
    perform similarly to random search if we allow more time for random search to
    explore the hyperparameter space. By combining the best of both worlds, BOHB is
    able to not only outperform random search in a limited time but also when given
    enough time for random search to catch up.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Comparison between random search, BO, HB, and BOHB'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – Comparison between random search, BO, HB, and BOHB
  prefs: []
  type: TYPE_NORMAL
- en: The Original BOHB Paper
  prefs: []
  type: TYPE_NORMAL
- en: '*BOHB: Robust and Efficient Hyperparameter Optimization at Scale* by Stefan
    Falkner, Aaron Klein, and Frank Hutter, Proceedings of the 35th International
    Conference on Machine Learning, PMLR 80:1437-1446, 2018 ([http://proceedings.mlr.press/v80/falkner18a.html](http://proceedings.mlr.press/v80/falkner18a.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: The following procedure further states how BOHB works formally as a hyperparameter
    tuning method. Note that BOHB and HB are very similar except that random search
    in HB is replaced by the combination of multivariate TPE and random search. Since
    HB just performs SH several times iteratively, the actual replacement is actually
    performed in each of the SH runs (each bracket) in HB.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s pick up from the previous instructions again.
  prefs: []
  type: TYPE_NORMAL
- en: '*6\. (The first six steps are the same as those in the Understanding HB section.)*'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Define the probability of just performing a random search rather than fitting
    the multivariate TPE, *random_prob*.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Define the percentage of the good set of hyperparameters for the multivariate
    TPE fitting procedure, *top_n_percent*. (See [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*.)
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Define a dictionary, *candidates_dict*, that stores the budget/resources
    used in a particular SH iteration and the pairs of hyperparameter candidates and
    the objective function score as the key and value, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Define the minimum number of sets of hyperparameters that are randomly
    sampled before starting to fit the multivariate TPE, `n_min`. By default, we set
    `n_min` to match the number of hyperparameters in the space plus one.
  prefs: []
  type: TYPE_NORMAL
- en: '11\. For each bracket-*j*, starting from `j=1` until `j=nbrackets`, do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the minimum number of resources to be used on the first iteration
    of SH for bracket-*j*, ![](img/Formula_B18753_06_023.png), using the following
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate the initial number of hyperparameters candidates to be evaluated
    on the first iteration of SH for bracket-j, ![](img/Formula_B18753_06_025.png),
    using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perform steps 7 – 11 from the SH procedure stated in the Understanding SH section
    by utilizing ![](img/Formula_B18753_06_027.png) as min_resources and ![](img/Formula_B18753_06_028.png)
    as n_candidates for the current SH run, respectively, where step 9\. I. is replaced
    with the following procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a random number between zero and one from a uniform distribution, rnd.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `rnd<random_prod` or *models_dict* is empty, perform a random search to sample
    the initial hyperparameter candidates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the number of sampled hyperparameters in `candidates_dict[`![](img/Formula_B18753_06_029.png)`]`,
    and store it as `num_curr_candidates`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `num_curr_candidates < n_min`, then perform a random search to sample the
    initial hyperparameter candidates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, utilize the multivariate TPE (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*) to sample the initial hyperparameter candidates.
    Note that we always utilize multivariate TPE on the largest budget available in
    `candidates_dict`. The number of hyperparameter sets for both good and bad groups
    is defined based on the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_06_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_B18753_06_031.png)'
  prefs: []
  type: TYPE_IMG
- en: Store the sampled initial hyperparameter candidates along with the objective
    function score (either from step ii, iv, or v) in `candidates_dict[`![](img/Formula_B18753_06_032.png)`]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the best set of hyperparameters output from the current SH run, along
    with the objective function score, in the `top_candidates` dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best candidate that has the most optimal objective function score
    from the `top_candidates` dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the best set of hyperparameters from *step
    14*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that to ensure that BOHB tracks all of the evaluations on all budgets,
    we also need to store the hyperparameter candidates in each of the SH iterations
    for each HB bracket to `candidates_dict[budget]` along with their objective function
    score. Here, hyperparameter candidates in each of the SH iterations refer to *candidatesi*,
    while budget refers to *resourcesi* in *step 10* in the *Understanding SH* section,
    which also can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – BOHB tracks all the evaluations on all budgets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – BOHB tracks all the evaluations on all budgets
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder whether it is possible for BOHB to take advantage of parallel
    resources since it utilizes a BO method that is notorious for not being able to
    exploit parallel computing resources. The answer is *yes, it is possible*! You
    can take advantage of parallel resources since in each of the BOHB iterations,
    specifically in the HB iterations, we can utilize more than one worker to evaluate
    multiple sets of hyperparameters, in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about the sequential nature of the multivariate TPE utilized in BOHB?
    Yes, there may be some sequential processes that need to be performed inside the
    TPE model. However, BOHB limits the number of sets of hyperparameters given to
    the multivariate TPE so it might not take too much time. Furthermore, the limitation
    on the number of hyperparameter sets is actually specifically designed by the
    authors of BOHB. The following is a direct quote from the original paper on BOHB:'
  prefs: []
  type: TYPE_NORMAL
- en: The parallelism in TPE is achieved by limiting the number of samples to optimize
    EI, purposefully not optimizing it fully to obtain diversity. This ensures that
    consecutive suggestions by the model are diverse enough to yield near-linear speedups
    when evaluated in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that we always utilize the multivariate TPE on the largest
    budget available to ensure that it is fitted on enough budget (high-fidelity)
    to minimize the chance of a noisy estimation. So, combined with the limitation
    on the number of hyperparameter sets passed to the TPE, we are trying to ensure
    that the multivariate TPE is fitted on the right number of hyperparameter sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the pros and cons of utilizing BOHB as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Pros and cons of BOHB'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_06_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – Pros and cons of BOHB
  prefs: []
  type: TYPE_NORMAL
- en: Just as HB may run *nbrackets* times slower compared to random search when we
    are faced with a situation where the bad and good hyperparameters cannot be easily
    distinguished with a small budget value, BOHB will also run *nbrackets* times
    slower compared to the vanilla BO, where we are faced with the same condition.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered BOHB in detail, including what it is, how it
    works, and its pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed the fourth of the four groups of hyperparameter
    tuning methods, called the MFO group. We have discussed MFO in general and what
    makes it different from black-box optimization methods, as well as discussing
    several variants, including CFS, SH, HB, and BOHB. We have seen the differences
    between them and the pros and cons of each. From now on, you should be able to
    explain MFO with confidence when someone asks you about it. You should also be
    able to debug and set up the most suitable configuration for the chosen method
    that suits your specific problem definition.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin implementing the various hyperparameter tuning
    methods that we have learned about so far using the scikit-learn package. We will
    become familiar with the scikit-learn package and learn how to utilize it in various
    hyperparameter tuning methods.
  prefs: []
  type: TYPE_NORMAL
