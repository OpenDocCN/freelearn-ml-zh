- en: EM Algorithm and Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to introduce a very important algorithmic framework
    for many statistical learning tasks: the EM algorithm. Contrary to its name, this
    is not a method to solve a single problem, but a methodology that can be applied
    in several contexts. Our goal is to explain the rationale and show the mathematical
    derivation, together with some practical examples. In particular, we are going
    to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum Likelihood Estimation** (**MLE**) and **Maximum A Posteriori **(**MAP**)
    learning approaches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EM algorithm with a simple application for the estimation of unknown parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gaussian mixture algorithm, which is one the most famous EM applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief explanation of the **Hidden Markov Models** (**HMMs**) forward-backward
    algorithm considering the EM steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLE and MAP learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a data generating process *p*[*data*, ]used to draw
    a dataset *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e94722b0-f5c4-4f12-bcd0-aa81d515c695.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In many statistical learning tasks, our goal is to find the optimal parameter
    set *θ* according to a maximization criterion. The most common approach is based
    on the likelihood and is called MLE. In this case, the optimal set *θ* is found
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1491b637-4841-422a-9351-f60118dc1a36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This approach has the advantage of being unbiased by wrong preconditions, but,
    at the same time, it excludes any possibility of incorporating prior knowledge
    into the model. It simply looks for the best *θ* in a wider subspace, so that
    *p(X|**θ)* is maximized. Even if this approach is almost unbiased, there''s a
    higher probability of finding a sub-optimal solution that can also be quite different
    from a reasonable (even if not sure) prior. After all, several models are too
    complex to allow us to define a suitable prior probability (think, for example,
    of reinforcement learning strategies where there''s a huge number of complex states).
    Therefore, MLE offers the most reliable solution. Moreover, it''s possible to
    prove that the MLE of a parameter *θ* converges in probability to the real value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8205fc93-f5bf-4691-a1d2-c04a54ffa35c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, if we consider Bayes'' theorem, we can derive the following
    relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2542d3e7-10b1-4595-9d67-96821f5f411d.png)'
  prefs: []
  type: TYPE_IMG
- en: The posterior probability, *p(θ|X),* is obtained using both the likelihood and
    a prior probability, *p(θ),* and hence takes into account existing knowledge encoded
    in *p(**θ)*. The choice to maximize *p(θ|X)* is called the MAP approach and it's
    often a good alternative to MLE when it's possible to formulate trustworthy priors
    or, as in the case of **Latent Dirichlet Allocation** (**LDA**), where the model
    is on purpose based on some specific prior assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, a wrong or incomplete prior distribution can bias the model
    leading to unacceptable results. For this reason, MLE is often the default choice
    even when it''s possible to formulate reasonable assumptions on the structure
    of *p(θ)*. To understand the impact of a prior on an estimation, let''s consider
    to have observed *n*=1000 binomial distributed (*θ* corresponds to the parameter
    *p*) experiments and *k*=800 had a successful outcome. The likelihood is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da8aa927-3117-4921-939d-87d3174fc50b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, let''s compute the log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee4937fe-54cc-40b4-9dcf-47683ff96d06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we compute the derivative with respect to *θ* and set it equal to zero,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f3d1309-2ddf-4a0c-8675-a76cd87dcf78.png)'
  prefs: []
  type: TYPE_IMG
- en: So the MLE for *θ* is 0.8, which is coherent with the observations (we can say
    that after observing 1000 experiments with 800 successful outcomes, *p(X|Success)=0.8*).
    If we have only the data *X*, we could say that a success is more likely than
    a failure because 800 out of 1000 experiments are positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, after this simple exercise, an expert can tell us that, considering
    the largest possible population, the marginal probability *p(Success)=0.001* (Bernoulli
    distributed with *p(Failure) = 1 - P(success)*) and our sample is not representative.
    If we trust the expert, we need to compute the posterior probability using Bayes''
    theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3de50cde-852d-42fd-9b8b-360d5c41e81b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Surprisingly, the posterior probability is very close to zero and we should
    reject our initial hypothesis! At this point, there are two options: if we want
    to build a model based only on our data, the MLE is the only reasonable choice,
    because, considering the posterior, we need to accept we have a very poor dataset
    (this is probably a bias when drawing the samples from the data generating process
    *p[data]*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we really trust the expert, we have a few options for
    managing the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the sampling process in order to assess its quality (we can discover
    that a better sampling leads to a very lower *k* value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the MAP estimation of *θ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I suggest that the reader tries both approaches with simple models, to be able
    to compare the relative accuracies. In this book, we're always going to adopt
    the MLE when it's necessary to estimate the parameters of a model with a statistical
    approach. This choice is based on the assumption that our datasets are correctly
    sampled from *p[data]*. If this is not possible (think about an image classifier
    that must distinguish between horses, dogs, and cats, built with a dataset where
    there are pictures of 500 horses, 500 dogs, and 5 cats), we should expand our
    dataset or use data augmentation techniques to create artificial samples.
  prefs: []
  type: TYPE_NORMAL
- en: EM algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EM algorithm is a generic framework that can be employed in the optimization
    of many generative models. It was originally proposed in *Maximum likelihood from
    incomplete data via the em algorithm*, *Dempster A. P.*, *Laird N. M.*, *Rubin
    D. B**.*, *Journal of the Royal Statistical Society*, *B*, *39(1):1–38*, *11/1977*,
    where the authors also proved its convergence at different levels of genericity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we are going to consider a dataset, *X,* and a set of latent
    variables, *Z,* that we cannot observe. They can be part of the original model
    or introduced artificially as a trick to simplify the problem. A generative model
    parameterized with the vector *θ* has a log-likelihood equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cdb05ce-95bf-4574-aadd-e6d994da6abc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, a large log-likelihood implies that the model is able to generate
    the original distribution with a small error. Therefore, our goal is to find the
    optimal set of parameters *θ* that maximizes the marginalized log-likelihood (we
    need to sum—or integrate out for continuous variables—the latent variables out
    because we cannot observe them):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8411ee6-ac8d-4de1-b727-988d89b2bc7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Theoretically, this operation is correct, but, unfortunately, it''s almost
    always impracticable because of its complexity (in particular, the logarithm of
    a sum is often very problematic to manage). However, the presence of the latent
    variables can help us in finding a good proxy that is easy to compute and whose
    maximization corresponds to the maximization of the original log-likelihood. Let''s
    start by rewriting the expression of the likelihood using the chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c215c707-e6f0-4c87-9c6e-8be1bcc24287.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we consider an iterative process, our goal is to find a procedure that satisfies
    the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba47f7df-0d6f-415f-8610-f9a0327754cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can start by considering a generic step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35b2da74-5532-41aa-ad22-6b10fa4aa19e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first problem to solve is the logarithm of the sum. Fortunately, we can
    employ the *Jensen''s inequality*, which allows us to move the logarithm inside
    the summation. Let''s first define the concept of a *convex function*: a function,
    *f(x),* defined on a convex set, *D,* is said to be convex if the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65a9f769-a0d2-47e1-bca0-94f5240a15e4.png)'
  prefs: []
  type: TYPE_IMG
- en: If the inequality is strict, the function is said to be *strictly convex*. Intuitively,
    and considering a function of a single variable *f(x)*, the previous definition
    states that the function is never above the segment that connects two points (*x[1]*,
    *f(x[1])*) and (*x**[2]*, *f(**x[2]**)*). In the case of strict convexity, *f(x)*
    is always below the segment. Inverting these definitions, we obtain the conditions
    for a function to be *concave* or *strictly concave*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a function *f(x)* is concave in *D*, the function *-f(x)* is convex in *D*;
    therefore, as *log(x)* is concave in *[0, ∞)* (or with an equivalent notation
    in *[0, ∞[), -log(x)* is convex in *[0, ∞)*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/330d64e8-255f-4350-8f61-16e03189581b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *Jensen''s inequality* (the proof is omitted but further details can be
    found in *Jensen''s* *Operator Inequality*, *Hansen F.*, *Pedersen G. K.*, arXiv:math/0204049
    [math.OA] states that if *f(x)* is a convex function defined on a convex set *D*,
    if we select n points *x[1]*, *x[2]*, ..., *x[n]* ∈ *D* and *n* constants *λ[1]*, *λ[2]*,
    ..., *λ[n]* ≥ *0* satisfying the condition *λ[1] + λ[2] + ... + λ[n] = 1*, then
    the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee100e07-caba-41fc-a796-d54acf0a812f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, considering that *-log(x)* is convex, the *Jensen''s inequality*
    for *log(x)* becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/302a6644-870e-4feb-ab0b-d3e09eda66e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the generic iterative step can be rewritten, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d651914c-a165-404e-bba1-fde1c04afc0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the Jensen''s inequality, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bdbe909-f1e6-48f0-8f6a-736a1343bc18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All the conditions are met, because the terms *P(z[i]|X, θ[t])* are, by definition,
    bounded between [0, 1] and the sum over all *z* must always be equal to 1 (laws
    of probability). The previous expression implies that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95860f42-dbac-4803-bc80-bcea6c8258b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, if we maximize the right side of the inequality, we also maximize
    the log-likelihood. However, the problem can be further simplified, considering
    that we are optimizing only the parameter vector *θ* and we can remove all the
    terms that don''t depend on it. Hence, we can define a *Q function* (there are
    no relationships with the Q-Learning that we''re going to discuss in [Chapter
    14](51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml), *Introduction to Reinforcement
    Learning*) whose expression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6510d2-dec4-4501-be01-9854f147482f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Q* is the expected value of the log-likelihood considering the complete data
    *Y* = (*X*, *Z*) and the current iteration parameter *set θ[t]*. At each iteration,
    *Q* is computed considering the current estimation *θ*[*t* ]and it''s maximized
    considering the variable *θ*. It''s now clearer why the latent variables can be
    often artificially introduced: they allow us to apply the *Jensen''s inequality*
    and transform the original expression into an expected value that is easy to evaluate
    and optimize.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can formalize the EM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Set a threshold *Thr* (for example, *Thr* = 0.01)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a random parameter vector *θ[0.]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *|L(θ[t]|X, Z) - L(θ[t-1]|X, Z)| >* *Thr*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**E-Step**: Compute the *Q*(*θ*|*θ[t]*). In general, this step consists in
    computing the conditional probability *p*(*z*|*X*, *θ[t]*) or some of its moments
    (sometimes, the sufficient statistics are limited to mean and covariance) using
    the current parameter estimation *θ[t]*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M-Step**: Find *θ[t+1]* = *argmax[θ] Q*(*θ*|*θ[t]*). The new parameter estimation
    is computed to maximize the *Q* function.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The procedure ends when the log-likelihood stops increasing or after a fixed
    number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: An example of parameter estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we see how it's possible to apply the EM algorithm for the
    estimation of unknown parameters (inspired by an example discussed in the original
    paper *Maximum likelihood from incomplete data via the em algorithm*,*Dempster
    A. P.*, *Laird N. M.*, *Rubin D. B.*, *Journal of the Royal Statistical Society*,
    *B, 39(1):1–38*, *11/1977*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a sequence of *n* independent experiments modeled with a multinomial
    distribution with three possible outcomes *x[1]*, *x[2]*, *x[3]* and corresponding
    probabilities *p[1]*, *p[2]* and *p[3]*. The probability mass function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da7e3274-4ca4-400f-b3a2-99677ba9686d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose that we can observe *z[1] = x[1 ]+ x[2]* and *x[3]*, but we
    don''t have any direct access to the single values *x[1]* and *x[2]*. Therefore, *x[1]*
    and *x[2]* are latent variables, while *z[1]* and *x[3]* are observed ones. The
    probability vector *p* is parameterized in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c06aa4ef-5882-43be-b878-b0686432ead3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to find the MLE for *θ* given *n*, *z[1]*, and *x[3]*. Let''s start
    computing the log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce429bb1-7dc7-4769-8ce5-c289b6351bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can derive the expression for the corresponding *Q* function, exploiting
    the linearity of the expected value operator *E*[*•*]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc3db94c-f9a9-44ce-ae49-d20c9fdcacd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variables *x[1]* and *x[2,]* given *z[1,]* are binomially distributed and
    can be expressed as a function of *θ[t]* (we need to recompute them at each iteration).
    Hence, the expected value of *x[1]*^((t*+*1*)*) becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53dfda53-ce60-490a-9575-e6d0570eee65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the expected value of *x[2]*^((*t*+*1*)) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d8e8892-8cc4-4ad8-a3d1-9bacb29a205f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we apply these expressions in ![](img/b0a1acac-af44-42fd-b0a8-56201c4e3774.png) and
    compute the derivative with respect to *θ*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/270056ef-2322-4a4d-8314-9950dc7eb4ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, solving for *θ*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e02ea8ad-1f45-4036-8fc1-8e7f65f7228f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we can derive the iterative expression for *θ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59099387-badd-4905-8cc6-6fa09249e47e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the value of *θ* for *z[1]* = 50 and *x[3]* = 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have parameterized all probabilities and, considering that *z[1] *=
    *x[1 ]*+ x*[2]*, we have one degree of freedom for the choice of *θ*. The reader
    can repeat the example by setting the value of one of *p[1]* or *p[2]* and leaving
    the other probabilities as functions of *θ*. The computation is almost identical
    but in this case, there are no degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction to
    Semi-Supervised Learning*, we discussed the generative Gaussian mixture model
    in the context of semi-supervised learning. In this paragraph, we're going to
    apply the EM algorithm to derive the formulas for the parameter updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start considering a dataset, *X,* drawn from a data generating process,
    *p[data]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c86d60ac-0692-4c32-b3fe-7134cafd9798.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We assume that the whole distribution is generated by the sum of *k* Gaussian
    distributions so that the probability of each sample can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43b2677f-f55c-4bf4-8601-260e4bfb251b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, the term *w[j]* = *P*(*N*=j) is the relative weight
    of the *j^(th)* Gaussian, while *μ*[*j* ]and *Σ[j]* are the mean and the covariance
    matrix. For consistency with the laws of probability, we also need to impose the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3154b0c1-e8e4-418f-92e6-4bc338d9c902.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfortunately, if we try to solve the problem directly, we need to manage the
    logarithm of a sum and the procedure becomes very complex. However, we have learned
    that it's possible to use latent variables as helpers, whenever this trick can
    simplify the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a single parameter set *θ*=(*w[j]*, *μ[j]*, *Σ[j]*) and a latent
    indicator matrix *Z* where each element *z[ij]* is equal to 1 if the point *x[i]* has
    been generated by the *j^(th)* Gaussian, and 0 otherwise. Therefore, each *z[ij]* is
    Bernoulli distributed with parameters equal to *p*(*j*|*x[i]*, *θ[t]*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The joint log-likelihood can hence be expressed using the exponential-indicator
    notation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc57de2f-997f-44c5-b090-a609d0362c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The index, *i,* is referred to the samples, while *j* refers to the Gaussian
    distributions. If we apply the chain rule and the properties of a logarithm, the
    expression becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c1df3d3-f8aa-41d2-bcac-024e140109db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term represents the probability of *x[i]* under the *j^(th)* Gaussian,
    while the second one is the relative weight of the *j^(th)* Gaussian. We can now
    compute the *Q*(*θ*;*θ[t]*) function using the joint log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dbc339d-5608-4170-a1ff-fb330ba49ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Exploiting the linearity of *E[•]*, the previous expression becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45c1ddc3-5b20-4f8e-b97f-4827eca10a48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The term *p*(*j*|*x**[i]*, *θ[t]*) corresponds to the expected value of *z[ij]* considering
    the complete data, and expresses the probability of the *j^(th)* Gaussian given
    the sample *x[i]*. It can be simplified considering Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/682b9783-ce0e-41b0-b56b-5109d6ec3b7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term is the probability of *x[i]* under the *j^(th)* Gaussian with
    parameters *θ[t]*, while the second one is the weight of the *j^(th)* Gaussian
    considering the same parameter set *θ[t]*. In order to derive the iterative expressions
    for the parameters, it''s useful to write the complete formula for the logarithm
    of a multivariate Gaussian distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/631ee1a8-e22e-4b3c-b0a1-9dc90d98eb87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To simplify this expression, we use the trace trick. In fact, as (*x[i]* -* μ[j]*)*^T* Σ^(-1) (*x**[i]* - *μ[j]*)
    is a scalar, we can exploit the properties *tr*(*AB*) = *tr*(*BA*) and *tr*(*c*)
    = *c* where *A* and *B* are matrices and *c* ∈ *ℜ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b77a9af-5e3d-464e-9d0f-083c7e443d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start considering the estimation of the mean (only the first term of
    *Q*(*θ*;*θ**[t]*) depends on mean and covariance):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1b6f81d-ad1a-4f23-aed8-002f1167444d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Setting the derivative equal to zero, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/049ce298-27ae-4414-94ac-31321c8701ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we obtain the expression of the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0f30780-0542-45a2-be5b-060feb432d82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To obtain the iterative expressions for the weights, the procedure is a little
    bit more complex, because we need to use the Lagrange multipliers(further information
    can be found in [http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html](http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html)).
    Considering that the sum of the weights must always be equal to 1, it''s possible
    to write the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/158892a8-78cb-4e03-a49b-195cf5ed898a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Setting both derivatives equal to zero, from the first one, considering that
    *wj* = *p*(*j*|*θ*), we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61be7b2e-d780-412a-b991-adcd21e196a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While from the second derivative, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a87943e-41b8-46b4-9c6b-1b6f923ca193.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last step derives from the fundamental condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe93fa4e-ed32-4de1-bdc9-5136d9e73d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the final expression of the weights is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee17e911-2233-4dec-8113-ea506de214be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we can formalize the Gaussian mixture algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Set random initial values for *w[j]*^((*0*)), *θ*^((*0*))*[j]* and *Σ*^((0))*[j]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-Step**: Compute *p*(*j*|*x[i]*, *θ[t]*) using Bayes'' theorem: *p*(*j*|*x**[i]*, *θ[t]*)
    = *α w*^((*t*))[j] *p*(*x[i]*|*j,* *θ[t]*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M-Step**: Compute *w[j]*^((*t+1*)),* θ*^((*t+1*))[j] and Σ^((t+1))*[j]* using
    the formulas provided previously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process must be iterated until the parameters become stable. In general,
    the best practice is using both a threshold and a maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: An example of Gaussian Mixtures using Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now implement the Gaussian mixture algorithm using the Scikit-Learn
    implementation. The direct approach has already been shown in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml),
    *Introduction to Semi-Supervised Learning*. The dataset is generated to have three
    cluster centers and a moderate overlap due to a standard deviation equal to 1.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a02e97c3-25bf-4a5a-9687-191ed0bac26c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Scikit-Learn implementation is based on the `GaussianMixture` class , which
    accepts as parameters the number of Gaussians (`n_components`), the type of covariance
    (`covariance_type`), which can be `full` (the default value), if all components
    have their own matrix, `tied` if the matrix is shared, `diag` if all components
    have their own diagonal matrix (this condition imposes an uncorrelation among
    the features), and `spherical` when each Gaussian is symmetric in every direction.
    The other parameters allow setting regularization and initialization factors (for
    further information, the reader can directly check the documentation). Our implementation
    is based on full covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After fitting the model, it''s possible to access to the learned parameters
    through the instance variables `weights_`, `means_`, and `covariances_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Considering the covariance matrices, we can already understand that the features
    are very uncorrelated and the Gaussians are almost spherical. The final plot can
    be obtained by assigning each point to the corresponding cluster (Gaussian distribution)
    through the `Yp = gm.transform(X)` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4962263-c815-4c52-9303-449380b931e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Labeled dataset obtained through the application of a Gaussian mixture with
    three components
  prefs: []
  type: TYPE_NORMAL
- en: The reader should have noticed a strong analogy between Gaussian mixture and
    k-means(which we're going to discuss in [Chapter 7](59f765c2-2ad0-4605-826e-349080f85f1f.xhtml),
    *Clustering Algorithms*). In particular, we can state that K-means is a particular
    case of spherical Gaussian mixture with a covariance *Σ* → 0\. This condition
    transforms the approach from a soft clustering, where each sample belongs to all
    clusters with a precise probability distribution, into a hard clustering, where
    the assignment is done by considering the shortest distance between sample and
    centroid (or mean). For this reason, in some books, the Gaussian mixture algorithm
    is also called soft K-means.A conceptually similar approach that we are going
    to present is Fuzzy K-means, which is based on assignments characterized by membership
    functions, which are analogous to probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a Gaussian data generating process, *p*[*data* ]∼ *N*(*0*, *Σ*),
    and *M* n-dimensional zero-centered samples drawn from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd2c1b29-1765-4463-86b2-e448460c1598.png)'
  prefs: []
  type: TYPE_IMG
- en: If *p**[data]* has a mean *μ* ≠ *0*, it's also possible to use this model, but
    it's necessary to account for this non-null value with slight changes in some
    formulas. As the zero-centering normally has no drawbacks, it's easier to remove
    the mean to simplify the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common problems in unsupervised learning is finding a lower
    dimensional distribution *p[lower]* such that the Kullback-Leibler divergence
    with *p[data]* is minimized. When performing a **factor analysis **(**FA**), following
    the original proposal published in *EM algorithms for ML factor analysis*, *Rubin
    D.*, *Thayer D.*, *Psychometrika*, 47/1982, Issue 1, and *The EM algorithm for
    Mixtures of Factor Analyzers*, *Ghahramani Z.*, *Hinton G. E.*, CRC-TG-96-1, 05/1996,
    we start from the assumption to model the generic sample *x* as a linear combination
    of Gaussian latent variables, *z,* (whose dimension *p* is normally *p* < *n*)
    plus an additive and decorrelated Gaussian noise term, *ν*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ce65823-73b9-42bc-978d-55aafab340f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix, A, is called a *factor loading matrix* because it determines the
    contribution of each latent variable (factor) to the reconstruction of *x*. Factors
    and input data are assumed to be statistically independent. Instead, considering
    the last term, if *ω[0]²* ≠ *ω[1]²* ≠ ... ≠ *ω[n]²* the noise is called *heteroscedastic*,
    while it's defined *homoscedastic* if the variances are equal *ω[0]²* = *ω[1]²* =
    ... = *ω[n]²* = *ω²*. To understand the difference between these two kinds of
    noise, think about a signal x which is the sum of two identical voices, recorded
    in different places (for example, an airport and a wood). In this case, we can
    suppose to also have different noise variances (the first one should be higher
    than the second considering the number of different noise sources). If instead
    both voices are recorded in a soundproofed room or even in the same airport, homoscedastic
    noise is surely more likely (we're not considering the power, but the difference
    between the variances).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important strengths of FA in respect to other methods (such
    as PCA) is its intrinsic robustness to heteroscedastic noise. In fact, including
    the noise term in the model (with only the constraint to be decorrelated) allows
    partial denoising filtering based on the single components, while one of the preconditions
    for the PCA is to impose only homoscedastic noise (which, in many cases, is very
    similar to the total absence of noise). Considering the previous example, we could
    make the assumption to have the first variance be *ω[0]²* = *k ω[1]²* with *k*
    > *1*. In this way, the model will be able to understand that a high variance
    in the first component should be considered (with a higher probability) as the
    product of the noise and not an intrinsic property of the component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now analyze the linear relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08e836ef-686e-4155-b7ff-4020e1d5948a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the properties of Gaussian distributions, we know that *x* ∼ *N*(*μ*, *Σ*)
    and it''s easy to determine either the mean or the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aec4d0d8-e9e6-4b5b-b8bf-7765455b39fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, in order to solve the problem, we need to find the best *θ*=(*A*, Ω)
    so that *AA^T* + Ω ≈ *Σ* (with a zero-centered dataset, the estimation is limited
    to the input covariance matrix *Σ*).The ability to cope with noisy variables should
    be clearer now. If *AA^T* + Ω is exactly equal to *Σ* and the estimation of Ω
    is correct, the algorithm will optimize the factor loading matrix A, excluding
    the interference produced by the noise term; therefore, the components will be
    approximately denoised.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to adopt the EM algorithm, we need to determine the joint probability
    *p*(*X*, *z*; *θ*) = *p*(*X*|*z*; *θ*)*p*(*z*|*θ*). The first term on the right
    side can be easily determined, considering that *x* - *Az* ∼ *N*(*0*, Ω); therefore,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1c90edf-a50f-4b9b-8791-c77fc50ec9ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now determine the *Q*(*θ*;*θ[t]*) function, discarding the constant
    term (*2π*)*^k* and term *z^Tz*, which don''t depend on *θ* (in this particular
    case, as we''re going to see, we don''t need to compute the probability *p*(*z*|*X*;*θ*)
    because it''s enough to obtain sufficient statistics for expected value and second
    moment). Moreover, it''s useful to expand the multiplication in the exponential:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/191ed831-9326-49b6-80dd-2df8c532f7e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the trace trick with the last term (which is a scalar), we can rewrite
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69496cf8-3686-40ab-ae89-9cf8bc033dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Exploiting the linearity of *E*[•], we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3c5f66d-411b-4405-8e12-01fa576f4014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This expression is similar to what we have seen in the Gaussian mixture model,
    but in this case, we need to compute the conditional expectation and the conditional
    second moment of *z*. Unfortunately, we cannot do this directly, but it''s possible
    to compute them exploiting the joint normality of *x* and *z*. In particular,
    using a classic theorem, we can partition the full joint probability *p*(*z*,
    *x*), considering the following relations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99948ccc-10dd-4167-9e12-bbaad7488ddf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The conditional distribution *p*(*z*|*x*=*x[i]*) has a mean equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb3c890-8624-4897-9aad-ab99e431cb9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The conditional variance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89e7ea57-3852-4d17-aa49-dd31424899d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the conditional second moment is equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee93199c-6dae-48cc-8bfe-0f99fb74b19e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we define the auxiliary matrix *K* = (*AA^T* + Ω)*^(-1)*, the previous expressions
    become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adaa0e9e-b648-40d5-b776-bb6dd3612434.png)'
  prefs: []
  type: TYPE_IMG
- en: The reader in search of further details about this technique can read *Preview*
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Statistical Decision Theory*, *Pratt J.*, *Raiffa H.*, *Schlaifer
    R.*, *The MIT Press*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the previous expression, it''s possible to build the inverse model (sometimes
    called a *recognition model *because it starts with the effects and rebuilds the
    causes), which is still Gaussian distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a559961b-f113-49c4-831e-707757c833ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now able to maximize *Q*(*θ*;*θ**[t]*) with respect to *A* and Ω, considering
    *θ[t]*=(*A[t]*, Ω*[t]*) and both the conditional expectation and the second moment
    computed according to the previous estimation *θ[t-1]*=(*A[t-1]*, Ω*[t-1])*. For
    this reason, they are not involved in the derivation process. We are adopting
    the convention that the term subject to maximization is computed at time *t*,
    while all the others are obtained through the previous estimations (*t* - *1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/286253c5-2e8b-49d8-9748-c066e1c65a9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression for *A*[*t* ]is therefore as follows (*Q* is the biased input
    covariance matrix *E*[*X^TX*] for a zero-centered dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb5e1019-bad7-4da0-909e-6632d6c7de11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we can obtain an expression for Ω*[t]* by computing the derivative
    with respect to Ω*^(-1)* (this choice simplifies the calculation and doesn''t
    affect the result, because we must set the derivative equal to zero):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08c62f0b-649d-4300-9f88-91bb6883e290.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative of the first term, which is the determinant of a real diagonal
    matrix, is obtained using the adjugate matrix *Adj*(Ω) and exploiting the properties
    of the inverse matrix *T^(-1)* = *det(T)^(-1)Adj(T)* and the properties *det(T)^(-1) =
    det(T^(-1))* and *det(T^T) = det(T)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cefb8de-78af-42e0-80a6-f87cb9177396.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression for Ω[t] (imposing the diagonality constraint) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36b470f5-fcf0-441e-81e0-9f5ffdf6d963.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Summarizing the steps, we can define the completeFA algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Set random initial values for *A^((0))* and Ω*^((0))*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the biased input covariance matrix *Q = E[X^TX]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'E-Step: Compute *A^((t))*, Ω*^((t))*, and *K^((t))*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M-Step: Compute A^((t+1)), Ω*^((t+1))*, and *K**^((t+1))* using the previous
    estimations and the formulas provided previously
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrices *B* and *Ψ* for the inverse model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process must be repeated until *A^((t))*, Ω*^((t))*, and *K**^((t))* stop
    modifying their values (using a threshold) together with a constraint on the maximum
    number of iterations. The factors can be easily obtained using the inverse model
    *z = Bx + λ*.
  prefs: []
  type: TYPE_NORMAL
- en: An example of factor analysis with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now make an example of FA with Scikit-Learn using the MNIST handwritten
    digits dataset (70,000 28 × 28 grayscale images) in the original version and with
    added heteroscedastic noise (*ω[i]* randomly selected from [0, 0.75]).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load and zero-center the original dataset (I''m using
    the functions defined in the first chapter, [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml),
    *Machine Learning Model Fundamentals*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After this step, the  `X `variable  will contain the zero-center original dataset,
    while `Xh` is the noisy version. The following screenshot shows a random selection
    of samples from both versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34920020-dfd7-48dd-b23b-9599754ded36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can perform FA on both datasets using the Scikit-Learn `FactorAnalysis`
    class with the `n_components=64` parameter and check the score (the average log-likelihood
    over all samples). If the noise variance is known (or there''s a good estimation),
    it''s possible to include the starting point through the `noise_variance_init`
    parameter; otherwise, it will be initialized with the identity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the presence of noise has reduced the final accuracy (MLE). Following
    an example provided by *A. Gramfort* and *D. A. Engemann* in the original Scikit-Learn
    documentation, we can create a benchmark for the MLE using the *Lodoit-Wolf* algorithm
    (a shrinking method for improving the condition of the covariance that is beyond
    the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For further information, read *A Well-Conditioned Estimator for Large-Dimensional
    Covariance Matrices*, *Ledoit O.*, *Wolf M.*, *Journal of Multivariate Analysis*,
    88, 2/2004":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With the original dataset, FA performs much better than the benchmark, while
    it''s slightly worse in the presence of heteroscedastic noise. The reader can
    try other combinations using the grid search with different numbers of components
    and noise variances, and experiment with the effect of removing the zero-centering
    step. It''s possible to plot the extracted components using the `components_`
    instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e09541a-e1bf-4653-9b59-b9c0ba0f3647.png)'
  prefs: []
  type: TYPE_IMG
- en: A plot of the 64 components extracted with the factor analysis on the original
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: A careful analysis shows that the components are a superimposition of many low-level
    visual features. This is a consequence of the assumption to have a Gaussian prior
    distribution over the components (*z* ∼ *N(0, I)*). In fact, one of the disadvantages
    of this distribution is its intrinsic denseness (the probability of sampling values
    far from the mean is often too high, while in some case, it would be desirable
    to have a peaked distribution that discourages values not close to its mean, to
    be able to observe more selective components). Moreover, considering the distribution
    *p[Z|X; θ]*, the covariance matrix *ψ* could not be diagonal (trying to impose
    this constraint can lead to an unsolvable problem), leading to a resulting multivariate
    Gaussian distribution, which isn't normally made up of independent components.
    In general, the single variables *z[i,]* (conditioned to an input sample, *x[i]*)
    are statistically dependent and the reconstruction *x[i,]* is obtained with the
    participation of almost all extracted features. In all these cases, we say that
    the *coding is dense* and the dictionary of features in *under-complete* (the
    dimensionality of the components is lower than *dim(x[i])*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The lack of independence can be also an issue considering that any orthogonal
    transformation *Q* applied to *A* (the factor loading matrix) don''t affect the
    distribution *p[X|Z, θ]*. In fact, as *QQ^T=I*, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14717969-bbbd-4109-bb03-de12efafb53e.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, any feature rotation *(x = AQz + ν)* is always a solution to
    the original problem and it's impossible to decide which is the real loading matrix.
    All these conditions lead to the further conclusion that the mutual information
    among components is not equal to zero and neither close to a minimum (in this
    case, each of them carries a specific portion of information). On the other side,
    our main goal was to reduce the dimensionality. Therefore, it's not surprising
    to have dependent components because we aim to preserve the maximum amount of
    original information contained in *p(X)* (remember that the amount of information
    is related to the entropy and the latter is proportional to the variance).
  prefs: []
  type: TYPE_NORMAL
- en: The same phenomenon can be observed in the PCA (which is still based on the
    Gaussian assumption), but in the last paragraph, we're going to discuss a technique,
    calledICA, whose goal is to create a representation of each sample (without the
    constraint of the dimensionality reduction) after starting from a set of statistically
    independent features. This approach, even if it has its peculiarities, belongs
    to a large family of algorithms called *sparse coding*. In this scenario, if the
    corresponding dictionary has *dim(z[i]) > dim(x[i]),*it is called *over-complete*
    (of course, the main goal is no longer the dimensionality reduction).
  prefs: []
  type: TYPE_NORMAL
- en: However, we're going to consider only the case when the dictionary is at most
    complete *dim(z[i]) = dim(x[i]**)*, because ICA with over-complete dictionaries
    requires a more complex approach. The level of sparsity, of course, is proportional
    to *dim(z[i])* and with ICA, it's always achieved as a secondary goal (the primary
    one is always the independence between components).
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common approach to the problem of reducing the dimensionality of a
    high-dimensional dataset is based on the assumption that, normally, the total
    variance is not explained equally by all components. If *p[data]* is a multivariate
    Gaussian distribution with covariance matrix *Σ*, then the entropy (which is a
    measure of the amount of information contained in the distribution) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/779ac8db-243b-486a-8f23-8a7426e7c89e.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, if some components have a very low variance, they also have a limited
    contribution to the entropy, providing little additional information. Hence, they
    can be removed without a high loss of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we''ve done with FA, let''s consider a dataset drawn from *p[data]* ∼
    *N(0, Σ)*(for simplicity, we assume that it''s zero-centered, even if it''s not
    necessary):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14849ea4-4ed0-4929-ba73-832ab5135d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to define a linear transformation, *z = A^Tx* (a vector is normally
    considered a column, therefore *x* has a shape *(n × 1)*), such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/871b2980-2211-4a6d-93df-a285b3aee07e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we want to find out the directions where the variance is higher, we can
    build our transformation matrix, *A,* starting from the eigen decomposition of
    the input covariance matrix, *Σ* (which is real, symmetric, and positive definite):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/685886b8-fd80-4df1-9944-b1dd724b8e7c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*V* is an *(n × n)* matrix containing the eigenvectors (as columns), while Ω
    is a diagonal matrix containing the eigenvalues. Moreover, *V* is also orthogonal,
    hence the eigenvectors constitute a basis. An alternative approach is based on
    the **singular value decomposition** (**SVD**), which has an incremental variant
    and there are algorithms that can perform a decomposition truncated at an arbitrary
    number of components, speeding up the convergence process (such as the Scikit-Learn
    implementation `TruncatedSVD`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, it''s immediately noticeable that the sample covariance is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b19af4dc-fb3a-46f9-a690-6bcedab0fcc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we apply the SVD to the matrix *X* (each row represents a single sample
    with a shape *(1, n)*), we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80bb6690-725d-46c3-b8fc-889d102f8832.png)'
  prefs: []
  type: TYPE_IMG
- en: '*U* is a unitary matrix containing (as rows) the left singular vectors (the
    eigenvectors of *XX^T*), *V* (also unitary) contains (as rows) the right singular
    vectors (corresponding to the eigenvectors of *X^TX*), while *Λ* is a diagonal
    matrix containing the singular values of *Σ[s]* (which are the square roots of
    the eigenvalues of both *XX^(T )*and *X^TX*). Conventionally, the eigenvalues
    are sorted by descending order and the eigenvectors are rearranged to match the
    corresponding position.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we can directly use the matrix *Λ* to select the most relevant eigenvalues
    (the square root is an increasing function and doesn''t change the order) and
    the matrix *V* to retrieve the corresponding eigenvectors (the factor *1/M* is
    a proportionality constant). In this way, we don''t need to compute and eigen
    decompose the covariance matrix *Σ* (contains *n × n* elements) and we can exploit
    some very fast approximate algorithms that work only with the dataset (without
    computing *X^TX*). Using the SVD, the transformation of *X* can be done directly,
    considering that *U* and *V* are unitary matrices (this means that *UU^T* = *U^TU*
    = *I*; therefore, the conjugate transpose is also the inverse):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9e58f1-76cf-4488-b718-eb9cc004ae7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Right now, *X* has only been projected in the eigenvector space (it has been
    simply rotated) and its dimensionality hasn''t changed. However, from the definition
    of the eigenvector, we know that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c9a5609-fbda-4231-b881-0c02e8f1ded0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *λ* is large, the projection of *v* will be amplified proportionally to
    the variance explained by the direction of the corresponding eigenvector. Therefore,
    if it has not been already done, we can sort (and rename) the eigenvalues and
    the corresponding eigenvectors to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a9575b3-b16c-49aa-8b86-97e648bb289d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we select the first top *k* eigenvalues, we can build a transformation matrix
    based on the corresponding eigenvectors (principal components) that projects *X*
    onto a subspace of the original eigenvector space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa6a4060-db56-4110-9299-7892d99b2b60.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the SVD, instead of *A[k]*, we can directly truncate *U* and *Λ*, creating
    the matrices *U[k]* (which contains only the top k eigenvectors) and *Λ[k]*, a
    diagonal matrix with the top k eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing the value for *k*, we are assuming that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbf9019c-eeb6-4ec9-b961-a4c0cfe08aa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To achieve this goal, it is normally necessary to compare the performances
    with a different number of components. In the following graph, there''s a plot
    where the variance ratio (variance explained by component n/total variance) and
    the cumulative variance are plotted as functions of the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/911c3e26-23fd-4da1-9c5a-f0d5a3fcffa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Explained variance per component (left) and cumulative variance per component
    (right)
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the first 10 components are able to explain 80% of the total variance.
    The remaining 25 components have a slighter and slighter impact and could be removed.
    However, the choice must be always based on the specific context, considering
    the loss of value induced by the loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: A trick for determining the right number of components is based on the analysis
    of the eigenvalues of X. After sorting them, it's possible to consider the differences
    between subsequent values d = {λ[1] - λ[2], λ[2] - λ[3], ..., λ[n-1] - λ[n]}.
    The highest difference *λ[k] - λ[k+1]* determines the index *k* of a potential
    optimal reduction (obviously, it's necessary to consider a constraint on the minimum
    value, because normally *λ[1] - λ[2]* is the highest difference). For example,
    if d = {4, 4, 3, 0.2, 0.18, 0.05} the original dimensionality is n=6; however, *λ[4] - λ*[*5* ]is
    the smallest difference, so, it's reasonable to reduce the dimensionality to *(n
    + 1) - k = 3*. The reason is straightforward, the eigenvalues determine the magnitude
    of each component, but we need a relative measure because the scale changes. In
    the example, the last three eigenvectors point to directions where the explained
    variance is negligible when compared to the first three components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve defined the transformation matrix *A[k]*, it''s possible to perform
    the actual projection of the original vectors in the new subspace, through the
    relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ef33cb7-a521-48bb-8f78-5ac18de9fb4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The complete transformation of the whole dataset is simply obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c18c569e-5d93-4006-8ffa-3673914f2e42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s analyze the new covariance matrix *E[Z^TZ]*. If the original distribution
    *p[data] x ∼ N(0, Σ)*, *p(z)* will also be Gaussian with mean and covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba582816-1287-4ec2-b1a5-c23c22c33cc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that *Σ* is orthogonal; therefore, *v[i] • v[j] = 0* if *i ≠ j*. If
    we analyze the term *A^TV*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30a5b689-ff6a-4f72-98cb-1316a6659234.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering that Ω is diagonal, the resulting matrix *Σ[z]* will be diagonal
    as well. This means that the PCA decorrelates the transformed covariance matrix.
    At the same time, we can state that every algorithm that decorrelates the input
    covariance matrix performs a PCA (with or without dimensionality reduction). For
    example, the *whitening process* is a particular PCA without dimensionality reduction,
    while Isomap (see [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning*) performs the same operation working with the Gram matrix
    with a more geometric approach. This result will be used in [Chapter 6](caf0e7e4-42e3-4807-a97d-6f9968eddb29.xhtml),
    *Hebbian Learning*, to show how some particular neural networks can perform a
    PCA without eigen decomposing *Σ*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now consider a FA with homoscedastic noise. We have seen that the covariance
    matrix of the conditional distribution, *p(X|Z; θ),* is equal to *AA^T + Ω*. In
    the case of homoscedastic noise, it becomes *AA^T + ωI*. For a generic covariance
    matrix, *Σ*, it''s possible to prove that adding a constant diagonal matrix *(Σ
    + aI)* doesn''t modify the original eigenvectors and shifts the eigenvalues by
    the same quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84cf4d44-0842-4919-bdb7-5ceffdf8b011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can consider the generic case of absence of noise without loss
    of generality. We know that the goal of FA (with Ω = (0)) is finding the matrix,
    *A,* so that *AA^T ≈ Q* (the input covariance). Hence, thanks to the symmetry
    and imposing the asymptotic equality, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/812743e5-3e18-42d4-8efa-47a614ff5e97.png)'
  prefs: []
  type: TYPE_IMG
- en: This result implies that the FA is a more generic (and robust) way to manage
    the dimensionality reduction in the presence of heteroscedastic noise, and the
    PCA is a restriction to homoscedastic noise. When a PCA is performed on datasets
    affected by heteroscedastic noise, the MLE worsens because the different noise
    components, altering the magnitude of the eigenvalues at different levels, can
    drive to the selection of eigenvectors that, in the original dataset, explain
    only a low percentage of the variance (and in a noiseless scenario, it would be
    normally discarded in favor of more important directions). If you think of the
    example discussed at the beginning of the previous paragraph, we know that the
    noise is strongly heteroscedastic, but we don't have any tools to inform the PCA
    to cope with it and the variance of the first component will be much higher than
    expected, considering that the two sources are identical. Unfortunately, in a
    real- life scenario, the noise is correlated and neither a factor nor a PCA can
    efficiently solve the problem when the noise power is very high. In all those
    cases, more sophisticated denoising techniques must be employed. Whenever, instead,
    it's possible to define an approximate diagonal noise covariance matrix, FA is
    surely more robust and efficient than PCA. The latter should be considered only
    in noiseless or *quasi-*noiseless scenarios. In both cases, the results can never
    lead to well-separated features. For this reason, the ICA has been studied and
    many different strategies have been engineered.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete algorithm for the PCA is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a matrix *X^((M × n))* containing all the samples x[i] as rows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigen decomposition version:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the covariance matrix *Σ = [X^TX]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Eigen decompose *Σ = VΩV^T*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SVD version:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the SVD on the matrix *X = UΛV^T*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the top k eigenvalues (from Ω or Λ) and the corresponding eigenvectors
    (from V)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the matrix A with shape *(n × k),* whose columns are the top k eigenvectors
    (each of them has a shape (n × 1))
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the dataset into the low-dimensional space Z = XA (eigen decomposition)
    or *Z = UΛ (SVD)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Some packages (such as Scipy, which is the backend for many NumPy function,
    such as`np.linalg.svd()`) return the matrix V (right singular vectors) already
    transposed. In this case, it's necessary to use *V^T* instead of V in step 3 of
    the algorithm. I suggest always checking the documentation when implementing these
    kinds of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: An example of PCA with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can repeat the same experiment made with the FA and heteroscedastic noise
    to assess the MLE score of the PCA. We are going to use the `PCA` class with the
    same number of components (`n_components=64`). To achieve the maximum accuracy,
    we also set the  `svd_solver=''full'' `parameter, to force Scikit-Learn to apply
    a full SVD instead of the truncated version. In this way, the top eigenvalues
    are selected only after the decomposition, avoiding the risk of imprecise estimations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is not surprising: the MLE is much lower than FA, because of the
    wrong estimations made due to the heteroscedastic noise. I invite the reader to
    compare the results with different datasets and noise levels, considering that
    the training performance of PCA is normally higher than FA. Therefore, when working
    with large datasets, a good trade-off is surely desirable. As with FA, it''s possible
    to retrieve the components through the  `components_ `instance variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s interesting to check the total explained variance (as a fraction of the
    total input variance) through the component-wise instance array `explained_variance_ratio_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With 64 components, we are explaining 86% of the total input variance. Of course,
    it''s also useful to compare the explained variance using a plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01a974ed-00a8-41bd-aabe-e99aedde239f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As usual, the first components explain the largest part of the variance; however,
    after about the twentieth component, each contribution becomes lower than 1% (decreasing
    till about 0%). This analysis suggests two observations: it''s possible to further
    reduce the number of components with an acceptable loss (using the previous snippet,
    it''s easy to extend the sum only the first *n* components and compare the results)
    and, at the same time, the PCA will be able to overcome a higher threshold (such
    as 95%) only by adding a large number of new components. In this particular case,
    we know that the dataset is made up of handwritten digits; therefore, we can suppose
    that the tail is due to secondary differences (a line slightly longer than average,
    a marked stroke, and so on); hence, we can drop all the components with n > 64
    (or less) without problems (it''s also easy to verify visually a rebuilt image
    using the `inverse_transform()` method). However, it is always best practice to
    perform a complete analysis before moving on to further processing steps, particularly
    when the dimensionality of X is high.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting approach to determine the optimal number of components has
    been proposed by Minka (*Automatic Choice of Dimensionality for PCA*, *Minka T.P.*,
    *NIPS* 2000") and it's based on the Bayesian model selection. The idea is to use
    the MLE to optimize the likelihood *p(X|k)* where k is a parameter indicating
    the number of components. In other words, it doesn't start analyzing the explained
    variance, but determines a value of *k < n* so that the likelihood keeps being
    the highest possible (implicitly, k will explain the maximum possible variance
    under the constraint of *max(k) = k[max]*). The theoretical foundation (with tedious
    mathematical derivations) of the method is presented in the previously mentioned
    paper however, it's possible to use this method with Scikit-Learn by setting the
    `n_components='mle'` and `svd_solver='full'` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Independent component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that the factors extracted by a PCA are decorrelated, but not independent.
    A classic example is the *cocktail party:* we have a recording of many overlapped
    voices and we would like to separate them. Every single voice can be modeled as
    a random process and it's possible to assume that they are statistically independent
    (this means that the joint probability can be factorized using the marginal probabilities
    of each source). Using FA or PCA, we are able to find uncorrelated factors, but
    there's no way to assess whether they are also independent (normally, they aren't).
    In this section, we are going to study a model that is able to produce sparse
    representations (when the dictionary isn't under-complete) with a set of statistically
    independent components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we have a zero-centered and whitened dataset *X* sampled from
    *N(0, I)* and noiseless linear transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/019cead7-d719-4320-97b3-2464b44ea74b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the prior over, *z,* is modeled as a product of independent variables
    (*α* is the normalization factor), each of them represented as a generic exponential
    where the function *f[k](z)* must be non-quadratic, that is, *p(z; θ)* cannot
    be Gaussian. Furthermore, we assume that the variance of *z[i]* is equal to 1,
    therefore, *p(x|z; **θ) ∼ N(Az, AA^T)*. The joint probability *p(X, z; θ) = p(X|z;
    θ)p(z|θ)* is equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4246f391-0499-415a-9f92-52bd8ce9c61b.png)'
  prefs: []
  type: TYPE_IMG
- en: If *X* has been whitened, *A* is orthogonal (the proof is straightforward);
    hence, the previous expression can be simplified. However, applying the EM algorithm
    requires determining *p(z|X; θ)* and this is quite difficult. The process could
    be easier after choosing a suitable prior distribution for *z*, that is, *f[k](z)*,
    but as we discussed at the beginning of the chapter, this assumption can have
    dramatic consequences if the real factors are distributed differently. For these
    reasons, other strategies have been studied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main concept that we need to enforce is having a non-Gaussian distribution
    of the factors. In particular, we''d like to have a peaked distribution (inducing
    sparseness) with heavy tails. From the theory, we know that the standardized fourth
    moment (also called *Kurtosis*) is a perfect measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f63dbea7-b46f-4bd9-9ee8-6c69aa3fe559.png)'
  prefs: []
  type: TYPE_IMG
- en: For a Gaussian distribution, *Kurt[X]* is equal to three (which is often considered
    as the reference point, determining the so called *Excess Kurtosis* = *Kurtosis
    - 3*), while it's larger for a family of distributions, called *Leptokurtotic*
    or super-Gaussian, which are peaked and heavy-tailed (also, the distributions
    with *Kurt[X] < 3*, called *Platykurtotic* or sub-Gaussian, can be good candidates,
    but they are less peaked and normally only the super-Gaussian distributions are
    taken into account). However, even if accurate, this measure is very sensitive
    to outliers because of the fourth power. For example, if *x ∼ N(0, 1)* and *z
    = x + ν*, where *ν* is a noise term that alters a few samples, increasing their
    value to two, the result can be a super-Gaussian (*Kurt[x] > 3*) even if, after
    filtering the outliers out, the distribution has *Kurt[x] = 3 (Gaussian)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this problem, Hyvärinen and Oja (*Independent Component Analysis:
    Algorithms and Applications*, *Hyvarinen A.*, *Oja E.*, Neural Networks 13/2000)
    proposed a solution based on another measure, the *negentropy*. We know that the
    entropy is proportional to the variance and, given the variance, the Gaussian
    distribution has the maximum entropy (for further information, read *Mathematical
    Foundations of Information Theory*, *Khinchin A. I., Dover Publications*); therefore,
    we can define the measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7966184b-0868-4d64-bf41-803973421c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Formally, the negentropy of *X* is the difference between the entropy of a Gaussian
    distribution with the same covariance and the entropy of *X* (we are assuming
    both zero-centered). It's immediately possible to understand that *H[N](X) ≥ 0*,
    hence the only way to maximize it is by reducing *H(X)*. In this way, *X* becomes
    less random, concentrating the probability around the mean (in other words, it
    becomes super-Gaussian). However, the previous expression cannot be easily adapted
    to closed-form solutions, because *H(X)* needs to be computed over all the distribution
    of *X*, which must be estimated. For this reason, the same authors proposed an
    approximation based on non-quadratic functions (remember that in the context of
    ICA, a quadratic function can be never be employed because it would lead to a
    Gaussian distribution) that is useful to derive a fixed-point iterative algorithm
    called *FastICA*(indeed, it's really faster than EM).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using k functions *f[k](x)*, the approximation becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0993dabf-0638-4931-8d07-0e436cb9c81a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In many real-life scenarios, a single function is enough to achieve a reasonable
    accuracy and one of the most common choices for f(x) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b421b596-abdb-46d4-a009-bf899ca38553.png)'
  prefs: []
  type: TYPE_IMG
- en: In the aforementioned paper, the reader can find some alternatives that can
    be employed when this function fails in forcing statistical independence between
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we invert the model, we get *z = Wx* with *W = A^(-1)*; therefore, considering
    a single sample, the approximation becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13fc3cb9-3bc8-40c8-b86c-562573caee5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, the second term doesn''t depend on *w* (in fact, it''s only a reference)
    and can be excluded from the optimization. Moreover, considering the initial assumptions,
    *E[Z^TZ]=W E[X^TX] W^T = I*, therefore *WW^T = I, i.e. ||w||² = 1*. Hence, our
    goal is to find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e460161-d1a1-47ac-bb9e-9b966292ef27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this way, we are forcing the matrix *W* to transform the input vector *x*,
    so that *z* has the lowest possible entropy; therefore, it''s super-Gaussian.
    The maximization process is based on convex optimization techniques that are beyond
    the scope of this book (the reader can find all the details of Lagrange theorems
    in *Luenberger D. G., Optimization by Vector Space Methods, Wiley*); therefore,
    we directly provide the iterative step that must be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8463cdf0-16e2-421f-9612-23fb5740d83f.png)'
  prefs: []
  type: TYPE_IMG
- en: Of course, to ensure *||w||²** = 1*, after each step, the weight vector w must
    be normalized *(w[t+1] = w[t+1] / ||w[t+1]||)*.
  prefs: []
  type: TYPE_NORMAL
- en: In a more general context, the matrix *W* contains more than one weight vector
    and, if we apply the previous rule to find out the independent factors, it can
    happen that some elements, *w[i]^Tx,* are correlated. A strategy to avoid this
    problem is based on the gram-schmidt orthonormalization process, which decorrelates
    the components one by one, subtracting the projections of the current component
    *(w[n])* onto all the previous ones *(w[1], w[2], ..., w[n-1])* to *w[n]*. In
    this way, *w[n]* is forced to be orthogonal to all the other components.
  prefs: []
  type: TYPE_NORMAL
- en: Even if this method is simple and doesn't require much effort, it's preferable
    a global approach that can work directly with the matrix *W* at the end of an
    iteration (so that the order of the weights is not fixed). As explained in *Fast
    and robust fixedpoint*
  prefs: []
  type: TYPE_NORMAL
- en: '*algorithms for independent component analysis*, *Hyvarinen A.*, *IEEE Transactions
    on Neural Networks* this result can be achieved with a simple sub-algorithm that
    we are including in the final *FastICA* algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Set random initial values for *W[0]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a threshold *Thr* (for example 0.001)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Independent component extraction
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *w* in *W*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||w[t+1] - w[t]|| > Thr*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *w[t+1] = E[x · f^'(w[t]^Tx)] - E[f^('')(w[t]^Tx)] w[t]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*w[t+1] = w[t+1] / ||w[t+1]||*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Orthonormalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||W[t+1] - W[t]||[F] > Thr*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*W[t] = W[t] / sqrt(||W[t]W[t]^T||)*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*W[t][+1] = (3/2)W[t] - (1/2)WW^TW*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This process can be also iterated for a fixed number of times, but the best
    approach is based on using both a threshold and a maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: An example of FastICA with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the same dataset, we can now test the performance of the ICA. However,
    in this case, as explained, we need to zero-center and whiten the dataset, but
    fortunately these preprocessing steps are done by the Scikit-Learn implementation
    (if the parameter `whiten=True` is omitted).
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the ICA on the MNIST dataset, we''re going to instantiate the  `FastICA `class,
    passing the arguments `n_components=64` and the maximum number of iterations `max_iter=5000`.
    It''s also possible to specify which function will be used to approximate the
    negentropy; however, the default is *log cosh(x)*, which is normally a good choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can visualize the components (which are always available
    through the  `components_ `instance variance):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50c13e9a-68a3-4d63-88fd-56de1116c432.png)'
  prefs: []
  type: TYPE_IMG
- en: Independent components of the MNIST dataset extracted by the FastICA algorithm
    (64 components)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are still some redundancies (the reader can try to increase the number
    of components) and background noise; however, it''s now possible to distinguish
    some low-level features (such as oriented stripes) that are common to many digits.
    This representation isn''t very sparse yet. In fact, we''re always using 64 components
    (like for FA and PCA); therefore, the dictionary is under-complete (the input
    dimensionality is 28 × 28 = 784). To see the difference, we can repeat the experiment
    with a dictionary ten times larger, setting `n_components=640`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A subset of the new components (100) is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2ba3dfa-30ae-4a9d-aaa4-b8b58c52b910.png)'
  prefs: []
  type: TYPE_IMG
- en: Independent components of the MNIST dataset extracted by the FastICA algorithm
    (640 components)
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of these components is almost elementary. They represent oriented
    stripes and positional dots. To check how an input is rebuilt, we can consider
    the mixing matrix *A* (which is available as the `mixing_ `instance variable).
    Considering the first input sample, we can check how many factors have a weight
    less than half of the average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The sample is rebuilt using approximately 410 components. The level of sparsity
    is higher, but considering the granularity of the factors, it's easy to understand
    that many of them are needed to rebuild even a single structure (like the image
    of a 1) where long lines are present. However, this is not a drawback because,
    as already mentioned, the main goal of the ICA is to extract independent components.
    Considering an analogy with the *cocktail party* example, we could deduce that
    each component represents a phoneme, not the complete sound of a word or a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: The reader can test a different number of components and compare the results
    with the ones achieved by other sparse coding algorithms (such as Dictionary Learning
    or Sparse PCA).
  prefs: []
  type: TYPE_NORMAL
- en: Addendum to HMMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed how it''s possible to train a HMM using
    the forward-backward algorithmand we have seen that it is a particular application
    of the EM algorithm. The reader can now understand the internal dynamic in terms
    of E and M steps. In fact, the procedure starts with randomly initialized A and
    B matrices and proceeds in an alternating manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation of the probability *α^t[ij]* that the HMM is in the state *i*
    at time *t* and in the state *j* at time *t+1* given the observations and the
    current parameter estimations (A and B)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation of the probability *β^t*[*i* ]that the HMM is in the state *i*
    at time *t* given the observations and the current parameter estimations (A and
    B)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M-Step**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the new estimation for the transition probabilities *a[ij] (A)* and
    for the emission probabilities *b[ip] (B)*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The procedure is repeated until the convergence is reached. Even if there's
    no explicit definition of a Q function, the E-step determines a split expression
    for the expected complete data likelihood of the model given the observations
    (using both the Forward and Backward algorithms), while the M-Step corrects parameters
    A and B to maximize this likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the EM algorithm, explaining the reasons that
    justify its application in many statistical learning contexts. We also discussed
    the fundamental role of hidden (latent) variables, in order to derive an expression
    that is easier to maximize (the Q function).
  prefs: []
  type: TYPE_NORMAL
- en: We applied the EM algorithm to solve a simple parameter estimation problem and
    afterward to prove the Gaussian Mixture estimation formulas. We showed how it's
    possible to employ the Scikit-Learn implementation instead of writing the whole
    procedure from scratch (like in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*).
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, we analyzed three different approaches to component extraction. FA
    assumes that we have a small number of Gaussian latent variables and a Gaussian
    decorrelated noise term. The only restriction on the noise is to have a diagonal
    covariance matrix, so two different scenarios are possible. When we are in the
    presence of heteroscedastic noise, the process is an actual FA. When, instead,
    the noise is homoscedastic, the algorithm becomes the equivalent of a PCA. In
    this case, the process is equivalent to check the sample space in order to find
    the directions where the variance is higher. Selecting only the most important
    directions, we can project the original dataset onto a low-dimensional subspace,
    where the covariance matrix becomes decorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems of both FA and PCA is their assumption to model the latent
    variables with Gaussian distributions. This choice simplifies the model, but at
    the same time, yields dense representations where the single components are statistically
    dependent. For this reason, we have investigated how it's possible to force the
    factor distribution to become sparse. The resulting algorithm, which is generally
    faster and more accurate than the MLE, is called FastICA and its goal is to extract
    a set of statistically independent components with the maximization of an approximation
    of the negentropy.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we provided a brief explanation of the HMM forward-backward algorithm
    (discussed in the previous chapter) considering the subdivision into E and M steps.
    Other EM-specific applications will be discussed in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce the fundamental concepts of Hebbian
    learning and self-organizing maps, which are still very useful to solve many specific
    problems, such as principal component extraction, and have a strong neurophysiological
    foundation.
  prefs: []
  type: TYPE_NORMAL
