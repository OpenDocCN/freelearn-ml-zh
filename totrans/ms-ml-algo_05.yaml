- en: EM Algorithm and Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EM算法及其应用
- en: 'In this chapter, we are going to introduce a very important algorithmic framework
    for many statistical learning tasks: the EM algorithm. Contrary to its name, this
    is not a method to solve a single problem, but a methodology that can be applied
    in several contexts. Our goal is to explain the rationale and show the mathematical
    derivation, together with some practical examples. In particular, we are going
    to discuss the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍许多统计学习任务的一个重要算法框架：EM算法。与它的名字相反，这不是解决单个问题的方法，而是一种可以在多个环境中应用的方法论。我们的目标是解释其原理，展示数学推导，以及一些实际例子。特别是，我们将讨论以下主题：
- en: '**Maximum Likelihood Estimation** (**MLE**) and **Maximum A Posteriori **(**MAP**)
    learning approaches'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大似然估计** (**MLE**)和**最大后验**(**MAP**)学习方法'
- en: The EM algorithm with a simple application for the estimation of unknown parameters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有简单应用的EM算法以估计未知参数
- en: The Gaussian mixture algorithm, which is one the most famous EM applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合算法，这是EM应用中最著名的之一
- en: Factor analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析** (**PCA**)'
- en: '**Independent Component Analysis** (**ICA**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析** (**ICA**)'
- en: A brief explanation of the **Hidden Markov Models** (**HMMs**) forward-backward
    algorithm considering the EM steps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到EM步骤的**隐马尔可夫模型** (**HMM**)的前向-后向算法的简要说明
- en: MLE and MAP learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLE和MAP学习
- en: 'Let''s suppose we have a data generating process *p*[*data*, ]used to draw
    a dataset *X*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据生成过程 *p*[*data*]，用于生成数据集 *X*：
- en: '![](img/e94722b0-f5c4-4f12-bcd0-aa81d515c695.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e94722b0-f5c4-4f12-bcd0-aa81d515c695.png)'
- en: 'In many statistical learning tasks, our goal is to find the optimal parameter
    set *θ* according to a maximization criterion. The most common approach is based
    on the likelihood and is called MLE. In this case, the optimal set *θ* is found
    as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多统计学习任务中，我们的目标是根据最大化标准找到最优参数集 *θ*。最常见的方法是基于似然，称为MLE。在这种情况下，最优集 *θ* 的寻找如下：
- en: '![](img/1491b637-4841-422a-9351-f60118dc1a36.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1491b637-4841-422a-9351-f60118dc1a36.png)'
- en: 'This approach has the advantage of being unbiased by wrong preconditions, but,
    at the same time, it excludes any possibility of incorporating prior knowledge
    into the model. It simply looks for the best *θ* in a wider subspace, so that
    *p(X|**θ)* is maximized. Even if this approach is almost unbiased, there''s a
    higher probability of finding a sub-optimal solution that can also be quite different
    from a reasonable (even if not sure) prior. After all, several models are too
    complex to allow us to define a suitable prior probability (think, for example,
    of reinforcement learning strategies where there''s a huge number of complex states).
    Therefore, MLE offers the most reliable solution. Moreover, it''s possible to
    prove that the MLE of a parameter *θ* converges in probability to the real value:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势是不受错误先决条件的影响，但与此同时，它排除了将先验知识纳入模型的可能性。它只是在更广泛的子空间中寻找最佳的 *θ*，以便最大化 *p(X|θ)*。即使这种方法几乎是无偏的，也有更高的概率找到次优解，这可能与合理的（即使不确定）先验相当不同。毕竟，有些模型太复杂，以至于我们无法定义合适的先验概率（例如，考虑强化学习策略，其中存在大量复杂的状态）。因此，MLE提供了最可靠的解决方案。此外，可以证明参数
    *θ* 的MLE在概率上收敛到真实值：
- en: '![](img/8205fc93-f5bf-4691-a1d2-c04a54ffa35c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8205fc93-f5bf-4691-a1d2-c04a54ffa35c.png)'
- en: 'On the other hand, if we consider Bayes'' theorem, we can derive the following
    relation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们考虑贝叶斯定理，我们可以推导出以下关系：
- en: '![](img/2542d3e7-10b1-4595-9d67-96821f5f411d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2542d3e7-10b1-4595-9d67-96821f5f411d.png)'
- en: The posterior probability, *p(θ|X),* is obtained using both the likelihood and
    a prior probability, *p(θ),* and hence takes into account existing knowledge encoded
    in *p(**θ)*. The choice to maximize *p(θ|X)* is called the MAP approach and it's
    often a good alternative to MLE when it's possible to formulate trustworthy priors
    or, as in the case of **Latent Dirichlet Allocation** (**LDA**), where the model
    is on purpose based on some specific prior assumptions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率 *p(θ|X)* 是通过似然和先验概率 *p(θ)* 共同获得的，因此考虑了编码在 *p(θ)* 中的现有知识。选择最大化 *p(θ|X)*
    的方法称为MAP方法，当可以制定可信的先验或，如在**潜在狄利克雷分配** (**LDA**)的情况下，模型故意基于一些特定的先验假设时，它通常是MLE的一个很好的替代方案。
- en: 'Unfortunately, a wrong or incomplete prior distribution can bias the model
    leading to unacceptable results. For this reason, MLE is often the default choice
    even when it''s possible to formulate reasonable assumptions on the structure
    of *p(θ)*. To understand the impact of a prior on an estimation, let''s consider
    to have observed *n*=1000 binomial distributed (*θ* corresponds to the parameter
    *p*) experiments and *k*=800 had a successful outcome. The likelihood is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，一个错误或不完整的先验分布可能会使模型产生不可接受的结果。因此，MLE通常是默认选择，即使有可能对*p(θ)*的结构提出合理的假设。为了理解先验对估计的影响，让我们考虑观察到*n*=1000个二项分布（*θ*对应于参数*p*）的实验，其中*k*=800次成功。似然如下：
- en: '![](img/da8aa927-3117-4921-939d-87d3174fc50b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da8aa927-3117-4921-939d-87d3174fc50b.png)'
- en: 'For simplicity, let''s compute the log-likelihood:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们计算对数似然：
- en: '![](img/ee4937fe-54cc-40b4-9dcf-47683ff96d06.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee4937fe-54cc-40b4-9dcf-47683ff96d06.png)'
- en: 'If we compute the derivative with respect to *θ* and set it equal to zero,
    we get the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算相对于*θ*的导数并将其设置为等于零，我们得到以下结果：
- en: '![](img/5f3d1309-2ddf-4a0c-8675-a76cd87dcf78.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f3d1309-2ddf-4a0c-8675-a76cd87dcf78.png)'
- en: So the MLE for *θ* is 0.8, which is coherent with the observations (we can say
    that after observing 1000 experiments with 800 successful outcomes, *p(X|Success)=0.8*).
    If we have only the data *X*, we could say that a success is more likely than
    a failure because 800 out of 1000 experiments are positive.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*θ*的MLE是0.8，这与观察结果一致（我们可以这样说，在观察了1000次实验，其中800次成功之后，*p(X|Success)=0.8*）。如果我们只有数据*X*，我们可以说成功比失败更有可能，因为1000次实验中有800次是积极的。
- en: 'However, after this simple exercise, an expert can tell us that, considering
    the largest possible population, the marginal probability *p(Success)=0.001* (Bernoulli
    distributed with *p(Failure) = 1 - P(success)*) and our sample is not representative.
    If we trust the expert, we need to compute the posterior probability using Bayes''
    theorem:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个简单的练习之后，专家可以告诉我们，考虑到最大的可能人群，边缘概率*p(Success)=0.001*（伯努利分布，*p(Failure) =
    1 - P(success)*）并且我们的样本不具有代表性。如果我们信任专家，我们需要使用贝叶斯定理计算后验概率：
- en: '![](img/3de50cde-852d-42fd-9b8b-360d5c41e81b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3de50cde-852d-42fd-9b8b-360d5c41e81b.png)'
- en: 'Surprisingly, the posterior probability is very close to zero and we should
    reject our initial hypothesis! At this point, there are two options: if we want
    to build a model based only on our data, the MLE is the only reasonable choice,
    because, considering the posterior, we need to accept we have a very poor dataset
    (this is probably a bias when drawing the samples from the data generating process
    *p[data]*).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，后验概率非常接近零，我们应该拒绝我们的初始假设！在这个时候，有两个选择：如果我们只想基于我们的数据构建模型，MLE是唯一合理的选项，因为考虑到后验，我们需要接受我们有一个非常差的数据集（这可能是从数据生成过程*p[data]*抽取样本时的偏差）。
- en: 'On the other hand, if we really trust the expert, we have a few options for
    managing the problem:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们真的信任专家，我们有几种处理问题的方法：
- en: Checking the sampling process in order to assess its quality (we can discover
    that a better sampling leads to a very lower *k* value)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查采样过程以评估其质量（我们可以发现更好的采样会导致非常低的*k*值）
- en: Increasing the number of samples
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加样本数量
- en: Computing the MAP estimation of *θ*
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算*θ*的MAP估计
- en: I suggest that the reader tries both approaches with simple models, to be able
    to compare the relative accuracies. In this book, we're always going to adopt
    the MLE when it's necessary to estimate the parameters of a model with a statistical
    approach. This choice is based on the assumption that our datasets are correctly
    sampled from *p[data]*. If this is not possible (think about an image classifier
    that must distinguish between horses, dogs, and cats, built with a dataset where
    there are pictures of 500 horses, 500 dogs, and 5 cats), we should expand our
    dataset or use data augmentation techniques to create artificial samples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议读者尝试使用简单模型来比较两种方法，以便能够比较相对精度。在这本书中，当我们需要使用统计方法估计模型的参数时，我们总是会采用MLE。这个选择基于我们的数据集是从*p[data]*正确采样的假设。如果这不可能（想想一个必须区分马、狗和猫的图像分类器，它使用的数据集中有500张马的照片，500张狗的照片和5张猫的照片），我们应该扩展我们的数据集或使用数据增强技术来创建人工样本。
- en: EM algorithm
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EM算法
- en: The EM algorithm is a generic framework that can be employed in the optimization
    of many generative models. It was originally proposed in *Maximum likelihood from
    incomplete data via the em algorithm*, *Dempster A. P.*, *Laird N. M.*, *Rubin
    D. B**.*, *Journal of the Royal Statistical Society*, *B*, *39(1):1–38*, *11/1977*,
    where the authors also proved its convergence at different levels of genericity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法是一个通用的框架，可以用于许多生成模型的优化。它最初由*Dempster A. P.*，*Laird N. M.*，*Rubin D. B**.*在*《通过EM算法从不完全数据中估计最大似然》*，*《皇家统计学会杂志》*，*B*，*39(1):1–38*，*1977年11月*提出，其中作者还证明了其在不同通用的水平上的收敛性。
- en: 'For our purposes, we are going to consider a dataset, *X,* and a set of latent
    variables, *Z,* that we cannot observe. They can be part of the original model
    or introduced artificially as a trick to simplify the problem. A generative model
    parameterized with the vector *θ* has a log-likelihood equal to the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们将考虑一个数据集，*X*，以及一组我们无法观察到的潜在变量，*Z*。它们可以是原始模型的一部分，或者人为地作为简化问题的技巧引入。用向量*θ*参数化的生成模型具有等于以下的对数似然：
- en: '![](img/1cdb05ce-95bf-4574-aadd-e6d994da6abc.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cdb05ce-95bf-4574-aadd-e6d994da6abc.png)'
- en: 'Of course, a large log-likelihood implies that the model is able to generate
    the original distribution with a small error. Therefore, our goal is to find the
    optimal set of parameters *θ* that maximizes the marginalized log-likelihood (we
    need to sum—or integrate out for continuous variables—the latent variables out
    because we cannot observe them):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，大的对数似然意味着模型能够以小的误差生成原始分布。因此，我们的目标是找到最优的参数集*θ*，以最大化边缘对数似然（由于我们无法观察它们，我们需要对潜在变量求和或积分）：
- en: '![](img/f8411ee6-ac8d-4de1-b727-988d89b2bc7a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8411ee6-ac8d-4de1-b727-988d89b2bc7a.png)'
- en: 'Theoretically, this operation is correct, but, unfortunately, it''s almost
    always impracticable because of its complexity (in particular, the logarithm of
    a sum is often very problematic to manage). However, the presence of the latent
    variables can help us in finding a good proxy that is easy to compute and whose
    maximization corresponds to the maximization of the original log-likelihood. Let''s
    start by rewriting the expression of the likelihood using the chain rule:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，这个操作是正确的，但不幸的是，由于其复杂性（特别是求和的对数通常很难处理），它几乎总是不可行的。然而，潜在变量的存在可以帮助我们找到一个容易计算的良好代理，其最大化对应于原始对数似然的最大化。让我们首先使用链式法则重写似然的表达式：
- en: '![](img/c215c707-e6f0-4c87-9c6e-8be1bcc24287.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c215c707-e6f0-4c87-9c6e-8be1bcc24287.png)'
- en: 'If we consider an iterative process, our goal is to find a procedure that satisfies
    the following condition:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑一个迭代过程，我们的目标是找到一个满足以下条件的程序：
- en: '![](img/ba47f7df-0d6f-415f-8610-f9a0327754cd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba47f7df-0d6f-415f-8610-f9a0327754cd.png)'
- en: 'We can start by considering a generic step:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从考虑一个通用的步骤开始：
- en: '![](img/35b2da74-5532-41aa-ad22-6b10fa4aa19e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35b2da74-5532-41aa-ad22-6b10fa4aa19e.png)'
- en: 'The first problem to solve is the logarithm of the sum. Fortunately, we can
    employ the *Jensen''s inequality*, which allows us to move the logarithm inside
    the summation. Let''s first define the concept of a *convex function*: a function,
    *f(x),* defined on a convex set, *D,* is said to be convex if the following applies:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个要解决的问题是对数和。幸运的是，我们可以使用*詹森不等式*，它允许我们将对数移到求和内部。让我们首先定义*凸函数*的概念：一个在凸集*D*上定义的函数*f(x)*，如果满足以下条件，则称为凸函数：
- en: '![](img/65a9f769-a0d2-47e1-bca0-94f5240a15e4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65a9f769-a0d2-47e1-bca0-94f5240a15e4.png)'
- en: If the inequality is strict, the function is said to be *strictly convex*. Intuitively,
    and considering a function of a single variable *f(x)*, the previous definition
    states that the function is never above the segment that connects two points (*x[1]*,
    *f(x[1])*) and (*x**[2]*, *f(**x[2]**)*). In the case of strict convexity, *f(x)*
    is always below the segment. Inverting these definitions, we obtain the conditions
    for a function to be *concave* or *strictly concave*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不等式是严格的，那么函数被称为*严格凸的*。直观地，考虑一个单变量函数*f(x)*，前面的定义表明函数永远不会高于连接两个点(*x[1]*, *f(x[1])*)和(*x**[2]*,
    *f(**x[2]**)*)的线段。在严格凸性的情况下，*f(x)*总是位于该线段下方。逆这些定义，我们得到函数是*凹的*或*严格凹的*的条件。
- en: 'If a function *f(x)* is concave in *D*, the function *-f(x)* is convex in *D*;
    therefore, as *log(x)* is concave in *[0, ∞)* (or with an equivalent notation
    in *[0, ∞[), -log(x)* is convex in *[0, ∞)*, as shown in the following diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数 *f(x)* 在 *D* 上是凹的，那么函数 *-f(x)* 在 *D* 上是凸的；因此，由于 *log(x)* 在 *[0, ∞)* 上是凹的（或用等价记法在
    *[0, ∞[) 上），-log(x)* 在 *[0, ∞)* 上是凸的，如下图所示：
- en: '![](img/330d64e8-255f-4350-8f61-16e03189581b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/330d64e8-255f-4350-8f61-16e03189581b.png)'
- en: 'The *Jensen''s inequality* (the proof is omitted but further details can be
    found in *Jensen''s* *Operator Inequality*, *Hansen F.*, *Pedersen G. K.*, arXiv:math/0204049
    [math.OA] states that if *f(x)* is a convex function defined on a convex set *D*,
    if we select n points *x[1]*, *x[2]*, ..., *x[n]* ∈ *D* and *n* constants *λ[1]*, *λ[2]*,
    ..., *λ[n]* ≥ *0* satisfying the condition *λ[1] + λ[2] + ... + λ[n] = 1*, then
    the following applies:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jensen 不等式*（证明被省略，但更详细的内容可以在 *Jensen 的算子不等式*，*Hansen F.*，*Pedersen G. K.*，arXiv:math/0204049
    [math.OA] 中找到）表明，如果 *f(x)* 是在凸集 *D* 上定义的凸函数，如果我们选择 n 个点 *x[1]*，*x[2]*，...，*x[n]*
    ∈ *D* 和 n 个常数 *λ[1]*，*λ[2]*，...，*λ[n]* ≥ *0* 满足条件 *λ[1] + λ[2] + ... + λ[n] =
    1*，那么以下适用：'
- en: '![](img/ee100e07-caba-41fc-a796-d54acf0a812f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee100e07-caba-41fc-a796-d54acf0a812f.png)'
- en: 'Therefore, considering that *-log(x)* is convex, the *Jensen''s inequality*
    for *log(x)* becomes as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到 *-log(x)* 是凸的，*Jensen 不等式* 对于 *log(x)* 变为如下：
- en: '![](img/302a6644-870e-4feb-ab0b-d3e09eda66e6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/302a6644-870e-4feb-ab0b-d3e09eda66e6.png)'
- en: 'Hence, the generic iterative step can be rewritten, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通用的迭代步骤可以重写如下：
- en: '![](img/d651914c-a165-404e-bba1-fde1c04afc0d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d651914c-a165-404e-bba1-fde1c04afc0d.png)'
- en: 'Applying the Jensen''s inequality, we obtain the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 Jensen 不等式，我们得到以下结果：
- en: '![](img/5bdbe909-f1e6-48f0-8f6a-736a1343bc18.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5bdbe909-f1e6-48f0-8f6a-736a1343bc18.png)'
- en: 'All the conditions are met, because the terms *P(z[i]|X, θ[t])* are, by definition,
    bounded between [0, 1] and the sum over all *z* must always be equal to 1 (laws
    of probability). The previous expression implies that the following is true:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有条件都满足，因为根据定义，项 *P(z[i]|X, θ[t])* 都在 [0, 1] 之间，并且所有 *z* 的和必须始终等于 1（概率定律）。前面的表达式意味着以下陈述是正确的：
- en: '![](img/95860f42-dbac-4803-bc80-bcea6c8258b0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/95860f42-dbac-4803-bc80-bcea6c8258b0.png)'
- en: 'Therefore, if we maximize the right side of the inequality, we also maximize
    the log-likelihood. However, the problem can be further simplified, considering
    that we are optimizing only the parameter vector *θ* and we can remove all the
    terms that don''t depend on it. Hence, we can define a *Q function* (there are
    no relationships with the Q-Learning that we''re going to discuss in [Chapter
    14](51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml), *Introduction to Reinforcement
    Learning*) whose expression is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们最大化不等式的右侧，我们也会最大化对数似然。然而，考虑到我们只优化参数向量 *θ* 并且可以删除所有不依赖于它的项，问题可以进一步简化。因此，我们可以定义一个
    *Q 函数*（它与我们在第 14 章（51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml，*强化学习导论*）中将要讨论的
    Q-Learning 没有关系），其表达式如下：
- en: '![](img/1f6510d2-dec4-4501-be01-9854f147482f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f6510d2-dec4-4501-be01-9854f147482f.png)'
- en: '*Q* is the expected value of the log-likelihood considering the complete data
    *Y* = (*X*, *Z*) and the current iteration parameter *set θ[t]*. At each iteration,
    *Q* is computed considering the current estimation *θ*[*t* ]and it''s maximized
    considering the variable *θ*. It''s now clearer why the latent variables can be
    often artificially introduced: they allow us to apply the *Jensen''s inequality*
    and transform the original expression into an expected value that is easy to evaluate
    and optimize.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q* 是在完整数据 *Y* = (*X*, *Z*) 和当前迭代参数集 *θ[t]* 下对数似然的期望值。在每次迭代中，*Q* 是基于当前的估计 *θ*[*t* ]计算的，并且它是基于变量
    *θ* 最大化计算的。现在更清楚为什么潜在变量经常被人为引入：它们允许我们应用 *Jensen 不等式* 并将原始表达式转换为易于评估和优化的期望值。'
- en: 'At this point, we can formalize the EM algorithm:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以形式化 EM 算法：
- en: Set a threshold *Thr* (for example, *Thr* = 0.01)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个阈值 *Thr*（例如，*Thr* = 0.01）
- en: Set a random parameter vector *θ[0.]*
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个随机参数向量 *θ[0.]*
- en: 'While *|L(θ[t]|X, Z) - L(θ[t-1]|X, Z)| >* *Thr*:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *|L(θ[t]|X, Z) - L(θ[t-1]|X, Z)| >* *Thr* 时：
- en: '**E-Step**: Compute the *Q*(*θ*|*θ[t]*). In general, this step consists in
    computing the conditional probability *p*(*z*|*X*, *θ[t]*) or some of its moments
    (sometimes, the sufficient statistics are limited to mean and covariance) using
    the current parameter estimation *θ[t]*.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E-Step**: 计算条件概率 *Q*(*θ*|*θ[t]*). 通常，这一步包括使用当前的参数估计 *θ[t]* 来计算条件概率 *p*(*z*|*X*, *θ[t]*)
    或其某些矩（有时，充分统计量仅限于均值和协方差）。'
- en: '**M-Step**: Find *θ[t+1]* = *argmax[θ] Q*(*θ*|*θ[t]*). The new parameter estimation
    is computed to maximize the *Q* function.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M-步**：找到 *θ[t+1]* = *argmax[θ] Q*(*θ*|*θ[t]*)。新的参数估计是通过最大化 *Q* 函数来计算的。'
- en: The procedure ends when the log-likelihood stops increasing or after a fixed
    number of iterations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当对数似然不再增加或达到固定迭代次数后，程序结束。
- en: An example of parameter estimation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数估计的一个例子
- en: In this example, we see how it's possible to apply the EM algorithm for the
    estimation of unknown parameters (inspired by an example discussed in the original
    paper *Maximum likelihood from incomplete data via the em algorithm*,*Dempster
    A. P.*, *Laird N. M.*, *Rubin D. B.*, *Journal of the Royal Statistical Society*,
    *B, 39(1):1–38*, *11/1977*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们看到如何应用 EM 算法来估计未知参数（灵感来源于原始论文 *通过 EM 算法从不完全数据中估计最大似然*，Dempster A. P.，Laird
    N. M.，Rubin D. B.，《皇家统计学会杂志》，B, 39(1):1–38，1977年11月）。
- en: 'Let''s consider a sequence of *n* independent experiments modeled with a multinomial
    distribution with three possible outcomes *x[1]*, *x[2]*, *x[3]* and corresponding
    probabilities *p[1]*, *p[2]* and *p[3]*. The probability mass function is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个由三个可能结果 *x[1]*，*x[2]*，*x[3]* 和相应的概率 *p[1]*，*p[2]* 和 *p[3]* 组成的多项分布模型化的独立实验序列。概率质量函数如下：
- en: '![](img/da7e3274-4ca4-400f-b3a2-99677ba9686d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da7e3274-4ca4-400f-b3a2-99677ba9686d.png)'
- en: 'Let''s suppose that we can observe *z[1] = x[1 ]+ x[2]* and *x[3]*, but we
    don''t have any direct access to the single values *x[1]* and *x[2]*. Therefore, *x[1]*
    and *x[2]* are latent variables, while *z[1]* and *x[3]* are observed ones. The
    probability vector *p* is parameterized in the following way:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以观察到 *z[1]* = *x[1]* + *x[2]* 和 *x[3]*，但我们无法直接访问单个值 *x[1]* 和 *x[2]*。因此，*x[1]*
    和 *x[2]* 是潜在变量，而 *z[1]* 和 *x[3]* 是观测变量。概率向量 *p* 按以下方式参数化：
- en: '![](img/c06aa4ef-5882-43be-b878-b0686432ead3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c06aa4ef-5882-43be-b878-b0686432ead3.png)'
- en: 'Our goal is to find the MLE for *θ* given *n*, *z[1]*, and *x[3]*. Let''s start
    computing the log-likelihood:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是在给定 *n*，*z[1]* 和 *x[3]* 的情况下找到 *θ* 的最大似然估计。让我们开始计算对数似然：
- en: '![](img/ce429bb1-7dc7-4769-8ce5-c289b6351bb5.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce429bb1-7dc7-4769-8ce5-c289b6351bb5.png)'
- en: 'We can derive the expression for the corresponding *Q* function, exploiting
    the linearity of the expected value operator *E*[*•*]:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用期望值算子 *E*[*•*] 的线性来推导相应的 *Q* 函数的表达式：
- en: '![](img/cc3db94c-f9a9-44ce-ae49-d20c9fdcacd9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc3db94c-f9a9-44ce-ae49-d20c9fdcacd9.png)'
- en: 'The variables *x[1]* and *x[2,]* given *z[1,]* are binomially distributed and
    can be expressed as a function of *θ[t]* (we need to recompute them at each iteration).
    Hence, the expected value of *x[1]*^((t*+*1*)*) becomes as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *z[1]* 的变量 *x[1]* 和 *x[2]* 是二项分布的，可以表示为 *θ[t]* 的函数（我们需要在每次迭代中重新计算它们）。因此，*x[1]*^((t+1))
    的期望值如下：
- en: '![](img/53dfda53-ce60-490a-9575-e6d0570eee65.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53dfda53-ce60-490a-9575-e6d0570eee65.png)'
- en: 'While the expected value of *x[2]*^((*t*+*1*)) is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 而 *x[2]*^((*t*+*1*)) 的期望值如下：
- en: '![](img/8d8e8892-8cc4-4ad8-a3d1-9bacb29a205f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d8e8892-8cc4-4ad8-a3d1-9bacb29a205f.png)'
- en: 'If we apply these expressions in ![](img/b0a1acac-af44-42fd-b0a8-56201c4e3774.png) and
    compute the derivative with respect to *θ*, we get the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些表达式应用于 ![](img/b0a1acac-af44-42fd-b0a8-56201c4e3774.png) 并对 *θ* 求导，我们得到以下结果：
- en: '![](img/270056ef-2322-4a4d-8314-9950dc7eb4ae.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/270056ef-2322-4a4d-8314-9950dc7eb4ae.png)'
- en: 'Therefore, solving for *θ*, we get the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，求解 *θ*，我们得到以下结果：
- en: '![](img/e02ea8ad-1f45-4036-8fc1-8e7f65f7228f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e02ea8ad-1f45-4036-8fc1-8e7f65f7228f.png)'
- en: 'At this point, we can derive the iterative expression for *θ*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以推导出 *θ* 的迭代表达式：
- en: '![](img/59099387-badd-4905-8cc6-6fa09249e47e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59099387-badd-4905-8cc6-6fa09249e47e.png)'
- en: 'Let''s compute the value of *θ* for *z[1]* = 50 and *x[3]* = 10:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算当 *z[1]* = 50 和 *x[3]* = 10 时 *θ* 的值：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we have parameterized all probabilities and, considering that *z[1] *=
    *x[1 ]*+ x*[2]*, we have one degree of freedom for the choice of *θ*. The reader
    can repeat the example by setting the value of one of *p[1]* or *p[2]* and leaving
    the other probabilities as functions of *θ*. The computation is almost identical
    but in this case, there are no degrees of freedom.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已将所有概率进行了参数化，考虑到 *z[1]* = *x[1]* + *x[2]*，我们有一个自由度来选择 *θ*。读者可以通过设置
    *p[1]* 或 *p[2]* 中的一个值，并将其他概率作为 *θ* 的函数来重复此例子。计算几乎相同，但在这个情况下，没有自由度。
- en: Gaussian mixture
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合
- en: In [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction to
    Semi-Supervised Learning*, we discussed the generative Gaussian mixture model
    in the context of semi-supervised learning. In this paragraph, we're going to
    apply the EM algorithm to derive the formulas for the parameter updates.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 2 章](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml) 《半监督学习导论》中，我们讨论了半监督学习背景下的生成高斯混合模型。在本段中，我们将应用
    EM 算法推导参数更新的公式。
- en: 'Let''s start considering a dataset, *X,* drawn from a data generating process,
    *p[data]*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个从数据生成过程 *p[data]* 中抽取的数据集 *X*：
- en: '![](img/c86d60ac-0692-4c32-b3fe-7134cafd9798.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c86d60ac-0692-4c32-b3fe-7134cafd9798.png)'
- en: 'We assume that the whole distribution is generated by the sum of *k* Gaussian
    distributions so that the probability of each sample can be expressed as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设整个分布是由 *k* 个高斯分布的和生成的，这样每个样本的概率可以表示如下：
- en: '![](img/43b2677f-f55c-4bf4-8601-260e4bfb251b.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/43b2677f-f55c-4bf4-8601-260e4bfb251b.png)'
- en: 'In the previous expression, the term *w[j]* = *P*(*N*=j) is the relative weight
    of the *j^(th)* Gaussian, while *μ*[*j* ]and *Σ[j]* are the mean and the covariance
    matrix. For consistency with the laws of probability, we also need to impose the
    following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，项 *w[j]* = *P*(*N*=j) 是第 *j* 个高斯分布的相对权重，而 *μ*[*j*] 和 *Σ[j]* 是均值和协方差矩阵。为了与概率定律保持一致，我们还需要施加以下条件：
- en: '![](img/3154b0c1-e8e4-418f-92e6-4bc338d9c902.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3154b0c1-e8e4-418f-92e6-4bc338d9c902.png)'
- en: Unfortunately, if we try to solve the problem directly, we need to manage the
    logarithm of a sum and the procedure becomes very complex. However, we have learned
    that it's possible to use latent variables as helpers, whenever this trick can
    simplify the solution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果我们直接尝试解决这个问题，我们需要管理求和的对数，这个过程变得非常复杂。然而，我们已经了解到，可以使用潜在变量作为辅助工具，每当这个技巧可以简化解决方案时。
- en: Let's consider a single parameter set *θ*=(*w[j]*, *μ[j]*, *Σ[j]*) and a latent
    indicator matrix *Z* where each element *z[ij]* is equal to 1 if the point *x[i]* has
    been generated by the *j^(th)* Gaussian, and 0 otherwise. Therefore, each *z[ij]* is
    Bernoulli distributed with parameters equal to *p*(*j*|*x[i]*, *θ[t]*).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个单个参数集 *θ*=(*w[j]*, *μ[j]*, *Σ[j]*) 和一个潜在指示矩阵 *Z*，其中每个元素 *z[ij]* 等于 1
    如果点 *x[i]* 已经由第 *j* 个高斯分布生成，否则为 0。因此，每个 *z[ij]* 是伯努利分布，参数等于 *p*(*j*|*x[i]*, *θ[t]*)。
- en: 'The joint log-likelihood can hence be expressed using the exponential-indicator
    notation, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，联合对数似然函数可以使用指数指示符表示法表示如下：
- en: '![](img/dc57de2f-997f-44c5-b090-a609d0362c2f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc57de2f-997f-44c5-b090-a609d0362c2f.png)'
- en: 'The index, *i,* is referred to the samples, while *j* refers to the Gaussian
    distributions. If we apply the chain rule and the properties of a logarithm, the
    expression becomes as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 索引 *i* 指的是样本，而 *j* 指的是高斯分布。如果我们应用链式法则和对数性质，表达式变为以下形式：
- en: '![](img/1c1df3d3-f8aa-41d2-bcac-024e140109db.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c1df3d3-f8aa-41d2-bcac-024e140109db.png)'
- en: 'The first term represents the probability of *x[i]* under the *j^(th)* Gaussian,
    while the second one is the relative weight of the *j^(th)* Gaussian. We can now
    compute the *Q*(*θ*;*θ[t]*) function using the joint log-likelihood:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项表示在 *j* 个高斯分布下 *x[i]* 的概率，而第二个项是第 *j* 个高斯分布的相对权重。现在我们可以使用联合对数似然函数来计算 *Q*(*θ*;*θ[t]*)
    函数：
- en: '![](img/0dbc339d-5608-4170-a1ff-fb330ba49ab6.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0dbc339d-5608-4170-a1ff-fb330ba49ab6.png)'
- en: 'Exploiting the linearity of *E[•]*, the previous expression becomes as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 *E[•]* 的线性特性，前面的表达式变为以下形式：
- en: '![](img/45c1ddc3-5b20-4f8e-b97f-4827eca10a48.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45c1ddc3-5b20-4f8e-b97f-4827eca10a48.png)'
- en: 'The term *p*(*j*|*x**[i]*, *θ[t]*) corresponds to the expected value of *z[ij]* considering
    the complete data, and expresses the probability of the *j^(th)* Gaussian given
    the sample *x[i]*. It can be simplified considering Bayes'' theorem:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 项 *p*(*j*|*x**[i]*, *θ[t]*) 对应于考虑完整数据的 *z[ij]* 的期望值，并表达了给定样本 *x[i]* 的 *j* 个高斯分布的概率。考虑到贝叶斯定理，它可以简化如下：
- en: '![](img/682b9783-ce0e-41b0-b56b-5109d6ec3b7c.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/682b9783-ce0e-41b0-b56b-5109d6ec3b7c.png)'
- en: 'The first term is the probability of *x[i]* under the *j^(th)* Gaussian with
    parameters *θ[t]*, while the second one is the weight of the *j^(th)* Gaussian
    considering the same parameter set *θ[t]*. In order to derive the iterative expressions
    for the parameters, it''s useful to write the complete formula for the logarithm
    of a multivariate Gaussian distribution:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是 *x[i]* 在 *j^(th)* 高斯下的概率，而第二个项是在相同的参数集 *θ[t]* 下 *j^(th)* 高斯的权重。为了推导参数的迭代表达式，写出多元高斯分布对数的完整公式是有用的：
- en: '![](img/631ee1a8-e22e-4b3c-b0a1-9dc90d98eb87.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/631ee1a8-e22e-4b3c-b0a1-9dc90d98eb87.png)'
- en: 'To simplify this expression, we use the trace trick. In fact, as (*x[i]* -* μ[j]*)*^T* Σ^(-1) (*x**[i]* - *μ[j]*)
    is a scalar, we can exploit the properties *tr*(*AB*) = *tr*(*BA*) and *tr*(*c*)
    = *c* where *A* and *B* are matrices and *c* ∈ *ℜ*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个表达式，我们使用迹技巧。实际上，由于 (*x[i]* -* μ[j*)*^T* Σ^(-1) (*x**[i]* - *μ[j*) 是一个标量，我们可以利用
    *tr*(*AB*) = *tr*(*BA*) 和 *tr*(*c*) = *c* 的性质，其中 *A* 和 *B* 是矩阵，*c* ∈ *ℜ*：
- en: '![](img/2b77a9af-5e3d-464e-9d0f-083c7e443d24.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b77a9af-5e3d-464e-9d0f-083c7e443d24.png)'
- en: 'Let''s start considering the estimation of the mean (only the first term of
    *Q*(*θ*;*θ**[t]*) depends on mean and covariance):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从均值的估计开始考虑（只有 *Q*(*θ*;*θ**[t]*) 的第一个项依赖于均值和协方差）：
- en: '![](img/d1b6f81d-ad1a-4f23-aed8-002f1167444d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1b6f81d-ad1a-4f23-aed8-002f1167444d.png)'
- en: 'Setting the derivative equal to zero, we get the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将导数设为零，我们得到以下结果：
- en: '![](img/049ce298-27ae-4414-94ac-31321c8701ff.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/049ce298-27ae-4414-94ac-31321c8701ff.png)'
- en: 'In the same way, we obtain the expression of the covariance matrix:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们得到协方差矩阵的表达式：
- en: '![](img/d0f30780-0542-45a2-be5b-060feb432d82.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0f30780-0542-45a2-be5b-060feb432d82.png)'
- en: 'To obtain the iterative expressions for the weights, the procedure is a little
    bit more complex, because we need to use the Lagrange multipliers(further information
    can be found in [http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html](http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html)).
    Considering that the sum of the weights must always be equal to 1, it''s possible
    to write the following equation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得权重的迭代表达式，过程稍微复杂一些，因为我们需要使用拉格朗日乘数（更多信息可以在[http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html](http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html)找到）。考虑到权重的总和必须始终等于1，可以写出以下方程：
- en: '![](img/158892a8-78cb-4e03-a49b-195cf5ed898a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/158892a8-78cb-4e03-a49b-195cf5ed898a.png)'
- en: 'Setting both derivatives equal to zero, from the first one, considering that
    *wj* = *p*(*j*|*θ*), we get the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个导数都设为零，从第一个导数出发，考虑到 *wj* = *p*(*j*|*θ*)，我们得到以下结果：
- en: '![](img/61be7b2e-d780-412a-b991-adcd21e196a1.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61be7b2e-d780-412a-b991-adcd21e196a1.png)'
- en: 'While from the second derivative, we obtain the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 而从第二个导数，我们得到以下结果：
- en: '![](img/1a87943e-41b8-46b4-9c6b-1b6f923ca193.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1a87943e-41b8-46b4-9c6b-1b6f923ca193.png)'
- en: 'The last step derives from the fundamental condition:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步来源于基本条件：
- en: '![](img/fe93fa4e-ed32-4de1-bdc9-5136d9e73d7e.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe93fa4e-ed32-4de1-bdc9-5136d9e73d7e.png)'
- en: 'Therefore, the final expression of the weights is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，权重的最终表达式如下：
- en: '![](img/ee17e911-2233-4dec-8113-ea506de214be.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee17e911-2233-4dec-8113-ea506de214be.png)'
- en: 'At this point, we can formalize the Gaussian mixture algorithm:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以正式化高斯混合算法：
- en: Set random initial values for *w[j]*^((*0*)), *θ*^((*0*))*[j]* and *Σ*^((0))*[j]*
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 *w[j]*^((*0*)), *θ*^((*0*))[j]* 和 *Σ*^((0))*[j]* 设置随机初始值
- en: '**E-Step**: Compute *p*(*j*|*x[i]*, *θ[t]*) using Bayes'' theorem: *p*(*j*|*x**[i]*, *θ[t]*)
    = *α w*^((*t*))[j] *p*(*x[i]*|*j,* *θ[t]*)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E-Step**: 使用贝叶斯定理计算 *p*(*j*|*x[i]*, *θ[t]*)：*p*(*j*|*x**[i]*, *θ[t]*) = *α
    w*^((*t*))[j] * *p*(*x[i]*|*j,* *θ[t]*)'
- en: '**M-Step**: Compute *w[j]*^((*t+1*)),* θ*^((*t+1*))[j] and Σ^((t+1))*[j]* using
    the formulas provided previously'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M-Step**: 使用之前提供的公式计算 *w[j]*^((*t+1*)), *θ*^((*t+1*))[j] 和 Σ^((t+1))*[j]*。'
- en: The process must be iterated until the parameters become stable. In general,
    the best practice is using both a threshold and a maximum number of iterations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程必须迭代，直到参数变得稳定。通常，最佳实践是使用阈值和最大迭代次数。
- en: An example of Gaussian Mixtures using Scikit-Learn
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn的Gaussian Mixtures示例
- en: 'We can now implement the Gaussian mixture algorithm using the Scikit-Learn
    implementation. The direct approach has already been shown in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml),
    *Introduction to Semi-Supervised Learning*. The dataset is generated to have three
    cluster centers and a moderate overlap due to a standard deviation equal to 1.5:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Scikit-Learn 的实现来实施高斯混合算法。直接方法已在[第2章](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml)，“半监督学习简介”中展示。数据集生成时具有三个聚类中心和由于标准差等于1.5的适度重叠：
- en: '[PRE1]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The corresponding plot is shown in the following diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的图表如下所示：
- en: '![](img/a02e97c3-25bf-4a5a-9687-191ed0bac26c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a02e97c3-25bf-4a5a-9687-191ed0bac26c.png)'
- en: 'The Scikit-Learn implementation is based on the `GaussianMixture` class , which
    accepts as parameters the number of Gaussians (`n_components`), the type of covariance
    (`covariance_type`), which can be `full` (the default value), if all components
    have their own matrix, `tied` if the matrix is shared, `diag` if all components
    have their own diagonal matrix (this condition imposes an uncorrelation among
    the features), and `spherical` when each Gaussian is symmetric in every direction.
    The other parameters allow setting regularization and initialization factors (for
    further information, the reader can directly check the documentation). Our implementation
    is based on full covariance:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 的实现基于 `GaussianMixture` 类，它接受参数如高斯数量 (`n_components`)、协方差类型 (`covariance_type`)，可以是
    `full`（默认值），如果所有组件都有自己的矩阵，`tied` 如果矩阵是共享的，`diag` 如果所有组件都有自己的对角矩阵（这条件使得特征之间不相关），以及
    `spherical` 当每个高斯在所有方向上都是对称的。其他参数允许设置正则化和初始化因子（有关更多信息，读者可以直接查看文档）。我们的实现基于完全协方差：
- en: '[PRE2]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After fitting the model, it''s possible to access to the learned parameters
    through the instance variables `weights_`, `means_`, and `covariances_`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型拟合后，可以通过实例变量 `weights_`、`means_` 和 `covariances_` 访问学习到的参数：
- en: '[PRE3]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Considering the covariance matrices, we can already understand that the features
    are very uncorrelated and the Gaussians are almost spherical. The final plot can
    be obtained by assigning each point to the corresponding cluster (Gaussian distribution)
    through the `Yp = gm.transform(X)` command:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到协方差矩阵，我们可以理解特征之间非常不相关，高斯几乎呈球形。最终的图表可以通过将每个点分配给相应的聚类（高斯分布）通过 `Yp = gm.transform(X)`
    命令获得：
- en: '![](img/e4962263-c815-4c52-9303-449380b931e0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e4962263-c815-4c52-9303-449380b931e0.png)'
- en: Labeled dataset obtained through the application of a Gaussian mixture with
    three components
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用具有三个组件的高斯混合得到的有标签数据集
- en: The reader should have noticed a strong analogy between Gaussian mixture and
    k-means(which we're going to discuss in [Chapter 7](59f765c2-2ad0-4605-826e-349080f85f1f.xhtml),
    *Clustering Algorithms*). In particular, we can state that K-means is a particular
    case of spherical Gaussian mixture with a covariance *Σ* → 0\. This condition
    transforms the approach from a soft clustering, where each sample belongs to all
    clusters with a precise probability distribution, into a hard clustering, where
    the assignment is done by considering the shortest distance between sample and
    centroid (or mean). For this reason, in some books, the Gaussian mixture algorithm
    is also called soft K-means.A conceptually similar approach that we are going
    to present is Fuzzy K-means, which is based on assignments characterized by membership
    functions, which are analogous to probability distributions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该已经注意到高斯混合和 k-means（我们将在[第7章](59f765c2-2ad0-4605-826e-349080f85f1f.xhtml)，“聚类算法”中讨论）之间有很强的类比。特别是，我们可以声明
    K-means 是球形高斯混合的一个特例，其中协方差 *Σ* → 0。这个条件将方法从软聚类（每个样本以精确的概率分布属于所有聚类）转变为硬聚类（通过考虑样本和质心（或均值）之间的最短距离来完成分配）。因此，在某些书中，高斯混合算法也被称为软
    K-means。我们将要介绍的一个概念上类似的方法是模糊 K-means，它基于由隶属函数定义的分配，这些函数类似于概率分布。
- en: Factor analysis
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因子分析
- en: 'Let''s suppose we have a Gaussian data generating process, *p*[*data* ]∼ *N*(*0*, *Σ*),
    and *M* n-dimensional zero-centered samples drawn from it:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个高斯数据生成过程，*p*[*data* ]∼ *N*(*0*, *Σ*)，并且从中抽取了 *M* 个零均值样本：
- en: '![](img/dd2c1b29-1765-4463-86b2-e448460c1598.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd2c1b29-1765-4463-86b2-e448460c1598.png)'
- en: If *p**[data]* has a mean *μ* ≠ *0*, it's also possible to use this model, but
    it's necessary to account for this non-null value with slight changes in some
    formulas. As the zero-centering normally has no drawbacks, it's easier to remove
    the mean to simplify the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*p**[data]*的均值*μ* ≠ *0*，也可以使用这个模型，但需要通过一些公式的微小变化来考虑这个非零值。由于零中心化通常没有缺点，更容易去除均值以简化模型。
- en: 'One of the most common problems in unsupervised learning is finding a lower
    dimensional distribution *p[lower]* such that the Kullback-Leibler divergence
    with *p[data]* is minimized. When performing a **factor analysis **(**FA**), following
    the original proposal published in *EM algorithms for ML factor analysis*, *Rubin
    D.*, *Thayer D.*, *Psychometrika*, 47/1982, Issue 1, and *The EM algorithm for
    Mixtures of Factor Analyzers*, *Ghahramani Z.*, *Hinton G. E.*, CRC-TG-96-1, 05/1996,
    we start from the assumption to model the generic sample *x* as a linear combination
    of Gaussian latent variables, *z,* (whose dimension *p* is normally *p* < *n*)
    plus an additive and decorrelated Gaussian noise term, *ν*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，最常见的问题之一是找到一个低维分布*p[lower]*，使得与*p[data]*的Kullback-Leibler散度最小化。当进行**因子分析（FA**）时，根据发表在*EM算法在机器学习因子分析中的应用*，*Rubin
    D.*，*Thayer D.*，*Psychometrika*，47/1982，第1期，以及*混合因子分析中的EM算法*，*Ghahramani Z.*，*Hinton
    G. E.*，CRC-TG-96-1，1996年5月，的原提案，我们假设将通用样本*x*建模为高斯潜在变量*z*（其维度*p*通常是*p* < *n*）的线性组合，再加上一个可加且去相关的高斯噪声项*ν*：
- en: '![](img/3ce65823-73b9-42bc-978d-55aafab340f7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ce65823-73b9-42bc-978d-55aafab340f7.png)'
- en: The matrix, A, is called a *factor loading matrix* because it determines the
    contribution of each latent variable (factor) to the reconstruction of *x*. Factors
    and input data are assumed to be statistically independent. Instead, considering
    the last term, if *ω[0]²* ≠ *ω[1]²* ≠ ... ≠ *ω[n]²* the noise is called *heteroscedastic*,
    while it's defined *homoscedastic* if the variances are equal *ω[0]²* = *ω[1]²* =
    ... = *ω[n]²* = *ω²*. To understand the difference between these two kinds of
    noise, think about a signal x which is the sum of two identical voices, recorded
    in different places (for example, an airport and a wood). In this case, we can
    suppose to also have different noise variances (the first one should be higher
    than the second considering the number of different noise sources). If instead
    both voices are recorded in a soundproofed room or even in the same airport, homoscedastic
    noise is surely more likely (we're not considering the power, but the difference
    between the variances).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵A被称为*因子载荷矩阵*，因为它决定了每个潜在变量（因子）对*x*重建的贡献。因子和输入数据被认为是统计独立的。相反，考虑到最后一个项，如果*ω[0]²*
    ≠ *ω[1]²* ≠ ... ≠ *ω[n]²*，噪声被称为*异方差*，而如果方差相等*ω[0]²* = *ω[1]²* = ... = *ω[n]²*
    = *ω²*，则定义为*同方差*。为了理解这两种噪声之间的区别，考虑一个信号x，它是两个相同声音的和，记录在不同的地方（例如，机场和森林）。在这种情况下，我们可以假设也有不同的噪声方差（第一个应该比第二个高，考虑到不同的噪声源数量）。如果相反，两个声音都在隔音室中录制，或者在同一个机场中录制，同方差噪声更有可能（我们不考虑功率，但方差之间的差异）。
- en: One of the most important strengths of FA in respect to other methods (such
    as PCA) is its intrinsic robustness to heteroscedastic noise. In fact, including
    the noise term in the model (with only the constraint to be decorrelated) allows
    partial denoising filtering based on the single components, while one of the preconditions
    for the PCA is to impose only homoscedastic noise (which, in many cases, is very
    similar to the total absence of noise). Considering the previous example, we could
    make the assumption to have the first variance be *ω[0]²* = *k ω[1]²* with *k*
    > *1*. In this way, the model will be able to understand that a high variance
    in the first component should be considered (with a higher probability) as the
    product of the noise and not an intrinsic property of the component.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法（如PCA）相比，FA的一个重要优势是其对异方差噪声的内禀鲁棒性。事实上，在模型中包含噪声项（仅限于去相关约束）允许基于单个成分进行部分去噪滤波，而PCA的一个前提条件是只施加同方差噪声（在许多情况下，这与噪声完全不存在非常相似）。考虑到前面的例子，我们可以假设第一个方差为*ω[0]²*
    = *k ω[1]²*，其中*k* > *1*。这样，模型将能够理解第一个成分的高方差应该（以更高的概率）被视为噪声与成分固有属性的乘积。
- en: 'Let''s now analyze the linear relation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来分析线性关系：
- en: '![](img/08e836ef-686e-4155-b7ff-4020e1d5948a.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08e836ef-686e-4155-b7ff-4020e1d5948a.png)'
- en: 'Considering the properties of Gaussian distributions, we know that *x* ∼ *N*(*μ*, *Σ*)
    and it''s easy to determine either the mean or the covariance matrix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑高斯分布的性质，我们知道 *x* 符合正态分布 *N*(*μ*, *Σ*)，因此很容易确定均值或协方差矩阵：
- en: '![](img/aec4d0d8-e9e6-4b5b-b8bf-7765455b39fe.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aec4d0d8-e9e6-4b5b-b8bf-7765455b39fe.png)'
- en: Therefore, in order to solve the problem, we need to find the best *θ*=(*A*, Ω)
    so that *AA^T* + Ω ≈ *Σ* (with a zero-centered dataset, the estimation is limited
    to the input covariance matrix *Σ*).The ability to cope with noisy variables should
    be clearer now. If *AA^T* + Ω is exactly equal to *Σ* and the estimation of Ω
    is correct, the algorithm will optimize the factor loading matrix A, excluding
    the interference produced by the noise term; therefore, the components will be
    approximately denoised.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决问题，我们需要找到最佳的 *θ*=(*A*, *Ω*)，使得 *AA^T* + *Ω* ≈ *Σ*（对于零均值数据集，估计仅限于输入协方差矩阵
    *Σ*）。现在应该更清楚地理解处理噪声变量的能力。如果 *AA^T* + *Ω* 等于 *Σ* 并且 *Ω* 的估计是正确的，算法将优化因子加载矩阵 A，排除噪声项产生的干扰；因此，成分将大约去噪。
- en: 'In order to adopt the EM algorithm, we need to determine the joint probability
    *p*(*X*, *z*; *θ*) = *p*(*X*|*z*; *θ*)*p*(*z*|*θ*). The first term on the right
    side can be easily determined, considering that *x* - *Az* ∼ *N*(*0*, Ω); therefore,
    we get the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了采用EM算法，我们需要确定联合概率 *p*(*X*, *z*; *θ*) = *p*(*X*|*z*; *θ*)*p*(*z*|*θ*)。右侧的第一个项可以很容易地确定，考虑到
    *x* - *Az* 符合正态分布 *N*(*0*, *Ω*)；因此，我们得到以下：
- en: '![](img/f1c90edf-a50f-4b9b-8791-c77fc50ec9ef.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f1c90edf-a50f-4b9b-8791-c77fc50ec9ef.png)'
- en: 'We can now determine the *Q*(*θ*;*θ[t]*) function, discarding the constant
    term (*2π*)*^k* and term *z^Tz*, which don''t depend on *θ* (in this particular
    case, as we''re going to see, we don''t need to compute the probability *p*(*z*|*X*;*θ*)
    because it''s enough to obtain sufficient statistics for expected value and second
    moment). Moreover, it''s useful to expand the multiplication in the exponential:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以确定 *Q*(*θ*;*θ[t]*) 函数，忽略常数项 (*2π*)^k 和与 *θ* 无关的项 *z^Tz*。此外，展开指数中的乘法是有用的：
- en: '![](img/191ed831-9326-49b6-80dd-2df8c532f7e5.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/191ed831-9326-49b6-80dd-2df8c532f7e5.png)'
- en: 'Using the trace trick with the last term (which is a scalar), we can rewrite
    it as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迹技巧对最后一个项（它是一个标量）进行重写，我们可以将其表示如下：
- en: '![](img/69496cf8-3686-40ab-ae89-9cf8bc033dbd.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69496cf8-3686-40ab-ae89-9cf8bc033dbd.png)'
- en: 'Exploiting the linearity of *E*[•], we obtain the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 *E*[•] 的线性，我们得到以下：
- en: '![](img/e3c5f66d-411b-4405-8e12-01fa576f4014.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e3c5f66d-411b-4405-8e12-01fa576f4014.png)'
- en: 'This expression is similar to what we have seen in the Gaussian mixture model,
    but in this case, we need to compute the conditional expectation and the conditional
    second moment of *z*. Unfortunately, we cannot do this directly, but it''s possible
    to compute them exploiting the joint normality of *x* and *z*. In particular,
    using a classic theorem, we can partition the full joint probability *p*(*z*,
    *x*), considering the following relations:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式与我们之前在高斯混合模型中看到的是相似的，但在这个情况下，我们需要计算 *z* 的条件期望和条件二阶矩。不幸的是，我们无法直接计算它们，但可以通过利用
    *x* 和 *z* 的联合正态性来计算。特别是，使用一个经典定理，我们可以根据以下关系对整个联合概率 *p*(*z*, *x*) 进行划分：
- en: '![](img/99948ccc-10dd-4167-9e12-bbaad7488ddf.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/99948ccc-10dd-4167-9e12-bbaad7488ddf.png)'
- en: 'The conditional distribution *p*(*z*|*x*=*x[i]*) has a mean equal to the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 条件分布 *p*(*z*|*x*=*x[i]*) 的均值等于以下：
- en: '![](img/ccb3c890-8624-4897-9aad-ab99e431cb9d.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ccb3c890-8624-4897-9aad-ab99e431cb9d.png)'
- en: 'The conditional variance is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 条件方差如下：
- en: '![](img/89e7ea57-3852-4d17-aa49-dd31424899d3.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89e7ea57-3852-4d17-aa49-dd31424899d3.png)'
- en: 'Therefore, the conditional second moment is equal to the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，条件二阶矩等于以下：
- en: '![](img/ee93199c-6dae-48cc-8bfe-0f99fb74b19e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee93199c-6dae-48cc-8bfe-0f99fb74b19e.png)'
- en: 'If we define the auxiliary matrix *K* = (*AA^T* + Ω)*^(-1)*, the previous expressions
    become as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们定义辅助矩阵 *K* = (*AA^T* + *Ω*)^(-1)，则前面的表达式变为以下：
- en: '![](img/adaa0e9e-b648-40d5-b776-bb6dd3612434.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/adaa0e9e-b648-40d5-b776-bb6dd3612434.png)'
- en: The reader in search of further details about this technique can read *Preview*
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于此技术细节的读者可以阅读《Preview》。
- en: '*Introduction to Statistical Decision Theory*, *Pratt J.*, *Raiffa H.*, *Schlaifer
    R.*, *The MIT Press*.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 《统计决策理论导论》，作者：*Pratt J.*，*Raiffa H.*，*Schlaifer R.*，出版社：*MIT Press*。
- en: 'Using the previous expression, it''s possible to build the inverse model (sometimes
    called a *recognition model *because it starts with the effects and rebuilds the
    causes), which is still Gaussian distributed:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的表达式，可以构建逆模型（有时称为*识别模型*，因为它从效果开始重建原因），该模型仍然是高斯分布的：
- en: '![](img/a559961b-f113-49c4-831e-707757c833ea.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a559961b-f113-49c4-831e-707757c833ea.png)'
- en: 'We are now able to maximize *Q*(*θ*;*θ**[t]*) with respect to *A* and Ω, considering
    *θ[t]*=(*A[t]*, Ω*[t]*) and both the conditional expectation and the second moment
    computed according to the previous estimation *θ[t-1]*=(*A[t-1]*, Ω*[t-1])*. For
    this reason, they are not involved in the derivation process. We are adopting
    the convention that the term subject to maximization is computed at time *t*,
    while all the others are obtained through the previous estimations (*t* - *1*):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够根据*A*和Ω最大化*Q*(*θ*;*θ**[t]*)，考虑*θ[t]*=(*A[t]*, Ω*[t]*)以及根据之前的估计*θ[t-1]*=(*A[t-1]*,
    Ω*[t-1]*)计算的条件期望和二阶矩。因此，它们不涉及推导过程。我们采用惯例，在时间*t*计算要最大化的项，而所有其他项都是通过之前的估计获得的（*t*
    - *1*）：
- en: '![](img/286253c5-2e8b-49d8-9748-c066e1c65a9a.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/286253c5-2e8b-49d8-9748-c066e1c65a9a.png)'
- en: 'The expression for *A*[*t* ]is therefore as follows (*Q* is the biased input
    covariance matrix *E*[*X^TX*] for a zero-centered dataset):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此*A*[*t*]的表达式如下（*Q*是有偏输入协方差矩阵*E*[*X^TX*]对于零中心数据集）：
- en: '![](img/fb5e1019-bad7-4da0-909e-6632d6c7de11.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb5e1019-bad7-4da0-909e-6632d6c7de11.png)'
- en: 'In the same way, we can obtain an expression for Ω*[t]* by computing the derivative
    with respect to Ω*^(-1)* (this choice simplifies the calculation and doesn''t
    affect the result, because we must set the derivative equal to zero):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过计算相对于Ω*^(-1)*的导数来获得Ω*[t]*的表达式（这个选择简化了计算，并且不会影响结果，因为我们必须将导数设为零）：
- en: '![](img/08c62f0b-649d-4300-9f88-91bb6883e290.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08c62f0b-649d-4300-9f88-91bb6883e290.png)'
- en: 'The derivative of the first term, which is the determinant of a real diagonal
    matrix, is obtained using the adjugate matrix *Adj*(Ω) and exploiting the properties
    of the inverse matrix *T^(-1)* = *det(T)^(-1)Adj(T)* and the properties *det(T)^(-1) =
    det(T^(-1))* and *det(T^T) = det(T)*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项的导数，即实对角矩阵的行列式，是通过使用伴随矩阵*Adj*(Ω)并利用逆矩阵的性质*T^(-1)* = *det(T)^(-1)Adj(T)*和性质*det(T)^(-1) =
    det(T^(-1))*以及*det(T^T) = det(T)*获得的：
- en: '![](img/3cefb8de-78af-42e0-80a6-f87cb9177396.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cefb8de-78af-42e0-80a6-f87cb9177396.png)'
- en: 'The expression for Ω[t] (imposing the diagonality constraint) is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对Ω[t]（施加对角性约束）的表达式如下：
- en: '![](img/36b470f5-fcf0-441e-81e0-9f5ffdf6d963.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36b470f5-fcf0-441e-81e0-9f5ffdf6d963.png)'
- en: 'Summarizing the steps, we can define the completeFA algorithm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总结步骤，我们可以定义完整的FA算法：
- en: Set random initial values for *A^((0))* and Ω*^((0))*
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为*A^((0))*和Ω*^((0))*设置随机初始值
- en: Compute the biased input covariance matrix *Q = E[X^TX]*
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算有偏输入协方差矩阵*Q = E[X^TX]*
- en: 'E-Step: Compute *A^((t))*, Ω*^((t))*, and *K^((t))*'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E步骤：计算*A^((t))*, Ω*^((t))*, 和*K^((t))*
- en: M-Step: Compute A^((t+1)), Ω*^((t+1))*, and *K**^((t+1))* using the previous
    estimations and the formulas provided previously
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M步骤：使用之前的估计和之前提供的公式计算A^((t+1)), Ω*^((t+1))*, 和*K**^((t+1))*
- en: Compute the matrices *B* and *Ψ* for the inverse model
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算逆模型的矩阵*B*和*Ψ*
- en: The process must be repeated until *A^((t))*, Ω*^((t))*, and *K**^((t))* stop
    modifying their values (using a threshold) together with a constraint on the maximum
    number of iterations. The factors can be easily obtained using the inverse model
    *z = Bx + λ*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 必须重复这个过程，直到*A^((t))*, Ω*^((t))*, 和*K**^((t))*的值停止改变（使用阈值）以及最大迭代次数的约束。因子可以通过逆模型*z
    = Bx + λ*轻松获得。
- en: An example of factor analysis with Scikit-Learn
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn的因子分析示例
- en: We can now make an example of FA with Scikit-Learn using the MNIST handwritten
    digits dataset (70,000 28 × 28 grayscale images) in the original version and with
    added heteroscedastic noise (*ω[i]* randomly selected from [0, 0.75]).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以用Scikit-Learn和MNIST手写数字数据集（原始版本中包含70,000个28 × 28的灰度图像，并添加了异方差噪声[*ω[i]*随机选择自[0,
    0.75]）来做一个FA的例子。
- en: 'The first step is to load and zero-center the original dataset (I''m using
    the functions defined in the first chapter, [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml),
    *Machine Learning Model Fundamentals*):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载并零中心原始数据集（我使用第一章节中定义的函数，[第1章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)，*机器学习模型基础*）：
- en: '[PRE4]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After this step, the  `X `variable  will contain the zero-center original dataset,
    while `Xh` is the noisy version. The following screenshot shows a random selection
    of samples from both versions:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，`X`变量将包含零中心的原数据集，而`Xh`是带噪声的版本。以下截图显示了从两个版本中随机选择的样本：
- en: '![](img/34920020-dfd7-48dd-b23b-9599754ded36.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34920020-dfd7-48dd-b23b-9599754ded36.png)'
- en: 'We can perform FA on both datasets using the Scikit-Learn `FactorAnalysis`
    class with the `n_components=64` parameter and check the score (the average log-likelihood
    over all samples). If the noise variance is known (or there''s a good estimation),
    it''s possible to include the starting point through the `noise_variance_init`
    parameter; otherwise, it will be initialized with the identity matrix:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Scikit-Learn的`FactorAnalysis`类，并设置`n_components=64`参数，对两个数据集执行因子分析，并检查得分（所有样本的平均对数似然）。如果已知噪声方差（或有一个良好的估计），可以通过`noise_variance_init`参数包含起始点；否则，它将使用单位矩阵初始化：
- en: '[PRE5]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected, the presence of noise has reduced the final accuracy (MLE). Following
    an example provided by *A. Gramfort* and *D. A. Engemann* in the original Scikit-Learn
    documentation, we can create a benchmark for the MLE using the *Lodoit-Wolf* algorithm
    (a shrinking method for improving the condition of the covariance that is beyond
    the scope of this book.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，噪声的存在降低了最终的准确度（最大似然估计）。根据*A. Gramfort*和*D. A. Engemann*在原始Scikit-Learn文档中提供的示例，我们可以使用*Lodoit-Wolf*算法（一种用于改善协方差条件的收缩方法，超出了本书的范围）为MLE创建一个基准。
- en: 'For further information, read *A Well-Conditioned Estimator for Large-Dimensional
    Covariance Matrices*, *Ledoit O.*, *Wolf M.*, *Journal of Multivariate Analysis*,
    88, 2/2004":'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '对于更多信息，请阅读*A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices*，*Ledoit
    O.*，*Wolf M.*，*Journal of Multivariate Analysis*，88，2/2004":'
- en: '[PRE6]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With the original dataset, FA performs much better than the benchmark, while
    it''s slightly worse in the presence of heteroscedastic noise. The reader can
    try other combinations using the grid search with different numbers of components
    and noise variances, and experiment with the effect of removing the zero-centering
    step. It''s possible to plot the extracted components using the `components_`
    instance variable:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始数据集，因子分析比基准表现得更好，而在存在异方差噪声的情况下则略差。读者可以尝试使用网格搜索不同的成分数量和噪声方差的其他组合，并实验去除零中心化步骤的效果。可以使用`components_`实例变量绘制提取的成分：
- en: '![](img/8e09541a-e1bf-4653-9b59-b9c0ba0f3647.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8e09541a-e1bf-4653-9b59-b9c0ba0f3647.png)'
- en: A plot of the 64 components extracted with the factor analysis on the original
    dataset
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始数据集进行因子分析提取的64个成分的图
- en: A careful analysis shows that the components are a superimposition of many low-level
    visual features. This is a consequence of the assumption to have a Gaussian prior
    distribution over the components (*z* ∼ *N(0, I)*). In fact, one of the disadvantages
    of this distribution is its intrinsic denseness (the probability of sampling values
    far from the mean is often too high, while in some case, it would be desirable
    to have a peaked distribution that discourages values not close to its mean, to
    be able to observe more selective components). Moreover, considering the distribution
    *p[Z|X; θ]*, the covariance matrix *ψ* could not be diagonal (trying to impose
    this constraint can lead to an unsolvable problem), leading to a resulting multivariate
    Gaussian distribution, which isn't normally made up of independent components.
    In general, the single variables *z[i,]* (conditioned to an input sample, *x[i]*)
    are statistically dependent and the reconstruction *x[i,]* is obtained with the
    participation of almost all extracted features. In all these cases, we say that
    the *coding is dense* and the dictionary of features in *under-complete* (the
    dimensionality of the components is lower than *dim(x[i])*).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细分析表明，这些成分是许多低级视觉特征的叠加。这是由于对成分具有高斯先验分布的假设（*z* ∼ *N(0, I)*）。事实上，这种分布的一个缺点是其固有的密集性（采样值远离均值的概率通常太高，而在某些情况下，人们希望有一个峰值分布，以阻止值远离其均值，以便能够观察到更具有选择性的成分）。此外，考虑到分布*p[Z|X;
    θ]*，协方差矩阵*ψ*不能对角化（试图施加此约束可能导致无法解决的问题），导致一个结果的多变量高斯分布，它通常不是由独立的成分组成的。一般来说，单个变量*z[i,]*（在输入样本*x[i]*的条件下）在统计上是相关的，并且重建*x[i,]*是通过几乎所有提取的特征的参与获得的。在这些所有情况下，我们说*编码是密集的*，特征字典在*欠完备的*（成分的维度低于*dim(x[i])*）。
- en: 'The lack of independence can be also an issue considering that any orthogonal
    transformation *Q* applied to *A* (the factor loading matrix) don''t affect the
    distribution *p[X|Z, θ]*. In fact, as *QQ^T=I*, the following applies:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到任何正交变换*Q*应用于*A*（因子负载矩阵）不会影响分布*p[X|Z, θ]*，缺乏独立性也可能是一个问题。事实上，由于*QQ^T=I*，以下适用：
- en: '![](img/14717969-bbbd-4109-bb03-de12efafb53e.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14717969-bbbd-4109-bb03-de12efafb53e.png)'
- en: In other words, any feature rotation *(x = AQz + ν)* is always a solution to
    the original problem and it's impossible to decide which is the real loading matrix.
    All these conditions lead to the further conclusion that the mutual information
    among components is not equal to zero and neither close to a minimum (in this
    case, each of them carries a specific portion of information). On the other side,
    our main goal was to reduce the dimensionality. Therefore, it's not surprising
    to have dependent components because we aim to preserve the maximum amount of
    original information contained in *p(X)* (remember that the amount of information
    is related to the entropy and the latter is proportional to the variance).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，任何特征旋转*(x = AQz + ν)*都是原始问题的解决方案，并且无法决定哪个是真实的负载矩阵。所有这些条件导致进一步的结论，即成分之间的互信息不等于零，也不接近最小值（在这种情况下，每个都携带特定的信息部分）。另一方面，我们的主要目标是降低维度。因此，有依赖的成分并不奇怪，因为我们旨在保留*p(X)*中包含的最大原始信息量（记住，信息量与熵相关，而熵与方差成正比）。
- en: The same phenomenon can be observed in the PCA (which is still based on the
    Gaussian assumption), but in the last paragraph, we're going to discuss a technique,
    calledICA, whose goal is to create a representation of each sample (without the
    constraint of the dimensionality reduction) after starting from a set of statistically
    independent features. This approach, even if it has its peculiarities, belongs
    to a large family of algorithms called *sparse coding*. In this scenario, if the
    corresponding dictionary has *dim(z[i]) > dim(x[i]),*it is called *over-complete*
    (of course, the main goal is no longer the dimensionality reduction).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的现象可以在PCA（它仍然基于高斯假设）中观察到，但在最后一段，我们将讨论一种称为ICA的技术，其目标是基于一组统计独立的特征创建每个样本的表示（不受到降维的约束）。这种方法，尽管有其特殊性，但仍属于被称为*稀疏编码*的算法大家族。在这种情况下，如果相应的字典满足*dim(z[i])
    > dim(x[i])*，则称为*过完备*（当然，主要目标不再是降维）。
- en: However, we're going to consider only the case when the dictionary is at most
    complete *dim(z[i]) = dim(x[i]**)*, because ICA with over-complete dictionaries
    requires a more complex approach. The level of sparsity, of course, is proportional
    to *dim(z[i])* and with ICA, it's always achieved as a secondary goal (the primary
    one is always the independence between components).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们只考虑字典最多是完备的情况*dim(z[i]) = dim(x[i]*)*，因为过完备字典的ICA需要更复杂的方法。稀疏度水平当然与*dim(z[i])*成正比，并且在使用ICA时，它总是作为次要目标（主要目标始终是成分之间的独立性）实现的。
- en: Principal Component Analysis
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'Another common approach to the problem of reducing the dimensionality of a
    high-dimensional dataset is based on the assumption that, normally, the total
    variance is not explained equally by all components. If *p[data]* is a multivariate
    Gaussian distribution with covariance matrix *Σ*, then the entropy (which is a
    measure of the amount of information contained in the distribution) is as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决高维数据集降维问题的常见方法基于这样的假设：通常，总方差不是由所有成分均匀解释的。如果*p[data]*是一个协方差矩阵为*Σ*的多变量高斯分布，那么熵（它是分布中包含的信息量的度量）如下所示：
- en: '![](img/779ac8db-243b-486a-8f23-8a7426e7c89e.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/779ac8db-243b-486a-8f23-8a7426e7c89e.png)'
- en: Therefore, if some components have a very low variance, they also have a limited
    contribution to the entropy, providing little additional information. Hence, they
    can be removed without a high loss of accuracy.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果某些成分具有非常低的方差，它们对熵的贡献也有限，提供的信息很少。因此，它们可以被移除而不会导致精度损失过高。
- en: 'Just as we''ve done with FA, let''s consider a dataset drawn from *p[data]* ∼
    *N(0, Σ)*(for simplicity, we assume that it''s zero-centered, even if it''s not
    necessary):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在FA中所做的那样，让我们考虑一个从*p[data]* ∼ *N(0, Σ)*（为了简单起见，我们假设它是零中心的，即使这不是必要的）抽取的数据集：
- en: '![](img/14849ea4-4ed0-4929-ba73-832ab5135d7e.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14849ea4-4ed0-4929-ba73-832ab5135d7e.png)'
- en: 'Our goal is to define a linear transformation, *z = A^Tx* (a vector is normally
    considered a column, therefore *x* has a shape *(n × 1)*), such as the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是定义一个线性变换*z = A^Tx*（向量通常被认为是一列，因此*x*的形状是*(n × 1)*），如下所示：
- en: '![](img/871b2980-2211-4a6d-93df-a285b3aee07e.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/871b2980-2211-4a6d-93df-a285b3aee07e.png)'
- en: 'As we want to find out the directions where the variance is higher, we can
    build our transformation matrix, *A,* starting from the eigen decomposition of
    the input covariance matrix, *Σ* (which is real, symmetric, and positive definite):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要找出方差较高的方向，我们可以从输入协方差矩阵*Σ*（它是实数、对称和正定的）的特征分解开始构建我们的变换矩阵*A*：
- en: '![](img/685886b8-fd80-4df1-9944-b1dd724b8e7c.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/685886b8-fd80-4df1-9944-b1dd724b8e7c.png)'
- en: '*V* is an *(n × n)* matrix containing the eigenvectors (as columns), while Ω
    is a diagonal matrix containing the eigenvalues. Moreover, *V* is also orthogonal,
    hence the eigenvectors constitute a basis. An alternative approach is based on
    the **singular value decomposition** (**SVD**), which has an incremental variant
    and there are algorithms that can perform a decomposition truncated at an arbitrary
    number of components, speeding up the convergence process (such as the Scikit-Learn
    implementation `TruncatedSVD`).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*V*是一个包含特征向量（作为列）的*(n × n)*矩阵，而*Ω*是一个包含特征值的对角矩阵。此外，*V*也是正交的，因此特征向量构成了一个基。另一种方法是基于**奇异值分解**（**SVD**），它有一个增量变体，并且有一些算法可以在任意数量的组件上进行截断分解，从而加快收敛过程（例如Scikit-Learn实现`TruncatedSVD`）。'
- en: 'In this case, it''s immediately noticeable that the sample covariance is as
    follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，样本协方差如下所示：
- en: '![](img/b19af4dc-fb3a-46f9-a690-6bcedab0fcc3.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b19af4dc-fb3a-46f9-a690-6bcedab0fcc3.png)'
- en: 'If we apply the SVD to the matrix *X* (each row represents a single sample
    with a shape *(1, n)*), we obtain the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将SVD应用于矩阵*X*（每一行代表一个形状为*(1, n)*的单个样本），我们得到以下结果：
- en: '![](img/80bb6690-725d-46c3-b8fc-889d102f8832.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/80bb6690-725d-46c3-b8fc-889d102f8832.png)'
- en: '*U* is a unitary matrix containing (as rows) the left singular vectors (the
    eigenvectors of *XX^T*), *V* (also unitary) contains (as rows) the right singular
    vectors (corresponding to the eigenvectors of *X^TX*), while *Λ* is a diagonal
    matrix containing the singular values of *Σ[s]* (which are the square roots of
    the eigenvalues of both *XX^(T )*and *X^TX*). Conventionally, the eigenvalues
    are sorted by descending order and the eigenvectors are rearranged to match the
    corresponding position.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*U*是一个包含（作为行）左奇异向量的单位矩阵（*XX^T*的特征向量），*V*（也是单位矩阵）包含（作为行）右奇异向量（对应于*X^TX*的特征向量），而*Λ*是一个包含*Σ[s]*的奇异值的对角矩阵（它们是*XX^(T)*和*X^TX*的特征值的平方根）。通常，特征值按降序排序，特征向量被重新排列以匹配相应的位置。'
- en: 'Hence, we can directly use the matrix *Λ* to select the most relevant eigenvalues
    (the square root is an increasing function and doesn''t change the order) and
    the matrix *V* to retrieve the corresponding eigenvectors (the factor *1/M* is
    a proportionality constant). In this way, we don''t need to compute and eigen
    decompose the covariance matrix *Σ* (contains *n × n* elements) and we can exploit
    some very fast approximate algorithms that work only with the dataset (without
    computing *X^TX*). Using the SVD, the transformation of *X* can be done directly,
    considering that *U* and *V* are unitary matrices (this means that *UU^T* = *U^TU*
    = *I*; therefore, the conjugate transpose is also the inverse):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以直接使用矩阵*Λ*来选择最相关的特征值（平方根是一个增函数，不会改变顺序）和矩阵*V*来检索相应的特征向量（因子*1/M*是一个比例常数）。这样，我们不需要计算和特征分解协方差矩阵*Σ*（包含*n
    × n*个元素），并且可以利用一些仅与数据集（不计算*X^TX*）工作的非常快速的近似算法。使用奇异值分解（SVD），可以直接对*X*进行变换，考虑到*U*和*V*是单位矩阵（这意味着*UU^T*
    = *U^TU* = *I*；因此，共轭转置也是逆）：
- en: '![](img/9b9e58f1-76cf-4488-b718-eb9cc004ae7c.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b9e58f1-76cf-4488-b718-eb9cc004ae7c.png)'
- en: 'Right now, *X* has only been projected in the eigenvector space (it has been
    simply rotated) and its dimensionality hasn''t changed. However, from the definition
    of the eigenvector, we know that the following is true:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，*X*只被投影到特征向量空间（它只是简单地旋转了）并且其维度没有改变。然而，从特征向量的定义中，我们知道以下是真的：
- en: '![](img/2c9a5609-fbda-4231-b881-0c02e8f1ded0.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c9a5609-fbda-4231-b881-0c02e8f1ded0.png)'
- en: 'If *λ* is large, the projection of *v* will be amplified proportionally to
    the variance explained by the direction of the corresponding eigenvector. Therefore,
    if it has not been already done, we can sort (and rename) the eigenvalues and
    the corresponding eigenvectors to have the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*λ*很大，*v*的投影将按对应特征向量方向解释的方差成比例放大。因此，如果尚未这样做，我们可以对特征值及其对应的特征向量进行排序（并重命名），以获得以下结果：
- en: '![](img/8a9575b3-b16c-49aa-8b86-97e648bb289d.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8a9575b3-b16c-49aa-8b86-97e648bb289d.png)'
- en: 'If we select the first top *k* eigenvalues, we can build a transformation matrix
    based on the corresponding eigenvectors (principal components) that projects *X*
    onto a subspace of the original eigenvector space:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择前*k*个最大的特征值，我们可以根据对应的特征向量（主成分）构建一个转换矩阵，该矩阵将*X*投影到原始特征向量空间的一个子空间：
- en: '![](img/fa6a4060-db56-4110-9299-7892d99b2b60.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fa6a4060-db56-4110-9299-7892d99b2b60.png)'
- en: Using the SVD, instead of *A[k]*, we can directly truncate *U* and *Λ*, creating
    the matrices *U[k]* (which contains only the top k eigenvectors) and *Λ[k]*, a
    diagonal matrix with the top k eigenvalues.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奇异值分解（SVD），我们可以直接截断*U*和*Λ*，创建包含仅包含前k个特征向量的矩阵*U[k]*和包含前k个特征值的对角矩阵*Λ[k]*。
- en: 'When choosing the value for *k*, we are assuming that the following is true:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择*k*的值时，我们假设以下情况是正确的：
- en: '![](img/fbf9019c-eeb6-4ec9-b961-a4c0cfe08aa6.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fbf9019c-eeb6-4ec9-b961-a4c0cfe08aa6.png)'
- en: 'To achieve this goal, it is normally necessary to compare the performances
    with a different number of components. In the following graph, there''s a plot
    where the variance ratio (variance explained by component n/total variance) and
    the cumulative variance are plotted as functions of the components:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，通常需要将性能与不同数量的组件进行比较。在下面的图表中，有一个图表显示了方差比（由组件n解释的方差/总方差）和累积方差作为组件的函数：
- en: '![](img/911c3e26-23fd-4da1-9c5a-f0d5a3fcffa0.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/911c3e26-23fd-4da1-9c5a-f0d5a3fcffa0.png)'
- en: Explained variance per component (left) and cumulative variance per component
    (right)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件的解释方差（左）和每个组件的累积方差（右）
- en: In this case, the first 10 components are able to explain 80% of the total variance.
    The remaining 25 components have a slighter and slighter impact and could be removed.
    However, the choice must be always based on the specific context, considering
    the loss of value induced by the loss of information.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，前10个组件能够解释总方差的80%。剩余的25个组件的影响越来越小，可以删除。然而，选择必须始终基于具体上下文，考虑到由于信息丢失而引起的价值损失。
- en: A trick for determining the right number of components is based on the analysis
    of the eigenvalues of X. After sorting them, it's possible to consider the differences
    between subsequent values d = {λ[1] - λ[2], λ[2] - λ[3], ..., λ[n-1] - λ[n]}.
    The highest difference *λ[k] - λ[k+1]* determines the index *k* of a potential
    optimal reduction (obviously, it's necessary to consider a constraint on the minimum
    value, because normally *λ[1] - λ[2]* is the highest difference). For example,
    if d = {4, 4, 3, 0.2, 0.18, 0.05} the original dimensionality is n=6; however, *λ[4] - λ*[*5* ]is
    the smallest difference, so, it's reasonable to reduce the dimensionality to *(n
    + 1) - k = 3*. The reason is straightforward, the eigenvalues determine the magnitude
    of each component, but we need a relative measure because the scale changes. In
    the example, the last three eigenvectors point to directions where the explained
    variance is negligible when compared to the first three components.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 确定正确组件数量的一个技巧是基于X的特征值分析。在排序后，可以考虑到后续值之间的差异d = {λ[1] - λ[2]，λ[2] - λ[3]，...，λ[n-1]
    - λ[n]}。最大的差异*λ[k] - λ[k+1]*决定了潜在最佳降维的索引*k*（显然，需要考虑最小值的约束，因为通常*λ[1] - λ[2]*是最大的差异）。例如，如果d
    = {4，4，3，0.2，0.18，0.05}，则原始维度为n=6；然而，*λ[4] - λ[5]*是最小的差异，因此，将维度降低到*(n + 1) - k
    = 3*是合理的。原因是直截了当的，特征值决定了每个组件的大小，但我们需要一个相对度量，因为尺度会变化。在示例中，最后三个特征向量指向的方向，与前三个组件相比，解释方差可以忽略不计。
- en: 'Once we''ve defined the transformation matrix *A[k]*, it''s possible to perform
    the actual projection of the original vectors in the new subspace, through the
    relation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了转换矩阵*A[k]*，就可以通过以下关系在新的子空间中执行原始向量的实际投影：
- en: '![](img/5ef33cb7-a521-48bb-8f78-5ac18de9fb4b.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ef33cb7-a521-48bb-8f78-5ac18de9fb4b.png)'
- en: 'The complete transformation of the whole dataset is simply obtained as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集的完全转换可以简单地按以下方式获得：
- en: '![](img/c18c569e-5d93-4006-8ffa-3673914f2e42.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c18c569e-5d93-4006-8ffa-3673914f2e42.png)'
- en: 'Now, let''s analyze the new covariance matrix *E[Z^TZ]*. If the original distribution
    *p[data] x ∼ N(0, Σ)*, *p(z)* will also be Gaussian with mean and covariance:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分析新的协方差矩阵 *E[Z^TZ]*。如果原始分布 *p[data] x ∼ N(0, Σ)*，则 *p(z)* 也将是高斯分布，具有均值和协方差：
- en: '![](img/ba582816-1287-4ec2-b1a5-c23c22c33cc2.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba582816-1287-4ec2-b1a5-c23c22c33cc2.png)'
- en: 'We know that *Σ* is orthogonal; therefore, *v[i] • v[j] = 0* if *i ≠ j*. If
    we analyze the term *A^TV*, we get the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 *Σ* 是正交的；因此，当 *i ≠ j* 时，*v[i] · v[j] = 0*。如果我们分析 *A^TV* 这个项，我们得到以下结果：
- en: '![](img/30a5b689-ff6a-4f72-98cb-1316a6659234.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30a5b689-ff6a-4f72-98cb-1316a6659234.png)'
- en: Considering that Ω is diagonal, the resulting matrix *Σ[z]* will be diagonal
    as well. This means that the PCA decorrelates the transformed covariance matrix.
    At the same time, we can state that every algorithm that decorrelates the input
    covariance matrix performs a PCA (with or without dimensionality reduction). For
    example, the *whitening process* is a particular PCA without dimensionality reduction,
    while Isomap (see [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning*) performs the same operation working with the Gram matrix
    with a more geometric approach. This result will be used in [Chapter 6](caf0e7e4-42e3-4807-a97d-6f9968eddb29.xhtml),
    *Hebbian Learning*, to show how some particular neural networks can perform a
    PCA without eigen decomposing *Σ*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 Ω 是对角的，结果矩阵 *Σ[z]* 也将是对角的。这意味着PCA去除了变换后的协方差矩阵的相关性。同时，我们可以声明，每个去除了输入协方差矩阵相关性的算法都执行了PCA（带有或没有降维）。例如，*白化过程*是一个没有降维的特定PCA，而Isomap（见第3章，*基于图的无监督学习*）使用更几何的方法通过Gram矩阵执行相同的操作。这个结果将在第6章，*赫布学习*中使用，以展示一些特定的神经网络如何在不分解
    *Σ* 的特征值的情况下执行PCA。
- en: 'Let''s now consider a FA with homoscedastic noise. We have seen that the covariance
    matrix of the conditional distribution, *p(X|Z; θ),* is equal to *AA^T + Ω*. In
    the case of homoscedastic noise, it becomes *AA^T + ωI*. For a generic covariance
    matrix, *Σ*, it''s possible to prove that adding a constant diagonal matrix *(Σ
    + aI)* doesn''t modify the original eigenvectors and shifts the eigenvalues by
    the same quantity:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个具有同方差噪声的FA。我们已经看到，条件分布的协方差矩阵 *p(X|Z; θ)* 等于 *AA^T + Ω*。在同方差噪声的情况下，它变为
    *AA^T + ωI*。对于一般的协方差矩阵 *Σ*，可以证明添加一个常数对角矩阵 *(Σ + aI)* 不会改变原始的特征向量，并且将特征值移动相同的数量：
- en: '![](img/84cf4d44-0842-4919-bdb7-5ceffdf8b011.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84cf4d44-0842-4919-bdb7-5ceffdf8b011.png)'
- en: 'Therefore, we can consider the generic case of absence of noise without loss
    of generality. We know that the goal of FA (with Ω = (0)) is finding the matrix,
    *A,* so that *AA^T ≈ Q* (the input covariance). Hence, thanks to the symmetry
    and imposing the asymptotic equality, we can write the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以考虑没有噪声的通用情况，而不会失去一般性。我们知道FA（Ω = (0)）的目标是找到矩阵 *A*，使得 *AA^T ≈ Q*（输入协方差）。因此，由于对称性和施加渐近等价性，我们可以写出以下结果：
- en: '![](img/812743e5-3e18-42d4-8efa-47a614ff5e97.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/812743e5-3e18-42d4-8efa-47a614ff5e97.png)'
- en: This result implies that the FA is a more generic (and robust) way to manage
    the dimensionality reduction in the presence of heteroscedastic noise, and the
    PCA is a restriction to homoscedastic noise. When a PCA is performed on datasets
    affected by heteroscedastic noise, the MLE worsens because the different noise
    components, altering the magnitude of the eigenvalues at different levels, can
    drive to the selection of eigenvectors that, in the original dataset, explain
    only a low percentage of the variance (and in a noiseless scenario, it would be
    normally discarded in favor of more important directions). If you think of the
    example discussed at the beginning of the previous paragraph, we know that the
    noise is strongly heteroscedastic, but we don't have any tools to inform the PCA
    to cope with it and the variance of the first component will be much higher than
    expected, considering that the two sources are identical. Unfortunately, in a
    real- life scenario, the noise is correlated and neither a factor nor a PCA can
    efficiently solve the problem when the noise power is very high. In all those
    cases, more sophisticated denoising techniques must be employed. Whenever, instead,
    it's possible to define an approximate diagonal noise covariance matrix, FA is
    surely more robust and efficient than PCA. The latter should be considered only
    in noiseless or *quasi-*noiseless scenarios. In both cases, the results can never
    lead to well-separated features. For this reason, the ICA has been studied and
    many different strategies have been engineered.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果意味着FA是在存在异方差噪声的情况下管理降维的更通用（且更稳健）的方法，而PCA是对同方差噪声的限制。当对受异方差噪声影响的数据集进行PCA时，MLE会变差，因为不同的噪声成分，在不同程度上改变特征值的幅度，可能导致选择到在原始数据集中仅解释了低百分比的方差（在无噪声场景下，通常会被更重要的方向所取代）。如果你想到前一段开头讨论的例子，我们知道噪声是强异方差的，但我们没有工具来告知PCA如何应对它，并且第一个组件的方差将远高于预期，考虑到两个来源是相同的。不幸的是，在现实场景中，噪声是相关的，当噪声功率非常高时，无论是因子分析还是PCA都无法有效地解决问题。在所有这些情况下，必须采用更复杂的去噪技术。相反，如果可以定义一个近似的对角噪声协方差矩阵，FA肯定比PCA更稳健和高效。后者应仅在无噪声或*准*无噪声场景下考虑。在这两种情况下，结果永远不会导致特征分离。因此，ICA已经被研究，并已经设计了许多不同的策略。
- en: 'The complete algorithm for the PCA is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的完整算法如下：
- en: Create a matrix *X^((M × n))* containing all the samples x[i] as rows
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含所有样本x[i]作为行的矩阵*X^((M × n))* 
- en: 'Eigen decomposition version:'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征分解版本：
- en: Compute the covariance matrix *Σ = [X^TX]*
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵*Σ = [X^TX]*
- en: Eigen decompose *Σ = VΩV^T*
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对Σ进行特征分解*Σ = VΩV^T*
- en: 'SVD version:'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVD版本：
- en: Compute the SVD on the matrix *X = UΛV^T*
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对矩阵*X = UΛV^T*进行SVD
- en: Select the top k eigenvalues (from Ω or Λ) and the corresponding eigenvectors
    (from V)
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最大的k个特征值（来自Ω或Λ）和相应的特征向量（来自V）
- en: Create the matrix A with shape *(n × k),* whose columns are the top k eigenvectors
    (each of them has a shape (n × 1))
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个形状为*(n × k)*的矩阵A，其列是最大的k个特征向量（每个特征向量形状为(n × 1)）
- en: Project the dataset into the low-dimensional space Z = XA (eigen decomposition)
    or *Z = UΛ (SVD)*
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集投影到低维空间Z = XA（特征分解）或*Z = UΛ (SVD)*
- en: Some packages (such as Scipy, which is the backend for many NumPy function,
    such as`np.linalg.svd()`) return the matrix V (right singular vectors) already
    transposed. In this case, it's necessary to use *V^T* instead of V in step 3 of
    the algorithm. I suggest always checking the documentation when implementing these
    kinds of algorithms.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一些包（如Scipy，它是许多NumPy函数的后端，如`np.linalg.svd()`）已经返回了转置的矩阵V（右奇异向量）。在这种情况下，算法的第3步中需要使用*V^T*而不是V。我建议在实现这些类型的算法时始终检查文档。
- en: An example of PCA with Scikit-Learn
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn进行PCA的示例
- en: 'We can repeat the same experiment made with the FA and heteroscedastic noise
    to assess the MLE score of the PCA. We are going to use the `PCA` class with the
    same number of components (`n_components=64`). To achieve the maximum accuracy,
    we also set the  `svd_solver=''full'' `parameter, to force Scikit-Learn to apply
    a full SVD instead of the truncated version. In this way, the top eigenvalues
    are selected only after the decomposition, avoiding the risk of imprecise estimations:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复使用与FA和异方差噪声相同的实验来评估PCA的MLE得分。我们将使用具有相同数量组件的`PCA`类（`n_components=64`）。为了达到最大精度，我们还设置了`svd_solver='full'`参数，以强制Scikit-Learn应用完整的SVD而不是截断版本。这样，只有分解后才会选择最大的特征值，避免了不精确估计的风险：
- en: '[PRE7]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result is not surprising: the MLE is much lower than FA, because of the
    wrong estimations made due to the heteroscedastic noise. I invite the reader to
    compare the results with different datasets and noise levels, considering that
    the training performance of PCA is normally higher than FA. Therefore, when working
    with large datasets, a good trade-off is surely desirable. As with FA, it''s possible
    to retrieve the components through the  `components_ `instance variable.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 结果并不令人惊讶：由于异方差噪声导致的错误估计，最大似然估计（MLE）远低于因子分析（FA）。我邀请读者比较不同数据集和噪声水平下的结果，考虑到主成分分析（PCA）的训练性能通常高于因子分析。因此，当处理大型数据集时，一个好的权衡无疑是可取的。与因子分析一样，可以通过`components_`实例变量检索组件。
- en: 'It''s interesting to check the total explained variance (as a fraction of the
    total input variance) through the component-wise instance array `explained_variance_ratio_`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，可以通过组件实例数组`explained_variance_ratio_`检查总解释方差（作为总输入方差的分数）：
- en: '[PRE8]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With 64 components, we are explaining 86% of the total input variance. Of course,
    it''s also useful to compare the explained variance using a plot:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用64个组件，我们解释了总输入方差的86%。当然，使用图表比较解释方差也是有用的：
- en: '![](img/01a974ed-00a8-41bd-aabe-e99aedde239f.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01a974ed-00a8-41bd-aabe-e99aedde239f.png)'
- en: 'As usual, the first components explain the largest part of the variance; however,
    after about the twentieth component, each contribution becomes lower than 1% (decreasing
    till about 0%). This analysis suggests two observations: it''s possible to further
    reduce the number of components with an acceptable loss (using the previous snippet,
    it''s easy to extend the sum only the first *n* components and compare the results)
    and, at the same time, the PCA will be able to overcome a higher threshold (such
    as 95%) only by adding a large number of new components. In this particular case,
    we know that the dataset is made up of handwritten digits; therefore, we can suppose
    that the tail is due to secondary differences (a line slightly longer than average,
    a marked stroke, and so on); hence, we can drop all the components with n > 64
    (or less) without problems (it''s also easy to verify visually a rebuilt image
    using the `inverse_transform()` method). However, it is always best practice to
    perform a complete analysis before moving on to further processing steps, particularly
    when the dimensionality of X is high.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，前几个组件解释了最大的方差部分；然而，大约在第二十个组件之后，每个贡献都低于1%（逐渐降低到大约0%）。这种分析表明两个观察结果：可以通过接受可接受的损失进一步减少组件数量（使用前面的代码片段，很容易只扩展前*n*个组件并比较结果），同时，PCA只能通过添加大量新组件来克服更高的阈值（例如95%）。在这个特定案例中，我们知道数据集由手写数字组成；因此，我们可以假设尾部是由于次要差异（一条略长于平均的线，明显的笔触等）；因此，我们可以没有问题地丢弃所有n
    > 64（或更少）的组件（使用`inverse_transform()`方法直观地验证重建的图像也很容易）。然而，在进行进一步处理步骤之前始终进行完整分析是最佳实践，尤其是在X的维度很高时。
- en: Another interesting approach to determine the optimal number of components has
    been proposed by Minka (*Automatic Choice of Dimensionality for PCA*, *Minka T.P.*,
    *NIPS* 2000") and it's based on the Bayesian model selection. The idea is to use
    the MLE to optimize the likelihood *p(X|k)* where k is a parameter indicating
    the number of components. In other words, it doesn't start analyzing the explained
    variance, but determines a value of *k < n* so that the likelihood keeps being
    the highest possible (implicitly, k will explain the maximum possible variance
    under the constraint of *max(k) = k[max]*). The theoretical foundation (with tedious
    mathematical derivations) of the method is presented in the previously mentioned
    paper however, it's possible to use this method with Scikit-Learn by setting the
    `n_components='mle'` and `svd_solver='full'` parameters.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Minka（*自动选择PCA的维度*，*Minka T.P.*，*NIPS 2000*）提出了另一种确定组件最佳数量的有趣方法，它基于贝叶斯模型选择。想法是使用最大似然估计（MLE）来优化似然度*p(X|k)*，其中k是一个表示组件数量的参数。换句话说，它不是从分析解释方差开始的，而是确定一个*k
    < n*的值，使得似然度保持尽可能高（隐含地，k将在*max(k) = k[max]*的约束下解释最大的可能方差）。该方法的理论基础（包括繁琐的数学推导）在前面提到的论文中已有介绍，然而，可以通过设置Scikit-Learn的`n_components='mle'`和`svd_solver='full'`参数来使用这种方法。
- en: Independent component analysis
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: We have seen that the factors extracted by a PCA are decorrelated, but not independent.
    A classic example is the *cocktail party:* we have a recording of many overlapped
    voices and we would like to separate them. Every single voice can be modeled as
    a random process and it's possible to assume that they are statistically independent
    (this means that the joint probability can be factorized using the marginal probabilities
    of each source). Using FA or PCA, we are able to find uncorrelated factors, but
    there's no way to assess whether they are also independent (normally, they aren't).
    In this section, we are going to study a model that is able to produce sparse
    representations (when the dictionary isn't under-complete) with a set of statistically
    independent components.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，PCA提取的因子是去相关的，但不是独立的。一个经典的例子是鸡尾酒会：我们有一段许多重叠声音的录音，我们想要将它们分开。每个声音都可以被建模为一个随机过程，并且可以假设它们在统计上是独立的（这意味着联合概率可以使用每个源的边缘概率来分解）。使用FA或PCA，我们能够找到去相关的因子，但无法评估它们是否也是独立的（通常它们不是）。在本节中，我们将研究一个能够产生稀疏表示（当字典不是欠完备时）的模型，该模型具有一组统计上独立的成分。
- en: 'Let''s assume we have a zero-centered and whitened dataset *X* sampled from
    *N(0, I)* and noiseless linear transformation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个以零为中心并经过白化的数据集*X*，它是从*N(0, I)*采样的，并且是噪声无线的变换：
- en: '![](img/019cead7-d719-4320-97b3-2464b44ea74b.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/019cead7-d719-4320-97b3-2464b44ea74b.png)'
- en: 'In this case, the prior over, *z,* is modeled as a product of independent variables
    (*α* is the normalization factor), each of them represented as a generic exponential
    where the function *f[k](z)* must be non-quadratic, that is, *p(z; θ)* cannot
    be Gaussian. Furthermore, we assume that the variance of *z[i]* is equal to 1,
    therefore, *p(x|z; **θ) ∼ N(Az, AA^T)*. The joint probability *p(X, z; θ) = p(X|z;
    θ)p(z|θ)* is equal to the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对*z*的先验是建模为一个独立变量的乘积（*α*是归一化因子），每个变量都表示为一个通用的指数函数，其中函数*f[k](z)*必须是非二次的，即*p(z;
    θ)*不能是高斯分布。此外，我们假设*z[i]*的方差等于1，因此*p(x|z; θ) ∼ N(Az, AA^T)*。联合概率*p(X, z; θ) = p(X|z;
    θ)p(z|θ)*等于以下：
- en: '![](img/4246f391-0499-415a-9f92-52bd8ce9c61b.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4246f391-0499-415a-9f92-52bd8ce9c61b.png)'
- en: If *X* has been whitened, *A* is orthogonal (the proof is straightforward);
    hence, the previous expression can be simplified. However, applying the EM algorithm
    requires determining *p(z|X; θ)* and this is quite difficult. The process could
    be easier after choosing a suitable prior distribution for *z*, that is, *f[k](z)*,
    but as we discussed at the beginning of the chapter, this assumption can have
    dramatic consequences if the real factors are distributed differently. For these
    reasons, other strategies have been studied.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X*已经被白化，*A*是正交的（证明是直接的）；因此，前面的表达式可以简化。然而，应用EM算法需要确定*p(z|X; θ)*，这相当困难。在选择*z*的合适先验分布，即*f[k](z)*之后，这个过程可能会更容易，但正如我们在本章开头讨论的那样，如果真实因子分布不同，这个假设可能会有戏剧性的后果。因此，已经研究了其他策略。
- en: 'The main concept that we need to enforce is having a non-Gaussian distribution
    of the factors. In particular, we''d like to have a peaked distribution (inducing
    sparseness) with heavy tails. From the theory, we know that the standardized fourth
    moment (also called *Kurtosis*) is a perfect measure:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要强制执行的主要概念是因子的非高斯分布。特别是，我们希望有一个峰度分布（诱导稀疏性）和重尾。从理论我们知道，标准化的第四矩（也称为*峰度*）是一个完美的度量：
- en: '![](img/f63dbea7-b46f-4bd9-9ee8-6c69aa3fe559.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f63dbea7-b46f-4bd9-9ee8-6c69aa3fe559.png)'
- en: For a Gaussian distribution, *Kurt[X]* is equal to three (which is often considered
    as the reference point, determining the so called *Excess Kurtosis* = *Kurtosis
    - 3*), while it's larger for a family of distributions, called *Leptokurtotic*
    or super-Gaussian, which are peaked and heavy-tailed (also, the distributions
    with *Kurt[X] < 3*, called *Platykurtotic* or sub-Gaussian, can be good candidates,
    but they are less peaked and normally only the super-Gaussian distributions are
    taken into account). However, even if accurate, this measure is very sensitive
    to outliers because of the fourth power. For example, if *x ∼ N(0, 1)* and *z
    = x + ν*, where *ν* is a noise term that alters a few samples, increasing their
    value to two, the result can be a super-Gaussian (*Kurt[x] > 3*) even if, after
    filtering the outliers out, the distribution has *Kurt[x] = 3 (Gaussian)*.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高斯分布，*Kurt[X]* 等于三（这通常被认为是参考点，确定所谓的 *Excess Kurtosis* = *Kurtosis - 3*），而对于称为
    *Leptokurtotic* 或超高斯的一族分布，它们的峰值高且尾部重（此外，*Kurt[X] < 3* 的分布，称为 *Platykurtotic* 或亚高斯分布，也可以是好的候选者，但它们不太尖锐，通常只考虑超高斯分布）。然而，即使准确，这个度量由于四次方的原因对异常值非常敏感。例如，如果
    *x ∼ N(0, 1)* 且 *z = x + ν*，其中 *ν* 是一个噪声项，它改变了一些样本的值，增加到两个，结果可以是一个超高斯分布（*Kurt[x]
    > 3*），即使过滤掉异常值后，分布的 *Kurt[x] = 3 (高斯)*。
- en: 'To overcome this problem, Hyvärinen and Oja (*Independent Component Analysis:
    Algorithms and Applications*, *Hyvarinen A.*, *Oja E.*, Neural Networks 13/2000)
    proposed a solution based on another measure, the *negentropy*. We know that the
    entropy is proportional to the variance and, given the variance, the Gaussian
    distribution has the maximum entropy (for further information, read *Mathematical
    Foundations of Information Theory*, *Khinchin A. I., Dover Publications*); therefore,
    we can define the measure:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，Hyvärinen 和 Oja（*独立成分分析：算法与应用*，*Hyvarinen A.*，*Oja E.*，神经网络 13/2000）提出了基于另一个度量，即
    *负熵* 的解决方案。我们知道熵与方差成正比，给定方差，高斯分布具有最大的熵（对于更多信息，请参阅 *信息论数学基础*，*Khinchin A. I.*，Dover
    出版公司）；因此，我们可以定义这个度量：
- en: '![](img/7966184b-0868-4d64-bf41-803973421c7f.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7966184b-0868-4d64-bf41-803973421c7f.png)'
- en: Formally, the negentropy of *X* is the difference between the entropy of a Gaussian
    distribution with the same covariance and the entropy of *X* (we are assuming
    both zero-centered). It's immediately possible to understand that *H[N](X) ≥ 0*,
    hence the only way to maximize it is by reducing *H(X)*. In this way, *X* becomes
    less random, concentrating the probability around the mean (in other words, it
    becomes super-Gaussian). However, the previous expression cannot be easily adapted
    to closed-form solutions, because *H(X)* needs to be computed over all the distribution
    of *X*, which must be estimated. For this reason, the same authors proposed an
    approximation based on non-quadratic functions (remember that in the context of
    ICA, a quadratic function can be never be employed because it would lead to a
    Gaussian distribution) that is useful to derive a fixed-point iterative algorithm
    called *FastICA*(indeed, it's really faster than EM).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，*X* 的负熵是具有相同协方差的高斯分布的熵与 *X* 的熵（我们假设两者都是零中心的）之差。可以立即理解到 *H[N](X) ≥ 0*，因此最大化它的唯一方法是通过减少
    *H(X)*。这样，*X* 就变得不那么随机，概率集中在均值周围（换句话说，它变成了超高斯分布）。然而，前面的表达式不能轻易地适应闭式解，因为 *H(X)*
    需要计算 *X* 的整个分布，这必须被估计。因此，相同的作者提出了基于非二次函数的近似，这对于推导一个称为 *FastICA* 的固定点迭代算法是有用的（实际上，它比EM算法要快得多）。
- en: 'Using k functions *f[k](x)*, the approximation becomes as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k 个函数 *f[k](x)*，近似变为如下：
- en: '![](img/0993dabf-0638-4931-8d07-0e436cb9c81a.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0993dabf-0638-4931-8d07-0e436cb9c81a.png)'
- en: 'In many real-life scenarios, a single function is enough to achieve a reasonable
    accuracy and one of the most common choices for f(x) is as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际场景中，一个函数就足够达到合理的精度，对于 f(x) 的最常见选择如下：
- en: '![](img/b421b596-abdb-46d4-a009-bf899ca38553.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b421b596-abdb-46d4-a009-bf899ca38553.png)'
- en: In the aforementioned paper, the reader can find some alternatives that can
    be employed when this function fails in forcing statistical independence between
    components.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述论文中，读者可以找到一些当这个函数无法强制组件之间统计独立时可以采用的替代方案。
- en: 'If we invert the model, we get *z = Wx* with *W = A^(-1)*; therefore, considering
    a single sample, the approximation becomes as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13fc3cb9-3bc8-40c8-b86c-562573caee5e.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, the second term doesn''t depend on *w* (in fact, it''s only a reference)
    and can be excluded from the optimization. Moreover, considering the initial assumptions,
    *E[Z^TZ]=W E[X^TX] W^T = I*, therefore *WW^T = I, i.e. ||w||² = 1*. Hence, our
    goal is to find the following:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e460161-d1a1-47ac-bb9e-9b966292ef27.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: 'In this way, we are forcing the matrix *W* to transform the input vector *x*,
    so that *z* has the lowest possible entropy; therefore, it''s super-Gaussian.
    The maximization process is based on convex optimization techniques that are beyond
    the scope of this book (the reader can find all the details of Lagrange theorems
    in *Luenberger D. G., Optimization by Vector Space Methods, Wiley*); therefore,
    we directly provide the iterative step that must be performed:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8463cdf0-16e2-421f-9612-23fb5740d83f.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Of course, to ensure *||w||²** = 1*, after each step, the weight vector w must
    be normalized *(w[t+1] = w[t+1] / ||w[t+1]||)*.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In a more general context, the matrix *W* contains more than one weight vector
    and, if we apply the previous rule to find out the independent factors, it can
    happen that some elements, *w[i]^Tx,* are correlated. A strategy to avoid this
    problem is based on the gram-schmidt orthonormalization process, which decorrelates
    the components one by one, subtracting the projections of the current component
    *(w[n])* onto all the previous ones *(w[1], w[2], ..., w[n-1])* to *w[n]*. In
    this way, *w[n]* is forced to be orthogonal to all the other components.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Even if this method is simple and doesn't require much effort, it's preferable
    a global approach that can work directly with the matrix *W* at the end of an
    iteration (so that the order of the weights is not fixed). As explained in *Fast
    and robust fixedpoint*
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '*algorithms for independent component analysis*, *Hyvarinen A.*, *IEEE Transactions
    on Neural Networks* this result can be achieved with a simple sub-algorithm that
    we are including in the final *FastICA* algorithm:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Set random initial values for *W[0]*
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a threshold *Thr* (for example 0.001)
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Independent component extraction
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *w* in *W*:'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||w[t+1] - w[t]|| > Thr*:'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *w[t+1] = E[x · f^'(w[t]^Tx)] - E[f^('')(w[t]^Tx)] w[t]*
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*w[t+1] = w[t+1] / ||w[t+1]||*'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Orthonormalization
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||W[t+1] - W[t]||[F] > Thr*:'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*W[t] = W[t] / sqrt(||W[t]W[t]^T||)*'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*W[t][+1] = (3/2)W[t] - (1/2)WW^TW*'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This process can be also iterated for a fixed number of times, but the best
    approach is based on using both a threshold and a maximum number of iterations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: An example of FastICA with Scikit-Learn
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the same dataset, we can now test the performance of the ICA. However,
    in this case, as explained, we need to zero-center and whiten the dataset, but
    fortunately these preprocessing steps are done by the Scikit-Learn implementation
    (if the parameter `whiten=True` is omitted).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the ICA on the MNIST dataset, we''re going to instantiate the  `FastICA `class,
    passing the arguments `n_components=64` and the maximum number of iterations `max_iter=5000`.
    It''s also possible to specify which function will be used to approximate the
    negentropy; however, the default is *log cosh(x)*, which is normally a good choice:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point, we can visualize the components (which are always available
    through the  `components_ `instance variance):'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50c13e9a-68a3-4d63-88fd-56de1116c432.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: Independent components of the MNIST dataset extracted by the FastICA algorithm
    (64 components)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'There are still some redundancies (the reader can try to increase the number
    of components) and background noise; however, it''s now possible to distinguish
    some low-level features (such as oriented stripes) that are common to many digits.
    This representation isn''t very sparse yet. In fact, we''re always using 64 components
    (like for FA and PCA); therefore, the dictionary is under-complete (the input
    dimensionality is 28 × 28 = 784). To see the difference, we can repeat the experiment
    with a dictionary ten times larger, setting `n_components=640`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A subset of the new components (100) is shown in the following screenshot:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2ba3dfa-30ae-4a9d-aaa4-b8b58c52b910.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Independent components of the MNIST dataset extracted by the FastICA algorithm
    (640 components)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of these components is almost elementary. They represent oriented
    stripes and positional dots. To check how an input is rebuilt, we can consider
    the mixing matrix *A* (which is available as the `mixing_ `instance variable).
    Considering the first input sample, we can check how many factors have a weight
    less than half of the average:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The sample is rebuilt using approximately 410 components. The level of sparsity
    is higher, but considering the granularity of the factors, it's easy to understand
    that many of them are needed to rebuild even a single structure (like the image
    of a 1) where long lines are present. However, this is not a drawback because,
    as already mentioned, the main goal of the ICA is to extract independent components.
    Considering an analogy with the *cocktail party* example, we could deduce that
    each component represents a phoneme, not the complete sound of a word or a sentence.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The reader can test a different number of components and compare the results
    with the ones achieved by other sparse coding algorithms (such as Dictionary Learning
    or Sparse PCA).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Addendum to HMMs
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed how it''s possible to train a HMM using
    the forward-backward algorithmand we have seen that it is a particular application
    of the EM algorithm. The reader can now understand the internal dynamic in terms
    of E and M steps. In fact, the procedure starts with randomly initialized A and
    B matrices and proceeds in an alternating manner:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '**E-Step**:'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation of the probability *α^t[ij]* that the HMM is in the state *i*
    at time *t* and in the state *j* at time *t+1* given the observations and the
    current parameter estimations (A and B)
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation of the probability *β^t*[*i* ]that the HMM is in the state *i*
    at time *t* given the observations and the current parameter estimations (A and
    B)
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M-Step**:'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the new estimation for the transition probabilities *a[ij] (A)* and
    for the emission probabilities *b[ip] (B)*
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The procedure is repeated until the convergence is reached. Even if there's
    no explicit definition of a Q function, the E-step determines a split expression
    for the expected complete data likelihood of the model given the observations
    (using both the Forward and Backward algorithms), while the M-Step corrects parameters
    A and B to maximize this likelihood.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the EM algorithm, explaining the reasons that
    justify its application in many statistical learning contexts. We also discussed
    the fundamental role of hidden (latent) variables, in order to derive an expression
    that is easier to maximize (the Q function).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: We applied the EM algorithm to solve a simple parameter estimation problem and
    afterward to prove the Gaussian Mixture estimation formulas. We showed how it's
    possible to employ the Scikit-Learn implementation instead of writing the whole
    procedure from scratch (like in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, we analyzed three different approaches to component extraction. FA
    assumes that we have a small number of Gaussian latent variables and a Gaussian
    decorrelated noise term. The only restriction on the noise is to have a diagonal
    covariance matrix, so two different scenarios are possible. When we are in the
    presence of heteroscedastic noise, the process is an actual FA. When, instead,
    the noise is homoscedastic, the algorithm becomes the equivalent of a PCA. In
    this case, the process is equivalent to check the sample space in order to find
    the directions where the variance is higher. Selecting only the most important
    directions, we can project the original dataset onto a low-dimensional subspace,
    where the covariance matrix becomes decorrelated.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems of both FA and PCA is their assumption to model the latent
    variables with Gaussian distributions. This choice simplifies the model, but at
    the same time, yields dense representations where the single components are statistically
    dependent. For this reason, we have investigated how it's possible to force the
    factor distribution to become sparse. The resulting algorithm, which is generally
    faster and more accurate than the MLE, is called FastICA and its goal is to extract
    a set of statistically independent components with the maximization of an approximation
    of the negentropy.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: FA和PCA的一个问题是它们假设用高斯分布来建模潜在变量。这个选择简化了模型，但同时也产生了密集的表示，其中单个成分在统计上是相互依赖的。因此，我们研究了如何强制因子分布变得稀疏。这个结果算法通常比MLE更快、更准确，被称为FastICA，其目标是通过对负熵的近似最大化提取一组统计上独立的成分。
- en: In the end, we provided a brief explanation of the HMM forward-backward algorithm
    (discussed in the previous chapter) considering the subdivision into E and M steps.
    Other EM-specific applications will be discussed in the next chapters.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要解释了HMM前向-后向算法（在上一章中讨论过），考虑到将其分为E步和M步。其他EM特定应用将在下一章中讨论。
- en: In the next chapter, we are going to introduce the fundamental concepts of Hebbian
    learning and self-organizing maps, which are still very useful to solve many specific
    problems, such as principal component extraction, and have a strong neurophysiological
    foundation.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍赫布学习法和自组织映射的基本概念，这些概念对于解决许多具体问题仍然非常有用，例如主成分提取，并且具有强大的神经生理学基础。
