<html><head></head><body>
		<div class="Content" id="_idContainer048">
			<h1 id="_idParaDest-57"><em class="italics"><a id="_idTextAnchor085"/>Chapter 2</em></h1>
		</div>
		<div class="Content" id="_idContainer049">
			<h1 id="_idParaDest-58"><a id="_idTextAnchor086"/>Advanced Clustering Methods</h1>
		</div>
		<div class="Content" id="_idContainer050">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Perform k-modes clustering</li>
				<li class="bullets">Implement DBSCAN clustering</li>
				<li class="bullets">Perform hierarchical clustering and record clusters in a dendrogram</li>
				<li class="bullets">Perform divisive and agglomerative clustering</li>
			</ul>
			<p>In this chapter, we will have a look at some advanced clustering methods and how to record clusters in a dendrogram.</p>
		</div>
		<div class="Content" id="_idContainer079">
			<h2 id="_idParaDest-59"><a id="_idTextAnchor087"/>Introduction</h2>
			<p>So far, we've learned about some of the most basic algorithms of unsupervised learning: k-means clustering and k-medoids clustering. These are not only important for practical use, but are also important for understanding clustering itself. </p>
			<p>In this chapter, we're going to study some other advanced clustering algorithms. We aren't calling them advanced because they are difficult to understand, but because, before using them, a data scientist should have insights into why he or she is using these algorithms instead of the general clustering algorithms we studied in the last chapter. k-means is a general-purpose clustering algorithm that is sufficient for most cases, but in some special cases, depending on the type of data, advanced clustering algorithms can produce better results.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor088"/>Introduction to k-modes Clustering</h2>
			<p>All the types of clustering that we have studied so far are based on a distance metric. But what if we get a dataset in which it's not possible to measure the distance between variables in a traditional sense, as in the case of categorical variables? In such cases, we use k-modes clustering.</p>
			<p>k-modes clustering is an extension of k-means clustering, dealing with modes instead of means. One of the major applications of k-modes clustering is analyzing categorical data such as survey results.</p>
			<h3 id="_idParaDest-61"><a id="_idTextAnchor089"/>Steps for k-Modes Clustering</h3>
			<p>In statistics, mode is defined as the most frequently occurring value. So, for k-modes clustering, we're going to calculate the mode of categorical values to choose centers. So, the steps to perform k-modes clustering are as follows:</p>
			<ol>
				<li>Choose any k number of random points as cluster centers.</li>
				<li>Find the Hamming distance (discussed in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Clustering Methods</em>) of each point from the center.</li>
				<li>Assign each point to a cluster whose center it is closest to according to the Hamming distance.</li>
				<li>Choose new cluster centers in each cluster by finding the mode of all data points in that cluster.</li>
				<li>Repeat this from <em class="italics">Step 2</em> until the cluster centers stop changing.</li>
			</ol>
			<p>You might have noticed that these steps are very similar to those for k-means clustering. Only the type of distance metric is changed here. So, if you understand k-means clustering, it will be very easy for you to understand k-modes clustering as well.<a id="_idTextAnchor090"/></p>
			<h3 id="_idParaDest-62"><a id="_idTextAnchor091"/>Exercise 10: Implementing k-modes Clustering</h3>
			<h4>Note</h4>
			<p class="callout">For all the exercises and activities where we are importing external CSV's or images, go to <strong class="bold">RStudio</strong>-&gt; <strong class="bold">Session</strong>-&gt; <strong class="bold">Set Working Directory</strong>-&gt; <strong class="bold">To Source File Location</strong>. You can see in the console that the path is set automatically.</p>
			<p>The data for this exercise can be downloaded from here: <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Exercise10/breast_cancer.csv">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Exercise10/breast_cancer.csv</a>. This is a categorical dataset that includes nine variables, some categorical and some nominal, describing different breast cancer cases. After saving this data in a file called <strong class="inline">breast_cancer.csv</strong>, we'll do the following:</p>
			<h4>Note </h4>
			<p class="callout">This dataset is taken from the UCI Machine Learning Repository. You can find the dataset at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data</a>. This breast cancer domain was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data. We have downloaded the file and saved it at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Exercise10/breast_cancer.csv">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Exercise10/breast_cancer.csv</a>.</p>
			<ol>
				<li value="1">Read the dataset and store it in a variable:<p class="snippet">bc_data&lt;-read.csv('breast_cancer.csv',header = FALSE)</p></li>
				<li>Store all of the columns from the second column to the end in a new variable:<p class="snippet">k_bc_data&lt;-bc_data[,2:10]</p></li>
				<li>View the first six rows of the <strong class="inline">k_bc_data</strong> variable:<p class="snippet">head(k_bc_data)</p><p>The output contains the six rows with values for different attributes describing the patient, their symptoms, and their treatment:</p><p class="snippet">   V2      V3    V4   V5  V6 V7   V8      V9     V10</p><p class="snippet">1 30-39 premeno 30-34 0-2 no  3  left  left_low  no</p><p class="snippet">2 40-49 premeno 20-24 0-2 no  2 right  right_up  no</p><p class="snippet">3 40-49 premeno 20-24 0-2 no  2  left  left_low  no</p><p class="snippet">4 60-69    ge40 15-19 0-2 no  2 right   left_up  no</p><p class="snippet">5 40-49 premeno   0-4 0-2 no  2 right right_low  no</p><p class="snippet">6 60-69    ge40 15-19 0-2 no  2  left  left_low  no</p></li>
				<li>Import the <strong class="inline">klaR</strong> library, which has the <strong class="inline">kmodes</strong> function. <strong class="inline">klaR</strong> is an R library that is used for classification and visualization:<p class="snippet">install.packages("klaR")</p><p class="snippet">library(klaR)</p></li>
				<li>Predict and store the final cluster centers in a variable. In this step, we enter the dataset and the number of clusters (that is, <strong class="inline">k</strong> and the maximum amount of iterations to find the number of clusters):<p class="snippet">k.centers&lt;-kmodes(k_bc_data,2,iter.max = 100)</p></li>
				<li>View the cluster centers:<p class="snippet">k.centers</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer051">
					<img alt="Figure 2.1: Screenshot of the cluster centers" src="image/C12628_02_01.jpg"/>
				</div>
			</div>
			<h6>Figure 2.1: Screenshot of the cluster centers</h6>
			<p>The clustering algorithm has grouped all of the breast cancer cases into two clusters, with each cluster containing cases that are similar to each other. In the output, there are two main components: the cluster modes and the clustering vector. The cluster modes section is telling us the modes or coordinates of the centers for cluster 1 and cluster 2. Below that, the clustering vector contains the cluster number of each data point in the index sequence.</p>
			<h4>Note</h4>
			<p class="callout">You could get different results every time you run the algorithm because of the random starting positions of the centers. </p>
			<p>As it is a multi-dimensional categorical dataset, there's no easy way to visualize the results other than printing the data to the R console. </p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor092"/>Activity 5: Implementing k-modes Clustering on the Mushroom Dataset</h3>
			<h4>Note </h4>
			<p class="callout">This dataset is taken from the UCI Machine Learning Repository. You can find the dataset at <a href="https://archive.ics.uci.edu/ml/datasets/Mushroom">https://archive.ics.uci.edu/ml/datasets/Mushroom</a>. We have downloaded the file, cleaned the data, and saved it at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity05">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity05</a>. </p>
			<p>In this activity, we will perform k-modes clustering on the mushroom dataset. This dataset lists the attributes of 23 different species of mushrooms. Each species is classified as being either edible (e) or poisonous (p). We will see how well unsupervised learning can classify poisonous and edible mushrooms by grouping the data into two clusters. These steps will help you complete the activity: </p>
			<ol>
				<li value="1">Download <strong class="inline">mushrooms.csv</strong> from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity05">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity05</a>.</li>
				<li>Read the <strong class="inline">mushroom.csv</strong> file into a variable.</li>
				<li>Import the <strong class="inline">klaR</strong> library.</li>
				<li>Calculate the clusters according to k-modes clustering.</li>
				<li>Check the results of clustering by forming a matrix of data labels versus the cluster assigned.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 212.</p></li>
			</ol>
			<p>The output will be a table of true labels and cluster labels, as follows:</p>
			<p class="snippet">       1    2</p>
			<p class="snippet">  e   80 4128</p>
			<p class="snippet">  p 3052  864</p>
			<h2 id="_idParaDest-64">I<a id="_idTextAnchor093"/>ntroduction to Density-Based Clustering (DBSCAN)</h2>
			<p><strong class="bold">Density-based clustering</strong> or DBSCAN is one of the most intuitive forms of clustering. It is very easy to find naturally occurring clusters and outliers in data with this type of clustering. Also, it doesn't require you to define a number of clusters. For example, consider the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer052">
					<img alt="Figure 2.2: A sample scatter plot" src="image/C12628_02_02.jpg"/>
				</div>
			</div>
			<h6>Figure 2.2: A sample scatter plot</h6>
			<p>There are four natural clusters in this dataset and a few outliers. So, DBSCAN will segregate the clusters and outliers, as depicted in the following figure, without you having to tell it how many clusters to identify in the dataset:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer053">
					<img alt="Figure 2.3: Clusters and outliers classified by DBSCAN" src="image/C12628_02_03.jpg"/>
				</div>
			</div>
			<h6>Figure 2.3: Clusters and outliers classified by DBSCAN</h6>
			<p>So, DBSCAN can find regions of high density separated by regions of low density in a scatter plot.</p>
			<h3 id="_idParaDest-65">Ste<a id="_idTextAnchor094"/>ps for DBSCAN </h3>
			<p>As mentioned before, DBSCAN doesn't require you to choose a number of clusters, but you have to choose the other two parameters to perform DBSCAN. The first parameter is commonly denoted by ε (epsilon), which denotes the maximum distance between two points in the same cluster. Another parameter is the minimum number of points in a cluster, which is usually denoted by <strong class="inline">minPts</strong>. Now we'll look at the DBSCAN clustering algorithm step by step:</p>
			<ol>
				<li value="1">Select any point, <strong class="inline">R</strong>, in the dataset.</li>
				<li>Find all the points within distance epsilon from point <strong class="inline">R</strong>.</li>
				<li>If the total number of points within distance epsilon from point <strong class="inline">R</strong> is greater than <strong class="inline">minPts</strong>, then it is a cluster and <strong class="inline">R</strong> is the core point.</li>
				<li>If the total number of points within distance epsilon from point p is less than <strong class="inline">minPts</strong>, all the points within distance epsilon will be classified as noise. We then start the process again from step 2 after selecting a new point, <strong class="inline">R</strong>, that has neither been classified as noise nor as part of the cluster.</li>
				<li>Repeat the process for other points in the cluster to find points within distance epsilon that are not already in the cluster. Those new points will also be classified in the same cluster.</li>
				<li>Once these steps have been performed for all points in the cluster, repeat the same process by selecting a new random point, <strong class="inline">R</strong>, that has neither been classified in a cluster nor as noise.</li>
			</ol>
			<p>So, to illustrate of the preceding algorithm, let's take an example with epsilon as <strong class="bold">x</strong> and the minimum number of points in a cluster as 4. Look at the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 2.4:  Only 2 points lie in the x distance of point R1" src="image/C12628_02_04.jpg"/>
				</div>
			</div>
			<h6>Figure 2.4:  Only 2 points lie within x distance of point R1</h6>
			<p>Only three points lie within <strong class="bold">x</strong> distance of point <strong class="bold">R1</strong> and our threshold for the minimum number of points within <strong class="bold">x</strong> radius is four. So, these four points will be classified as outliers or noise. But if there were one more point, <strong class="bold">R5</strong>, somewhere between <strong class="bold">R1</strong> and <strong class="bold">R4</strong>, all of these four points would belong to a cluster as in the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 2.5: All these four points belong to a cluster" src="image/C12628_02_05.jpg"/>
				</div>
			</div>
			<h6>Figure 2.5: All of these four points belong to a cluster</h6>
			<p>In the preceding figure, points <strong class="bold">R1</strong> and <strong class="bold">R5</strong> are core points, as they have four points each within <strong class="bold">x</strong> distance from them. And points <strong class="bold">R4</strong>, <strong class="bold">R2</strong>, and <strong class="bold">R3</strong> are not core points, as illustrated in the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer056">
					<img alt="Figure 2.6: Core versus non-core points" src="image/C12628_02_06.jpg"/>
				</div>
			</div>
			<h6>Figure 2.6: Core versus non-core points</h6>
			<p>Any point outside of any of these circles would be classified as a noise point like, <strong class="bold">R6</strong> in the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer057">
					<img alt="Figure 2.7: Noise point R6" src="image/C12628_02_07.jpg"/>
				</div>
			</div>
			<h6>Figure 2.7: Noise point R6</h6>
			<h3 id="_idParaDest-66"><a id="_idTextAnchor095"/>Exercise 11: Implementing DBSCAN </h3>
			<p>In this exercise, we will have a look at the implementation of DBSCAN using the Iris dataset. To perform DBSCAN, we will execute the following steps:</p>
			<ol>
				<li value="1">Store the first two columns of the Iris dataset in <strong class="inline">iris_data</strong>:<p class="snippet">iris_data&lt;-iris[,1:2]</p></li>
				<li>Import the <strong class="inline">dbscan</strong> library, which contains the implementation of various DBSCAN algorithms: <p class="snippet">install.packages("dbscan") </p><p class="snippet">library(dbscan)</p></li>
				<li>Calculate and store clusters in the <strong class="inline">clus</strong> variable. In this step, you also have to choose the values of epsilon and <strong class="inline">minPts</strong>:<p class="snippet">clus&lt;-dbscan(iris_data,eps=.3,minPts = 5)</p><h4>Note</h4><p class="callout">You can experiment with the values of epsilon. To get the desired output, we have set the value as 0.3.</p></li>
				<li>Import the <strong class="inline">factoextra</strong> library for visualizing clusters:<p class="snippet">install.packages("factoextra") #use it only the first time if library is not installed already</p><p class="snippet">library(factoextra)</p></li>
				<li>Plot the cluster centers. You need to enter the variable in which you stored the results of DBSCAN as the first parameter of the <strong class="inline">plot</strong> function. As the second parameter, you need to enter a dataset. In our case, it's <strong class="inline">iris_data</strong>. The <strong class="inline">geom</strong> variable in the function is used to define the geometry of the graph. We will only use <strong class="inline">point</strong>. Now, <strong class="inline">ggtheme</strong> is used to select a theme for the plot. <strong class="inline">palette</strong> is used to select the geometry of the points. The <strong class="inline">ellipse</strong> value is set to false so that the function does not draw an outline of the clusters.:<p class="snippet">fviz_cluster(clus,data=iris_data,geom = "point",palette="set2",ggtheme=theme_minimal(),ellipse=FALSE)</p><p>Here is the output:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer058">
					<img alt="Figure 2.8: DBSCAN clusters in different colors" src="image/C12628_02_08.jpg"/>
				</div>
			</div>
			<h6>Figure 2.8: DBSCAN clusters in different colors</h6>
			<p>In Figure 2.8, there are three clusters in orange, blue, and green. The points in black are noise or outliers. Points in orange here belong to one species, but points in green belong to two different species. </p>
			<h4>Note</h4>
			<p class="callout">You can make a quick comparison of this scatter plot with Figure 1.18 of <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Clustering Methods</em>.</p>
			<p>DBSCAN is different from all the clustering we've studied so far and is one of the most common and one of the most frequently cited types of clustering methods in scientific literature. There are a few advantages that DBSCAN has over other clustering methods:</p>
			<ul>
				<li>You don't need to select the number of clusters initially.</li>
				<li>It does not produce different results every time you run it, unlike k-means or other "k-type" clustering methods, where starting points can have an effect on the end results. So, results are reproducible.</li>
				<li>It can discover clusters of any shape in data.</li>
			</ul>
			<p>But there are also a few disadvantages of DBSCAN:</p>
			<ul>
				<li>The correct set of parameters, that is, epsilon and <strong class="inline">minPts</strong>, are hard to determine for the proper clustering of data.</li>
				<li>DBSCAN cannot differentiate between clusters based on density. If a dataset has large variations in density, it may not perform as well.</li>
			</ul>
			<h3 id="_idParaDest-67">Uses of <a id="_idTextAnchor096"/>DBSCAN </h3>
			<p>As DBSCAN is one of the most frequently cited clustering methods, it has many practical uses. Some of the practical real-life uses of DBSCAN clustering are the following:</p>
			<ul>
				<li>DBSCAN can be used in urban planning in many ways. For example, given data about crime incidents with locations, DBSCAN can be used to identify crime-ridden areas in a city. This data can be used to plan police force deployment or even investigate potential gang activity.</li>
				<li>DBSCAN can be used to form strategies in games such as cricket and basketball. Given the data related to ball pitching on a cricket pitch, you can identify the strengths and weaknesses of batsmen and bowlers. Or if we have data on a batsman hitting the ball, data gained from DBSCAN can be used to adjust fielding accordingly.</li>
				<li>During natural disasters or the spread of viral diseases, the geolocation of tweets can be used to identify highly affected areas with DBSCAN.</li>
			</ul>
			<h3 id="_idParaDest-68">Activity<a id="_idTextAnchor097"/> 6: Implementing DBSCAN and Visualizing the Results</h3>
			<p>In this activity, we will perform DBSCAN and compare the results with k-means clustering. To do this, we are going to use the <strong class="inline">multishapes</strong> dataset, which contains simulated data that represents different shapes. </p>
			<p>These steps will help you complete the activity:</p>
			<ol>
				<li value="1">Generate and store the <strong class="inline">multishapes</strong> dataset in the <strong class="inline">factoextra</strong> library.</li>
				<li>Plot the first two columns of the <strong class="inline">multishapes</strong> dataset.</li>
				<li>Perform k-means clustering on the dataset and visualize.</li>
				<li>Perform DBSCAN on the dataset and visualize the data to compare the results with k-means clustering.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 213.</p></li>
			</ol>
			<p>The plot of DBSCAN clustering will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer059">
					<img alt="Figure 2.9: Expected plot of DBCAN on the multishapes dataset" src="image/C12628_02_09.jpg"/>
				</div>
			</div>
			<h6>Figure 2.9: Expected plot of DBCAN on the multishapes dataset</h6>
			<p>Here, all the points in black are anomalies and are not classified in any cluster, and the clusters formed in DBSCAN cannot be obtained with any other type of clustering method. These clusters have taken several types of shapes and sizes, whereas, in k-means, all clusters are of approximately spherical shape.</p>
			<h3 id="_idParaDest-69"><a id="_idTextAnchor098"/>Introduction to Hierarchical Clustering</h3>
			<p>The last type of clustering that we're going to study is hierarchical clustering. A hierarchy is defined as "a system in which people or things are placed in a series of levels with different importance or status." Hierarchical clustering merges clusters sequentially. This sequence of merged clusters is called a hierarchy. We can see that more clearly with the help of the output of a hierarchical clustering algorithm called a <strong class="bold">dendrogram</strong>.</p>
			<p>Hierarchical clustering comes in two types:</p>
			<ul>
				<li>Agglomerative</li>
				<li>Divisive</li>
			</ul>
			<p>Since both types of hierarchical clustering are similar, it makes sense to study both of them together.</p>
			<p>Agglomerative clustering is also known as the bottom-up approach to hierarchical clustering. In this method, each data point is assumed to be a single cluster at the outset. From there, we start merging the most similar clusters according to a similarity or distance metric until all the data points are merged in a single cluster.</p>
			<p>In divisive clustering, we do exactly the opposite. It is a top-down approach to hierarchical clustering. In this method, all the data points are assumed to be in a single cluster initially. From there on, we start splitting the cluster into multiple clusters until each data point is a cluster on its own. Differences and similarities between the two clustering types will become clear in further sections. But first, we should try to understand why we need this other type of clustering and the special purpose it serves that other types of clustering don't serve. Hierarchical clustering is used mainly for the following reasons:</p>
			<ul>
				<li>Just like DBSCAN, we don't have to choose a number of clusters initially.</li>
				<li>The final output of hierarchical clustering, dendrograms, can help us visualize the clustering results in a way that means we don't need to re-run the algorithm to see a different number of clusters in the results.</li>
				<li>Unlike k-means, any type of distance metric can be used in hierarchical clustering.</li>
				<li>It can find complex-shaped clusters, unlike other clustering algorithms, such as k-means, which only finds approximately spherical clusters.</li>
			</ul>
			<p>The combination of all the preceding factors make hierarchical clustering an important clustering method in unsupervised learning.</p>
			<h3 id="_idParaDest-70"><a id="_idTextAnchor099"/>Types of Similarity Metrics</h3>
			<p>As described previously, agglomerative hierarchical clustering is a bottom-up approach to hierarchical clustering. We merge the most similar clusters one by one based on a similarity metric. This similarity metric can be chosen from one of several different types:</p>
			<ul>
				<li><strong class="bold">Single link</strong>: In single-link similarity, we measure the distance or similarity between the two most similar points of two clusters:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer060">
					<img alt="Figure 2.10: Demonstration of the single-link metric" src="image/C12628_02_10.jpg"/>
				</div>
			</div>
			<h6>Figure 2.10: Demonstration of the single-link metric</h6>
			<ul>
				<li><strong class="bold">Complete link</strong>: In this type of metric, we measure the distance or similarity between the two most distant points of a cluster:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer061">
					<img alt="Figure 2.11: Demonstration of the complete-link metric" src="image/C12628_02_11.jpg"/>
				</div>
			</div>
			<h6>Figure 2.11: Demonstration of the complete-link metric</h6>
			<ul>
				<li><strong class="bold">Group average</strong>: In this metric, we measure the average distance between all members of one cluster and any members of a second cluster:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer062">
					<img alt="Figure 2.12: Demonstration of the group-average metric" src="image/C12628_02_12.jpg"/>
				</div>
			</div>
			<h6>Figure 2.12: Demonstration of the group-average metric</h6>
			<h4>Note</h4>
			<p class="callout">The distance between members of the same cluster is not measured in these similarity metrics.</p>
			<ul>
				<li><strong class="bold">Centroid similarity</strong>: In this type of similarity, the similarity between two clusters is defined as the similarity between the centroids of both clusters:</li>
			</ul>
			<p class="Normal" lang="en-US" xml:lang="en-US"><img alt="Figure 2.13: Demonstration of centroid similarity" src="image/C12628_02_13.png"/></p>
			<h6>Figure 2.13: Demonstration of centroid similarity</h6>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor100"/>Steps to Perform Agglomerative Hierarchical Clustering</h3>
			<p>With the knowledge of these similarity metrics, we can now understand the algorithm to perform agglomerative hierarchical clustering:</p>
			<ol>
				<li value="1">Initialize each point as a single cluster.</li>
				<li>Calculate the similarity metric between every pair of clusters. The similarity metric can be any of the four metrics we just read about.</li>
				<li>Merge the two most similar clusters according to the similarity metric selected in step 2.</li>
				<li>Repeat the process from step 2 until we have only one cluster left.</li>
			</ol>
			<p>This whole process will produce a graph called a dendrogram. This graph records the clusters formed at each step. A simple dendrogram with very few elements would look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 2.14: A sample dendrogram" src="image/C12628_02_14.jpg"/>
				</div>
			</div>
			<h6>Figure 2.14: A sample dendrogram</h6>
			<p>In the preceding dendogram, suppose that point A and point B are the closest among all points on the similarity measure that we are using. Their closeness is used to determine the height of the joining line, which is at <strong class="bold">L1</strong> in the case of point <strong class="bold">A</strong> and point <strong class="bold">B</strong>. So, point <strong class="bold">A</strong> and point <strong class="bold">B</strong> are clustered first. After that, at <strong class="bold">L2</strong>, point <strong class="bold">D</strong> and point <strong class="bold">E</strong> are clustered, then, at <strong class="bold">L3</strong>, points <strong class="bold">A</strong>, <strong class="bold">B</strong>, and <strong class="bold">C</strong> are clustered. At this point, we have two clusters that are joined together to form one cluster at <strong class="bold">L4</strong>.</p>
			<p>Now, to get clusters from this dendrogram, we make horizontal cuts. For example, if we make a cut between L4 and L3, we will get two clusters:</p>
			<ul>
				<li>Cluster 1 – A, B, and C</li>
				<li>Cluster 2 – D and E</li>
			</ul>
			<p>The clusters will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 2.15: Clusters represented in the dendrogram" src="image/C12628_02_15.jpg"/>
				</div>
			</div>
			<h6>Figure 2.15: Clusters represented in the dendrogram</h6>
			<p>Similarly, if we make a horizontal cut in the dendrogram between L3 and L2, we will get three clusters:</p>
			<ul>
				<li>Cluster 1 – A and B</li>
				<li>Cluster 2 – C</li>
				<li>Cluster 3 – D and E</li>
			</ul>
			<p>The clusters will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="Figure 2.16: Representation of clusters in a dendrogram" src="image/C12628_02_16.jpg"/>
				</div>
			</div>
			<h6>Figure 2.16: Representation of clusters in a dendrogram</h6>
			<p>So, to get a different number of clusters, we didn't need to rerun the process. With this method, we can get any number of clusters possible in the data without executing the whole process again.</p>
			<h3 id="_idParaDest-72"><a id="_idTextAnchor101"/>Exercise 12: Agglomerative Clustering with Different Similarity Measures</h3>
			<p>In this exercise, we're going to perform agglomerative hierarchical clustering with different similarity measures and compare the results:</p>
			<ol>
				<li value="1">Let's enter the last three columns of the <strong class="inline">iris_flowers</strong> dataset, which are petal length, petal width, and species, in the <strong class="inline">iris_data</strong> variable, as follows:<p class="snippet">iris_data&lt;-iris[,3:5]</p><p class="snippet">install.packages('cluster')</p><p class="snippet">library(cluster)</p></li>
				<li>In this step, we use the <strong class="inline">hclust</strong> function to get hierarchical clustering. In the <strong class="inline">hclust</strong> function, we need to enter the pair-wise distance of all the points with each other, for which we use the <strong class="inline">dist()</strong> function. The second parameter, <strong class="inline">method</strong>, is used to define the similarity measure for the hierarchical clustering:<p class="snippet">h.clus&lt;-hclust(dist(iris_data),method="complete")</p></li>
				<li>Now, let's plot the results of the hierarchical clustering in a dendrogram, as follows:<p class="snippet">plot(h.clus)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer067"><img alt="Figure 2.17: Dendrogram derived from the complete similarity metric" src="image/C12628_02_17.jpg"/></div><h6>Figure 2.17: Dendrogram derived from the complete similarity metric</h6></li>
				<li>To select a number of clusters from the preceding dendrogram, we can use an R function called <strong class="inline">cutree</strong>. We feed the results of <strong class="inline">hclust</strong> along with the number of clusters to this function:<p class="snippet">clusterCut &lt;- cutree(h.clus, 3)</p><p class="snippet">table(clusterCut, iris_data$Species)</p><p>The output table is as follows:</p><div class="IMG---Figure" id="_idContainer068"><img alt="" src="image/C12628_02_18.jpg"/></div><h6>Figure 2.18: Table displaying the distribution of clusters</h6><p>The setosa and virginica species get classified accurately with this clustering method.</p></li>
				<li>Use <strong class="inline">single</strong> as a similarity metric as follows:.<p class="snippet">h.clus&lt;-hclust(dist(iris_data),method = "single")</p><p class="snippet">plot(h.clus)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer069"><img alt="Figure 2.19: Dendrogram derived from the single similarity metric" src="image/C12628_02_19.jpg"/></div><h6>Figure 2.19: Dendrogram derived from the single similarity metric</h6><p>Notice how this dendrogram is different from the dendrogram created with the <strong class="inline">complete</strong> similarity metric.</p></li>
				<li>Divide this dataset into three clusters:<p class="snippet">clusterCut &lt;- cutree(h.clus, 3)</p><p class="snippet">table(clusterCut, iris_data$Species)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer070"><img alt="Figure 2.20: Table displaying the distribution of clusters" src="image/C12628_02_20.jpg"/></div><h6>Figure 2.20: Table displaying the distribution of clusters</h6><p>Here, our clustering method is successfully able to separate only one class from the other two.</p></li>
				<li>Now let's perform hierarchical clustering with the <strong class="inline">average</strong> similarity metric:<p class="snippet">h.clus&lt;-hclust(dist(iris_data),method = "average")</p><p class="snippet">plot(h.clus)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer071"><img alt="Figure 2.21: Dendrogram derived from the average similarity metric" src="image/C12628_02_21.jpg"/></div><h6>Figure 2.21: Dendrogram derived from the average similarity metric</h6></li>
				<li>Let's divide the preceding dendrogram into three clusters again and see the results:<p class="snippet">clusterCut &lt;- cutree(h.clus, 3)</p><p class="snippet">table(clusterCut, iris_data$Species)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer072"><img alt="Figure 2.22: Table displaying the distribution of clusters" src="image/C12628_02_22.jpg"/></div><h6>Figure 2.22: Table displaying the distribution of clusters</h6><p>Here, with the <strong class="inline">average</strong> similarity metric, we get almost completely correct classification results.</p></li>
				<li>Now let's try creating a dendrogram with the last similarity metric, <strong class="inline">centroid</strong>:<p class="snippet">h.clus&lt;-hclust(dist(iris_data),method = "centroid")</p><p class="snippet">plot(h.clus)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer073"><img alt="Figure 2.23: Dendrogram derived from the centroid similarity metric" src="image/C12628_02_23.jpg"/></div><h6>Figure 2.23: Dendrogram derived from the centroid similarity metric</h6></li>
				<li>Now, let's divide the preceding dendrogram into three clusters and see the results:<p class="snippet">clusterCut &lt;- cutree(h.clus, 3)</p><p class="snippet">table(clusterCut, iris_data$Species)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="Figure 2.24: Table displaying the distribution of clusters" src="image/C12628_02_24.jpg"/>
				</div>
			</div>
			<h6>Figure 2.24: Table displaying the distribution of clusters</h6>
			<p>Although dendrograms of clusters with the <strong class="inline">average</strong> and <strong class="inline">centroid</strong> similarity metrics look different, they have the same number of elements in each cluster when we cut the dendrogram at three clusters.</p>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor102"/>Divisive Clustering</h3>
			<p>Divisive clustering is the opposite of agglomerative clustering. In agglomerative clustering, we start with each point as its own cluster, while in divisive clustering, we start with the whole dataset as one cluster and from there, we start dividing it into more clusters:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="Figure 2.25: Representation of agglomerative and divisive clustering" src="image/C12628_02_25.jpg"/>
				</div>
			</div>
			<h6>Figure 2.25: Representation of agglomerative and divisive clustering</h6>
			<p>So, the divisive clustering process can be summarized in one figure as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer076">
					<img alt="Figure 2.26: Representation of the divisive clustering process" src="image/C12628_02_26.jpg"/>
				</div>
			</div>
			<h6>Figure 2.26: Representation of the divisive clustering process</h6>
			<h3 id="_idParaDest-74"><a id="_idTextAnchor103"/>Steps to Perform Divisive Clustering</h3>
			<p>From step 1 to step 6, the divisive clustering process keeps on dividing points into further clusters until each point is a cluster on its own. Figure 2.26 shows the first 6 steps of the divisive clustering process, but to correctly run the complete process, we will need to execute as many steps as there are points in the dataset.</p>
			<p>So, for divisive clustering, we will use the DIANA algorithm – DIANA stands for Divisive Analysis. For this, we need to carry out the following steps:</p>
			<ol>
				<li value="1">Start with all the points in the dataset in one single cluster.</li>
				<li>Choose two of the most dissimilar clusters of all the possible clusters in the dataset according to any distance metric you like.</li>
				<li>Repeat step 2 until all the points in the dataset are clustered on their own.</li>
			</ol>
			<p>We're going to use the <strong class="inline">cluster</strong> library in R to perform DIANA clustering.</p>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor104"/>Exercise 13: Performing DIANA Clustering </h3>
			<p>In this exercise, we will perform DIANA clustering:</p>
			<ol>
				<li value="1">Put the petal length, petal width and species name in the <strong class="inline">iris_data</strong> variable:<p class="snippet">iris_data&lt;-iris[,3:5]</p></li>
				<li>Import the <strong class="inline">cluster</strong> library:<p class="snippet">install.packages("cluster")</p><p class="snippet">library("cluster")</p></li>
				<li>Pass the <strong class="inline">iris_data</strong> dataset and the metric by which to measure dissimilarity to the <strong class="inline">diana()</strong> function:<p class="snippet">h.clus&lt;-diana(iris_data, metric="euclidean")</p></li>
				<li>Plot the dendrogram with the <strong class="inline">pltree()</strong> function. To plot the dendrogram, pass the results of the <strong class="inline">diana</strong> function and the title of the graph to the <strong class="inline">pltree()</strong> function:<p class="snippet">pltree(h.clus, cex = 0.6, main = "Dendrogram of divisive clustering")</p><p>The dendrogram of the divisive clustering results appears as follows:</p><div class="IMG---Figure" id="_idContainer077"><img alt="Figure 2.27: Dendrogram of divisive clustering" src="image/C12628_02_27.jpg"/></div><h6>Figure 2.27: Dendrogram of divisive clustering</h6></li>
				<li>If we divide the preceding dendrogram into three clusters, we'll see that this clustering method is also able to identify different species of flowers on its own:<p class="snippet">clusterCut &lt;- cutree(h.clus, 3)</p><p class="snippet">table(clusterCut, iris_data$Species)</p><p>The output is as follows:</p><p class="snippet">clusterCut setosa versicolor virginica</p><p class="snippet">         1     50          1         0</p><p class="snippet">         2      0         49         0</p><p class="snippet">         3      0          0        50</p></li>
			</ol>
			<p>In the preceding output, only one flower is misclassified into another category. This is the best performance of all the clustering algorithms we have encountered in this book when it comes to classifying flower species without knowing anything about them.</p>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor105"/>Activity 7: Performing Hierarchical Cluster Analysis on the Seeds Dataset</h3>
			<p>In this activity, we will perform hierarchical cluster analysis on the seeds dataset. We will see what the results of the clustering are when classifying three types of seeds.</p>
			<h4>Note </h4>
			<p class="callout">This dataset is taken from the UCI Machine Learning Repository. You can find the dataset at <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00236/">http://archive.ics.uci.edu/ml/machine-learning-databases/00236/</a>. We have downloaded the file, cleaned it, and saved it at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity07/seeds_data.txt">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity07/seeds_data.txt</a>. </p>
			<p>These steps will help you complete the activity:</p>
			<ol>
				<li value="1">Download the seeds dataset from <a href=" https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity07/seeds_data.txt"> https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson02/Activity07/seeds_data.txt</a>.</li>
				<li>Perform agglomerative hierarchical clustering of the dataset and plot the dendrogram.</li>
				<li>Make a cut at <strong class="inline">k=3</strong> and check the results of the clustering by forming a table with original labels.</li>
				<li>Perform divisive clustering on the dataset and plot the dendrogram.</li>
				<li>Make cut at <strong class="inline">k=3</strong> and check the results of the clustering by forming a table with original labels.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 215.</p></li>
			</ol>
			<p>The output of this activity will be a table that shows how the results of the clustering have performed at classifying the three types of seeds. It will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="Figure 2.28: Expected table classifying the three types of seeds" src="image/C12628_02_28.jpg"/>
				</div>
			</div>
			<h6>Figure 2.28: Expected table classifying the three types of seeds</h6>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor106"/>Summary</h2>
			<p>Congratulations on comple<a id="_idTextAnchor107"/>tin<a id="_idTextAnchor108"/>g the second chapter on clustering techniques! With this, we've covered all the major clustering techniques, including k-modes, DBSCAN, and both types of hierarchical clustering, and we've also looked at what connects them. We can apply these techniques to any type of dataset we may encounter. These new methods, at times, also produced better results on the same dataset that we used in the first chapter. In the next chapter, we're going to study probability distributions and their uses in exploratory data analysis.</p>
		</div>
	</body></html>