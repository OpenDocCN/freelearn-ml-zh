["```py\n$ git clone https://github.com/apache/spark.git\nCloning into 'spark'...\nremote: Counting objects: 301864, done.\n...\n$ cd spark\n$sh ./ dev/change-scala-version.sh 2.11\n...\n$./make-distribution.sh --name alex-build-2.6-yarn --skip-java-test --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-2.11 -Phadoop-2.6\n...\n\n```", "```py\n$ bin/spark-shell --master spark://<master-address>:7077\n\n```", "```py\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 4987\n\n# Describe the sink (the instructions to configure and start HDFS are provided in the Appendix)\na1.sinks.k1.type=hdfs\na1.sinks.k1.hdfs.path=hdfs://localhost:8020/flume/netcat/data\na1.sinks.k1.hdfs.filePrefix=chapter03.example\na1.sinks.k1.channel=c1\na1.sinks.k1.hdfs.writeFormat = Text\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```", "```py\n$ wget http://mirrors.ocf.berkeley.edu/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz\n$ md5sum apache-flume-1.6.0-bin.tar.gz\nMD5 (apache-flume-1.6.0-bin.tar.gz) = defd21ad8d2b6f28cc0a16b96f652099\n$ tar xf apache-flume-1.6.0-bin.tar.gz\n$ cd apache-flume-1.6.0-bin\n$ ./bin/flume-ng agent -Dlog.dir=. -Dflume.log.level=DEBUG,console -n a1 -f ../chapter03/conf/flume.conf\nInfo: Including Hadoop libraries found via (/Users/akozlov/hadoop-2.6.4/bin/hadoop) for HDFS access\nInfo: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-api-1.7.5.jar from classpath\nInfo: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar from classpath\n...\n\n```", "```py\n$ nc localhost 4987\nHello\nOK\nWorld\nOK\n\n...\n\n```", "```py\n$ bin/hdfs dfs -text /flume/netcat/data/chapter03.example.1463052301372\n16/05/12 04:27:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n1463052302380  Hello\n1463052304307  World\n\n```", "```py\nval lines = scala.io.Source.fromFile(\"...\").getLines.toSeq\nval counts = lines.flatMap(line => line.split(\"\\\\W+\")).sorted.\n  foldLeft(List[(String,Int)]()){ (r,c) =>\n    r match {\n      case (key, count) :: tail =>\n        if (key == c) (c, count+1) :: tail\n        else (c, 1) :: r\n        case Nil =>\n          List((c, 1))\n  }\n}\n```", "```py\nval linesRdd = sc.textFile(\"hdfs://...\")\nval counts = linesRdd.flatMap(line => line.split(\"\\\\W+\"))\n    .map(_.toLowerCase)\n    .map(word => (word, 1)).\n    .reduceByKey(_+_)\ncounts.collect\n```", "```py\n$ wget http://mirrors.sonic.net/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz\n$ tar xvf spark-1.6.1-bin-hadoop2.6.tgz\n$ cd spark-1.6.1-bin-hadoop2.6\n$ mkdir leotolstoy\n$ (cd leotolstoy; wget http://www.gutenberg.org/files/1399/1399-0.txt)\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\nscala> val linesRdd = sc.textFile(\"leotolstoy\", minPartitions=10)\nlinesRdd: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[3] at textFile at <console>:27\n\n```", "```py\nscala> val countsRdd = linesRdd.flatMap(line => line.split(\"\\\\W+\")).\n | map(_.toLowerCase).\n | map(word => (word, 1)).\n | reduceByKey(_+_)\ncountsRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:31\n\n```", "```py\nscala> countsRdd.collect.filter(_._2 > 99)\nres3: Array[(String, Int)] = Array((been,1061), (them,841), (found,141), (my,794), (often,105), (table,185), (this,1410), (here,364), (asked,320), (standing,132), (\"\",13514), (we,592), (myself,140), (is,1454), (carriage,181), (got,277), (won,153), (girl,117), (she,4403), (moment,201), (down,467), (me,1134), (even,355), (come,667), (new,319), (now,872), (upon,207), (sister,115), (veslovsky,110), (letter,125), (women,134), (between,138), (will,461), (almost,124), (thinking,159), (have,1277), (answer,146), (better,231), (men,199), (after,501), (only,654), (suddenly,173), (since,124), (own,359), (best,101), (their,703), (get,304), (end,110), (most,249), (but,3167), (was,5309), (do,846), (keep,107), (having,153), (betsy,111), (had,3857), (before,508), (saw,421), (once,334), (side,163), (ough...\n\n```", "```py\n# The sink is Spark\na1.sinks.k1.type=org.apache.spark.streaming.flume.sink.SparkSink\na1.sinks.k1.hostname=localhost\na1.sinks.k1.port=4989\n\n```", "```py\nobject FlumeWordCount {\n  def main(args: Array[String]) {\n    // Create the context with a 2 second batch size\n    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"FlumeWordCount\")\n    val ssc = new StreamingContext(sparkConf, Seconds(2))\n    ssc.checkpoint(\"/tmp/flume_check\")\n    val hostPort=args(0).split(\":\")\n    System.out.println(\"Opening a sink at host: [\" + hostPort(0) + \"] port: [\" + hostPort(1).toInt + \"]\")\n    val lines = FlumeUtils.createPollingStream(ssc, hostPort(0), hostPort(1).toInt, StorageLevel.MEMORY_ONLY)\n    val words = lines\n      .map(e => new String(e.event.getBody.array)).map(_.toLowerCase).flatMap(_.split(\"\\\\W+\"))\n      .map(word => (word, 1L))\n      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```", "```py\n$ ./bin/flume-ng agent -Dflume.log.level=DEBUG,console -n a1 –f ../chapter03/conf/flume-spark.conf\n...\n\n```", "```py\n$ cd ../chapter03\n$ sbt \"run-main org.akozlov.chapter03.FlumeWordCount localhost:4989\n...\n\n```", "```py\n$ echo \"Happy families are all alike; every unhappy family is unhappy in its own way\" | nc localhost 4987\n...\n-------------------------------------------\nTime: 1464161488000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n...\n\n-------------------------------------------\nTime: 1464161490000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n...\n\n```", "```py\nobject KafkaWordCount {\n  def main(args: Array[String]) {\n    // Create the context with a 2 second batch size\n    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaWordCount\")\n    val ssc = new StreamingContext(sparkConf, Seconds(2))\n    ssc.checkpoint(\"/tmp/kafka_check\")\n    System.out.println(\"Opening a Kafka consumer at zk:[\" + args(0) + \"] for group group-1 and topic example\")\n    val lines = KafkaUtils.createStream(ssc, args(0), \"group-1\", Map(\"example\" -> 1), StorageLevel.MEMORY_ONLY)\n    val words = lines\n      .flatMap(_._2.toLowerCase.split(\"\\\\W+\"))\n      .map(word => (word, 1L))\n      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```", "```py\n$ wget http://apache.cs.utah.edu/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz\n...\n$ tar xf kafka_2.11-0.9.0.1.tgz\n$ bin/zookeeper-server-start.sh config/zookeeper.properties\n...\n\n```", "```py\n$ bin/kafka-server-start.sh config/server.properties\n...\n\n```", "```py\n$ sbt \"run-main org.akozlov.chapter03.KafkaWordCount localhost:2181\"\n...\n\n```", "```py\n$ echo \"Happy families are all alike; every unhappy family is unhappy in its own way\" | ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic example\n...\n\n$ sbt \"run-main org.akozlov.chapter03.FlumeWordCount localhost:4989\n...\n-------------------------------------------\nTime: 1464162712000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n\n```", "```py\n$ ./bin/spark-sql\n…\nspark-sql> select min(duration), max(duration), avg(duration) from kddcup;\n…\n0  58329  48.34243046395876\nTime taken: 11.073 seconds, Fetched 1 row(s)\n\n```", "```py\n$ ./bin/spark-shell\n…\nscala> val df = sqlContext.sql(\"select min(duration), max(duration), avg(duration) from kddcup\"\n16/05/12 13:35:34 INFO parse.ParseDriver: Parsing command: select min(duration), max(duration), avg(duration) from alex.kddcup_parquet\n16/05/12 13:35:34 INFO parse.ParseDriver: Parse Completed\ndf: org.apache.spark.sql.DataFrame = [_c0: bigint, _c1: bigint, _c2: double]\nscala> df.collect.foreach(println)\n…\n16/05/12 13:36:32 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:22, took 4.593210 s\n[0,58329,48.34243046395876]\n\n```", "```py\n    $ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/h/hadoop-2.6.4.tar.gz\n    --2016-05-12 00:10:55--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\n     => 'hadoop-2.6.4.tar.gz.1'\n    Resolving apache.cs.utah.edu... 155.98.64.87\n    Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.\n    Logging in as anonymous ... Logged in!\n    ==> SYST ... done.    ==> PWD ... done.\n    ==> TYPE I ... done.  ==> CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.\n    ==> SIZE hadoop-2.6.4.tar.gz ... 196015975\n    ==> PASV ... done.    ==> RETR hadoop-2.6.4.tar.gz ... done.\n    ...\n    $ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds\n    --2016-05-12 00:13:58--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds\n     => 'hadoop-2.6.4.tar.gz.mds'\n    Resolving apache.cs.utah.edu... 155.98.64.87\n    Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.\n    Logging in as anonymous ... Logged in!\n    ==> SYST ... done.    ==> PWD ... done.\n    ==> TYPE I ... done.  ==> CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.\n    ==> SIZE hadoop-2.6.4.tar.gz.mds ... 958\n    ==> PASV ... done.    ==> RETR hadoop-2.6.4.tar.gz.mds ... done.\n    ...\n    $ shasum -a 512 hadoop-2.6.4.tar.gz\n    493cc1a3e8ed0f7edee506d99bfabbe2aa71a4776e4bff5b852c6279b4c828a0505d4ee5b63a0de0dcfecf70b4bb0ef801c767a068eaeac938b8c58d8f21beec  hadoop-2.6.4.tar.gz\n    $ cat !$.mds\n    hadoop-2.6.4.tar.gz:    MD5 = 37 01 9F 13 D7 DC D8 19  72 7B E1 58 44 0B 94 42\n    hadoop-2.6.4.tar.gz:   SHA1 = 1E02 FAAC 94F3 35DF A826  73AC BA3E 7498 751A 3174\n    hadoop-2.6.4.tar.gz: RMD160 = 2AA5 63AF 7E40 5DCD 9D6C  D00E EBB0 750B D401 2B1F\n    hadoop-2.6.4.tar.gz: SHA224 = F4FDFF12 5C8E754B DAF5BCFC 6735FCD2 C6064D58\n     36CB9D80 2C12FC4D\n    hadoop-2.6.4.tar.gz: SHA256 = C58F08D2 E0B13035 F86F8B0B 8B65765A B9F47913\n     81F74D02 C48F8D9C EF5E7D8E\n    hadoop-2.6.4.tar.gz: SHA384 = 87539A46 B696C98E 5C7E352E 997B0AF8 0602D239\n     5591BF07 F3926E78 2D2EF790 BCBB6B3C EAF5B3CF\n     ADA7B6D1 35D4B952\n    hadoop-2.6.4.tar.gz: SHA512 = 493CC1A3 E8ED0F7E DEE506D9 9BFABBE2 AA71A477\n     6E4BFF5B 852C6279 B4C828A0 505D4EE5 B63A0DE0\n     DCFECF70 B4BB0EF8 01C767A0 68EAEAC9 38B8C58D\n     8F21BEEC\n\n    $ tar xf hadoop-2.6.4.tar.gz\n    $ cd hadoop-2.6.4\n\n    ```", "```py\n    $ cat << EOF > etc/hadoop/core-site.xml\n    <configuration>\n     <property>\n     <name>fs.defaultFS</name>\n     <value>hdfs://localhost:8020</value>\n     </property>\n    </configuration>\n    EOF\n    $ cat << EOF > etc/hadoop/hdfs-site.xml\n    <configuration>\n     <property>\n     <name>dfs.replication</name>\n     <value>1</value>\n     </property>\n    </configuration>\n    EOF\n\n    ```", "```py\n    $ bin/hdfs namenode -format\n    16/05/12 00:55:40 INFO namenode.NameNode: STARTUP_MSG: \n    /************************************************************\n    STARTUP_MSG: Starting NameNode\n    STARTUP_MSG:   host = alexanders-macbook-pro.local/192.168.1.68\n    STARTUP_MSG:   args = [-format]\n    STARTUP_MSG:   version = 2.6.4\n    STARTUP_MSG:   classpath =\n    ...\n\n    ```", "```py\n    $ bin/hdfs namenode &\n    ...\n    $ bin/hdfs secondarynamenode &\n    ...\n    $ bin/hdfs datanode &\n    ...\n\n    ```", "```py\n    $ date | bin/hdfs dfs –put – date.txt\n    ...\n    $ bin/hdfs dfs –ls\n    Found 1 items\n    -rw-r--r-- 1 akozlov supergroup 29 2016-05-12 01:02 date.txt\n    $ bin/hdfs dfs -text date.txt\n    Thu May 12 01:02:36 PDT 2016\n\n    ```", "```py\n    $ cat /tmp/hadoop-akozlov/dfs/data/current/BP-1133284427-192.168.1.68-1463039756191/current/finalized/subdir0/subdir0/blk_1073741827\n    Thu May 12 01:02:36 PDT 2016\n\n    ```"]