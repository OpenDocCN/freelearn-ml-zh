<html><head></head><body><div class="chapter" title="Chapter&#xA0;12.&#xA0;Reinforcement learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Reinforcement learning</h1></div></div></div><p>We have covered supervised and unsupervised learning methods in-depth in <a class="link" href="ch05.html" title="Chapter 5. Decision Tree based learning">Chapter 5</a>, <span class="emphasis"><em>Decision Tree based learning</em></span>, with various algorithms. In this chapter, we will be covering a new learning technique that is different from both supervised and unsupervised learning called <a id="id1297" class="indexterm"/>
<span class="strong"><strong>Reinforcement Learning</strong></span> (<span class="strong"><strong>RL</strong></span>). Reinforcement Learning is a particular type of Machine learning where the learning is driven by the feedback from the environment, and the learning technique is iterative and adaptive. RL is believed to be closer to human learning. The primary goal of RL is decision making and at the heart of it lies <a id="id1298" class="indexterm"/>
<span class="strong"><strong>Markov's Decision Process</strong></span> (<span class="strong"><strong>MDP</strong></span>). In this chapter, we will cover some basic Reinforcement Learning methods like <a id="id1299" class="indexterm"/>
<span class="strong"><strong>Temporal Difference</strong></span> (<span class="strong"><strong>TD</strong></span>), certainty equivalence, policy gradient, dynamic programming, and more. The following figure depicts different data architecture paradigms that will be covered in this chapter:</p><div class="mediaobject"><img src="graphics/B03980_12_01.jpg" alt="Reinforcement learning"/></div><p>The following topics are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Recap of supervised, semi-supervised, and unsupervised learning, and the context of Reinforcement Learning.</li><li class="listitem" style="list-style-type: disc">Understanding MDP is key to Reinforcement Learning. Regarding this, the following topics are covered in this chapter:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What does MDP mean, key attributes, states, reward, actions, and transitions (discounts)</li><li class="listitem" style="list-style-type: disc">The underlying process of MDP and how it helps in the decision process</li><li class="listitem" style="list-style-type: disc">Policies and value functions (also called utilities, as in a group of rewards) and how we assign value to an infinite sequence of rewards</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bellman Equation</strong></span>—the value<a id="id1300" class="indexterm"/> iteration and policy iteration</li></ul></div></li><li class="listitem" style="list-style-type: disc">Regarding Reinforcement Learning, we will cover the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Planning and learning in MDP</li><li class="listitem" style="list-style-type: disc">Connection planning and functional approximation in RL</li><li class="listitem" style="list-style-type: disc">Different RL methods and approaches to RL, such as simple decision theory, the <a id="id1301" class="indexterm"/><span class="strong"><strong>temporal difference</strong></span> (<span class="strong"><strong>TD</strong></span>), dynamic programming, policy gradient, certainty equivalence, and eligibility traces</li><li class="listitem" style="list-style-type: disc">Key algorithms such as Q-learning, Sarsa, and others</li><li class="listitem" style="list-style-type: disc">Reinforcement learning applications</li></ul></div></li></ul></div><div class="section" title="Reinforcement Learning (RL)"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec56"/>Reinforcement Learning (RL)</h1></div></div></div><p>Let's do a recap of <a id="id1302" class="indexterm"/>supervised, semi-supervised, and unsupervised learning, and set the context for Reinforcement Learning. In <a class="link" href="ch01.html" title="Chapter 1. Introduction to Machine learning">Chapter 1</a>, <span class="emphasis"><em>Introduction to Machine Learning</em></span>, we covered the basic definitions of supervised, semi-supervised, and unsupervised learning. Inductive learning is a reasoning process that uses the results of one experiment to run the next set of experiments and iteratively evolve a model from specific information.</p><p>The following figure depicts various subfields of Machine learning. These subfields are one of the ways the Machine learning algorithms are classified:</p><div class="mediaobject"><img src="graphics/B03980_12_30.jpg" alt="Reinforcement Learning (RL)"/></div><p>Supervised learning<a id="id1303" class="indexterm"/> is all about operating to a known expectation, and in this <a id="id1304" class="indexterm"/>case, what needs to be analyzed from the data being defined. The input datasets in this context are also referred to as<a id="id1305" class="indexterm"/> <span class="strong"><strong>labeled</strong></span> datasets. Algorithms classified under this category focus on establishing a relationship between the input and output attributes and uses this relationship speculatively to generate an output for new input data points. In the preceding section, the example defined for the classification problem is also an example of supervised learning. Labeled data helps build reliable models and are usually expensive and limited. The following diagram depicts the workflow for supervised learning:</p><div class="mediaobject"><img src="graphics/B03980_12_03.jpg" alt="Reinforcement Learning (RL)"/></div><p>So, this is a function approximation, where given the <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span> pairs, our goal is to find the function <span class="emphasis"><em>f</em></span> that maps the new <span class="emphasis"><em>x</em></span> to a proper <span class="emphasis"><em>y</em></span>:</p><p>
<span class="emphasis"><em>y = f(x)</em></span>
</p><p>In some of the<a id="id1306" class="indexterm"/> learning problems, we do not have any specific target in mind to solve for; this kind of learning is specifically called unsupervised analysis or learning. The goal, in this case, is to decipher structures in data as against build mapping between input and output attributes of data, and in fact, the output attributes are not defined. These learning algorithms operate on an <span class="strong"><strong>unlabelled</strong></span> dataset<a id="id1307" class="indexterm"/> for this reason.</p><div class="mediaobject"><img src="graphics/B03980_12_04.jpg" alt="Reinforcement Learning (RL)"/></div><p>So, given a bunch of <span class="emphasis"><em>x</em></span>'s, the goal here is to define a function <span class="emphasis"><em>f</em></span> that can give a concise description for a set of <span class="emphasis"><em>x</em></span>'s. Hence, this is called clustering:</p><p>
<span class="emphasis"><em>f(x)</em></span>
</p><p>Semi-supervised learning<a id="id1308" class="indexterm"/> is about using both labeled and unlabeled data to learn better models. It is important that there are appropriate assumptions for the unlabeled data as any incorrect assumptions can invalidate the model. Semi-supervised learning takes its motivation from the human way of learning.</p><div class="section" title="The context of Reinforcement Learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec141"/>The context of Reinforcement Learning</h2></div></div></div><p>Reinforcement Learning<a id="id1309" class="indexterm"/> is about learning that is focused on maximizing the rewards from the result. For example, while teaching toddlers new habits, rewarding toddlers every time they follow instructions works very well. In fact, they figure out what behavior helps them earn rewards. This is exactly what Reinforcement Learning is it is also called as credit assessment learning.</p><p>The most important thing in Reinforcement Learning is that, the model is additionally responsible for making decisions for which a periodic reward is received. The results, in this case, unlike supervised learning, are not immediate and may require a sequence of steps to be executed before the final result is seen. Ideally, the algorithm will generate a sequence of decisions that will help achieve the highest reward or utility.</p><div class="mediaobject"><img src="graphics/B03980_12_05.jpg" alt="The context of Reinforcement Learning"/></div><p>The goal of this<a id="id1310" class="indexterm"/> learning technique is to measure the trade-offs effectively by exploring and exploiting the data. For example, when a person has to travel from a point A to point B, there will be many ways that include traveling by air, water, road, or on foot, and there is a significant value in considering this data measuring the trade-offs for each of these options. Another important aspect is, what would a delay in the rewards mean? Moreover, how it would affect learning? For example, in games like chess, any delay in reward identification may change or impact the result.</p><p>So, the representation is very similar to supervised learning, the difference being that the input is not <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span> pairs but <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>z</em></span> pairs. The goal is to find a function <span class="emphasis"><em>f</em></span> that identifies a <span class="emphasis"><em>y</em></span>, given <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>z</em></span>. In the following sections, we will explore more of what the <span class="emphasis"><em>z</em></span> is. The equation for definition of the goal function is as given here:</p><p>
<span class="emphasis"><em>y = f(x)</em></span> given <span class="emphasis"><em>z</em></span>.</p><p>A formal definition of Reinforcement Learning is as follows:</p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>"Reinforcement Learning is defined as a way of programming agents by reward and punishment without needing to specify how the task is to be achieved."</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><span class="emphasis"><em>Kaelbling, Littman, &amp; Moore, 96</em></span></span></td></tr></table></div><p>So, overall, RL is neither a type of neural network nor is an alternative to neural networks, but an orthogonal approach for Machine learning with emphasis being on learning feedback that is used for evaluating the learner's performance with no standard behavioral targets against which the performance is measured, for example, learning to ride a bicycle.</p><p>Let's now look at the<a id="id1311" class="indexterm"/> formal or basic RL model and understand different elements<a id="id1312" class="indexterm"/> in action. As a first step, let's understand some basic terms.</p><div class="mediaobject"><img src="graphics/B03980_12_06.jpg" alt="The context of Reinforcement Learning"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Agent</strong></span>: An agent<a id="id1313" class="indexterm"/><a id="id1314" class="indexterm"/> is an entity that is a learner as well as a decision maker, typically an intelligent program in this case.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Environment</strong></span>: An environment<a id="id1315" class="indexterm"/> is an <a id="id1316" class="indexterm"/>entity that is responsible for producing a new situation given an action performed by the agent. It gives rewards or feedback for the action. So, in short, the environment is everything other than an agent.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>State</strong></span>: A state<a id="id1317" class="indexterm"/> is a situation that an <a id="id1318" class="indexterm"/>action lands an entity in.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Action</strong></span>: An action<a id="id1319" class="indexterm"/> is a step executed <a id="id1320" class="indexterm"/>by an agent that results in a change in state.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Policy</strong></span>: A<a id="id1321" class="indexterm"/> policy<a id="id1322" class="indexterm"/> is a definition of how an agent behaves at a given point in time. It elaborates the mapping between the states and actions and is usually a simple business rule or a function.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Reward</strong></span>: A<a id="id1323" class="indexterm"/> reward<a id="id1324" class="indexterm"/> lays down short-term benefit of an action that helps in reaching the goal.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Value:</strong></span> There is <a id="id1325" class="indexterm"/>another important <a id="id1326" class="indexterm"/>element in Reinforcement Learning, and that is a value function. While reward function is all about the short-term or immediate benefit of an action, a value function is about the good in long run. This value is an accumulation of rewards an agent is expected to get from the time the world started.</li></ul></div><div class="section" title="Examples of Reinforcement Learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec113"/>Examples of Reinforcement Learning</h3></div></div></div><p>The easiest way to <a id="id1327" class="indexterm"/>understand Reinforcement Learning is to look at some of the practical and real-world applications of it. In this section, we will list down and understand some of them.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Chess game</strong></span>: In the <a id="id1328" class="indexterm"/>game of chess, a player makes a move; this move is driven by an informed selection of an action that comes with a set of counter moves from the opponent player. The next action of the player is determined by what moves the opponent takes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Elevator Scheduling</strong></span>: Let's<a id="id1329" class="indexterm"/> take an example of a building with many floors and many elevators. The key optimization requirement here is to choose which elevator should be sent to which floor and is categorized as a control problem. The input here is a set of buttons pressed (inside and outside the lift) across the floors, locations of the elevators, and a set of floors. The reward, in this case, is the least waiting time of the people wanting to use the lift. Here, the system learns how to control the elevators again; through learning in a simulation of the building, the system learns to control the elevators through the estimates of the value of actions from the past.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Network packet routing</strong></span>: This is a <a id="id1330" class="indexterm"/>case of defining a routing policy for dynamically changing networks. Q-learning techniques (covered a little later in the chapter) are used to identify which adjacent node the packet should be routed to. In this case, each node has a queue, and one packet is dispatched at a time.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mobile robot behavior</strong></span>: A <a id="id1331" class="indexterm"/>mobile robot needs to decide between it reaching the recharge point or the next trash point depending on how quickly it has been able to find a recharge point in the past.</li></ul></div></div><div class="section" title="Evaluative Feedback"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec114"/>Evaluative Feedback</h3></div></div></div><p>One of the key features that <a id="id1332" class="indexterm"/>differentiates Reinforcement Learning from the other learning types is that it uses the information to evaluate the impact of a particular action than instructing blindly what action needs to be taken. Evaluative feedback on one hand indicates how good the action taken is while instructive feedback indicates what the correct action is irrespective of whether the action is taken or not. Although these two mechanisms are different in their way, there are some cases where techniques are employed in conjunction. In this section, we will explore some evaluative feedback methods that will lay the foundation for the rest of the chapter.</p><div class="section" title="n-Armed Bandit problem"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl4sec49"/>n-Armed Bandit problem</h4></div></div></div><p>A formal definition of <a id="id1333" class="indexterm"/>this problem with the<a id="id1334" class="indexterm"/> original gambler analogy is given as follows:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>According to Wikipedia, n-armed bandit problem is an issue where the "gambler" decides which machine to play, the order of play and the duration of play, he then plays and collects the reward that is unique for a machine with a goal to maximize the overall rewards.</p></div></div><p>Let's consider a case where there are thousands of actions that can be taken. Each action fetches a reward, and our goal is to ensure that we take actions in such a way that the total of the rewards is maximized over a period. The selection of a particular action is called a <span class="emphasis"><em>play</em></span>.</p><p>An example case that explains the analogy of n-Armed Bandit the problem is that of a doctor who needs to choose from a series of options to treat a serious ailment where the survival of the patient becomes the reward for the choice of action (in this case, the treatment). In this problem, each action is associated with an expected reward for a selected action; this is called <span class="emphasis"><em>value</em></span>. If the value of each action was known to us, solving n-armed bandit problem is easy as we will choose those actions that have a maximum value. It is only possible that we have the estimates of the values and are not certain about the actual values.</p><p>Now let us see the difference between exploration and exploitation.</p><p>Assuming we maintain the estimates of the values, if we choose an action with the greatest value (this action is called a greedy action), this situation is called exploitation, as we are best using the current knowledge on hand. Moreover, all the cases where any non-greedy action is chosen, it would be more of exploring, and it would help improve the estimates of the non-greedy action. While exploitation helps maximize the expected reward, exploration helps increase the total reward in the longer run. The short-term rewards are lower in the case of exploration while there might be better long-term total reward. For every action, exploration or exploitation approach can be chosen, and what works is a fine balance between these two techniques.</p><p>So, with this, we will<a id="id1335" class="indexterm"/> now look<a id="id1336" class="indexterm"/> at some techniques to best estimate the values for actions and to choose the best-suited actions.</p></div><div class="section" title="Action-value methods"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl4sec50"/>Action-value methods</h4></div></div></div><p>If value of an <a id="id1337" class="indexterm"/>action <span class="emphasis"><em>a</em></span> is <span class="emphasis"><em>Q*(a)</em></span>, then the <a id="id1338" class="indexterm"/>assessed value of the <span class="emphasis"><em>t</em></span><sup>th</sup> play is <span class="emphasis"><em>Q</em></span><sub>t</sub><span class="emphasis"><em>(a) I</em></span>, the mean of the rewards given that action is chosen, the following equation represents this:</p><p>
<span class="emphasis"><em>Q</em></span><sub>t</sub><span class="emphasis"><em>(a) = (r1+r2+ … r</em></span><sub>ka</sub><span class="emphasis"><em> ) / ka</em></span>, where <span class="emphasis"><em>r</em></span> is the reward and <span class="emphasis"><em>ka</em></span> is the number of times the action <span class="emphasis"><em>a</em></span> is chosen. This is one way of estimating action value and not necessarily the best way. Let's live with this and now look at the methods to choose actions.</p><p>The easiest action selection rule is to select an action or one of the actions, <span class="emphasis"><em>a</em></span>, that has the highest estimated action value. So, for a given play <span class="emphasis"><em>t</em></span>, choosing a greedy action <span class="emphasis"><em>a*</em></span> can be shown as follows:</p><p>Q<sub>t</sub>(a*) = max<sub>a</sub> Q<sub>t</sub>(a)</p><p>This method by definition exploits the current knowledge with a little focus on whether the action is a better option. As an alternative to this method, we can choose to be greedy most of the time, and once in a while, select an action independent of the value estimation. With a probability of ԑ, this method is called the<a id="id1339" class="indexterm"/> ԑ<span class="strong"><strong>-greedy method</strong></span>.</p></div><div class="section" title="Reinforcement comparison methods"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl4sec51"/>Reinforcement comparison methods</h4></div></div></div><p>We have been seeing in<a id="id1340" class="indexterm"/> most of the <a id="id1341" class="indexterm"/>selection methods that an action that has the largest reward has a higher likelihood of being selected than an action with a lesser reward. The important question is how to qualify whether a reward is big or small. We will always need to have a reference number that qualifies if a reward has a high value or a low value. This reference value is called <a id="id1342" class="indexterm"/>
<span class="strong"><strong>reference reward</strong></span>. A reference reward, to start with, can be an average of previously received rewards. Learning methods that use this idea are called comparison reinforcement methods. These methods are more efficient than actor-value methods and form a basis for an actor-critic method that we will discuss in the sections to come.</p></div></div><div class="section" title="The Reinforcement Learning problem – the world grid example"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec115"/>The Reinforcement Learning problem – the world grid example</h3></div></div></div><p>We will try to understand the <a id="id1343" class="indexterm"/>Reinforcement Learning problem using a famous example: the grid world. This particular grid world is a 3X4 grid, as shown in the following screenshot, and is an approximation of the complexity of the world:</p><div class="mediaobject"><img src="graphics/B03980_12_07.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div><p>This example assumes the world is kind of a game where you start with a state called start state (from the location <span class="emphasis"><em>1,1</em></span>). Let's assume four actions can be taken that include moving left, right, up, and down. The goal is to ensure using these actions that we move towards the goal that is represented in the location <span class="emphasis"><em>4,3</em></span>. We need to avoid the red box that is shown in the location <span class="emphasis"><em>4,2</em></span> like it is shown in the next image.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Start state</strong></span>: position <span class="emphasis"><em>(1,1) --&gt;</em></span> The world starts here.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Success state</strong></span>: position <span class="emphasis"><em>(4,3) --&gt;</em></span> The world ends here in a success state.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Failure state</strong></span>: position <span class="emphasis"><em>(4,2) --&gt;</em></span> The world ends here in a failure state.</li><li class="listitem" style="list-style-type: disc">When the world ends, we need to start over again.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Wall</strong></span>: There is a roadblock or a wall shown in the position <span class="emphasis"><em>(2,2)</em></span>. This position cannot be navigated:<div class="mediaobject"><img src="graphics/B03980_12_08.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc">To reach the goal <span class="emphasis"><em>(4,3)</em></span> from the start point <span class="emphasis"><em>(1,1)</em></span>, steps can be taken in the following directions:<div class="mediaobject"><img src="graphics/B03980_12_09.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc">Every step in a<a id="id1344" class="indexterm"/> direction moves you from one position to another (position here is nothing but the state). For example, a movement in the <span class="emphasis"><em>UP</em></span> direction from the position <span class="emphasis"><em>(1,1)</em></span> will take you to the position <span class="emphasis"><em>(1,2)</em></span> and so on.</li><li class="listitem" style="list-style-type: disc">All directions cannot be taken from a given position. Let us consider the example shown in the following screenshot. From the position <span class="emphasis"><em>(3,2)</em></span>, only <span class="emphasis"><em>UP</em></span>, <span class="emphasis"><em>DOWN</em></span>, and <span class="emphasis"><em>RIGHT</em></span> can be taken. <span class="emphasis"><em>LEFT</em></span> movement will hit the wall and hence cannot be taken. That said, only <span class="emphasis"><em>UP</em></span> and <span class="emphasis"><em>DOWN</em></span> movements make sense, as <span class="emphasis"><em>RIGHT</em></span> will make a move to the danger position that results in failure in reaching the goal.<div class="mediaobject"><img src="graphics/B03980_12_10.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc">Similarly, any of the positions in the boundaries of the grid will have limitations, for example, the position <span class="emphasis"><em>(1,3)</em></span> allows <span class="emphasis"><em>RIGHT</em></span> and <span class="emphasis"><em>DOWN</em></span> movements and any other movements do not alter the position.<div class="mediaobject"><img src="graphics/B03980_12_11.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc">Let's now look at the <a id="id1345" class="indexterm"/>shortest path the from <span class="emphasis"><em>Start (1,1)</em></span> to the <span class="emphasis"><em>Goal (4,3)</em></span>. There are two solutions:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Solution 1</strong></span>: <span class="emphasis"><em>RIGHT</em></span> --&gt; <span class="emphasis"><em>RIGHT</em></span> --&gt; <span class="emphasis"><em>UP</em></span> --&gt; <span class="emphasis"><em>UP</em></span> --&gt; <span class="emphasis"><em>RIGHT</em></span> (5 steps)<div class="mediaobject"><img src="graphics/B03980_12_12.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Solution 2</strong></span>: <span class="emphasis"><em>UP</em></span> --&gt; <span class="emphasis"><em>UP</em></span> --&gt; <span class="emphasis"><em>RIGHT</em></span> --&gt; <span class="emphasis"><em>RIGHT</em></span> --&gt; <span class="emphasis"><em>RIGHT</em></span> (5 steps)</li></ul></div><div class="mediaobject"><img src="graphics/B03980_12_13.jpg" alt="The Reinforcement Learning problem – the world grid example"/></div></li><li class="listitem" style="list-style-type: disc">In the real world, not<a id="id1346" class="indexterm"/> all actions get executed as expected. There is a reliability factor that affects the performance, or rather, there is uncertainty. If we add a small caveat to the example and say that every time there is an action to move from one position to another, the probability that the movement is correct is 0.8. This means there is an 80% possibility that a movement executes as expected. In this case, if we want to measure the probability of Solution 1, (<span class="emphasis"><em>R</em></span>--&gt;<span class="emphasis"><em>R</em></span>--&gt;<span class="emphasis"><em>U</em></span>--&gt;<span class="emphasis"><em>U</em></span>--&gt;<span class="emphasis"><em>R</em></span>) is succeeding:<p>Probability of actions happening as expected + Probability of actions not happening as expected</p><p>
<span class="emphasis"><em>= 0.8 x 0.8 x 0.8 x 0.8 x 0.8 + 0.1 x 0.1 x 0.1 x 0.1 x 0.8</em></span>
</p><p>
<span class="emphasis"><em>= 0.32768 + 0.00008 = 0.32776</em></span>
</p></li><li class="listitem" style="list-style-type: disc">As we see, the element of uncertainty does change the result. In the next section, we will discuss the decision process framework that captures these uncertainties.</li></ul></div></div><div class="section" title="Markov Decision Process (MDP)"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec116"/>Markov Decision Process (MDP)</h3></div></div></div><p>Markov's Decision <a id="id1347" class="indexterm"/>Process is<a id="id1348" class="indexterm"/> an essential framework or process to make decisions, and we will be bringing it up in most of the sections that follow on Reinforcement Learning.</p><p>
<span class="strong"><strong>Markov property</strong></span>
<a id="id1349" class="indexterm"/> is core to the Markov Decision Process, and it states that what matters is the present or current state and that the situation is stationary, which means that the rules do not change.</p><p>MDP tries to capture the world that we discussed in the preceding section that has the following features:</p><div class="mediaobject"><img src="graphics/B03980_12_14.jpg" alt="Markov Decision Process (MDP)"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>States</strong></span>: In the preceding example, every grid position denotes a state</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Model</strong></span> (Transition function): A Transition function includes three attributes: given state, action, and the destination state. It describes the probability of an end state <span class="emphasis"><em>s</em></span>, given the current state <span class="emphasis"><em>s</em></span> and action <span class="emphasis"><em>a</em></span>:<p>T (s, a,s') ~ P(s'/s, a)</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Actions</strong></span>: A(s), A In the preceding example, <span class="emphasis"><em>A (1, 2)</em></span> = <span class="emphasis"><em>UP</em></span> in the <span class="emphasis"><em>UP</em></span>, <span class="emphasis"><em>UP</em></span> <span class="emphasis"><em>RIGHT</em></span>, <span class="emphasis"><em>RIGHT</em></span>, <span class="emphasis"><em>RIGHT</em></span> solution</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rewards</strong></span>: <span class="emphasis"><em>R(s)</em></span>, <span class="emphasis"><em>R(s,a)</em></span>, <span class="emphasis"><em>R(s,a,s1)</em></span> Rewards tell us the usefulness of entering into a state<p>
<span class="emphasis"><em>R(s)</em></span>: Reward for entering a state <span class="emphasis"><em>s</em></span>
</p><p>
<span class="emphasis"><em>R(s, a)</em></span>: Reward for the opening of a state <span class="emphasis"><em>s</em></span> for an action <span class="emphasis"><em>a</em></span>
</p><p>
<span class="emphasis"><em>R(s, a,s1)</em></span>: Reward for the opening of a state <span class="emphasis"><em>s1</em></span> for an action <span class="emphasis"><em>a</em></span> given that you were in state <span class="emphasis"><em>s</em></span>
</p></li><li class="listitem" style="list-style-type: disc">State, Action, Model and <a id="id1350" class="indexterm"/>Rewards <a id="id1351" class="indexterm"/>make the problem statement of MDP</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Policy</strong></span>: It is a solution for a problem; it says what action should be taken given a state:<p>
<span class="emphasis"><em>π(s)</em></span> --&gt; <span class="emphasis"><em>a</em></span>
</p></li></ul></div></div><div class="section" title="Basic RL model – agent-environment interface"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec117"/>Basic RL model – agent-environment interface</h3></div></div></div><p>As we have discovered, an RL problem<a id="id1352" class="indexterm"/> is a straightforward way of learning from interaction to achieve a goal. Agent is the learner or decision-maker, and it interacts with the environment, and that is everything outside in this environment gives rise to rewards. The thing it interacts with, comprising everything outside the agent, is called the environment. The complete specification of an environment is defined as a task—a single instance of Reinforcement Learning problem.</p><p>The following model depicts the agent-environment interface:</p><div class="mediaobject"><img src="graphics/B03980_12_15.jpg" alt="Basic RL model – agent-environment interface"/></div><p>An environment <a id="id1353" class="indexterm"/>model here means a context where an agent uses anything to predict the behavior of the environment for a given action. This environment model produces a prediction of the next state and the reward, given the action and the current state. There will be multiple next state rewards possible in case the model is stochastic. Again, these models can be distributed or sample-based. The distributed models identify all the potential probabilities, while a sample model produces the probability given that sample.</p><p>Finally, the goals of Reinforcement Learning can be defined as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In an environment where every action taken results in a new situation, RL is about how to take actions. The following can be the actions:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Define a policy that maps the action and the resultant situation</li><li class="listitem" style="list-style-type: disc">Identify the policy that results in highest rewards being given</li></ul></div></li></ul></div><p>Steps in Reinforcement Learning are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The agent observes the input state.</li><li class="listitem">By applying the policy that is a decision-making function, an action is identified.</li><li class="listitem">The action is executed that results in a state change.</li><li class="listitem">As a consequence of this action, the agent receives a significant reward from the environment.</li><li class="listitem">The <a id="id1354" class="indexterm"/>details of the reward, given the change in the state, are recorded.</li></ol></div></div><div class="section" title="Delayed rewards"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec118"/>Delayed rewards</h3></div></div></div><p>One of the aspects that<a id="id1355" class="indexterm"/> differentiate Reinforcement Learning from supervised learning is <span class="emphasis"><em>rewards</em></span>. In this section, we will explore what delayed rewards mean. As we know, every action that results in a particular state change results in a reward. The realization of this reward in some cases is not immediate. Let's look at an example a chess game. Let's assume it took 65 steps to end a chess game and only at the end of 65 steps or moves we get to know if we have won the game or lost it. Which of the 65 steps or moves were the cause of the success or failure is what is complex here. So, the reward is not known until the end of the game or the sequence of actions. Technically, we are looking at identifying which sequence of actions resulted in gaining the reward that was seen. This process is called <a id="id1356" class="indexterm"/>
<span class="strong"><strong>Temporal Credit Assignment</strong></span>.</p><p>Now, in this journey of achieving the ultimate reward that is a success (+1) or failure (-1), every step or move or action would fetch a reward. Let's assume every step in solution 1 of the grid world problem fetches a reward of -0.4. The collective rewards that take to success or failure will determine long-term reward.</p></div><div class="section" title="The policy"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec119"/>The policy</h3></div></div></div><p>An optimal policy<a id="id1357" class="indexterm"/> is a policy or solution that maximizes the expected long-term reward and can be represented by the following formula:</p><div class="mediaobject"><img src="graphics/B03980_12_26.jpg" alt="The policy"/></div><p>Now, let's measure the utility of a particular state(<span class="emphasis"><em>s</em></span>) that depends on the policy (<span class="emphasis"><em>π</em></span>):</p><div class="mediaobject"><img src="graphics/B03980_12_27.jpg" alt="The policy"/></div><p>A reward to enter a state(<span class="emphasis"><em>s</em></span>) (this is an immediate benefit) is not equal to the utility of that state (this is a long-term benefit of entering the state).</p><p>Now, we can define the optimal policy using the utility of the state value:</p><div class="mediaobject"><img src="graphics/B03980_12_28.jpg" alt="The policy"/></div><p>Now, if we have to define the utility of being in a state(<span class="emphasis"><em>s</em></span>), it is equal to the reward for getting into that state, discounting the reward that we get from that point on:</p><div class="mediaobject"><img src="graphics/B03980_12_29.jpg" alt="The policy"/></div><p>This is called <a id="id1358" class="indexterm"/>
<span class="strong"><strong>Bellman Equation</strong></span>.</p><p>
<span class="emphasis"><em>V*</em></span> is a value function<a id="id1359" class="indexterm"/> for a policy, and the following is the Bellman optimality equation that expresses the fact that the value of a state with an optimal policy is the same as the best expected return from the best action for that state:</p><div class="mediaobject"><img src="graphics/B03980_12_16.jpg" alt="The policy"/></div></div></div><div class="section" title="Reinforcement Learning – key features"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec142"/>Reinforcement Learning – key features</h2></div></div></div><p>Reinforcement Learning<a id="id1360" class="indexterm"/> is not a set of techniques but is a set of problems that focuses on what the task is as, against how the task should be addressed.</p><p>Reinforcement Learning is considered as a tool for machines to learn using the rewards and punishments that are more trial-and-error driven.</p><p>Reinforcement Learning employs evaluative feedback. Evaluative feedback measures how effective the action taken is as against measuring the action if it is best or worst. (Note that supervised learning is more of an instructive learning and determines the correctness of an action irrespective of the action being executed.)</p><p>The tasks in Reinforcement <a id="id1361" class="indexterm"/>Learning are more of related tasks. Associative tasks are dependent on the situation where actions that suit best to the given situation are identified and executed. Non-associative tasks are those that are independent of the particular situation, and the learner finds the best action when the task is stationary.</p></div></div></div>
<div class="section" title="Reinforcement learning solution methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec57"/>Reinforcement learning solution methods</h1></div></div></div><p>In this section, we will discuss<a id="id1362" class="indexterm"/> in detail some of the methods<a id="id1363" class="indexterm"/> to solve Reinforcement Learning problems. Specifically, dynamic programming (DP), Monte Carlo method, and temporal-difference (TD) learning. These methods address the problem of delayed rewards as well.</p><div class="section" title="Dynamic Programming (DP)"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec143"/>Dynamic Programming (DP)</h2></div></div></div><p>DP is a set of algorithms that are <a id="id1364" class="indexterm"/>used to<a id="id1365" class="indexterm"/> compute optimal policies given a model of environment like Markov Decision Process. Dynamic programming models are both computationally expensive and assume perfect models; hence, they have low adoption or utility. Conceptually, DP is a basis for many algorithms or methods used in the following sections:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Evaluating the policy</strong></span>: A policy can be assessed by computing the value function of the policy in an iterative manner. Computing value function for a policy helps find better policies.</li><li class="listitem"><span class="strong"><strong>Improving the policy</strong></span>: Policy improvement is a process of computing the revised policy using its value function information.</li><li class="listitem"><span class="strong"><strong>Value iteration and Policy Iteration</strong></span>: Policy evaluation and improvement together derive value and policy iteration. These are two of the most popular DP methods that are used to compute the optimal policies and value functions given complete knowledge of the MDPs.</li></ol></div><p>The following algorithm depicts the iteration policy process:</p><div class="mediaobject"><img src="graphics/B03980_12_17.jpg" alt="Dynamic Programming (DP)"/></div><p>Value iteration<a id="id1366" class="indexterm"/> combines <a id="id1367" class="indexterm"/>a solid policy improvement and process evaluation. The following are the steps involved:</p><div class="mediaobject"><img src="graphics/B03980_12_18.jpg" alt="Dynamic Programming (DP)"/></div><div class="section" title="Generalized Policy Iteration (GPI)"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec120"/>Generalized Policy Iteration (GPI)</h3></div></div></div><p>GPI <a id="id1368" class="indexterm"/>is a way of categorizing Dynamic Programming (DP) methods. GPI involves interaction between two processes—one around the approximate policy, and the other around the approximate value.</p><p>In the first case, the process picks the policy as it is and performs policy evaluation to identify the true or exact value function associated with the policy. The other process picks the value function as the input and uses that to change the policy such that it improves the policy, which is its total reward. If you observe, each process changes the basis for the other process, and they work in conjunction to find a joint solution that results in an optimal policy and value function.</p></div></div><div class="section" title="Monte Carlo methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec144"/>Monte Carlo methods</h2></div></div></div><p>Monte Carlo methods in<a id="id1369" class="indexterm"/> Reinforcement<a id="id1370" class="indexterm"/> Learning learn policies and values from experience as samples. Monte Carlo methods have additional advantages over Dynamic Programming methods because of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learning optimal behavior happens directly from the interactions with the environment without any model that simulates model dynamics.</li><li class="listitem" style="list-style-type: disc">These methods can be used on simulated data or sample models; this feature becomes paramount in real-world applications.</li><li class="listitem" style="list-style-type: disc">With Monte Carlo methods, we can easily focus on smaller sets of states, and we can explore a region of interest without necessarily going into complete state set.</li><li class="listitem" style="list-style-type: disc">Monte Carlo methods are least impacted for any violation of Markov's property because the estimation for the value is not updated using any of the successor states. This also means that they do not bootstrap.</li></ul></div><p>Monte Carlo <a id="id1371" class="indexterm"/>methods are designed by the <span class="strong"><strong>Generalized Policy Iteration</strong></span> (<span class="strong"><strong>GPI</strong></span>) method. These methods provide an <a id="id1372" class="indexterm"/>alternative way of evaluating the policies. For each state, instead of independently computing the value, an average value of the returns for starting at that state is taken, and this can be a good approximation of the value of that state. The focus is to apply action-value functions to improve the policies as this does not require environment's transition changes. Monte Carlo methods mix policy evaluation and improvement methods and can be implemented on a step-by-step basis.</p><p>How much of exploration is good enough? This is a crucial question to answer in Monte Carlo methods. It is not sufficient to select actions that are best based on their value; it is also important to know how much of this is contributing to ending reward.</p><p>Two methods can be used <a id="id1373" class="indexterm"/>in this case—<span class="strong"><strong>on-policy</strong></span> or <span class="strong"><strong>off-policy</strong></span>
<a id="id1374" class="indexterm"/> methods. In an on-policy method, the agent is responsible for finding the optimal policy using exploration technique; and in off-policy methods, agent exploration is not central, but along with it learns a deterministic optimal policy that need not be related to the policy followed. In short, off-policy learning methods are all about learning behavior through behavior.</p></div><div class="section" title="Temporal difference (TD) learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec145"/>Temporal difference (TD) learning</h2></div></div></div><p>TD learning is one of<a id="id1375" class="indexterm"/> the unique <a id="id1376" class="indexterm"/>techniques of Reinforcement Learning. Temporal difference learning is a combination of Monte Carlo methods and dynamic programming methods. The most discussed technique in Reinforcement Learning is the relationship between temporal difference (TD), dynamic programming (DP), and Monte Carlo methods:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Evaluate a policy that includes estimating the value function <span class="emphasis"><em>V</em></span>π for a given policy <span class="emphasis"><em>π</em></span>.</li><li class="listitem">Select an optimal policy. For policy selection, all the DP, TD, and Monte Carlo methods use a variation of generalized policy iteration (GPI). Hence, the difference in these three methods is nothing but these variations in GPI.</li></ol></div><p>TD methods follow bootstrapping technique to derive an estimate; they fall back on the successor states and similar estimates.</p><p>Let's now look at some advantages of TD over DP and Monte Carlo methods. We will cover this in brief and without delving into too much of complexities. Following are the key benefits:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">TD methods<a id="id1377" class="indexterm"/> do not require the model of the environment and probability <a id="id1378" class="indexterm"/>distributions of the next states and rewards</li><li class="listitem" style="list-style-type: disc">TD methods can easily and elegantly run in an online and incremental manner</li></ul></div><div class="section" title="Sarsa - on-Policy TD"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec121"/>Sarsa - on-Policy TD</h3></div></div></div><p>Let's look at using <a id="id1379" class="indexterm"/>TD methods<a id="id1380" class="indexterm"/> for the control problems. We will continue to use GPI techniques, but now in conjunction with TD methods for the evaluation and prediction. While we need to have a balance between exploration and exploitation options, we have the option to choose on-policy or off-policy learning methods in here. We will stick to the on-policy method:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Learn the action-value function in relation to the state-value function. We will define <span class="emphasis"><em>Q</em></span><sup>π</sup><span class="emphasis"><em>(s, a)</em></span> for the policy <span class="emphasis"><em>π</em></span>:<div class="mediaobject"><img src="graphics/B03980_12_19.jpg" alt="Sarsa - on-Policy TD"/></div></li><li class="listitem">Learn the value of transition from one state-action pair to another state-action pair. This is computed iteratively as follows:<div class="mediaobject"><img src="graphics/B03980_12_20.jpg" alt="Sarsa - on-Policy TD"/></div></li></ol></div><p>This is defined as a Sarsa prediction method and is on-policy as the agent uses the identified policy to do this.</p><p>The Sarsa algorithm is stated as follows:</p><div class="mediaobject"><img src="graphics/B03980_12_21.jpg" alt="Sarsa - on-Policy TD"/></div></div></div><div class="section" title="Q-Learning – off-Policy TD"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec146"/>Q-Learning – off-Policy TD</h2></div></div></div><p>The<a id="id1381" class="indexterm"/> Q-Learning technique<a id="id1382" class="indexterm"/> that employs the off-policy learning method is one of the groundbreaking strategies of TD. This control algorithm called Q-learning (Watkins, 1989) in a simple form is defined as follows:</p><div class="mediaobject"><img src="graphics/B03980_12_22.jpg" alt="Q-Learning – off-Policy TD"/></div><p>We can see the optimal action-value function <span class="emphasis"><em>Q*</em></span> is directly approximated using the learned action-value function <span class="emphasis"><em>Q</em></span> irrespective of the policy that it follows. This makes it an off-policy method.</p><p>There is still a small effect on the policy seen as the policy value functions are used and updated. Moreover, updates to all the pairs diligently mark convergence.</p><p>Based on this understanding, the Q-learning algorithm can be depicted as follows:</p><div class="mediaobject"><img src="graphics/B03980_12_23.jpg" alt="Q-Learning – off-Policy TD"/></div></div><div class="section" title="Actor-critic methods (on-policy)"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec147"/>Actor-critic methods (on-policy)</h2></div></div></div><p>Actor-critic methods are <a id="id1383" class="indexterm"/>temporal difference <a id="id1384" class="indexterm"/>learning methods that ensure policy and value independence using a separate memory structure. In this case, the policy structure is called as an <span class="emphasis"><em>actor,</em></span> and the value structure is called as a <span class="emphasis"><em>critic</em></span>. The name critic comes from the fact that it criticizes the value of the policy. Since this critic always criticizes the value of the policy, it is also called the TD error. The following screenshot shows the actor-critic method flow:</p><div class="mediaobject"><img src="graphics/B03980_12_24.jpg" alt="Actor-critic methods (on-policy)"/></div></div><div class="section" title="R Learning (Off-policy)"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec148"/>R Learning (Off-policy)</h2></div></div></div><p>R-learning <a id="id1385" class="indexterm"/>is an advanced <a id="id1386" class="indexterm"/>Reinforcement Learning technique that is used in cases where there are no discounts with definitive and finite returns. The algorithm is as follows:</p><div class="mediaobject"><img src="graphics/B03980_12_25.jpg" alt="R Learning (Off-policy)"/></div></div><div class="section" title="Implementing Reinforcement Learning algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec15222000"/>Implementing Reinforcement Learning algorithms</h2></div></div></div><p>Refer to the source code provided for this chapter to implement Reinforcement learning algorithms. (Source code path <code class="literal">.../chapter12/...</code> under each of the folder for the technology.)</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec15222"/>Using Mahout</h3></div></div></div><p>Refer to the folder <code class="literal">.../mahout/chapter12/rlexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec15895"/>Using R</h3></div></div></div><p>Refer to the folder <code class="literal">.../r/chapter12/rlexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec158958"/>Using Spark</h3></div></div></div><p>Refer to the folder <code class="literal">.../spark/chapter12/rlexample/</code>.</p></div><div class="section" title="Using Python (Scikit-learn)"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec15888958"/>Using Python (Scikit-learn)</h3></div></div></div><p>Refer to the folder <code class="literal">.../python-scikit-learn/chapter12/rlexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl2sec1588988958"/>Using Julia</h3></div></div></div><p>Refer to the folder <code class="literal">.../julia/chapter12/rlexample/</code>.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec58"/>Summary</h1></div></div></div><p>In this chapter, we explored a new learning technique called Reinforcement Learning. We saw how this was different from traditional supervised and unsupervised learning techniques. The goal of Reinforcement Learning is decision making and at the heart of it is MDP. We explored the elements of MDP and learned about it using an example. We then covered some fundamental Reinforcement Learning techniques that are on-policy and off-policy, and some of them are indirect and direct methods of learning. We covered dynamic programming (DP) methods, Monte Carlo methods, and some key temporal difference (TD) methods like Q-learning, Sarsa, R-learning, and actor-critic methods. Finally, we had hands-on implementations for some of these algorithms using our standard technology stack identified for this book. In the next chapter, we will cover ensemble learning methods.</p></div></body></html>