<html><head></head><body>
		<div id="_idContainer086">
			<h1 class="chapter-number"><a id="_idTextAnchor085"/>7</h1>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor086"/>Feature Engineering for Numerical and Image Data</h1>
			<p>In most cases, when we design large-scale machine learning systems, the types of data we get require more processing than just visualization. This visualization is only for the design and development of machine learning systems. During deployment, we can monitor the data, as we discussed in the previous chapters, but we need to make sure that we use optimized data <span class="No-Break">for inference.</span></p>
			<p>Therefore, in this chapter, we’ll focus on feature engineering – finding the right features that describe our data closer to the problem domain rather than closer to the data itself. Feature engineering is a process where we extract and transform variables from raw data so that we can use them for predictions, classifications, and other machine learning tasks. The goal of feature engineering is to analyze and prepare the data for different machine learning tasks, such as making predictions <span class="No-Break">or classifications.</span></p>
			<p>In this chapter, we’ll focus on the feature engineering process for numerical and image data. We’ll start by going through the typical methods, such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), which we used previously for visualization. Then, we’ll cover more advanced methods, such as <strong class="bold">t-student distribution stochastic network embedding</strong> (<strong class="bold">t-SNE</strong>) and <strong class="bold">independent component analysis</strong> (<strong class="bold">ICA</strong>). What we’ll end up with is the use of autoencoders as a dimensionality reduction technique for both numerical and <span class="No-Break">image data.</span></p>
			<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Feature engineering <span class="No-Break">process fundamentals</span></li>
				<li>PCA and <span class="No-Break">similar methods</span></li>
				<li>Autoencoders for numerical and <span class="No-Break">image data</span></li>
			</ul>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor087"/>Feature engineering</h1>
			<p>Feature engineering<a id="_idIndexMarker253"/> is the process of transforming raw data into numerical values that can be used in machine learning algorithms. For example, we can transform raw data about software defects (for example, their description, the characteristics of the module they come from, and so on) into a table of numerical values that we can use for machine learning. The raw numerical values, as we saw in the previous chapter, are the result of quantifying entities that we use as sources of data. They are the results of applying measurement instruments to the data. Therefore, by definition, they are closer to the problem domain rather than the <span class="No-Break">solution domain.</span></p>
			<p>The features, on the other hand, quantify the raw data and contain only the information that is important for the machine learning task at hand. We use these features to make sure that we find the patterns in the data during training that we can then use during deployment. If we look at this process from the perspective of measurement theory, this process changes the abstraction level of the data. If we look at this process from a statistical perspective, this is the process of removing noise and reducing the dimensions of <span class="No-Break">the data.</span></p>
			<p>In this chapter, we’ll focus on the process of reducing the dimensions of the data and denoising the image data using advanced methods such <span class="No-Break">as autoencoders.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> presents where feature extraction is placed in a typical machine learning pipeline. This pipeline was presented in <a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="Figure 7.1 – Feature engineering in a typical machine learning pipeline" src="image/B19548_07_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Feature engineering in a typical machine learning pipeline</p>
			<p>This<a id="_idIndexMarker254"/> figure shows that the features are as close to the clean and validated data as possible, so we need to rely on the techniques from the previous chapters to visualize the data and reduce the noise. The next activity, after feature engineering, is modeling the data, as presented in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>. This figure shows a somewhat simplified view of the entire pipeline. This was also presented in <a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer079">
					<img alt="Figure 7.2 – A typical machine learning pipeline. A somewhat simplified view from Chapter 2" src="image/B19548_07_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – A typical machine learning pipeline. A somewhat simplified view from <a href="B19548_02.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a></p>
			<p>We <a id="_idIndexMarker255"/>covered modeling previously, so let’s dive deeper into the feature extraction process. Since numerical and image data are somewhat similar from this perspective, we’ll discuss them together in this chapter. Text data is different and therefore we have devoted the next chapter <span class="No-Break">to it.</span></p>
			<p>My first best practice in this chapter, however, is related to the link between feature extraction <span class="No-Break">and models.</span></p>
			<p class="callout-heading">Best practice #39</p>
			<p class="callout">Use feature engineering techniques if the data is complex, but the task is simple – for example, creating a <span class="No-Break">classification model.</span></p>
			<p>If the data is complex and the task is complex, try to use complex but capable models, such as the transformer models presented later in this book. An example of such a task can be code completion when the model finished creating a piece of a program that a programmer started to write. Simplifying complex data for simpler models allows us to increase the explainability of the trained models because we, as AI engineers, are more involved in the process through <span class="No-Break">data wrangling.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor088"/>Feature engineering for numerical data</h1>
			<p>We’ll introduce feature engineering for numerical data by using the same technique that we used previously but for visualizing data – <span class="No-Break">PCA.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor089"/>PCA</h2>
			<p>PCA is <a id="_idIndexMarker256"/>used to transform a set of variables into <a id="_idIndexMarker257"/>components that are supposed to be independent of one another. The first component should explain the variability of the data or be correlated with most of the variables. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> illustrates such <span class="No-Break">a transformation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer080">
					<img alt="Figure 7.3 – Graphical illustration of the PCA transformation from two dimensions to two dimensions" src="image/B19548_07_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Graphical illustration of the PCA transformation from two dimensions to two dimensions</p>
			<p>This figure contains two axes – the blue ones, which are the original coordinates, and the orange ones, which are the imaginary axes and provide the coordinates for the principal components. The transformation does not change the values of the <em class="italic">x</em> and <em class="italic">y</em> axes and instead finds such a transformation that the axes align with the data points. Here, we can see that the transformed <em class="italic">Y</em> axis aligns better with the data points than the original <span class="No-Break"><em class="italic">Y</em></span><span class="No-Break"> axis.</span></p>
			<p>Now, let’s execute a bit of code that can read the data and make such a PCA transformation. In<a id="_idIndexMarker258"/> this example, the data has<a id="_idIndexMarker259"/> six dimensions – that is, <span class="No-Break">six variables:</span></p>
			<pre class="source-code">
# read the file with data using openpyxl
import pandas as pd
# we read the data from the excel file,
# which is the defect data from the ant 1.3 system
dfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.
              xlsx',sheet_name='ant_1_3', index_col=0)
dfDataAnt13</pre>			<p>The preceding code fragment reads the data and shows that it has six dimensions. Now, let’s create the PCA transformation. First, we must remove the dependent variable in our dataset – <span class="No-Break"><strong class="source-inline">Defect</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# let's remove the defect column, as this is the one that
# we could potentially predict
dfDataAnt13Pred = dfDataAnt13.drop(['Defect'], axis = 1)</pre>			<p>Then, we must import the PCA transformation and execute it. We want to find a transformation from the five variables (six minus the <strong class="source-inline">Defect</strong> variable) to three dimensions. The number of dimensions is completely arbitrary, but since we used two in the previous chapters, let’s use more <span class="No-Break">this time:</span></p>
			<pre class="source-code">
# now, let's import PCA and find a few components
from sklearn.decomposition import PCA
# previously, we used 2 components, now, let's go with
# three
pca = PCA(n_components=3)
# now, the transformation to the new components
dfDataAnt13PCA = pca.fit_transform(dfDataAnt13Pred)
# and printing the resulting array
# or at least the three first elements
dfDataAnt13PCA[:3]</pre>			<p>The<a id="_idIndexMarker260"/> resulting DataFrame – <strong class="source-inline">dfDataAnt13PCA</strong> – contains the values of the transformed variables. They are as independent from one another as possible (<span class="No-Break">linearly independent).</span></p>
			<p>I would like to<a id="_idIndexMarker261"/> emphasize the general scheme of how we work with this kind of data transformation because that is a relatively standard way of <span class="No-Break">doing things.</span></p>
			<p>First, we instantiate the transformation module and provide the arguments. In most cases, the arguments are plenty, but there is one, <strong class="source-inline">n_components</strong>, that describes how many components we want <span class="No-Break">to have.</span></p>
			<p>Second, we use the <strong class="source-inline">fit_transform()</strong> function to train the classifier and transform it into these components. We use these two operations together, simply because these transformations are data-specific. There is no need to train the transformation on one data and apply it to <span class="No-Break">another one.</span></p>
			<p>What we can also do with PCA, which we cannot do with other types of transformations, is check how much variability each component explains – that is, how well the components align with the data. We can do this with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# and let's visualize that using the seaborn library
import seaborn as sns
sns.set(rc={"figure.figsize":(8, 8)})
sns.set_style("white")
sns.set_palette('rocket')
sns.barplot(x=['PC 1', 'PC 2', 'PC 3'], y=pca.explained_variance_ratio_)</pre>			<p>This code <a id="_idIndexMarker262"/>fragment results in the diagram<a id="_idIndexMarker263"/> presented in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="Figure 7.4 – Variability explained by the principal components" src="image/B19548_07_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Variability explained by the principal components</p>
			<p>This figure shows that the first component is the most important one – that is, it explains the largest amount of variability. This variability can be seen as the amount of information that the data contains. In the case of this dataset, the first component explains about 80% of the variability and the second one almost 20%. This means that our dataset has one dominating dimension and some dispersion of the data along a second dimension. The third dimension is <span class="No-Break">almost non-existent.</span></p>
			<p>This is where my next best practice <span class="No-Break">comes in.</span></p>
			<p class="callout-heading">Best practice #40</p>
			<p class="callout">Use PCA if the data is somehow linearly separable and on <span class="No-Break">similar scales.</span></p>
			<p>If the data is linear, or multilinear, PCA makes a large difference for training the model. However, if the data is not linear, use a more complex model, such <span class="No-Break">as t-SNE.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor090"/>t-SNE</h2>
			<p>As a <a id="_idIndexMarker264"/>transformation, PCA works well when <a id="_idIndexMarker265"/>data is linearly separable to some extent. In practice, this means that the coordinate system can be positioned in such a way that most of the data is on one of its axes. However, not all data is like that. One example of data that is not like that is data that can be visualized as a circle – it is equally distributed along <span class="No-Break">both axes.</span></p>
			<p>To reduce the dimensions of non-linear data, we can use another technique – t-SNE. This kind of dimensionality reduction technique is based on extracting the activation values of a neural network, which is trained to fit the <span class="No-Break">input data.</span></p>
			<p>The following code fragment creates such a t-SNE transformation of the data. It follows the same schema that was described for the PCA and it also reduces the dimensions <span class="No-Break">to three:</span></p>
			<pre class="source-code">
# for t-SNE, we use the same data as we used previously
# i.e., the predictor dfDataAnt13Pred
from sklearn.manifold import TSNE
# we create the t-sne transformation with three components
# just like we did with the PCA
tsne = TSNE(n_components = 3)
# we fit and transform the data
dfDataAnt13TSNE = tsne.fit_transform(dfDataAnt13Pred)
# and print the three first rows
dfDataAnt13TSNE[:3]</pre>			<p>The resulting DataFrame – <strong class="source-inline">dfDataAnt13TSNE</strong> – contains the transformed data. Unfortunately, the t-SNE transformation does not allow us to get the value of the explained<a id="_idIndexMarker266"/> variability, simply because this<a id="_idIndexMarker267"/> concept does not exist for such a transformation. However, we can visualize it. The following figure presents a 3D projection of the <span class="No-Break">three components:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="Figure 7.5 – Visualization of the t-SNE components. Green dots represent defect-free components and red dots represent components with defects" src="image/B19548_07_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Visualization of the t-SNE components. Green dots represent defect-free components and red dots represent components with defects</p>
			<p>Here is my next best practice for <span class="No-Break">this chapter.</span></p>
			<p class="callout-heading">Best practice #41</p>
			<p class="callout">Use t-SNE if you do not know the properties of the data and the dataset is large (more than 1,000 <span class="No-Break">data points).</span></p>
			<p>t-SNE is a <a id="_idIndexMarker268"/>very good and robust transformation. It <a id="_idIndexMarker269"/>works particularly well for large datasets – that is, those that consist of hundreds of data points. One of the challenges, however, is that there is no interpretation of the components that t-SNE delivers. We should also know that the best results from t-SNE require hyperparameters to be <span class="No-Break">tuned carefully.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor091"/>ICA</h2>
			<p>We can use <a id="_idIndexMarker270"/>another kind of transformation here – ICA. This <a id="_idIndexMarker271"/>transformation works in such a way that it finds the least correlated data points and separates them. It’s been historically used in the medical domain to remove disturbances and artifacts from high-frequency <strong class="bold">electroencephalography</strong> (<strong class="bold">EEG</strong>) signals. An <a id="_idIndexMarker272"/>example of such a disturbance is the 50 - Hz electrical <span class="No-Break">power signal.</span></p>
			<p>However, it can be used for any kind of data. The following code fragment illustrates how ICA can be used for the same dataset that we used in the <span class="No-Break">previous transformations:</span></p>
			<pre class="source-code">
# we import the package
from sklearn.decomposition import FastICA
# instantiate the ICA
ica = FastICA(n_components=3)
# transform the data
dfDataAnt13ICA = ica.fit_transform(dfDataAnt13Pred)
# and check the first three rows
dfDataAnt13ICA[:3]</pre>			<p>ICA needs to<a id="_idIndexMarker273"/> result in fewer components than the original data, although we only used three in the preceding code fragment. The visualization of these components is presented<a id="_idIndexMarker274"/> in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="Figure 7.6 – Visualization of the dataset transformed using ICA" src="image/B19548_07_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Visualization of the dataset transformed using ICA</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em>, green components are the ones without defects and the red ones <span class="No-Break">contain defects.</span></p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor092"/>Locally linear embedding</h2>
			<p>A technique that is somewhat in between t-SNE and PCA (or ICA) is known as <strong class="bold">locally linear embedding</strong> (<strong class="bold">LLE</strong>). This <a id="_idIndexMarker275"/>technique assumes that neighboring nodes are placed close to one another <a id="_idIndexMarker276"/>on some kind of virtual plane. The algorithm trains a neural network in such a way that it preserves the distances between the <span class="No-Break">neighboring nodes.</span></p>
			<p>The following code fragment illustrates how to use the <span class="No-Break">LLE technique:</span></p>
			<pre class="source-code">
from sklearn.manifold import LocallyLinearEmbedding
# instantiate the classifier
lle = LocallyLinearEmbedding(n_components=3)
# transform the data
dfDataAnt13LLE = lle.fit_transform(dfDataAnt13Pred)
# print the three first rows
dfDataAnt13LLE[:3]</pre>			<p>This fragment results in a similar DataFrame to the previous algorithms. Here is <span class="No-Break">the visualization:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="Figure 7.7 – Visualization of the LLE components" src="image/B19548_07_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Visualization of the LLE components</p>
			<p>All the<a id="_idIndexMarker277"/> techniques<a id="_idIndexMarker278"/> we’ve discussed so far are flexible and allow us to indicate how many components we need in the transformed data. However, sometimes, the problem is that we do not know how many components <span class="No-Break">we need.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor093"/>Linear discriminant analysis</h2>
			<p><strong class="bold">Linear discriminant analysis</strong> (<strong class="bold">LDA</strong>) is a<a id="_idIndexMarker279"/> technique that results in as many components <a id="_idIndexMarker280"/>as we have in our dataset. This means that the number of columns in our dataset is the same as the number of components that LDA provides. This, in turn, means that we need to define one of the variables as the dependent one for the algorithm <span class="No-Break">to work.</span></p>
			<p>The LDA algorithm <a id="_idIndexMarker281"/>finds a <a id="_idIndexMarker282"/>projection of the dataset on a lower dimensional space in such a way that it separates the data in the classes of the dependent variable. Therefore, we need one. The following code fragment illustrates the use of LDA on <span class="No-Break">our dataset:</span></p>
			<pre class="source-code">
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# create the classifier
# please note that we can only use one component, because
# we have only one predicted variable
lda = LinearDiscriminantAnalysis(n_components=1)
# fit to the data
# please note that this transformation requires the predicted
# variable too
dfDataAnt13LDA = lda.fit(dfDataAnt13Pred, dfDataAnt13.Defect).transform(dfDataAnt13Pred)
# print the transformed data
dfDataAnt13LDA[:3]</pre>			<p>The resulting DataFrame contains only one component as we have only one dependent variable in <span class="No-Break">our dataset.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor094"/>Autoencoders</h2>
			<p>In <a id="_idIndexMarker283"/>recent years, a new technique for feature extraction has been gaining popularity – autoencoders. Autoencoders <a id="_idIndexMarker284"/>are special kinds of neural networks that are designed to transform data from one type of data into another. Usually, they are used to recreate the input data in a slightly modified form. For example, they can be used to remove noise from images or change images to use different styles <span class="No-Break">of brushes.</span></p>
			<p>Autoencoders are quite generic and can be used for other kinds of data, which we’ll learn about in the remainder of this chapter (for example, for image data). <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.8</em> presents the conceptual model of autoencoders. They consist of two parts – an encoder and a decoder. The role of the encoder is to transform the input data – an image in this example – into an abstract representation. This abstract representation is stored in a specific layer (or layers), which is called the bottleneck. The role of the bottleneck is to store such properties of the input data that allow the decoder to recreate the data. The role of the decoder is to take the abstract representation of the data from the bottleneck layer and re-create the input data as best <span class="No-Break">as possible:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer085">
					<img alt="Figure 7.8 – Conceptual visualization of an autoencoder. Here, the input data is in the form of an image" src="image/B19548_07_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Conceptual visualization of an autoencoder. Here, the input data is in the form of an image</p>
			<p>Since autoencoders are trained to recreate the data as best as possible, the bottleneck values are generally believed to be a good internal representation of the input data. It is such a good representation that it allows us to discriminate between different input <span class="No-Break">data points.</span></p>
			<p>The bottleneck values are also very flexible. As opposed to the techniques presented previously, there is no limit on how many features we can extract. If we need to, we can even extract more features than we have columns in our dataset, although it does not make <span class="No-Break">much sense.</span></p>
			<p>So, let’s <a id="_idIndexMarker285"/>construct a pipeline for <a id="_idIndexMarker286"/>extracting features from an autoencoder that’s designed to learn the representation of the <span class="No-Break">defect data.</span></p>
			<p>The following code fragment illustrates reading the dataset and removing the defect column <span class="No-Break">from it:</span></p>
			<pre class="source-code">
# read the file with data using openpyxl
import pandas as pd
# we read the data from the excel file,
# which is the defect data from the ant 1.3 system
dfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.
              xlsx',sheet_name='ant_1_3',index_col=0)
# let's remove the defect column, as this is the one that we could
# potentially predict
X = dfDataAnt13.drop(['Defect'], axis = 1)
y = dfDataAnt13.Defect</pre>			<p>In addition to the removal of the column, we need to scale the data so that the autoencoder has a good chance of recognizing small patterns in <span class="No-Break">all columns:</span></p>
			<pre class="source-code">
# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)
# scale data
t = MinMaxScaler()
t.fit(X_train)
X_train = t.transform(X_train)
X_test = t.transform(X_test)</pre>			<p>Now, we<a id="_idIndexMarker287"/> can create the encoder part <a id="_idIndexMarker288"/>of our autoencoder, which is shown in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# number of input columns
n_inputs = X.shape[1]
# the first layer - the visible one
visible = Input(shape=(n_inputs,))
# encoder level 1
e = Dense(n_inputs*2)(visible)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# encoder level 2
e = Dense(n_inputs)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)</pre>			<p>The preceding code creates two levels of the autoencoder since our data is quite simple. Now, the interesting part is the bottleneck, which can be created by running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
n_bottleneck = 3
bottleneck = Dense(n_bottleneck)(e)</pre>			<p>In our case, the bottleneck is very narrow – only three neurons – as the dataset is rather small and it is not very complex. In the next part, when we use the autoencoder for images, we will see that the bottleneck can be much wider. The general idea is that wider bottlenecks allow us to capture more complex dependencies in the data. For example, for color images, we need more neurons as we need to capture colors, while for grayscale images, we need <span class="No-Break">narrower bottlenecks.</span></p>
			<p>Finally, we <a id="_idIndexMarker289"/>can create the decoder part of <a id="_idIndexMarker290"/>the autoencoder by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# output layer
output = Dense(n_inputs, activation='linear')(d)</pre>			<p>The last part of the construction process is to put these three parts together – the encoder, the bottleneck, and the decoder. We can use the following code <span class="No-Break">for this:</span></p>
			<pre class="source-code">
# we place both of these into one model
# define autoencoder model
model = Model(inputs=visible, outputs=output)
# compile autoencoder model
model.compile(optimizer='adam', loss='mse')</pre>			<p>At this point, we have constructed our autoencoder. We’ve defined its layers and the bottleneck. Now, the autoencoder must be trained to understand how to represent our data. We can do this using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# we train the autoencoder model
history = model.fit(X_train, X_train,
                    epochs=100,
                    batch_size=16,
                    verbose=2,
                    validation_data=(X_test,X_test))</pre>			<p>Please <a id="_idIndexMarker291"/>note that we use the same data <a id="_idIndexMarker292"/>as input and as validation since we need to train the encoder to re-create the same data as accurately as possible, given the size of the bottleneck. After training the encoder model, we can use it to extract the bottleneck values from the model. We can do this by defining a submodel and using it for <span class="No-Break">input data:</span></p>
			<pre class="source-code">
submodel = Model(model.inputs, model.get_layer("dense_8").output)
# this is the actual feature extraction -
# where we make prediction for the train dataset
# please note that the autoencoder requires a two dimensional array
# so we need to take one datapoint and make it into a two dimensional array
# with only one row
results = submodel.predict(np.array([X_train[0]]))
results[0]</pre>			<p>The outcome of executing this code is a vector of three values – the bottleneck values of <span class="No-Break">the autoencoder.</span></p>
			<p>My next best practice in this chapter is related to the use <span class="No-Break">of autoencoders.</span></p>
			<p class="callout-heading">Best practice #42</p>
			<p class="callout">Use autoencoders for numerical data when the dataset is really large since autoencoders are complex and require a lot of data <span class="No-Break">for training.</span></p>
			<p>Since the <a id="_idIndexMarker293"/>quality of the features is a <a id="_idIndexMarker294"/>function of how well the autoencoder is trained, we need to make sure that the training dataset is large. Therefore, autoencoders are often used for <span class="No-Break">image data.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor095"/>Feature engineering for image data</h1>
			<p>One of the<a id="_idIndexMarker295"/> most prominent feature extraction methods for image data is the use of <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) and <a id="_idIndexMarker296"/>extracting embeddings from these networks. In recent years, a new type of this kind of neural network was introduced – autoencoders. Although we can use autoencoders for all kinds of data, they are particularly well-suited for images. So, let’s construct an autoencoder for the MNIST dataset and extract bottleneck values <span class="No-Break">from it.</span></p>
			<p>First, we need to download the MNIST dataset using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# first, let's read the image data from the Keras library
from tensorflow.keras.datasets import mnist
# and load it with the pre-defined train/test splits
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train/255.0
X_test = X_test/255.0</pre>			<p>Now, we can construct the encoder part by using the following code. Please note that there is one extra layer in the encoder part. The goal of this layer is to transform a two-dimensional image into a one-dimensional input array – to flatten <span class="No-Break">the image:</span></p>
			<pre class="source-code">
# image size is 28 pixels
n_inputs = 28
# the first layer - the visible one
visible = Input(shape=(n_inputs,n_inputs,))
# encoder level 1
e = Flatten(input_shape = (28, 28))(visible)
e = LeakyReLU()(e)
e = Dense(n_inputs*2)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# encoder level 2
e = Dense(n_inputs)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)</pre>			<p>Now, we<a id="_idIndexMarker297"/> can construct our bottleneck. In this case, the bottleneck can be much wider as the images are more complex (and there are more of them) than the array of numerical values of modules, which we used in the <span class="No-Break">previous autoencoder:</span></p>
			<pre class="source-code">
n_bottleneck = 32
bottleneck = Dense(n_bottleneck)(e)</pre>			<p>The decoder part is very similar to the previous example, with one extra layer that re-creates the image from its <span class="No-Break">flat representation:</span></p>
			<pre class="source-code">
# and now, we define the decoder part
# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# output layer
d = Dense(n_inputs*n_inputs, activation='linear')(d)
output = Reshape((28,28))(d)</pre>			<p>Now, we can <a id="_idIndexMarker298"/>compile and train <span class="No-Break">the autoencoder:</span></p>
			<pre class="source-code">
# we place both of these into one model
# define autoencoder model
model = Model(inputs=visible, outputs=output)
# compile autoencoder model
model.compile(optimizer='adam', loss='mse')
# we train the autoencoder model
history = model.fit(X_train, X_train,
                    epochs=100,
                    batch_size=16,
                    verbose=2,
                    validation_data=(X_test,X_test))</pre>			<p>Finally, we can extract the bottleneck values from <span class="No-Break">the model:</span></p>
			<pre class="source-code">
submodel = Model(model.inputs, bottleneck)
# this is the actual feature extraction -
# where we make prediction for the train dataset
# please note that the autoencoder requires a two dimensional array
# so we need to take one datapoint and make it into a two dimensional array
# with only one row
results = submodel.predict(np.array([X_train[0]]))
results[0]</pre>			<p>Now, the<a id="_idIndexMarker299"/> resulting array of values is much larger – it has 32 values, the same number of neurons that we have in <span class="No-Break">our bottleneck.</span></p>
			<p>The number of neurons in the bottleneck is essentially arbitrary. Here’s a best practice for selecting the number <span class="No-Break">of neurons.</span></p>
			<p class="callout-heading">Best practice #43</p>
			<p class="callout">Start with a small number of neurons in the bottleneck – usually one third of the number of columns. If the autoencoder does not learn, increase the <span class="No-Break">number gradually.</span></p>
			<p>There is no specific reason why I chose 1/3rd of the number of columns, just experience. You can start from the opposite direction – make the bottleneck layer as wide as the input – and decrease gradually. However, having the same number of features as the number of columns is not why we use feature extraction in the <span class="No-Break">first place.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor096"/>Summary</h1>
			<p>In this chapter, our focus was on feature extraction techniques. We explored how we can use dimensionality reduction techniques and autoencoders to reduce the number of features in order to make machine learning models more effective. </p>
			<p>However, numerical and image data are only two examples of data. In the next chapter, we continue with the feature engineering methods, but for textual data, which is more common in contemporary <span class="No-Break">software engineering.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor097"/>References</h1>
			<ul>
				<li><em class="italic">Zheng, A. and A. Casari, Feature engineering for machine learning: principles and techniques for data scientists. 2018:  O’Reilly </em><span class="No-Break"><em class="italic">Media, Inc</em></span></li>
				<li><em class="italic">Heaton, J. An empirical analysis of feature engineering for predictive modeling. In SoutheastCon 2016. </em><span class="No-Break"><em class="italic">2016. IEEE.</em></span></li>
				<li><em class="italic">Staron, M. and W. Meding, Software Development Measurement Programs. Springer. https://doi.org/10.1007/978-3-319-91836-5. Vol. 10. </em><span class="No-Break"><em class="italic">2018. 3281333.</em></span></li>
				<li><em class="italic">Abran, A., Software metrics and software metrology. 2010: John Wiley &amp; </em><span class="No-Break"><em class="italic">Sons.</em></span></li>
				<li><em class="italic">Meng, Q., et al. Relational autoencoder for feature extraction. In 2017 International joint conference on neural networks (IJCNN). </em><span class="No-Break"><em class="italic">2017. IEEE.</em></span></li>
				<li><em class="italic">Masci, J., et al. Stacked convolutional auto-encoders for hierarchical feature extraction. In Artificial Neural Networks and Machine Learning, ICANN 2011: 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part I 21. </em><span class="No-Break"><em class="italic">2011. Springer.</em></span></li>
				<li><em class="italic">Rumelhart, D.E., G.E. Hinton, and R.J. Williams, Learning representations by back-propagating errors. nature, 1986. 323(6088): </em><span class="No-Break"><em class="italic">p. 533-536.</em></span></li>
				<li><em class="italic">Mosin, V., et al., Comparing autoencoder-based approaches for anomaly detection in highway driving scenario images. SN Applied Sciences, 2022. 4(12): </em><span class="No-Break"><em class="italic">p. 334.</em></span></li>
			</ul>
		</div>
	</body></html>