- en: Chapter 4. What's in the Image? Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Segmentation is any process that partitions an image into multiple regions
    or segments. These will typically correspond to meaningful regions or objects,
    such as face, car, road, sky, grass, and so on. Segmentation is one of the most
    important stages in a computer vision system. In OpenCV, there is no specific
    module for segmentation, though a number of ready-to-use methods are available
    in other modules (most of them in `imgproc`). In this chapter, we will cover the
    most important and frequently used methods available in the library. In some cases,
    additional processing will have to be added to improve the results or obtain seeds
    (this refers to rough segments that allow an algorithm to perform a complete segmentation).
    In this chapter we will look at the following major segmentation methods: thresholding,
    contours and connected components, flood filling, watershed segmentation, and
    the GrabCut algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thresholding is one of the simplest yet most useful segmentation operations.
    We can safely say that you will end up using some sort of thresholding in almost
    any image-processing application. We consider it a segmentation operation since
    it partitions an image into two regions, typically, an object and its background.
    In OpenCV, thresholding is performed with the function `double threshold(InputArray
    src, OutputArray dst, double thresh, double maxval, int type)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two parameters are the input and output images, respectively. The
    third input parameter is the threshold chosen. The meaning of `maxval` is controlled
    by the type of thresholding we want to perform. The following table shows the
    operation performed for each type:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | dst(x,y) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `THRESH_BINARY` | `maxval` if `src(x,y)` is greater than `thresh` and `0`
    if otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `THRESH_BINARY_INV` | `0` if `src(x,y)` is greater than `thresh` and `maxval`
    if otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `THRESH_TRUNC` | `thresh` if `src(x,y)` is greater than `thresh` and `src(x,y)`
    if otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `THRESH_TOZERO` | `src(x,y)` if `src(x,y)` is greater than `thresh` and `0`
    if otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `THRESH_TOZERO_INV` | `0` if `src(x,y)` is greater than `thresh` and `src(x,y)`
    if otherwise |'
  prefs: []
  type: TYPE_TB
- en: 'While in previous OpenCV books (and the available reference manual) each type
    of thresholding is illustrated with the help of 1D signal plots, our experience
    shows that numbers and gray levels allow you to grasp the concept faster. The
    following table shows the effect of the different threshold types using a single-line
    image as an example input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The special value `THRESH_OTSU` may be combined with the previous values (with
    the OR operator). In such cases, the threshold value is automatically estimated
    by the function (using Otsu's algorithm). This function returns the estimated
    threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Otsu's method obtains a threshold that best separates the background from the
    foreground's pixels (in an interclass/intraclass variance ratio sense). See the
    full explanation and demos at [http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html](http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'While the function described uses a single threshold for the whole image, adaptive
    thresholding estimates a different threshold for each pixel. This produces a better
    result when the input image is less homogeneous (with unevenly illuminated regions,
    for example). The function to perform adaptive thresholding is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This function is similar to the previous one. The parameter `thresholdType`
    must be either `THRESH_BINARY` or `THRESH_BINARY_INV`. This function computes
    a threshold for each pixel by computing a weighted average of pixels in a neighborhood
    minus a constant (`C`). When `thresholdType` is ADAPTIVE_THRESH_MEAN_C, the threshold
    computed is the mean of the neighborhood (that is, all the elements are weighted
    equally).When `thresholdType` is ADAPTIVE_THRESH_GAUSSIAN_C, the pixels in the
    neighborhood are weighted according to a Gaussian function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `thresholding` example shows how to perform thresholding operations
    on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The example in the preceding code creates three windows with the source image,
    which is loaded in grayscale, and the result of thresholding and adaptive thresholding.
    Then, it creates three trackbars: one associated to the thresholding result window
    (to handle the threshold value) and two associated to the adaptive thresholding
    result window (to handle the block''s size and the value of the constant `C`).
    Note that since two callback functions are necessary in this case, and we do not
    want to repeat code, the call to `adaptiveThreshold` is embedded in the function,
    `adaptThreshAndShow`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a call is made to the functions that perform the operations using default
    parameter values. Finally, the `moveWindow` function from `highgui` is used to
    reposition the windows on the screen (otherwise they will be displayed on top
    of each other, and only the third one will be visible). Also, note that the first
    six lines in the function `adaptiveThresholding1` are needed to keep an odd value
    in the parameter `block_size`. The following screenshot shows the output of the
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the thresholding example
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function `inRange(InputArray src, InputArray lowerb, InputArray upperb,
    OutputArray dst)` is also useful for thresholding as it checks whether the pixels
    lie between lower and upper thresholds. Both `lowerb` and `upperb` must be provided
    using Scalar, as in `inRange(src, Scalar(bl,gl,rl), Scalar(bh,gh,rh), tgt);`.
  prefs: []
  type: TYPE_NORMAL
- en: Contours and connected components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contour extraction operations can be considered halfway between feature extraction
    and segmentation, since a binary image is produced in which image contours are
    separated from other homogeneous regions. Contours will typically correspond to
    object boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: While a number of simple methods detect edges in images (for example, the Sobel
    and Laplace filters), the **Canny** method is a robust algorithm for doing this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method uses two thresholds to decide whether a pixel is an edge. In what
    is called a hysteresis procedure, a lower and an upper threshold are used (see
    [http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)).
    Since OpenCV already includes a good example of the Canny edge detector (in `[opencv_source_code]/samples/cpp/edge.cpp`),
    we do not include one here (but see the following `floodFill` example). Instead,
    we will go on to describe other highly useful functions based on detected edges.
  prefs: []
  type: TYPE_NORMAL
- en: To detect straight lines, the Hough transform is a classical method. While the
    Hough transform method is available in OpenCV (the functions `HoughLines` and
    `HoughLinesP`, for example, `[opencv_source_code]/samples/cpp/houghlines.cpp`),
    the more recent **Line Segment Detector** (**LSD**) method is generally a more
    robust one. LSD works by finding alignments of high-gradient magnitude pixels,
    given its alignment tolerance feature. This method has been shown to be more robust
    and faster than the best previous Hough-based detector (Progressive Probabilistic
    Hough Transform).
  prefs: []
  type: TYPE_NORMAL
- en: The LSD method is not available in the 2.4.9 release of OpenCV; although, at
    the time of this writing, it is already available in the code source's repository
    in GitHub. The method will be available in Version 3.0\. A short example (`[opencv_source_code]/samples/cpp/lsd_lines.cpp`)
    in the library covers this functionality. However, we will provide an additional
    example that shows different features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test the latest source code available in GitHub, go to [https://github.com/itseez/opencv](https://github.com/itseez/opencv)
    and download the library code as a ZIP file. Then, unzip it to a local folder
    and follow the same steps described in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Getting Started"), *Getting Started*, to compile and install the library.
  prefs: []
  type: TYPE_NORMAL
- en: The LSD detector is a C++ class. The function `cv::Ptr<LineSegmentDetector>
    cv::createLineSegmentDetector (int _refine=LSD_REFINE_STD, double _scale=0.8,
    double_sigma_scale=0.6, double _quant=2.0, double _ang_th=22.5, double _log_eps=0,
    double _density_th=0.7, int _n_bins=1024)` creates an object of the class and
    returns a pointer to it. Note that several arguments define the detector created.
    The meaning of those parameters requires you to know the underlying algorithm,
    which is out of the scope of this book. Fortunately, the default values will suffice
    for most purposes, so we refer the reader to the reference manual (for Version
    3.0 of the library) for special cases. Having said that, the first parameter scale
    roughly controls the number of lines that are returned. The input image is automatically
    rescaled by this factor. At lower resolutions, fewer lines are detected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `cv::Ptr<>` type is a template class for wrapping pointers. This template
    is available in the 2.x API to facilitate automatic deallocation using reference
    counting. The `cv:: Ptr<>` type is analogous to `std::unique_ptr`.'
  prefs: []
  type: TYPE_NORMAL
- en: Detection itself is accomplished with the method `LineSegmentDetector::detect(const
    InputArray _image, OutputArray _lines, OutputArray width=noArray(), OutputArray
    prec=noArray(), OutputArraynfa=noArray())`. The first parameter is the input image,
    while the `_lines` array will be filled with a (STL) vector of `Vec4i` objects
    that represent the (x, y) location of one end of the line followed by the location
    of the other end. The optional parameters `width`, `prec`, and `noArray` return
    additional information about the lines detected. The first one, `width`, contains
    the estimated line widths. Lines can be drawn with the convenient (yet simple)
    method called `LineSegmentDetector::drawSegments(InputOutputArray _image, InputArray
    lines)`. Lines will be drawn on top of the input, namely, `_image`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `lineSegmentDetector` example shows the detector in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example creates a window with the source image, which is loaded
    in grayscale, and shows the `drawSegments` method. However, it allows you to impose
    a segment length threshold and specify the line colors (`drawSegments` will draw
    all the lines in red). Besides, lines will be drawn with a thickness given by
    the widths estimated by the detector. A trackbar is associated with the main window
    to control the length of the threshold. The following screenshot shows an output
    of the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Contours and connected components](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the lineSegmentDetector example
  prefs: []
  type: TYPE_NORMAL
- en: Circles can be detected using the function `HoughCircles(InputArray image, OutputArray
    circles, int method, double dp, double minDist, double param1=100, double param2=100,
    intminRadius=0, int maxRadius=0)`. The first parameter is a grayscale input image.
    Output parameter circles will be filled with a vector of `Vec3f` objects. Each
    object represents the `(center_x, center_y, radius)` components of a circle. The
    last two parameters represent the minimum and maximum search radii, so they have
    an effect on the number of circles detected. OpenCV already contains a straightforward
    example of this function, `[opencv_source_code]/samples/cpp/houghcircles.cpp`.
    The example detects circles with a radius between 1 and 30 and displays them on
    top of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation algorithms typically form connected components, that is, the regions
    of connected pixels in a binary image. In the following section, we show how to
    obtain connected components and their contours from a binary image. Contours can
    be retrieved using the now classical function, `findContours`. Examples of this
    function are available in the reference manual (also see the `[opencv_source_code]/samples/cpp/contours2.cpp`
    and `[opencv_source_code]/samples/cpp/segment_objects.cpp` examples). Also note
    that in the 3.0 release of OpenCV (and in the code already available in the GitHub
    repository), the class `ShapeDistanceExtractor` allows you to compare the contours
    with the Shape Context descriptor (an example of this is available at `[opencv_source_code]/samples/cpp/shape_example.cpp`)
    and the Hausdorff distance. This class is in a new module of the library called
    `shape`. Shape transformations are also available through the class `ShapeTransformer`
    (example, `[opencv_source_code]/samples/cpp/shape_transformation.cpp`).
  prefs: []
  type: TYPE_NORMAL
- en: The new functions `connectedComponents` and `connectedComponentsWithStats` retrieve
    connected components. These functions will be part of the 3.0 release, and they
    are already available in the GitHub repository. An example of this is included
    in OpenCV that shows how to use the first one, `[opencv_source_code]/samples/cpp/connected_components.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The connected component that labels the functionality was actually removed in
    previous OpenCV 2.4.x versions and has now been added again.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide another example (`connectedComponents`) that shows how to use the
    second function, `int connectedComponentsWithStats(InputArray image, OutputArray
    labels, OutputArray stats, OutputArray centroids, int connectivity=8, intltype=CV_32S)`,
    which provides useful statistics about each connected component. These statistics
    are accessed via `stats(label, column)` where the column can be the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `CC_STAT_LEFT ` | The leftmost (*x*) coordinate that is the inclusive start
    of the bounding box in the horizontal direction |'
  prefs: []
  type: TYPE_TB
- en: '| `CC_STAT_TOP ` | The topmost (y) coordinate that is the inclusive start of
    the bounding box in the vertical direction |'
  prefs: []
  type: TYPE_TB
- en: '| `CC_STAT_WIDTH ` | The horizontal size of the bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| `CC_STAT_HEIGHT ` | The vertical size of the bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| `CC_STAT_AREA ` | The total area (in pixels) of the connected component |'
  prefs: []
  type: TYPE_TB
- en: 'The following is the code for the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example creates a window with an associated trackbar. The trackbar
    controls the threshold to apply to the source image. Inside the `on_trackbar`
    function, a call is made to `connectedComponentsWithStats` using the result of
    the thresholding. This is followed by two sections of the code. The first section
    fills the pixels that correspond to each connected component with a random color.
    The pixels that belong to each component are in `labelImage` (a `labelImage` output
    is also given by the function `connectedComponents`). The second part displays
    a text with the area of each component. This text is positioned at the centroid
    of each component. The following screenshot shows the output of the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Contours and connected components](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The output of the connectedComponents example
  prefs: []
  type: TYPE_NORMAL
- en: Flood fill
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The flood fill operation fills the connected components with a given color.
    Starting from a seed point, the neighboring pixels are colored with a uniform
    color. The neighboring pixels can be within a specified range of the current pixel.
    The flood fill function is `int floodFill(InputOutputArray image, Point seedPoint,
    Scalar newVal, Rect* rect=0, Scalar loDiff=Scalar(), Scalar upDiff=Scalar(),int
    flags=4)`. The parameters `loDiff` and `upDiff` represent the range to check for
    every neighboring pixel (note that 3-channel difference thresholds can be specified).
    The parameter `newVal` is the color to apply to the pixels that are in range.
    The lower part of the parameter `flags` contains the pixel's connectivity value
    to use (`4` or `8`). The upper part defines the mode of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on this mode, the flood fill function will color a neighboring pixel
    in the input image if it is within the specified range (given by `loDiff` and
    `upDiff`) of either the current pixel or if the neighboring pixel is within the
    specified range of the original seed's value. The function can also be called
    with a mask image as the second parameter. If specified, the flood-filling operation
    will not go across non-zero pixels in the mask. Note that the mask should be a
    single-channel 8-bit image that is 2 pixels wider and 2 pixels taller than the
    input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The upper bit of `flags` can be 0 or a combination of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FLOODFILL_FIXED_RANGE`: If set, the difference between the current pixel and
    seed pixel is considered. Otherwise, the difference between neighbor pixels is
    considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLOODFILL_MASK_ONLY`: If set, the function does not change the image (`newVal`
    is ignored) but fills the mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In OpenCV''s flood fill example (`[opencv_source_code]/samples/cpp/ffilldemo.cpp`),
    the mask is used only as an output parameter. In our `floodFill` example, shown
    as the following code, we will use it as an input parameter in order to constrain
    the filling. The idea is to use the output of an edge detector as a mask. This
    should stop the filling process at the edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example reads and displays a color image and then creates four
    trackbars. The first two trackbars control `loDiff` and `upDiffvalues` for the
    `floodFill` function. The other two trackbars control the lower and upper threshold
    parameters for the Canny edge detector. In this example, the user can click anywhere
    on the input image. The click position will be used as a seed point to perform
    a flood fill operation. Actually, upon each click, two calls are made to the `floodFill`
    function. The first one simply fills a region using a random color. The second
    one uses a mask created from the output of the Canny edge detector. Note that
    the `copyMakeBorder` function is necessary to form a 1-pixel wide border around
    the mask. The following screenshot shows the output of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flood fill](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the floodFill example
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output that uses Canny edges (right) has filled in less pixels
    than the standard operation (left).
  prefs: []
  type: TYPE_NORMAL
- en: Watershed segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Watershed is a segmentation method that is known for its efficiency. The method
    essentially starts from user-specified starting (seed) points from which regions
    grow. Assuming that good starting seeds can be provided, the resulting segmentations
    are useful for many purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details and examples about the watershed transform for image segmentation,
    see [http://cmm.ensmp.fr/~beucher/wtshed.html](http://cmm.ensmp.fr/~beucher/wtshed.html).
  prefs: []
  type: TYPE_NORMAL
- en: The function `watershed(InputArray image, InputOutputArray markers)` accepts
    a 3-channel input image and an image called `markers` with the seeds. The latter
    has to be a 32-bit single-channel image. Seeds may be specified in `markers` as
    connected components with positive values (0 cannot be used as a value for seeds).
    As an output argument, each pixel in `markers` will be set to a value of the seed
    components or `-1` at boundaries between the regions. OpenCV includes a watershed
    example (`[opencv_source_code]/samples/cpp/watershed.cpp`) in which the user has
    to draw the seed's regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, the selection of the seed regions is important. Ideally, seeds will
    be selected automatically without user intervention. A typical use of watershed
    is to first threshold the image to separate the object from the background, apply
    the distance transform, and then use the local maxima of the distance transform
    image as seed points for segmentation. However, the first thresholding step is
    critical, as parts of the object may be considered as the background. In this
    case, the object seed region will be too small and segmentation will be poor.
    On the other hand, to perform a watershed segmentation, we need seeds for the
    background too. While we can use points over the corners of the image as seeds,
    this will not be sufficient. In this case, the background seed region is too small.
    If we use those seeds, the object region given by the segmentation will be generally
    much larger than the real object. In our following `watershed` example, a different
    approach is followed that produces better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Watershed` function in the preceding code performs three steps. First,
    a background seed region is obtained by performing a flood fill. The flood fill
    seed is the upper left corner of the image, that is, pixel (0, 0). Next, another
    flood fill is performed to obtain an object''s (hand in the sample image) seed
    region. The seed for this flood fill is taken as the center of the image. Then,
    a seed region image is formed by performing an `OR` operation between the previous
    two flood fill results. The resulting image is used as the seed image for the
    watershed operation. See the output of the example in the following screenshot
    where the seed image is shown at the center of the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Watershed segmentation](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The output of the watershed example
  prefs: []
  type: TYPE_NORMAL
- en: GrabCut
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GrabCut is an excellent iterative background/foreground segmentation algorithm
    that is available since Version 2.1 of OpenCV. GrabCut is especially useful to
    separate objects from the background with minimal additional information (a bounding
    rectangle is sufficient in most cases). However, it is computationally intensive,
    and so it is only appropriate to segment still images.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GrabCut is the underlying algorithm for the Background Removal tool in Microsoft
    Office 2010\. This algorithm was first proposed by researchers at Microsoft Research
    Cambridge. Starting with a user-provided bounding box of the object to segment,
    the algorithm estimates the color distributions of both the target object and
    the background. This estimate is further refined by minimizing an energy function
    in which connected regions that have the same label receive more weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main function is `grabCut(InputArray img, InputOutputArray mask, Rect rect,
    InputOutputArray bgdModel, InputOutputArray fgdModel, int iterCount, int mode=GC_EVAL)`.
    The parameters `bgdModel` and `fgdModel` are only used internally by the function
    (though they have to be declared). The `iterCount` variable is the number of iterations
    to be performed. In our experience, few iterations of the algorithm are required
    to produce good segmentations. The algorithm is aided by a bounding rectangle,
    a mask image, or both. The option chosen is indicated in the `mode` parameter,
    which can be `GC_INIT_WITH_RECT`, `GC_INIT_WITH_MASK`, or an `OR` combination
    of the two. In the former case, `rect` defines the rectangle. Pixels outside the
    rectangle are considered as the obvious background. In the latter case, the mask
    is an 8-bit image in which pixels may have the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GC_BGD`: This defines an obvious background pixel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_FGD`: This defines an obvious foreground (object) pixel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_PR_BGD`: This defines a possible background pixel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GC_PR_FGD`: This defines a possible foreground pixel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image mask is also the output image with the resulting segmentation, which
    is derived using those same previous values. OpenCV includes an example of GrabCut
    (`[opencv_source_code]/samples/cpp/grabcut.cpp`) in which the user can draw a
    bounding rectangle as well as foreground and background pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `grabcut` example uses the algorithm with an initial bounding
    rectangle and then copies the resulting foreground onto another position in the
    same image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example simply uses a fixed rectangle around the coin in the source
    image (see the fifth screenshot in this chapter) and performs the segmentation.
    The `result` image will contain values between 0 (`GC_BGD`) and 3 (`GC_PR_FGD`).
    The ensuing `AND` operation is needed to convert values other than `GC_FGD` to
    zero and thus get a binary foreground mask. Then, both the source image and the
    mask are translated by 50 pixels in the horizontal. An affine warping operation
    is used with an identity matrix in which only the x translation component is changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the translated image is copied onto the target image, using the (also
    translated) mask. Both source and target images are shown in the following screenshot.
    Increasing the number of iterations did not have any significant effect in this
    particular example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![GrabCut](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Source and target images in the GrabCut example
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has covered one of the most important subjects in computer vision.
    Segmentation is often one of the first steps, and also, it is typically one of
    the trickiest. In this chapter, we have provided the reader with insight and samples
    to use the most useful segmentation methods in OpenCV, such as thresholding, contours
    and connected components, flood filling of regions, the watershed segmentation
    method, and the GrabCut method.
  prefs: []
  type: TYPE_NORMAL
- en: What else?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The meanshift segmentation (the function `pyrMeanShiftFiltering`) has been omitted.
    OpenCV includes an example showing how to use this function (`[opencv_source_code]/samples/cpp/meanshift_segmentation.cpp`).This
    method is, however, relatively slow and tends to produce oversegmented results.
  prefs: []
  type: TYPE_NORMAL
- en: Background/foreground segmentations can also be achieved using video, which
    will be covered in [Chapter 7](part0055_split_000.html#page "Chapter 7. What Is
    He Doing? Motion"), *What Is He Doing? Motion*.
  prefs: []
  type: TYPE_NORMAL
