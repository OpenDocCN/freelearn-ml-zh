- en: Machine Learning Definitions and Concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习定义和概念
- en: 'This chapter offers a high-level definition and explanation of the machine
    learning concepts needed to use the Amazon Machine Learning (Amazon ML) service
    and fully understand how it works. The chapter has three specific goals:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了使用Amazon机器学习（Amazon ML）服务所需的高级机器学习概念的定义和解释，并全面理解其工作原理。本章有三个具体目标：
- en: Listing the main techniques to improve the quality of predictions used when
    dealing with raw data. You will learn how to deal with the most common types of
    data problems. Some of these techniques are available in Amazon ML, while others
    aren't.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出在处理原始数据时用于提高预测质量的主要技术。你将学习如何处理最常见的数据问题。其中一些技术可在Amazon ML中使用，而其他则不行。
- en: Presenting the predictive analytics workflow and introducing the concept of
    cross validation or how to split your data to train and test your models.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示预测分析工作流程并介绍交叉验证的概念，即如何分割你的数据以训练和测试你的模型。
- en: Showing how to detect poor performance of your model and presenting strategies
    to improve these performances.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何检测模型的较差性能并介绍提高这些性能的策略。
- en: 'The reader will learn the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 读者将学习以下内容：
- en: How to spot common problems and anomalies within a given dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在给定数据集中发现常见问题和异常
- en: How to extract the most information out of a dataset in order to build robust
    models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从数据集中提取最多信息以构建稳健的模型
- en: How to detect and improve upon poor predictive performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检测并改进较差的预测性能
- en: What's an algorithm? What's a model?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法是什么？模型是什么？
- en: Before we dive into data munging, let's take a moment to explain the difference
    between an algorithm and a model, two terms we've been using up until now without
    a formal definition.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入数据清洗之前，让我们花一点时间解释算法和模型之间的区别，这两个术语我们至今尚未给出正式定义。
- en: 'Consider the simple linear regression example we saw in [Chapter 1](767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml),
    *Introduction to Machine Learning and Predictive Analytics —* the linear regression
    equation with one predictor:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们在[第1章](767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml)“机器学习与预测分析导论”中看到的简单线性回归示例，只有一个预测因子的线性回归方程：
- en: '![](img/image_02_001.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_001.png)'
- en: 'Here, *x* is the variable, *ŷ* the prediction, not the real value, and *(a,b)*
    the parameters of the linear regression model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是变量，*ŷ* 是预测值，而不是真实值，而 *(a,b)* 是线性回归模型的参数：
- en: The conceptual or theoretical model is the representation of the data that is
    the most adapted to the actual dataset. It is chosen at the beginning by the data
    scientist. In this case, the conceptual model is the linear regression model,
    where the prediction is a linear combination of a variable. Other conceptual models
    include decision trees, naive bayes, neural networks, and so on. All these models
    have parameters that need to be tuned to the actual data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念或理论模型是数据表示，它最适合实际数据集。数据科学家在开始时选择它。在这种情况下，概念模型是线性回归模型，其中预测是变量的线性组合。其他概念模型包括决策树、朴素贝叶斯、神经网络等。所有这些模型都有需要调整到实际数据的参数。
- en: The algorithm is the computational process that will calculate the optimal parameters
    of the conceptual model. In our simple linear regression case, the algorithm will
    calculate the optimal parameters *a* and *b*. Here optimal means that it gives
    the best predictions given the available dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法是计算过程，将计算概念模型的最佳参数。在我们的简单线性回归案例中，算法将计算最佳参数 *a* 和 *b*。这里的最佳意味着它给出了给定可用数据集的最佳预测。
- en: Finally, the predictive model corresponds to the conceptual model associated
    with the optimal parameters found for the available dataset.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，预测模型对应于与可用数据集找到的最佳参数相关联的概念模型。
- en: In reality, no one explicitly distinguishes between the conceptual model and
    the predictive model. Both are called the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，没有人明确区分概念模型和预测模型。两者都被称为模型。
- en: In short, the algorithm is the method of learning, and the model is what results
    form the learning phase. The model is the conceptual model (trees, svm, linear)
    trained by the algorithm on your training dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，算法是学习的方法，模型是学习阶段的结果。模型是算法在训练数据集上训练的概念模型（树、SVM、线性）。
- en: Dealing with messy data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理杂乱的数据
- en: As the dataset grows, so do inconsistencies and errors. Whether as a result
    of human error, system failure, or data structure evolutions, real-world data
    is rife with invalid, absurd, or missing values. Even when the dataset is spotless,
    the nature of some variables need to be adapted to the model. We look at the most
    common data anomalies and characteristics that need to be corrected in the context of Amazon
    ML linear models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的增长，不一致性和错误也随之增加。这些错误可能是由于人为错误、系统故障或数据结构演变造成的，真实世界数据中充满了无效、荒谬或缺失的值。即使数据集本身很干净，某些变量的性质也需要适应模型。我们将在Amazon
    ML线性模型的背景下查看最常见的数据异常和特征，这些特征需要被纠正。
- en: Classic datasets versus real-world datasets
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典数据集与真实世界数据集的比较
- en: Data scientists and machine-learning practitioners often use classic datasets
    to demonstrate the behavior of certain models. The **Iris** dataset, composed
    of 150 samples of three types of iris flowers, is one of the most commonly used
    to demonstrate or to teach predictive analytics. It has been around since 1936!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和机器学习从业者经常使用经典数据集来展示某些模型的行为。由150个三种鸢尾花样本组成的**Iris**数据集，是用于演示或教授预测分析的常用数据集之一。它自1936年以来一直存在！
- en: The **Boston housing** dataset and the **Titanic** dataset are other very popular
    datasets for predictive analytics. For text classification, the **Reuters** or
    the **20 newsgroups** text datasets are very common, while image recognition datasets
    are used to benchmark deep learning models. These classic datasets are used to
    establish baselines when evaluating the performances of algorithms and models.
    Their characteristics are well known, and data scientists know what performances
    to expect.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Boston housing**数据集和**Titanic**数据集是预测分析中其他非常流行的数据集。对于文本分类，**Reuters**或**20
    newsgroups**文本数据集非常常见，而图像识别数据集用于评估深度学习模型。这些经典数据集用于评估算法和模型性能时建立基线。它们的特征是众所周知的，数据科学家知道可以期待什么样的性能。'
- en: 'These classic datasets can be downloaded:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经典数据集可以下载：
- en: '**Iris**: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Iris**：[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
- en: '**Boston housing**: [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boston housing**：[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
- en: '**Titanic dataset**: [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
    or [http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Titanic dataset**：[https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
    或 [http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/)'
- en: '**Reuters**: [https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reuters**：[https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)'
- en: '**20 newsgroups**: [http://scikit-learn.org/stable/datasets/twenty_newsgroups.html](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**20 newsgroups**：[http://scikit-learn.org/stable/datasets/twenty_newsgroups.html](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)'
- en: '**Image recognition and deep learning**: [http://deeplearning.net/datasets/](http://deeplearning.net/datasets/)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别和深度学习**：[http://deeplearning.net/datasets/](http://deeplearning.net/datasets/)'
- en: 'However, classic datasets can be weak equivalents of real datasets, which have
    been extracted and aggregated from a diverse set of sources: databases, APIs,
    free form documents, social networks, spreadsheets, and so on. In a real-life
    situation, the data scientist must often deal with messy data that has missing
    values, absurd outliers, human errors, weird formatting, strange inputs, and skewed
    distributions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经典数据集可能是真实数据集的弱等价物，这些真实数据集是从各种来源提取和汇总而来的：数据库、API、自由格式文档、社交网络、电子表格等等。在现实情况下，数据科学家必须经常处理含有缺失值、荒谬的异常值、人为错误、奇怪的格式、奇怪的输入和偏斜分布的杂乱数据。
- en: The first task in a predictive analytics project is to clean up the data. In
    the following section, we will look at the main issues with raw data and what
    strategies can be applied. Since we will ultimately be using a linear model for
    our predictions, we will process the data with that in mind.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析项目的第一个任务是清理数据。在下一节中，我们将探讨原始数据的主要问题以及可以应用的战略。由于我们最终将使用线性模型进行预测，我们将以此为目标处理数据。
- en: Assumptions for multiclass linear models
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类线性模型的假设
- en: 'For a linear model to offer reliable predictions, predictors must satisfy a
    certain number of conditions. These conditions are known as the *Assumptions of
    Multiple Linear Regression* ([http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使线性模型提供可靠的预测，预测变量必须满足一定数量的条件。这些条件被称为**多元线性回归的假设**([http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/))：
- en: '**Linear relationship**: The predictors should have some level of linear relationship
    with the outcome'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性关系**：预测变量应与结果有一定的线性关系'
- en: '**Multivariate normality**: The predictors should follow a Gaussian distribution'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元正态性**：预测变量应遵循高斯分布'
- en: '**No or little multicollinearity**: The predictors should not be correlated
    to one another'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无或低多重共线性**：预测变量之间不应相互关联'
- en: '**Homoscedasticity**: The variance of each predictor should remain more or
    less constant across the whole range of values'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同方差性**：每个预测变量的方差应在整个值域内保持大致恒定'
- en: Of course, these assumptions are seldom verified. But there are ways to transform
    the data to approach these optimal conditions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些假设很少被验证。但有一些方法可以将数据转换为接近这些理想条件。
- en: Missing values
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值
- en: 'Data aggregation, extraction, and consolidation is often not perfect and sometimes
    results in missing values. There are several common strategies to deal with missing
    values in datasets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚合、提取和合并往往并不完美，有时会导致缺失值。在数据集中处理缺失值有几种常见的策略：
- en: Removing all the rows with missing values from the dataset. This is simple to
    apply, but you may end up throwing away a big chunk of information that would
    have been valuable to your model.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中移除所有包含缺失值的行。这种方法简单易行，但你可能会丢失大量对模型有价值的信息。
- en: 'Using models that are, by nature, not impacted by missing values such as decision
    tree-based models: random forests, boosted trees. Unfortunately, the linear regression
    model, and by extension the SGD algorithm, does not work with missing values ([http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf](http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf)).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本质上不受缺失值影响的模型，例如基于决策树的模型：随机森林、提升树。不幸的是，线性回归模型以及由此扩展的 SGD 算法不能处理缺失值([http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf](http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf))。
- en: Imputing the missing data with replacement values; for example, replacing missing
    values with the median, the average, or the harmonic mean of all the existing
    values, or using clustering or linear regression to predict the missing values.
    It may be interesting to add the information that these values were missing in
    the first place to the dataset.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用替换值填充缺失数据；例如，用所有现有值的中位数、平均值或调和平均值替换缺失值，或使用聚类或线性回归预测缺失值。将这些值最初缺失的信息添加到数据集中可能很有趣。
- en: In the end, the right strategy will depend on the type of missing data and of
    course, the context. While replacing missing blood pressure numbers in a patient
    medical record by some average may not be acceptable in a healthcare context, replacing
    missing age values by the average age in the Titanic dataset is definitely adapted
    to a data science competition.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正确的策略将取决于缺失数据的类型以及当然，上下文。虽然在一个患者的医疗记录中用平均值替换缺失的血压数值在医疗保健环境中可能不可接受，但在数据科学竞赛中用泰坦尼克号数据集中的平均年龄替换缺失的年龄值无疑是适用的。
- en: 'However, Amazon ML''s documentation is not 100% clear on the strategy used
    to deal with missing values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Amazon ML 的文档并没有100%清晰地说明处理缺失值所使用的策略：
- en: If the target attribute is present in the record, but a value for another numeric
    attribute is missing, then Amazon ML overlooks the missing value. In this case,
    Amazon ML creates a substitute attribute and sets it to 1 to indicate that this
    attribute is missing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果记录中存在目标属性，但另一个数值属性缺失，那么 Amazon ML 会忽略这个缺失值。在这种情况下，Amazon ML 会创建一个替代属性，并将其设置为1，以表示该属性缺失。
- en: In the case of missing values, a new column is created with a Boolean flag to
    indicate that the value was missing in the first place. But it is not clear whether
    the whole row or sample is dismissed or overlooked or if just the cell is removed.
    There is no mention of any type of imputation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺失值的情况下，创建了一个新的布尔标志列，以指示原始值缺失。但并不清楚是整行或样本被丢弃或忽略，还是只是单元格被删除。没有提到任何类型的插补。
- en: Normalization
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: 'Machine learning algorithms incrementally update the model parameters by minimizing
    the error between the real value and the one predicted with the last iteration''s
    parameters. To measure this prediction error we introduce the concept of loss
    functions. A loss function is a measure of the prediction error. For a certain
    algorithm, using different loss functions will create variants of the algorithm. Most
    common loss functions use the **L2** or the **L1** norm to measure the error:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法通过最小化真实值与使用上次迭代参数预测的值之间的误差来增量更新模型参数。为了衡量这种预测误差，我们引入了损失函数的概念。损失函数是预测误差的度量。对于某种算法，使用不同的损失函数将创建算法的变体。最常见的损失函数使用**L2**或**L1**范数来衡量误差：
- en: '**●  ****L2 norm:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**●  L2范数：**'
- en: '![](img/B05028_2_02_eqn.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_2_02_eqn.png)'
- en: '**●  ****L1 norm:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**●  L1范数：**'
- en: '![](img/B05028_2_03_eqn.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_2_03_eqn.png)'
- en: Where *y[i]* and *ŷ* are the real and predicted values of the samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y[i]* 和 *ŷ* 分别是样本的真实值和预测值。
- en: The measure of the prediction error can end up being skewed when the different
    predictors differ by an order of magnitude. The large predictors obfuscate the
    importance of the smaller valued ones, thus making it difficult to infer the relative
    importance of each predictor in the model. This impacts how the respective weights
    of the linear model converge to their optimal value and as a consequence the performance
    of the algorithm. Predictors with the highest magnitude will end up dominating
    the model even if the predictor has little predictive power with regard to the
    real outcome value. Normalizing the data is a way to mitigate that problem by
    forcing the predictors to all be on the same scale.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同的预测变量相差一个数量级时，预测误差的度量可能会变得偏斜。大预测变量会掩盖小值变量的重要性，从而使得推断每个预测变量在模型中的相对重要性变得困难。这会影响线性模型各自的权重收敛到最优值，进而影响算法的性能。具有最高幅度的预测变量最终会主导模型，即使该预测变量对真实结果值几乎没有预测能力。通过强制所有预测变量处于相同的尺度，数据归一化是一种减轻该问题的方法。
- en: 'There are two common types of normalization; data can be normalized or standardized:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的归一化类型；数据可以被归一化或标准化：
- en: 'The **min-max normalization**, or **normalization**, which sets all values
    between *[0,1]*:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min-max归一化**，或**归一化**，将所有值设置为*[0,1]*之间：'
- en: '![](img/image_02_006.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_006.png)'
- en: 'The **z-score normalization**, or **standardization**, which normalizes with
    respect to the standard deviation. All predictors will have a mean of 0 and a
    standard deviation of 1:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z分数标准化**，或**标准化**，根据标准差进行归一化。所有预测变量都将具有0均值和1标准差：'
- en: '![](img/image_02_007.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_007.png)'
- en: The tree-based methods (decision trees, random forests, boosted trees) are the
    only machine learning models whose performance is not improved by normalization
    or standardization. All other distance/variance-based predictive algorithms may
    benefit from normalization. It has been shown that standardization is particularly
    useful for SGD, as it ensures that all the weights will be adapted at the same
    speed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法（决策树、随机森林、提升树）是唯一一种其性能不会因归一化或标准化而提高的机器学习模型。所有其他基于距离/方差预测算法都可能从归一化中受益。已经证明，标准化对于随机梯度下降（SGD）特别有用，因为它确保所有权重将以相同的速度适应。
- en: '*Efficient BackProp Yann A. LeCun et al. in Neural Networks*: *Tricks of the
    Trade pp. 9-48, Springer Verlag*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*高效反向传播 Yann A. LeCun 等人在神经网络中*：*技巧贸易 pp. 9-48，Springer Verlag*'
- en: Amazon ML offers z-score standardization as part of the available data transformations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习（Amazon ML）提供z分数标准化作为可用数据转换的一部分。
- en: Imbalanced datasets
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡数据集
- en: Dealing with imbalanced datasets is a very common classification problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集是一个非常常见的分类问题。
- en: 'Consider a Binary classification problem. Your goal is to predict a positive
    versus a negative class. The ratio between the two classes is highly skewed in
    favor of the positive class. This situation is frequently encountered in the following
    instance:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个二元分类问题。你的目标是预测正类与负类。两个类别之间的比例高度偏向正类。这种情况在以下实例中经常遇到：
- en: In a medical context where the positive class corresponds to the presence of
    cancerous cells in people out of a large random population
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在医疗环境中，正类对应于在大量随机人群中存在癌细胞的人群
- en: In a marketing context where the positive class corresponds to prospects buying
    an insurance while the majority of people are not buying it
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在营销环境中，正类对应于购买保险的潜在客户，而大多数人并没有购买
- en: In both these cases, we want to detect the samples in the minority class, but
    they are overwhelmingly outnumbered by the samples in the majority (negative)
    class. Most predictive models will be highly biased toward the majority class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们希望检测少数类别的样本，但它们在多数（负）类别的样本中占绝对少数。大多数预测模型都会高度偏向多数类别。
- en: In the presence of highly imbalanced classes, a very simplistic model that always
    predicts the majority class and never the minority one will have excellent accuracy but
    would never detect the important and valuable class. Consider for instance a dataset
    composed of 1,000 samples, with 50 positive samples that we want to detect or
    predict and 950 negative ones of little interest. That simplistic model has an
    accuracy rate of 95% which is obviously a decent accuracy even though that model
    is totally useless. This problem is known as the **Accuracy paradox** ([https://en.wikipedia.org/wiki/Accuracy_paradox](https://en.wikipedia.org/wiki/Accuracy_paradox)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在高度不平衡的类别存在的情况下，一个总是预测多数类别而从不预测少数类别的非常简单的模型将具有极高的准确率，但永远不会检测到重要且有价值的类别。例如，考虑一个由1,000个样本组成的数据集，其中包含50个我们想要检测或预测的正样本和950个无趣的负样本。这个简单模型有95%的准确率，这显然是一个不错的准确率，尽管这个模型完全无用。这个问题被称为**准确率悖论**([https://en.wikipedia.org/wiki/Accuracy_paradox](https://en.wikipedia.org/wiki/Accuracy_paradox))。
- en: A straightforward solution would be to gather more data, with a focus on collecting
    samples of the minority class and in order to balance out the two classes. But
    that's not always a possibility.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接的解决方案是收集更多数据，重点关注收集少数类别的样本，以便平衡两个类别。但这并不总是可能。
- en: 'There are many other strategies to deal with imbalanced datasets. We will briefly
    look at some of the most common ones. One approach is to resample the data by
    under sampling or oversampling the available data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集有许多其他策略。我们将简要介绍其中一些最常见的方法。一种方法是通过欠采样或过采样现有数据来重新采样数据：
- en: '**Undersampling** consists in discarding most samples in the majority class
    in order to tilt back the minority/majority class ratio toward *50/50*. The obvious
    problem with that strategy is that a lot of data is discarded and along with that,
    meaningful signal for the model. This technique can be useful in the presence
    of large enough datasets.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠采样**包括丢弃多数类别的多数样本，以便将少数/多数类别的比例调整为*50/50*。这种策略的明显问题是会丢弃大量数据，以及随之而来的对模型有意义的信号。这种技术在存在足够大的数据集时可能是有用的。'
- en: '**Oversampling** consists in duplicating samples that belong to the minority
    class. Contrary to under sampling, there is no loss of data with that strategy.
    However, oversampling adds extra weight to certain patterns from the minority
    class, which may not bring useful information to the model. Oversampling adds
    noise to the model. Oversampling is useful when the dataset is small and you can''t
    afford to leave some data out.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样**包括复制属于少数类别的样本。与欠采样相反，该策略不会丢失数据。然而，过采样会给少数类别的某些模式添加额外的权重，这可能不会给模型带来有用的信息。过采样会给模型添加噪声。当数据集较小且无法承受丢失一些数据时，过采样是有用的。'
- en: Under sampling and oversampling are two simple and easy-to-implement methods
    that are useful in establishing a baseline. Another widely-used method consists
    in creating synthetic samples from the existing data. A popular sample creation
    technique is the **SMOTE** method, which stands for **Synthetic Minority Over-Sampling
    Technique**. SMOTE works by selecting similar samples (with respect to some distance
    measure) from the minority class and adding perturbations on the selected attributes.
    SMOTE then creates new minority samples within clusters of existing minority samples.
    SMOTE is less of a solution in the presence of high-dimensional datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样和过采样是两种简单且易于实现的方法，在建立基线时很有用。另一种广泛使用的方法是从现有数据中创建合成样本。一种流行的样本创建技术是 **SMOTE**
    方法，代表 **合成少数类过采样技术**。SMOTE 通过选择与某些距离度量相似的少数类样本，并在选定的属性上添加扰动来工作。然后，SMOTE 在现有少数样本的簇内创建新的少数样本。在存在高维数据集的情况下，SMOTE
    的解决方案作用较小。
- en: The **imbalanced library** in Python ([http://](http://github.com/scikit-learn-contrib/imbalanced-learn)[github.com/scikit-learn-contrib/imbalanced-learn](http://github.com/scikit-learn-contrib/imbalanced-learn))
    or the **unbalanced package** in R ([https://cran.r-project.org/web/packages/unbalanced/index.html](https://cran.r-project.org/web/packages/unbalanced/index.html))
    both offer a large set of advanced techniques on top of the ones mentioned.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中的 **不平衡库** ([http://](http://github.com/scikit-learn-contrib/imbalanced-learn)[github.com/scikit-learn-contrib/imbalanced-learn](http://github.com/scikit-learn-contrib/imbalanced-learn))
    或 R 中的 **不平衡包** ([https://cran.r-project.org/web/packages/unbalanced/index.html](https://cran.r-project.org/web/packages/unbalanced/index.html))
    都在所提及的技术之上提供了一组大量的高级技术。
- en: Note that the choice of the metric used to assess the performances of the model
    is particularly important in the context of an imbalanced dataset. The accuracy
    rate, which is defined as the ratio of correctly predicted samples to the total
    number of samples is the most straightforward metric in classification problems.
    But as we have seen, this accuracy rate is not a good indicator of the model's
    predictive power in the presence of a highly skewed class distribution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在数据不平衡的背景下，选择用于评估模型性能的指标尤为重要。准确率，定义为正确预测样本数与总样本数的比率，是分类问题中最直接的指标。但正如我们所见，这种准确率在存在高度倾斜的类别分布的情况下，并不是模型预测能力的良好指标。
- en: 'In such a context, two metrics are recommended:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，建议使用两个指标：
- en: '**Cohen''s kappa:** A robust measure of the agreement between real and predicted
    classes ([https://en.wikipedia.org/wiki/Cohen%27s_kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cohen''s kappa**：衡量真实类别和预测类别之间一致性的稳健度量 ([https://en.wikipedia.org/wiki/Cohen%27s_kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))'
- en: '**The F1 score**: The harmonic mean between Precision and Recall ([https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score))'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 分数**：精确率和召回率的调和平均数 ([https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score))'
- en: The F1 score is the metric used by Amazon ML to assess the quality of a classification
    model. We give the definition of the F1-score under the *Evaluating the performance
    of your model* section at the end of this chapter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是 Amazon ML 用于评估分类模型质量的指标。我们在本章末尾的“评估模型性能”部分给出了 F1 分数的定义。
- en: Addressing multicollinearity
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决多重共线性问题
- en: 'The following is the definition of multicollinearity according to Wikipedia
    ([https://en.wikipedia.org/wiki/Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是根据维基百科的定义的多重共线性 ([https://en.wikipedia.org/wiki/Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)):'
- en: '**Multicollinearity** is a phenomenon in which two or more predictor variables
    in a multiple regression model are highly correlated, meaning that one can be
    linearly predicted from the others with a substantial degree of accuracy.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**多重共线性**是多个回归模型中两个或多个预测变量高度相关的现象，这意味着一个可以从其他变量中线性预测出来，具有相当高的准确性。'
- en: 'Basically, let''s say you have a model with three predictors:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，假设你有一个包含三个预测因子的模型：
- en: '![](img/B05028_2_04_eqn.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_2_04_eqn.png)'
- en: And one of the predictors is a linear combination (perfect multicollinearity)
    or is approximated by a linear combination (near multicollinearity) of two other
    predictors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个预测因子是一个线性组合（完美多重共线性）或者近似为两个其他预测因子的线性组合（近多重共线性）。
- en: For instance: ![](img/B05028_2_05_eqn.png)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如： ![图片](img/B05028_2_05_eqn.png)
- en: Here, ![](img/B05028_2_06_eqn.png) is some noise variable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， ![图片](img/B05028_2_06_eqn.png) 是一些噪声变量。
- en: In that case, changes in ![](img/B05028_2_07_eqn.png) and ![](img/B05028_2_08_eqn.png)
    will drive changes in ![](img/B05028_2_09_eqn.png), and as a consequence, ![](img/B05028_2_10_eqn.png)
    will be tied to ![](img/B05028_2_11_eqn.png) and ![](img/B05028_2_12_eqn.png).
    The information already contained in ![](img/B05028_2_07_eqn.png) and ![](img/B05028_2_08_eqn.png)
    will be shared with the third predictor ![](img/B05028_2_09_eqn.png), which will
    cause high uncertainty and instability in the model. Small changes in the predictors'
    values will bring large variations in the coefficients. The regression may no
    longer be reliable. In more technical terms, the standard errors of the coefficient
    would increase, which would lower the significance of otherwise important predictors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，![](img/B05028_2_07_eqn.png)和![](img/B05028_2_08_eqn.png)的变化将驱动![](img/B05028_2_09_eqn.png)的变化，因此，![](img/B05028_2_10_eqn.png)将与![](img/B05028_2_11_eqn.png)和![](img/B05028_2_12_eqn.png)相关联。![](img/B05028_2_07_eqn.png)和![](img/B05028_2_08_eqn.png)中已经包含的信息将与第三个预测因子![](img/B05028_2_09_eqn.png)共享，这将在模型中引起高度不确定性和不稳定性。预测因子值的小幅变化将导致系数的大幅变化。回归可能不再可靠。在更技术性的术语中，系数的标准误差会增加，这会降低其他重要预测因子的显著性。
- en: There are several ways to detect multicollinearity. Calculating the correlation
    matrix of the predictors is a first step, but that would only detect collinearity
    between pairs of predictors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以检测多重共线性。计算预测因子的相关矩阵是第一步，但这只会检测预测因子对之间的共线性。
- en: 'A widely used detection method for multicollinearity is to calculate the **variance
    inflation factor** or **VIF** for each of the predictors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 多重共线性的一个常用检测方法是计算每个预测因子的**方差膨胀因子**或**VIF**：
- en: '![](img/B05028_2_13_eqn.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![方程式图片](img/B05028_2_13_eqn.png)'
- en: Here, ![](img/B05028_2_14_eqn.png) is the coefficient of determination of the
    regression equation in step one, with ![](img/B05028_2_15_eqn.png) on the left-hand
    side and all other predictor variables (all the other ![](img/B05028_2_16_eqn.png)
    variables) on the right-hand side.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B05028_2_14_eqn.png)是第一步中回归方程的确定系数，其中![](img/B05028_2_15_eqn.png)在左侧，所有其他预测变量（所有其他![](img/B05028_2_16_eqn.png)变量）在右侧。
- en: A large value for one of the VIFs is an indication that the variance (the square
    of the standard error) of a particular coefficient is larger than it would be
    if that predictor was completely uncorrelated with all the other predictors. For
    instance, a VIF of 1.8 indicates that the variance of a predictor is 80% larger
    than what it would be in the uncorrelated case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: VIF（方差膨胀因子）中的一个值很大表明，特定系数的方差（标准误差的平方）比该预测因子与其他所有预测因子完全不相关时的方差要大。例如，VIF为1.8表示预测因子的方差比不相关情况下的方差大80%。
- en: 'Once the attributes with high collinearity have been identified, the following
    options will reduce collinearity:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了高度共线性的属性，以下选项将减少共线性：
- en: Removing the high collinearity predictors from the dataset
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中移除高度共线性预测因子
- en: Using **Partial Least Squares Regression (PLS)** or **Principal Components Analysis
    (PCA)**, regression methods ([https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis));
    PLS and PCA will reduce the number of predictors to a smaller set of uncorrelated
    variables
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**偏最小二乘回归（PLS）**或**主成分分析（PCA）**，回归方法([https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis))；PLS和PCA将预测因子的数量减少到一组不相关的变量
- en: Unfortunately, detection and removal of multicollinear variables is not available
    from the Amazon ML platform.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，从Amazon ML平台无法检测和移除多重共线性变量。
- en: Detecting outliers
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测异常值
- en: Given a variable, outliers are values that are very distant from other values
    of that variable. Outliers are quite common, and often caused by human or measurement
    errors. Outliers can strongly derail a model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个变量，异常值是该变量中与其他值非常不同的值。异常值相当常见，通常由人为或测量错误引起。异常值可能会严重破坏模型。
- en: To demonstrate, let's look at two simple datasets and see how their mean is
    influenced by the presence of an outlier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，让我们看看两个简单的数据集，并观察它们的平均值是如何受到异常值存在的影响的。
- en: 'Consider the two datasets with few samples each: *A = [1,2,3,4]* and *B = [1,2,3,4,
    100]*. The 5^(th) value in the B dataset,  100, is obviously an outlier: *mean(A)
    = 2.5*, while *mean(B) = 22*. An outlier can have a large impact on a metric.
    Since most machine learning algorithms are based on distance or variance measurements,
    outliers can have a high impact on the performance of a model.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个样本量都很少的数据集：*A = [1,2,3,4]* 和 *B = [1,2,3,4, 100]*。B数据集中的第5个值，100，显然是一个异常值：*mean(A)
    = 2.5*，而*mean(B) = 22*。异常值可以对指标产生重大影响。由于大多数机器学习算法基于距离或方差测量，异常值可以对模型的性能产生重大影响。
- en: 'Multiple linear regression is sensitive to outlier effects, as shown in the
    following graph where adding a single outlier point derails the solid regression
    line into the dashed one:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归对异常值效应敏感，如下面的图表所示，添加一个单独的异常值点会将坚实的回归线破坏成虚线：
- en: '![](img/image_02_024.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_024.png)'
- en: Removing the samples associated with the outliers is the simplest solution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 移除与异常值相关的样本是最简单的解决方案。
- en: Another solution can be to apply **quantile binning** to the predictor by splitting
    the values into *N* ordered intervals or bins, each approximately containing an
    equal number of samples. This will transform a numeric (continuous) predictor
    into a categorical one. For example, [1,2,3,4,5,6,7,8,9,10,11,100] split into
    three equally sized bins becomes [*1,1,1,1,2,2,2,2,3,3,3,3*]; the outlier value
    100 has been included in the third bin and hidden.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是对预测变量应用**分位数分组**，通过将值分成*N*个有序的区间或分组，每个分组大约包含相等数量的样本。这将把一个数值（连续）预测变量转换为分类变量。例如，[1,2,3,4,5,6,7,8,9,10,11,100]分成三个大小相等的分组变为[*1,1,1,1,2,2,2,2,3,3,3,3*]；异常值100被包含在第三个分组中并隐藏。
- en: The downside of quantile binning is that some granularity of information is
    lost in the process, which may degrade the performance of the model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数分组的缺点是在过程中会丢失一些信息粒度，这可能会降低模型的性能。
- en: '**Quantile binning** is available as a data transformation process in Amazon
    ML and is also used to quantify non-linearities in the original dataset.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**分位数分组**在Amazon ML中作为数据变换过程可用，也用于量化原始数据集中的非线性。'
- en: In fact, **Quantile Binning** (**QB**) is applied by default by Amazon ML to
    all continuous variables that do not exhibit a straightforward linear relation
    to the outcome. In all our trials, and contrary to our prior assumptions, we have
    found that QB is a very efficient data transformation in the Amazon ML context.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，**分位数分组**（**QB**）是Amazon ML默认应用于所有与结果没有直接线性关系的连续变量的。在我们所有的试验中，与我们的先前假设相反，我们发现QB在Amazon
    ML环境中是一种非常高效的数据变换。
- en: Accepting non-linear patterns
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接受非线性模式
- en: A linear regression model implies that the outcome can be estimated by a linear
    combination of the predictors. This, of course, is not always the case, as features
    often exhibit nonlinear patterns.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型意味着结果可以通过预测变量的线性组合来估计。当然，这并不总是情况，因为特征通常表现出非线性模式。
- en: 'Consider the following graph, where *Y* axis depends on *X* axis but the relationship
    displays an obvious quadratic pattern. Fitting a line (*y = aX + b*) as a prediction
    model of *Y* as a function of *X* does not work:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图表，其中*Y*轴依赖于*X*轴，但关系显示出明显的二次模式。将线（*y = aX + b*）作为*Y*作为*X*的函数的预测模型并不适用：
- en: '![](img/image_02_025.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_025.png)'
- en: Some models and algorithms are able to naturally handle non-linearities, for
    example, tree-based models or support vector machines with non-linear kernels.
    Linear regression and SGD are not.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型和算法能够自然地处理非线性，例如，基于树的模型或具有非线性核的支持向量机。线性回归和SGD则不行。
- en: '**Transformations**: One way to deal with these nonlinear patterns in the context
    of linear regression is to transform the predictors. In the preceding simple example,
    adding the square of the predictor X to the model would give a much better result.
    The model would now be of the following form:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**变换**：在线性回归的背景下处理这些非线性模式的一种方法是对预测变量进行变换。在前面的简单例子中，将预测变量X的平方添加到模型中会得到更好的结果。现在的模型将具有以下形式：'
- en: '![](img/B05028_2_28_eqn.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_2_28_eqn.png)'
- en: 'And as shown in the following diagram, the new quadratic model fits the data
    much better:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图表所示，新的二次模型更好地拟合了数据：
- en: '![](img/image_02_027.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_027.png)'
- en: We are not restricted to the quadratic case, and a power function of higher
    order can be used to transform existing attributes and create new predictors.
    Other useful transformations could include taking the logarithm, exponential,
    sine and cosine, and so on. The **Boxcox** **transformation** ([http://onlinestatbook.com/2/transformations/box-cox.html](http://onlinestatbook.com/2/transformations/box-cox.html))
    is worth citing at this point. It's an efficient data transformation that reduces
    skewness and kurtosis of a variable distribution. It reshapes the variable distribution
    into one closer to a Gaussian distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于二次情况，可以使用更高阶的幂函数来转换现有属性并创建新的预测因子。其他有用的转换可能包括取对数、指数、正弦和余弦等。在此处值得提及的是**Box-Cox转换**([http://onlinestatbook.com/2/transformations/box-cox.html](http://onlinestatbook.com/2/transformations/box-cox.html))。它是一种有效的数据转换，可以减少变量分布的偏度和峰度。它将变量分布重塑为更接近高斯分布的形状。
- en: '**Splines** are an excellent and more powerful alternative to polynomial interpolation.
    Splines are piece-wise polynomials that join smoothly. At their simplest level,
    splines consists of lines that are connected together at different points. Splines
    are not available in Amazon ML.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**样条曲线**是多项式插值的优秀且更强大的替代品。样条曲线是分段多项式，它们在平滑处连接。在其最简单层面上，样条曲线由在不同点连接的线条组成。样条曲线在Amazon
    ML中不可用。'
- en: '**Quantile binning** is the Amazon ML solution to non-linearities. By splitting
    the data into N bins, you remove any non-linearities in the bin''s intervals.
    Although binning has several drawbacks ([http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous)),
    the main one being that information is discarded in the process, it has been shown
    to generate excellent prediction performance in the Amazon ML platform.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**分位数分组**是Amazon ML解决非线性问题的方法。通过将数据分成N个分组，你消除了分组区间中的任何非线性。尽管分组有几个缺点([http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous))，主要缺点是信息在过程中被丢弃，但它已经在Amazon
    ML平台上显示出优秀的预测性能。'
- en: Adding features?
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加特征？
- en: In general, adding new features that are correlated in some ways to the outcome
    brings information and improves a model. However, adding too many features with
    little predictive power may end up bringing confusion to that same model and in
    the end degrading its performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，添加与结果以某种方式相关的新的特征会带来信息并提高模型。然而，添加太多预测能力很小的特征可能会使同一个模型变得混乱，最终降低其性能。
- en: Feature selection by removal of the least interesting features is worth trying
    when the sample size is small compared to the number of features; it leads to
    too few observations or too many features. There are different strategies ([http://machinelearningmastery.com/an-introduction-to-feature-selection/](http://machinelearningmastery.com/an-introduction-to-feature-selection/))
    to identify and remove weak features. Selecting features based on their correlation
    with the outcome and discarding features with little or no correlation with the
    outcome will usually improve your model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当样本量相对于特征数量较小时，通过移除最不有趣的特征进行特征选择是值得尝试的；这会导致观察值过少或特征过多。有不同策略([http://machinelearningmastery.com/an-introduction-to-feature-selection/](http://machinelearningmastery.com/an-introduction-to-feature-selection/))来识别和移除弱特征。根据特征与结果的相关性选择特征，并丢弃与结果相关性很小或没有相关性的特征，通常会提高你的模型。
- en: Preprocessing recapitulation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理回顾
- en: 'The following table recapitulates the different issues that one can find in
    raw data and whether Amazon ML offers ways to deal with them:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了在原始数据中可能遇到的不同问题，以及Amazon ML提供处理这些问题的方法：
- en: '|  | **Linear model sensitivity** | **Available on Amazon ML** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | **线性模型敏感性** | **在Amazon ML上可用** |'
- en: '| **Missing values** | Yes | Dealt with automatically |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **缺失值** | 是 | 自动处理 |'
- en: '| **Standardization** | Yes | z-score standardization |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **标准化** | 是 | z分数标准化 |'
- en: '| **Outliers** | Yes | Quantile binning |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **异常值** | 是 | 分位数分组 |'
- en: '| **Multicollinearity** | Yes | No |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **多重共线性** | 是 | 否 |'
- en: '| **Imbalanced datasets** | Yes | Uses the right metric F1 Score No sampling strategy
    (may exist in background) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **不平衡数据集** | 是 | 使用正确的指标F1分数，没有采样策略（可能存在于后台） |'
- en: '| **Non linearities** | Yes | Quantile binning |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **非线性** | 是 | 分位数分组 |'
- en: The predictive analytics workflow
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测分析工作流程
- en: We have been talking about training the model. What does that mean in practice?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在谈论训练模型。这在实践中意味着什么？
- en: 'In supervised learning, the dataset is usually split into three non-equal parts:
    training, validation, and test:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，数据集通常被分成三个不等的部分：训练、验证和测试：
- en: '![](img/image_02_028.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_028.png)'
- en: The **training** set on which you train your model. It has to be big enough
    to give the model as much information on the data as possible. This subset of
    the data is used by the algorithm to estimate the best parameters of the model.
    In our case, the SGD algorithm will use that training subset to find the optimal
    weights of the linear regression model.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你训练模型的**训练集**。它必须足够大，以便尽可能多地给模型提供关于数据的信息。这个数据子集被算法用来估计模型的最佳参数。在我们的情况下，SGD算法将使用这个训练子集来找到线性回归模型的最优权重。
- en: The **validation** set is used to assess the performance of a trained model.
    By measuring the performance of the trained model on a subset that has not been
    used in its training, we have an objective assessment of its performance. That
    way we can train different models with different meta parameters and see which
    one is performing the best on the validation set. This is also called model selection. Note
    that this creates a feedback loop, since the validation dataset now has an influence
    on your model selection. Another model may have performed worse on that particular
    validation subset but overall better on new data.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**用于评估训练模型的性能。通过测量训练模型在未用于其训练的子集上的性能，我们可以对其性能进行客观评估。这样我们就可以用不同的元参数训练不同的模型，并查看哪个模型在验证集上的表现最好。这也被称为模型选择。请注意，这会创建一个反馈循环，因为验证数据集现在对你的模型选择有影响。另一个模型可能在那个特定的验证子集上表现较差，但总体上在新数据上表现更好。'
- en: The **test** set corresponds to data that is set aside until you have fully
    optimized your features and model. The test subset is also called the **held-out**
    dataset.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**对应于在你完全优化了特征和模型之后保留的数据。测试子集也称为**保留集**。'
- en: In real life, your model will face previously unforeseen data, since the ultimate
    *raison d'etre* for a model is to predict unseen data. Therefore, it is important
    to assess the performance of the model on data it has never encountered before.
    The held-out dataset is a proxy for yet unseen data. It is paramount to leave
    this dataset aside until the end. It should never be used to optimize the model
    or the data attributes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，你的模型将面临之前未预见的数据，因为模型的最终*存在理由*是预测未见数据。因此，评估模型在之前未曾遇到的数据上的性能是很重要的。保留的数据集是未见数据的代理。在最后之前，绝对不能使用这个数据集来优化模型或数据属性。
- en: These three subsets should be large enough to represent the real data accurately.
    More precisely, the distribution of all the variables should be equivalent in
    the three subsets. If the original dataset is ordered in some way, it is important
    to make sure that the data is shuffled prior to the train, validation, test split.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个子集应该足够大，以准确代表真实数据。更精确地说，所有变量的分布应该在三个子集中是等效的。如果原始数据集以某种方式排序，确保在训练、验证和测试分割之前对数据进行洗牌是很重要的。
- en: As mentioned previously, the model you choose based on its performance on the
    validation set may have had a positive bias toward that particular dataset. In
    order to minimize such a dependency, it is common to train and evaluate several
    models with the same parameter settings and to average the performance of the
    model over several training validation dataset pairs. This reduces the model selection
    dependence with regard to the specific distribution of variables in the validation
    dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，基于其在验证集上的性能而选择的模型可能对该特定数据集有积极的偏差。为了最小化这种依赖性，通常会对具有相同参数设置的多个模型进行训练和评估，并对多个训练验证数据集对的模型性能进行平均。这减少了模型选择对验证数据集中变量特定分布的依赖性。
- en: This third-split method is basic and as we've seen, the model could end up being
    dependent on some specificities of the validation subset. Cross-validation is
    a standard method to reduce that dependency and improve our model selection. Cross
    validation consists in carrying out several training/validation split and averaging
    the model performance on the different validation subsets. The most frequent cross
    validation technique is k-fold cross validation which consists in splitting the
    dataset in K-chunks, recursively using each part as validation and the k-1 other
    parts as training. Other cross validation techniques include Monte-Carlo cross
    validation where the different training and validation sets are randomly sampled
    from the initial dataset. We will implement Monte Carlo cross validation in a
    later chapter. Cross validation is not a feature included in the Amazon ML service
    and needs to be implemented programatically. In Amazon ML, the training and evaluation
    of a model is done on one training-validation split only.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种三分法是基本的，正如我们所看到的，模型最终可能会依赖于验证子集的一些特定性。交叉验证是一种标准方法，可以减少这种依赖性并改进我们的模型选择。交叉验证包括执行多个训练/验证分割，并对不同验证子集上的模型性能进行平均。最常用的交叉验证技术是
    k 折交叉验证，它包括将数据集分割成 K 个块，递归地使用每个部分作为验证集，其余 k-1 个部分作为训练集。其他交叉验证技术包括蒙特卡洛交叉验证，其中不同的训练和验证集是从初始数据集中随机采样的。我们将在后面的章节中实现蒙特卡洛交叉验证。交叉验证不是
    Amazon ML 服务中包含的功能，需要通过编程实现。在 Amazon ML 中，模型的训练和评估仅在一个训练-验证分割上完成。
- en: Training and evaluation in Amazon ML
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon ML 中的训练和评估
- en: In the context of Amazon ML, the model is linear regression and the algorithm
    the **Stochastic Gradient Descent** (**SGD**) algorithm. This algorithm has one
    main meta parameter called the learning rate and often noted ![](img/B05028_2_17_eqn.png),
    which dictates how much of a new sample is taken into account for each iterative
    update of the weights. A larger learning rate makes the algorithm converge faster
    but stabilizes further from the optimal weights, while a smaller learning induces
    a slower convergence but a more precise set of regression coefficients.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon ML 的上下文中，模型是线性回归，算法是**随机梯度下降**（**SGD**）算法。该算法有一个主要元参数，称为学习率，通常表示为 ![](img/B05028_2_17_eqn.png)，它决定了每个迭代更新权重时考虑新样本的程度。较大的学习率使算法收敛更快，但离最佳权重更远，而较小的学习率会导致收敛较慢，但回归系数集更精确。
- en: 'Given a training and a validation dataset, this is how Amazon ML tunes and
    select the best model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练集和验证集，这是 Amazon ML 调整和选择最佳模型的方法：
- en: Amazon trains several models, each with a different learning rate
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon 训练了多个模型，每个模型具有不同的学习率。
- en: 'For a given a learning rate:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的学习率：
- en: The training dataset allows the SGD to train the model by finding the best regression
    coefficients
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集允许 SGD 通过寻找最佳回归系数来训练模型。
- en: The model is used on the validation dataset to make predictions
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在验证集上用于进行预测。
- en: By comparing the quality of the predictions of the different models on that
    validation set, Amazon ML is able to select the best model and the associated
    best learning rate
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过比较不同模型在该验证集上的预测质量，Amazon ML 能够选择最佳模型及其相关的最佳学习率。
- en: The held-out set is used as final confirmation that the model is reliable
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留集被用作最终确认模型是否可靠。
- en: 'Usual splitting ratios for the training, validation, and held-out subsets are
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的训练、验证和保留子集分割比例如下：
- en: 'Training : *70%* validation and held-out 15% each'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练：*70%* 验证和保留 15% 各自
- en: 'Training : *60%* validation and held-out 20% each'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练：*60%* 验证和保留 20% 各自
- en: '**Shuffling**: It is important to make sure that the predictors and the outcome
    follow the same distribution in all three subsets. Shuffling the data before splitting
    it is an important part of creating reliable training, validation, and held-out subsets.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**洗牌**: 确保预测值和结果在所有三个子集中遵循相同的分布非常重要。在分割数据之前对数据进行洗牌是创建可靠训练集、验证集和保留集的重要部分。'
- en: It is important to define the data transformations on the training dataset and
    apply the transformation parameters on the validation and held-out subsets so
    that the validation and held-out subsets do not leak information back in the training
    set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集上定义数据转换，并在验证和保留子集上应用转换参数，以确保验证和保留子集不会将信息泄露回训练集。
- en: 'Take standardization as an example: the standard deviation and the mean of
    the predictors should be calculated on the training dataset. And these values
    then applied to standardize the validation and held-out sets. If you use the whole
    original dataset to calculate the mean and SGD, you leak information from the held-out
    set into the training set.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以标准化为例：预测变量的标准差和平均值应在训练数据集上计算。然后应用这些值来标准化验证集和保留集。如果你使用整个原始数据集来计算平均值和随机梯度下降（SGD），你将从保留集中泄露信息到训练集中。
- en: 'A common Supervised Predictive Analytics workflow follows these steps - Let''s
    assume we have an already extracted dataset and that we have chosen a metric to
    assess the quality of our predictions:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的监督预测分析工作流程遵循以下步骤 - 假设我们已经提取了数据集，并且我们已经选择了一个指标来评估预测质量：
- en: Building the dataset
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建数据集
- en: Cleaning up and transforming the data to handle noisy data issues
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和转换数据以处理噪声数据问题
- en: Creating new predictors
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新的预测变量
- en: Shuffling and splitting the data into a training, a validation and a held-out
    set
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱并分割数据为训练、验证和保留集
- en: Selecting the best model
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: Choosing a model (linear, tree-based, bayesian , ...)
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个模型（线性、基于树的、贝叶斯等）
- en: 'Repeat for several values of the meta parameters:'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对元参数的几个值进行重复：
- en: Train the model on the training set
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型
- en: Assess the model performance on the validation set
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证集上评估模型性能
- en: Repeat steps 1 and 2 with new data, new predictors, and other model parameters
    until you are satisfied with the performances of your model. Keep the best model.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新数据、新预测变量和其他模型参数重复步骤1和2，直到你对模型的性能满意。保留最佳模型。
- en: Final test of the model on the held-out subset.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保留的子集上对模型的最终测试。
- en: In the context of Amazon ML, there is no possibility to choose a model (step
    2) other than a linear regression one (logistic regression for classification).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon ML的上下文中，除了线性回归模型（分类时为逻辑回归）之外，没有其他选择模型（步骤2）的可能性。
- en: Identifying and correcting poor performances
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和纠正不良性能
- en: A performant predictive model is one that produces reliable and satisfying predictions
    on new data. There are two situations where the model will fail to consistently
    produce good predictions, and both depend on how the model is trained. A poorly
    trained model will result in underfitting, while an overly trained model will
    result in overfitting.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一个性能良好的预测模型是指在新数据上产生可靠和令人满意的预测。模型无法持续产生良好预测的情况有两种，并且两者都取决于模型是如何训练的。一个训练不良的模型会导致欠拟合，而一个过度训练的模型会导致过拟合。
- en: Underfitting
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合
- en: '**Underfitting** means that the model was poorly trained. Either the training
    dataset did not have enough information to infer strong predictions, or the algorithm
    that trained the model on the training dataset was not adequate for the context.
    The algorithm was not well parameterized or simply inadequate for the data.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合**意味着模型训练不良。要么是训练数据集没有足够的信息来推断强预测，要么是训练数据集上训练模型的算法不适合该上下文。算法参数设置不当或简单地不适合数据。'
- en: 'If we measure the prediction error not only on the validation set but also
    on the training set, the prediction error will be large if the model is underfitting.
    Which makes sense: if the model cannot predict the training, it won''t be able
    to predict the outcomes in the validation set it has not seen before. Underfitting
    basically means your model is not working.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在验证集和训练集上测量预测误差，当模型欠拟合时，预测误差会很大。这是有道理的：如果模型无法预测训练数据，它将无法预测之前未见过的验证集的输出。欠拟合基本上意味着你的模型不起作用。
- en: 'Common strategies to palliate this problem include:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解此问题的常见策略包括：
- en: Getting more data samples – If the problem comes from a dataset that is too
    small or does not contain sufficient information, getting more data may improve
    the model performance.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更多数据样本 - 如果问题来自一个太小或信息不足的数据集，获取更多数据可能会提高模型性能。
- en: Adding more features, raw or via feature engineering – by taking the log, squaring,
    binning, using splines or power functions. Adding many features and seeing how
    that improves the predictions.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多特征，原始的或通过特征工程 - 通过取对数、平方、分箱、使用样条或幂函数。添加许多特征并观察这如何提高预测。
- en: Choosing another model – Support Vector Machine, Random Forest, Boosted trees,
    Bayes classifiers all have different strengths in different contexts.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择另一个模型 - 支持向量机、随机森林、提升树、贝叶斯分类器在不同的上下文中都有不同的优势。
- en: Overfitting
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: '**Overfitting** occurs when the model was so well trained that it fits the
    training data too perfectly and cannot handle new data.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**发生在模型训练得如此之好，以至于它完美地拟合了训练数据，却无法处理新数据。'
- en: 'Say you have a unique predictor of an outcome and that the data follows a quadratic
    pattern:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个结果的唯一预测变量，并且数据遵循二次模式：
- en: You fit a linear regression on that data ![](img/B05028_2_18_eqn.png), the predictions
    are weak. Your model is underfitting the data. There is a high error level on
    both the training error and the validation dataset.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你在那些数据上拟合了线性回归![img/B05028_2_18_eqn.png]，预测效果较弱。你的模型未能很好地拟合数据。训练误差和验证数据集上的误差水平都很高。
- en: You add the square of the predictor in the model ![](img/B05028_2_19_eqn.png)
    and find that your model makes good predictions. The error on both the training
    and the validation datasets are equivalent and lower than for the simpler model.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你在模型中添加了预测变量的平方![img/B05028_2_19_eqn.png]，并发现你的模型做出了良好的预测。训练集和验证数据集上的误差都相当于更简单的模型，且更低。
- en: If you increase the number and power of polynomial features so that the model
    is now ![](img/B05028_2_20_eqn.png), you end up fitting the training data too
    closely. The model has a very low prediction error on the training dataset but
    is unable to predict anything on new data. The prediction error on the validation
    dataset remains high.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你增加多项式特征的数量和幂，使得模型现在是![img/B05028_2_20_eqn.png]，你最终会过于紧密地拟合训练数据。模型在训练数据集上的预测误差非常低，但无法对新数据进行预测。验证数据集上的预测误差仍然很高。
- en: This is a case of overfitting.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个过拟合的例子。
- en: 'The following graph shows an example of an overfitting model with regard to
    the previous quadratic dataset, by setting a high order for the polynomial regression
    (*n = 16*). The polynomial regression fits the training data so well it would
    be incapable of any predictions on new data whereas the quadratic model (*n =
    2*) would be more robust:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了关于先前二次数据集的一个过拟合模型示例，通过设置多项式回归的高阶（*n = 16*）。多项式回归拟合训练数据如此之好，以至于它将无法对新数据进行任何预测，而二次模型（*n
    = 2*）将更加稳健：
- en: '![](img/image_02_035.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![img/image_02_035.png]'
- en: The best way to detect overfitting is, therefore, to compare the prediction
    errors on the training and validation sets. A significant gap between the two
    errors implies overfitting. A way to prevent this overfitting from happening is
    to add constraints on the model. In machine learning, we use regularization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，检测过拟合的最佳方式是对比训练集和验证集上的预测误差。两个误差之间存在显著差距意味着过拟合。防止这种过拟合发生的一种方法是在模型上添加约束。在机器学习中，我们使用正则化。
- en: Regularization on linear models
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型上的正则化
- en: 'The Stochastic Gradient Descent algorithm (SGD) finds the optimal weights *{w[i]}*
    of the model by minimizing the error between the true and the predicted values
    on the N training samples:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降算法（SGD）通过最小化N个训练样本上真实值和预测值之间的误差来找到模型的最佳权重 *{w[i]}*：
- en: '![](img/B05028_2_22_eqn.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![img/B05028_2_22_eqn.png]'
- en: Where ![](img/image_02_038.png) are the predicted values, *ŷ[i]* the real values
    to be predicted; we have *N* samples, and each sample has *n* dimensions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![img/image_02_038.png]是预测值，*ŷ[i]*是待预测的真实值；我们有*N*个样本，每个样本有*n*个维度。
- en: 'Regularization consists of adding a term to the previous equation and to minimize
    the regularized error:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化包括向先前方程中添加一个项，并最小化正则化误差：
- en: '![](img/B05028_2_24_eqn.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![img/B05028_2_24_eqn.png]'
- en: The ![](img/image_02_042.png) parameter helps quantify the amount of regularization,
    while *R(w)* is the regularization term dependent on the regression coefficients.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image_02_042.png]参数有助于量化正则化的程度，而*R(w)*是依赖于回归系数的正则化项。'
- en: 'There are two types of weight constraints usually considered:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通常考虑两种类型的权重约束：
- en: 'L2 regularization as the sum of the squares of the coefficients:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化作为系数平方的和：
- en: '![](img/B05028_2_25_eqn.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![img/B05028_2_25_eqn.png]'
- en: 'L1 regularization as the sum of the absolute value of the coefficients:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1正则化作为系数绝对值的和：
- en: '![](img/B05028_2_26_eqn.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![img/B05028_2_26_eqn.png]'
- en: 'The constraint on the coefficients introduced by the regularization term *R(w)*
    prevents the model from overfitting the training data. The coefficients become
    tied together by the regularization and can no longer be tightly leashed to the
    predictors. Each type of regularization has its characteristic and gives rise
    to different variations on the SGD algorithm, which we now introduce:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项*R(w)*引入的系数约束防止模型过度拟合训练数据。系数通过正则化相互关联，不能再紧密地与预测因子绑定。每种正则化都有其特征，并导致SGD算法的不同变体，我们现在介绍：
- en: L2 regularization and Ridge
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2正则化和岭
- en: L2 regularization prevents the weights *{w[i]}* from being too spread. The smaller
    weights that rise up for non-correlated though potentially meaningful, features
    will not become insignificant when compared to the weights associated to the important
    correlated features. L2 regularization will enforce similar scaling of the weights.
    A direct consequence of L2 regularization is to reduce the negative impact of
    collinearity, since the weights can no longer diverge from one another.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化防止权重 *{w[i]}* 过度分散。对于非相关但可能具有意义的特征，上升的小权重不会与重要的相关特征关联的权重相比变得不重要。L2正则化将强制执行相似的权重缩放。L2正则化的直接后果是减少共线性对负面的影响，因为权重不能再相互分离。
- en: The Stochastic Gradient Descent algorithm with L2 regularization is known as
    the **Ridge algorithm**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 带有L2正则化的随机梯度下降算法被称为**岭算法**。
- en: L1 regularization and Lasso
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1正则化和Lasso
- en: L1 regularization usually entails some loss of predictive power of the model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化通常会导致模型预测能力的某些损失。
- en: One of the properties of L1 regularization is to force the smallest weights
    to 0 and thereby reduce the number of features taken into account in the model.
    This is a desired behavior when the number of features (*n*) is large compared
    to the number of samples (*N*). L1 is better suited for datasets with many features.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化的一种特性是迫使最小的权重为0，从而减少模型中考虑的特征数量。当特征数量（*n*）与样本数量（*N*）相比很大时，这是一种期望的行为。L1更适合具有许多特征的集合。
- en: The Stochastic Gradient Descent algorithm with L1 regularization is known as
    the **Least Absolute Shrinkage and Selection Operator** **(Lasso)** algorithm.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 带有L1正则化的随机梯度下降算法被称为**最小绝对收缩和选择算子**（Lasso）算法。
- en: 'In both cases the hyper-parameters of the model are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，模型的超参数如下：
- en: The learning rate ![](img/B05028_2_17_eqn.png) of the SGD algorithm
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD算法的学习率 ![](img/B05028_2_17_eqn.png)
- en: A parameter ![](img/image_02_042.png) to tune the amount of regularization added
    to the model
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个参数 ![](img/image_02_042.png)来调整添加到模型中的正则化量
- en: A third type of regularization called **ElasticNet** consists in adding both
    a L2 and a L1 regularization term to the model. This brings up the best of both
    regularization schemas at the expense of an extra hyper-parameter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为**ElasticNet**的第三种正则化方法，包括在模型中添加L2和L1正则化项。这以增加一个额外的超参数为代价，带来了两种正则化方案的优点。
- en: In other contexts, although experts have different opinions ([https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization))
    on which type of regularization is more effective, the consensus seems to favor
    L2 over L1 regularization.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，尽管专家们对哪种正则化更有效有不同意见（[https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization)），但共识似乎更倾向于L2正则化优于L1正则化。
- en: 'L2 and L1 regularization are both available in Amazon ML while ElasticNet is
    not. The amount of regularization available is limited to three values for  ![](img/image_02_042.png):
    mild (1*0^(-6)*), medium (*10^(-4)*), and aggressive (*10^(-2)*).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon ML中，L2和L1正则化都是可用的，而ElasticNet不可用。可用的正则化量限于三个值：温和的（1*0^(-6)）、中等（*10^(-4)）和激进（*10^(-2)）。
- en: Evaluating the performance of your model
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型的性能
- en: 'Evaluating the predictive performance of a model requires defining a measure
    of the quality of its predictions. There are several available metrics both for
    regression and classification. The metrics used in the context of Amazon ML are
    the following ones:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型的预测性能需要定义其预测质量的一个度量。对于回归和分类都有几个可用的度量。在Amazon ML上下文中使用的度量如下：
- en: '**RMSE for regression**: The root mean squared error is defined by the square
    of the difference between the true outcome values and their predictions:'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE回归**：均方根误差定义为真实结果值与其预测值之间差异的平方：'
- en: '![](img/B05028_2_27_eqn.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_2_27_eqn.png)'
- en: '**F-1 Score and ROC-AUC for classification**: Amazon ML uses logistic regression
    for binary classification problems. For each prediction, logistic regression returns
    a value between 0 and 1\. This value is interpreted as a probability of the sample
    belonging to one of the two classes. A probability lower than 0.5 indicates belonging to
    the first class, while a probability higher than 0.5 indicates a belonging to
    the second class. The decision is therefore highly dependent on the value of the
    threshold. A value which we can modify.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-1分数和ROC-AUC用于分类**：Amazon ML使用逻辑回归来解决二元分类问题。对于每个预测，逻辑回归返回一个介于0和1之间的值。这个值被解释为样本属于两个类别之一的概率。小于0.5的概率表示属于第一个类别，而大于0.5的概率表示属于第二个类别。因此，决策高度依赖于阈值值。我们可以修改这个值。'
- en: 'Denoting one class positive and the other negative, we have four possibilities
    depicted in the following table:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示一个类别为正，另一个为负，我们有以下表格中的四种可能性：
- en: '|  | **Predicted Yes** | **Predicted No** |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测为是** | **预测为否** |'
- en: '| **Real value: Yes** | True Positive (TP) | False Negative (FN) (or type II
    error) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **真实值：是** | 真正例（TP） | 假负例（FN）（或第二类错误） |'
- en: '| **Real value: No** | False Positive (FP) | True Negative |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| **真实值：否** | 假正例（FP） | 真负例 |'
- en: 'This matrix is called a **confusion matrix** ([https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix))
    . It defines four indicators of the performance of a classification model:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个矩阵被称为**混淆矩阵**([https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix))。它定义了分类模型性能的四个指标：
- en: 'TP: How many Yes were correctly predicted Yes'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TP：有多少个“是”被正确预测为“是”
- en: 'FP: How many No were wrongly predicted Yes'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP：有多少个“否”被错误地预测为“是”
- en: 'FN: How many Yes were wrongly predicted No'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FN：有多少个“是”被错误地预测为“否”
- en: 'TN: How many No were correctly predicted No'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TN：有多少个“否”被正确预测为“否”
- en: 'From these four indicators, we can define the following metrics:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这四个指标中，我们可以定义以下度量：
- en: '**Recall:** This denotes the amount of predicted positives actually positive.
    Recall is also called **True Positive Rate** (**TPR**) or sensitivity. It is the
    probability of detection:'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这表示预测为正的实际正例数量。召回率也称为**真正例率**（**TPR**）或灵敏度。它是检测的概率：'
- en: '*Recall = (TP / TP + FN)*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率 = (TP / TP + FN)*'
- en: '**Precision** as the fraction of the real positives over all the positive predicted
    values:'
  id: totrans-237
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度**作为所有预测为正的实际正例的分数：'
- en: '*Precision = (TP / TP + FP)*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*精度 = (TP / TP + FP)*'
- en: '**False Positive Rate** is the number of falsely predicted positives over all
    the true negatives. It''s the probability of false alarm:'
  id: totrans-239
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例率**是所有真实负例中错误预测为正例的数量。它是误报的概率：'
- en: '*FPR = FP / FP + TN*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*FPR = FP / FP + TN*'
- en: 'Finally, the **F1-score** is defined as the weighted average of the recall
    and the precision, and is given by the following:'
  id: totrans-241
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，**F1分数**定义为召回率和精度的加权平均值，其计算公式如下：
- en: '*F1-score = 2 TP / ( 2 TP + FP + FN)*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1分数 = 2 TP / ( 2 TP + FP + FN)*'
- en: A F1 score is always between 0 and 1, with 1 the best value and 0 the worst
    one.
  id: totrans-243
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1分数总是在0到1之间，1是最佳值，0是最差值。
- en: As noted previously, these scores are all dependent on the initial threshold
    used to interpret the result of the logistic regression in order to decide when
    a prediction belongs to one class or the other. We can choose to vary that threshold.
    This is where the ROC-AUC comes in.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些分数都依赖于最初用于解释逻辑回归结果并决定预测属于哪个类别的初始阈值。我们可以选择改变这个阈值。这就是ROC-AUC发挥作用的地方。
- en: 'If you plot the True Positive Rate (Recall) against the False Positive Rate
    for different values of the decision threshold, you obtain a graph like the following,
    called the **Receiver Operating Characteristic** or **ROC curve**:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将真正例率（召回率）与不同决策阈值下的假正例率进行比较，你会得到一个如图所示的图形，称为**接收者操作特征**或**ROC曲线**：
- en: '![](img/image_02_054-1.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_054-1.png)'
- en: The diagonal line indicates an equal probability of belonging to one class or
    another. The closer the curve is to the upper-left corner, the better your model
    performances are.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角线表示属于一个类别或另一个类别的概率相等。曲线越接近右上角，你的模型性能越好。
- en: The ROC curve has been widely used since WWII, when it was first invented to
    detect enemy planes in radar signals.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线自二战以来被广泛使用，当时它首次被发明出来用于在雷达信号中检测敌机。
- en: Once you have the ROC curve, you can calculate the **Area Under the Curve**
    or **AUC**.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦您有了ROC曲线，您就可以计算**曲线下面积**或**AUC**。
- en: The AUC will give you a unique score for your model taking into account all
    the possible values for the probability threshold from 0 to 1\. The higher the
    AUC the better.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC将为您提供一个独特的分数，考虑了从0到1的所有可能的概率阈值。AUC值越高，表示模型越好。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we focused on two important elements of a predictive analytics
    project: the data and the evaluation of the predictive power of the model. We
    first listed the most common problems encountered with raw data, their impact
    on the linear regression model, and ways to solve them. The reader should now
    be able to identify and deal with missing values, outliers, imbalanced datasets,
    and normalization.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们关注了预测分析项目中的两个重要元素：数据和模型预测能力的评估。我们首先列出了与原始数据最常见的问题，它们对线性回归模型的影响以及解决这些问题的方法。现在，读者应该能够识别和处理缺失值、异常值、不平衡数据集和归一化问题。
- en: 'We also introduced the two most frequent problems in predictive analytics:
    underfitting and overfitting. L1 and L2 regularization is an important element
    in the Amazon ML platform, which helps overcome overfitting and make models more
    robust and able to handle previously unseen data.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了预测分析中最常见的两个问题：欠拟合和过拟合。L1和L2正则化是Amazon ML平台的一个重要元素，它有助于克服过拟合，并使模型更加稳健，能够处理以前未见过的数据。
- en: We are now ready to dive into the Amazon Machine Learning platform in the next
    chapter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好在下一章深入探讨Amazon机器学习平台。
